{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c463bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'MaxText' from '/home/shuningjin/maxtext/MaxText/__init__.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/home/shuningjin_google_com/maxtext\")\n",
    "# sys.path.append(\"/home/shuningjin_google_com/maxtext/MaxText\")\n",
    "\n",
    "\n",
    "# import os\n",
    "\n",
    "# # 1. Get the current working directory (optional, for verification)\n",
    "# print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# # 2. Define the path to your MaxText directory\n",
    "# # Replace '/path/to/your/maxtext_directory' with the actual absolute path\n",
    "# # to your MaxText project directory.\n",
    "# # Example: If MaxText is in your home folder, it might be:\n",
    "# # maxtext_path = os.path.expanduser(\"~/MaxText\")\n",
    "# maxtext_path = \"/home/shuningjin_google_com/maxtext\"\n",
    "\n",
    "# # 3. Change the current working directory\n",
    "# try:\n",
    "#     os.chdir(maxtext_path)\n",
    "#     print(f\"Successfully changed directory to: {os.getcwd()}\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: The directory '{maxtext_path}' was not found.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd822109",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright 2023 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=g-bad-todo, abstract-method, consider-using-with\n",
    "\"\"\"Transforms a \"full state\" including optimizer state to a bfloat16 \"parameter state\" without optimizer state.\n",
    "   This typically used for turning a state output by training.py into a state than can be consumed by decode.py.\n",
    "\n",
    "   The input \"fullstate\" is passed in via:\n",
    "     load_full_state_path.\n",
    "   The output \"parameter state\" is output to the checkpoint directory. Additionally it is cast down to bf16.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "import MaxText\n",
    "reload(MaxText)\n",
    "\n",
    "\n",
    "import os.path\n",
    "from typing import Sequence\n",
    "\n",
    "from absl import app\n",
    "\n",
    "from etils import epath\n",
    "\n",
    "import jax\n",
    "from jax.sharding import Mesh\n",
    "from jax import random\n",
    "\n",
    "from MaxText import checkpointing\n",
    "reload(checkpointing)\n",
    "from MaxText import max_logging\n",
    "from MaxText import max_utils\n",
    "from MaxText import maxtext_utils\n",
    "reload(maxtext_utils)\n",
    "from MaxText import optimizers\n",
    "from MaxText import pyconfig\n",
    "from MaxText.common_types import DecoderBlockType\n",
    "from MaxText.layers import models, quantizations\n",
    "from MaxText.train import save_checkpoint\n",
    "from MaxText.utils import gcs_utils\n",
    "from MaxText.utils import lora_utils\n",
    "\n",
    "Transformer = models.Transformer\n",
    "\n",
    "\n",
    "def _possibly_unroll_params(config, training_state, training_state_annotations, mesh):\n",
    "  \"\"\"Unroll scanned input layers when force_unroll is set.\"\"\"\n",
    "  if not config.scan_layers or not config.force_unroll:\n",
    "    return\n",
    "\n",
    "  def unroll_layer_group(num_layers, layer_name=\"layers\"):\n",
    "    \"\"\"Helper function to unroll layers (e.g. dense or MoE) into individual layers.\"\"\"\n",
    "    layers = training_state.params[\"params\"][\"decoder\"].get(layer_name, None)\n",
    "    layers_annotations = training_state_annotations.params[\"params\"][\"decoder\"].get(layer_name, None)\n",
    "\n",
    "    if layers is None or layers_annotations is None:\n",
    "      raise ValueError(f\"Missing {layer_name} in training_state or training_state_annotations.\")\n",
    "\n",
    "    def new_pspec(x):\n",
    "      return jax.sharding.PartitionSpec(*(x[0 : config.param_scan_axis] + x[config.param_scan_axis + 1 :]))\n",
    "\n",
    "    new_layer_annotation = jax.tree_util.tree_map(new_pspec, layers_annotations)\n",
    "    new_layer_sharding = jax.tree_util.tree_map(lambda x: jax.sharding.NamedSharding(mesh, x), new_layer_annotation)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "\n",
    "      def slice_ith(input_layers):\n",
    "        return jax.tree_util.tree_map(lambda x: jax.numpy.take(x, i, axis=config.param_scan_axis), input_layers)\n",
    "\n",
    "      # pylint: disable=not-callable\n",
    "      new_layer = jax.jit(slice_ith, out_shardings=new_layer_sharding)(layers)\n",
    "\n",
    "      training_state.params[\"params\"][\"decoder\"][f\"{layer_name}_{i}\"] = new_layer\n",
    "      training_state_annotations.params[\"params\"][\"decoder\"][f\"{layer_name}_{i}\"] = new_layer_annotation\n",
    "\n",
    "    # Remove the original layer collection\n",
    "    del training_state.params[\"params\"][\"decoder\"][layer_name]\n",
    "    del training_state_annotations.params[\"params\"][\"decoder\"][layer_name]\n",
    "\n",
    "    jax.tree_util.tree_map(lambda x: x.delete(), layers)\n",
    "\n",
    "  if config.decoder_block == DecoderBlockType.DEEPSEEK:\n",
    "    # Unroll dense and MoE layers separately\n",
    "    unroll_layer_group(config.first_num_dense_layers, layer_name=\"dense_layers\")\n",
    "    unroll_layer_group(config.num_decoder_layers - config.first_num_dense_layers, layer_name=\"moe_layers\")\n",
    "  else:\n",
    "    unroll_layer_group(config.num_decoder_layers, layer_name=\"layers\")\n",
    "\n",
    "\n",
    "def _read_train_checkpoint(config, checkpoint_manager, mesh):\n",
    "  \"\"\"Read training checkpoint at path defined by load_full_state_path.\"\"\"\n",
    "  # Model and Optimizer definition\n",
    "  quant = quantizations.configure_quantization(config)\n",
    "  model = Transformer(config, mesh, quant)\n",
    "  rng = random.PRNGKey(0)\n",
    "  learning_rate_schedule = maxtext_utils.create_learning_rate_schedule(config)\n",
    "  tx = optimizers.get_optimizer(config, learning_rate_schedule)\n",
    "  state, state_mesh_notations, _, _ = maxtext_utils.setup_training_state(\n",
    "      model, None, tx, config, rng, mesh, checkpoint_manager\n",
    "  )\n",
    "  num_params = max_utils.calculate_num_params_from_pytree(state.params)\n",
    "  max_logging.log(f\"In input checkpoint Number of model params={num_params/1e9:.3f} billion\")\n",
    "  return state, state_mesh_notations\n",
    "\n",
    "\n",
    "# def _generate_lora_decode_checkpoints(config, mesh):\n",
    "#   \"\"\"Read lora checkpoints checkpoint at path defined by load_full_state_path.\"\"\"\n",
    "#   # Model and Optimizer definition\n",
    "#   quant = quantizations.configure_quantization(config)\n",
    "#   model = Transformer(config, mesh, quant)\n",
    "#   rng = random.PRNGKey(0)\n",
    "#   learning_rate_schedule = maxtext_utils.create_learning_rate_schedule(config)\n",
    "#   tx = optimizers.get_optimizer(config, learning_rate_schedule)\n",
    "\n",
    "#   lora_adapters = gcs_utils.gcs_list_directories(config.lora_input_adapters_path)\n",
    "#   for lora_id in lora_adapters:\n",
    "#     # Expected lora_checkpoint_dir = <checkpoint_dir>/loras/<lora_id>\n",
    "#     lora_checkpoint_dir = os.path.join(config.checkpoint_dir, \"loras\", lora_id, \"\")\n",
    "\n",
    "#     lora_adapter_path = os.path.join(config.lora_input_adapters_path, lora_id, \"\")\n",
    "\n",
    "#     # Create a checkpoint manager to save decode checkpoint at lora_checkpoint_dir\n",
    "#     checkpoint_manager = checkpointing.create_orbax_checkpoint_manager(\n",
    "#         lora_checkpoint_dir,\n",
    "#         config.enable_checkpointing,\n",
    "#         config.async_checkpointing,\n",
    "#         config.checkpoint_period,\n",
    "#     )\n",
    "\n",
    "#     lora_config, lora_state, lora_state_annotations = lora_utils.setup_initial_lora_state(\n",
    "#         model, None, tx, config, rng, mesh, checkpoint_manager, lora_adapter_path\n",
    "#     )\n",
    "\n",
    "#     _possibly_unroll_params(config, lora_state, lora_state_annotations, mesh)\n",
    "\n",
    "#     gcs_utils.write_dict_to_gcs_json(lora_config, os.path.join(lora_checkpoint_dir, \"adapter_config.json\"))\n",
    "\n",
    "#     # Save decode state to config's checkpoint directory at step 0\n",
    "#     _save_decode_checkpoint(config, lora_state, checkpoint_manager)\n",
    "#     max_logging.log(f\"Successfully saved LoRA checkpoint at: {os.path.join(lora_checkpoint_dir, '0', 'items')}\")\n",
    "\n",
    "\n",
    "# def _save_decode_checkpoint(config, state, checkpoint_manager):\n",
    "#   \"\"\"Generate checkpoint for decode from the training_state.\"\"\"\n",
    "#   decode_state = maxtext_utils.init_decode_state(\n",
    "#       None, jax.tree_util.tree_map(lambda x: x.astype(jax.numpy.bfloat16), state.params)\n",
    "#   )\n",
    "#   if checkpoint_manager is not None:\n",
    "#     if save_checkpoint(checkpoint_manager, 0, decode_state):\n",
    "#       max_logging.log(f\"saved an decode checkpoint at {config.checkpoint_dir}\")\n",
    "#   checkpoint_manager.wait_until_finished()\n",
    "\n",
    "\n",
    "def generate_decode_checkpoint(config):\n",
    "  \"\"\"\n",
    "  Generate an decode checkpoint from a given training checkpoint.\n",
    "  - Training checkpoint is loaded from config.load_full_state_path.\n",
    "  - Inference checkpoint will be saved at the config's checkpoint directory.\n",
    "  \"\"\"\n",
    "\n",
    "  devices_array = maxtext_utils.create_device_mesh(config)\n",
    "  mesh = Mesh(devices_array, config.mesh_axes)\n",
    "\n",
    "  assert config.checkpoint_dir, \"checkpoint_dir not configured\"\n",
    "  # Remove any old checkpoint\n",
    "  path = epath.Path(config.checkpoint_dir)\n",
    "  if path.exists():\n",
    "    if jax.process_index() == 0:\n",
    "      path.rmtree()\n",
    "\n",
    "  # Create a checkpoint manager to save decode checkpoint at config.checkpoint_dir\n",
    "  base_checkpoint_dir = config.checkpoint_dir\n",
    "\n",
    "  # if config.lora_input_adapters_path:\n",
    "  #   base_checkpoint_dir += \"base/\"\n",
    "\n",
    "  checkpoint_manager = checkpointing.create_orbax_checkpoint_manager(\n",
    "      base_checkpoint_dir,\n",
    "      config.enable_checkpointing,\n",
    "      config.async_checkpointing,\n",
    "      config.checkpoint_period,\n",
    "  )\n",
    "  # Read training state from config.load_paramaters_path\n",
    "  max_logging.log(f\"Read training checkpoint from: {config.load_full_state_path}\")\n",
    "  training_state, training_state_annotations = _read_train_checkpoint(config, checkpoint_manager, mesh)\n",
    "  _possibly_unroll_params(config, training_state, training_state_annotations, mesh)\n",
    "  return training_state, training_state_annotations\n",
    "  # assert training_state.opt_state != {}, \"missing opt_state in training checkpoint\"\n",
    "\n",
    "  # print(training_state)\n",
    "  # print_nested_keys(training_state.params)\n",
    "\n",
    "  # def print_nested_keys(data, prefix=\"\"):\n",
    "  #   \"\"\"\n",
    "  #   Prints nested keys of a dictionary-like structure in a directory-like format.\n",
    "  #   Args:\n",
    "  #       data: The dictionary-like structure to traverse.\n",
    "  #       prefix: The current path prefix.\n",
    "  #   \"\"\"\n",
    "  #   if isinstance(data, dict):\n",
    "  #     for key, value in data.items():\n",
    "  #       current_path = f\"{prefix}{key}.\"\n",
    "  #       print_nested_keys(value, current_path)\n",
    "  #   else:\n",
    "  #     print(f\"key: {prefix}\")\n",
    "  #     print(f\"value shape: {data.shape}\")\n",
    "\n",
    "\n",
    "#   \n",
    "\n",
    "#   # Save decode state to config's checkpoint directory at step 0\n",
    "#   max_logging.log(f\"Save decode checkpoint at: {base_checkpoint_dir}\")\n",
    "#   _save_decode_checkpoint(config, training_state, checkpoint_manager)\n",
    "#   max_logging.log(f\"Successfully generated decode checkpoint at: {base_checkpoint_dir}0/items\")\n",
    "\n",
    "#   if config.lora_input_adapters_path:\n",
    "#     _generate_lora_decode_checkpoints(config, mesh)\n",
    "\n",
    "  # return True\n",
    "\n",
    "\n",
    "# def main(argv: Sequence[str]) -> None:\n",
    "#   print(argv)\n",
    "#   config = pyconfig.initialize(argv)\n",
    "#   generate_decode_checkpoint(config)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#   app.run(main)\n",
    "\n",
    "\n",
    "def print_nested_keys(data, prefix=\"\"):\n",
    "  \"\"\"\n",
    "  Prints nested keys of a dictionary-like structure in a directory-like format.\n",
    "  Args:\n",
    "      data: The dictionary-like structure to traverse.\n",
    "      prefix: The current path prefix.\n",
    "  \"\"\"\n",
    "  if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "      current_path = f\"{prefix}{key}.\"\n",
    "      print_nested_keys(value, current_path)\n",
    "  else:\n",
    "    print(f\"key: {prefix}\")\n",
    "    print(f\"value shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f137f",
   "metadata": {},
   "source": [
    "## scanned param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc1550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'force_unroll', 'scan_layers', 'base_output_directory', 'skip_jax_distributed_system']\n",
      "Running Model: llama2-7b\n",
      "Updating following parameters in config\n",
      "\n",
      "base_emb_dim: 4096\n",
      "base_num_query_heads: 32\n",
      "base_num_kv_heads: 32\n",
      "base_mlp_dim: 11008\n",
      "base_num_decoder_layers: 32\n",
      "head_dim: 128\n",
      "mlp_activations: ['silu', 'linear']\n",
      "vocab_size: 32000\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "normalization_layer_epsilon: 1e-05\n",
      "decoder_block: llama2\n",
      "logical_axis_rules: [['norm', 'fsdp']]\n",
      "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'enable_dropout', 'logits_via_embedding', 'normalization_layer_epsilon', 'decoder_block', 'logical_axis_rules']\n",
      "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: autoselected\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 4096\n",
      "Config param base_mlp_dim: 11008\n",
      "Config param base_moe_mlp_dim: 7168\n",
      "Config param base_num_decoder_layers: 32\n",
      "Config param base_num_kv_heads: 32\n",
      "Config param base_num_query_heads: 32\n",
      "Config param base_output_directory: gs://runner-maxtext-logs\n",
      "Config param beta_fast: 32\n",
      "Config param beta_slow: 1\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: gs://runner-maxtext-logs/test3/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_concurrent_gb: 96\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param chunk_attn_window_size: 0\n",
      "Config param collect_stack_trace: False\n",
      "Config param colocated_python_data_input: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param context: remat\n",
      "Config param context_parallel_load_balance: True\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_context_autoregressive_parallelism: 1\n",
      "Config param dcn_context_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param dcn_tensor_transpose_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: DecoderBlockType.LLAMA2\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dtype_mm: float32\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param dump_step: -1\n",
      "Config param emb_dim: 4096\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_gcp_goodput_metrics: True\n",
      "Config param enable_gcp_step_deviation_metrics: True\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_llm_inference_pool: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_padding_causal_mask: True\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_prefix_caching: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param enable_tensorboard: True\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 12.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param first_num_dense_layers: 0\n",
      "Config param float32_logits: False\n",
      "Config param float32_qk_product: False\n",
      "Config param force_unroll: False\n",
      "Config param freeze_vision_encoder_params: True\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param generate_slice: v5e-16\n",
      "Config param global_batch_size_to_eval_on: 12\n",
      "Config param global_batch_size_to_load: 12\n",
      "Config param global_batch_size_to_load_eval: 12\n",
      "Config param global_batch_size_to_train_on: 12\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 30\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_file_type: arrayrecord\n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param grain_worker_count_eval: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param heartbeat_reporting_interval_in_seconds: 5\n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param hidden_size_for_vit: 1408\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_context_autoregressive_parallelism: 1\n",
      "Config param ici_context_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param ici_tensor_transpose_parallelism: 1\n",
      "Config param image_path: \n",
      "Config param image_size_for_vit: 896\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param inhomogeneous_layer_cycle_interval: 1\n",
      "Config param init_weights_seed: 0\n",
      "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
      "Config param interleave_moe_layer_step: 1\n",
      "Config param intermediate_size_for_vit: 5632\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_lora_rank: 512\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 150001\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: gs://shuningjin-multipod-dev/llama2-7b/2025-06-22/scanned/0/items\n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param local_rope_max_timescale: -1\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()), ('norm', 'fsdp'))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param lora_input_adapters_path: \n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_position_embeddings: 163840\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 2048\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: gs://runner-maxtext-logs/test3/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 12\n",
      "Config param micro_batch_size_to_train_on: 12\n",
      "Config param mla_naive_kvcache: True\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 11008\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_name: llama2-7b\n",
      "Config param moe_mlp_dim: 7168\n",
      "Config param monitor_goodput: False\n",
      "Config param monitor_step_time_deviation: True\n",
      "Config param mscale: 1.0\n",
      "Config param mu_dtype: float32\n",
      "Config param multi_sampling: False\n",
      "Config param n_routing_groups: -1\n",
      "Config param nope_layer_interval: -1\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_attention_heads_for_vit: 16\n",
      "Config param num_channels_for_vit: 3\n",
      "Config param num_decoder_layers: 32\n",
      "Config param num_epoch: 1\n",
      "Config param num_experts: 1\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_hidden_layers_for_vit: 34\n",
      "Config param num_kv_heads: 32\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 32\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimize_mesh_for_tpu_v6e: False\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param original_max_position_embeddings: 4096\n",
      "Config param out_proj: remat\n",
      "Config param override_model_config: False\n",
      "Config param packing: True\n",
      "Config param pagedattn_head_dim_alignment: 128\n",
      "Config param pagedattn_max_pages_per_group: 64\n",
      "Config param pagedattn_num_pages: 64\n",
      "Config param pagedattn_pages_per_compute_block: 4\n",
      "Config param pagedattn_tokens_per_page: 32\n",
      "Config param param_scan_axis: 1\n",
      "Config param parameter_memory_host_offload: False\n",
      "Config param patch_size_for_vit: 14\n",
      "Config param per_device_batch_size: 12.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param pipeline_fsdp_ag_once: False\n",
      "Config param pipeline_parallel_layers: -1\n",
      "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param prefill_chunk_size: 256\n",
      "Config param prefill_slice: v5e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1751012391.182729 1106180 common_lib.cc:528] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/libtpu_init_utils.cc:285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config param prefix_caching_dram_byte: 100000000000\n",
      "Config param prefix_caching_hbm_byte: 10000000000\n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param projector_dropout_for_vit: 0.0\n",
      "Config param projector_input_dim_for_vit: 4096\n",
      "Config param projector_output_dim_for_vit: 4096\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param q_lora_rank: 0\n",
      "Config param qk_nope_head_dim: 128\n",
      "Config param qk_rope_head_dim: 64\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param remat_policy_for_vit: minimal\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
      "Config param report_performance_metric_for_gcp_monitoring: False\n",
      "Config param reshape_q: False\n",
      "Config param return_log_prob: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_factor: 40\n",
      "Config param rope_max_timescale: 10000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param rope_theta_for_vit: 10000\n",
      "Config param rope_type: default\n",
      "Config param rope_use_scale: True\n",
      "Config param routed_bias: False\n",
      "Config param routed_scaling_factor: 1.0\n",
      "Config param routed_score_func: \n",
      "Config param run_name: test3\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_layers_per_stage: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sft_train_on_completion_only: False\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param shared_experts: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: True\n",
      "Config param sliding_window_size: 0\n",
      "Config param sparse_matmul: True\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param step_deviation_interval_seconds: 30\n",
      "Config param steps: 150001\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param temperature_tuning: False\n",
      "Config param tensorboard_dir: gs://runner-maxtext-logs/test3/tensorboard/\n",
      "Config param tile_activation_dim: 1024\n",
      "Config param tile_batch_seq: 512\n",
      "Config param tile_size_for_vit: 336\n",
      "Config param tile_weight_dim: 1024\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param tokenizer_type: sentencepiece\n",
      "Config param topk_routing_group: -1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param train_split: train\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_chat_template: False\n",
      "Config param use_chunked_prefill: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_multimodal: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_qk_norm: False\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_random_routing: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_sft: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param v_head_dim: 128\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vision_output_dim_for_vit: 4096\n",
      "Config param vocab_size: 32000\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: float32\n"
     ]
    }
   ],
   "source": [
    "cmd=\"python MaxText/configs/base.yml model_name=llama2-7b \\\n",
    "base_output_directory=gs://runner-maxtext-logs run_name=test3 \\\n",
    "load_parameters_path=gs://shuningjin-multipod-dev/llama2-7b/2025-06-22/scanned/0/items scan_layers=true force_unroll=false \\\n",
    "async_checkpointing=false skip_jax_distributed_system=true\"\n",
    "argv = cmd.split()\n",
    "config = pyconfig.initialize(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e54d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "Checkpoint manager created!\n",
      "Read training checkpoint from: \n",
      "checkpoint manager exists so trying to load this run's existing checkpoint\n",
      "restoring params from gs://shuningjin-multipod-dev/llama2-7b/2025-06-22/scanned/0/items\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "{'params': {'decoder': {'decoder_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('fsdp',), memory_kind=unpinned_host), global_shape=(4096,), shape=(4096,), strict=True)}, 'layers': {'mlp': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(4096, 32, 11008), shape=(4096, 32, 11008), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(4096, 32, 11008), shape=(4096, 32, 11008), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(11008, 32, 4096), shape=(11008, 32, 4096), strict=True)}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('fsdp', 'stage'), memory_kind=unpinned_host), global_shape=(4096, 32), shape=(4096, 32), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('fsdp', 'stage'), memory_kind=unpinned_host), global_shape=(4096, 32), shape=(4096, 32), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(4096, 32, 32, 128), shape=(4096, 32, 32, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(32, 32, 128, 4096), shape=(32, 32, 128, 4096), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(4096, 32, 32, 128), shape=(4096, 32, 32, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(4096, 32, 32, 128), shape=(4096, 32, 32, 128), strict=True)}}}, 'logits_dense': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(4096, 32000), shape=(4096, 32000), strict=True)}}, 'token_embedder': {'embedding': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(32000, 4096), shape=(32000, 4096), strict=True)}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0627 08:19:54.038715 1107048 google_auth_provider.cc:181] Running on GCE, using service account 770040921623-compute@developer.gserviceaccount.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In input checkpoint Number of model params=6.738 billion\n"
     ]
    }
   ],
   "source": [
    "training_state, training_state_annotations = generate_decode_checkpoint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f87216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainState(step=Array(0, dtype=int32, weak_type=True), apply_fn=<bound method Module.apply of Transformer(\n",
      "    # attributes\n",
      "    config = <MaxText.pyconfig.HyperParameters object at 0x7f7af17ca590>\n",
      "    mesh = Mesh(device_ids=array([[[[[[[[[[[[0]]]]]]]]]]]]), axis_names=('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'), axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto))\n",
      "    quant = None\n",
      ")>, params={'params': {'decoder': {'decoder_norm': {'scale': Array([1.8671875, 1.8671875, 1.8046875, ..., 1.71875  , 1.828125 ,\n",
      "       1.6015625], dtype=float32)}, 'layers': {'mlp': {'wi_0': {'kernel': Array([[[ 1.57470703e-02, -2.16674805e-03,  6.83593750e-03, ...,\n",
      "          1.41263008e-05,  2.66113281e-02, -5.98907471e-04],\n",
      "        [ 2.91748047e-02, -2.80761719e-02, -6.07299805e-03, ...,\n",
      "         -4.00390625e-02, -1.61132812e-02, -2.49023438e-02],\n",
      "        [ 8.60595703e-03, -2.27928162e-04,  1.76239014e-03, ...,\n",
      "          2.96630859e-02, -1.24511719e-02, -1.60980225e-03],\n",
      "        ...,\n",
      "        [ 8.97216797e-03,  7.41577148e-03,  3.80859375e-02, ...,\n",
      "         -1.44042969e-02,  1.32446289e-02,  2.12097168e-03],\n",
      "        [-3.34472656e-02, -1.91650391e-02, -2.18505859e-02, ...,\n",
      "         -2.44140625e-02,  3.14941406e-02, -8.54492188e-03],\n",
      "        [ 2.36511230e-03, -7.27539062e-02,  1.79443359e-02, ...,\n",
      "         -1.50756836e-02,  3.36914062e-02,  2.35595703e-02]],\n",
      "\n",
      "       [[ 1.70898438e-02, -6.01196289e-03, -2.16064453e-02, ...,\n",
      "         -3.22265625e-02,  2.01416016e-02, -1.13525391e-02],\n",
      "        [-9.52148438e-03, -1.81579590e-03, -2.89306641e-02, ...,\n",
      "          1.63574219e-02,  3.64685059e-03,  1.45874023e-02],\n",
      "        [ 2.39257812e-02,  2.44140625e-03, -8.85009766e-03, ...,\n",
      "         -1.04370117e-02, -1.57470703e-02,  1.22680664e-02],\n",
      "        ...,\n",
      "        [ 2.62451172e-02,  2.33154297e-02,  1.23596191e-03, ...,\n",
      "          7.50732422e-03,  1.37939453e-02,  1.44653320e-02],\n",
      "        [-3.14941406e-02, -3.46374512e-03, -1.06811523e-02, ...,\n",
      "         -1.13525391e-02,  2.50244141e-02, -2.41699219e-02],\n",
      "        [-9.52148438e-03, -2.63671875e-02,  2.97546387e-03, ...,\n",
      "         -2.53906250e-02,  1.64794922e-02, -3.24707031e-02]],\n",
      "\n",
      "       [[ 3.14941406e-02,  5.64575195e-03,  2.05078125e-02, ...,\n",
      "          5.79833984e-03, -1.70898438e-02, -2.36816406e-02],\n",
      "        [ 3.73535156e-02, -3.14941406e-02,  3.12805176e-03, ...,\n",
      "         -4.24194336e-03, -3.73535156e-02,  5.46264648e-03],\n",
      "        [-1.27563477e-02, -4.39453125e-03,  2.66113281e-02, ...,\n",
      "         -1.48925781e-02, -3.14331055e-03,  4.79125977e-03],\n",
      "        ...,\n",
      "        [-1.02539062e-02, -1.10473633e-02, -1.19628906e-02, ...,\n",
      "         -1.51367188e-02, -3.27148438e-02,  1.47247314e-03],\n",
      "        [-4.51660156e-03,  3.22265625e-02, -3.02734375e-02, ...,\n",
      "          1.96533203e-02, -1.29394531e-02, -2.16064453e-02],\n",
      "        [ 3.03955078e-02,  2.82287598e-03, -1.18408203e-02, ...,\n",
      "          9.88769531e-03, -3.29589844e-02, -3.12500000e-02]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.58691406e-02,  1.61132812e-02, -1.30004883e-02, ...,\n",
      "         -8.91113281e-03, -3.43322754e-03,  1.10626221e-03],\n",
      "        [-3.39355469e-02,  1.44042969e-02, -9.33837891e-03, ...,\n",
      "         -1.59454346e-03,  2.06298828e-02,  3.63769531e-02],\n",
      "        [-8.60595703e-03,  1.04980469e-02, -1.21459961e-02, ...,\n",
      "         -3.39355469e-02,  2.23388672e-02, -1.29394531e-02],\n",
      "        ...,\n",
      "        [-6.01196289e-03, -2.31933594e-02, -1.12915039e-02, ...,\n",
      "         -5.03540039e-03, -1.79443359e-02,  2.14843750e-02],\n",
      "        [ 1.22070312e-02, -3.64685059e-03,  1.40380859e-02, ...,\n",
      "         -1.38549805e-02,  1.39160156e-02, -1.60980225e-03],\n",
      "        [ 1.53808594e-02,  9.27734375e-03, -3.96728516e-03, ...,\n",
      "          3.63159180e-03,  3.63769531e-02, -1.47094727e-02]],\n",
      "\n",
      "       [[ 6.50024414e-03, -8.66699219e-03,  1.89208984e-02, ...,\n",
      "         -1.34887695e-02, -6.40869141e-03,  5.92041016e-03],\n",
      "        [-1.85546875e-02, -2.18505859e-02, -1.50756836e-02, ...,\n",
      "          1.44042969e-02,  1.95312500e-02,  9.88769531e-03],\n",
      "        [ 2.74658203e-02, -1.46484375e-02,  1.81884766e-02, ...,\n",
      "         -4.07714844e-02, -4.95605469e-02, -2.90527344e-02],\n",
      "        ...,\n",
      "        [-2.85644531e-02,  1.11694336e-02,  2.67333984e-02, ...,\n",
      "          1.05590820e-02, -6.50024414e-03, -1.96533203e-02],\n",
      "        [-7.62939453e-03, -2.59399414e-03,  8.72802734e-03, ...,\n",
      "         -2.29492188e-02, -3.95507812e-02,  1.03149414e-02],\n",
      "        [ 1.79443359e-02, -1.72119141e-02, -1.51367188e-02, ...,\n",
      "          1.84326172e-02, -1.36108398e-02,  1.36718750e-02]],\n",
      "\n",
      "       [[ 1.58691406e-02,  9.88769531e-03,  1.94091797e-02, ...,\n",
      "          4.02832031e-02, -1.89208984e-02, -2.47802734e-02],\n",
      "        [-1.12915039e-02, -4.07714844e-02, -2.11181641e-02, ...,\n",
      "         -1.44958496e-03, -1.20239258e-02, -3.46679688e-02],\n",
      "        [ 1.89208984e-03, -7.65991211e-03,  7.32421875e-03, ...,\n",
      "         -1.22070312e-02,  1.30615234e-02,  5.64575195e-03],\n",
      "        ...,\n",
      "        [ 3.27148438e-02,  4.18090820e-03,  3.80859375e-02, ...,\n",
      "          8.17871094e-03,  1.79443359e-02, -1.44042969e-02],\n",
      "        [ 3.57055664e-03,  3.14941406e-02, -5.67626953e-03, ...,\n",
      "         -1.52587891e-02,  2.70080566e-03, -2.22167969e-02],\n",
      "        [ 2.85339355e-03,  1.78222656e-02, -7.59887695e-03, ...,\n",
      "          3.12500000e-02, -8.91113281e-03,  5.79833984e-03]]],      dtype=float32)}, 'wi_1': {'kernel': Array([[[ 0.00026703, -0.0111084 , -0.00592041, ..., -0.00909424,\n",
      "         -0.01660156,  0.01904297],\n",
      "        [ 0.00169373,  0.00854492,  0.02783203, ..., -0.01367188,\n",
      "          0.03491211, -0.01586914],\n",
      "        [ 0.00160217,  0.00744629,  0.02722168, ..., -0.0088501 ,\n",
      "          0.0145874 , -0.01477051],\n",
      "        ...,\n",
      "        [-0.00273132, -0.00549316, -0.01647949, ...,  0.03271484,\n",
      "         -0.01068115,  0.01037598],\n",
      "        [-0.00097656, -0.02856445, -0.03369141, ..., -0.02062988,\n",
      "         -0.00872803, -0.00588989],\n",
      "        [-0.04125977,  0.02233887, -0.0065918 , ...,  0.01287842,\n",
      "          0.0055542 ,  0.04199219]],\n",
      "\n",
      "       [[-0.0291748 , -0.03125   ,  0.01489258, ..., -0.00164032,\n",
      "          0.0072937 ,  0.01965332],\n",
      "        [ 0.03564453, -0.00228882, -0.04467773, ..., -0.02709961,\n",
      "          0.00152588, -0.01818848],\n",
      "        [-0.00117493, -0.01794434,  0.01623535, ..., -0.01544189,\n",
      "         -0.00411987, -0.02880859],\n",
      "        ...,\n",
      "        [-0.00146484,  0.00698853, -0.0255127 , ...,  0.01184082,\n",
      "         -0.01544189, -0.00230408],\n",
      "        [ 0.00897217, -0.02868652, -0.02172852, ..., -0.01245117,\n",
      "          0.00427246, -0.02282715],\n",
      "        [-0.0168457 ,  0.03198242,  0.00747681, ..., -0.01086426,\n",
      "          0.00247192, -0.03808594]],\n",
      "\n",
      "       [[ 0.01477051,  0.01275635, -0.00842285, ..., -0.00665283,\n",
      "          0.0189209 , -0.00042534],\n",
      "        [-0.01745605, -0.0133667 ,  0.01287842, ...,  0.01330566,\n",
      "         -0.01672363, -0.00982666],\n",
      "        [ 0.01385498, -0.00891113,  0.02856445, ...,  0.00183868,\n",
      "          0.01287842, -0.02319336],\n",
      "        ...,\n",
      "        [ 0.00744629,  0.03344727, -0.00335693, ...,  0.01037598,\n",
      "         -0.00509644,  0.01306152],\n",
      "        [ 0.01098633,  0.03540039, -0.03857422, ...,  0.01464844,\n",
      "         -0.01367188,  0.00134277],\n",
      "        [ 0.02929688,  0.00634766, -0.01904297, ..., -0.00071335,\n",
      "         -0.02270508, -0.00279236]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.02099609,  0.01904297, -0.02270508, ...,  0.02954102,\n",
      "          0.00139618,  0.01184082],\n",
      "        [ 0.01159668, -0.00787354,  0.0168457 , ...,  0.02563477,\n",
      "          0.0043335 , -0.02307129],\n",
      "        [-0.01977539,  0.00346375, -0.01312256, ..., -0.01635742,\n",
      "         -0.00202942, -0.00576782],\n",
      "        ...,\n",
      "        [ 0.01623535,  0.00358582, -0.04589844, ...,  0.01062012,\n",
      "          0.01708984, -0.0057373 ],\n",
      "        [ 0.01635742,  0.00024796, -0.01696777, ...,  0.0168457 ,\n",
      "          0.00927734, -0.01409912],\n",
      "        [ 0.00723267, -0.01879883,  0.01055908, ...,  0.00138855,\n",
      "          0.02563477, -0.00701904]],\n",
      "\n",
      "       [[-0.02697754,  0.00604248,  0.00750732, ..., -0.00283813,\n",
      "         -0.01660156, -0.01434326],\n",
      "        [ 0.03015137,  0.0100708 , -0.04541016, ...,  0.01068115,\n",
      "         -0.00714111, -0.00747681],\n",
      "        [ 0.00692749,  0.01184082, -0.00595093, ..., -0.0078125 ,\n",
      "          0.0057373 ,  0.00090027],\n",
      "        ...,\n",
      "        [ 0.00531006, -0.02160645,  0.00228882, ...,  0.02514648,\n",
      "          0.01495361,  0.01708984],\n",
      "        [ 0.02185059, -0.01062012, -0.00741577, ..., -0.0133667 ,\n",
      "         -0.01464844,  0.00175476],\n",
      "        [-0.01135254,  0.00117493,  0.00213623, ...,  0.03491211,\n",
      "          0.01220703,  0.00358582]],\n",
      "\n",
      "       [[ 0.00653076,  0.00247192,  0.0017395 , ...,  0.01831055,\n",
      "          0.00314331, -0.03881836],\n",
      "        [-0.0177002 ,  0.00952148,  0.02893066, ..., -0.02514648,\n",
      "         -0.00123596,  0.02185059],\n",
      "        [-0.00068665,  0.0090332 ,  0.02026367, ...,  0.0213623 ,\n",
      "         -0.02600098, -0.00723267],\n",
      "        ...,\n",
      "        [ 0.00952148,  0.03417969, -0.03710938, ...,  0.03857422,\n",
      "         -0.00372314,  0.00454712],\n",
      "        [ 0.00263977, -0.01586914,  0.01574707, ..., -0.02160645,\n",
      "          0.01721191,  0.01757812],\n",
      "        [ 0.00305176, -0.00091171, -0.01019287, ..., -0.01757812,\n",
      "         -0.00048828,  0.02526855]]], dtype=float32)}, 'wo': {'kernel': Array([[[ 2.70080566e-03,  4.60815430e-03,  2.04467773e-03, ...,\n",
      "         -8.85009766e-03, -1.77001953e-02,  1.19628906e-02],\n",
      "        [ 7.93457031e-03,  1.07421875e-02, -1.70898438e-02, ...,\n",
      "         -1.01928711e-02, -1.29394531e-02,  2.44140625e-02],\n",
      "        [ 9.82666016e-03,  2.01416016e-02,  3.90625000e-02, ...,\n",
      "         -1.17797852e-02, -2.14843750e-02,  3.54003906e-02],\n",
      "        ...,\n",
      "        [ 4.61425781e-02,  1.86767578e-02,  1.91497803e-03, ...,\n",
      "          1.37939453e-02,  2.84423828e-02,  1.20849609e-02],\n",
      "        [ 1.95312500e-02,  2.57568359e-02,  2.25830078e-03, ...,\n",
      "         -3.17382812e-02, -5.87463379e-04, -6.37817383e-03],\n",
      "        [-2.21252441e-03,  4.22363281e-02, -1.40991211e-02, ...,\n",
      "          2.83813477e-03,  5.56945801e-04,  2.52685547e-02]],\n",
      "\n",
      "       [[-1.44653320e-02, -4.15039062e-03,  3.39355469e-02, ...,\n",
      "         -1.14135742e-02,  3.73535156e-02, -1.32751465e-03],\n",
      "        [ 9.70458984e-03,  6.40869141e-03, -1.46484375e-02, ...,\n",
      "         -6.50024414e-03,  2.14843750e-02, -3.79943848e-03],\n",
      "        [ 1.52587891e-02, -2.19726562e-02, -1.86767578e-02, ...,\n",
      "          2.80761719e-02,  3.46679688e-02,  9.27734375e-03],\n",
      "        ...,\n",
      "        [ 7.72094727e-03,  1.11694336e-02, -1.85546875e-02, ...,\n",
      "         -1.14135742e-02,  1.87988281e-02, -1.96533203e-02],\n",
      "        [-1.46484375e-02, -7.32421875e-04, -7.87353516e-03, ...,\n",
      "          4.36401367e-03, -3.19824219e-02, -1.33056641e-02],\n",
      "        [-2.75878906e-02,  2.04467773e-03, -1.09863281e-02, ...,\n",
      "          3.24707031e-02,  4.76074219e-02, -3.61328125e-02]],\n",
      "\n",
      "       [[ 8.30078125e-03,  8.97216797e-03, -4.36401367e-03, ...,\n",
      "          5.15747070e-03,  9.03320312e-03, -7.87353516e-03],\n",
      "        [ 5.59082031e-02, -3.58886719e-02, -2.31933594e-02, ...,\n",
      "         -1.62353516e-02,  1.48925781e-02, -1.64794922e-02],\n",
      "        [ 3.29589844e-02, -1.37329102e-02, -3.05175781e-03, ...,\n",
      "         -8.30078125e-03,  9.82666016e-03, -2.24609375e-02],\n",
      "        ...,\n",
      "        [-3.10897827e-04,  2.47802734e-02, -1.20849609e-02, ...,\n",
      "          8.78906250e-03,  3.10058594e-02, -7.65991211e-03],\n",
      "        [-3.17382812e-02,  4.37011719e-02, -1.98974609e-02, ...,\n",
      "         -2.33154297e-02,  3.50952148e-03, -2.63671875e-02],\n",
      "        [-7.32421875e-03, -5.15747070e-03,  2.50244141e-02, ...,\n",
      "         -7.09533691e-04, -8.05664062e-03,  1.70898438e-02]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.74560547e-02,  1.59912109e-02, -1.46484375e-02, ...,\n",
      "          2.30712891e-02, -6.92749023e-03, -2.86102295e-04],\n",
      "        [-1.69677734e-02,  9.09423828e-03, -1.41906738e-03, ...,\n",
      "          4.29687500e-02, -2.99072266e-02, -1.34887695e-02],\n",
      "        [-2.17285156e-02,  1.27563477e-02, -2.84423828e-02, ...,\n",
      "         -1.56164169e-05,  1.07421875e-02,  2.77099609e-02],\n",
      "        ...,\n",
      "        [ 1.45263672e-02,  2.99072266e-02,  4.18090820e-03, ...,\n",
      "         -1.58691406e-02,  1.06201172e-02, -1.44653320e-02],\n",
      "        [ 2.80761719e-02,  1.46484375e-02, -7.59124756e-04, ...,\n",
      "          2.08740234e-02,  2.07519531e-02,  1.02539062e-02],\n",
      "        [-3.71093750e-02, -2.01416016e-03,  3.12500000e-02, ...,\n",
      "          1.07421875e-02, -8.23974609e-03,  5.82885742e-03]],\n",
      "\n",
      "       [[-5.40161133e-03, -1.37939453e-02,  2.19726562e-02, ...,\n",
      "         -1.34887695e-02, -1.22070312e-02, -2.96020508e-03],\n",
      "        [ 2.38037109e-03, -1.31835938e-02, -1.06201172e-02, ...,\n",
      "         -6.28662109e-03, -1.89208984e-02, -1.42211914e-02],\n",
      "        [-2.44140625e-03,  2.99072266e-03,  1.85394287e-03, ...,\n",
      "         -3.76892090e-03, -1.42211914e-02, -4.19921875e-02],\n",
      "        ...,\n",
      "        [-1.66015625e-02,  7.53784180e-03,  7.59887695e-03, ...,\n",
      "         -5.12695312e-03, -4.05273438e-02, -6.71386719e-03],\n",
      "        [-5.06591797e-03,  4.48608398e-03,  1.22070312e-02, ...,\n",
      "         -2.17285156e-02,  2.44140625e-03, -2.91442871e-03],\n",
      "        [-1.49536133e-02, -1.56250000e-02, -2.16674805e-03, ...,\n",
      "          5.88989258e-03,  2.14843750e-02,  1.77764893e-03]],\n",
      "\n",
      "       [[ 1.40380859e-03,  3.34472656e-02,  1.67236328e-02, ...,\n",
      "          2.95410156e-02, -2.18505859e-02, -3.01513672e-02],\n",
      "        [-1.96533203e-02,  2.92968750e-03,  3.40270996e-03, ...,\n",
      "          7.38525391e-03,  2.33154297e-02,  2.97851562e-02],\n",
      "        [-3.14941406e-02, -2.38037109e-02, -3.39355469e-02, ...,\n",
      "         -1.30004883e-02, -1.00097656e-02, -7.04956055e-03],\n",
      "        ...,\n",
      "        [-1.49536133e-02,  1.84631348e-03, -2.72216797e-02, ...,\n",
      "         -2.08740234e-02,  3.00598145e-03,  1.54113770e-03],\n",
      "        [ 6.65283203e-03, -7.62939453e-03,  1.91650391e-02, ...,\n",
      "         -1.17187500e-02,  2.37226486e-05, -1.78222656e-02],\n",
      "        [ 2.34375000e-02,  1.32446289e-02,  2.38037109e-02, ...,\n",
      "          2.12402344e-02,  2.20947266e-02,  2.64892578e-02]]],      dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': Array([[0.05029297, 0.09960938, 0.13378906, ..., 0.46875   , 0.47851562,\n",
      "        0.43359375],\n",
      "       [0.05249023, 0.10058594, 0.13671875, ..., 0.47070312, 0.48828125,\n",
      "        0.4375    ],\n",
      "       [0.05004883, 0.09619141, 0.13574219, ..., 0.46679688, 0.47851562,\n",
      "        0.44140625],\n",
      "       ...,\n",
      "       [0.05249023, 0.10742188, 0.13574219, ..., 0.47265625, 0.48046875,\n",
      "        0.42382812],\n",
      "       [0.0534668 , 0.09960938, 0.13867188, ..., 0.47460938, 0.48242188,\n",
      "        0.41015625],\n",
      "       [0.04907227, 0.1015625 , 0.13574219, ..., 0.47265625, 0.47851562,\n",
      "        0.42773438]], dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': Array([[0.02966309, 0.11376953, 0.17382812, ..., 0.52734375, 0.57421875,\n",
      "        0.48632812],\n",
      "       [0.01361084, 0.10986328, 0.17773438, ..., 0.5390625 , 0.58203125,\n",
      "        0.484375  ],\n",
      "       [0.00196838, 0.10058594, 0.17382812, ..., 0.53125   , 0.5625    ,\n",
      "        0.43554688],\n",
      "       ...,\n",
      "       [0.01025391, 0.06298828, 0.17675781, ..., 0.52734375, 0.55078125,\n",
      "        0.43164062],\n",
      "       [0.01098633, 0.09423828, 0.17089844, ..., 0.53515625, 0.5625    ,\n",
      "        0.45507812],\n",
      "       [0.006073  , 0.07421875, 0.17480469, ..., 0.5546875 , 0.58203125,\n",
      "        0.48046875]], dtype=float32)}, 'self_attention': {'key': {'kernel': Array([[[[-1.62353516e-02,  1.91650391e-02, -2.35595703e-02, ...,\n",
      "           2.58789062e-02, -2.73437500e-02,  3.27148438e-02],\n",
      "         [ 6.62231445e-03,  1.12304688e-02, -2.51464844e-02, ...,\n",
      "           9.15527344e-04, -6.62231445e-03, -8.62121582e-04],\n",
      "         [-1.55639648e-03, -2.86865234e-03, -5.15747070e-03, ...,\n",
      "           1.53541565e-04,  7.47680664e-04,  3.43322754e-03],\n",
      "         ...,\n",
      "         [ 1.41601562e-02, -5.18798828e-03,  3.14941406e-02, ...,\n",
      "          -2.35595703e-02,  7.04956055e-03, -1.36718750e-02],\n",
      "         [-6.25610352e-03, -3.46374512e-03, -4.85229492e-03, ...,\n",
      "          -1.77001953e-02,  1.87988281e-02,  1.94091797e-02],\n",
      "         [-2.41699219e-02, -1.68457031e-02, -2.03857422e-02, ...,\n",
      "           1.27563477e-02, -5.61523438e-03,  3.67736816e-03]],\n",
      "\n",
      "        [[-2.47802734e-02, -2.95410156e-02, -2.50244141e-02, ...,\n",
      "           3.00292969e-02, -1.95312500e-02,  1.72119141e-02],\n",
      "         [ 3.96728516e-03, -8.36181641e-03, -1.09252930e-02, ...,\n",
      "           4.15039062e-03, -7.47680664e-03,  7.50732422e-03],\n",
      "         [-7.26318359e-03, -1.03759766e-02, -1.41601562e-02, ...,\n",
      "           9.82666016e-03,  1.96838379e-03,  1.04980469e-02],\n",
      "         ...,\n",
      "         [-1.55029297e-02, -3.64685059e-03, -1.20239258e-02, ...,\n",
      "           2.50244141e-02, -9.88769531e-03,  2.16674805e-03],\n",
      "         [-1.90429688e-02, -6.43920898e-03,  2.52685547e-02, ...,\n",
      "          -9.39941406e-03,  1.50756836e-02,  4.30297852e-03],\n",
      "         [ 3.63769531e-02,  4.51660156e-03,  1.72119141e-02, ...,\n",
      "          -1.11083984e-02,  7.99560547e-03, -7.99560547e-03]],\n",
      "\n",
      "        [[-1.07574463e-03,  1.85546875e-02,  1.35498047e-02, ...,\n",
      "           3.22265625e-02,  3.22265625e-02,  4.32128906e-02],\n",
      "         [-3.14941406e-02, -8.97216797e-03, -9.58251953e-03, ...,\n",
      "          -1.59912109e-02,  6.56127930e-04, -6.98852539e-03],\n",
      "         [ 7.62939453e-03,  2.30407715e-03,  2.06298828e-02, ...,\n",
      "          -3.19824219e-02, -2.06298828e-02, -3.37219238e-03],\n",
      "         ...,\n",
      "         [ 4.85229492e-03,  2.86865234e-02,  4.49218750e-02, ...,\n",
      "          -1.45874023e-02,  7.99560547e-03,  1.91650391e-02],\n",
      "         [ 8.05664062e-03, -1.70898438e-02,  9.64355469e-03, ...,\n",
      "           1.47705078e-02, -2.90527344e-02, -1.30004883e-02],\n",
      "         [ 2.42614746e-03, -2.30712891e-02, -2.78472900e-04, ...,\n",
      "           3.39355469e-02, -6.10351562e-03,  5.88989258e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.80859375e-02, -1.61132812e-02,  1.83105469e-02, ...,\n",
      "          -5.58471680e-03, -4.11987305e-03, -2.74658203e-02],\n",
      "         [ 1.01318359e-02, -5.21850586e-03,  6.56127930e-03, ...,\n",
      "          -1.85546875e-02, -1.50756836e-02,  4.32128906e-02],\n",
      "         [ 4.57763672e-03,  4.95910645e-04, -5.46264648e-03, ...,\n",
      "          -3.49121094e-02, -1.19781494e-03,  6.01196289e-03],\n",
      "         ...,\n",
      "         [ 3.73535156e-02,  2.89916992e-03, -9.57489014e-04, ...,\n",
      "          -1.90429688e-02,  2.19726562e-02,  1.31835938e-02],\n",
      "         [-1.86767578e-02, -5.61523438e-03,  2.31933594e-03, ...,\n",
      "           4.94384766e-03,  1.47094727e-02, -5.58471680e-03],\n",
      "         [-2.49023438e-02,  1.02233887e-03,  1.36718750e-02, ...,\n",
      "          -3.46679688e-02,  3.11279297e-03,  1.59912109e-02]],\n",
      "\n",
      "        [[ 2.63671875e-02,  3.80859375e-02, -5.40161133e-03, ...,\n",
      "           4.56542969e-02,  1.07421875e-02,  1.68609619e-03],\n",
      "         [ 9.46044922e-03, -7.11059570e-03,  3.35693359e-03, ...,\n",
      "          -2.66113281e-02,  5.37109375e-03, -1.49536133e-02],\n",
      "         [-9.70458984e-03,  2.01416016e-02, -1.67846680e-03, ...,\n",
      "           8.85009766e-03, -2.79541016e-02,  1.95312500e-02],\n",
      "         ...,\n",
      "         [-1.08642578e-02, -1.47094727e-02,  1.40991211e-02, ...,\n",
      "          -6.98852539e-03, -2.24609375e-02, -5.15136719e-02],\n",
      "         [ 3.22265625e-02,  6.58035278e-05,  4.63867188e-03, ...,\n",
      "          -5.05371094e-02, -2.34375000e-02,  5.92041016e-03],\n",
      "         [ 7.17163086e-03, -6.79016113e-04,  1.22680664e-02, ...,\n",
      "           7.41577148e-03,  1.11694336e-02, -3.90625000e-02]],\n",
      "\n",
      "        [[-1.26342773e-02,  2.50244141e-02,  9.09423828e-03, ...,\n",
      "          -2.29492188e-02, -3.00292969e-02, -5.43212891e-03],\n",
      "         [ 1.79443359e-02,  2.40478516e-02,  1.86767578e-02, ...,\n",
      "          -1.62353516e-02,  8.48388672e-03,  2.27050781e-02],\n",
      "         [-5.92041016e-03, -3.22265625e-02,  2.41699219e-02, ...,\n",
      "           1.22070312e-02,  4.33349609e-03,  1.28173828e-03],\n",
      "         ...,\n",
      "         [-2.79541016e-02, -2.89306641e-02,  8.54492188e-03, ...,\n",
      "          -5.03540039e-03, -2.08740234e-02,  3.32031250e-02],\n",
      "         [ 3.71093750e-02, -5.10253906e-02, -1.17187500e-02, ...,\n",
      "          -3.03955078e-02, -2.91748047e-02, -3.85742188e-02],\n",
      "         [-1.80664062e-02, -2.77099609e-02, -6.40869141e-03, ...,\n",
      "           2.36816406e-02,  1.58691406e-02, -7.71484375e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 7.87353516e-03,  1.54876709e-03, -2.17285156e-02, ...,\n",
      "           1.22680664e-02, -9.15527344e-03,  1.37939453e-02],\n",
      "         [ 2.71606445e-03,  1.25885010e-03,  1.09863281e-03, ...,\n",
      "           7.08007812e-03, -2.24609375e-02,  1.04980469e-02],\n",
      "         [-4.48608398e-03, -9.61303711e-04, -8.43048096e-04, ...,\n",
      "           3.67736816e-03,  3.79943848e-03,  3.38745117e-03],\n",
      "         ...,\n",
      "         [-9.76562500e-03, -1.03759766e-02, -1.01318359e-02, ...,\n",
      "           1.45874023e-02,  2.08740234e-02,  1.29394531e-02],\n",
      "         [ 2.29492188e-02, -5.40161133e-03,  3.41796875e-02, ...,\n",
      "          -1.41906738e-03, -6.06536865e-04, -1.21307373e-03],\n",
      "         [ 1.26953125e-02,  3.25012207e-03,  4.99725342e-04, ...,\n",
      "          -7.24792480e-04,  1.73339844e-02, -2.10571289e-03]],\n",
      "\n",
      "        [[-2.50244141e-03,  4.57763672e-03,  2.94189453e-02, ...,\n",
      "          -7.38525391e-03, -3.12805176e-03,  1.48010254e-03],\n",
      "         [ 4.97436523e-03, -9.84191895e-04, -1.11083984e-02, ...,\n",
      "           9.39941406e-03, -1.25122070e-02, -3.12805176e-03],\n",
      "         [-9.88769531e-03,  2.76184082e-03, -1.62353516e-02, ...,\n",
      "          -1.48010254e-03,  1.48315430e-02,  1.37329102e-02],\n",
      "         ...,\n",
      "         [-4.54711914e-03,  1.58691406e-02, -1.04370117e-02, ...,\n",
      "          -3.34472656e-02,  2.20947266e-02, -2.73437500e-02],\n",
      "         [ 5.46264648e-03,  1.44042969e-02, -2.06298828e-02, ...,\n",
      "           6.73828125e-02, -3.10058594e-02,  2.61230469e-02],\n",
      "         [ 1.55029297e-02, -2.64892578e-02, -3.17382812e-02, ...,\n",
      "           1.89208984e-02, -1.91650391e-02,  1.47094727e-02]],\n",
      "\n",
      "        [[ 8.66699219e-03, -1.17797852e-02,  1.28173828e-02, ...,\n",
      "          -3.61328125e-02,  3.01513672e-02, -1.90429688e-02],\n",
      "         [ 1.33056641e-02, -1.03759766e-03, -1.06201172e-02, ...,\n",
      "           9.64355469e-03, -3.38745117e-03,  4.94384766e-03],\n",
      "         [ 4.30297852e-03, -4.60815430e-03, -4.56542969e-02, ...,\n",
      "           6.62231445e-03, -4.80957031e-02,  1.01318359e-02],\n",
      "         ...,\n",
      "         [ 1.16577148e-02,  1.03149414e-02,  6.86645508e-05, ...,\n",
      "          -3.50952148e-04, -4.54101562e-02,  4.68750000e-02],\n",
      "         [ 1.63574219e-02,  1.56402588e-03, -3.11279297e-03, ...,\n",
      "           2.66113281e-02, -7.17773438e-02, -5.63964844e-02],\n",
      "         [ 7.11059570e-03,  1.03149414e-02,  4.10156250e-02, ...,\n",
      "           4.90722656e-02, -1.23291016e-02, -1.56250000e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.04370117e-02,  2.16064453e-02,  8.85009766e-03, ...,\n",
      "           2.12402344e-02, -2.18200684e-03, -2.82287598e-03],\n",
      "         [-1.92871094e-02,  1.91650391e-02,  1.40380859e-02, ...,\n",
      "           2.18505859e-02, -2.33650208e-04,  1.15356445e-02],\n",
      "         [ 1.79443359e-02,  1.19628906e-02,  8.97216797e-03, ...,\n",
      "           4.41894531e-02, -1.55029297e-02,  9.94873047e-03],\n",
      "         ...,\n",
      "         [-1.78222656e-02,  8.42285156e-03,  2.64892578e-02, ...,\n",
      "          -1.06811523e-02, -1.64794922e-03, -2.59399414e-03],\n",
      "         [ 3.02734375e-02, -5.31005859e-03, -1.48315430e-02, ...,\n",
      "          -2.83203125e-02,  1.61743164e-03,  7.99560547e-03],\n",
      "         [ 2.45361328e-02, -5.43212891e-03,  2.00195312e-02, ...,\n",
      "           3.78417969e-02, -1.70898438e-02, -9.52148438e-03]],\n",
      "\n",
      "        [[-2.23388672e-02,  1.57470703e-02, -1.17187500e-02, ...,\n",
      "          -1.85546875e-02,  2.73132324e-03,  2.64892578e-02],\n",
      "         [-6.46972656e-03, -2.35595703e-02, -2.18505859e-02, ...,\n",
      "          -1.04370117e-02, -1.78222656e-02, -1.58691406e-02],\n",
      "         [ 2.24304199e-03,  5.67626953e-03,  1.31225586e-02, ...,\n",
      "           2.18505859e-02,  4.58984375e-02,  2.45361328e-02],\n",
      "         ...,\n",
      "         [ 1.81884766e-02,  6.62231445e-03, -1.25122070e-02, ...,\n",
      "          -3.27148438e-02,  3.07617188e-02,  7.47680664e-03],\n",
      "         [-1.94091797e-02,  3.32031250e-02, -6.34765625e-02, ...,\n",
      "          -7.99560547e-03,  1.09863281e-02,  9.46044922e-03],\n",
      "         [-8.11767578e-03, -5.18798828e-03,  1.11083984e-02, ...,\n",
      "           2.61230469e-02, -6.54296875e-02, -1.34277344e-02]],\n",
      "\n",
      "        [[-2.18505859e-02, -1.25122070e-02,  1.83105469e-03, ...,\n",
      "          -1.89208984e-02,  3.88183594e-02,  2.81982422e-02],\n",
      "         [-5.52368164e-03,  1.77001953e-02, -2.44140625e-02, ...,\n",
      "           1.92871094e-02, -1.60217285e-03,  1.22680664e-02],\n",
      "         [ 2.51770020e-03,  2.03857422e-02, -1.34887695e-02, ...,\n",
      "           6.50024414e-03, -2.31933594e-02,  2.85644531e-02],\n",
      "         ...,\n",
      "         [-1.94091797e-02,  1.01318359e-02,  6.46972656e-03, ...,\n",
      "          -9.21630859e-03,  7.01904297e-04, -8.54492188e-03],\n",
      "         [ 1.67236328e-02, -1.83105469e-02, -2.47802734e-02, ...,\n",
      "           3.90625000e-03, -1.67236328e-02,  7.27539062e-02],\n",
      "         [ 3.22265625e-02, -3.12805176e-03,  1.20849609e-02, ...,\n",
      "          -3.29589844e-02,  1.72119141e-02, -3.19824219e-02]]],\n",
      "\n",
      "\n",
      "       [[[-1.26647949e-03,  3.64685059e-03,  1.68609619e-03, ...,\n",
      "          -2.30407715e-03,  7.93457031e-04, -3.47900391e-03],\n",
      "         [ 4.17709351e-04, -4.36782837e-04,  1.24931335e-04, ...,\n",
      "           2.48718262e-03, -1.28173828e-03,  2.77709961e-03],\n",
      "         [ 5.45501709e-04,  1.27410889e-03,  1.24359131e-03, ...,\n",
      "          -4.33349609e-03, -3.90625000e-03, -2.18200684e-03],\n",
      "         ...,\n",
      "         [-8.01086426e-04,  4.24194336e-03, -1.51062012e-03, ...,\n",
      "          -3.93676758e-03, -6.16455078e-03, -4.55975533e-06],\n",
      "         [-1.41143799e-03,  5.24902344e-03, -3.63159180e-03, ...,\n",
      "           3.64685059e-03, -2.42614746e-03, -2.27355957e-03],\n",
      "         [-1.57928467e-03, -1.06811523e-03, -1.28173828e-03, ...,\n",
      "          -8.04901123e-04, -3.21960449e-03,  1.28173828e-03]],\n",
      "\n",
      "        [[ 3.83300781e-02, -1.13525391e-02, -6.49414062e-02, ...,\n",
      "          -1.74713135e-03,  6.50024414e-03, -3.61328125e-02],\n",
      "         [-3.29589844e-02,  2.00195312e-02,  3.58886719e-02, ...,\n",
      "          -6.59179688e-03,  3.87573242e-03, -5.03540039e-03],\n",
      "         [-6.65283203e-03,  1.34887695e-02,  1.31835938e-02, ...,\n",
      "          -1.25732422e-02, -1.55639648e-03,  3.12805176e-03],\n",
      "         ...,\n",
      "         [-7.72094727e-03,  1.37939453e-02, -1.02519989e-05, ...,\n",
      "          -7.65991211e-03,  7.59887695e-03, -1.63269043e-03],\n",
      "         [ 1.12915039e-02,  1.15966797e-02, -6.50024414e-03, ...,\n",
      "           1.45263672e-02,  1.98364258e-03,  3.14941406e-02],\n",
      "         [ 1.24511719e-02,  1.40380859e-02,  2.40478516e-02, ...,\n",
      "          -1.50299072e-03,  3.99780273e-03,  7.17163086e-04]],\n",
      "\n",
      "        [[-7.08007812e-03, -4.36401367e-03,  3.54003906e-02, ...,\n",
      "          -2.18505859e-02, -1.62506104e-03,  4.19921875e-02],\n",
      "         [-2.44140625e-02,  1.85546875e-02,  9.52148438e-03, ...,\n",
      "           7.43865967e-04, -2.02941895e-03,  1.10473633e-02],\n",
      "         [-1.97753906e-02,  1.54418945e-02,  2.42919922e-02, ...,\n",
      "          -4.83398438e-02,  1.05590820e-02, -6.10351562e-03],\n",
      "         ...,\n",
      "         [ 2.64892578e-02,  1.61132812e-02, -1.50756836e-02, ...,\n",
      "          -7.75146484e-03, -4.91333008e-03,  3.90625000e-03],\n",
      "         [-1.55639648e-03,  1.18408203e-02, -2.75878906e-02, ...,\n",
      "          -7.72094727e-03, -2.85644531e-02,  3.22265625e-02],\n",
      "         [-7.99560547e-03,  4.10156250e-02,  6.67572021e-04, ...,\n",
      "          -2.84423828e-02, -6.67572021e-04, -4.45556641e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.68457031e-02, -2.30407715e-03,  1.87988281e-02, ...,\n",
      "           3.12500000e-02,  1.32446289e-02,  2.60009766e-02],\n",
      "         [-3.71093750e-02,  1.67236328e-02,  3.17382812e-02, ...,\n",
      "           2.49023438e-02, -3.75976562e-02, -2.13623047e-02],\n",
      "         [ 1.15203857e-03,  3.73840332e-03,  5.52368164e-03, ...,\n",
      "           7.01904297e-04, -9.76562500e-03,  2.25830078e-03],\n",
      "         ...,\n",
      "         [ 2.14843750e-02, -2.79235840e-03, -1.33666992e-02, ...,\n",
      "          -4.76074219e-03, -7.38525391e-03, -4.42504883e-03],\n",
      "         [ 3.64685059e-03, -8.05664062e-03, -5.18798828e-03, ...,\n",
      "          -7.17163086e-03,  9.94873047e-03, -3.87573242e-03],\n",
      "         [-6.67572021e-04,  6.01196289e-03,  1.40991211e-02, ...,\n",
      "           5.73730469e-03, -6.88476562e-02, -5.61523438e-02]],\n",
      "\n",
      "        [[ 3.77655029e-04,  4.51660156e-03,  2.49023438e-02, ...,\n",
      "           3.08837891e-02,  1.59912109e-02, -5.24902344e-02],\n",
      "         [-1.31225586e-03, -5.92041016e-03,  9.15527344e-03, ...,\n",
      "           2.25830078e-02,  1.91650391e-02, -1.40380859e-02],\n",
      "         [-4.80651855e-04,  9.76562500e-03,  1.78222656e-02, ...,\n",
      "           1.88446045e-03,  4.54711914e-03, -2.29492188e-02],\n",
      "         ...,\n",
      "         [ 1.63574219e-02, -1.58691406e-02,  3.58886719e-02, ...,\n",
      "           6.98852539e-03,  9.09423828e-03,  1.63574219e-02],\n",
      "         [-3.95507812e-02,  1.40991211e-02, -1.40991211e-02, ...,\n",
      "          -2.07519531e-02,  8.48388672e-03, -2.70996094e-02],\n",
      "         [ 7.87353516e-03, -7.87353516e-03, -1.46484375e-02, ...,\n",
      "          -3.24249268e-04,  3.66210938e-02,  1.83105469e-02]],\n",
      "\n",
      "        [[ 1.36108398e-02,  1.26953125e-02, -8.78906250e-03, ...,\n",
      "          -2.01416016e-02, -7.41577148e-03,  1.38549805e-02],\n",
      "         [ 2.47802734e-02, -1.78222656e-02,  2.60009766e-02, ...,\n",
      "           1.23291016e-02,  1.90429688e-02, -2.41699219e-02],\n",
      "         [-2.44140625e-02, -3.12500000e-02,  7.20214844e-03, ...,\n",
      "          -7.87353516e-03, -1.89208984e-02, -3.93676758e-03],\n",
      "         ...,\n",
      "         [ 9.76562500e-03,  1.81884766e-02,  2.22167969e-02, ...,\n",
      "           2.55126953e-02, -1.98974609e-02,  2.97851562e-02],\n",
      "         [ 3.06396484e-02,  4.15039062e-03, -4.55856323e-04, ...,\n",
      "           6.34765625e-02, -4.88281250e-02,  7.23266602e-03],\n",
      "         [-6.80541992e-03, -1.32446289e-02,  2.17285156e-02, ...,\n",
      "          -2.33154297e-02,  1.69677734e-02,  1.08642578e-02]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 1.66015625e-02, -2.11181641e-02,  1.50146484e-02, ...,\n",
      "          -1.95312500e-02,  1.61132812e-02, -2.45361328e-02],\n",
      "         [ 2.31933594e-03, -7.78198242e-04, -1.24359131e-03, ...,\n",
      "          -1.47705078e-02,  8.23974609e-03, -1.35498047e-02],\n",
      "         [ 6.28662109e-03, -5.34057617e-04, -7.11059570e-03, ...,\n",
      "           1.30615234e-02,  1.20239258e-02,  1.03149414e-02],\n",
      "         ...,\n",
      "         [ 1.36566162e-03, -1.55639648e-02, -9.70458984e-03, ...,\n",
      "           4.66918945e-03, -6.07299805e-03, -7.87353516e-03],\n",
      "         [ 3.54003906e-03, -8.97216797e-03,  4.18090820e-03, ...,\n",
      "           6.37054443e-04, -6.28662109e-03, -6.50024414e-03],\n",
      "         [-1.51062012e-03, -2.70080566e-03, -5.41687012e-04, ...,\n",
      "           2.39372253e-04, -3.23486328e-03,  6.98852539e-03]],\n",
      "\n",
      "        [[ 1.79443359e-02, -1.58691406e-02,  3.49121094e-02, ...,\n",
      "           2.29492188e-02,  3.41796875e-03, -1.77001953e-02],\n",
      "         [ 3.00598145e-03,  2.97546387e-03,  1.00708008e-02, ...,\n",
      "          -3.95507812e-02, -1.12915039e-02, -1.36718750e-02],\n",
      "         [ 2.42919922e-02, -6.68334961e-03, -2.42614746e-03, ...,\n",
      "          -5.52368164e-03, -5.12695312e-03, -9.39941406e-03],\n",
      "         ...,\n",
      "         [ 1.62353516e-02,  1.27563477e-02, -1.63574219e-02, ...,\n",
      "           8.11767578e-03,  6.90460205e-04, -1.02539062e-02],\n",
      "         [ 1.31225586e-03,  1.58691406e-02,  2.96020508e-03, ...,\n",
      "           5.43212891e-03, -1.00708008e-02, -3.61328125e-02],\n",
      "         [-1.36718750e-02, -1.19018555e-02,  3.20434570e-03, ...,\n",
      "           1.09252930e-02, -1.21459961e-02,  3.99780273e-03]],\n",
      "\n",
      "        [[-1.80664062e-02,  1.70898438e-02, -1.34887695e-02, ...,\n",
      "          -2.66113281e-02, -1.98974609e-02,  6.83593750e-03],\n",
      "         [ 1.12304688e-02,  5.85937500e-03,  7.44628906e-03, ...,\n",
      "           6.43920898e-03,  1.45721436e-03,  9.38415527e-04],\n",
      "         [ 1.33056641e-02, -5.07812500e-02,  2.51464844e-02, ...,\n",
      "           2.63671875e-02,  2.05078125e-02,  6.80541992e-03],\n",
      "         ...,\n",
      "         [-1.50756836e-02,  1.95312500e-02,  1.16577148e-02, ...,\n",
      "          -3.41796875e-03,  4.54101562e-02, -5.09643555e-03],\n",
      "         [-1.43432617e-02,  1.59912109e-02, -3.12500000e-02, ...,\n",
      "           5.02929688e-02, -2.07519531e-02,  2.61230469e-02],\n",
      "         [-1.06811523e-02, -3.83300781e-02,  3.06396484e-02, ...,\n",
      "           3.05175781e-02,  4.11987305e-03, -8.30078125e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.98364258e-03, -2.18505859e-02,  1.80664062e-02, ...,\n",
      "           4.22363281e-02,  1.36108398e-02, -5.55419922e-03],\n",
      "         [-9.33837891e-03,  3.95507812e-02,  4.78744507e-04, ...,\n",
      "          -4.19921875e-02,  3.06396484e-02, -3.47900391e-03],\n",
      "         [ 1.90734863e-03,  3.05175781e-03, -1.68457031e-02, ...,\n",
      "          -4.41894531e-02, -8.78906250e-03,  1.87683105e-03],\n",
      "         ...,\n",
      "         [-1.81884766e-02, -2.00195312e-02,  2.29492188e-02, ...,\n",
      "          -2.25830078e-03,  2.16064453e-02,  9.76562500e-03],\n",
      "         [-4.79125977e-03,  1.55639648e-02,  5.37109375e-03, ...,\n",
      "           1.56402588e-03,  6.46972656e-03,  3.71093750e-02],\n",
      "         [ 2.56347656e-03, -8.54492188e-03,  7.44628906e-03, ...,\n",
      "          -6.71386719e-03, -1.05590820e-02, -5.95703125e-02]],\n",
      "\n",
      "        [[-9.58251953e-03, -3.73840332e-03, -1.16729736e-03, ...,\n",
      "          -3.41796875e-02,  2.06298828e-02, -2.16484070e-04],\n",
      "         [-1.15356445e-02,  1.68457031e-02, -1.56250000e-02, ...,\n",
      "           1.74560547e-02,  2.63671875e-02, -1.70898438e-02],\n",
      "         [-1.34277344e-02,  1.61743164e-03,  2.44140625e-03, ...,\n",
      "          -4.17480469e-02, -1.53198242e-02,  5.98144531e-03],\n",
      "         ...,\n",
      "         [ 6.67572021e-04,  3.36914062e-02, -1.27563477e-02, ...,\n",
      "          -1.14135742e-02,  2.53906250e-02,  4.66918945e-03],\n",
      "         [-1.73950195e-03,  6.68334961e-03, -2.62451172e-02, ...,\n",
      "          -3.97949219e-02, -1.68457031e-02, -1.66015625e-02],\n",
      "         [ 7.53784180e-03, -1.14746094e-02,  1.49536133e-03, ...,\n",
      "           5.51757812e-02,  6.22558594e-03,  3.23486328e-03]],\n",
      "\n",
      "        [[ 6.56127930e-03,  8.54492188e-03, -2.79235840e-03, ...,\n",
      "          -1.95312500e-02,  1.67236328e-02, -5.95092773e-04],\n",
      "         [ 6.89697266e-03,  2.52685547e-02, -3.58886719e-02, ...,\n",
      "          -2.29492188e-02, -2.51464844e-02,  5.64575195e-03],\n",
      "         [ 2.92968750e-03,  1.56250000e-02,  2.77709961e-03, ...,\n",
      "           1.17797852e-02,  1.62353516e-02,  1.06811523e-02],\n",
      "         ...,\n",
      "         [-1.90429688e-02, -1.61132812e-02,  5.09643555e-03, ...,\n",
      "          -1.43432617e-03,  2.79541016e-02,  4.19921875e-02],\n",
      "         [ 1.34887695e-02, -4.02832031e-02,  7.87353516e-03, ...,\n",
      "          -1.03149414e-02,  2.35595703e-02,  3.54003906e-02],\n",
      "         [ 4.97436523e-03, -2.35595703e-02,  1.44958496e-03, ...,\n",
      "          -1.21459961e-02, -2.29492188e-02, -1.04980469e-02]]],\n",
      "\n",
      "\n",
      "       [[[-9.94873047e-03,  1.51977539e-02, -1.64794922e-02, ...,\n",
      "           1.34887695e-02, -7.99560547e-03,  1.30615234e-02],\n",
      "         [ 4.59671021e-04,  2.22778320e-03, -4.95910645e-04, ...,\n",
      "           9.27734375e-03, -9.03320312e-03,  1.11694336e-02],\n",
      "         [-4.25338745e-04,  2.27355957e-03, -2.60925293e-03, ...,\n",
      "           8.36181641e-03,  7.11059570e-03,  4.21142578e-03],\n",
      "         ...,\n",
      "         [-7.04956055e-03,  1.12304688e-02, -1.09863281e-02, ...,\n",
      "          -1.00708008e-02, -1.68457031e-02, -2.79541016e-02],\n",
      "         [-4.27246094e-03,  2.00195312e-02, -1.40991211e-02, ...,\n",
      "           1.64031982e-03, -1.12533569e-04,  5.29289246e-05],\n",
      "         [ 6.98852539e-03,  4.91333008e-03,  6.07299805e-03, ...,\n",
      "           3.05175781e-03,  1.14746094e-02, -1.14746094e-02]],\n",
      "\n",
      "        [[ 2.07519531e-02,  9.39941406e-03,  7.87353516e-03, ...,\n",
      "           6.59179688e-03,  2.28881836e-03, -1.48925781e-02],\n",
      "         [-5.21850586e-03, -2.05078125e-02,  1.78222656e-02, ...,\n",
      "           5.31005859e-03,  9.39941406e-03,  5.37109375e-03],\n",
      "         [-9.03320312e-03, -3.00292969e-02, -1.56250000e-02, ...,\n",
      "          -9.33837891e-03,  1.53198242e-02, -9.88769531e-03],\n",
      "         ...,\n",
      "         [ 2.84423828e-02,  1.10473633e-02, -3.03649902e-03, ...,\n",
      "           1.20239258e-02, -2.00195312e-02,  8.54492188e-03],\n",
      "         [-3.93066406e-02, -1.26342773e-02,  3.14941406e-02, ...,\n",
      "           1.01318359e-02,  9.72747803e-04, -1.79290771e-03],\n",
      "         [-7.04956055e-03,  1.87988281e-02, -2.16674805e-03, ...,\n",
      "           6.07967377e-05, -1.46484375e-03, -1.56402588e-03]],\n",
      "\n",
      "        [[-7.23266602e-03, -6.71386719e-03,  1.07421875e-02, ...,\n",
      "          -3.44238281e-02, -2.83203125e-02, -4.27246094e-02],\n",
      "         [ 1.06201172e-02, -4.05883789e-03,  2.55126953e-02, ...,\n",
      "           1.03149414e-02, -2.11715698e-04,  2.16674805e-03],\n",
      "         [-1.92871094e-02, -2.97851562e-02,  2.81982422e-02, ...,\n",
      "           1.42822266e-02, -2.50244141e-02, -1.62353516e-02],\n",
      "         ...,\n",
      "         [ 2.75878906e-02,  2.63671875e-02, -1.04370117e-02, ...,\n",
      "          -3.09753418e-03,  3.12500000e-02,  8.78906250e-03],\n",
      "         [-2.02636719e-02, -3.49426270e-03, -1.32446289e-02, ...,\n",
      "           7.66601562e-02, -1.89208984e-02,  6.17980957e-04],\n",
      "         [ 5.03540039e-03, -2.09960938e-02,  2.94189453e-02, ...,\n",
      "          -4.00390625e-02,  1.31988525e-03, -2.78320312e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.80541992e-03,  8.72802734e-03,  2.50244141e-02, ...,\n",
      "          -1.50756836e-02, -2.16064453e-02,  1.05590820e-02],\n",
      "         [ 9.76562500e-03,  2.60009766e-02,  1.57470703e-02, ...,\n",
      "           7.69042969e-03, -1.55639648e-02,  3.93066406e-02],\n",
      "         [-1.17187500e-02, -7.08007812e-03,  3.32641602e-03, ...,\n",
      "          -1.19628906e-02,  2.30712891e-02, -7.87353516e-03],\n",
      "         ...,\n",
      "         [ 1.30615234e-02,  2.39257812e-02, -1.51977539e-02, ...,\n",
      "           1.19781494e-03, -4.94384766e-03, -1.35498047e-02],\n",
      "         [-2.84423828e-02,  6.07299805e-03, -2.24609375e-02, ...,\n",
      "           2.07519531e-02,  3.83300781e-02,  5.98144531e-02],\n",
      "         [-6.68334961e-03,  5.49316406e-03,  3.44238281e-02, ...,\n",
      "          -3.99780273e-03, -1.10473633e-02, -2.39257812e-02]],\n",
      "\n",
      "        [[ 1.31835938e-02, -7.35473633e-03, -8.91113281e-03, ...,\n",
      "           3.83300781e-02, -1.44042969e-02,  3.02734375e-02],\n",
      "         [-7.20214844e-03,  4.48608398e-03,  2.21252441e-03, ...,\n",
      "           2.81982422e-02,  8.11767578e-03, -1.97753906e-02],\n",
      "         [ 8.48388672e-03, -1.36108398e-02,  2.38037109e-03, ...,\n",
      "           2.12402344e-02, -4.07714844e-02,  3.80859375e-02],\n",
      "         ...,\n",
      "         [-1.83105469e-02,  5.37109375e-03, -6.04248047e-03, ...,\n",
      "          -3.22265625e-02, -4.39453125e-03, -7.11059570e-03],\n",
      "         [ 3.44848633e-03, -5.46264648e-03, -9.09423828e-03, ...,\n",
      "          -3.17382812e-02,  5.07812500e-02, -1.14746094e-02],\n",
      "         [ 2.49023438e-02, -2.69775391e-02,  1.77001953e-02, ...,\n",
      "           4.60815430e-03, -9.27734375e-03,  1.36718750e-02]],\n",
      "\n",
      "        [[-2.28881836e-03, -2.95639038e-04,  8.36181641e-03, ...,\n",
      "           1.63574219e-02,  1.49536133e-03, -3.00292969e-02],\n",
      "         [-6.22558594e-03,  2.77099609e-02, -2.53906250e-02, ...,\n",
      "           1.35040283e-03, -3.95507812e-02, -1.77001953e-02],\n",
      "         [ 1.51062012e-03,  4.39453125e-03,  2.83813477e-03, ...,\n",
      "          -3.24707031e-02, -4.49218750e-02,  2.56347656e-02],\n",
      "         ...,\n",
      "         [ 4.73022461e-03, -1.17187500e-02, -2.42919922e-02, ...,\n",
      "           6.50024414e-03, -2.07519531e-02,  2.86865234e-02],\n",
      "         [ 2.09045410e-03,  2.30712891e-02,  9.58442688e-05, ...,\n",
      "          -9.03320312e-03,  4.56542969e-02,  6.39648438e-02],\n",
      "         [ 3.99780273e-03,  2.63671875e-02, -1.16577148e-02, ...,\n",
      "          -1.33056641e-02, -2.16064453e-02, -3.64303589e-04]]],\n",
      "\n",
      "\n",
      "       [[[-1.35498047e-02,  2.34375000e-02, -1.17797852e-02, ...,\n",
      "           1.32446289e-02, -1.23901367e-02,  1.20849609e-02],\n",
      "         [ 1.05285645e-03,  1.74713135e-03, -3.35693359e-03, ...,\n",
      "          -4.82177734e-03,  1.64794922e-02, -8.23974609e-03],\n",
      "         [-2.05993652e-03,  1.96838379e-03,  4.24194336e-03, ...,\n",
      "          -8.85009766e-03, -1.32446289e-02, -1.40991211e-02],\n",
      "         ...,\n",
      "         [ 8.05664062e-03,  1.93786621e-03, -7.35473633e-03, ...,\n",
      "          -1.08032227e-02, -1.23901367e-02, -2.07519531e-02],\n",
      "         [-1.02539062e-02,  2.07519531e-02, -1.96533203e-02, ...,\n",
      "          -1.44195557e-03,  8.05664062e-03,  8.60595703e-03],\n",
      "         [-1.50680542e-04,  3.71932983e-05,  2.97546387e-03, ...,\n",
      "           8.11767578e-03, -1.10473633e-02,  9.46044922e-03]],\n",
      "\n",
      "        [[-9.64355469e-03, -5.88378906e-02, -5.63964844e-02, ...,\n",
      "          -1.46484375e-02,  8.91113281e-03, -4.76074219e-03],\n",
      "         [ 2.57568359e-02, -2.79541016e-02, -1.77001953e-02, ...,\n",
      "          -1.36718750e-02,  2.18505859e-02,  3.49426270e-03],\n",
      "         [ 7.69042969e-03, -1.66015625e-02,  6.80541992e-03, ...,\n",
      "           6.25610352e-03, -6.83593750e-03,  3.29589844e-03],\n",
      "         ...,\n",
      "         [-8.39233398e-04, -2.65502930e-03,  2.50244141e-02, ...,\n",
      "          -7.41577148e-03,  1.90734863e-03, -1.03759766e-02],\n",
      "         [-1.49536133e-02,  1.68457031e-02, -2.03857422e-02, ...,\n",
      "          -9.33837891e-03, -1.36718750e-02, -7.99560547e-03],\n",
      "         [-3.56445312e-02,  1.63269043e-03, -9.82666016e-03, ...,\n",
      "           6.71386719e-03, -6.50024414e-03,  6.04248047e-03]],\n",
      "\n",
      "        [[ 1.03759766e-02, -9.23156738e-04,  5.24902344e-03, ...,\n",
      "           9.64355469e-03,  1.33056641e-02, -1.04980469e-02],\n",
      "         [ 2.19726562e-02,  8.11767578e-03,  1.11694336e-02, ...,\n",
      "           5.88989258e-03, -2.60925293e-03,  1.09672546e-04],\n",
      "         [-8.17871094e-03, -1.55029297e-02,  6.86645508e-03, ...,\n",
      "          -4.80957031e-02, -5.98144531e-02,  4.18090820e-03],\n",
      "         ...,\n",
      "         [ 6.17980957e-04,  5.27954102e-03,  1.57470703e-02, ...,\n",
      "           1.22680664e-02, -1.59912109e-02, -1.67236328e-02],\n",
      "         [ 4.51660156e-03, -2.13623047e-02,  7.53784180e-03, ...,\n",
      "          -2.22167969e-02,  7.24792480e-04,  3.41796875e-03],\n",
      "         [ 2.08740234e-02,  1.15356445e-02,  2.11181641e-02, ...,\n",
      "          -6.22558594e-03, -3.96728516e-03,  8.64257812e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.56347656e-03, -2.74658203e-02,  1.42669678e-03, ...,\n",
      "           2.60009766e-02,  4.34570312e-02, -1.03759766e-03],\n",
      "         [ 3.58886719e-02, -8.91113281e-03,  1.46484375e-02, ...,\n",
      "          -1.49536133e-02,  1.32446289e-02, -2.25830078e-02],\n",
      "         [-1.52587891e-03, -1.31225586e-02, -8.54492188e-03, ...,\n",
      "           5.15136719e-02,  6.43920898e-03,  3.03955078e-02],\n",
      "         ...,\n",
      "         [-1.18408203e-02, -1.87988281e-02, -1.54418945e-02, ...,\n",
      "           6.95800781e-03, -9.03320312e-03,  2.07519531e-03],\n",
      "         [ 1.03759766e-02, -3.43322754e-05, -1.03759766e-02, ...,\n",
      "          -1.91497803e-03,  9.58251953e-03,  2.56347656e-02],\n",
      "         [ 3.68652344e-02,  1.23901367e-02,  6.14166260e-04, ...,\n",
      "          -1.05590820e-02, -3.90625000e-02,  4.91333008e-03]],\n",
      "\n",
      "        [[ 8.36181641e-03, -3.67164612e-05, -1.25885010e-03, ...,\n",
      "          -2.74658203e-02, -1.25732422e-02,  8.78906250e-03],\n",
      "         [ 4.60815430e-03,  7.56835938e-03,  2.13623047e-03, ...,\n",
      "           3.14941406e-02,  1.73339844e-02,  4.66918945e-03],\n",
      "         [ 8.39233398e-04,  6.80541992e-03,  2.07519531e-02, ...,\n",
      "           3.10058594e-02,  5.82885742e-03, -3.99780273e-03],\n",
      "         ...,\n",
      "         [ 1.91650391e-02,  1.89208984e-02, -3.43322754e-03, ...,\n",
      "          -6.17675781e-02, -6.22558594e-03,  4.39453125e-02],\n",
      "         [ 6.50024414e-03, -4.21142578e-03,  1.90429688e-02, ...,\n",
      "           7.41577148e-03,  3.49121094e-02,  1.59912109e-02],\n",
      "         [ 2.71606445e-03, -1.41601562e-02,  6.98852539e-03, ...,\n",
      "          -2.60925293e-03, -2.39257812e-02, -2.03857422e-02]],\n",
      "\n",
      "        [[-2.23388672e-02, -7.11059570e-03, -2.75878906e-02, ...,\n",
      "          -1.57470703e-02, -1.26342773e-02, -1.29394531e-02],\n",
      "         [ 1.68457031e-02,  9.21630859e-03, -5.61523438e-03, ...,\n",
      "           1.40380859e-02, -4.05883789e-03, -3.17382812e-02],\n",
      "         [ 8.91113281e-03, -2.77709961e-03,  1.13525391e-02, ...,\n",
      "           1.81884766e-02,  3.17382812e-02, -3.12805176e-03],\n",
      "         ...,\n",
      "         [ 1.92260742e-03, -3.05175781e-03,  1.69677734e-02, ...,\n",
      "          -1.83105469e-02,  4.69207764e-04, -4.37011719e-02],\n",
      "         [-2.72216797e-02,  1.74560547e-02,  4.73022461e-03, ...,\n",
      "           3.34167480e-03,  5.12695312e-02, -5.66406250e-02],\n",
      "         [-2.01416016e-02, -1.68457031e-02,  1.51367188e-02, ...,\n",
      "          -4.88281250e-02,  1.97753906e-02,  3.99780273e-03]]]],      dtype=float32)}, 'out': {'kernel': Array([[[[-1.62124634e-05,  2.76184082e-03,  2.33459473e-03, ...,\n",
      "           4.18090820e-03, -3.35693359e-03,  6.19506836e-03],\n",
      "         [-1.92260742e-03,  1.84631348e-03, -2.72750854e-04, ...,\n",
      "          -3.32641602e-03, -2.48718262e-03, -6.56127930e-04],\n",
      "         [ 4.88281250e-03, -1.29699707e-03,  9.26971436e-04, ...,\n",
      "           5.88989258e-03, -2.57873535e-03,  2.62451172e-03],\n",
      "         ...,\n",
      "         [ 2.27355957e-03, -1.18255615e-03,  4.65393066e-04, ...,\n",
      "           1.17492676e-03, -7.93457031e-03,  5.37109375e-03],\n",
      "         [ 4.15039062e-03, -5.64575195e-03,  2.36511230e-03, ...,\n",
      "          -3.78417969e-03, -2.33459473e-03, -3.20434570e-04],\n",
      "         [-3.17382812e-03,  4.54711914e-03, -1.15966797e-03, ...,\n",
      "          -1.00708008e-03,  3.78417969e-03, -3.96728516e-03]],\n",
      "\n",
      "        [[ 4.82177734e-03, -3.00598145e-03,  2.23388672e-02, ...,\n",
      "           3.31115723e-03,  2.18200684e-03, -4.33349609e-03],\n",
      "         [-1.94091797e-02, -5.43212891e-03,  1.25732422e-02, ...,\n",
      "           6.89697266e-03, -1.25122070e-02, -2.05078125e-02],\n",
      "         [ 1.42211914e-02, -1.31835938e-02, -1.19628906e-02, ...,\n",
      "           5.12695312e-03,  5.03540039e-03,  5.43212891e-03],\n",
      "         ...,\n",
      "         [-1.78222656e-02, -6.80541992e-03, -6.22558594e-03, ...,\n",
      "           2.22778320e-03, -9.09423828e-03, -3.49426270e-03],\n",
      "         [ 1.44653320e-02, -2.85644531e-02, -9.82666016e-03, ...,\n",
      "           7.93457031e-03,  2.11181641e-02,  7.44628906e-03],\n",
      "         [ 1.24511719e-02, -1.27563477e-02,  2.74658203e-03, ...,\n",
      "           1.20544434e-03, -7.26318359e-03, -1.14746094e-02]],\n",
      "\n",
      "        [[-1.82342529e-03, -9.53674316e-04, -2.03857422e-02, ...,\n",
      "          -7.23266602e-03,  2.05078125e-02, -1.19628906e-02],\n",
      "         [ 1.52587891e-02, -1.84326172e-02,  3.23486328e-03, ...,\n",
      "          -2.74658203e-02, -1.57470703e-02,  1.30462646e-03],\n",
      "         [ 9.21630859e-03,  5.88989258e-03, -1.44653320e-02, ...,\n",
      "           3.02734375e-02,  3.58886719e-02,  9.70458984e-03],\n",
      "         ...,\n",
      "         [ 5.55419922e-03, -2.95639038e-04,  1.82342529e-03, ...,\n",
      "           2.11181641e-02,  3.26538086e-03, -1.35498047e-02],\n",
      "         [ 8.30078125e-03,  3.39355469e-02, -9.88769531e-03, ...,\n",
      "           1.01318359e-02,  2.09960938e-02,  9.39941406e-03],\n",
      "         [ 4.88281250e-03,  2.73437500e-02, -7.38525391e-03, ...,\n",
      "          -1.17797852e-02, -1.44653320e-02, -7.59887695e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.57763672e-03, -2.02636719e-02,  2.29492188e-02, ...,\n",
      "          -2.94189453e-02, -1.10473633e-02,  1.19628906e-02],\n",
      "         [ 3.44238281e-02, -2.17285156e-02,  2.62451172e-02, ...,\n",
      "          -1.57470703e-02, -6.98852539e-03,  1.09863281e-02],\n",
      "         [ 3.22265625e-02, -9.39941406e-03, -1.22680664e-02, ...,\n",
      "           2.23388672e-02,  3.32031250e-02,  4.48608398e-03],\n",
      "         ...,\n",
      "         [ 7.14111328e-03,  2.89306641e-02, -4.30297852e-03, ...,\n",
      "           2.10571289e-03, -1.00708008e-02, -3.19824219e-02],\n",
      "         [ 1.53808594e-02, -1.43647194e-05,  1.77001953e-02, ...,\n",
      "          -7.82012939e-04, -1.12915039e-02,  2.72216797e-02],\n",
      "         [ 4.54711914e-03,  2.13623047e-03,  1.35498047e-02, ...,\n",
      "           2.94494629e-03,  5.27954102e-03,  2.59399414e-03]],\n",
      "\n",
      "        [[-6.74438477e-03, -2.02636719e-02, -1.87988281e-02, ...,\n",
      "           6.62231445e-03,  2.70996094e-02, -5.37109375e-03],\n",
      "         [ 1.01928711e-02, -1.98974609e-02, -3.21960449e-03, ...,\n",
      "           4.48608398e-03, -1.22680664e-02,  1.13525391e-02],\n",
      "         [-1.11694336e-02, -6.52313232e-04,  1.78222656e-02, ...,\n",
      "           3.40270996e-03,  1.30004883e-02,  4.27246094e-03],\n",
      "         ...,\n",
      "         [ 2.27050781e-02, -2.66113281e-02,  3.31115723e-03, ...,\n",
      "          -1.92871094e-02, -9.21630859e-03,  4.54711914e-03],\n",
      "         [ 1.89208984e-02, -1.94091797e-02, -1.66015625e-02, ...,\n",
      "           1.19018555e-02, -1.12915039e-02,  8.11767578e-03],\n",
      "         [-1.09863281e-02,  3.61633301e-03, -1.47094727e-02, ...,\n",
      "           7.75146484e-03, -1.28173828e-02,  7.69042969e-03]],\n",
      "\n",
      "        [[ 6.65283203e-03, -7.78198242e-03,  1.12915039e-02, ...,\n",
      "           7.50732422e-03,  1.80816650e-03,  8.05664062e-03],\n",
      "         [ 2.77099609e-02,  3.96728516e-03, -1.77383423e-04, ...,\n",
      "          -1.63574219e-02,  2.02636719e-02,  1.59912109e-02],\n",
      "         [-3.87573242e-03, -2.67333984e-02,  2.44140625e-03, ...,\n",
      "           2.72216797e-02, -2.12402344e-02, -1.03149414e-02],\n",
      "         ...,\n",
      "         [ 1.94091797e-02, -1.12304688e-02,  8.78906250e-03, ...,\n",
      "          -3.70025635e-04, -2.68554688e-03,  1.87988281e-02],\n",
      "         [-1.67236328e-02, -1.84631348e-03,  3.19824219e-02, ...,\n",
      "           3.11279297e-02,  3.61328125e-02, -1.28784180e-02],\n",
      "         [-1.42822266e-02,  2.74658203e-03,  2.22167969e-02, ...,\n",
      "           1.13525391e-02, -3.93676758e-03, -4.05883789e-03]]],\n",
      "\n",
      "\n",
      "       [[[ 2.60925293e-03,  3.40270996e-03,  8.29696655e-05, ...,\n",
      "          -1.85394287e-03, -2.79235840e-03,  7.08007812e-03],\n",
      "         [ 6.04248047e-03,  7.01904297e-03, -1.87873840e-04, ...,\n",
      "           3.79943848e-03, -2.60925293e-03,  5.06591797e-03],\n",
      "         [-1.11389160e-03,  2.50244141e-03,  3.11279297e-03, ...,\n",
      "          -3.60107422e-03,  1.25122070e-03,  5.70678711e-03],\n",
      "         ...,\n",
      "         [ 4.63867188e-03, -4.33349609e-03, -5.72204590e-04, ...,\n",
      "           5.67626953e-03, -7.93457031e-03,  3.75366211e-03],\n",
      "         [ 3.35693359e-03,  3.06701660e-03, -5.30242920e-04, ...,\n",
      "           1.48010254e-03,  3.63159180e-03,  1.39617920e-03],\n",
      "         [ 3.89099121e-03,  8.36181641e-03,  1.72424316e-03, ...,\n",
      "          -3.58581543e-03, -5.67626953e-03,  3.43322754e-03]],\n",
      "\n",
      "        [[-8.46862793e-04,  9.07897949e-04,  4.85229492e-03, ...,\n",
      "           1.53350830e-03,  6.58035278e-05,  1.22070312e-03],\n",
      "         [-1.10626221e-03,  1.43432617e-03,  7.62939453e-04, ...,\n",
      "           4.27246094e-03, -1.35803223e-03, -1.38854980e-03],\n",
      "         [-6.10351562e-04, -1.43432617e-03,  1.25885010e-03, ...,\n",
      "          -1.78527832e-03, -3.03649902e-03, -1.18255615e-03],\n",
      "         ...,\n",
      "         [ 3.14331055e-03,  3.92913818e-04,  3.03649902e-03, ...,\n",
      "          -1.05285645e-03, -3.92913818e-04, -1.45721436e-03],\n",
      "         [-4.10079956e-04, -1.03759766e-03, -5.76019287e-04, ...,\n",
      "          -4.91142273e-05, -2.16674805e-03, -1.51062012e-03],\n",
      "         [-1.86920166e-03, -4.11987305e-03, -1.74713135e-03, ...,\n",
      "          -1.42669678e-03, -2.27355957e-03,  1.24359131e-03]],\n",
      "\n",
      "        [[-1.02539062e-02, -2.34375000e-02,  6.71386719e-03, ...,\n",
      "           1.20849609e-02, -6.37817383e-03, -1.68457031e-02],\n",
      "         [ 7.08007812e-03, -9.52148438e-03, -8.05664062e-03, ...,\n",
      "          -9.64355469e-03,  7.43865967e-05,  1.96533203e-02],\n",
      "         [ 1.72119141e-02, -1.81884766e-02, -2.27050781e-02, ...,\n",
      "          -1.92260742e-03,  6.31713867e-03,  1.19018555e-02],\n",
      "         ...,\n",
      "         [ 1.09252930e-02, -5.40161133e-03, -9.82666016e-03, ...,\n",
      "           1.35498047e-02, -1.02996826e-03, -2.33459473e-03],\n",
      "         [-2.99072266e-02,  2.34375000e-02, -2.24304199e-03, ...,\n",
      "          -5.40161133e-03,  4.88281250e-03, -1.21459961e-02],\n",
      "         [-8.88824463e-04, -4.63867188e-03, -4.79125977e-03, ...,\n",
      "          -1.26953125e-02,  1.38549805e-02, -7.99560547e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.81250000e-03, -6.74438477e-03,  2.52685547e-02, ...,\n",
      "          -1.22833252e-03,  4.45556641e-03, -2.33154297e-02],\n",
      "         [-4.66918945e-03, -1.50146484e-02, -2.17285156e-02, ...,\n",
      "           3.68652344e-02, -1.28173828e-02, -3.73535156e-02],\n",
      "         [-1.57470703e-02,  2.68554688e-02, -6.04248047e-03, ...,\n",
      "           4.05273438e-02,  1.25732422e-02, -2.44140625e-02],\n",
      "         ...,\n",
      "         [-1.73339844e-02,  3.83300781e-02,  6.07299805e-03, ...,\n",
      "           1.08032227e-02,  1.75781250e-02, -7.87353516e-03],\n",
      "         [-3.60107422e-03, -6.95800781e-03, -1.98974609e-02, ...,\n",
      "          -1.23291016e-02,  2.02636719e-02,  2.38037109e-02],\n",
      "         [ 8.78906250e-03, -2.42919922e-02,  5.09643555e-03, ...,\n",
      "           1.48925781e-02, -1.87988281e-02, -9.39941406e-03]],\n",
      "\n",
      "        [[-7.20214844e-03, -9.27734375e-03, -9.76562500e-04, ...,\n",
      "           5.73730469e-03, -2.18505859e-02,  6.52313232e-04],\n",
      "         [-4.00390625e-02,  1.96533203e-02,  2.97546387e-03, ...,\n",
      "          -2.05078125e-02, -1.78222656e-02,  1.55639648e-02],\n",
      "         [ 5.82885742e-03,  4.80957031e-02, -2.13623047e-02, ...,\n",
      "           2.42919922e-02, -2.75878906e-02, -1.69677734e-02],\n",
      "         ...,\n",
      "         [ 4.24194336e-03,  3.34167480e-03,  2.31933594e-02, ...,\n",
      "           2.25830078e-02,  2.77099609e-02,  1.90429688e-02],\n",
      "         [ 3.39355469e-02,  3.24707031e-02, -5.56640625e-02, ...,\n",
      "           2.01416016e-02,  1.45874023e-02, -2.39257812e-02],\n",
      "         [ 1.35498047e-02,  3.51562500e-02,  1.06811523e-03, ...,\n",
      "           1.25122070e-02, -1.22680664e-02, -1.31835938e-02]],\n",
      "\n",
      "        [[-6.39648438e-02, -7.29370117e-03,  2.97851562e-02, ...,\n",
      "          -9.03320312e-03, -1.56250000e-02, -2.94189453e-02],\n",
      "         [-4.24804688e-02,  1.61132812e-02,  1.50756836e-02, ...,\n",
      "          -2.53906250e-02, -3.17382812e-02, -1.73339844e-02],\n",
      "         [ 1.88446045e-03, -1.75476074e-03, -4.66918945e-03, ...,\n",
      "          -3.27148438e-02,  1.92871094e-02, -1.13525391e-02],\n",
      "         ...,\n",
      "         [-1.86767578e-02, -1.17797852e-02,  3.05175781e-02, ...,\n",
      "          -2.06298828e-02, -4.00390625e-02,  2.56347656e-02],\n",
      "         [ 8.43048096e-04,  2.55126953e-02, -2.30712891e-02, ...,\n",
      "          -6.65283203e-03, -7.44628906e-03, -1.14135742e-02],\n",
      "         [ 5.85937500e-03,  5.03540039e-04, -6.16455078e-03, ...,\n",
      "           2.60009766e-02,  1.62353516e-02, -6.54296875e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 2.33459473e-03, -3.93676758e-03,  4.11987305e-03, ...,\n",
      "           5.07354736e-04, -1.83868408e-03,  6.33239746e-04],\n",
      "         [-2.13623047e-03, -2.45666504e-03, -1.62506104e-03, ...,\n",
      "           2.92968750e-03,  3.84521484e-03,  1.44958496e-03],\n",
      "         [-3.28063965e-03,  4.52041626e-04,  1.65557861e-03, ...,\n",
      "           4.73022461e-03,  9.65118408e-04, -2.85339355e-03],\n",
      "         ...,\n",
      "         [-1.59263611e-04, -6.48498535e-04, -2.85339355e-03, ...,\n",
      "           2.80761719e-03, -3.26538086e-03,  8.11767578e-03],\n",
      "         [-2.80761719e-03, -1.03759766e-03,  2.05993652e-03, ...,\n",
      "           2.05993652e-03, -2.33459473e-03,  6.07967377e-05],\n",
      "         [-6.62231445e-03,  3.08227539e-03, -3.05175781e-03, ...,\n",
      "           1.32751465e-03, -2.12097168e-03, -4.45556641e-03]],\n",
      "\n",
      "        [[-9.03320312e-03,  2.01416016e-03, -8.30078125e-03, ...,\n",
      "           5.37109375e-03, -8.85009766e-03,  8.05664062e-03],\n",
      "         [-3.67736816e-03,  7.51495361e-04, -4.36782837e-04, ...,\n",
      "          -6.77490234e-03, -4.76074219e-03, -1.25122070e-03],\n",
      "         [-1.40380859e-03,  1.55029297e-02, -3.81469727e-03, ...,\n",
      "          -1.13525391e-02, -2.88085938e-02,  3.96728516e-03],\n",
      "         ...,\n",
      "         [ 1.80053711e-03,  1.48773193e-03, -1.70898438e-02, ...,\n",
      "          -1.98974609e-02,  1.83105469e-02, -1.00708008e-02],\n",
      "         [-2.02941895e-03, -1.15203857e-03, -6.74438477e-03, ...,\n",
      "           8.17871094e-03, -6.67572021e-05,  1.66015625e-02],\n",
      "         [-9.33837891e-03, -8.05664062e-03, -3.17382812e-03, ...,\n",
      "          -4.36401367e-03,  1.53808594e-02, -3.23486328e-03]],\n",
      "\n",
      "        [[ 5.95092773e-03,  1.18408203e-02, -9.88769531e-03, ...,\n",
      "          -6.19506836e-03,  1.73339844e-02,  9.88769531e-03],\n",
      "         [-1.62353516e-02, -5.85937500e-03, -5.92041016e-03, ...,\n",
      "          -8.97216797e-03, -1.63574219e-02, -2.21252441e-04],\n",
      "         [ 1.19018555e-03, -7.38525391e-03, -4.45556641e-03, ...,\n",
      "          -3.47900391e-03, -1.22070312e-02,  4.97436523e-03],\n",
      "         ...,\n",
      "         [ 1.98974609e-02,  1.80664062e-02, -4.36401367e-03, ...,\n",
      "           1.73339844e-02, -4.73022461e-03,  9.15527344e-03],\n",
      "         [-6.68334961e-03, -1.14746094e-02, -1.72119141e-02, ...,\n",
      "          -4.18090820e-03, -2.51770020e-03, -1.51977539e-02],\n",
      "         [-1.77001953e-02, -2.03857422e-02,  6.89697266e-03, ...,\n",
      "          -7.50732422e-03, -8.54492188e-03,  1.96838379e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.86865234e-02,  1.00097656e-02, -5.76019287e-04, ...,\n",
      "          -1.50146484e-02,  2.09045410e-03, -1.96533203e-02],\n",
      "         [-4.15039062e-03, -1.79443359e-02,  1.55029297e-02, ...,\n",
      "           2.13623047e-02, -2.39257812e-02, -2.52685547e-02],\n",
      "         [ 1.44042969e-02, -3.90625000e-02,  7.56835938e-03, ...,\n",
      "          -2.00195312e-02,  1.03759766e-02, -4.33349609e-03],\n",
      "         ...,\n",
      "         [-8.85009766e-03,  1.00097656e-02,  4.76074219e-03, ...,\n",
      "          -6.31713867e-03,  1.52587891e-02,  1.20849609e-02],\n",
      "         [-1.81884766e-02, -2.02941895e-03, -1.17187500e-02, ...,\n",
      "           8.42285156e-03, -3.11279297e-03,  1.79443359e-02],\n",
      "         [ 1.26953125e-02,  9.82666016e-03,  5.58471680e-03, ...,\n",
      "           4.11987305e-04, -3.75366211e-03,  2.12402344e-02]],\n",
      "\n",
      "        [[ 1.41601562e-02,  1.91650391e-02,  5.15747070e-03, ...,\n",
      "           1.18408203e-02, -7.24792480e-04,  1.80664062e-02],\n",
      "         [-1.30615234e-02, -1.27563477e-02, -5.12695312e-03, ...,\n",
      "          -2.35595703e-02, -7.99560547e-03,  1.66893005e-05],\n",
      "         [-2.14843750e-02, -1.96838379e-03,  1.42211914e-02, ...,\n",
      "           3.24707031e-02,  8.97216797e-03,  4.24194336e-03],\n",
      "         ...,\n",
      "         [-1.15966797e-02,  2.40478516e-02, -2.28271484e-02, ...,\n",
      "          -1.31988525e-03,  1.90429688e-02,  6.59179688e-03],\n",
      "         [-1.87988281e-02,  2.44140625e-02, -8.91113281e-03, ...,\n",
      "          -3.17382812e-02,  1.17492676e-03,  6.40869141e-03],\n",
      "         [ 1.25122070e-02,  3.52478027e-03, -6.04248047e-03, ...,\n",
      "           2.12402344e-02, -3.87573242e-03,  2.51464844e-02]],\n",
      "\n",
      "        [[ 1.59912109e-02,  1.68457031e-02,  1.26953125e-02, ...,\n",
      "          -9.33837891e-03,  3.31115723e-03, -1.18255615e-03],\n",
      "         [-1.89208984e-02,  9.94873047e-03, -2.33154297e-02, ...,\n",
      "          -1.61132812e-02,  6.22558594e-03, -7.89642334e-04],\n",
      "         [ 2.12402344e-02, -7.09533691e-04,  9.33837891e-03, ...,\n",
      "          -8.11767578e-03,  8.97216797e-03,  6.46972656e-03],\n",
      "         ...,\n",
      "         [ 9.03320312e-03, -1.83105469e-02,  7.44628906e-03, ...,\n",
      "           1.28173828e-02,  2.85644531e-02,  9.82666016e-03],\n",
      "         [-1.61132812e-02,  1.36718750e-02,  3.50952148e-03, ...,\n",
      "          -6.62231445e-03, -2.74658203e-02,  1.19628906e-02],\n",
      "         [-5.34057617e-03,  1.12304688e-02,  4.33349609e-03, ...,\n",
      "           2.42919922e-02,  4.48608398e-03, -9.09423828e-03]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[-4.48608398e-03, -2.82287598e-04, -1.15966797e-02, ...,\n",
      "           1.88446045e-03, -1.58691406e-02, -4.36401367e-03],\n",
      "         [-4.30297852e-03, -4.21142578e-03,  1.41601562e-02, ...,\n",
      "          -4.54711914e-03, -6.10351562e-03, -2.02636719e-02],\n",
      "         [-2.47192383e-03,  1.48315430e-02, -6.40869141e-03, ...,\n",
      "           1.01470947e-03, -2.71606445e-03,  1.93786621e-03],\n",
      "         ...,\n",
      "         [-2.80761719e-03,  2.69775391e-02,  2.51770020e-03, ...,\n",
      "          -3.72314453e-03,  2.96630859e-02,  2.20947266e-02],\n",
      "         [-1.64794922e-02,  8.78906250e-03,  1.65557861e-03, ...,\n",
      "          -6.53076172e-03,  2.30407715e-03,  1.95312500e-03],\n",
      "         [ 8.72802734e-03,  7.75146484e-03,  9.70458984e-03, ...,\n",
      "          -6.01196289e-03, -5.40161133e-03, -2.00195312e-02]],\n",
      "\n",
      "        [[ 4.02832031e-03,  1.36566162e-03,  2.07519531e-03, ...,\n",
      "           2.68554688e-03, -3.90625000e-03, -4.51660156e-03],\n",
      "         [-1.64031982e-03, -5.43212891e-03, -4.79125977e-03, ...,\n",
      "           8.85009766e-04,  3.31878662e-04, -4.69970703e-03],\n",
      "         [-2.70080566e-03, -1.64031982e-03, -9.15527344e-03, ...,\n",
      "           1.95312500e-03,  3.86047363e-03, -2.68554688e-03],\n",
      "         ...,\n",
      "         [-6.04248047e-03,  2.71606445e-03,  4.42504883e-03, ...,\n",
      "          -1.73950195e-03, -2.15148926e-03, -3.63159180e-03],\n",
      "         [ 3.34167480e-03, -2.80761719e-03,  8.81195068e-04, ...,\n",
      "          -5.00488281e-03, -2.13623047e-03, -2.59399414e-03],\n",
      "         [ 6.07299805e-03,  4.27246094e-03,  2.22778320e-03, ...,\n",
      "           5.06591797e-03, -1.29938126e-05, -3.08227539e-03]],\n",
      "\n",
      "        [[ 1.03759766e-03,  1.62353516e-02, -1.35803223e-03, ...,\n",
      "           6.16455078e-03,  5.03540039e-03,  8.85009766e-03],\n",
      "         [-1.62353516e-02,  1.84326172e-02,  2.02941895e-03, ...,\n",
      "          -1.41906738e-03,  1.71661377e-03, -5.52368164e-03],\n",
      "         [ 1.74560547e-02,  7.85827637e-04,  7.20214844e-03, ...,\n",
      "          -1.54418945e-02,  9.58251953e-03, -1.11389160e-03],\n",
      "         ...,\n",
      "         [-1.25732422e-02, -1.25122070e-02, -9.15527344e-03, ...,\n",
      "           1.59912109e-02,  7.08007812e-03,  1.28173828e-02],\n",
      "         [-1.41601562e-02, -4.94384766e-03,  1.12915039e-02, ...,\n",
      "           6.43920898e-03,  6.86645508e-04, -1.03759766e-03],\n",
      "         [-8.36181641e-03,  1.39770508e-02,  2.66113281e-02, ...,\n",
      "          -2.93731689e-04,  4.15039062e-03, -1.48773193e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.72314453e-03, -3.28063965e-03,  3.32031250e-02, ...,\n",
      "           2.01416016e-03,  4.36401367e-03,  4.02832031e-03],\n",
      "         [-7.20214844e-03, -4.11987305e-03,  1.58691406e-02, ...,\n",
      "          -3.34472656e-02,  2.06298828e-02, -2.66113281e-02],\n",
      "         [ 2.27355957e-03,  1.84326172e-02, -1.63574219e-02, ...,\n",
      "          -2.16064453e-02, -1.12915039e-02,  5.12695312e-03],\n",
      "         ...,\n",
      "         [ 5.34057617e-03, -1.28173828e-02,  6.92749023e-03, ...,\n",
      "          -1.44653320e-02,  1.55639648e-03, -1.63574219e-02],\n",
      "         [ 1.97753906e-02,  3.66210938e-02,  1.39160156e-02, ...,\n",
      "          -1.37329102e-02,  1.02539062e-02,  9.15527344e-03],\n",
      "         [-1.04370117e-02, -1.14135742e-02, -6.01196289e-03, ...,\n",
      "          -3.43322754e-03, -3.94821167e-04,  5.95092773e-03]],\n",
      "\n",
      "        [[ 1.28173828e-02,  9.57489014e-04,  1.04370117e-02, ...,\n",
      "          -2.20947266e-02, -8.66699219e-03, -4.17480469e-02],\n",
      "         [ 2.14843750e-02,  8.17871094e-03,  2.19726562e-02, ...,\n",
      "          -5.18798828e-03,  2.63671875e-02,  5.87463379e-04],\n",
      "         [-2.70996094e-02, -3.39355469e-02, -4.27246094e-03, ...,\n",
      "           1.17492676e-03,  1.25122070e-02, -4.12597656e-02],\n",
      "         ...,\n",
      "         [ 5.83648682e-04, -8.91113281e-03, -2.39562988e-03, ...,\n",
      "          -2.17285156e-02, -1.91650391e-02, -3.17382812e-02],\n",
      "         [ 8.65936279e-04, -4.37011719e-02, -2.85644531e-02, ...,\n",
      "          -3.01513672e-02, -3.34472656e-02, -3.23486328e-03],\n",
      "         [ 2.68554688e-02,  2.85644531e-02, -7.01904297e-03, ...,\n",
      "          -5.92041016e-03, -8.48388672e-03, -1.18408203e-02]],\n",
      "\n",
      "        [[-1.66015625e-02,  1.53198242e-02,  1.19018555e-02, ...,\n",
      "          -9.03320312e-03,  9.76562500e-03,  3.12500000e-02],\n",
      "         [ 4.10156250e-02,  2.16064453e-02, -2.47192383e-03, ...,\n",
      "          -2.74658203e-02,  6.74438477e-03, -1.66893005e-04],\n",
      "         [-9.72747803e-04,  8.97216797e-03,  8.66699219e-03, ...,\n",
      "          -2.49023438e-02, -1.22070312e-03,  2.39257812e-02],\n",
      "         ...,\n",
      "         [-1.23901367e-02,  2.46582031e-02,  3.97949219e-02, ...,\n",
      "          -4.76074219e-03, -2.56347656e-02, -2.41699219e-02],\n",
      "         [-1.59912109e-02,  3.17382812e-02,  9.76562500e-03, ...,\n",
      "           9.64355469e-03, -1.98974609e-02, -3.83377075e-04],\n",
      "         [-1.66015625e-02,  1.63574219e-02,  7.75146484e-03, ...,\n",
      "           2.72216797e-02,  1.75781250e-02,  1.70898438e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 3.38745117e-03, -9.94873047e-03,  2.09045410e-03, ...,\n",
      "          -1.30462646e-03, -3.02124023e-03,  3.70788574e-03],\n",
      "         [ 2.68554688e-03,  3.99780273e-03,  2.21252441e-03, ...,\n",
      "           2.99072266e-03, -4.66918945e-03, -2.91442871e-03],\n",
      "         [ 6.21795654e-04, -2.67028809e-03, -3.08227539e-03, ...,\n",
      "          -2.59399414e-03, -3.55529785e-03, -2.41088867e-03],\n",
      "         ...,\n",
      "         [ 2.94494629e-03,  1.10626221e-03, -5.58471680e-03, ...,\n",
      "          -3.21960449e-03,  7.75146484e-03, -4.60815430e-03],\n",
      "         [ 2.50244141e-03, -5.79833984e-03,  3.78417969e-03, ...,\n",
      "           4.88281250e-03,  6.10351562e-03,  2.12097168e-03],\n",
      "         [ 5.40161133e-03,  7.40051270e-04, -1.12152100e-03, ...,\n",
      "           3.15856934e-03, -1.05590820e-02, -4.42504883e-03]],\n",
      "\n",
      "        [[ 2.08740234e-02, -4.42504883e-03, -1.61132812e-02, ...,\n",
      "          -2.50244141e-03,  2.27050781e-02,  1.20239258e-02],\n",
      "         [-1.20239258e-02,  5.06591797e-03, -6.77490234e-03, ...,\n",
      "           2.34375000e-02, -5.73730469e-03,  3.15856934e-03],\n",
      "         [-9.70458984e-03,  2.06298828e-02,  8.60595703e-03, ...,\n",
      "           4.27246094e-03,  5.61523438e-03, -6.16455078e-03],\n",
      "         ...,\n",
      "         [ 5.21850586e-03, -4.08935547e-03, -1.80816650e-03, ...,\n",
      "          -3.72314453e-03, -1.98364258e-03, -4.85229492e-03],\n",
      "         [ 6.13403320e-03, -9.46044922e-03,  3.21960449e-03, ...,\n",
      "          -2.54821777e-03, -4.21142578e-03,  1.60217285e-03],\n",
      "         [-5.60760498e-04, -7.04956055e-03,  7.29370117e-03, ...,\n",
      "           4.95910645e-04,  1.73950195e-03, -3.44848633e-03]],\n",
      "\n",
      "        [[ 7.75146484e-03,  5.98144531e-03, -9.15527344e-03, ...,\n",
      "          -1.62353516e-02, -3.73840332e-03, -1.42822266e-02],\n",
      "         [-8.23974609e-03, -1.33666992e-02, -2.27050781e-02, ...,\n",
      "          -8.11767578e-03,  4.54711914e-03, -3.14941406e-02],\n",
      "         [-6.43920898e-03,  1.81579590e-03, -5.49316406e-03, ...,\n",
      "           2.25830078e-02, -5.12695312e-03,  5.18798828e-03],\n",
      "         ...,\n",
      "         [-1.17797852e-02,  1.04370117e-02,  1.75781250e-02, ...,\n",
      "           8.36181641e-03,  1.17187500e-02,  9.03320312e-03],\n",
      "         [ 6.86645508e-03,  2.42614746e-03,  9.21630859e-03, ...,\n",
      "          -6.71386719e-03, -9.76562500e-04, -2.92968750e-02],\n",
      "         [ 8.48388672e-03, -5.79833984e-03, -8.66699219e-03, ...,\n",
      "           1.31835938e-02,  1.59912109e-02,  1.11694336e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.11083984e-02,  1.28173828e-02,  2.12402344e-02, ...,\n",
      "          -3.34167480e-03,  1.17797852e-02,  1.22680664e-02],\n",
      "         [-6.06536865e-04,  2.02636719e-02, -2.76184082e-03, ...,\n",
      "          -2.17285156e-02,  1.56402588e-03,  2.52685547e-02],\n",
      "         [ 6.89697266e-03, -1.91650391e-02, -2.66113281e-02, ...,\n",
      "           3.12500000e-02,  1.06811523e-02,  1.54418945e-02],\n",
      "         ...,\n",
      "         [ 3.28063965e-03,  1.24931335e-04, -2.51770020e-03, ...,\n",
      "           6.07299805e-03, -1.11083984e-02,  8.08715820e-04],\n",
      "         [-1.95312500e-03,  1.51367188e-02, -3.17382812e-02, ...,\n",
      "           9.58251953e-03,  6.37817383e-03,  1.87988281e-02],\n",
      "         [-5.34057617e-03, -2.35595703e-02, -6.50024414e-03, ...,\n",
      "          -1.53808594e-02,  3.95507812e-02, -1.05285645e-03]],\n",
      "\n",
      "        [[-4.61425781e-02,  2.35595703e-02, -4.34570312e-02, ...,\n",
      "          -2.92968750e-02, -2.90527344e-02, -1.47705078e-02],\n",
      "         [ 1.35803223e-03,  1.73339844e-02, -9.82666016e-03, ...,\n",
      "          -1.16577148e-02, -1.17797852e-02, -8.91113281e-03],\n",
      "         [ 2.41699219e-02, -1.70898438e-02, -2.85644531e-02, ...,\n",
      "           4.94384766e-03, -6.65283203e-03,  1.51367188e-02],\n",
      "         ...,\n",
      "         [ 2.56347656e-02,  2.73437500e-02, -1.06811523e-02, ...,\n",
      "          -1.12304688e-02, -9.03320312e-03,  1.46484375e-02],\n",
      "         [-1.20239258e-02, -3.24707031e-02,  1.73339844e-02, ...,\n",
      "          -7.04956055e-03, -1.26342773e-02,  4.19921875e-02],\n",
      "         [ 8.16345215e-04,  4.02832031e-03, -2.94189453e-02, ...,\n",
      "           2.70996094e-02,  4.45556641e-03,  9.58251953e-03]],\n",
      "\n",
      "        [[-5.95092773e-03,  1.30615234e-02,  2.97851562e-02, ...,\n",
      "           1.19628906e-02,  3.32031250e-02, -4.30297852e-03],\n",
      "         [ 8.85009766e-03,  4.54711914e-03, -1.13525391e-02, ...,\n",
      "          -8.43048096e-04, -4.30297852e-03,  1.40991211e-02],\n",
      "         [-4.66918945e-03,  1.02539062e-02,  3.24707031e-02, ...,\n",
      "           5.79833984e-03, -1.61132812e-02,  2.56347656e-02],\n",
      "         ...,\n",
      "         [-1.23291016e-02, -1.61132812e-02,  4.71191406e-02, ...,\n",
      "           2.73132324e-03, -1.28173828e-02,  2.41699219e-02],\n",
      "         [ 6.65283203e-03, -9.52148438e-03,  1.96533203e-02, ...,\n",
      "           1.78222656e-02,  1.42822266e-02,  2.31933594e-02],\n",
      "         [ 9.15527344e-04, -3.46679688e-02, -1.80664062e-02, ...,\n",
      "          -5.95092773e-03,  3.34472656e-02, -1.42211914e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 3.37219238e-03, -3.15856934e-03,  2.27355957e-03, ...,\n",
      "          -1.73950195e-03, -3.67736816e-03, -4.66918945e-03],\n",
      "         [ 1.60980225e-03,  2.31933594e-03, -2.51770020e-03, ...,\n",
      "          -5.98144531e-03, -2.82287598e-03,  3.20434570e-03],\n",
      "         [ 5.06591797e-03,  8.50677490e-04, -7.20214844e-03, ...,\n",
      "           7.29370117e-03, -1.73187256e-03, -1.50299072e-03],\n",
      "         ...,\n",
      "         [ 5.92041016e-03, -1.02996826e-03, -1.65557861e-03, ...,\n",
      "           1.21307373e-03,  6.19506836e-03,  5.49316406e-03],\n",
      "         [ 3.44848633e-03,  1.80816650e-03, -5.73730469e-03, ...,\n",
      "           2.60925293e-03, -3.47900391e-03, -7.59887695e-03],\n",
      "         [-9.52148438e-03,  6.25610352e-03, -6.37054443e-04, ...,\n",
      "           4.30297852e-03, -5.11169434e-04, -6.68334961e-03]],\n",
      "\n",
      "        [[ 2.21252441e-03,  2.10571289e-03, -4.45556641e-03, ...,\n",
      "           1.05285645e-03, -6.37054443e-04, -9.94873047e-03],\n",
      "         [ 6.01196289e-03, -4.24194336e-03,  4.06265259e-04, ...,\n",
      "          -2.16674805e-03,  1.45912170e-04, -3.40938568e-05],\n",
      "         [-1.29699707e-03,  1.75476074e-03,  1.29699707e-03, ...,\n",
      "          -4.99725342e-04,  1.28936768e-03,  3.96728516e-03],\n",
      "         ...,\n",
      "         [-4.97436523e-03, -7.43865967e-04,  4.73022461e-03, ...,\n",
      "           2.86102295e-04, -2.15148926e-03,  1.28936768e-03],\n",
      "         [-1.28746033e-04, -2.73132324e-03,  1.99890137e-03, ...,\n",
      "           2.22778320e-03,  1.83105469e-03, -4.27246094e-03],\n",
      "         [ 2.74658203e-04,  3.21960449e-03, -2.16674805e-03, ...,\n",
      "          -1.93786621e-03,  2.10571289e-03,  1.91497803e-03]],\n",
      "\n",
      "        [[ 1.60217285e-03,  1.08642578e-02,  2.66113281e-02, ...,\n",
      "           2.74658203e-03,  8.97216797e-03, -7.87353516e-03],\n",
      "         [ 1.22680664e-02,  7.81250000e-03, -6.86645508e-04, ...,\n",
      "          -2.01416016e-02, -1.20849609e-02, -5.00488281e-03],\n",
      "         [-2.86865234e-02, -1.62353516e-02, -7.04956055e-03, ...,\n",
      "           1.36718750e-02, -2.18505859e-02, -4.24194336e-03],\n",
      "         ...,\n",
      "         [ 1.80664062e-02,  4.66918945e-03, -1.86767578e-02, ...,\n",
      "          -4.24194336e-03,  1.66015625e-02, -8.60595703e-03],\n",
      "         [ 3.32641602e-03, -1.19018555e-02, -8.54492188e-03, ...,\n",
      "          -2.16064453e-02,  1.93786621e-03,  1.08032227e-02],\n",
      "         [-2.79541016e-02, -2.07519531e-02,  9.64355469e-03, ...,\n",
      "           2.06298828e-02, -2.25830078e-02, -1.45874023e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.50024414e-03,  1.98974609e-02,  1.98364258e-03, ...,\n",
      "           1.14135742e-02, -1.74560547e-02,  4.33349609e-03],\n",
      "         [ 8.30078125e-03, -2.78320312e-02, -9.58251953e-03, ...,\n",
      "           1.38549805e-02,  2.64892578e-02,  1.95312500e-03],\n",
      "         [-8.85009766e-03,  1.95312500e-02, -1.58691406e-02, ...,\n",
      "          -2.83813477e-03, -1.84326172e-02,  1.09252930e-02],\n",
      "         ...,\n",
      "         [ 2.01416016e-03,  1.17187500e-02, -3.71093750e-02, ...,\n",
      "          -8.11767578e-03, -1.38092041e-03,  6.31713867e-03],\n",
      "         [-1.74560547e-02, -2.67333984e-02,  1.62353516e-02, ...,\n",
      "          -9.21630859e-03, -1.52587891e-02,  1.57470703e-02],\n",
      "         [-7.35473633e-03,  1.42211914e-02, -1.83105469e-02, ...,\n",
      "          -1.75781250e-02,  6.50024414e-03, -2.81982422e-02]],\n",
      "\n",
      "        [[ 1.54418945e-02,  1.02539062e-02,  4.02832031e-03, ...,\n",
      "          -2.57568359e-02,  2.80761719e-02, -3.39355469e-02],\n",
      "         [-9.82666016e-03, -3.10058594e-02, -2.70996094e-02, ...,\n",
      "          -2.94189453e-02, -1.83105469e-02,  1.79443359e-02],\n",
      "         [ 2.96630859e-02,  5.83496094e-02,  1.80664062e-02, ...,\n",
      "           8.54492188e-03,  3.54003906e-02,  1.75781250e-02],\n",
      "         ...,\n",
      "         [ 7.78198242e-03,  1.68457031e-02, -5.17578125e-02, ...,\n",
      "          -2.57568359e-02,  3.05175781e-02, -2.74658203e-03],\n",
      "         [-1.72119141e-02, -2.11181641e-02, -1.50756836e-02, ...,\n",
      "           8.42285156e-03, -2.44140625e-03,  1.32446289e-02],\n",
      "         [-2.22167969e-02, -2.08740234e-02, -1.17187500e-02, ...,\n",
      "           1.42822266e-02,  8.05664062e-03, -1.64794922e-02]],\n",
      "\n",
      "        [[-2.51464844e-02, -9.46044922e-04, -1.52587891e-02, ...,\n",
      "           2.86865234e-02, -3.60107422e-03, -5.63964844e-02],\n",
      "         [ 7.26318359e-03, -3.22265625e-02, -3.51562500e-02, ...,\n",
      "          -2.20947266e-02, -1.29394531e-02,  7.93457031e-03],\n",
      "         [-1.59912109e-02, -1.11083984e-02, -2.81982422e-02, ...,\n",
      "          -8.23974609e-03, -2.31933594e-02,  1.59912109e-02],\n",
      "         ...,\n",
      "         [-4.48608398e-03,  2.62451172e-02, -1.52587891e-02, ...,\n",
      "          -1.97753906e-02, -2.23388672e-02,  3.34472656e-02],\n",
      "         [-1.15966797e-02, -1.90429688e-02,  2.53906250e-02, ...,\n",
      "          -1.08032227e-02, -1.39770508e-02, -1.90429688e-02],\n",
      "         [-4.90722656e-02, -1.50756836e-02,  8.78906250e-03, ...,\n",
      "          -1.62353516e-02, -1.86767578e-02,  6.31713867e-03]]]],      dtype=float32)}, 'query': {'kernel': Array([[[[-5.45501709e-04,  1.25885010e-03, -1.28936768e-03, ...,\n",
      "           9.84191895e-04, -1.48010254e-03,  1.22070312e-03],\n",
      "         [ 6.71386719e-04,  8.69750977e-04, -1.47819519e-05, ...,\n",
      "          -2.19726562e-03,  1.36566162e-03, -2.04467773e-03],\n",
      "         [-1.76429749e-04,  2.51770020e-04,  1.57356262e-04, ...,\n",
      "          -3.17382812e-03, -3.28063965e-03, -3.43322754e-03],\n",
      "         ...,\n",
      "         [ 1.90734863e-03,  1.22833252e-03,  1.89208984e-03, ...,\n",
      "           3.44848633e-03,  1.38854980e-03,  2.21252441e-03],\n",
      "         [-3.14712524e-04, -4.48226929e-04,  3.66210938e-04, ...,\n",
      "           8.08715820e-04, -6.21795654e-04, -6.14166260e-04],\n",
      "         [ 1.40190125e-04, -8.01086426e-05,  6.58035278e-05, ...,\n",
      "           1.15394592e-04,  2.27355957e-03, -1.19018555e-03]],\n",
      "\n",
      "        [[-1.10626221e-03, -5.26905060e-05,  2.71606445e-03, ...,\n",
      "           1.77764893e-03, -1.64985657e-04,  6.21795654e-04],\n",
      "         [-1.81579590e-03,  2.48718262e-03, -1.92642212e-04, ...,\n",
      "          -1.87873840e-04, -6.67572021e-04, -3.56674194e-04],\n",
      "         [ 1.85966492e-04, -1.76429749e-04, -1.60980225e-03, ...,\n",
      "          -1.09863281e-03, -9.48905945e-05, -1.54113770e-03],\n",
      "         ...,\n",
      "         [ 2.71606445e-03,  1.46484375e-03, -2.86102295e-04, ...,\n",
      "          -7.78198242e-04, -7.97271729e-04, -1.12915039e-03],\n",
      "         [ 2.30407715e-03, -9.91821289e-04, -6.96182251e-05, ...,\n",
      "           2.50244141e-03,  1.00708008e-03,  1.96838379e-03],\n",
      "         [ 2.41088867e-03, -6.06536865e-04,  2.91824341e-04, ...,\n",
      "          -1.57356262e-05, -1.85012817e-04,  2.27689743e-05]],\n",
      "\n",
      "        [[-2.05993652e-03, -1.31225586e-03, -1.26647949e-03, ...,\n",
      "           1.69372559e-03,  8.69750977e-04, -2.01416016e-03],\n",
      "         [ 6.37054443e-04, -6.79016113e-04, -8.08715820e-04, ...,\n",
      "          -1.68609619e-03,  2.24113464e-04,  5.22136688e-05],\n",
      "         [ 1.18255615e-03, -1.56402588e-03, -3.79943848e-03, ...,\n",
      "           2.09045410e-03, -1.41143799e-03,  7.53402710e-05],\n",
      "         ...,\n",
      "         [-6.79016113e-04, -2.27355957e-03, -3.28063965e-03, ...,\n",
      "           2.67028809e-03, -2.85339355e-03, -4.80651855e-04],\n",
      "         [-1.12056732e-04, -1.62124634e-04,  4.42504883e-04, ...,\n",
      "          -7.17163086e-04, -3.66210938e-03,  4.15039062e-03],\n",
      "         [ 1.19018555e-03, -1.29699707e-03, -1.61743164e-03, ...,\n",
      "          -4.21142578e-03,  2.11715698e-04, -5.83648682e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.46047974e-04, -2.13623047e-03, -1.67083740e-03, ...,\n",
      "          -5.53131104e-04, -2.45666504e-03,  8.85009766e-04],\n",
      "         [-1.83105469e-04, -2.74658203e-03,  9.26971436e-04, ...,\n",
      "           5.36441803e-05, -2.02178955e-04,  1.44958496e-04],\n",
      "         [-3.47137451e-04, -3.96728516e-04, -3.77655029e-04, ...,\n",
      "           1.45721436e-03,  7.05718994e-04, -1.31225586e-03],\n",
      "         ...,\n",
      "         [ 3.79943848e-03, -3.20434570e-04, -2.47955322e-04, ...,\n",
      "          -9.07897949e-04, -5.30242920e-04,  2.63977051e-03],\n",
      "         [ 6.33239746e-04, -1.20544434e-03,  9.99450684e-04, ...,\n",
      "           2.73132324e-03, -1.83105469e-03,  1.89971924e-03],\n",
      "         [-3.05175781e-04, -1.82342529e-03, -1.45721436e-03, ...,\n",
      "          -6.79016113e-04, -2.07519531e-03,  4.26769257e-05]],\n",
      "\n",
      "        [[-3.26633453e-05,  1.44958496e-03, -5.37872314e-04, ...,\n",
      "          -1.34277344e-03, -1.86920166e-03,  2.60925293e-03],\n",
      "         [ 1.87683105e-03,  3.96728516e-03,  1.25122070e-03, ...,\n",
      "          -6.59942627e-04,  3.17382812e-03, -1.73950195e-03],\n",
      "         [-1.09100342e-03,  3.14712524e-04, -9.34600830e-04, ...,\n",
      "          -3.49426270e-03, -2.95639038e-04, -1.96838379e-03],\n",
      "         ...,\n",
      "         [ 4.69207764e-04, -3.20434570e-04,  8.62121582e-04, ...,\n",
      "           1.98364258e-03, -2.26974487e-04, -4.05883789e-03],\n",
      "         [-1.00708008e-03,  2.38037109e-03,  1.33514404e-03, ...,\n",
      "          -4.33921814e-05, -1.19018555e-03,  1.29699707e-03],\n",
      "         [ 1.80053711e-03, -2.48718262e-03, -7.05718994e-04, ...,\n",
      "          -7.59124756e-04, -1.15633011e-05, -6.13403320e-03]],\n",
      "\n",
      "        [[-2.92968750e-03, -7.28607178e-04, -1.11389160e-03, ...,\n",
      "          -3.00598145e-03, -7.51495361e-04,  1.51062012e-03],\n",
      "         [ 1.24359131e-03,  2.15148926e-03, -1.06048584e-03, ...,\n",
      "          -1.27410889e-03,  1.96838379e-03,  7.40051270e-04],\n",
      "         [-8.96453857e-04, -6.59942627e-04, -4.44412231e-04, ...,\n",
      "           3.49426270e-03, -2.12669373e-04,  1.80053711e-03],\n",
      "         ...,\n",
      "         [ 2.42614746e-03, -8.69750977e-04, -1.31988525e-03, ...,\n",
      "           1.44958496e-03, -2.82287598e-04,  6.46972656e-03],\n",
      "         [ 3.02124023e-03, -2.67028809e-03,  3.05175781e-05, ...,\n",
      "          -2.74658203e-03,  1.20544434e-03, -1.64794922e-03],\n",
      "         [ 1.22070312e-03,  6.94274902e-04, -1.44004822e-04, ...,\n",
      "           1.42669678e-03, -1.71661377e-03, -4.85229492e-03]]],\n",
      "\n",
      "\n",
      "       [[[-1.30462646e-03, -3.83377075e-04,  1.11389160e-03, ...,\n",
      "           3.18527222e-04, -1.05285645e-03, -5.43594360e-05],\n",
      "         [ 8.96453857e-04,  1.12152100e-03,  1.64985657e-04, ...,\n",
      "           3.75747681e-04, -5.68389893e-04,  4.92095947e-04],\n",
      "         [ 7.29560852e-05,  3.14712524e-05, -1.16825104e-04, ...,\n",
      "          -3.14712524e-05,  4.45842743e-05,  5.79357147e-05],\n",
      "         ...,\n",
      "         [-1.18255615e-03, -1.50203705e-05, -7.51495361e-04, ...,\n",
      "          -2.47192383e-03, -8.81195068e-04, -2.44140625e-03],\n",
      "         [-4.88758087e-05,  3.75747681e-04, -1.31988525e-03, ...,\n",
      "          -6.40869141e-04,  7.43865967e-04,  7.28607178e-04],\n",
      "         [-1.95503235e-04, -2.80141830e-05, -1.14440918e-04, ...,\n",
      "           9.61303711e-04,  9.00268555e-04, -5.79833984e-04]],\n",
      "\n",
      "        [[ 6.44683838e-04, -7.28607178e-04,  2.86865234e-03, ...,\n",
      "           2.45666504e-03,  3.22341919e-04,  4.92095947e-04],\n",
      "         [ 4.99725342e-04,  6.37054443e-04,  2.67028809e-03, ...,\n",
      "           1.06811523e-03, -4.07695770e-05,  4.11987305e-04],\n",
      "         [ 7.55310059e-04, -1.45721436e-03,  9.76562500e-04, ...,\n",
      "          -1.78337097e-04,  6.33239746e-04, -5.49316406e-04],\n",
      "         ...,\n",
      "         [-2.82287598e-04,  1.53350830e-03,  6.29425049e-04, ...,\n",
      "          -4.92095947e-04, -5.18798828e-04, -5.18798828e-04],\n",
      "         [ 1.73950195e-03,  6.37054443e-04, -4.95910645e-04, ...,\n",
      "           3.73840332e-03, -3.31401825e-05,  4.69207764e-04],\n",
      "         [ 1.05381012e-04, -3.14712524e-04,  1.09863281e-03, ...,\n",
      "           1.60217285e-04, -3.33786011e-04,  4.23431396e-04]],\n",
      "\n",
      "        [[-8.04901123e-04,  9.46044922e-04,  2.91442871e-03, ...,\n",
      "           3.86047363e-03, -2.91442871e-03,  2.63977051e-03],\n",
      "         [ 2.18200684e-03,  5.30242920e-04,  1.44958496e-03, ...,\n",
      "          -1.85012817e-04, -1.85012817e-04, -6.67572021e-04],\n",
      "         [-1.15203857e-03,  1.55639648e-03,  8.20159912e-04, ...,\n",
      "          -1.38854980e-03,  5.52368164e-03, -6.44683838e-04],\n",
      "         ...,\n",
      "         [-1.29699707e-03,  1.68609619e-03, -3.81469727e-05, ...,\n",
      "          -2.12669373e-04,  3.40270996e-03,  1.54113770e-03],\n",
      "         [-2.24113464e-04, -8.69750977e-04,  1.38092041e-03, ...,\n",
      "          -9.76562500e-04,  9.61303711e-04,  5.40161133e-03],\n",
      "         [-1.41143799e-03,  4.11987305e-03,  4.17709351e-04, ...,\n",
      "           1.19018555e-03, -8.23974609e-04, -1.77001953e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.02722168e-04, -1.86920166e-03,  3.00598145e-03, ...,\n",
      "          -1.19781494e-03,  4.48608398e-03,  1.55639648e-03],\n",
      "         [-3.32641602e-03,  4.27246094e-03,  2.77709961e-03, ...,\n",
      "          -1.43432617e-03,  1.72424316e-03,  2.18200684e-03],\n",
      "         [-4.55856323e-04, -1.27792358e-04, -6.82830811e-04, ...,\n",
      "          -9.61303711e-04, -9.84191895e-04,  8.77380371e-05],\n",
      "         ...,\n",
      "         [ 5.03540039e-04,  1.31988525e-03,  6.25610352e-04, ...,\n",
      "          -1.54113770e-03,  1.12915039e-03, -1.09672546e-05],\n",
      "         [ 4.17709351e-04,  1.64794922e-03,  2.92968750e-03, ...,\n",
      "           2.96020508e-03, -8.20159912e-04, -1.94549561e-03],\n",
      "         [ 1.22833252e-03,  7.24792480e-04,  3.91006470e-04, ...,\n",
      "           5.31005859e-03, -2.07519531e-03, -3.64685059e-03]],\n",
      "\n",
      "        [[-1.24359131e-03,  1.02233887e-03, -1.94549561e-03, ...,\n",
      "          -2.84194946e-04, -4.22000885e-05,  6.14166260e-04],\n",
      "         [ 5.95092773e-04, -2.25830078e-03,  5.76019287e-04, ...,\n",
      "          -4.02832031e-03, -1.17492676e-03,  3.62396240e-04],\n",
      "         [ 5.03540039e-04,  1.00708008e-03, -3.91006470e-04, ...,\n",
      "          -4.08935547e-03, -7.00950623e-05,  1.43051147e-04],\n",
      "         ...,\n",
      "         [-1.32918358e-05, -1.75476074e-04,  1.71661377e-03, ...,\n",
      "          -7.58171082e-05,  4.82177734e-03, -3.11279297e-03],\n",
      "         [ 1.06048584e-03, -3.92913818e-04, -9.76562500e-04, ...,\n",
      "           2.30407715e-03,  3.47137451e-04, -1.05285645e-03],\n",
      "         [-6.17980957e-04,  7.01904297e-04, -8.73565674e-04, ...,\n",
      "          -3.58581543e-04, -1.74713135e-03, -1.19781494e-03]],\n",
      "\n",
      "        [[ 1.17492676e-03, -1.89971924e-03,  1.76239014e-03, ...,\n",
      "          -1.50299072e-03,  2.09045410e-03,  1.73568726e-04],\n",
      "         [ 9.00268555e-04,  7.13348389e-04, -6.79016113e-04, ...,\n",
      "           1.37329102e-03, -1.38854980e-03,  3.40270996e-03],\n",
      "         [-3.22341919e-04,  1.06811523e-03, -9.38415527e-04, ...,\n",
      "           6.04248047e-03, -4.39453125e-03, -2.86865234e-03],\n",
      "         ...,\n",
      "         [-4.92095947e-04,  1.89971924e-03,  1.35803223e-03, ...,\n",
      "           1.96838379e-03, -2.30407715e-03,  1.05285645e-03],\n",
      "         [ 4.63485718e-04, -2.10571289e-03,  1.07574463e-03, ...,\n",
      "           7.51495361e-04, -8.31604004e-04,  1.58691406e-03],\n",
      "         [-1.70707703e-04,  2.85339355e-03, -5.83648682e-04, ...,\n",
      "          -6.67572021e-04,  3.52859497e-04, -7.28607178e-04]]],\n",
      "\n",
      "\n",
      "       [[[-1.94549561e-04,  2.46047974e-04,  3.98159027e-05, ...,\n",
      "          -3.01361084e-04,  2.37464905e-04, -1.88827515e-04],\n",
      "         [ 4.73022461e-04,  3.50952148e-04,  1.22070312e-04, ...,\n",
      "          -2.47955322e-04,  1.32560730e-04, -1.30653381e-04],\n",
      "         [-2.64644623e-05, -1.34706497e-05,  3.05175781e-05, ...,\n",
      "           3.05175781e-04,  2.93731689e-04,  2.28881836e-04],\n",
      "         ...,\n",
      "         [-1.32560730e-04, -3.24249268e-04, -5.53131104e-04, ...,\n",
      "           4.22000885e-05, -2.72750854e-04, -4.52995300e-05],\n",
      "         [ 1.05857849e-04,  4.26769257e-05, -2.04086304e-04, ...,\n",
      "          -1.27792358e-04, -5.14984131e-05, -7.43865967e-05],\n",
      "         [-2.51531601e-05,  3.76701355e-05, -3.48091125e-05, ...,\n",
      "          -3.05175781e-05,  2.84194946e-04,  1.62124634e-04]],\n",
      "\n",
      "        [[-3.37219238e-03,  7.01904297e-04,  1.81579590e-03, ...,\n",
      "           5.64575195e-04,  2.33459473e-03, -3.39508057e-04],\n",
      "         [ 1.48773193e-03, -1.94311142e-05, -2.10571289e-03, ...,\n",
      "          -7.40051270e-04, -5.53131104e-05, -4.40597534e-04],\n",
      "         [-8.34465027e-05, -2.05993652e-03, -3.14712524e-04, ...,\n",
      "           2.65502930e-03,  3.62396240e-04,  2.71606445e-03],\n",
      "         ...,\n",
      "         [-4.76837158e-04,  3.71932983e-04, -9.11712646e-04, ...,\n",
      "           3.52859497e-04, -1.18255615e-03, -1.61743164e-03],\n",
      "         [-1.24359131e-03,  1.41143799e-03,  2.63452530e-05, ...,\n",
      "           1.71661377e-03, -3.22341919e-04, -9.00268555e-04],\n",
      "         [-2.09045410e-03,  4.53948975e-04, -7.00950623e-05, ...,\n",
      "           3.18527222e-04, -3.70025635e-04,  5.91278076e-04]],\n",
      "\n",
      "        [[ 6.79016113e-04, -3.29589844e-03, -2.27355957e-03, ...,\n",
      "          -2.31742859e-04,  3.32641602e-03, -1.51824951e-03],\n",
      "         [ 1.71661377e-04,  3.31878662e-04,  5.87463379e-04, ...,\n",
      "           1.71661377e-04, -1.64985657e-04,  1.13964081e-04],\n",
      "         [ 1.38854980e-03, -3.52478027e-03,  1.71661377e-04, ...,\n",
      "          -7.55310059e-04,  2.42614746e-03, -1.96838379e-03],\n",
      "         ...,\n",
      "         [-1.53350830e-03, -3.62396240e-04,  3.96728516e-03, ...,\n",
      "           2.96020508e-03, -2.56347656e-03,  2.59399414e-04],\n",
      "         [-1.15203857e-03, -2.12669373e-04, -9.00268555e-04, ...,\n",
      "           3.11279297e-03,  6.43920898e-03,  1.90734863e-03],\n",
      "         [-2.89916992e-03,  3.20434570e-04,  1.15966797e-03, ...,\n",
      "          -1.99890137e-03,  1.53541565e-04,  2.22778320e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.82559204e-04, -2.27355957e-03, -1.15871429e-04, ...,\n",
      "          -6.10351562e-04, -2.36511230e-03,  8.82148743e-05],\n",
      "         [ 2.59399414e-04,  1.13677979e-03, -6.63757324e-04, ...,\n",
      "          -3.38745117e-03, -3.37600708e-04, -2.86865234e-03],\n",
      "         [-8.85009766e-04,  4.11987305e-04, -3.52859497e-05, ...,\n",
      "          -4.99725342e-04, -4.18090820e-03, -4.18090820e-03],\n",
      "         ...,\n",
      "         [ 4.99725342e-04, -1.23596191e-03, -3.38745117e-03, ...,\n",
      "          -1.64794922e-03,  1.35803223e-03, -3.24249268e-04],\n",
      "         [-1.84631348e-03,  7.85827637e-04, -7.59124756e-04, ...,\n",
      "          -1.00708008e-03, -1.87683105e-03, -8.73565674e-04],\n",
      "         [-7.40051270e-04,  1.69372559e-03,  2.82287598e-03, ...,\n",
      "          -3.02124023e-03,  1.74713135e-03, -9.76562500e-04]],\n",
      "\n",
      "        [[-1.18732452e-04,  9.72747803e-04,  4.08935547e-03, ...,\n",
      "           1.92260742e-03, -2.85339355e-03,  1.96838379e-03],\n",
      "         [-1.42669678e-03,  1.51062012e-03, -1.09863281e-03, ...,\n",
      "           3.14331055e-03, -2.27355957e-03, -4.15802002e-04],\n",
      "         [ 1.06811523e-03,  7.82012939e-04,  9.65118408e-04, ...,\n",
      "           1.21307373e-03,  1.66320801e-03, -8.88824463e-04],\n",
      "         ...,\n",
      "         [-9.46044922e-04,  1.02233887e-03,  2.37464905e-04, ...,\n",
      "          -2.27355957e-03, -1.48773193e-03, -1.17301941e-04],\n",
      "         [-7.13348389e-04,  8.62121582e-04, -1.44958496e-03, ...,\n",
      "           1.19018555e-03, -5.11169434e-04, -2.39562988e-03],\n",
      "         [-2.08854675e-04,  5.93662262e-05, -1.82342529e-03, ...,\n",
      "          -3.05175781e-04, -1.42669678e-03,  3.52478027e-03]],\n",
      "\n",
      "        [[ 1.87683105e-03, -2.97546387e-03,  1.50299072e-03, ...,\n",
      "           8.35418701e-04,  2.21252441e-03, -8.96453857e-04],\n",
      "         [ 6.29425049e-04,  6.02722168e-04, -1.81579590e-03, ...,\n",
      "          -1.87683105e-03,  2.34985352e-03, -1.53541565e-04],\n",
      "         [-4.48226929e-04, -8.46862793e-04,  2.34603882e-04, ...,\n",
      "           2.22778320e-03, -7.78198242e-04, -3.93676758e-03],\n",
      "         ...,\n",
      "         [-4.82559204e-04,  1.73950195e-03,  1.11389160e-03, ...,\n",
      "           5.18798828e-04, -3.93676758e-03, -2.65121460e-04],\n",
      "         [-1.23596191e-03,  3.79562378e-04,  2.33459473e-03, ...,\n",
      "           2.96020508e-03,  2.70843506e-04,  2.93731689e-04],\n",
      "         [ 1.20544434e-03,  1.22833252e-03, -5.76019287e-04, ...,\n",
      "          -1.83105469e-03, -8.62121582e-04,  2.67028809e-03]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 3.92913818e-04, -8.23974609e-04,  5.56945801e-04, ...,\n",
      "           6.14166260e-04, -4.44412231e-04,  1.81198120e-04],\n",
      "         [ 3.20434570e-04,  2.05039978e-04,  7.12275505e-06, ...,\n",
      "          -8.58306885e-04,  8.69750977e-04, -9.38415527e-04],\n",
      "         [-1.71661377e-04, -4.23431396e-04, -2.68936157e-04, ...,\n",
      "          -8.96453857e-04, -8.88824463e-04, -8.31604004e-04],\n",
      "         ...,\n",
      "         [-3.92913818e-04,  1.29699707e-03,  1.20544434e-03, ...,\n",
      "           2.80761719e-03, -3.50952148e-04,  4.15802002e-04],\n",
      "         [ 1.79290771e-04,  5.03540039e-04, -6.21795654e-04, ...,\n",
      "          -4.29153442e-05,  1.12056732e-04,  1.60217285e-04],\n",
      "         [ 1.63078308e-04, -2.36511230e-04, -3.86238098e-05, ...,\n",
      "           8.62121582e-04, -2.96020508e-03,  1.59454346e-03]],\n",
      "\n",
      "        [[-2.12669373e-04, -7.32421875e-04, -9.59634781e-06, ...,\n",
      "           1.50299072e-03,  1.81579590e-03, -1.13677979e-03],\n",
      "         [ 9.34600830e-04, -1.15203857e-03,  1.80053711e-03, ...,\n",
      "          -5.49316406e-04,  1.22833252e-03,  7.28607178e-04],\n",
      "         [ 1.25122070e-03, -1.01470947e-03, -1.20544434e-03, ...,\n",
      "           1.13964081e-04, -7.24792480e-04,  2.84194946e-04],\n",
      "         ...,\n",
      "         [-4.74929810e-04,  2.74658203e-04, -4.86373901e-04, ...,\n",
      "           6.67572021e-04, -4.92095947e-04,  1.06048584e-03],\n",
      "         [ 7.51495361e-04,  2.89916992e-04,  3.47137451e-04, ...,\n",
      "           3.43322754e-03, -1.05857849e-04,  3.93676758e-03],\n",
      "         [ 3.89099121e-03, -9.84191895e-04, -9.49859619e-04, ...,\n",
      "          -7.66754150e-04,  7.78198242e-04, -6.94274902e-04]],\n",
      "\n",
      "        [[-9.00268555e-04, -3.21960449e-03, -4.73022461e-04, ...,\n",
      "          -3.32641602e-03, -5.70678711e-03,  8.31604004e-04],\n",
      "         [-5.49316406e-04,  1.56402588e-04,  2.21252441e-03, ...,\n",
      "           1.77001953e-03, -4.61578369e-04,  1.13677979e-03],\n",
      "         [-1.66893005e-04,  4.48226929e-04, -2.27355957e-03, ...,\n",
      "           6.21795654e-04, -5.37872314e-04, -3.29971313e-04],\n",
      "         ...,\n",
      "         [-2.92968750e-03,  3.93676758e-03, -1.83105469e-03, ...,\n",
      "           3.58581543e-03, -2.07901001e-04, -3.43322754e-03],\n",
      "         [-4.74929810e-04, -2.02941895e-03,  3.52859497e-04, ...,\n",
      "           3.01361084e-04,  9.23156738e-04,  2.38037109e-03],\n",
      "         [ 3.84521484e-03,  6.79016113e-04,  3.03649902e-03, ...,\n",
      "          -6.02722168e-04, -1.04427338e-04,  5.52368164e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.94297934e-07, -1.15203857e-03, -6.82473183e-06, ...,\n",
      "           3.06701660e-03,  2.45666504e-03, -4.80651855e-04],\n",
      "         [-1.46484375e-03,  5.76019287e-04, -3.07083130e-04, ...,\n",
      "          -2.59399414e-03,  3.23486328e-03, -1.18255615e-03],\n",
      "         [ 1.51062012e-03,  2.19726562e-03, -4.42504883e-04, ...,\n",
      "           6.29425049e-04, -1.80053711e-03, -1.50680542e-04],\n",
      "         ...,\n",
      "         [ 1.37329102e-03, -7.17163086e-04,  1.48773193e-03, ...,\n",
      "          -1.64031982e-03,  2.86865234e-03, -9.84191895e-04],\n",
      "         [-7.97271729e-04,  3.50952148e-04, -2.25830078e-03, ...,\n",
      "          -2.19726562e-03, -1.13677979e-03, -5.26428223e-04],\n",
      "         [-9.84191895e-04, -1.19209290e-04, -1.95312500e-03, ...,\n",
      "           8.58306885e-04, -7.59124756e-04,  1.02233887e-03]],\n",
      "\n",
      "        [[-2.78472900e-04, -2.28881836e-04,  1.42669678e-03, ...,\n",
      "          -5.73730469e-03, -3.28063965e-03,  1.28173828e-03],\n",
      "         [-1.99317932e-04,  1.72424316e-03, -8.48770142e-05, ...,\n",
      "          -3.49426270e-03, -2.10571289e-03,  5.37872314e-04],\n",
      "         [ 1.89208984e-03, -6.02722168e-04, -1.15966797e-03, ...,\n",
      "           2.97546387e-04, -2.30407715e-03, -5.83648682e-04],\n",
      "         ...,\n",
      "         [ 2.27355957e-03,  3.64780426e-05,  1.28936768e-03, ...,\n",
      "          -9.07897949e-04,  4.11987305e-04, -9.49859619e-04],\n",
      "         [ 1.44958496e-03, -1.43432617e-03, -3.26538086e-03, ...,\n",
      "          -2.47192383e-03,  6.14166260e-04, -1.90734863e-03],\n",
      "         [-1.64794922e-03, -1.11389160e-03,  9.72747803e-04, ...,\n",
      "           1.28173828e-03,  1.58691406e-03, -6.40869141e-04]],\n",
      "\n",
      "        [[ 1.48773193e-03,  2.30789185e-04,  1.47819519e-04, ...,\n",
      "           3.52859497e-04, -1.63269043e-03, -1.99890137e-03],\n",
      "         [ 5.11169434e-04,  1.27410889e-03, -2.46047974e-04, ...,\n",
      "          -8.69750977e-04, -1.03759766e-03,  2.12097168e-03],\n",
      "         [-2.21252441e-04, -1.48773193e-03,  2.45666504e-03, ...,\n",
      "          -3.02124023e-03,  2.16674805e-03,  1.25885010e-03],\n",
      "         ...,\n",
      "         [-9.00268555e-04,  5.22613525e-04,  2.78472900e-04, ...,\n",
      "          -6.21795654e-04,  4.79125977e-03, -1.64794922e-03],\n",
      "         [ 1.00708008e-03, -2.67028809e-03, -4.76837158e-04, ...,\n",
      "           8.81195068e-04,  9.53674316e-04,  1.95312500e-03],\n",
      "         [-1.43432617e-03,  9.49859619e-04, -3.84521484e-03, ...,\n",
      "          -1.11389160e-03, -1.82151794e-04,  3.63159180e-03]]],\n",
      "\n",
      "\n",
      "       [[[ 1.50680542e-04, -1.00708008e-03,  1.66320801e-03, ...,\n",
      "          -6.21795654e-04,  7.43865967e-04, -1.20162964e-04],\n",
      "         [-6.59942627e-04, -1.94549561e-04, -1.34468079e-04, ...,\n",
      "          -1.03759766e-03,  2.80380249e-04, -8.46862793e-04],\n",
      "         [ 1.64985657e-04,  1.18255615e-04, -2.15530396e-04, ...,\n",
      "          -6.94274902e-04, -7.32421875e-04, -8.58306885e-04],\n",
      "         ...,\n",
      "         [-7.17163086e-04, -2.44140625e-04, -8.35418701e-04, ...,\n",
      "          -1.75476074e-04, -1.45721436e-03, -1.36566162e-03],\n",
      "         [ 1.46865845e-04,  3.01361084e-04, -4.08172607e-04, ...,\n",
      "          -7.29560852e-05,  3.50952148e-04,  3.43322754e-04],\n",
      "         [-1.91688538e-04,  2.20298767e-04,  4.69684601e-05, ...,\n",
      "          -2.63977051e-03, -1.38092041e-03,  1.46484375e-03]],\n",
      "\n",
      "        [[-5.18798828e-03, -4.30297852e-03, -6.59179688e-03, ...,\n",
      "           6.79016113e-04,  1.09863281e-03, -1.03759766e-03],\n",
      "         [ 7.28607178e-04, -9.48905945e-05, -2.20298767e-04, ...,\n",
      "          -2.26974487e-04, -5.43594360e-05, -3.16619873e-04],\n",
      "         [ 8.46862793e-04, -7.78198242e-04,  5.07354736e-04, ...,\n",
      "           2.07519531e-03,  2.44140625e-03, -5.64575195e-04],\n",
      "         ...,\n",
      "         [-6.79016113e-04,  2.02178955e-04,  6.67572021e-04, ...,\n",
      "          -4.63485718e-04,  8.08715820e-04, -1.28173828e-03],\n",
      "         [ 6.10351562e-04, -9.38415527e-04, -1.49726868e-04, ...,\n",
      "           2.61306763e-04,  2.78472900e-04, -1.60980225e-03],\n",
      "         [-5.37872314e-04, -6.21795654e-04,  1.83105469e-03, ...,\n",
      "          -3.45230103e-04,  4.57763672e-04, -4.45842743e-05]],\n",
      "\n",
      "        [[-1.64794922e-03, -1.57165527e-03, -1.77764893e-03, ...,\n",
      "           3.43322754e-03,  4.02832031e-03, -2.02941895e-03],\n",
      "         [ 8.50677490e-04,  9.72747803e-04,  1.36566162e-03, ...,\n",
      "          -1.51062012e-03, -6.43730164e-05,  2.20298767e-04],\n",
      "         [ 1.24359131e-03,  1.09863281e-03, -2.97546387e-04, ...,\n",
      "           2.76184082e-03, -2.78472900e-04, -8.88824463e-04],\n",
      "         ...,\n",
      "         [-1.27410889e-03, -2.19726562e-03, -1.09863281e-03, ...,\n",
      "           6.21795654e-04, -6.44683838e-04, -2.09045410e-03],\n",
      "         [ 1.19018555e-03, -9.84191895e-04, -1.46484375e-03, ...,\n",
      "          -1.72424316e-03,  5.30242920e-04,  3.79943848e-03],\n",
      "         [ 2.01416016e-03,  3.84521484e-03, -1.64031982e-03, ...,\n",
      "          -2.67028809e-03, -1.38092041e-03, -4.15039062e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.70025635e-04,  6.07967377e-05, -7.93457031e-04, ...,\n",
      "           4.73022461e-04,  1.26647949e-03,  3.79562378e-04],\n",
      "         [-1.15203857e-03,  9.00268555e-04, -1.46484375e-03, ...,\n",
      "           6.48498535e-04, -2.45666504e-03,  9.46044922e-04],\n",
      "         [ 4.11987305e-04,  1.96456909e-04,  1.11389160e-03, ...,\n",
      "          -3.63159180e-03, -3.40270996e-03,  2.80761719e-03],\n",
      "         ...,\n",
      "         [-1.06811523e-03,  1.02996826e-03, -1.09863281e-03, ...,\n",
      "           1.87683105e-03,  2.60925293e-03, -1.28173828e-03],\n",
      "         [-3.14712524e-04,  1.29699707e-03,  6.79016113e-04, ...,\n",
      "          -3.23486328e-03,  9.61303711e-04,  3.08990479e-04],\n",
      "         [ 5.62667847e-05, -2.05993652e-03,  1.59454346e-03, ...,\n",
      "          -8.85009766e-04, -1.96838379e-03,  6.10351562e-04]],\n",
      "\n",
      "        [[-4.50134277e-04, -8.43048096e-04,  3.70025635e-04, ...,\n",
      "          -1.15203857e-03, -9.11712646e-04,  2.22778320e-03],\n",
      "         [ 6.29425049e-04,  2.10571289e-03,  1.32751465e-03, ...,\n",
      "          -6.14166260e-04,  6.90460205e-04, -3.64685059e-03],\n",
      "         [ 2.21252441e-03, -8.85009766e-04,  7.13348389e-04, ...,\n",
      "           7.28607178e-04, -1.66320801e-03, -1.57356262e-04],\n",
      "         ...,\n",
      "         [-3.73840332e-03, -1.48010254e-03,  1.12915039e-03, ...,\n",
      "          -8.35418701e-04,  2.12097168e-03,  4.55856323e-04],\n",
      "         [ 3.06701660e-03,  1.89971924e-03, -1.57356262e-05, ...,\n",
      "          -1.70135498e-03,  3.54003906e-03, -3.17382812e-03],\n",
      "         [-3.41415405e-04,  1.67083740e-03, -3.50475311e-05, ...,\n",
      "          -8.88824463e-04,  9.34600830e-04, -5.27501106e-06]],\n",
      "\n",
      "        [[-7.00950623e-05, -2.97546387e-04,  2.33650208e-04, ...,\n",
      "           5.91278076e-04,  3.08227539e-03, -2.68554688e-03],\n",
      "         [ 2.04467773e-03, -6.90460205e-04,  3.41415405e-04, ...,\n",
      "           1.61743164e-03,  4.88281250e-04,  4.02832031e-03],\n",
      "         [ 2.72750854e-04, -1.71661377e-03,  7.55310059e-04, ...,\n",
      "          -4.08935547e-03,  1.35803223e-03,  3.58581543e-03],\n",
      "         ...,\n",
      "         [ 7.05718994e-05, -2.48718262e-03, -2.50244141e-03, ...,\n",
      "          -3.18908691e-03, -3.66210938e-03,  8.46862793e-04],\n",
      "         [ 1.10626221e-03,  9.15527344e-05, -3.24249268e-04, ...,\n",
      "          -3.14712524e-04,  2.47955322e-05,  2.22778320e-03],\n",
      "         [ 1.21307373e-03, -5.79833984e-04,  3.02124023e-03, ...,\n",
      "           2.13623047e-03,  1.07765198e-04,  1.07574463e-03]]],\n",
      "\n",
      "\n",
      "       [[[-3.20434570e-04,  6.71386719e-04, -2.76565552e-04, ...,\n",
      "          -6.94274902e-04,  1.16729736e-03, -9.23156738e-04],\n",
      "         [-8.01086426e-05, -2.41279602e-04,  1.82151794e-04, ...,\n",
      "          -4.02450562e-04,  7.85827637e-04, -4.86373901e-04],\n",
      "         [ 1.31607056e-04,  1.37329102e-04,  3.69548798e-05, ...,\n",
      "          -1.95312500e-03, -1.90734863e-03, -1.72424316e-03],\n",
      "         ...,\n",
      "         [-5.64575195e-04, -2.46047974e-04, -4.88281250e-04, ...,\n",
      "           8.85009766e-04,  9.72747803e-04,  1.96456909e-04],\n",
      "         [-3.83853912e-05, -3.16619873e-04,  2.02178955e-04, ...,\n",
      "           1.99317932e-04, -3.14712524e-05, -5.25265932e-07],\n",
      "         [-2.52723694e-05,  8.88109207e-06, -7.91549683e-05, ...,\n",
      "           8.58306885e-04, -1.09100342e-03, -7.28607178e-04]],\n",
      "\n",
      "        [[ 3.14331055e-03,  2.45666504e-03,  2.02941895e-03, ...,\n",
      "           4.97436523e-03,  2.15530396e-04, -6.40869141e-04],\n",
      "         [ 2.89916992e-03,  3.85284424e-04, -1.33514404e-03, ...,\n",
      "          -1.70135498e-03, -1.11389160e-03,  3.66210938e-04],\n",
      "         [ 3.79943848e-03, -1.70135498e-03, -5.49316406e-04, ...,\n",
      "           3.38745117e-03,  2.44140625e-04, -7.32421875e-04],\n",
      "         ...,\n",
      "         [ 8.23974609e-04,  9.10758972e-05, -1.23596191e-03, ...,\n",
      "           3.33786011e-04,  6.10351562e-04, -2.07901001e-04],\n",
      "         [-1.66320801e-03, -6.33239746e-04,  1.00708008e-03, ...,\n",
      "           1.71661377e-03, -1.21593475e-04,  9.26971436e-04],\n",
      "         [-2.33459473e-03,  1.54495239e-04, -3.92913818e-04, ...,\n",
      "          -5.34057617e-04,  5.45501709e-04, -9.84191895e-04]],\n",
      "\n",
      "        [[-6.86645508e-04,  2.39562988e-03,  1.86920166e-03, ...,\n",
      "          -8.08715820e-04, -2.80761719e-03, -1.09863281e-03],\n",
      "         [-8.91685486e-05,  1.22833252e-03, -2.82287598e-03, ...,\n",
      "          -9.07897949e-04,  5.84125519e-05,  4.50134277e-04],\n",
      "         [ 6.67572021e-04,  1.93023682e-03,  9.07897949e-04, ...,\n",
      "          -5.37109375e-03, -7.55310059e-04,  1.06048584e-03],\n",
      "         ...,\n",
      "         [ 6.59942627e-04, -1.83105469e-04, -1.15203857e-03, ...,\n",
      "          -2.34985352e-03,  2.38037109e-03,  5.45501709e-04],\n",
      "         [ 2.33459473e-03, -1.91688538e-04, -2.38037109e-03, ...,\n",
      "           5.45501709e-04,  6.82830811e-04, -5.03540039e-03],\n",
      "         [ 7.43865967e-04, -1.72424316e-03,  8.69750977e-04, ...,\n",
      "          -3.21960449e-03,  1.20544434e-03,  1.92642212e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.67300415e-04, -4.34875488e-04,  1.28936768e-03, ...,\n",
      "           2.01416016e-03,  2.91442871e-03, -4.21524048e-04],\n",
      "         [ 2.22778320e-03, -2.19726562e-03,  1.12152100e-03, ...,\n",
      "           2.63977051e-03,  1.35040283e-03, -1.57356262e-04],\n",
      "         [-7.43865967e-04, -5.56945801e-04, -9.46044922e-04, ...,\n",
      "           2.53295898e-03,  1.55639648e-03,  1.67083740e-03],\n",
      "         ...,\n",
      "         [ 1.31607056e-04, -9.07897949e-04, -1.45721436e-03, ...,\n",
      "          -2.12669373e-04,  1.86920166e-03,  5.64575195e-04],\n",
      "         [ 1.00708008e-03, -6.48498535e-04,  3.79562378e-04, ...,\n",
      "           4.08935547e-03,  1.29699707e-03,  3.11279297e-03],\n",
      "         [ 1.93023682e-03,  3.21960449e-03, -2.27928162e-04, ...,\n",
      "          -2.86865234e-03,  1.94549561e-03, -3.52478027e-03]],\n",
      "\n",
      "        [[-2.10762024e-04, -1.67083740e-03, -2.77709961e-03, ...,\n",
      "          -9.46044922e-04,  9.99450684e-04, -3.26538086e-03],\n",
      "         [ 1.02519989e-04, -1.07288361e-04, -2.80380249e-04, ...,\n",
      "          -2.49862671e-04,  2.30407715e-03, -2.31933594e-03],\n",
      "         [ 6.02722168e-04, -4.99725342e-04,  2.22778320e-03, ...,\n",
      "          -7.59124756e-04, -8.96453857e-04, -1.15203857e-03],\n",
      "         ...,\n",
      "         [ 3.18908691e-03,  2.92968750e-03, -1.22070312e-03, ...,\n",
      "          -4.02832031e-03, -3.06701660e-03,  4.97436523e-03],\n",
      "         [-1.17492676e-03, -1.74713135e-03,  2.42614746e-03, ...,\n",
      "           4.57763672e-03,  2.33459473e-03,  3.60107422e-03],\n",
      "         [ 8.62121582e-04, -2.28881836e-03, -7.05718994e-04, ...,\n",
      "           4.10079956e-04, -2.86865234e-03, -5.18798828e-04]],\n",
      "\n",
      "        [[-1.85394287e-03, -6.48498535e-04,  1.53541565e-04, ...,\n",
      "          -1.18017197e-05,  3.24249268e-04, -7.01904297e-04],\n",
      "         [ 6.10947609e-06,  3.98635864e-04, -2.54821777e-03, ...,\n",
      "          -7.17163086e-04,  8.50677490e-04, -1.50299072e-03],\n",
      "         [-2.55584717e-04,  6.21795654e-04,  1.42097473e-04, ...,\n",
      "          -7.05718994e-04,  9.84191895e-04, -6.33239746e-04],\n",
      "         ...,\n",
      "         [ 8.31604004e-04,  1.22070312e-03,  1.33514404e-03, ...,\n",
      "           7.43865967e-04,  6.14166260e-04,  1.19781494e-03],\n",
      "         [ 1.27410889e-03,  1.96838379e-03,  6.72340393e-05, ...,\n",
      "           1.99890137e-03,  5.85937500e-03,  1.28936768e-03],\n",
      "         [-1.00708008e-03,  6.25610352e-04,  1.83105469e-03, ...,\n",
      "          -2.07519531e-03,  3.01361084e-04,  2.65502930e-03]]]],      dtype=float32)}, 'value': {'kernel': Array([[[[ 7.85827637e-04, -6.86645508e-03,  1.82342529e-03, ...,\n",
      "          -8.17871094e-03,  1.12304688e-02,  8.34465027e-05],\n",
      "         [-1.60217285e-03, -4.94384766e-03, -4.42504883e-03, ...,\n",
      "           6.13403320e-03, -2.24304199e-03, -8.11767578e-03],\n",
      "         [ 1.25122070e-02, -1.23596191e-03,  2.59399414e-03, ...,\n",
      "          -1.00708008e-02,  1.59454346e-03, -1.08337402e-03],\n",
      "         ...,\n",
      "         [ 1.85546875e-02,  2.66113281e-02,  1.67846680e-03, ...,\n",
      "           6.10351562e-03, -2.55126953e-02,  1.67236328e-02],\n",
      "         [-2.94494629e-03,  1.00708008e-02,  4.76074219e-03, ...,\n",
      "          -5.40161133e-03, -1.11694336e-02, -5.24902344e-03],\n",
      "         [ 1.08032227e-02,  6.22558594e-03,  7.32421875e-03, ...,\n",
      "          -6.31713867e-03,  3.05175781e-03,  1.35421753e-04]],\n",
      "\n",
      "        [[ 6.83593750e-03, -3.55529785e-03,  5.92041016e-03, ...,\n",
      "           1.58691406e-03, -6.07299805e-03, -3.38745117e-03],\n",
      "         [ 1.12915039e-03, -3.90625000e-03, -2.51770020e-03, ...,\n",
      "          -1.99890137e-03, -4.91333008e-03,  2.18200684e-03],\n",
      "         [ 5.00488281e-03,  4.94384766e-03, -7.14111328e-03, ...,\n",
      "          -1.87683105e-03,  9.70458984e-03,  3.96728516e-03],\n",
      "         ...,\n",
      "         [ 9.11712646e-04,  3.43322754e-03,  2.89916992e-03, ...,\n",
      "          -7.32421875e-03,  1.92260742e-03, -3.06701660e-03],\n",
      "         [ 1.11694336e-02,  3.20434570e-03,  1.08642578e-02, ...,\n",
      "          -1.68457031e-02,  5.88989258e-03,  8.96453857e-04],\n",
      "         [ 5.64575195e-03, -3.11279297e-03, -9.04083252e-04, ...,\n",
      "          -5.27954102e-03, -6.28662109e-03, -1.02233887e-03]],\n",
      "\n",
      "        [[-4.54711914e-03,  2.83813477e-03, -7.01904297e-04, ...,\n",
      "           5.70678711e-03,  2.94189453e-02,  9.82666016e-03],\n",
      "         [ 2.35595703e-02,  7.81250000e-03, -1.96533203e-02, ...,\n",
      "          -2.22167969e-02,  1.39770508e-02, -9.39941406e-03],\n",
      "         [ 2.53295898e-03, -1.09863281e-03, -9.76562500e-03, ...,\n",
      "          -2.79235840e-03, -1.55029297e-02, -9.09423828e-03],\n",
      "         ...,\n",
      "         [-8.27789307e-04,  1.31225586e-02, -2.10571289e-03, ...,\n",
      "           8.11767578e-03,  4.94384766e-03,  1.10473633e-02],\n",
      "         [ 1.43051147e-04,  4.18090820e-03,  2.14843750e-02, ...,\n",
      "           1.42822266e-02,  4.51660156e-03, -2.64892578e-02],\n",
      "         [-1.30004883e-02,  2.10571289e-03, -2.31933594e-03, ...,\n",
      "           3.70025635e-04, -3.66210938e-02, -1.22070312e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.18408203e-02, -3.03649902e-03, -1.44042969e-02, ...,\n",
      "           2.28271484e-02, -1.39770508e-02, -5.46875000e-02],\n",
      "         [ 1.72424316e-03,  4.36401367e-03, -4.21142578e-03, ...,\n",
      "          -1.25122070e-02, -4.78515625e-02, -3.35693359e-03],\n",
      "         [ 1.87683105e-03,  2.06298828e-02,  7.08007812e-03, ...,\n",
      "          -3.32031250e-02,  8.85009766e-03, -1.03759766e-02],\n",
      "         ...,\n",
      "         [ 3.52859497e-04,  3.09753418e-03, -1.91650391e-02, ...,\n",
      "          -1.19018555e-02, -9.52148438e-03,  3.67736816e-03],\n",
      "         [ 2.07519531e-02, -7.04956055e-03,  2.64892578e-02, ...,\n",
      "          -7.01904297e-03,  1.11083984e-02, -3.12500000e-02],\n",
      "         [-1.08642578e-02, -1.06811523e-02, -9.27734375e-03, ...,\n",
      "           8.72802734e-03,  4.36306000e-05,  2.10762024e-04]],\n",
      "\n",
      "        [[-2.41088867e-03,  1.48315430e-02, -1.84326172e-02, ...,\n",
      "           3.87573242e-03, -1.64794922e-02,  1.05285645e-03],\n",
      "         [ 6.19506836e-03,  1.40991211e-02, -1.68609619e-03, ...,\n",
      "           1.63574219e-02, -9.15527344e-03, -2.74658203e-02],\n",
      "         [ 1.37939453e-02,  6.53076172e-03,  6.14166260e-04, ...,\n",
      "          -1.95312500e-03, -2.29492188e-02, -1.74560547e-02],\n",
      "         ...,\n",
      "         [-7.62939453e-03,  6.82830811e-04,  1.96533203e-02, ...,\n",
      "          -5.12695312e-03,  7.69042969e-03, -3.63769531e-02],\n",
      "         [ 6.31713867e-03, -9.82666016e-03, -2.51770020e-03, ...,\n",
      "           2.18505859e-02,  4.39453125e-02,  1.02539062e-02],\n",
      "         [ 5.98907471e-04, -2.84423828e-02, -2.10571289e-03, ...,\n",
      "           1.86767578e-02, -1.83105469e-03, -1.09863281e-03]],\n",
      "\n",
      "        [[ 1.30004883e-02,  2.13623047e-02,  1.21459961e-02, ...,\n",
      "           1.14746094e-02, -5.24902344e-02,  1.23291016e-02],\n",
      "         [ 3.02734375e-02,  2.03857422e-02,  1.39160156e-02, ...,\n",
      "          -1.38549805e-02, -2.22167969e-02, -9.03320312e-03],\n",
      "         [ 7.35473633e-03,  1.28173828e-02, -7.72094727e-03, ...,\n",
      "           2.65502930e-03,  2.91442871e-03, -1.49536133e-02],\n",
      "         ...,\n",
      "         [ 2.49023438e-02,  2.44140625e-03, -1.68457031e-02, ...,\n",
      "          -1.06811523e-02,  1.44653320e-02,  1.64794922e-02],\n",
      "         [-2.88391113e-03, -5.34057617e-03,  1.26953125e-02, ...,\n",
      "          -9.76562500e-03, -3.99780273e-03, -2.49023438e-02],\n",
      "         [-5.46264648e-03,  4.34570312e-02, -1.05590820e-02, ...,\n",
      "          -2.83813477e-03,  1.09252930e-02, -1.17797852e-02]]],\n",
      "\n",
      "\n",
      "       [[[-5.95092773e-04, -4.57763672e-04,  9.64355469e-03, ...,\n",
      "          -4.82177734e-03,  2.13623047e-03, -1.00135803e-04],\n",
      "         [-3.07559967e-05, -3.79943848e-03,  3.43322754e-03, ...,\n",
      "           1.22070312e-02, -1.20239258e-02, -1.47247314e-03],\n",
      "         [ 6.77490234e-03, -8.23974609e-04,  6.62231445e-03, ...,\n",
      "          -2.71606445e-03,  1.21459961e-02, -5.52368164e-03],\n",
      "         ...,\n",
      "         [-2.22167969e-02, -8.30078125e-03, -9.03320312e-03, ...,\n",
      "           2.50244141e-03,  4.88281250e-02,  9.27734375e-03],\n",
      "         [-4.48608398e-03,  3.82995605e-03, -5.61523438e-03, ...,\n",
      "           7.23266602e-03,  2.42614746e-03,  1.13525391e-02],\n",
      "         [ 3.14331055e-03, -1.09863281e-02,  4.85229492e-03, ...,\n",
      "          -5.70678711e-03,  4.79125977e-03,  2.53295898e-03]],\n",
      "\n",
      "        [[-8.36181641e-03,  2.38037109e-03, -1.18408203e-02, ...,\n",
      "           4.76074219e-03,  1.49536133e-02,  6.89697266e-03],\n",
      "         [-1.78337097e-04, -2.53295898e-03,  1.16729736e-03, ...,\n",
      "          -1.84631348e-03, -3.08227539e-03, -8.81195068e-04],\n",
      "         [-8.97216797e-03, -2.82287598e-03, -1.97753906e-02, ...,\n",
      "           1.37329102e-02, -7.23266602e-03, -1.10473633e-02],\n",
      "         ...,\n",
      "         [ 3.75747681e-04,  7.47680664e-03,  2.41279602e-04, ...,\n",
      "          -5.46264648e-03,  9.27734375e-03, -7.26318359e-03],\n",
      "         [-3.75366211e-03,  2.38037109e-02,  7.32421875e-03, ...,\n",
      "           7.69042969e-03, -1.44004822e-04,  1.18255615e-03],\n",
      "         [-8.11767578e-03,  5.92041016e-03, -1.06811523e-03, ...,\n",
      "           2.70080566e-03, -5.58471680e-03,  1.64031982e-03]],\n",
      "\n",
      "        [[ 7.66754150e-04,  1.40991211e-02,  1.51977539e-02, ...,\n",
      "           2.50244141e-03, -1.26953125e-02, -1.53808594e-02],\n",
      "         [ 2.75878906e-02,  1.64794922e-02, -2.47802734e-02, ...,\n",
      "          -1.62353516e-02,  1.12915039e-02,  5.21850586e-03],\n",
      "         [-6.71386719e-03, -6.07299805e-03,  1.18255615e-03, ...,\n",
      "           1.66015625e-02,  4.02832031e-03, -2.42919922e-02],\n",
      "         ...,\n",
      "         [ 1.53198242e-02, -1.02996826e-03,  1.42822266e-02, ...,\n",
      "           2.53906250e-02, -4.82177734e-03,  2.29492188e-02],\n",
      "         [-9.94873047e-03, -1.09252930e-02,  6.01196289e-03, ...,\n",
      "           7.44628906e-03, -1.55029297e-02,  1.08032227e-02],\n",
      "         [-7.17163086e-03, -2.77099609e-02,  1.34887695e-02, ...,\n",
      "          -3.06396484e-02, -6.40869141e-04,  1.68457031e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.03540039e-03,  2.41699219e-02,  5.73730469e-03, ...,\n",
      "          -2.78320312e-02, -2.90527344e-02, -2.92968750e-02],\n",
      "         [-1.83105469e-02, -1.23901367e-02, -1.13525391e-02, ...,\n",
      "          -2.57568359e-02,  9.58251953e-03,  3.46679688e-02],\n",
      "         [ 5.95092773e-03,  1.26342773e-02, -4.19921875e-02, ...,\n",
      "           3.15856934e-03, -1.85546875e-02,  1.06811523e-02],\n",
      "         ...,\n",
      "         [-7.11059570e-03, -3.28063965e-03, -3.75976562e-02, ...,\n",
      "          -9.39941406e-03, -2.83203125e-02,  1.72119141e-02],\n",
      "         [-3.60107422e-03, -5.37109375e-03, -2.50244141e-02, ...,\n",
      "          -1.47705078e-02, -7.14111328e-03, -3.90625000e-03],\n",
      "         [-2.44140625e-02,  2.12097168e-03, -3.01513672e-02, ...,\n",
      "           2.22167969e-02,  3.46679688e-02, -1.69677734e-02]],\n",
      "\n",
      "        [[-3.54003906e-02,  2.51464844e-02,  1.33056641e-02, ...,\n",
      "           2.60009766e-02, -4.97436523e-03,  3.83300781e-02],\n",
      "         [ 9.33837891e-03, -2.29492188e-02, -4.39453125e-02, ...,\n",
      "           2.00195312e-02, -4.15039062e-02, -2.80761719e-02],\n",
      "         [ 4.02832031e-02, -2.12402344e-02, -1.84326172e-02, ...,\n",
      "          -9.82666016e-03,  1.97753906e-02,  5.85937500e-03],\n",
      "         ...,\n",
      "         [ 1.14746094e-02,  1.25885010e-03, -3.10897827e-04, ...,\n",
      "          -9.21630859e-03, -2.36816406e-02, -2.57873535e-03],\n",
      "         [-2.27050781e-02,  3.34472656e-02,  3.50952148e-03, ...,\n",
      "           1.03149414e-02,  3.41415405e-04,  7.81250000e-03],\n",
      "         [-1.87988281e-02, -3.35693359e-03,  4.10156250e-02, ...,\n",
      "          -5.82885742e-03, -4.68750000e-02,  5.40161133e-03]],\n",
      "\n",
      "        [[-2.78320312e-02,  7.72094727e-03, -3.37219238e-03, ...,\n",
      "           1.35803223e-03, -1.44042969e-02, -1.95312500e-02],\n",
      "         [ 3.23486328e-03,  1.20849609e-02,  3.72314453e-03, ...,\n",
      "           2.13623047e-02, -2.09960938e-02,  1.27563477e-02],\n",
      "         [ 3.49426270e-03, -1.15966797e-02, -4.15039062e-03, ...,\n",
      "           1.01928711e-02, -3.11279297e-02, -9.15527344e-03],\n",
      "         ...,\n",
      "         [ 3.29589844e-02,  4.15039062e-02, -1.04370117e-02, ...,\n",
      "           3.66210938e-03, -1.90429688e-02,  3.70788574e-03],\n",
      "         [ 8.42285156e-03, -2.14843750e-02,  1.00097656e-02, ...,\n",
      "           3.89099121e-03, -2.80761719e-02,  2.31933594e-02],\n",
      "         [-3.22265625e-02, -4.98046875e-02, -3.10058594e-02, ...,\n",
      "          -8.48388672e-03, -2.41699219e-02, -2.55126953e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 1.88446045e-03, -7.72094727e-03,  9.99450684e-04, ...,\n",
      "           8.34465027e-05,  3.34167480e-03,  2.12097168e-03],\n",
      "         [ 1.28936768e-03, -5.00488281e-03,  3.87573242e-03, ...,\n",
      "           4.36401367e-03,  4.33349609e-03,  2.41088867e-03],\n",
      "         [-2.80761719e-03, -3.35693359e-03, -2.86865234e-03, ...,\n",
      "           2.34603882e-04, -4.11987305e-04,  3.81469727e-03],\n",
      "         ...,\n",
      "         [-8.48388672e-03, -1.59912109e-02, -1.50146484e-02, ...,\n",
      "           5.49316406e-03,  1.33666992e-02,  1.58691406e-02],\n",
      "         [-6.62231445e-03,  3.17382812e-03, -6.83593750e-03, ...,\n",
      "          -1.09863281e-02,  1.30462646e-03,  2.30407715e-03],\n",
      "         [-1.88827515e-04,  5.40161133e-03, -5.18798828e-03, ...,\n",
      "           1.02539062e-02, -9.76562500e-04,  5.55419922e-03]],\n",
      "\n",
      "        [[-4.08935547e-03, -2.39562988e-03,  1.08642578e-02, ...,\n",
      "          -2.05993652e-03,  5.09643555e-03, -4.05883789e-03],\n",
      "         [-3.09753418e-03,  2.05993652e-03, -2.04467773e-03, ...,\n",
      "          -2.18200684e-03,  3.35693359e-04,  7.20977783e-04],\n",
      "         [ 1.77001953e-02, -6.74438477e-03,  1.80664062e-02, ...,\n",
      "           2.67333984e-02, -6.83593750e-03, -1.31835938e-02],\n",
      "         ...,\n",
      "         [-3.69262695e-03,  1.00097656e-02, -3.25012207e-03, ...,\n",
      "          -6.16455078e-03,  1.06811523e-03,  4.05883789e-03],\n",
      "         [-5.58471680e-03,  2.52685547e-02,  1.03712082e-05, ...,\n",
      "           7.26318359e-03,  1.92260742e-03, -4.91333008e-03],\n",
      "         [ 3.92913818e-04, -1.60980225e-03,  2.99072266e-03, ...,\n",
      "           4.63867188e-03,  9.39941406e-03,  4.15039062e-03]],\n",
      "\n",
      "        [[ 2.12402344e-02, -2.18505859e-02, -1.26647949e-03, ...,\n",
      "           3.12500000e-02,  4.45556641e-03,  8.78906250e-03],\n",
      "         [ 6.50024414e-03,  2.97851562e-02,  1.26953125e-02, ...,\n",
      "          -2.36816406e-02,  4.95910645e-04,  4.08935547e-03],\n",
      "         [-2.41699219e-02, -7.56835938e-03, -2.14843750e-02, ...,\n",
      "           9.39941406e-03, -1.16729736e-03,  5.61523438e-03],\n",
      "         ...,\n",
      "         [-1.39160156e-02,  1.11083984e-02, -1.86767578e-02, ...,\n",
      "          -7.75146484e-03, -1.98974609e-02, -2.18505859e-02],\n",
      "         [-1.14746094e-02,  2.25830078e-02, -1.02539062e-02, ...,\n",
      "          -1.36718750e-02,  2.44140625e-03,  4.60815430e-03],\n",
      "         [ 1.59912109e-02, -2.74658203e-02,  1.21459961e-02, ...,\n",
      "          -1.90429688e-02,  1.78222656e-02,  1.34277344e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.99072266e-02,  4.07714844e-02, -8.36181641e-03, ...,\n",
      "          -3.19824219e-02,  1.14135742e-02,  3.88183594e-02],\n",
      "         [-2.86865234e-03,  1.09252930e-02,  3.56445312e-02, ...,\n",
      "           5.54199219e-02, -2.45361328e-02, -4.02450562e-04],\n",
      "         [ 1.89208984e-02, -1.20849609e-02,  3.66210938e-02, ...,\n",
      "           2.19726562e-02,  9.11712646e-04, -4.82559204e-04],\n",
      "         ...,\n",
      "         [-2.89306641e-02, -2.25830078e-03, -2.08740234e-02, ...,\n",
      "           1.14135742e-02, -2.24609375e-02, -8.54492188e-03],\n",
      "         [-4.30297852e-03,  1.49536133e-02, -2.06298828e-02, ...,\n",
      "          -4.37011719e-02,  1.75781250e-02,  1.57470703e-02],\n",
      "         [ 1.40380859e-02,  1.43432617e-02,  1.54876709e-03, ...,\n",
      "           1.21459961e-02, -2.42919922e-02,  6.95800781e-03]],\n",
      "\n",
      "        [[ 2.56347656e-02,  3.01513672e-02, -1.70898438e-02, ...,\n",
      "          -2.11181641e-02,  2.11181641e-02,  1.25732422e-02],\n",
      "         [-1.44042969e-02, -2.51770020e-03,  2.81982422e-02, ...,\n",
      "          -2.49023438e-02,  2.58789062e-02, -2.10571289e-03],\n",
      "         [ 8.42285156e-03,  2.05993652e-04, -1.00097656e-02, ...,\n",
      "          -2.88085938e-02, -6.46972656e-03, -2.16064453e-02],\n",
      "         ...,\n",
      "         [ 4.22363281e-02,  1.58691406e-02,  1.28173828e-02, ...,\n",
      "           3.51562500e-02,  1.06201172e-02,  1.09252930e-02],\n",
      "         [ 3.01513672e-02,  1.51367188e-02,  3.32641602e-03, ...,\n",
      "          -1.14746094e-02,  9.27734375e-03,  2.51770020e-03],\n",
      "         [ 8.36181641e-03,  5.09643555e-03,  1.32446289e-02, ...,\n",
      "           6.59942627e-04,  2.18505859e-02,  1.61132812e-02]],\n",
      "\n",
      "        [[-4.88281250e-03,  1.74560547e-02, -1.59912109e-02, ...,\n",
      "           2.64892578e-02,  7.99560547e-03,  2.81982422e-02],\n",
      "         [-2.27050781e-02, -2.33154297e-02,  1.74560547e-02, ...,\n",
      "           7.81250000e-03,  3.71093750e-02,  2.47802734e-02],\n",
      "         [-4.51660156e-03, -6.71386719e-03,  1.33666992e-02, ...,\n",
      "          -1.15966797e-03,  2.30407715e-03, -3.27148438e-02],\n",
      "         ...,\n",
      "         [-1.62353516e-02, -5.06591797e-03, -1.00097656e-02, ...,\n",
      "          -6.89697266e-03, -4.83398438e-02,  4.73022461e-03],\n",
      "         [-3.03955078e-02, -2.01416016e-02, -7.38525391e-03, ...,\n",
      "          -1.34887695e-02, -8.97216797e-03,  1.55029297e-02],\n",
      "         [-3.32031250e-02, -3.27148438e-02,  1.91650391e-02, ...,\n",
      "          -1.40380859e-02,  1.64794922e-02,  2.74658203e-02]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 5.85937500e-03, -1.06201172e-02,  4.79125977e-03, ...,\n",
      "          -8.91113281e-03,  1.26647949e-03,  6.16455078e-03],\n",
      "         [-1.00708008e-02, -1.85394287e-03,  3.73840332e-03, ...,\n",
      "          -7.05718994e-04, -4.39453125e-03,  3.91006470e-04],\n",
      "         [ 2.22778320e-03,  3.52478027e-03,  1.09863281e-02, ...,\n",
      "          -7.56835938e-03,  4.27246094e-03,  1.10626221e-04],\n",
      "         ...,\n",
      "         [-3.83300781e-02,  3.17382812e-02, -2.55126953e-02, ...,\n",
      "           2.64892578e-02,  2.22167969e-02, -3.41796875e-02],\n",
      "         [-2.21252441e-03,  1.04522705e-03, -5.07354736e-04, ...,\n",
      "          -3.44848633e-03,  3.00598145e-03, -1.25732422e-02],\n",
      "         [ 2.41088867e-03,  1.85546875e-02, -2.47192383e-03, ...,\n",
      "           3.09753418e-03,  5.37109375e-03, -6.86645508e-04]],\n",
      "\n",
      "        [[-6.13403320e-03,  7.78198242e-04,  4.13894653e-04, ...,\n",
      "           2.12097168e-03, -9.88769531e-03,  2.51464844e-02],\n",
      "         [-5.95092773e-03,  3.79943848e-03,  9.76562500e-04, ...,\n",
      "           5.61523438e-03, -2.19345093e-04, -8.12530518e-04],\n",
      "         [-1.55448914e-04, -6.19506836e-03,  1.44958496e-03, ...,\n",
      "           2.72216797e-02, -9.94873047e-03, -3.89099121e-03],\n",
      "         ...,\n",
      "         [-1.48773193e-03,  4.92095947e-04,  6.22558594e-03, ...,\n",
      "          -1.13525391e-02, -1.09863281e-03, -4.48608398e-03],\n",
      "         [ 9.52148438e-03,  1.98364258e-03, -2.62451172e-02, ...,\n",
      "           3.58581543e-04,  3.31115723e-03,  1.13010406e-04],\n",
      "         [ 4.36401367e-03, -4.30297852e-03,  1.27410889e-03, ...,\n",
      "          -1.74713135e-03, -8.05664062e-03,  1.87683105e-03]],\n",
      "\n",
      "        [[ 3.95507812e-02, -9.33837891e-03, -1.62353516e-02, ...,\n",
      "          -1.18408203e-02, -8.97216797e-03, -1.72119141e-02],\n",
      "         [-2.10571289e-03,  4.05883789e-03,  3.25012207e-03, ...,\n",
      "          -3.63769531e-02,  4.76074219e-03,  1.25732422e-02],\n",
      "         [ 1.58691406e-02, -9.82666016e-03, -3.22341919e-04, ...,\n",
      "          -1.86767578e-02, -1.05590820e-02,  2.73132324e-03],\n",
      "         ...,\n",
      "         [ 4.51660156e-03, -7.56835938e-03,  9.88769531e-03, ...,\n",
      "          -8.36181641e-03, -1.59912109e-02,  1.95312500e-02],\n",
      "         [ 1.81579590e-03, -7.93457031e-03, -1.90734863e-03, ...,\n",
      "          -1.07421875e-02, -1.90429688e-02,  2.07519531e-02],\n",
      "         [-1.10626221e-03,  2.50244141e-03,  2.13623047e-02, ...,\n",
      "          -1.30615234e-02, -7.59887695e-03, -5.21850586e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.36816406e-02, -2.03857422e-02, -6.29425049e-04, ...,\n",
      "           2.01416016e-02, -3.85742188e-02,  1.85546875e-02],\n",
      "         [ 7.53784180e-03, -5.92041016e-03, -1.44195557e-03, ...,\n",
      "          -3.27148438e-02, -4.36401367e-03, -8.30078125e-03],\n",
      "         [ 7.47680664e-03,  7.08007812e-03,  1.58691406e-02, ...,\n",
      "          -1.15356445e-02, -2.62451172e-02,  2.46582031e-02],\n",
      "         ...,\n",
      "         [-2.90527344e-02, -1.57470703e-02, -1.49536133e-02, ...,\n",
      "          -1.36718750e-02, -3.17382812e-02,  2.12402344e-02],\n",
      "         [-6.46972656e-03,  3.17382812e-02, -2.21252441e-03, ...,\n",
      "          -2.82287598e-03, -1.53198242e-02,  4.97436523e-03],\n",
      "         [ 2.58789062e-02,  1.18408203e-02, -1.50756836e-02, ...,\n",
      "          -2.55126953e-02,  1.81884766e-02,  3.57055664e-03]],\n",
      "\n",
      "        [[-3.07617188e-02, -1.18408203e-02,  5.76782227e-03, ...,\n",
      "           1.03759766e-02, -5.88378906e-02, -3.22265625e-02],\n",
      "         [-8.30078125e-03, -1.27563477e-02,  1.68457031e-02, ...,\n",
      "          -1.06811523e-02, -3.56445312e-02,  8.17871094e-03],\n",
      "         [-8.23974609e-03, -3.19824219e-02, -6.25610352e-03, ...,\n",
      "          -1.51824951e-03, -2.19726562e-02,  2.68554688e-02],\n",
      "         ...,\n",
      "         [-1.09252930e-02, -1.30615234e-02,  4.21142578e-03, ...,\n",
      "           1.14746094e-02, -3.96728516e-03, -3.51562500e-02],\n",
      "         [-2.20947266e-02,  3.46679688e-02, -2.30712891e-02, ...,\n",
      "           2.58789062e-02,  4.19921875e-02,  5.73730469e-03],\n",
      "         [ 1.38549805e-02, -1.68457031e-02, -1.80664062e-02, ...,\n",
      "          -2.57568359e-02,  3.17382812e-02, -1.07421875e-02]],\n",
      "\n",
      "        [[ 8.66699219e-03,  5.67626953e-03,  1.39770508e-02, ...,\n",
      "           8.05664062e-03,  1.06811523e-02,  1.57470703e-02],\n",
      "         [ 1.94091797e-02,  4.07695770e-05,  1.74560547e-02, ...,\n",
      "           3.63159180e-03,  4.91333008e-03,  9.82666016e-03],\n",
      "         [-2.60925293e-03, -2.89306641e-02, -3.78417969e-03, ...,\n",
      "           2.12402344e-02,  1.68457031e-02,  4.45556641e-03],\n",
      "         ...,\n",
      "         [ 1.11083984e-02,  1.03149414e-02,  5.82885742e-03, ...,\n",
      "          -4.18090820e-03,  4.52041626e-04, -3.96728516e-04],\n",
      "         [-1.21459961e-02,  1.40991211e-02,  2.70996094e-02, ...,\n",
      "           5.52368164e-03,  7.32421875e-03, -3.34472656e-02],\n",
      "         [ 4.24804688e-02,  6.89697266e-03, -1.77001953e-02, ...,\n",
      "          -1.78222656e-02, -9.70458984e-03,  2.06298828e-02]]],\n",
      "\n",
      "\n",
      "       [[[-6.02722168e-04,  1.25732422e-02, -1.39160156e-02, ...,\n",
      "          -2.01416016e-03,  8.11767578e-03, -3.90625000e-03],\n",
      "         [ 2.02941895e-03,  5.30242920e-04, -6.74438477e-03, ...,\n",
      "          -6.59179688e-03,  8.97216797e-03,  2.24304199e-03],\n",
      "         [ 7.72094727e-03, -1.12304688e-02, -4.45556641e-03, ...,\n",
      "           1.15356445e-02,  6.04248047e-03, -5.46264648e-03],\n",
      "         ...,\n",
      "         [-2.47802734e-02,  3.06396484e-02, -4.52041626e-04, ...,\n",
      "           1.14135742e-02, -1.06201172e-02,  1.57470703e-02],\n",
      "         [ 1.41601562e-02, -8.23974609e-03, -4.27246094e-03, ...,\n",
      "           1.18255615e-03, -3.46374512e-03,  3.12805176e-03],\n",
      "         [ 6.22558594e-03, -4.60815430e-03, -1.99890137e-03, ...,\n",
      "           3.99780273e-03,  1.56250000e-02, -6.82830811e-04]],\n",
      "\n",
      "        [[-3.12500000e-02,  1.10473633e-02, -9.38415527e-04, ...,\n",
      "           7.69042969e-03, -2.20947266e-02,  7.50732422e-03],\n",
      "         [ 2.34985352e-03,  8.12530518e-04,  9.46044922e-03, ...,\n",
      "           9.61303711e-04,  4.45556641e-03,  1.89781189e-04],\n",
      "         [ 1.35040283e-03,  8.85009766e-03,  1.89208984e-02, ...,\n",
      "          -1.70898438e-02, -2.19726562e-02, -1.52587891e-02],\n",
      "         ...,\n",
      "         [ 2.09045410e-03,  2.02941895e-03, -2.19726562e-03, ...,\n",
      "          -4.76074219e-03,  1.01928711e-02, -1.96838379e-03],\n",
      "         [ 7.29370117e-03,  2.42919922e-02,  2.78320312e-02, ...,\n",
      "           3.29589844e-03, -1.06201172e-02, -7.14111328e-03],\n",
      "         [ 4.69970703e-03,  3.08227539e-03, -5.40161133e-03, ...,\n",
      "          -9.84191895e-04,  2.99072266e-03, -5.79833984e-03]],\n",
      "\n",
      "        [[-5.95092773e-03,  5.31005859e-03, -2.92968750e-02, ...,\n",
      "          -2.86865234e-03, -5.34057617e-03,  4.73022461e-03],\n",
      "         [ 1.05590820e-02,  1.16577148e-02, -3.27148438e-02, ...,\n",
      "           5.03540039e-03,  1.55639648e-02,  2.68554688e-03],\n",
      "         [ 1.73950195e-03,  8.97216797e-03,  4.63867188e-03, ...,\n",
      "           1.22070312e-02,  1.20849609e-02,  1.67236328e-02],\n",
      "         ...,\n",
      "         [-9.52148438e-03,  1.26953125e-02, -6.92749023e-03, ...,\n",
      "           6.74438477e-03, -5.88989258e-03, -1.51824951e-03],\n",
      "         [-8.23974609e-03, -2.22167969e-02,  2.41088867e-03, ...,\n",
      "          -1.35498047e-02, -1.66015625e-02, -9.52148438e-03],\n",
      "         [ 2.10571289e-03, -8.69750977e-04,  8.85009766e-03, ...,\n",
      "          -1.67846680e-03, -1.03759766e-02,  1.57470703e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.60980225e-03,  3.99780273e-03,  2.89306641e-02, ...,\n",
      "           1.62353516e-02,  6.07299805e-03,  1.14746094e-02],\n",
      "         [ 7.93457031e-03,  4.63867188e-02, -1.04980469e-02, ...,\n",
      "           8.30078125e-03, -1.99890137e-03, -2.70996094e-02],\n",
      "         [ 3.51562500e-02, -1.04980469e-02,  9.70458984e-03, ...,\n",
      "          -1.61132812e-02, -2.64892578e-02, -8.85009766e-03],\n",
      "         ...,\n",
      "         [-1.38854980e-03, -5.07812500e-02, -3.64685059e-03, ...,\n",
      "          -9.39941406e-03,  7.93457031e-03, -7.20214844e-03],\n",
      "         [-7.43865967e-05,  1.64794922e-02, -1.08032227e-02, ...,\n",
      "          -3.12500000e-02,  6.62231445e-03, -4.15039062e-02],\n",
      "         [-3.95507812e-02, -7.44628906e-03,  4.88281250e-03, ...,\n",
      "          -7.17163086e-03,  3.01513672e-02, -1.47705078e-02]],\n",
      "\n",
      "        [[-3.39355469e-02,  3.73535156e-02, -2.60009766e-02, ...,\n",
      "           2.34375000e-02, -1.10473633e-02,  1.00708008e-02],\n",
      "         [ 1.04522705e-03,  1.64794922e-02,  5.73730469e-03, ...,\n",
      "          -6.77490234e-03,  5.31005859e-03,  1.67846680e-03],\n",
      "         [ 1.40991211e-02,  2.22778320e-03,  6.04248047e-03, ...,\n",
      "           1.84326172e-02,  7.26318359e-03, -8.96453857e-05],\n",
      "         ...,\n",
      "         [ 4.05273438e-02, -9.33837891e-03, -1.96533203e-02, ...,\n",
      "           7.93457031e-04,  3.85742188e-02,  5.58471680e-03],\n",
      "         [-3.36914062e-02,  2.14843750e-02,  2.91442871e-03, ...,\n",
      "          -2.73437500e-02, -1.70898438e-02, -1.80664062e-02],\n",
      "         [ 5.43212891e-03, -2.56347656e-02,  4.68750000e-02, ...,\n",
      "          -2.21252441e-03, -4.02832031e-03, -5.61523438e-02]],\n",
      "\n",
      "        [[ 6.48498535e-04,  3.63769531e-02,  1.47819519e-04, ...,\n",
      "           1.51977539e-02, -1.72119141e-02,  7.56835938e-03],\n",
      "         [ 2.47802734e-02,  7.38525391e-03, -1.00097656e-02, ...,\n",
      "           2.79541016e-02,  3.88183594e-02, -1.61132812e-02],\n",
      "         [ 3.02734375e-02,  8.48388672e-03,  9.58251953e-03, ...,\n",
      "          -1.28784180e-02, -3.46679688e-02, -1.62506104e-03],\n",
      "         ...,\n",
      "         [ 1.25122070e-02,  7.59887695e-03, -2.27355957e-03, ...,\n",
      "           1.03149414e-02,  8.11767578e-03,  3.18908691e-03],\n",
      "         [-7.04956055e-03, -2.01416016e-02,  2.97851562e-02, ...,\n",
      "          -1.91650391e-02, -2.08740234e-02,  4.21142578e-03],\n",
      "         [-6.59179688e-03, -1.50146484e-02, -3.06396484e-02, ...,\n",
      "          -2.74658203e-03, -2.91748047e-02, -5.15747070e-03]]],\n",
      "\n",
      "\n",
      "       [[[ 1.03149414e-02,  4.76074219e-03, -1.42211914e-02, ...,\n",
      "           2.83813477e-03,  3.67736816e-03, -6.13403320e-03],\n",
      "         [-8.97216797e-03, -1.01928711e-02,  7.20214844e-03, ...,\n",
      "           1.36375427e-04, -4.02832031e-03,  5.40161133e-03],\n",
      "         [-5.24902344e-03, -7.93457031e-03,  1.63269043e-03, ...,\n",
      "          -8.96453857e-04,  6.77490234e-03, -9.21630859e-03],\n",
      "         ...,\n",
      "         [-4.60815430e-03,  1.42822266e-02,  1.25122070e-02, ...,\n",
      "          -8.85009766e-03,  2.16064453e-02, -1.09252930e-02],\n",
      "         [-8.17871094e-03,  7.56835938e-03, -1.01928711e-02, ...,\n",
      "           4.48608398e-03,  1.53808594e-02, -2.59399414e-03],\n",
      "         [ 3.82995605e-03, -4.94384766e-03,  3.69262695e-03, ...,\n",
      "          -2.24304199e-03,  7.01904297e-04,  1.48010254e-03]],\n",
      "\n",
      "        [[-2.62451172e-02,  3.32031250e-02, -1.94549561e-03, ...,\n",
      "           8.05664062e-03, -1.28784180e-02, -3.15856934e-03],\n",
      "         [-2.34985352e-03,  7.89642334e-04, -1.15966797e-03, ...,\n",
      "           3.32641602e-03,  6.56127930e-03, -4.08935547e-03],\n",
      "         [-1.86920166e-03, -1.89781189e-04,  7.23266602e-03, ...,\n",
      "           7.29370117e-03, -8.60595703e-03,  2.01416016e-02],\n",
      "         ...,\n",
      "         [ 5.70678711e-03, -7.93457031e-04, -5.21850586e-03, ...,\n",
      "          -2.89916992e-03,  1.20849609e-02, -4.27246094e-03],\n",
      "         [-2.01416016e-02,  6.74438477e-03, -3.82995605e-03, ...,\n",
      "          -7.23266602e-03,  2.77709961e-03,  6.50024414e-03],\n",
      "         [ 5.18798828e-03, -1.20544434e-03,  6.10351562e-03, ...,\n",
      "           1.48773193e-03, -1.03759766e-03,  8.66699219e-03]],\n",
      "\n",
      "        [[ 4.79125977e-03, -1.58691406e-02, -2.97851562e-02, ...,\n",
      "          -3.18908691e-03,  1.01928711e-02, -2.05078125e-02],\n",
      "         [ 2.33154297e-02, -1.14135742e-02, -1.62353516e-02, ...,\n",
      "          -5.76019287e-04,  2.02636719e-02,  9.82666016e-03],\n",
      "         [-1.45874023e-02, -6.62231445e-03, -2.06298828e-02, ...,\n",
      "           1.58691406e-02, -3.89099121e-03,  1.89208984e-02],\n",
      "         ...,\n",
      "         [-1.06811523e-02, -1.02539062e-02, -7.35473633e-03, ...,\n",
      "           3.24707031e-02, -1.55029297e-02,  1.58691406e-02],\n",
      "         [ 1.64794922e-02,  2.23388672e-02,  4.85229492e-03, ...,\n",
      "          -9.76562500e-03,  9.33837891e-03, -2.61230469e-02],\n",
      "         [ 9.82666016e-03, -3.29589844e-03,  9.58251953e-03, ...,\n",
      "           1.09863281e-03, -5.95092773e-03, -7.75146484e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.97753906e-02, -5.88989258e-03,  1.22680664e-02, ...,\n",
      "          -3.89099121e-03,  2.60009766e-02, -5.88989258e-03],\n",
      "         [ 2.67333984e-02, -9.19342041e-04, -1.97753906e-02, ...,\n",
      "           1.83868408e-03,  2.03857422e-02,  2.53906250e-02],\n",
      "         [ 9.29832458e-05, -1.50756836e-02,  7.85827637e-04, ...,\n",
      "          -2.36511230e-03, -1.72119141e-02, -9.88769531e-03],\n",
      "         ...,\n",
      "         [ 3.20434570e-03,  2.00195312e-02,  5.37109375e-03, ...,\n",
      "          -1.37939453e-02,  1.18255615e-03, -3.40270996e-03],\n",
      "         [-2.92968750e-02, -2.63671875e-02, -8.30078125e-03, ...,\n",
      "           7.81250000e-03,  1.86767578e-02,  7.26318359e-03],\n",
      "         [-1.69677734e-02,  1.36718750e-02,  1.05590820e-02, ...,\n",
      "          -1.50146484e-02, -1.18408203e-02,  2.51464844e-02]],\n",
      "\n",
      "        [[ 3.14941406e-02,  4.91333008e-03, -1.26342773e-02, ...,\n",
      "           4.24804688e-02,  2.37226486e-05,  4.11987305e-03],\n",
      "         [-1.22070312e-02, -4.18090820e-03, -1.84631348e-03, ...,\n",
      "          -3.03955078e-02, -2.39257812e-02,  1.34277344e-03],\n",
      "         [ 9.64355469e-03, -1.66015625e-02, -1.27563477e-02, ...,\n",
      "          -2.01416016e-03,  6.86645508e-03, -8.66699219e-03],\n",
      "         ...,\n",
      "         [-3.93676758e-03,  3.11279297e-02,  3.27148438e-02, ...,\n",
      "          -3.49121094e-02,  2.22167969e-02, -3.38745117e-03],\n",
      "         [-2.47192383e-03,  2.94189453e-02,  2.67028809e-03, ...,\n",
      "          -3.66210938e-03, -1.50756836e-02,  4.48608398e-03],\n",
      "         [-3.66210938e-02,  1.92871094e-02,  7.14111328e-03, ...,\n",
      "           3.83300781e-02,  2.53906250e-02, -2.41699219e-02]],\n",
      "\n",
      "        [[ 1.31225586e-02,  1.31225586e-02,  3.14941406e-02, ...,\n",
      "          -1.18408203e-02, -7.23266602e-03,  1.68457031e-02],\n",
      "         [ 2.42614746e-03,  1.69677734e-02,  3.58581543e-03, ...,\n",
      "          -4.63867188e-02, -1.16577148e-02,  1.21459961e-02],\n",
      "         [ 5.70678711e-03, -4.45556641e-03,  6.40869141e-03, ...,\n",
      "          -5.34057617e-03,  1.91650391e-02,  3.66210938e-02],\n",
      "         ...,\n",
      "         [ 1.19018555e-03, -2.13623047e-03, -8.54492188e-03, ...,\n",
      "           3.10058594e-02, -1.70898438e-02,  4.27246094e-02],\n",
      "         [-1.90429688e-02, -1.94549561e-03, -3.41796875e-02, ...,\n",
      "          -3.27148438e-02,  1.89208984e-02,  1.51977539e-02],\n",
      "         [-6.04248047e-03, -1.96533203e-02, -1.84631348e-03, ...,\n",
      "           2.60009766e-02,  1.00097656e-02,  3.97949219e-02]]]],      dtype=float32)}}}, 'logits_dense': {'kernel': Array([[-0.00389099, -0.03149414, -0.01245117, ..., -0.02807617,\n",
      "         0.02294922,  0.00799561],\n",
      "       [ 0.00317383,  0.04663086,  0.00360107, ..., -0.01953125,\n",
      "         0.0255127 , -0.00878906],\n",
      "       [-0.00714111, -0.00231934,  0.01953125, ..., -0.00239563,\n",
      "         0.03149414,  0.00634766],\n",
      "       ...,\n",
      "       [ 0.00531006, -0.02111816, -0.02709961, ...,  0.01226807,\n",
      "         0.00668335, -0.02929688],\n",
      "       [-0.00817871,  0.01733398,  0.01428223, ..., -0.01165771,\n",
      "        -0.00921631, -0.02001953],\n",
      "       [ 0.00701904,  0.03344727, -0.00817871, ..., -0.02368164,\n",
      "        -0.00582886,  0.03369141]], dtype=float32)}}, 'token_embedder': {'embedding': Array([[ 1.22934580e-06, -1.81794167e-06, -4.35113907e-06, ...,\n",
      "         8.71717930e-07, -6.52670860e-06,  8.90344381e-07],\n",
      "       [ 1.86157227e-03, -3.37219238e-03,  3.98635864e-04, ...,\n",
      "        -8.30078125e-03,  2.57873535e-03, -3.93676758e-03],\n",
      "       [ 1.09863281e-02,  9.88769531e-03, -5.09643555e-03, ...,\n",
      "         2.51770020e-03,  7.70568848e-04, -5.00488281e-03],\n",
      "       ...,\n",
      "       [-1.39770508e-02, -2.73132324e-03, -1.98974609e-02, ...,\n",
      "        -1.04370117e-02,  9.58251953e-03, -1.80053711e-03],\n",
      "       [-1.07421875e-02,  9.33837891e-03,  1.29394531e-02, ...,\n",
      "        -3.32031250e-02, -1.63574219e-02,  3.38745117e-03],\n",
      "       [-8.30078125e-03, -4.05883789e-03, -1.10626221e-03, ...,\n",
      "         3.47900391e-03, -1.29394531e-02,  3.19480896e-05]],      dtype=float32)}}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7f79d03fd360>, update=<function chain.<locals>.update_fn at 0x7f79d03fd750>), opt_state=(ScaleByAdamState(count=Array(0, dtype=int32), mu={'params': {'decoder': {'decoder_norm': {'scale': Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)}, 'layers': {'mlp': {'wi_0': {'kernel': Array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}, 'wi_1': {'kernel': Array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}, 'wo': {'kernel': Array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}, 'self_attention': {'key': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}, 'out': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}, 'query': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}, 'value': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}}}, 'logits_dense': {'kernel': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}}, 'token_embedder': {'embedding': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}}}, nu={'params': {'decoder': {'decoder_norm': {'scale': Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)}, 'layers': {'mlp': {'wi_0': {'kernel': Array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}, 'wi_1': {'kernel': Array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}, 'wo': {'kernel': Array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}, 'self_attention': {'key': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}, 'out': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}, 'query': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}, 'value': {'kernel': Array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)}}}, 'logits_dense': {'kernel': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}}, 'token_embedder': {'embedding': Array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}}}), EmptyState(), ScaleByScheduleState(count=Array(0, dtype=int32))))\n"
     ]
    }
   ],
   "source": [
    "print(training_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d962c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'decoder': {'decoder_norm': {'scale': Array([1.8671875, 1.8671875, 1.8046875, ..., 1.71875  , 1.828125 ,\n",
      "       1.6015625], dtype=float32)}, 'layers': {'mlp': {'wi_0': {'kernel': Array([[[ 1.57470703e-02, -2.16674805e-03,  6.83593750e-03, ...,\n",
      "          1.41263008e-05,  2.66113281e-02, -5.98907471e-04],\n",
      "        [ 2.91748047e-02, -2.80761719e-02, -6.07299805e-03, ...,\n",
      "         -4.00390625e-02, -1.61132812e-02, -2.49023438e-02],\n",
      "        [ 8.60595703e-03, -2.27928162e-04,  1.76239014e-03, ...,\n",
      "          2.96630859e-02, -1.24511719e-02, -1.60980225e-03],\n",
      "        ...,\n",
      "        [ 8.97216797e-03,  7.41577148e-03,  3.80859375e-02, ...,\n",
      "         -1.44042969e-02,  1.32446289e-02,  2.12097168e-03],\n",
      "        [-3.34472656e-02, -1.91650391e-02, -2.18505859e-02, ...,\n",
      "         -2.44140625e-02,  3.14941406e-02, -8.54492188e-03],\n",
      "        [ 2.36511230e-03, -7.27539062e-02,  1.79443359e-02, ...,\n",
      "         -1.50756836e-02,  3.36914062e-02,  2.35595703e-02]],\n",
      "\n",
      "       [[ 1.70898438e-02, -6.01196289e-03, -2.16064453e-02, ...,\n",
      "         -3.22265625e-02,  2.01416016e-02, -1.13525391e-02],\n",
      "        [-9.52148438e-03, -1.81579590e-03, -2.89306641e-02, ...,\n",
      "          1.63574219e-02,  3.64685059e-03,  1.45874023e-02],\n",
      "        [ 2.39257812e-02,  2.44140625e-03, -8.85009766e-03, ...,\n",
      "         -1.04370117e-02, -1.57470703e-02,  1.22680664e-02],\n",
      "        ...,\n",
      "        [ 2.62451172e-02,  2.33154297e-02,  1.23596191e-03, ...,\n",
      "          7.50732422e-03,  1.37939453e-02,  1.44653320e-02],\n",
      "        [-3.14941406e-02, -3.46374512e-03, -1.06811523e-02, ...,\n",
      "         -1.13525391e-02,  2.50244141e-02, -2.41699219e-02],\n",
      "        [-9.52148438e-03, -2.63671875e-02,  2.97546387e-03, ...,\n",
      "         -2.53906250e-02,  1.64794922e-02, -3.24707031e-02]],\n",
      "\n",
      "       [[ 3.14941406e-02,  5.64575195e-03,  2.05078125e-02, ...,\n",
      "          5.79833984e-03, -1.70898438e-02, -2.36816406e-02],\n",
      "        [ 3.73535156e-02, -3.14941406e-02,  3.12805176e-03, ...,\n",
      "         -4.24194336e-03, -3.73535156e-02,  5.46264648e-03],\n",
      "        [-1.27563477e-02, -4.39453125e-03,  2.66113281e-02, ...,\n",
      "         -1.48925781e-02, -3.14331055e-03,  4.79125977e-03],\n",
      "        ...,\n",
      "        [-1.02539062e-02, -1.10473633e-02, -1.19628906e-02, ...,\n",
      "         -1.51367188e-02, -3.27148438e-02,  1.47247314e-03],\n",
      "        [-4.51660156e-03,  3.22265625e-02, -3.02734375e-02, ...,\n",
      "          1.96533203e-02, -1.29394531e-02, -2.16064453e-02],\n",
      "        [ 3.03955078e-02,  2.82287598e-03, -1.18408203e-02, ...,\n",
      "          9.88769531e-03, -3.29589844e-02, -3.12500000e-02]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.58691406e-02,  1.61132812e-02, -1.30004883e-02, ...,\n",
      "         -8.91113281e-03, -3.43322754e-03,  1.10626221e-03],\n",
      "        [-3.39355469e-02,  1.44042969e-02, -9.33837891e-03, ...,\n",
      "         -1.59454346e-03,  2.06298828e-02,  3.63769531e-02],\n",
      "        [-8.60595703e-03,  1.04980469e-02, -1.21459961e-02, ...,\n",
      "         -3.39355469e-02,  2.23388672e-02, -1.29394531e-02],\n",
      "        ...,\n",
      "        [-6.01196289e-03, -2.31933594e-02, -1.12915039e-02, ...,\n",
      "         -5.03540039e-03, -1.79443359e-02,  2.14843750e-02],\n",
      "        [ 1.22070312e-02, -3.64685059e-03,  1.40380859e-02, ...,\n",
      "         -1.38549805e-02,  1.39160156e-02, -1.60980225e-03],\n",
      "        [ 1.53808594e-02,  9.27734375e-03, -3.96728516e-03, ...,\n",
      "          3.63159180e-03,  3.63769531e-02, -1.47094727e-02]],\n",
      "\n",
      "       [[ 6.50024414e-03, -8.66699219e-03,  1.89208984e-02, ...,\n",
      "         -1.34887695e-02, -6.40869141e-03,  5.92041016e-03],\n",
      "        [-1.85546875e-02, -2.18505859e-02, -1.50756836e-02, ...,\n",
      "          1.44042969e-02,  1.95312500e-02,  9.88769531e-03],\n",
      "        [ 2.74658203e-02, -1.46484375e-02,  1.81884766e-02, ...,\n",
      "         -4.07714844e-02, -4.95605469e-02, -2.90527344e-02],\n",
      "        ...,\n",
      "        [-2.85644531e-02,  1.11694336e-02,  2.67333984e-02, ...,\n",
      "          1.05590820e-02, -6.50024414e-03, -1.96533203e-02],\n",
      "        [-7.62939453e-03, -2.59399414e-03,  8.72802734e-03, ...,\n",
      "         -2.29492188e-02, -3.95507812e-02,  1.03149414e-02],\n",
      "        [ 1.79443359e-02, -1.72119141e-02, -1.51367188e-02, ...,\n",
      "          1.84326172e-02, -1.36108398e-02,  1.36718750e-02]],\n",
      "\n",
      "       [[ 1.58691406e-02,  9.88769531e-03,  1.94091797e-02, ...,\n",
      "          4.02832031e-02, -1.89208984e-02, -2.47802734e-02],\n",
      "        [-1.12915039e-02, -4.07714844e-02, -2.11181641e-02, ...,\n",
      "         -1.44958496e-03, -1.20239258e-02, -3.46679688e-02],\n",
      "        [ 1.89208984e-03, -7.65991211e-03,  7.32421875e-03, ...,\n",
      "         -1.22070312e-02,  1.30615234e-02,  5.64575195e-03],\n",
      "        ...,\n",
      "        [ 3.27148438e-02,  4.18090820e-03,  3.80859375e-02, ...,\n",
      "          8.17871094e-03,  1.79443359e-02, -1.44042969e-02],\n",
      "        [ 3.57055664e-03,  3.14941406e-02, -5.67626953e-03, ...,\n",
      "         -1.52587891e-02,  2.70080566e-03, -2.22167969e-02],\n",
      "        [ 2.85339355e-03,  1.78222656e-02, -7.59887695e-03, ...,\n",
      "          3.12500000e-02, -8.91113281e-03,  5.79833984e-03]]],      dtype=float32)}, 'wi_1': {'kernel': Array([[[ 0.00026703, -0.0111084 , -0.00592041, ..., -0.00909424,\n",
      "         -0.01660156,  0.01904297],\n",
      "        [ 0.00169373,  0.00854492,  0.02783203, ..., -0.01367188,\n",
      "          0.03491211, -0.01586914],\n",
      "        [ 0.00160217,  0.00744629,  0.02722168, ..., -0.0088501 ,\n",
      "          0.0145874 , -0.01477051],\n",
      "        ...,\n",
      "        [-0.00273132, -0.00549316, -0.01647949, ...,  0.03271484,\n",
      "         -0.01068115,  0.01037598],\n",
      "        [-0.00097656, -0.02856445, -0.03369141, ..., -0.02062988,\n",
      "         -0.00872803, -0.00588989],\n",
      "        [-0.04125977,  0.02233887, -0.0065918 , ...,  0.01287842,\n",
      "          0.0055542 ,  0.04199219]],\n",
      "\n",
      "       [[-0.0291748 , -0.03125   ,  0.01489258, ..., -0.00164032,\n",
      "          0.0072937 ,  0.01965332],\n",
      "        [ 0.03564453, -0.00228882, -0.04467773, ..., -0.02709961,\n",
      "          0.00152588, -0.01818848],\n",
      "        [-0.00117493, -0.01794434,  0.01623535, ..., -0.01544189,\n",
      "         -0.00411987, -0.02880859],\n",
      "        ...,\n",
      "        [-0.00146484,  0.00698853, -0.0255127 , ...,  0.01184082,\n",
      "         -0.01544189, -0.00230408],\n",
      "        [ 0.00897217, -0.02868652, -0.02172852, ..., -0.01245117,\n",
      "          0.00427246, -0.02282715],\n",
      "        [-0.0168457 ,  0.03198242,  0.00747681, ..., -0.01086426,\n",
      "          0.00247192, -0.03808594]],\n",
      "\n",
      "       [[ 0.01477051,  0.01275635, -0.00842285, ..., -0.00665283,\n",
      "          0.0189209 , -0.00042534],\n",
      "        [-0.01745605, -0.0133667 ,  0.01287842, ...,  0.01330566,\n",
      "         -0.01672363, -0.00982666],\n",
      "        [ 0.01385498, -0.00891113,  0.02856445, ...,  0.00183868,\n",
      "          0.01287842, -0.02319336],\n",
      "        ...,\n",
      "        [ 0.00744629,  0.03344727, -0.00335693, ...,  0.01037598,\n",
      "         -0.00509644,  0.01306152],\n",
      "        [ 0.01098633,  0.03540039, -0.03857422, ...,  0.01464844,\n",
      "         -0.01367188,  0.00134277],\n",
      "        [ 0.02929688,  0.00634766, -0.01904297, ..., -0.00071335,\n",
      "         -0.02270508, -0.00279236]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.02099609,  0.01904297, -0.02270508, ...,  0.02954102,\n",
      "          0.00139618,  0.01184082],\n",
      "        [ 0.01159668, -0.00787354,  0.0168457 , ...,  0.02563477,\n",
      "          0.0043335 , -0.02307129],\n",
      "        [-0.01977539,  0.00346375, -0.01312256, ..., -0.01635742,\n",
      "         -0.00202942, -0.00576782],\n",
      "        ...,\n",
      "        [ 0.01623535,  0.00358582, -0.04589844, ...,  0.01062012,\n",
      "          0.01708984, -0.0057373 ],\n",
      "        [ 0.01635742,  0.00024796, -0.01696777, ...,  0.0168457 ,\n",
      "          0.00927734, -0.01409912],\n",
      "        [ 0.00723267, -0.01879883,  0.01055908, ...,  0.00138855,\n",
      "          0.02563477, -0.00701904]],\n",
      "\n",
      "       [[-0.02697754,  0.00604248,  0.00750732, ..., -0.00283813,\n",
      "         -0.01660156, -0.01434326],\n",
      "        [ 0.03015137,  0.0100708 , -0.04541016, ...,  0.01068115,\n",
      "         -0.00714111, -0.00747681],\n",
      "        [ 0.00692749,  0.01184082, -0.00595093, ..., -0.0078125 ,\n",
      "          0.0057373 ,  0.00090027],\n",
      "        ...,\n",
      "        [ 0.00531006, -0.02160645,  0.00228882, ...,  0.02514648,\n",
      "          0.01495361,  0.01708984],\n",
      "        [ 0.02185059, -0.01062012, -0.00741577, ..., -0.0133667 ,\n",
      "         -0.01464844,  0.00175476],\n",
      "        [-0.01135254,  0.00117493,  0.00213623, ...,  0.03491211,\n",
      "          0.01220703,  0.00358582]],\n",
      "\n",
      "       [[ 0.00653076,  0.00247192,  0.0017395 , ...,  0.01831055,\n",
      "          0.00314331, -0.03881836],\n",
      "        [-0.0177002 ,  0.00952148,  0.02893066, ..., -0.02514648,\n",
      "         -0.00123596,  0.02185059],\n",
      "        [-0.00068665,  0.0090332 ,  0.02026367, ...,  0.0213623 ,\n",
      "         -0.02600098, -0.00723267],\n",
      "        ...,\n",
      "        [ 0.00952148,  0.03417969, -0.03710938, ...,  0.03857422,\n",
      "         -0.00372314,  0.00454712],\n",
      "        [ 0.00263977, -0.01586914,  0.01574707, ..., -0.02160645,\n",
      "          0.01721191,  0.01757812],\n",
      "        [ 0.00305176, -0.00091171, -0.01019287, ..., -0.01757812,\n",
      "         -0.00048828,  0.02526855]]], dtype=float32)}, 'wo': {'kernel': Array([[[ 2.70080566e-03,  4.60815430e-03,  2.04467773e-03, ...,\n",
      "         -8.85009766e-03, -1.77001953e-02,  1.19628906e-02],\n",
      "        [ 7.93457031e-03,  1.07421875e-02, -1.70898438e-02, ...,\n",
      "         -1.01928711e-02, -1.29394531e-02,  2.44140625e-02],\n",
      "        [ 9.82666016e-03,  2.01416016e-02,  3.90625000e-02, ...,\n",
      "         -1.17797852e-02, -2.14843750e-02,  3.54003906e-02],\n",
      "        ...,\n",
      "        [ 4.61425781e-02,  1.86767578e-02,  1.91497803e-03, ...,\n",
      "          1.37939453e-02,  2.84423828e-02,  1.20849609e-02],\n",
      "        [ 1.95312500e-02,  2.57568359e-02,  2.25830078e-03, ...,\n",
      "         -3.17382812e-02, -5.87463379e-04, -6.37817383e-03],\n",
      "        [-2.21252441e-03,  4.22363281e-02, -1.40991211e-02, ...,\n",
      "          2.83813477e-03,  5.56945801e-04,  2.52685547e-02]],\n",
      "\n",
      "       [[-1.44653320e-02, -4.15039062e-03,  3.39355469e-02, ...,\n",
      "         -1.14135742e-02,  3.73535156e-02, -1.32751465e-03],\n",
      "        [ 9.70458984e-03,  6.40869141e-03, -1.46484375e-02, ...,\n",
      "         -6.50024414e-03,  2.14843750e-02, -3.79943848e-03],\n",
      "        [ 1.52587891e-02, -2.19726562e-02, -1.86767578e-02, ...,\n",
      "          2.80761719e-02,  3.46679688e-02,  9.27734375e-03],\n",
      "        ...,\n",
      "        [ 7.72094727e-03,  1.11694336e-02, -1.85546875e-02, ...,\n",
      "         -1.14135742e-02,  1.87988281e-02, -1.96533203e-02],\n",
      "        [-1.46484375e-02, -7.32421875e-04, -7.87353516e-03, ...,\n",
      "          4.36401367e-03, -3.19824219e-02, -1.33056641e-02],\n",
      "        [-2.75878906e-02,  2.04467773e-03, -1.09863281e-02, ...,\n",
      "          3.24707031e-02,  4.76074219e-02, -3.61328125e-02]],\n",
      "\n",
      "       [[ 8.30078125e-03,  8.97216797e-03, -4.36401367e-03, ...,\n",
      "          5.15747070e-03,  9.03320312e-03, -7.87353516e-03],\n",
      "        [ 5.59082031e-02, -3.58886719e-02, -2.31933594e-02, ...,\n",
      "         -1.62353516e-02,  1.48925781e-02, -1.64794922e-02],\n",
      "        [ 3.29589844e-02, -1.37329102e-02, -3.05175781e-03, ...,\n",
      "         -8.30078125e-03,  9.82666016e-03, -2.24609375e-02],\n",
      "        ...,\n",
      "        [-3.10897827e-04,  2.47802734e-02, -1.20849609e-02, ...,\n",
      "          8.78906250e-03,  3.10058594e-02, -7.65991211e-03],\n",
      "        [-3.17382812e-02,  4.37011719e-02, -1.98974609e-02, ...,\n",
      "         -2.33154297e-02,  3.50952148e-03, -2.63671875e-02],\n",
      "        [-7.32421875e-03, -5.15747070e-03,  2.50244141e-02, ...,\n",
      "         -7.09533691e-04, -8.05664062e-03,  1.70898438e-02]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1.74560547e-02,  1.59912109e-02, -1.46484375e-02, ...,\n",
      "          2.30712891e-02, -6.92749023e-03, -2.86102295e-04],\n",
      "        [-1.69677734e-02,  9.09423828e-03, -1.41906738e-03, ...,\n",
      "          4.29687500e-02, -2.99072266e-02, -1.34887695e-02],\n",
      "        [-2.17285156e-02,  1.27563477e-02, -2.84423828e-02, ...,\n",
      "         -1.56164169e-05,  1.07421875e-02,  2.77099609e-02],\n",
      "        ...,\n",
      "        [ 1.45263672e-02,  2.99072266e-02,  4.18090820e-03, ...,\n",
      "         -1.58691406e-02,  1.06201172e-02, -1.44653320e-02],\n",
      "        [ 2.80761719e-02,  1.46484375e-02, -7.59124756e-04, ...,\n",
      "          2.08740234e-02,  2.07519531e-02,  1.02539062e-02],\n",
      "        [-3.71093750e-02, -2.01416016e-03,  3.12500000e-02, ...,\n",
      "          1.07421875e-02, -8.23974609e-03,  5.82885742e-03]],\n",
      "\n",
      "       [[-5.40161133e-03, -1.37939453e-02,  2.19726562e-02, ...,\n",
      "         -1.34887695e-02, -1.22070312e-02, -2.96020508e-03],\n",
      "        [ 2.38037109e-03, -1.31835938e-02, -1.06201172e-02, ...,\n",
      "         -6.28662109e-03, -1.89208984e-02, -1.42211914e-02],\n",
      "        [-2.44140625e-03,  2.99072266e-03,  1.85394287e-03, ...,\n",
      "         -3.76892090e-03, -1.42211914e-02, -4.19921875e-02],\n",
      "        ...,\n",
      "        [-1.66015625e-02,  7.53784180e-03,  7.59887695e-03, ...,\n",
      "         -5.12695312e-03, -4.05273438e-02, -6.71386719e-03],\n",
      "        [-5.06591797e-03,  4.48608398e-03,  1.22070312e-02, ...,\n",
      "         -2.17285156e-02,  2.44140625e-03, -2.91442871e-03],\n",
      "        [-1.49536133e-02, -1.56250000e-02, -2.16674805e-03, ...,\n",
      "          5.88989258e-03,  2.14843750e-02,  1.77764893e-03]],\n",
      "\n",
      "       [[ 1.40380859e-03,  3.34472656e-02,  1.67236328e-02, ...,\n",
      "          2.95410156e-02, -2.18505859e-02, -3.01513672e-02],\n",
      "        [-1.96533203e-02,  2.92968750e-03,  3.40270996e-03, ...,\n",
      "          7.38525391e-03,  2.33154297e-02,  2.97851562e-02],\n",
      "        [-3.14941406e-02, -2.38037109e-02, -3.39355469e-02, ...,\n",
      "         -1.30004883e-02, -1.00097656e-02, -7.04956055e-03],\n",
      "        ...,\n",
      "        [-1.49536133e-02,  1.84631348e-03, -2.72216797e-02, ...,\n",
      "         -2.08740234e-02,  3.00598145e-03,  1.54113770e-03],\n",
      "        [ 6.65283203e-03, -7.62939453e-03,  1.91650391e-02, ...,\n",
      "         -1.17187500e-02,  2.37226486e-05, -1.78222656e-02],\n",
      "        [ 2.34375000e-02,  1.32446289e-02,  2.38037109e-02, ...,\n",
      "          2.12402344e-02,  2.20947266e-02,  2.64892578e-02]]],      dtype=float32)}}, 'post_self_attention_layer_norm': {'scale': Array([[0.05029297, 0.09960938, 0.13378906, ..., 0.46875   , 0.47851562,\n",
      "        0.43359375],\n",
      "       [0.05249023, 0.10058594, 0.13671875, ..., 0.47070312, 0.48828125,\n",
      "        0.4375    ],\n",
      "       [0.05004883, 0.09619141, 0.13574219, ..., 0.46679688, 0.47851562,\n",
      "        0.44140625],\n",
      "       ...,\n",
      "       [0.05249023, 0.10742188, 0.13574219, ..., 0.47265625, 0.48046875,\n",
      "        0.42382812],\n",
      "       [0.0534668 , 0.09960938, 0.13867188, ..., 0.47460938, 0.48242188,\n",
      "        0.41015625],\n",
      "       [0.04907227, 0.1015625 , 0.13574219, ..., 0.47265625, 0.47851562,\n",
      "        0.42773438]], dtype=float32)}, 'pre_self_attention_layer_norm': {'scale': Array([[0.02966309, 0.11376953, 0.17382812, ..., 0.52734375, 0.57421875,\n",
      "        0.48632812],\n",
      "       [0.01361084, 0.10986328, 0.17773438, ..., 0.5390625 , 0.58203125,\n",
      "        0.484375  ],\n",
      "       [0.00196838, 0.10058594, 0.17382812, ..., 0.53125   , 0.5625    ,\n",
      "        0.43554688],\n",
      "       ...,\n",
      "       [0.01025391, 0.06298828, 0.17675781, ..., 0.52734375, 0.55078125,\n",
      "        0.43164062],\n",
      "       [0.01098633, 0.09423828, 0.17089844, ..., 0.53515625, 0.5625    ,\n",
      "        0.45507812],\n",
      "       [0.006073  , 0.07421875, 0.17480469, ..., 0.5546875 , 0.58203125,\n",
      "        0.48046875]], dtype=float32)}, 'self_attention': {'key': {'kernel': Array([[[[-1.62353516e-02,  1.91650391e-02, -2.35595703e-02, ...,\n",
      "           2.58789062e-02, -2.73437500e-02,  3.27148438e-02],\n",
      "         [ 6.62231445e-03,  1.12304688e-02, -2.51464844e-02, ...,\n",
      "           9.15527344e-04, -6.62231445e-03, -8.62121582e-04],\n",
      "         [-1.55639648e-03, -2.86865234e-03, -5.15747070e-03, ...,\n",
      "           1.53541565e-04,  7.47680664e-04,  3.43322754e-03],\n",
      "         ...,\n",
      "         [ 1.41601562e-02, -5.18798828e-03,  3.14941406e-02, ...,\n",
      "          -2.35595703e-02,  7.04956055e-03, -1.36718750e-02],\n",
      "         [-6.25610352e-03, -3.46374512e-03, -4.85229492e-03, ...,\n",
      "          -1.77001953e-02,  1.87988281e-02,  1.94091797e-02],\n",
      "         [-2.41699219e-02, -1.68457031e-02, -2.03857422e-02, ...,\n",
      "           1.27563477e-02, -5.61523438e-03,  3.67736816e-03]],\n",
      "\n",
      "        [[-2.47802734e-02, -2.95410156e-02, -2.50244141e-02, ...,\n",
      "           3.00292969e-02, -1.95312500e-02,  1.72119141e-02],\n",
      "         [ 3.96728516e-03, -8.36181641e-03, -1.09252930e-02, ...,\n",
      "           4.15039062e-03, -7.47680664e-03,  7.50732422e-03],\n",
      "         [-7.26318359e-03, -1.03759766e-02, -1.41601562e-02, ...,\n",
      "           9.82666016e-03,  1.96838379e-03,  1.04980469e-02],\n",
      "         ...,\n",
      "         [-1.55029297e-02, -3.64685059e-03, -1.20239258e-02, ...,\n",
      "           2.50244141e-02, -9.88769531e-03,  2.16674805e-03],\n",
      "         [-1.90429688e-02, -6.43920898e-03,  2.52685547e-02, ...,\n",
      "          -9.39941406e-03,  1.50756836e-02,  4.30297852e-03],\n",
      "         [ 3.63769531e-02,  4.51660156e-03,  1.72119141e-02, ...,\n",
      "          -1.11083984e-02,  7.99560547e-03, -7.99560547e-03]],\n",
      "\n",
      "        [[-1.07574463e-03,  1.85546875e-02,  1.35498047e-02, ...,\n",
      "           3.22265625e-02,  3.22265625e-02,  4.32128906e-02],\n",
      "         [-3.14941406e-02, -8.97216797e-03, -9.58251953e-03, ...,\n",
      "          -1.59912109e-02,  6.56127930e-04, -6.98852539e-03],\n",
      "         [ 7.62939453e-03,  2.30407715e-03,  2.06298828e-02, ...,\n",
      "          -3.19824219e-02, -2.06298828e-02, -3.37219238e-03],\n",
      "         ...,\n",
      "         [ 4.85229492e-03,  2.86865234e-02,  4.49218750e-02, ...,\n",
      "          -1.45874023e-02,  7.99560547e-03,  1.91650391e-02],\n",
      "         [ 8.05664062e-03, -1.70898438e-02,  9.64355469e-03, ...,\n",
      "           1.47705078e-02, -2.90527344e-02, -1.30004883e-02],\n",
      "         [ 2.42614746e-03, -2.30712891e-02, -2.78472900e-04, ...,\n",
      "           3.39355469e-02, -6.10351562e-03,  5.88989258e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.80859375e-02, -1.61132812e-02,  1.83105469e-02, ...,\n",
      "          -5.58471680e-03, -4.11987305e-03, -2.74658203e-02],\n",
      "         [ 1.01318359e-02, -5.21850586e-03,  6.56127930e-03, ...,\n",
      "          -1.85546875e-02, -1.50756836e-02,  4.32128906e-02],\n",
      "         [ 4.57763672e-03,  4.95910645e-04, -5.46264648e-03, ...,\n",
      "          -3.49121094e-02, -1.19781494e-03,  6.01196289e-03],\n",
      "         ...,\n",
      "         [ 3.73535156e-02,  2.89916992e-03, -9.57489014e-04, ...,\n",
      "          -1.90429688e-02,  2.19726562e-02,  1.31835938e-02],\n",
      "         [-1.86767578e-02, -5.61523438e-03,  2.31933594e-03, ...,\n",
      "           4.94384766e-03,  1.47094727e-02, -5.58471680e-03],\n",
      "         [-2.49023438e-02,  1.02233887e-03,  1.36718750e-02, ...,\n",
      "          -3.46679688e-02,  3.11279297e-03,  1.59912109e-02]],\n",
      "\n",
      "        [[ 2.63671875e-02,  3.80859375e-02, -5.40161133e-03, ...,\n",
      "           4.56542969e-02,  1.07421875e-02,  1.68609619e-03],\n",
      "         [ 9.46044922e-03, -7.11059570e-03,  3.35693359e-03, ...,\n",
      "          -2.66113281e-02,  5.37109375e-03, -1.49536133e-02],\n",
      "         [-9.70458984e-03,  2.01416016e-02, -1.67846680e-03, ...,\n",
      "           8.85009766e-03, -2.79541016e-02,  1.95312500e-02],\n",
      "         ...,\n",
      "         [-1.08642578e-02, -1.47094727e-02,  1.40991211e-02, ...,\n",
      "          -6.98852539e-03, -2.24609375e-02, -5.15136719e-02],\n",
      "         [ 3.22265625e-02,  6.58035278e-05,  4.63867188e-03, ...,\n",
      "          -5.05371094e-02, -2.34375000e-02,  5.92041016e-03],\n",
      "         [ 7.17163086e-03, -6.79016113e-04,  1.22680664e-02, ...,\n",
      "           7.41577148e-03,  1.11694336e-02, -3.90625000e-02]],\n",
      "\n",
      "        [[-1.26342773e-02,  2.50244141e-02,  9.09423828e-03, ...,\n",
      "          -2.29492188e-02, -3.00292969e-02, -5.43212891e-03],\n",
      "         [ 1.79443359e-02,  2.40478516e-02,  1.86767578e-02, ...,\n",
      "          -1.62353516e-02,  8.48388672e-03,  2.27050781e-02],\n",
      "         [-5.92041016e-03, -3.22265625e-02,  2.41699219e-02, ...,\n",
      "           1.22070312e-02,  4.33349609e-03,  1.28173828e-03],\n",
      "         ...,\n",
      "         [-2.79541016e-02, -2.89306641e-02,  8.54492188e-03, ...,\n",
      "          -5.03540039e-03, -2.08740234e-02,  3.32031250e-02],\n",
      "         [ 3.71093750e-02, -5.10253906e-02, -1.17187500e-02, ...,\n",
      "          -3.03955078e-02, -2.91748047e-02, -3.85742188e-02],\n",
      "         [-1.80664062e-02, -2.77099609e-02, -6.40869141e-03, ...,\n",
      "           2.36816406e-02,  1.58691406e-02, -7.71484375e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 7.87353516e-03,  1.54876709e-03, -2.17285156e-02, ...,\n",
      "           1.22680664e-02, -9.15527344e-03,  1.37939453e-02],\n",
      "         [ 2.71606445e-03,  1.25885010e-03,  1.09863281e-03, ...,\n",
      "           7.08007812e-03, -2.24609375e-02,  1.04980469e-02],\n",
      "         [-4.48608398e-03, -9.61303711e-04, -8.43048096e-04, ...,\n",
      "           3.67736816e-03,  3.79943848e-03,  3.38745117e-03],\n",
      "         ...,\n",
      "         [-9.76562500e-03, -1.03759766e-02, -1.01318359e-02, ...,\n",
      "           1.45874023e-02,  2.08740234e-02,  1.29394531e-02],\n",
      "         [ 2.29492188e-02, -5.40161133e-03,  3.41796875e-02, ...,\n",
      "          -1.41906738e-03, -6.06536865e-04, -1.21307373e-03],\n",
      "         [ 1.26953125e-02,  3.25012207e-03,  4.99725342e-04, ...,\n",
      "          -7.24792480e-04,  1.73339844e-02, -2.10571289e-03]],\n",
      "\n",
      "        [[-2.50244141e-03,  4.57763672e-03,  2.94189453e-02, ...,\n",
      "          -7.38525391e-03, -3.12805176e-03,  1.48010254e-03],\n",
      "         [ 4.97436523e-03, -9.84191895e-04, -1.11083984e-02, ...,\n",
      "           9.39941406e-03, -1.25122070e-02, -3.12805176e-03],\n",
      "         [-9.88769531e-03,  2.76184082e-03, -1.62353516e-02, ...,\n",
      "          -1.48010254e-03,  1.48315430e-02,  1.37329102e-02],\n",
      "         ...,\n",
      "         [-4.54711914e-03,  1.58691406e-02, -1.04370117e-02, ...,\n",
      "          -3.34472656e-02,  2.20947266e-02, -2.73437500e-02],\n",
      "         [ 5.46264648e-03,  1.44042969e-02, -2.06298828e-02, ...,\n",
      "           6.73828125e-02, -3.10058594e-02,  2.61230469e-02],\n",
      "         [ 1.55029297e-02, -2.64892578e-02, -3.17382812e-02, ...,\n",
      "           1.89208984e-02, -1.91650391e-02,  1.47094727e-02]],\n",
      "\n",
      "        [[ 8.66699219e-03, -1.17797852e-02,  1.28173828e-02, ...,\n",
      "          -3.61328125e-02,  3.01513672e-02, -1.90429688e-02],\n",
      "         [ 1.33056641e-02, -1.03759766e-03, -1.06201172e-02, ...,\n",
      "           9.64355469e-03, -3.38745117e-03,  4.94384766e-03],\n",
      "         [ 4.30297852e-03, -4.60815430e-03, -4.56542969e-02, ...,\n",
      "           6.62231445e-03, -4.80957031e-02,  1.01318359e-02],\n",
      "         ...,\n",
      "         [ 1.16577148e-02,  1.03149414e-02,  6.86645508e-05, ...,\n",
      "          -3.50952148e-04, -4.54101562e-02,  4.68750000e-02],\n",
      "         [ 1.63574219e-02,  1.56402588e-03, -3.11279297e-03, ...,\n",
      "           2.66113281e-02, -7.17773438e-02, -5.63964844e-02],\n",
      "         [ 7.11059570e-03,  1.03149414e-02,  4.10156250e-02, ...,\n",
      "           4.90722656e-02, -1.23291016e-02, -1.56250000e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.04370117e-02,  2.16064453e-02,  8.85009766e-03, ...,\n",
      "           2.12402344e-02, -2.18200684e-03, -2.82287598e-03],\n",
      "         [-1.92871094e-02,  1.91650391e-02,  1.40380859e-02, ...,\n",
      "           2.18505859e-02, -2.33650208e-04,  1.15356445e-02],\n",
      "         [ 1.79443359e-02,  1.19628906e-02,  8.97216797e-03, ...,\n",
      "           4.41894531e-02, -1.55029297e-02,  9.94873047e-03],\n",
      "         ...,\n",
      "         [-1.78222656e-02,  8.42285156e-03,  2.64892578e-02, ...,\n",
      "          -1.06811523e-02, -1.64794922e-03, -2.59399414e-03],\n",
      "         [ 3.02734375e-02, -5.31005859e-03, -1.48315430e-02, ...,\n",
      "          -2.83203125e-02,  1.61743164e-03,  7.99560547e-03],\n",
      "         [ 2.45361328e-02, -5.43212891e-03,  2.00195312e-02, ...,\n",
      "           3.78417969e-02, -1.70898438e-02, -9.52148438e-03]],\n",
      "\n",
      "        [[-2.23388672e-02,  1.57470703e-02, -1.17187500e-02, ...,\n",
      "          -1.85546875e-02,  2.73132324e-03,  2.64892578e-02],\n",
      "         [-6.46972656e-03, -2.35595703e-02, -2.18505859e-02, ...,\n",
      "          -1.04370117e-02, -1.78222656e-02, -1.58691406e-02],\n",
      "         [ 2.24304199e-03,  5.67626953e-03,  1.31225586e-02, ...,\n",
      "           2.18505859e-02,  4.58984375e-02,  2.45361328e-02],\n",
      "         ...,\n",
      "         [ 1.81884766e-02,  6.62231445e-03, -1.25122070e-02, ...,\n",
      "          -3.27148438e-02,  3.07617188e-02,  7.47680664e-03],\n",
      "         [-1.94091797e-02,  3.32031250e-02, -6.34765625e-02, ...,\n",
      "          -7.99560547e-03,  1.09863281e-02,  9.46044922e-03],\n",
      "         [-8.11767578e-03, -5.18798828e-03,  1.11083984e-02, ...,\n",
      "           2.61230469e-02, -6.54296875e-02, -1.34277344e-02]],\n",
      "\n",
      "        [[-2.18505859e-02, -1.25122070e-02,  1.83105469e-03, ...,\n",
      "          -1.89208984e-02,  3.88183594e-02,  2.81982422e-02],\n",
      "         [-5.52368164e-03,  1.77001953e-02, -2.44140625e-02, ...,\n",
      "           1.92871094e-02, -1.60217285e-03,  1.22680664e-02],\n",
      "         [ 2.51770020e-03,  2.03857422e-02, -1.34887695e-02, ...,\n",
      "           6.50024414e-03, -2.31933594e-02,  2.85644531e-02],\n",
      "         ...,\n",
      "         [-1.94091797e-02,  1.01318359e-02,  6.46972656e-03, ...,\n",
      "          -9.21630859e-03,  7.01904297e-04, -8.54492188e-03],\n",
      "         [ 1.67236328e-02, -1.83105469e-02, -2.47802734e-02, ...,\n",
      "           3.90625000e-03, -1.67236328e-02,  7.27539062e-02],\n",
      "         [ 3.22265625e-02, -3.12805176e-03,  1.20849609e-02, ...,\n",
      "          -3.29589844e-02,  1.72119141e-02, -3.19824219e-02]]],\n",
      "\n",
      "\n",
      "       [[[-1.26647949e-03,  3.64685059e-03,  1.68609619e-03, ...,\n",
      "          -2.30407715e-03,  7.93457031e-04, -3.47900391e-03],\n",
      "         [ 4.17709351e-04, -4.36782837e-04,  1.24931335e-04, ...,\n",
      "           2.48718262e-03, -1.28173828e-03,  2.77709961e-03],\n",
      "         [ 5.45501709e-04,  1.27410889e-03,  1.24359131e-03, ...,\n",
      "          -4.33349609e-03, -3.90625000e-03, -2.18200684e-03],\n",
      "         ...,\n",
      "         [-8.01086426e-04,  4.24194336e-03, -1.51062012e-03, ...,\n",
      "          -3.93676758e-03, -6.16455078e-03, -4.55975533e-06],\n",
      "         [-1.41143799e-03,  5.24902344e-03, -3.63159180e-03, ...,\n",
      "           3.64685059e-03, -2.42614746e-03, -2.27355957e-03],\n",
      "         [-1.57928467e-03, -1.06811523e-03, -1.28173828e-03, ...,\n",
      "          -8.04901123e-04, -3.21960449e-03,  1.28173828e-03]],\n",
      "\n",
      "        [[ 3.83300781e-02, -1.13525391e-02, -6.49414062e-02, ...,\n",
      "          -1.74713135e-03,  6.50024414e-03, -3.61328125e-02],\n",
      "         [-3.29589844e-02,  2.00195312e-02,  3.58886719e-02, ...,\n",
      "          -6.59179688e-03,  3.87573242e-03, -5.03540039e-03],\n",
      "         [-6.65283203e-03,  1.34887695e-02,  1.31835938e-02, ...,\n",
      "          -1.25732422e-02, -1.55639648e-03,  3.12805176e-03],\n",
      "         ...,\n",
      "         [-7.72094727e-03,  1.37939453e-02, -1.02519989e-05, ...,\n",
      "          -7.65991211e-03,  7.59887695e-03, -1.63269043e-03],\n",
      "         [ 1.12915039e-02,  1.15966797e-02, -6.50024414e-03, ...,\n",
      "           1.45263672e-02,  1.98364258e-03,  3.14941406e-02],\n",
      "         [ 1.24511719e-02,  1.40380859e-02,  2.40478516e-02, ...,\n",
      "          -1.50299072e-03,  3.99780273e-03,  7.17163086e-04]],\n",
      "\n",
      "        [[-7.08007812e-03, -4.36401367e-03,  3.54003906e-02, ...,\n",
      "          -2.18505859e-02, -1.62506104e-03,  4.19921875e-02],\n",
      "         [-2.44140625e-02,  1.85546875e-02,  9.52148438e-03, ...,\n",
      "           7.43865967e-04, -2.02941895e-03,  1.10473633e-02],\n",
      "         [-1.97753906e-02,  1.54418945e-02,  2.42919922e-02, ...,\n",
      "          -4.83398438e-02,  1.05590820e-02, -6.10351562e-03],\n",
      "         ...,\n",
      "         [ 2.64892578e-02,  1.61132812e-02, -1.50756836e-02, ...,\n",
      "          -7.75146484e-03, -4.91333008e-03,  3.90625000e-03],\n",
      "         [-1.55639648e-03,  1.18408203e-02, -2.75878906e-02, ...,\n",
      "          -7.72094727e-03, -2.85644531e-02,  3.22265625e-02],\n",
      "         [-7.99560547e-03,  4.10156250e-02,  6.67572021e-04, ...,\n",
      "          -2.84423828e-02, -6.67572021e-04, -4.45556641e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.68457031e-02, -2.30407715e-03,  1.87988281e-02, ...,\n",
      "           3.12500000e-02,  1.32446289e-02,  2.60009766e-02],\n",
      "         [-3.71093750e-02,  1.67236328e-02,  3.17382812e-02, ...,\n",
      "           2.49023438e-02, -3.75976562e-02, -2.13623047e-02],\n",
      "         [ 1.15203857e-03,  3.73840332e-03,  5.52368164e-03, ...,\n",
      "           7.01904297e-04, -9.76562500e-03,  2.25830078e-03],\n",
      "         ...,\n",
      "         [ 2.14843750e-02, -2.79235840e-03, -1.33666992e-02, ...,\n",
      "          -4.76074219e-03, -7.38525391e-03, -4.42504883e-03],\n",
      "         [ 3.64685059e-03, -8.05664062e-03, -5.18798828e-03, ...,\n",
      "          -7.17163086e-03,  9.94873047e-03, -3.87573242e-03],\n",
      "         [-6.67572021e-04,  6.01196289e-03,  1.40991211e-02, ...,\n",
      "           5.73730469e-03, -6.88476562e-02, -5.61523438e-02]],\n",
      "\n",
      "        [[ 3.77655029e-04,  4.51660156e-03,  2.49023438e-02, ...,\n",
      "           3.08837891e-02,  1.59912109e-02, -5.24902344e-02],\n",
      "         [-1.31225586e-03, -5.92041016e-03,  9.15527344e-03, ...,\n",
      "           2.25830078e-02,  1.91650391e-02, -1.40380859e-02],\n",
      "         [-4.80651855e-04,  9.76562500e-03,  1.78222656e-02, ...,\n",
      "           1.88446045e-03,  4.54711914e-03, -2.29492188e-02],\n",
      "         ...,\n",
      "         [ 1.63574219e-02, -1.58691406e-02,  3.58886719e-02, ...,\n",
      "           6.98852539e-03,  9.09423828e-03,  1.63574219e-02],\n",
      "         [-3.95507812e-02,  1.40991211e-02, -1.40991211e-02, ...,\n",
      "          -2.07519531e-02,  8.48388672e-03, -2.70996094e-02],\n",
      "         [ 7.87353516e-03, -7.87353516e-03, -1.46484375e-02, ...,\n",
      "          -3.24249268e-04,  3.66210938e-02,  1.83105469e-02]],\n",
      "\n",
      "        [[ 1.36108398e-02,  1.26953125e-02, -8.78906250e-03, ...,\n",
      "          -2.01416016e-02, -7.41577148e-03,  1.38549805e-02],\n",
      "         [ 2.47802734e-02, -1.78222656e-02,  2.60009766e-02, ...,\n",
      "           1.23291016e-02,  1.90429688e-02, -2.41699219e-02],\n",
      "         [-2.44140625e-02, -3.12500000e-02,  7.20214844e-03, ...,\n",
      "          -7.87353516e-03, -1.89208984e-02, -3.93676758e-03],\n",
      "         ...,\n",
      "         [ 9.76562500e-03,  1.81884766e-02,  2.22167969e-02, ...,\n",
      "           2.55126953e-02, -1.98974609e-02,  2.97851562e-02],\n",
      "         [ 3.06396484e-02,  4.15039062e-03, -4.55856323e-04, ...,\n",
      "           6.34765625e-02, -4.88281250e-02,  7.23266602e-03],\n",
      "         [-6.80541992e-03, -1.32446289e-02,  2.17285156e-02, ...,\n",
      "          -2.33154297e-02,  1.69677734e-02,  1.08642578e-02]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 1.66015625e-02, -2.11181641e-02,  1.50146484e-02, ...,\n",
      "          -1.95312500e-02,  1.61132812e-02, -2.45361328e-02],\n",
      "         [ 2.31933594e-03, -7.78198242e-04, -1.24359131e-03, ...,\n",
      "          -1.47705078e-02,  8.23974609e-03, -1.35498047e-02],\n",
      "         [ 6.28662109e-03, -5.34057617e-04, -7.11059570e-03, ...,\n",
      "           1.30615234e-02,  1.20239258e-02,  1.03149414e-02],\n",
      "         ...,\n",
      "         [ 1.36566162e-03, -1.55639648e-02, -9.70458984e-03, ...,\n",
      "           4.66918945e-03, -6.07299805e-03, -7.87353516e-03],\n",
      "         [ 3.54003906e-03, -8.97216797e-03,  4.18090820e-03, ...,\n",
      "           6.37054443e-04, -6.28662109e-03, -6.50024414e-03],\n",
      "         [-1.51062012e-03, -2.70080566e-03, -5.41687012e-04, ...,\n",
      "           2.39372253e-04, -3.23486328e-03,  6.98852539e-03]],\n",
      "\n",
      "        [[ 1.79443359e-02, -1.58691406e-02,  3.49121094e-02, ...,\n",
      "           2.29492188e-02,  3.41796875e-03, -1.77001953e-02],\n",
      "         [ 3.00598145e-03,  2.97546387e-03,  1.00708008e-02, ...,\n",
      "          -3.95507812e-02, -1.12915039e-02, -1.36718750e-02],\n",
      "         [ 2.42919922e-02, -6.68334961e-03, -2.42614746e-03, ...,\n",
      "          -5.52368164e-03, -5.12695312e-03, -9.39941406e-03],\n",
      "         ...,\n",
      "         [ 1.62353516e-02,  1.27563477e-02, -1.63574219e-02, ...,\n",
      "           8.11767578e-03,  6.90460205e-04, -1.02539062e-02],\n",
      "         [ 1.31225586e-03,  1.58691406e-02,  2.96020508e-03, ...,\n",
      "           5.43212891e-03, -1.00708008e-02, -3.61328125e-02],\n",
      "         [-1.36718750e-02, -1.19018555e-02,  3.20434570e-03, ...,\n",
      "           1.09252930e-02, -1.21459961e-02,  3.99780273e-03]],\n",
      "\n",
      "        [[-1.80664062e-02,  1.70898438e-02, -1.34887695e-02, ...,\n",
      "          -2.66113281e-02, -1.98974609e-02,  6.83593750e-03],\n",
      "         [ 1.12304688e-02,  5.85937500e-03,  7.44628906e-03, ...,\n",
      "           6.43920898e-03,  1.45721436e-03,  9.38415527e-04],\n",
      "         [ 1.33056641e-02, -5.07812500e-02,  2.51464844e-02, ...,\n",
      "           2.63671875e-02,  2.05078125e-02,  6.80541992e-03],\n",
      "         ...,\n",
      "         [-1.50756836e-02,  1.95312500e-02,  1.16577148e-02, ...,\n",
      "          -3.41796875e-03,  4.54101562e-02, -5.09643555e-03],\n",
      "         [-1.43432617e-02,  1.59912109e-02, -3.12500000e-02, ...,\n",
      "           5.02929688e-02, -2.07519531e-02,  2.61230469e-02],\n",
      "         [-1.06811523e-02, -3.83300781e-02,  3.06396484e-02, ...,\n",
      "           3.05175781e-02,  4.11987305e-03, -8.30078125e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.98364258e-03, -2.18505859e-02,  1.80664062e-02, ...,\n",
      "           4.22363281e-02,  1.36108398e-02, -5.55419922e-03],\n",
      "         [-9.33837891e-03,  3.95507812e-02,  4.78744507e-04, ...,\n",
      "          -4.19921875e-02,  3.06396484e-02, -3.47900391e-03],\n",
      "         [ 1.90734863e-03,  3.05175781e-03, -1.68457031e-02, ...,\n",
      "          -4.41894531e-02, -8.78906250e-03,  1.87683105e-03],\n",
      "         ...,\n",
      "         [-1.81884766e-02, -2.00195312e-02,  2.29492188e-02, ...,\n",
      "          -2.25830078e-03,  2.16064453e-02,  9.76562500e-03],\n",
      "         [-4.79125977e-03,  1.55639648e-02,  5.37109375e-03, ...,\n",
      "           1.56402588e-03,  6.46972656e-03,  3.71093750e-02],\n",
      "         [ 2.56347656e-03, -8.54492188e-03,  7.44628906e-03, ...,\n",
      "          -6.71386719e-03, -1.05590820e-02, -5.95703125e-02]],\n",
      "\n",
      "        [[-9.58251953e-03, -3.73840332e-03, -1.16729736e-03, ...,\n",
      "          -3.41796875e-02,  2.06298828e-02, -2.16484070e-04],\n",
      "         [-1.15356445e-02,  1.68457031e-02, -1.56250000e-02, ...,\n",
      "           1.74560547e-02,  2.63671875e-02, -1.70898438e-02],\n",
      "         [-1.34277344e-02,  1.61743164e-03,  2.44140625e-03, ...,\n",
      "          -4.17480469e-02, -1.53198242e-02,  5.98144531e-03],\n",
      "         ...,\n",
      "         [ 6.67572021e-04,  3.36914062e-02, -1.27563477e-02, ...,\n",
      "          -1.14135742e-02,  2.53906250e-02,  4.66918945e-03],\n",
      "         [-1.73950195e-03,  6.68334961e-03, -2.62451172e-02, ...,\n",
      "          -3.97949219e-02, -1.68457031e-02, -1.66015625e-02],\n",
      "         [ 7.53784180e-03, -1.14746094e-02,  1.49536133e-03, ...,\n",
      "           5.51757812e-02,  6.22558594e-03,  3.23486328e-03]],\n",
      "\n",
      "        [[ 6.56127930e-03,  8.54492188e-03, -2.79235840e-03, ...,\n",
      "          -1.95312500e-02,  1.67236328e-02, -5.95092773e-04],\n",
      "         [ 6.89697266e-03,  2.52685547e-02, -3.58886719e-02, ...,\n",
      "          -2.29492188e-02, -2.51464844e-02,  5.64575195e-03],\n",
      "         [ 2.92968750e-03,  1.56250000e-02,  2.77709961e-03, ...,\n",
      "           1.17797852e-02,  1.62353516e-02,  1.06811523e-02],\n",
      "         ...,\n",
      "         [-1.90429688e-02, -1.61132812e-02,  5.09643555e-03, ...,\n",
      "          -1.43432617e-03,  2.79541016e-02,  4.19921875e-02],\n",
      "         [ 1.34887695e-02, -4.02832031e-02,  7.87353516e-03, ...,\n",
      "          -1.03149414e-02,  2.35595703e-02,  3.54003906e-02],\n",
      "         [ 4.97436523e-03, -2.35595703e-02,  1.44958496e-03, ...,\n",
      "          -1.21459961e-02, -2.29492188e-02, -1.04980469e-02]]],\n",
      "\n",
      "\n",
      "       [[[-9.94873047e-03,  1.51977539e-02, -1.64794922e-02, ...,\n",
      "           1.34887695e-02, -7.99560547e-03,  1.30615234e-02],\n",
      "         [ 4.59671021e-04,  2.22778320e-03, -4.95910645e-04, ...,\n",
      "           9.27734375e-03, -9.03320312e-03,  1.11694336e-02],\n",
      "         [-4.25338745e-04,  2.27355957e-03, -2.60925293e-03, ...,\n",
      "           8.36181641e-03,  7.11059570e-03,  4.21142578e-03],\n",
      "         ...,\n",
      "         [-7.04956055e-03,  1.12304688e-02, -1.09863281e-02, ...,\n",
      "          -1.00708008e-02, -1.68457031e-02, -2.79541016e-02],\n",
      "         [-4.27246094e-03,  2.00195312e-02, -1.40991211e-02, ...,\n",
      "           1.64031982e-03, -1.12533569e-04,  5.29289246e-05],\n",
      "         [ 6.98852539e-03,  4.91333008e-03,  6.07299805e-03, ...,\n",
      "           3.05175781e-03,  1.14746094e-02, -1.14746094e-02]],\n",
      "\n",
      "        [[ 2.07519531e-02,  9.39941406e-03,  7.87353516e-03, ...,\n",
      "           6.59179688e-03,  2.28881836e-03, -1.48925781e-02],\n",
      "         [-5.21850586e-03, -2.05078125e-02,  1.78222656e-02, ...,\n",
      "           5.31005859e-03,  9.39941406e-03,  5.37109375e-03],\n",
      "         [-9.03320312e-03, -3.00292969e-02, -1.56250000e-02, ...,\n",
      "          -9.33837891e-03,  1.53198242e-02, -9.88769531e-03],\n",
      "         ...,\n",
      "         [ 2.84423828e-02,  1.10473633e-02, -3.03649902e-03, ...,\n",
      "           1.20239258e-02, -2.00195312e-02,  8.54492188e-03],\n",
      "         [-3.93066406e-02, -1.26342773e-02,  3.14941406e-02, ...,\n",
      "           1.01318359e-02,  9.72747803e-04, -1.79290771e-03],\n",
      "         [-7.04956055e-03,  1.87988281e-02, -2.16674805e-03, ...,\n",
      "           6.07967377e-05, -1.46484375e-03, -1.56402588e-03]],\n",
      "\n",
      "        [[-7.23266602e-03, -6.71386719e-03,  1.07421875e-02, ...,\n",
      "          -3.44238281e-02, -2.83203125e-02, -4.27246094e-02],\n",
      "         [ 1.06201172e-02, -4.05883789e-03,  2.55126953e-02, ...,\n",
      "           1.03149414e-02, -2.11715698e-04,  2.16674805e-03],\n",
      "         [-1.92871094e-02, -2.97851562e-02,  2.81982422e-02, ...,\n",
      "           1.42822266e-02, -2.50244141e-02, -1.62353516e-02],\n",
      "         ...,\n",
      "         [ 2.75878906e-02,  2.63671875e-02, -1.04370117e-02, ...,\n",
      "          -3.09753418e-03,  3.12500000e-02,  8.78906250e-03],\n",
      "         [-2.02636719e-02, -3.49426270e-03, -1.32446289e-02, ...,\n",
      "           7.66601562e-02, -1.89208984e-02,  6.17980957e-04],\n",
      "         [ 5.03540039e-03, -2.09960938e-02,  2.94189453e-02, ...,\n",
      "          -4.00390625e-02,  1.31988525e-03, -2.78320312e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.80541992e-03,  8.72802734e-03,  2.50244141e-02, ...,\n",
      "          -1.50756836e-02, -2.16064453e-02,  1.05590820e-02],\n",
      "         [ 9.76562500e-03,  2.60009766e-02,  1.57470703e-02, ...,\n",
      "           7.69042969e-03, -1.55639648e-02,  3.93066406e-02],\n",
      "         [-1.17187500e-02, -7.08007812e-03,  3.32641602e-03, ...,\n",
      "          -1.19628906e-02,  2.30712891e-02, -7.87353516e-03],\n",
      "         ...,\n",
      "         [ 1.30615234e-02,  2.39257812e-02, -1.51977539e-02, ...,\n",
      "           1.19781494e-03, -4.94384766e-03, -1.35498047e-02],\n",
      "         [-2.84423828e-02,  6.07299805e-03, -2.24609375e-02, ...,\n",
      "           2.07519531e-02,  3.83300781e-02,  5.98144531e-02],\n",
      "         [-6.68334961e-03,  5.49316406e-03,  3.44238281e-02, ...,\n",
      "          -3.99780273e-03, -1.10473633e-02, -2.39257812e-02]],\n",
      "\n",
      "        [[ 1.31835938e-02, -7.35473633e-03, -8.91113281e-03, ...,\n",
      "           3.83300781e-02, -1.44042969e-02,  3.02734375e-02],\n",
      "         [-7.20214844e-03,  4.48608398e-03,  2.21252441e-03, ...,\n",
      "           2.81982422e-02,  8.11767578e-03, -1.97753906e-02],\n",
      "         [ 8.48388672e-03, -1.36108398e-02,  2.38037109e-03, ...,\n",
      "           2.12402344e-02, -4.07714844e-02,  3.80859375e-02],\n",
      "         ...,\n",
      "         [-1.83105469e-02,  5.37109375e-03, -6.04248047e-03, ...,\n",
      "          -3.22265625e-02, -4.39453125e-03, -7.11059570e-03],\n",
      "         [ 3.44848633e-03, -5.46264648e-03, -9.09423828e-03, ...,\n",
      "          -3.17382812e-02,  5.07812500e-02, -1.14746094e-02],\n",
      "         [ 2.49023438e-02, -2.69775391e-02,  1.77001953e-02, ...,\n",
      "           4.60815430e-03, -9.27734375e-03,  1.36718750e-02]],\n",
      "\n",
      "        [[-2.28881836e-03, -2.95639038e-04,  8.36181641e-03, ...,\n",
      "           1.63574219e-02,  1.49536133e-03, -3.00292969e-02],\n",
      "         [-6.22558594e-03,  2.77099609e-02, -2.53906250e-02, ...,\n",
      "           1.35040283e-03, -3.95507812e-02, -1.77001953e-02],\n",
      "         [ 1.51062012e-03,  4.39453125e-03,  2.83813477e-03, ...,\n",
      "          -3.24707031e-02, -4.49218750e-02,  2.56347656e-02],\n",
      "         ...,\n",
      "         [ 4.73022461e-03, -1.17187500e-02, -2.42919922e-02, ...,\n",
      "           6.50024414e-03, -2.07519531e-02,  2.86865234e-02],\n",
      "         [ 2.09045410e-03,  2.30712891e-02,  9.58442688e-05, ...,\n",
      "          -9.03320312e-03,  4.56542969e-02,  6.39648438e-02],\n",
      "         [ 3.99780273e-03,  2.63671875e-02, -1.16577148e-02, ...,\n",
      "          -1.33056641e-02, -2.16064453e-02, -3.64303589e-04]]],\n",
      "\n",
      "\n",
      "       [[[-1.35498047e-02,  2.34375000e-02, -1.17797852e-02, ...,\n",
      "           1.32446289e-02, -1.23901367e-02,  1.20849609e-02],\n",
      "         [ 1.05285645e-03,  1.74713135e-03, -3.35693359e-03, ...,\n",
      "          -4.82177734e-03,  1.64794922e-02, -8.23974609e-03],\n",
      "         [-2.05993652e-03,  1.96838379e-03,  4.24194336e-03, ...,\n",
      "          -8.85009766e-03, -1.32446289e-02, -1.40991211e-02],\n",
      "         ...,\n",
      "         [ 8.05664062e-03,  1.93786621e-03, -7.35473633e-03, ...,\n",
      "          -1.08032227e-02, -1.23901367e-02, -2.07519531e-02],\n",
      "         [-1.02539062e-02,  2.07519531e-02, -1.96533203e-02, ...,\n",
      "          -1.44195557e-03,  8.05664062e-03,  8.60595703e-03],\n",
      "         [-1.50680542e-04,  3.71932983e-05,  2.97546387e-03, ...,\n",
      "           8.11767578e-03, -1.10473633e-02,  9.46044922e-03]],\n",
      "\n",
      "        [[-9.64355469e-03, -5.88378906e-02, -5.63964844e-02, ...,\n",
      "          -1.46484375e-02,  8.91113281e-03, -4.76074219e-03],\n",
      "         [ 2.57568359e-02, -2.79541016e-02, -1.77001953e-02, ...,\n",
      "          -1.36718750e-02,  2.18505859e-02,  3.49426270e-03],\n",
      "         [ 7.69042969e-03, -1.66015625e-02,  6.80541992e-03, ...,\n",
      "           6.25610352e-03, -6.83593750e-03,  3.29589844e-03],\n",
      "         ...,\n",
      "         [-8.39233398e-04, -2.65502930e-03,  2.50244141e-02, ...,\n",
      "          -7.41577148e-03,  1.90734863e-03, -1.03759766e-02],\n",
      "         [-1.49536133e-02,  1.68457031e-02, -2.03857422e-02, ...,\n",
      "          -9.33837891e-03, -1.36718750e-02, -7.99560547e-03],\n",
      "         [-3.56445312e-02,  1.63269043e-03, -9.82666016e-03, ...,\n",
      "           6.71386719e-03, -6.50024414e-03,  6.04248047e-03]],\n",
      "\n",
      "        [[ 1.03759766e-02, -9.23156738e-04,  5.24902344e-03, ...,\n",
      "           9.64355469e-03,  1.33056641e-02, -1.04980469e-02],\n",
      "         [ 2.19726562e-02,  8.11767578e-03,  1.11694336e-02, ...,\n",
      "           5.88989258e-03, -2.60925293e-03,  1.09672546e-04],\n",
      "         [-8.17871094e-03, -1.55029297e-02,  6.86645508e-03, ...,\n",
      "          -4.80957031e-02, -5.98144531e-02,  4.18090820e-03],\n",
      "         ...,\n",
      "         [ 6.17980957e-04,  5.27954102e-03,  1.57470703e-02, ...,\n",
      "           1.22680664e-02, -1.59912109e-02, -1.67236328e-02],\n",
      "         [ 4.51660156e-03, -2.13623047e-02,  7.53784180e-03, ...,\n",
      "          -2.22167969e-02,  7.24792480e-04,  3.41796875e-03],\n",
      "         [ 2.08740234e-02,  1.15356445e-02,  2.11181641e-02, ...,\n",
      "          -6.22558594e-03, -3.96728516e-03,  8.64257812e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.56347656e-03, -2.74658203e-02,  1.42669678e-03, ...,\n",
      "           2.60009766e-02,  4.34570312e-02, -1.03759766e-03],\n",
      "         [ 3.58886719e-02, -8.91113281e-03,  1.46484375e-02, ...,\n",
      "          -1.49536133e-02,  1.32446289e-02, -2.25830078e-02],\n",
      "         [-1.52587891e-03, -1.31225586e-02, -8.54492188e-03, ...,\n",
      "           5.15136719e-02,  6.43920898e-03,  3.03955078e-02],\n",
      "         ...,\n",
      "         [-1.18408203e-02, -1.87988281e-02, -1.54418945e-02, ...,\n",
      "           6.95800781e-03, -9.03320312e-03,  2.07519531e-03],\n",
      "         [ 1.03759766e-02, -3.43322754e-05, -1.03759766e-02, ...,\n",
      "          -1.91497803e-03,  9.58251953e-03,  2.56347656e-02],\n",
      "         [ 3.68652344e-02,  1.23901367e-02,  6.14166260e-04, ...,\n",
      "          -1.05590820e-02, -3.90625000e-02,  4.91333008e-03]],\n",
      "\n",
      "        [[ 8.36181641e-03, -3.67164612e-05, -1.25885010e-03, ...,\n",
      "          -2.74658203e-02, -1.25732422e-02,  8.78906250e-03],\n",
      "         [ 4.60815430e-03,  7.56835938e-03,  2.13623047e-03, ...,\n",
      "           3.14941406e-02,  1.73339844e-02,  4.66918945e-03],\n",
      "         [ 8.39233398e-04,  6.80541992e-03,  2.07519531e-02, ...,\n",
      "           3.10058594e-02,  5.82885742e-03, -3.99780273e-03],\n",
      "         ...,\n",
      "         [ 1.91650391e-02,  1.89208984e-02, -3.43322754e-03, ...,\n",
      "          -6.17675781e-02, -6.22558594e-03,  4.39453125e-02],\n",
      "         [ 6.50024414e-03, -4.21142578e-03,  1.90429688e-02, ...,\n",
      "           7.41577148e-03,  3.49121094e-02,  1.59912109e-02],\n",
      "         [ 2.71606445e-03, -1.41601562e-02,  6.98852539e-03, ...,\n",
      "          -2.60925293e-03, -2.39257812e-02, -2.03857422e-02]],\n",
      "\n",
      "        [[-2.23388672e-02, -7.11059570e-03, -2.75878906e-02, ...,\n",
      "          -1.57470703e-02, -1.26342773e-02, -1.29394531e-02],\n",
      "         [ 1.68457031e-02,  9.21630859e-03, -5.61523438e-03, ...,\n",
      "           1.40380859e-02, -4.05883789e-03, -3.17382812e-02],\n",
      "         [ 8.91113281e-03, -2.77709961e-03,  1.13525391e-02, ...,\n",
      "           1.81884766e-02,  3.17382812e-02, -3.12805176e-03],\n",
      "         ...,\n",
      "         [ 1.92260742e-03, -3.05175781e-03,  1.69677734e-02, ...,\n",
      "          -1.83105469e-02,  4.69207764e-04, -4.37011719e-02],\n",
      "         [-2.72216797e-02,  1.74560547e-02,  4.73022461e-03, ...,\n",
      "           3.34167480e-03,  5.12695312e-02, -5.66406250e-02],\n",
      "         [-2.01416016e-02, -1.68457031e-02,  1.51367188e-02, ...,\n",
      "          -4.88281250e-02,  1.97753906e-02,  3.99780273e-03]]]],      dtype=float32)}, 'out': {'kernel': Array([[[[-1.62124634e-05,  2.76184082e-03,  2.33459473e-03, ...,\n",
      "           4.18090820e-03, -3.35693359e-03,  6.19506836e-03],\n",
      "         [-1.92260742e-03,  1.84631348e-03, -2.72750854e-04, ...,\n",
      "          -3.32641602e-03, -2.48718262e-03, -6.56127930e-04],\n",
      "         [ 4.88281250e-03, -1.29699707e-03,  9.26971436e-04, ...,\n",
      "           5.88989258e-03, -2.57873535e-03,  2.62451172e-03],\n",
      "         ...,\n",
      "         [ 2.27355957e-03, -1.18255615e-03,  4.65393066e-04, ...,\n",
      "           1.17492676e-03, -7.93457031e-03,  5.37109375e-03],\n",
      "         [ 4.15039062e-03, -5.64575195e-03,  2.36511230e-03, ...,\n",
      "          -3.78417969e-03, -2.33459473e-03, -3.20434570e-04],\n",
      "         [-3.17382812e-03,  4.54711914e-03, -1.15966797e-03, ...,\n",
      "          -1.00708008e-03,  3.78417969e-03, -3.96728516e-03]],\n",
      "\n",
      "        [[ 4.82177734e-03, -3.00598145e-03,  2.23388672e-02, ...,\n",
      "           3.31115723e-03,  2.18200684e-03, -4.33349609e-03],\n",
      "         [-1.94091797e-02, -5.43212891e-03,  1.25732422e-02, ...,\n",
      "           6.89697266e-03, -1.25122070e-02, -2.05078125e-02],\n",
      "         [ 1.42211914e-02, -1.31835938e-02, -1.19628906e-02, ...,\n",
      "           5.12695312e-03,  5.03540039e-03,  5.43212891e-03],\n",
      "         ...,\n",
      "         [-1.78222656e-02, -6.80541992e-03, -6.22558594e-03, ...,\n",
      "           2.22778320e-03, -9.09423828e-03, -3.49426270e-03],\n",
      "         [ 1.44653320e-02, -2.85644531e-02, -9.82666016e-03, ...,\n",
      "           7.93457031e-03,  2.11181641e-02,  7.44628906e-03],\n",
      "         [ 1.24511719e-02, -1.27563477e-02,  2.74658203e-03, ...,\n",
      "           1.20544434e-03, -7.26318359e-03, -1.14746094e-02]],\n",
      "\n",
      "        [[-1.82342529e-03, -9.53674316e-04, -2.03857422e-02, ...,\n",
      "          -7.23266602e-03,  2.05078125e-02, -1.19628906e-02],\n",
      "         [ 1.52587891e-02, -1.84326172e-02,  3.23486328e-03, ...,\n",
      "          -2.74658203e-02, -1.57470703e-02,  1.30462646e-03],\n",
      "         [ 9.21630859e-03,  5.88989258e-03, -1.44653320e-02, ...,\n",
      "           3.02734375e-02,  3.58886719e-02,  9.70458984e-03],\n",
      "         ...,\n",
      "         [ 5.55419922e-03, -2.95639038e-04,  1.82342529e-03, ...,\n",
      "           2.11181641e-02,  3.26538086e-03, -1.35498047e-02],\n",
      "         [ 8.30078125e-03,  3.39355469e-02, -9.88769531e-03, ...,\n",
      "           1.01318359e-02,  2.09960938e-02,  9.39941406e-03],\n",
      "         [ 4.88281250e-03,  2.73437500e-02, -7.38525391e-03, ...,\n",
      "          -1.17797852e-02, -1.44653320e-02, -7.59887695e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.57763672e-03, -2.02636719e-02,  2.29492188e-02, ...,\n",
      "          -2.94189453e-02, -1.10473633e-02,  1.19628906e-02],\n",
      "         [ 3.44238281e-02, -2.17285156e-02,  2.62451172e-02, ...,\n",
      "          -1.57470703e-02, -6.98852539e-03,  1.09863281e-02],\n",
      "         [ 3.22265625e-02, -9.39941406e-03, -1.22680664e-02, ...,\n",
      "           2.23388672e-02,  3.32031250e-02,  4.48608398e-03],\n",
      "         ...,\n",
      "         [ 7.14111328e-03,  2.89306641e-02, -4.30297852e-03, ...,\n",
      "           2.10571289e-03, -1.00708008e-02, -3.19824219e-02],\n",
      "         [ 1.53808594e-02, -1.43647194e-05,  1.77001953e-02, ...,\n",
      "          -7.82012939e-04, -1.12915039e-02,  2.72216797e-02],\n",
      "         [ 4.54711914e-03,  2.13623047e-03,  1.35498047e-02, ...,\n",
      "           2.94494629e-03,  5.27954102e-03,  2.59399414e-03]],\n",
      "\n",
      "        [[-6.74438477e-03, -2.02636719e-02, -1.87988281e-02, ...,\n",
      "           6.62231445e-03,  2.70996094e-02, -5.37109375e-03],\n",
      "         [ 1.01928711e-02, -1.98974609e-02, -3.21960449e-03, ...,\n",
      "           4.48608398e-03, -1.22680664e-02,  1.13525391e-02],\n",
      "         [-1.11694336e-02, -6.52313232e-04,  1.78222656e-02, ...,\n",
      "           3.40270996e-03,  1.30004883e-02,  4.27246094e-03],\n",
      "         ...,\n",
      "         [ 2.27050781e-02, -2.66113281e-02,  3.31115723e-03, ...,\n",
      "          -1.92871094e-02, -9.21630859e-03,  4.54711914e-03],\n",
      "         [ 1.89208984e-02, -1.94091797e-02, -1.66015625e-02, ...,\n",
      "           1.19018555e-02, -1.12915039e-02,  8.11767578e-03],\n",
      "         [-1.09863281e-02,  3.61633301e-03, -1.47094727e-02, ...,\n",
      "           7.75146484e-03, -1.28173828e-02,  7.69042969e-03]],\n",
      "\n",
      "        [[ 6.65283203e-03, -7.78198242e-03,  1.12915039e-02, ...,\n",
      "           7.50732422e-03,  1.80816650e-03,  8.05664062e-03],\n",
      "         [ 2.77099609e-02,  3.96728516e-03, -1.77383423e-04, ...,\n",
      "          -1.63574219e-02,  2.02636719e-02,  1.59912109e-02],\n",
      "         [-3.87573242e-03, -2.67333984e-02,  2.44140625e-03, ...,\n",
      "           2.72216797e-02, -2.12402344e-02, -1.03149414e-02],\n",
      "         ...,\n",
      "         [ 1.94091797e-02, -1.12304688e-02,  8.78906250e-03, ...,\n",
      "          -3.70025635e-04, -2.68554688e-03,  1.87988281e-02],\n",
      "         [-1.67236328e-02, -1.84631348e-03,  3.19824219e-02, ...,\n",
      "           3.11279297e-02,  3.61328125e-02, -1.28784180e-02],\n",
      "         [-1.42822266e-02,  2.74658203e-03,  2.22167969e-02, ...,\n",
      "           1.13525391e-02, -3.93676758e-03, -4.05883789e-03]]],\n",
      "\n",
      "\n",
      "       [[[ 2.60925293e-03,  3.40270996e-03,  8.29696655e-05, ...,\n",
      "          -1.85394287e-03, -2.79235840e-03,  7.08007812e-03],\n",
      "         [ 6.04248047e-03,  7.01904297e-03, -1.87873840e-04, ...,\n",
      "           3.79943848e-03, -2.60925293e-03,  5.06591797e-03],\n",
      "         [-1.11389160e-03,  2.50244141e-03,  3.11279297e-03, ...,\n",
      "          -3.60107422e-03,  1.25122070e-03,  5.70678711e-03],\n",
      "         ...,\n",
      "         [ 4.63867188e-03, -4.33349609e-03, -5.72204590e-04, ...,\n",
      "           5.67626953e-03, -7.93457031e-03,  3.75366211e-03],\n",
      "         [ 3.35693359e-03,  3.06701660e-03, -5.30242920e-04, ...,\n",
      "           1.48010254e-03,  3.63159180e-03,  1.39617920e-03],\n",
      "         [ 3.89099121e-03,  8.36181641e-03,  1.72424316e-03, ...,\n",
      "          -3.58581543e-03, -5.67626953e-03,  3.43322754e-03]],\n",
      "\n",
      "        [[-8.46862793e-04,  9.07897949e-04,  4.85229492e-03, ...,\n",
      "           1.53350830e-03,  6.58035278e-05,  1.22070312e-03],\n",
      "         [-1.10626221e-03,  1.43432617e-03,  7.62939453e-04, ...,\n",
      "           4.27246094e-03, -1.35803223e-03, -1.38854980e-03],\n",
      "         [-6.10351562e-04, -1.43432617e-03,  1.25885010e-03, ...,\n",
      "          -1.78527832e-03, -3.03649902e-03, -1.18255615e-03],\n",
      "         ...,\n",
      "         [ 3.14331055e-03,  3.92913818e-04,  3.03649902e-03, ...,\n",
      "          -1.05285645e-03, -3.92913818e-04, -1.45721436e-03],\n",
      "         [-4.10079956e-04, -1.03759766e-03, -5.76019287e-04, ...,\n",
      "          -4.91142273e-05, -2.16674805e-03, -1.51062012e-03],\n",
      "         [-1.86920166e-03, -4.11987305e-03, -1.74713135e-03, ...,\n",
      "          -1.42669678e-03, -2.27355957e-03,  1.24359131e-03]],\n",
      "\n",
      "        [[-1.02539062e-02, -2.34375000e-02,  6.71386719e-03, ...,\n",
      "           1.20849609e-02, -6.37817383e-03, -1.68457031e-02],\n",
      "         [ 7.08007812e-03, -9.52148438e-03, -8.05664062e-03, ...,\n",
      "          -9.64355469e-03,  7.43865967e-05,  1.96533203e-02],\n",
      "         [ 1.72119141e-02, -1.81884766e-02, -2.27050781e-02, ...,\n",
      "          -1.92260742e-03,  6.31713867e-03,  1.19018555e-02],\n",
      "         ...,\n",
      "         [ 1.09252930e-02, -5.40161133e-03, -9.82666016e-03, ...,\n",
      "           1.35498047e-02, -1.02996826e-03, -2.33459473e-03],\n",
      "         [-2.99072266e-02,  2.34375000e-02, -2.24304199e-03, ...,\n",
      "          -5.40161133e-03,  4.88281250e-03, -1.21459961e-02],\n",
      "         [-8.88824463e-04, -4.63867188e-03, -4.79125977e-03, ...,\n",
      "          -1.26953125e-02,  1.38549805e-02, -7.99560547e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.81250000e-03, -6.74438477e-03,  2.52685547e-02, ...,\n",
      "          -1.22833252e-03,  4.45556641e-03, -2.33154297e-02],\n",
      "         [-4.66918945e-03, -1.50146484e-02, -2.17285156e-02, ...,\n",
      "           3.68652344e-02, -1.28173828e-02, -3.73535156e-02],\n",
      "         [-1.57470703e-02,  2.68554688e-02, -6.04248047e-03, ...,\n",
      "           4.05273438e-02,  1.25732422e-02, -2.44140625e-02],\n",
      "         ...,\n",
      "         [-1.73339844e-02,  3.83300781e-02,  6.07299805e-03, ...,\n",
      "           1.08032227e-02,  1.75781250e-02, -7.87353516e-03],\n",
      "         [-3.60107422e-03, -6.95800781e-03, -1.98974609e-02, ...,\n",
      "          -1.23291016e-02,  2.02636719e-02,  2.38037109e-02],\n",
      "         [ 8.78906250e-03, -2.42919922e-02,  5.09643555e-03, ...,\n",
      "           1.48925781e-02, -1.87988281e-02, -9.39941406e-03]],\n",
      "\n",
      "        [[-7.20214844e-03, -9.27734375e-03, -9.76562500e-04, ...,\n",
      "           5.73730469e-03, -2.18505859e-02,  6.52313232e-04],\n",
      "         [-4.00390625e-02,  1.96533203e-02,  2.97546387e-03, ...,\n",
      "          -2.05078125e-02, -1.78222656e-02,  1.55639648e-02],\n",
      "         [ 5.82885742e-03,  4.80957031e-02, -2.13623047e-02, ...,\n",
      "           2.42919922e-02, -2.75878906e-02, -1.69677734e-02],\n",
      "         ...,\n",
      "         [ 4.24194336e-03,  3.34167480e-03,  2.31933594e-02, ...,\n",
      "           2.25830078e-02,  2.77099609e-02,  1.90429688e-02],\n",
      "         [ 3.39355469e-02,  3.24707031e-02, -5.56640625e-02, ...,\n",
      "           2.01416016e-02,  1.45874023e-02, -2.39257812e-02],\n",
      "         [ 1.35498047e-02,  3.51562500e-02,  1.06811523e-03, ...,\n",
      "           1.25122070e-02, -1.22680664e-02, -1.31835938e-02]],\n",
      "\n",
      "        [[-6.39648438e-02, -7.29370117e-03,  2.97851562e-02, ...,\n",
      "          -9.03320312e-03, -1.56250000e-02, -2.94189453e-02],\n",
      "         [-4.24804688e-02,  1.61132812e-02,  1.50756836e-02, ...,\n",
      "          -2.53906250e-02, -3.17382812e-02, -1.73339844e-02],\n",
      "         [ 1.88446045e-03, -1.75476074e-03, -4.66918945e-03, ...,\n",
      "          -3.27148438e-02,  1.92871094e-02, -1.13525391e-02],\n",
      "         ...,\n",
      "         [-1.86767578e-02, -1.17797852e-02,  3.05175781e-02, ...,\n",
      "          -2.06298828e-02, -4.00390625e-02,  2.56347656e-02],\n",
      "         [ 8.43048096e-04,  2.55126953e-02, -2.30712891e-02, ...,\n",
      "          -6.65283203e-03, -7.44628906e-03, -1.14135742e-02],\n",
      "         [ 5.85937500e-03,  5.03540039e-04, -6.16455078e-03, ...,\n",
      "           2.60009766e-02,  1.62353516e-02, -6.54296875e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 2.33459473e-03, -3.93676758e-03,  4.11987305e-03, ...,\n",
      "           5.07354736e-04, -1.83868408e-03,  6.33239746e-04],\n",
      "         [-2.13623047e-03, -2.45666504e-03, -1.62506104e-03, ...,\n",
      "           2.92968750e-03,  3.84521484e-03,  1.44958496e-03],\n",
      "         [-3.28063965e-03,  4.52041626e-04,  1.65557861e-03, ...,\n",
      "           4.73022461e-03,  9.65118408e-04, -2.85339355e-03],\n",
      "         ...,\n",
      "         [-1.59263611e-04, -6.48498535e-04, -2.85339355e-03, ...,\n",
      "           2.80761719e-03, -3.26538086e-03,  8.11767578e-03],\n",
      "         [-2.80761719e-03, -1.03759766e-03,  2.05993652e-03, ...,\n",
      "           2.05993652e-03, -2.33459473e-03,  6.07967377e-05],\n",
      "         [-6.62231445e-03,  3.08227539e-03, -3.05175781e-03, ...,\n",
      "           1.32751465e-03, -2.12097168e-03, -4.45556641e-03]],\n",
      "\n",
      "        [[-9.03320312e-03,  2.01416016e-03, -8.30078125e-03, ...,\n",
      "           5.37109375e-03, -8.85009766e-03,  8.05664062e-03],\n",
      "         [-3.67736816e-03,  7.51495361e-04, -4.36782837e-04, ...,\n",
      "          -6.77490234e-03, -4.76074219e-03, -1.25122070e-03],\n",
      "         [-1.40380859e-03,  1.55029297e-02, -3.81469727e-03, ...,\n",
      "          -1.13525391e-02, -2.88085938e-02,  3.96728516e-03],\n",
      "         ...,\n",
      "         [ 1.80053711e-03,  1.48773193e-03, -1.70898438e-02, ...,\n",
      "          -1.98974609e-02,  1.83105469e-02, -1.00708008e-02],\n",
      "         [-2.02941895e-03, -1.15203857e-03, -6.74438477e-03, ...,\n",
      "           8.17871094e-03, -6.67572021e-05,  1.66015625e-02],\n",
      "         [-9.33837891e-03, -8.05664062e-03, -3.17382812e-03, ...,\n",
      "          -4.36401367e-03,  1.53808594e-02, -3.23486328e-03]],\n",
      "\n",
      "        [[ 5.95092773e-03,  1.18408203e-02, -9.88769531e-03, ...,\n",
      "          -6.19506836e-03,  1.73339844e-02,  9.88769531e-03],\n",
      "         [-1.62353516e-02, -5.85937500e-03, -5.92041016e-03, ...,\n",
      "          -8.97216797e-03, -1.63574219e-02, -2.21252441e-04],\n",
      "         [ 1.19018555e-03, -7.38525391e-03, -4.45556641e-03, ...,\n",
      "          -3.47900391e-03, -1.22070312e-02,  4.97436523e-03],\n",
      "         ...,\n",
      "         [ 1.98974609e-02,  1.80664062e-02, -4.36401367e-03, ...,\n",
      "           1.73339844e-02, -4.73022461e-03,  9.15527344e-03],\n",
      "         [-6.68334961e-03, -1.14746094e-02, -1.72119141e-02, ...,\n",
      "          -4.18090820e-03, -2.51770020e-03, -1.51977539e-02],\n",
      "         [-1.77001953e-02, -2.03857422e-02,  6.89697266e-03, ...,\n",
      "          -7.50732422e-03, -8.54492188e-03,  1.96838379e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.86865234e-02,  1.00097656e-02, -5.76019287e-04, ...,\n",
      "          -1.50146484e-02,  2.09045410e-03, -1.96533203e-02],\n",
      "         [-4.15039062e-03, -1.79443359e-02,  1.55029297e-02, ...,\n",
      "           2.13623047e-02, -2.39257812e-02, -2.52685547e-02],\n",
      "         [ 1.44042969e-02, -3.90625000e-02,  7.56835938e-03, ...,\n",
      "          -2.00195312e-02,  1.03759766e-02, -4.33349609e-03],\n",
      "         ...,\n",
      "         [-8.85009766e-03,  1.00097656e-02,  4.76074219e-03, ...,\n",
      "          -6.31713867e-03,  1.52587891e-02,  1.20849609e-02],\n",
      "         [-1.81884766e-02, -2.02941895e-03, -1.17187500e-02, ...,\n",
      "           8.42285156e-03, -3.11279297e-03,  1.79443359e-02],\n",
      "         [ 1.26953125e-02,  9.82666016e-03,  5.58471680e-03, ...,\n",
      "           4.11987305e-04, -3.75366211e-03,  2.12402344e-02]],\n",
      "\n",
      "        [[ 1.41601562e-02,  1.91650391e-02,  5.15747070e-03, ...,\n",
      "           1.18408203e-02, -7.24792480e-04,  1.80664062e-02],\n",
      "         [-1.30615234e-02, -1.27563477e-02, -5.12695312e-03, ...,\n",
      "          -2.35595703e-02, -7.99560547e-03,  1.66893005e-05],\n",
      "         [-2.14843750e-02, -1.96838379e-03,  1.42211914e-02, ...,\n",
      "           3.24707031e-02,  8.97216797e-03,  4.24194336e-03],\n",
      "         ...,\n",
      "         [-1.15966797e-02,  2.40478516e-02, -2.28271484e-02, ...,\n",
      "          -1.31988525e-03,  1.90429688e-02,  6.59179688e-03],\n",
      "         [-1.87988281e-02,  2.44140625e-02, -8.91113281e-03, ...,\n",
      "          -3.17382812e-02,  1.17492676e-03,  6.40869141e-03],\n",
      "         [ 1.25122070e-02,  3.52478027e-03, -6.04248047e-03, ...,\n",
      "           2.12402344e-02, -3.87573242e-03,  2.51464844e-02]],\n",
      "\n",
      "        [[ 1.59912109e-02,  1.68457031e-02,  1.26953125e-02, ...,\n",
      "          -9.33837891e-03,  3.31115723e-03, -1.18255615e-03],\n",
      "         [-1.89208984e-02,  9.94873047e-03, -2.33154297e-02, ...,\n",
      "          -1.61132812e-02,  6.22558594e-03, -7.89642334e-04],\n",
      "         [ 2.12402344e-02, -7.09533691e-04,  9.33837891e-03, ...,\n",
      "          -8.11767578e-03,  8.97216797e-03,  6.46972656e-03],\n",
      "         ...,\n",
      "         [ 9.03320312e-03, -1.83105469e-02,  7.44628906e-03, ...,\n",
      "           1.28173828e-02,  2.85644531e-02,  9.82666016e-03],\n",
      "         [-1.61132812e-02,  1.36718750e-02,  3.50952148e-03, ...,\n",
      "          -6.62231445e-03, -2.74658203e-02,  1.19628906e-02],\n",
      "         [-5.34057617e-03,  1.12304688e-02,  4.33349609e-03, ...,\n",
      "           2.42919922e-02,  4.48608398e-03, -9.09423828e-03]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[-4.48608398e-03, -2.82287598e-04, -1.15966797e-02, ...,\n",
      "           1.88446045e-03, -1.58691406e-02, -4.36401367e-03],\n",
      "         [-4.30297852e-03, -4.21142578e-03,  1.41601562e-02, ...,\n",
      "          -4.54711914e-03, -6.10351562e-03, -2.02636719e-02],\n",
      "         [-2.47192383e-03,  1.48315430e-02, -6.40869141e-03, ...,\n",
      "           1.01470947e-03, -2.71606445e-03,  1.93786621e-03],\n",
      "         ...,\n",
      "         [-2.80761719e-03,  2.69775391e-02,  2.51770020e-03, ...,\n",
      "          -3.72314453e-03,  2.96630859e-02,  2.20947266e-02],\n",
      "         [-1.64794922e-02,  8.78906250e-03,  1.65557861e-03, ...,\n",
      "          -6.53076172e-03,  2.30407715e-03,  1.95312500e-03],\n",
      "         [ 8.72802734e-03,  7.75146484e-03,  9.70458984e-03, ...,\n",
      "          -6.01196289e-03, -5.40161133e-03, -2.00195312e-02]],\n",
      "\n",
      "        [[ 4.02832031e-03,  1.36566162e-03,  2.07519531e-03, ...,\n",
      "           2.68554688e-03, -3.90625000e-03, -4.51660156e-03],\n",
      "         [-1.64031982e-03, -5.43212891e-03, -4.79125977e-03, ...,\n",
      "           8.85009766e-04,  3.31878662e-04, -4.69970703e-03],\n",
      "         [-2.70080566e-03, -1.64031982e-03, -9.15527344e-03, ...,\n",
      "           1.95312500e-03,  3.86047363e-03, -2.68554688e-03],\n",
      "         ...,\n",
      "         [-6.04248047e-03,  2.71606445e-03,  4.42504883e-03, ...,\n",
      "          -1.73950195e-03, -2.15148926e-03, -3.63159180e-03],\n",
      "         [ 3.34167480e-03, -2.80761719e-03,  8.81195068e-04, ...,\n",
      "          -5.00488281e-03, -2.13623047e-03, -2.59399414e-03],\n",
      "         [ 6.07299805e-03,  4.27246094e-03,  2.22778320e-03, ...,\n",
      "           5.06591797e-03, -1.29938126e-05, -3.08227539e-03]],\n",
      "\n",
      "        [[ 1.03759766e-03,  1.62353516e-02, -1.35803223e-03, ...,\n",
      "           6.16455078e-03,  5.03540039e-03,  8.85009766e-03],\n",
      "         [-1.62353516e-02,  1.84326172e-02,  2.02941895e-03, ...,\n",
      "          -1.41906738e-03,  1.71661377e-03, -5.52368164e-03],\n",
      "         [ 1.74560547e-02,  7.85827637e-04,  7.20214844e-03, ...,\n",
      "          -1.54418945e-02,  9.58251953e-03, -1.11389160e-03],\n",
      "         ...,\n",
      "         [-1.25732422e-02, -1.25122070e-02, -9.15527344e-03, ...,\n",
      "           1.59912109e-02,  7.08007812e-03,  1.28173828e-02],\n",
      "         [-1.41601562e-02, -4.94384766e-03,  1.12915039e-02, ...,\n",
      "           6.43920898e-03,  6.86645508e-04, -1.03759766e-03],\n",
      "         [-8.36181641e-03,  1.39770508e-02,  2.66113281e-02, ...,\n",
      "          -2.93731689e-04,  4.15039062e-03, -1.48773193e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.72314453e-03, -3.28063965e-03,  3.32031250e-02, ...,\n",
      "           2.01416016e-03,  4.36401367e-03,  4.02832031e-03],\n",
      "         [-7.20214844e-03, -4.11987305e-03,  1.58691406e-02, ...,\n",
      "          -3.34472656e-02,  2.06298828e-02, -2.66113281e-02],\n",
      "         [ 2.27355957e-03,  1.84326172e-02, -1.63574219e-02, ...,\n",
      "          -2.16064453e-02, -1.12915039e-02,  5.12695312e-03],\n",
      "         ...,\n",
      "         [ 5.34057617e-03, -1.28173828e-02,  6.92749023e-03, ...,\n",
      "          -1.44653320e-02,  1.55639648e-03, -1.63574219e-02],\n",
      "         [ 1.97753906e-02,  3.66210938e-02,  1.39160156e-02, ...,\n",
      "          -1.37329102e-02,  1.02539062e-02,  9.15527344e-03],\n",
      "         [-1.04370117e-02, -1.14135742e-02, -6.01196289e-03, ...,\n",
      "          -3.43322754e-03, -3.94821167e-04,  5.95092773e-03]],\n",
      "\n",
      "        [[ 1.28173828e-02,  9.57489014e-04,  1.04370117e-02, ...,\n",
      "          -2.20947266e-02, -8.66699219e-03, -4.17480469e-02],\n",
      "         [ 2.14843750e-02,  8.17871094e-03,  2.19726562e-02, ...,\n",
      "          -5.18798828e-03,  2.63671875e-02,  5.87463379e-04],\n",
      "         [-2.70996094e-02, -3.39355469e-02, -4.27246094e-03, ...,\n",
      "           1.17492676e-03,  1.25122070e-02, -4.12597656e-02],\n",
      "         ...,\n",
      "         [ 5.83648682e-04, -8.91113281e-03, -2.39562988e-03, ...,\n",
      "          -2.17285156e-02, -1.91650391e-02, -3.17382812e-02],\n",
      "         [ 8.65936279e-04, -4.37011719e-02, -2.85644531e-02, ...,\n",
      "          -3.01513672e-02, -3.34472656e-02, -3.23486328e-03],\n",
      "         [ 2.68554688e-02,  2.85644531e-02, -7.01904297e-03, ...,\n",
      "          -5.92041016e-03, -8.48388672e-03, -1.18408203e-02]],\n",
      "\n",
      "        [[-1.66015625e-02,  1.53198242e-02,  1.19018555e-02, ...,\n",
      "          -9.03320312e-03,  9.76562500e-03,  3.12500000e-02],\n",
      "         [ 4.10156250e-02,  2.16064453e-02, -2.47192383e-03, ...,\n",
      "          -2.74658203e-02,  6.74438477e-03, -1.66893005e-04],\n",
      "         [-9.72747803e-04,  8.97216797e-03,  8.66699219e-03, ...,\n",
      "          -2.49023438e-02, -1.22070312e-03,  2.39257812e-02],\n",
      "         ...,\n",
      "         [-1.23901367e-02,  2.46582031e-02,  3.97949219e-02, ...,\n",
      "          -4.76074219e-03, -2.56347656e-02, -2.41699219e-02],\n",
      "         [-1.59912109e-02,  3.17382812e-02,  9.76562500e-03, ...,\n",
      "           9.64355469e-03, -1.98974609e-02, -3.83377075e-04],\n",
      "         [-1.66015625e-02,  1.63574219e-02,  7.75146484e-03, ...,\n",
      "           2.72216797e-02,  1.75781250e-02,  1.70898438e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 3.38745117e-03, -9.94873047e-03,  2.09045410e-03, ...,\n",
      "          -1.30462646e-03, -3.02124023e-03,  3.70788574e-03],\n",
      "         [ 2.68554688e-03,  3.99780273e-03,  2.21252441e-03, ...,\n",
      "           2.99072266e-03, -4.66918945e-03, -2.91442871e-03],\n",
      "         [ 6.21795654e-04, -2.67028809e-03, -3.08227539e-03, ...,\n",
      "          -2.59399414e-03, -3.55529785e-03, -2.41088867e-03],\n",
      "         ...,\n",
      "         [ 2.94494629e-03,  1.10626221e-03, -5.58471680e-03, ...,\n",
      "          -3.21960449e-03,  7.75146484e-03, -4.60815430e-03],\n",
      "         [ 2.50244141e-03, -5.79833984e-03,  3.78417969e-03, ...,\n",
      "           4.88281250e-03,  6.10351562e-03,  2.12097168e-03],\n",
      "         [ 5.40161133e-03,  7.40051270e-04, -1.12152100e-03, ...,\n",
      "           3.15856934e-03, -1.05590820e-02, -4.42504883e-03]],\n",
      "\n",
      "        [[ 2.08740234e-02, -4.42504883e-03, -1.61132812e-02, ...,\n",
      "          -2.50244141e-03,  2.27050781e-02,  1.20239258e-02],\n",
      "         [-1.20239258e-02,  5.06591797e-03, -6.77490234e-03, ...,\n",
      "           2.34375000e-02, -5.73730469e-03,  3.15856934e-03],\n",
      "         [-9.70458984e-03,  2.06298828e-02,  8.60595703e-03, ...,\n",
      "           4.27246094e-03,  5.61523438e-03, -6.16455078e-03],\n",
      "         ...,\n",
      "         [ 5.21850586e-03, -4.08935547e-03, -1.80816650e-03, ...,\n",
      "          -3.72314453e-03, -1.98364258e-03, -4.85229492e-03],\n",
      "         [ 6.13403320e-03, -9.46044922e-03,  3.21960449e-03, ...,\n",
      "          -2.54821777e-03, -4.21142578e-03,  1.60217285e-03],\n",
      "         [-5.60760498e-04, -7.04956055e-03,  7.29370117e-03, ...,\n",
      "           4.95910645e-04,  1.73950195e-03, -3.44848633e-03]],\n",
      "\n",
      "        [[ 7.75146484e-03,  5.98144531e-03, -9.15527344e-03, ...,\n",
      "          -1.62353516e-02, -3.73840332e-03, -1.42822266e-02],\n",
      "         [-8.23974609e-03, -1.33666992e-02, -2.27050781e-02, ...,\n",
      "          -8.11767578e-03,  4.54711914e-03, -3.14941406e-02],\n",
      "         [-6.43920898e-03,  1.81579590e-03, -5.49316406e-03, ...,\n",
      "           2.25830078e-02, -5.12695312e-03,  5.18798828e-03],\n",
      "         ...,\n",
      "         [-1.17797852e-02,  1.04370117e-02,  1.75781250e-02, ...,\n",
      "           8.36181641e-03,  1.17187500e-02,  9.03320312e-03],\n",
      "         [ 6.86645508e-03,  2.42614746e-03,  9.21630859e-03, ...,\n",
      "          -6.71386719e-03, -9.76562500e-04, -2.92968750e-02],\n",
      "         [ 8.48388672e-03, -5.79833984e-03, -8.66699219e-03, ...,\n",
      "           1.31835938e-02,  1.59912109e-02,  1.11694336e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.11083984e-02,  1.28173828e-02,  2.12402344e-02, ...,\n",
      "          -3.34167480e-03,  1.17797852e-02,  1.22680664e-02],\n",
      "         [-6.06536865e-04,  2.02636719e-02, -2.76184082e-03, ...,\n",
      "          -2.17285156e-02,  1.56402588e-03,  2.52685547e-02],\n",
      "         [ 6.89697266e-03, -1.91650391e-02, -2.66113281e-02, ...,\n",
      "           3.12500000e-02,  1.06811523e-02,  1.54418945e-02],\n",
      "         ...,\n",
      "         [ 3.28063965e-03,  1.24931335e-04, -2.51770020e-03, ...,\n",
      "           6.07299805e-03, -1.11083984e-02,  8.08715820e-04],\n",
      "         [-1.95312500e-03,  1.51367188e-02, -3.17382812e-02, ...,\n",
      "           9.58251953e-03,  6.37817383e-03,  1.87988281e-02],\n",
      "         [-5.34057617e-03, -2.35595703e-02, -6.50024414e-03, ...,\n",
      "          -1.53808594e-02,  3.95507812e-02, -1.05285645e-03]],\n",
      "\n",
      "        [[-4.61425781e-02,  2.35595703e-02, -4.34570312e-02, ...,\n",
      "          -2.92968750e-02, -2.90527344e-02, -1.47705078e-02],\n",
      "         [ 1.35803223e-03,  1.73339844e-02, -9.82666016e-03, ...,\n",
      "          -1.16577148e-02, -1.17797852e-02, -8.91113281e-03],\n",
      "         [ 2.41699219e-02, -1.70898438e-02, -2.85644531e-02, ...,\n",
      "           4.94384766e-03, -6.65283203e-03,  1.51367188e-02],\n",
      "         ...,\n",
      "         [ 2.56347656e-02,  2.73437500e-02, -1.06811523e-02, ...,\n",
      "          -1.12304688e-02, -9.03320312e-03,  1.46484375e-02],\n",
      "         [-1.20239258e-02, -3.24707031e-02,  1.73339844e-02, ...,\n",
      "          -7.04956055e-03, -1.26342773e-02,  4.19921875e-02],\n",
      "         [ 8.16345215e-04,  4.02832031e-03, -2.94189453e-02, ...,\n",
      "           2.70996094e-02,  4.45556641e-03,  9.58251953e-03]],\n",
      "\n",
      "        [[-5.95092773e-03,  1.30615234e-02,  2.97851562e-02, ...,\n",
      "           1.19628906e-02,  3.32031250e-02, -4.30297852e-03],\n",
      "         [ 8.85009766e-03,  4.54711914e-03, -1.13525391e-02, ...,\n",
      "          -8.43048096e-04, -4.30297852e-03,  1.40991211e-02],\n",
      "         [-4.66918945e-03,  1.02539062e-02,  3.24707031e-02, ...,\n",
      "           5.79833984e-03, -1.61132812e-02,  2.56347656e-02],\n",
      "         ...,\n",
      "         [-1.23291016e-02, -1.61132812e-02,  4.71191406e-02, ...,\n",
      "           2.73132324e-03, -1.28173828e-02,  2.41699219e-02],\n",
      "         [ 6.65283203e-03, -9.52148438e-03,  1.96533203e-02, ...,\n",
      "           1.78222656e-02,  1.42822266e-02,  2.31933594e-02],\n",
      "         [ 9.15527344e-04, -3.46679688e-02, -1.80664062e-02, ...,\n",
      "          -5.95092773e-03,  3.34472656e-02, -1.42211914e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 3.37219238e-03, -3.15856934e-03,  2.27355957e-03, ...,\n",
      "          -1.73950195e-03, -3.67736816e-03, -4.66918945e-03],\n",
      "         [ 1.60980225e-03,  2.31933594e-03, -2.51770020e-03, ...,\n",
      "          -5.98144531e-03, -2.82287598e-03,  3.20434570e-03],\n",
      "         [ 5.06591797e-03,  8.50677490e-04, -7.20214844e-03, ...,\n",
      "           7.29370117e-03, -1.73187256e-03, -1.50299072e-03],\n",
      "         ...,\n",
      "         [ 5.92041016e-03, -1.02996826e-03, -1.65557861e-03, ...,\n",
      "           1.21307373e-03,  6.19506836e-03,  5.49316406e-03],\n",
      "         [ 3.44848633e-03,  1.80816650e-03, -5.73730469e-03, ...,\n",
      "           2.60925293e-03, -3.47900391e-03, -7.59887695e-03],\n",
      "         [-9.52148438e-03,  6.25610352e-03, -6.37054443e-04, ...,\n",
      "           4.30297852e-03, -5.11169434e-04, -6.68334961e-03]],\n",
      "\n",
      "        [[ 2.21252441e-03,  2.10571289e-03, -4.45556641e-03, ...,\n",
      "           1.05285645e-03, -6.37054443e-04, -9.94873047e-03],\n",
      "         [ 6.01196289e-03, -4.24194336e-03,  4.06265259e-04, ...,\n",
      "          -2.16674805e-03,  1.45912170e-04, -3.40938568e-05],\n",
      "         [-1.29699707e-03,  1.75476074e-03,  1.29699707e-03, ...,\n",
      "          -4.99725342e-04,  1.28936768e-03,  3.96728516e-03],\n",
      "         ...,\n",
      "         [-4.97436523e-03, -7.43865967e-04,  4.73022461e-03, ...,\n",
      "           2.86102295e-04, -2.15148926e-03,  1.28936768e-03],\n",
      "         [-1.28746033e-04, -2.73132324e-03,  1.99890137e-03, ...,\n",
      "           2.22778320e-03,  1.83105469e-03, -4.27246094e-03],\n",
      "         [ 2.74658203e-04,  3.21960449e-03, -2.16674805e-03, ...,\n",
      "          -1.93786621e-03,  2.10571289e-03,  1.91497803e-03]],\n",
      "\n",
      "        [[ 1.60217285e-03,  1.08642578e-02,  2.66113281e-02, ...,\n",
      "           2.74658203e-03,  8.97216797e-03, -7.87353516e-03],\n",
      "         [ 1.22680664e-02,  7.81250000e-03, -6.86645508e-04, ...,\n",
      "          -2.01416016e-02, -1.20849609e-02, -5.00488281e-03],\n",
      "         [-2.86865234e-02, -1.62353516e-02, -7.04956055e-03, ...,\n",
      "           1.36718750e-02, -2.18505859e-02, -4.24194336e-03],\n",
      "         ...,\n",
      "         [ 1.80664062e-02,  4.66918945e-03, -1.86767578e-02, ...,\n",
      "          -4.24194336e-03,  1.66015625e-02, -8.60595703e-03],\n",
      "         [ 3.32641602e-03, -1.19018555e-02, -8.54492188e-03, ...,\n",
      "          -2.16064453e-02,  1.93786621e-03,  1.08032227e-02],\n",
      "         [-2.79541016e-02, -2.07519531e-02,  9.64355469e-03, ...,\n",
      "           2.06298828e-02, -2.25830078e-02, -1.45874023e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.50024414e-03,  1.98974609e-02,  1.98364258e-03, ...,\n",
      "           1.14135742e-02, -1.74560547e-02,  4.33349609e-03],\n",
      "         [ 8.30078125e-03, -2.78320312e-02, -9.58251953e-03, ...,\n",
      "           1.38549805e-02,  2.64892578e-02,  1.95312500e-03],\n",
      "         [-8.85009766e-03,  1.95312500e-02, -1.58691406e-02, ...,\n",
      "          -2.83813477e-03, -1.84326172e-02,  1.09252930e-02],\n",
      "         ...,\n",
      "         [ 2.01416016e-03,  1.17187500e-02, -3.71093750e-02, ...,\n",
      "          -8.11767578e-03, -1.38092041e-03,  6.31713867e-03],\n",
      "         [-1.74560547e-02, -2.67333984e-02,  1.62353516e-02, ...,\n",
      "          -9.21630859e-03, -1.52587891e-02,  1.57470703e-02],\n",
      "         [-7.35473633e-03,  1.42211914e-02, -1.83105469e-02, ...,\n",
      "          -1.75781250e-02,  6.50024414e-03, -2.81982422e-02]],\n",
      "\n",
      "        [[ 1.54418945e-02,  1.02539062e-02,  4.02832031e-03, ...,\n",
      "          -2.57568359e-02,  2.80761719e-02, -3.39355469e-02],\n",
      "         [-9.82666016e-03, -3.10058594e-02, -2.70996094e-02, ...,\n",
      "          -2.94189453e-02, -1.83105469e-02,  1.79443359e-02],\n",
      "         [ 2.96630859e-02,  5.83496094e-02,  1.80664062e-02, ...,\n",
      "           8.54492188e-03,  3.54003906e-02,  1.75781250e-02],\n",
      "         ...,\n",
      "         [ 7.78198242e-03,  1.68457031e-02, -5.17578125e-02, ...,\n",
      "          -2.57568359e-02,  3.05175781e-02, -2.74658203e-03],\n",
      "         [-1.72119141e-02, -2.11181641e-02, -1.50756836e-02, ...,\n",
      "           8.42285156e-03, -2.44140625e-03,  1.32446289e-02],\n",
      "         [-2.22167969e-02, -2.08740234e-02, -1.17187500e-02, ...,\n",
      "           1.42822266e-02,  8.05664062e-03, -1.64794922e-02]],\n",
      "\n",
      "        [[-2.51464844e-02, -9.46044922e-04, -1.52587891e-02, ...,\n",
      "           2.86865234e-02, -3.60107422e-03, -5.63964844e-02],\n",
      "         [ 7.26318359e-03, -3.22265625e-02, -3.51562500e-02, ...,\n",
      "          -2.20947266e-02, -1.29394531e-02,  7.93457031e-03],\n",
      "         [-1.59912109e-02, -1.11083984e-02, -2.81982422e-02, ...,\n",
      "          -8.23974609e-03, -2.31933594e-02,  1.59912109e-02],\n",
      "         ...,\n",
      "         [-4.48608398e-03,  2.62451172e-02, -1.52587891e-02, ...,\n",
      "          -1.97753906e-02, -2.23388672e-02,  3.34472656e-02],\n",
      "         [-1.15966797e-02, -1.90429688e-02,  2.53906250e-02, ...,\n",
      "          -1.08032227e-02, -1.39770508e-02, -1.90429688e-02],\n",
      "         [-4.90722656e-02, -1.50756836e-02,  8.78906250e-03, ...,\n",
      "          -1.62353516e-02, -1.86767578e-02,  6.31713867e-03]]]],      dtype=float32)}, 'query': {'kernel': Array([[[[-5.45501709e-04,  1.25885010e-03, -1.28936768e-03, ...,\n",
      "           9.84191895e-04, -1.48010254e-03,  1.22070312e-03],\n",
      "         [ 6.71386719e-04,  8.69750977e-04, -1.47819519e-05, ...,\n",
      "          -2.19726562e-03,  1.36566162e-03, -2.04467773e-03],\n",
      "         [-1.76429749e-04,  2.51770020e-04,  1.57356262e-04, ...,\n",
      "          -3.17382812e-03, -3.28063965e-03, -3.43322754e-03],\n",
      "         ...,\n",
      "         [ 1.90734863e-03,  1.22833252e-03,  1.89208984e-03, ...,\n",
      "           3.44848633e-03,  1.38854980e-03,  2.21252441e-03],\n",
      "         [-3.14712524e-04, -4.48226929e-04,  3.66210938e-04, ...,\n",
      "           8.08715820e-04, -6.21795654e-04, -6.14166260e-04],\n",
      "         [ 1.40190125e-04, -8.01086426e-05,  6.58035278e-05, ...,\n",
      "           1.15394592e-04,  2.27355957e-03, -1.19018555e-03]],\n",
      "\n",
      "        [[-1.10626221e-03, -5.26905060e-05,  2.71606445e-03, ...,\n",
      "           1.77764893e-03, -1.64985657e-04,  6.21795654e-04],\n",
      "         [-1.81579590e-03,  2.48718262e-03, -1.92642212e-04, ...,\n",
      "          -1.87873840e-04, -6.67572021e-04, -3.56674194e-04],\n",
      "         [ 1.85966492e-04, -1.76429749e-04, -1.60980225e-03, ...,\n",
      "          -1.09863281e-03, -9.48905945e-05, -1.54113770e-03],\n",
      "         ...,\n",
      "         [ 2.71606445e-03,  1.46484375e-03, -2.86102295e-04, ...,\n",
      "          -7.78198242e-04, -7.97271729e-04, -1.12915039e-03],\n",
      "         [ 2.30407715e-03, -9.91821289e-04, -6.96182251e-05, ...,\n",
      "           2.50244141e-03,  1.00708008e-03,  1.96838379e-03],\n",
      "         [ 2.41088867e-03, -6.06536865e-04,  2.91824341e-04, ...,\n",
      "          -1.57356262e-05, -1.85012817e-04,  2.27689743e-05]],\n",
      "\n",
      "        [[-2.05993652e-03, -1.31225586e-03, -1.26647949e-03, ...,\n",
      "           1.69372559e-03,  8.69750977e-04, -2.01416016e-03],\n",
      "         [ 6.37054443e-04, -6.79016113e-04, -8.08715820e-04, ...,\n",
      "          -1.68609619e-03,  2.24113464e-04,  5.22136688e-05],\n",
      "         [ 1.18255615e-03, -1.56402588e-03, -3.79943848e-03, ...,\n",
      "           2.09045410e-03, -1.41143799e-03,  7.53402710e-05],\n",
      "         ...,\n",
      "         [-6.79016113e-04, -2.27355957e-03, -3.28063965e-03, ...,\n",
      "           2.67028809e-03, -2.85339355e-03, -4.80651855e-04],\n",
      "         [-1.12056732e-04, -1.62124634e-04,  4.42504883e-04, ...,\n",
      "          -7.17163086e-04, -3.66210938e-03,  4.15039062e-03],\n",
      "         [ 1.19018555e-03, -1.29699707e-03, -1.61743164e-03, ...,\n",
      "          -4.21142578e-03,  2.11715698e-04, -5.83648682e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.46047974e-04, -2.13623047e-03, -1.67083740e-03, ...,\n",
      "          -5.53131104e-04, -2.45666504e-03,  8.85009766e-04],\n",
      "         [-1.83105469e-04, -2.74658203e-03,  9.26971436e-04, ...,\n",
      "           5.36441803e-05, -2.02178955e-04,  1.44958496e-04],\n",
      "         [-3.47137451e-04, -3.96728516e-04, -3.77655029e-04, ...,\n",
      "           1.45721436e-03,  7.05718994e-04, -1.31225586e-03],\n",
      "         ...,\n",
      "         [ 3.79943848e-03, -3.20434570e-04, -2.47955322e-04, ...,\n",
      "          -9.07897949e-04, -5.30242920e-04,  2.63977051e-03],\n",
      "         [ 6.33239746e-04, -1.20544434e-03,  9.99450684e-04, ...,\n",
      "           2.73132324e-03, -1.83105469e-03,  1.89971924e-03],\n",
      "         [-3.05175781e-04, -1.82342529e-03, -1.45721436e-03, ...,\n",
      "          -6.79016113e-04, -2.07519531e-03,  4.26769257e-05]],\n",
      "\n",
      "        [[-3.26633453e-05,  1.44958496e-03, -5.37872314e-04, ...,\n",
      "          -1.34277344e-03, -1.86920166e-03,  2.60925293e-03],\n",
      "         [ 1.87683105e-03,  3.96728516e-03,  1.25122070e-03, ...,\n",
      "          -6.59942627e-04,  3.17382812e-03, -1.73950195e-03],\n",
      "         [-1.09100342e-03,  3.14712524e-04, -9.34600830e-04, ...,\n",
      "          -3.49426270e-03, -2.95639038e-04, -1.96838379e-03],\n",
      "         ...,\n",
      "         [ 4.69207764e-04, -3.20434570e-04,  8.62121582e-04, ...,\n",
      "           1.98364258e-03, -2.26974487e-04, -4.05883789e-03],\n",
      "         [-1.00708008e-03,  2.38037109e-03,  1.33514404e-03, ...,\n",
      "          -4.33921814e-05, -1.19018555e-03,  1.29699707e-03],\n",
      "         [ 1.80053711e-03, -2.48718262e-03, -7.05718994e-04, ...,\n",
      "          -7.59124756e-04, -1.15633011e-05, -6.13403320e-03]],\n",
      "\n",
      "        [[-2.92968750e-03, -7.28607178e-04, -1.11389160e-03, ...,\n",
      "          -3.00598145e-03, -7.51495361e-04,  1.51062012e-03],\n",
      "         [ 1.24359131e-03,  2.15148926e-03, -1.06048584e-03, ...,\n",
      "          -1.27410889e-03,  1.96838379e-03,  7.40051270e-04],\n",
      "         [-8.96453857e-04, -6.59942627e-04, -4.44412231e-04, ...,\n",
      "           3.49426270e-03, -2.12669373e-04,  1.80053711e-03],\n",
      "         ...,\n",
      "         [ 2.42614746e-03, -8.69750977e-04, -1.31988525e-03, ...,\n",
      "           1.44958496e-03, -2.82287598e-04,  6.46972656e-03],\n",
      "         [ 3.02124023e-03, -2.67028809e-03,  3.05175781e-05, ...,\n",
      "          -2.74658203e-03,  1.20544434e-03, -1.64794922e-03],\n",
      "         [ 1.22070312e-03,  6.94274902e-04, -1.44004822e-04, ...,\n",
      "           1.42669678e-03, -1.71661377e-03, -4.85229492e-03]]],\n",
      "\n",
      "\n",
      "       [[[-1.30462646e-03, -3.83377075e-04,  1.11389160e-03, ...,\n",
      "           3.18527222e-04, -1.05285645e-03, -5.43594360e-05],\n",
      "         [ 8.96453857e-04,  1.12152100e-03,  1.64985657e-04, ...,\n",
      "           3.75747681e-04, -5.68389893e-04,  4.92095947e-04],\n",
      "         [ 7.29560852e-05,  3.14712524e-05, -1.16825104e-04, ...,\n",
      "          -3.14712524e-05,  4.45842743e-05,  5.79357147e-05],\n",
      "         ...,\n",
      "         [-1.18255615e-03, -1.50203705e-05, -7.51495361e-04, ...,\n",
      "          -2.47192383e-03, -8.81195068e-04, -2.44140625e-03],\n",
      "         [-4.88758087e-05,  3.75747681e-04, -1.31988525e-03, ...,\n",
      "          -6.40869141e-04,  7.43865967e-04,  7.28607178e-04],\n",
      "         [-1.95503235e-04, -2.80141830e-05, -1.14440918e-04, ...,\n",
      "           9.61303711e-04,  9.00268555e-04, -5.79833984e-04]],\n",
      "\n",
      "        [[ 6.44683838e-04, -7.28607178e-04,  2.86865234e-03, ...,\n",
      "           2.45666504e-03,  3.22341919e-04,  4.92095947e-04],\n",
      "         [ 4.99725342e-04,  6.37054443e-04,  2.67028809e-03, ...,\n",
      "           1.06811523e-03, -4.07695770e-05,  4.11987305e-04],\n",
      "         [ 7.55310059e-04, -1.45721436e-03,  9.76562500e-04, ...,\n",
      "          -1.78337097e-04,  6.33239746e-04, -5.49316406e-04],\n",
      "         ...,\n",
      "         [-2.82287598e-04,  1.53350830e-03,  6.29425049e-04, ...,\n",
      "          -4.92095947e-04, -5.18798828e-04, -5.18798828e-04],\n",
      "         [ 1.73950195e-03,  6.37054443e-04, -4.95910645e-04, ...,\n",
      "           3.73840332e-03, -3.31401825e-05,  4.69207764e-04],\n",
      "         [ 1.05381012e-04, -3.14712524e-04,  1.09863281e-03, ...,\n",
      "           1.60217285e-04, -3.33786011e-04,  4.23431396e-04]],\n",
      "\n",
      "        [[-8.04901123e-04,  9.46044922e-04,  2.91442871e-03, ...,\n",
      "           3.86047363e-03, -2.91442871e-03,  2.63977051e-03],\n",
      "         [ 2.18200684e-03,  5.30242920e-04,  1.44958496e-03, ...,\n",
      "          -1.85012817e-04, -1.85012817e-04, -6.67572021e-04],\n",
      "         [-1.15203857e-03,  1.55639648e-03,  8.20159912e-04, ...,\n",
      "          -1.38854980e-03,  5.52368164e-03, -6.44683838e-04],\n",
      "         ...,\n",
      "         [-1.29699707e-03,  1.68609619e-03, -3.81469727e-05, ...,\n",
      "          -2.12669373e-04,  3.40270996e-03,  1.54113770e-03],\n",
      "         [-2.24113464e-04, -8.69750977e-04,  1.38092041e-03, ...,\n",
      "          -9.76562500e-04,  9.61303711e-04,  5.40161133e-03],\n",
      "         [-1.41143799e-03,  4.11987305e-03,  4.17709351e-04, ...,\n",
      "           1.19018555e-03, -8.23974609e-04, -1.77001953e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.02722168e-04, -1.86920166e-03,  3.00598145e-03, ...,\n",
      "          -1.19781494e-03,  4.48608398e-03,  1.55639648e-03],\n",
      "         [-3.32641602e-03,  4.27246094e-03,  2.77709961e-03, ...,\n",
      "          -1.43432617e-03,  1.72424316e-03,  2.18200684e-03],\n",
      "         [-4.55856323e-04, -1.27792358e-04, -6.82830811e-04, ...,\n",
      "          -9.61303711e-04, -9.84191895e-04,  8.77380371e-05],\n",
      "         ...,\n",
      "         [ 5.03540039e-04,  1.31988525e-03,  6.25610352e-04, ...,\n",
      "          -1.54113770e-03,  1.12915039e-03, -1.09672546e-05],\n",
      "         [ 4.17709351e-04,  1.64794922e-03,  2.92968750e-03, ...,\n",
      "           2.96020508e-03, -8.20159912e-04, -1.94549561e-03],\n",
      "         [ 1.22833252e-03,  7.24792480e-04,  3.91006470e-04, ...,\n",
      "           5.31005859e-03, -2.07519531e-03, -3.64685059e-03]],\n",
      "\n",
      "        [[-1.24359131e-03,  1.02233887e-03, -1.94549561e-03, ...,\n",
      "          -2.84194946e-04, -4.22000885e-05,  6.14166260e-04],\n",
      "         [ 5.95092773e-04, -2.25830078e-03,  5.76019287e-04, ...,\n",
      "          -4.02832031e-03, -1.17492676e-03,  3.62396240e-04],\n",
      "         [ 5.03540039e-04,  1.00708008e-03, -3.91006470e-04, ...,\n",
      "          -4.08935547e-03, -7.00950623e-05,  1.43051147e-04],\n",
      "         ...,\n",
      "         [-1.32918358e-05, -1.75476074e-04,  1.71661377e-03, ...,\n",
      "          -7.58171082e-05,  4.82177734e-03, -3.11279297e-03],\n",
      "         [ 1.06048584e-03, -3.92913818e-04, -9.76562500e-04, ...,\n",
      "           2.30407715e-03,  3.47137451e-04, -1.05285645e-03],\n",
      "         [-6.17980957e-04,  7.01904297e-04, -8.73565674e-04, ...,\n",
      "          -3.58581543e-04, -1.74713135e-03, -1.19781494e-03]],\n",
      "\n",
      "        [[ 1.17492676e-03, -1.89971924e-03,  1.76239014e-03, ...,\n",
      "          -1.50299072e-03,  2.09045410e-03,  1.73568726e-04],\n",
      "         [ 9.00268555e-04,  7.13348389e-04, -6.79016113e-04, ...,\n",
      "           1.37329102e-03, -1.38854980e-03,  3.40270996e-03],\n",
      "         [-3.22341919e-04,  1.06811523e-03, -9.38415527e-04, ...,\n",
      "           6.04248047e-03, -4.39453125e-03, -2.86865234e-03],\n",
      "         ...,\n",
      "         [-4.92095947e-04,  1.89971924e-03,  1.35803223e-03, ...,\n",
      "           1.96838379e-03, -2.30407715e-03,  1.05285645e-03],\n",
      "         [ 4.63485718e-04, -2.10571289e-03,  1.07574463e-03, ...,\n",
      "           7.51495361e-04, -8.31604004e-04,  1.58691406e-03],\n",
      "         [-1.70707703e-04,  2.85339355e-03, -5.83648682e-04, ...,\n",
      "          -6.67572021e-04,  3.52859497e-04, -7.28607178e-04]]],\n",
      "\n",
      "\n",
      "       [[[-1.94549561e-04,  2.46047974e-04,  3.98159027e-05, ...,\n",
      "          -3.01361084e-04,  2.37464905e-04, -1.88827515e-04],\n",
      "         [ 4.73022461e-04,  3.50952148e-04,  1.22070312e-04, ...,\n",
      "          -2.47955322e-04,  1.32560730e-04, -1.30653381e-04],\n",
      "         [-2.64644623e-05, -1.34706497e-05,  3.05175781e-05, ...,\n",
      "           3.05175781e-04,  2.93731689e-04,  2.28881836e-04],\n",
      "         ...,\n",
      "         [-1.32560730e-04, -3.24249268e-04, -5.53131104e-04, ...,\n",
      "           4.22000885e-05, -2.72750854e-04, -4.52995300e-05],\n",
      "         [ 1.05857849e-04,  4.26769257e-05, -2.04086304e-04, ...,\n",
      "          -1.27792358e-04, -5.14984131e-05, -7.43865967e-05],\n",
      "         [-2.51531601e-05,  3.76701355e-05, -3.48091125e-05, ...,\n",
      "          -3.05175781e-05,  2.84194946e-04,  1.62124634e-04]],\n",
      "\n",
      "        [[-3.37219238e-03,  7.01904297e-04,  1.81579590e-03, ...,\n",
      "           5.64575195e-04,  2.33459473e-03, -3.39508057e-04],\n",
      "         [ 1.48773193e-03, -1.94311142e-05, -2.10571289e-03, ...,\n",
      "          -7.40051270e-04, -5.53131104e-05, -4.40597534e-04],\n",
      "         [-8.34465027e-05, -2.05993652e-03, -3.14712524e-04, ...,\n",
      "           2.65502930e-03,  3.62396240e-04,  2.71606445e-03],\n",
      "         ...,\n",
      "         [-4.76837158e-04,  3.71932983e-04, -9.11712646e-04, ...,\n",
      "           3.52859497e-04, -1.18255615e-03, -1.61743164e-03],\n",
      "         [-1.24359131e-03,  1.41143799e-03,  2.63452530e-05, ...,\n",
      "           1.71661377e-03, -3.22341919e-04, -9.00268555e-04],\n",
      "         [-2.09045410e-03,  4.53948975e-04, -7.00950623e-05, ...,\n",
      "           3.18527222e-04, -3.70025635e-04,  5.91278076e-04]],\n",
      "\n",
      "        [[ 6.79016113e-04, -3.29589844e-03, -2.27355957e-03, ...,\n",
      "          -2.31742859e-04,  3.32641602e-03, -1.51824951e-03],\n",
      "         [ 1.71661377e-04,  3.31878662e-04,  5.87463379e-04, ...,\n",
      "           1.71661377e-04, -1.64985657e-04,  1.13964081e-04],\n",
      "         [ 1.38854980e-03, -3.52478027e-03,  1.71661377e-04, ...,\n",
      "          -7.55310059e-04,  2.42614746e-03, -1.96838379e-03],\n",
      "         ...,\n",
      "         [-1.53350830e-03, -3.62396240e-04,  3.96728516e-03, ...,\n",
      "           2.96020508e-03, -2.56347656e-03,  2.59399414e-04],\n",
      "         [-1.15203857e-03, -2.12669373e-04, -9.00268555e-04, ...,\n",
      "           3.11279297e-03,  6.43920898e-03,  1.90734863e-03],\n",
      "         [-2.89916992e-03,  3.20434570e-04,  1.15966797e-03, ...,\n",
      "          -1.99890137e-03,  1.53541565e-04,  2.22778320e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.82559204e-04, -2.27355957e-03, -1.15871429e-04, ...,\n",
      "          -6.10351562e-04, -2.36511230e-03,  8.82148743e-05],\n",
      "         [ 2.59399414e-04,  1.13677979e-03, -6.63757324e-04, ...,\n",
      "          -3.38745117e-03, -3.37600708e-04, -2.86865234e-03],\n",
      "         [-8.85009766e-04,  4.11987305e-04, -3.52859497e-05, ...,\n",
      "          -4.99725342e-04, -4.18090820e-03, -4.18090820e-03],\n",
      "         ...,\n",
      "         [ 4.99725342e-04, -1.23596191e-03, -3.38745117e-03, ...,\n",
      "          -1.64794922e-03,  1.35803223e-03, -3.24249268e-04],\n",
      "         [-1.84631348e-03,  7.85827637e-04, -7.59124756e-04, ...,\n",
      "          -1.00708008e-03, -1.87683105e-03, -8.73565674e-04],\n",
      "         [-7.40051270e-04,  1.69372559e-03,  2.82287598e-03, ...,\n",
      "          -3.02124023e-03,  1.74713135e-03, -9.76562500e-04]],\n",
      "\n",
      "        [[-1.18732452e-04,  9.72747803e-04,  4.08935547e-03, ...,\n",
      "           1.92260742e-03, -2.85339355e-03,  1.96838379e-03],\n",
      "         [-1.42669678e-03,  1.51062012e-03, -1.09863281e-03, ...,\n",
      "           3.14331055e-03, -2.27355957e-03, -4.15802002e-04],\n",
      "         [ 1.06811523e-03,  7.82012939e-04,  9.65118408e-04, ...,\n",
      "           1.21307373e-03,  1.66320801e-03, -8.88824463e-04],\n",
      "         ...,\n",
      "         [-9.46044922e-04,  1.02233887e-03,  2.37464905e-04, ...,\n",
      "          -2.27355957e-03, -1.48773193e-03, -1.17301941e-04],\n",
      "         [-7.13348389e-04,  8.62121582e-04, -1.44958496e-03, ...,\n",
      "           1.19018555e-03, -5.11169434e-04, -2.39562988e-03],\n",
      "         [-2.08854675e-04,  5.93662262e-05, -1.82342529e-03, ...,\n",
      "          -3.05175781e-04, -1.42669678e-03,  3.52478027e-03]],\n",
      "\n",
      "        [[ 1.87683105e-03, -2.97546387e-03,  1.50299072e-03, ...,\n",
      "           8.35418701e-04,  2.21252441e-03, -8.96453857e-04],\n",
      "         [ 6.29425049e-04,  6.02722168e-04, -1.81579590e-03, ...,\n",
      "          -1.87683105e-03,  2.34985352e-03, -1.53541565e-04],\n",
      "         [-4.48226929e-04, -8.46862793e-04,  2.34603882e-04, ...,\n",
      "           2.22778320e-03, -7.78198242e-04, -3.93676758e-03],\n",
      "         ...,\n",
      "         [-4.82559204e-04,  1.73950195e-03,  1.11389160e-03, ...,\n",
      "           5.18798828e-04, -3.93676758e-03, -2.65121460e-04],\n",
      "         [-1.23596191e-03,  3.79562378e-04,  2.33459473e-03, ...,\n",
      "           2.96020508e-03,  2.70843506e-04,  2.93731689e-04],\n",
      "         [ 1.20544434e-03,  1.22833252e-03, -5.76019287e-04, ...,\n",
      "          -1.83105469e-03, -8.62121582e-04,  2.67028809e-03]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 3.92913818e-04, -8.23974609e-04,  5.56945801e-04, ...,\n",
      "           6.14166260e-04, -4.44412231e-04,  1.81198120e-04],\n",
      "         [ 3.20434570e-04,  2.05039978e-04,  7.12275505e-06, ...,\n",
      "          -8.58306885e-04,  8.69750977e-04, -9.38415527e-04],\n",
      "         [-1.71661377e-04, -4.23431396e-04, -2.68936157e-04, ...,\n",
      "          -8.96453857e-04, -8.88824463e-04, -8.31604004e-04],\n",
      "         ...,\n",
      "         [-3.92913818e-04,  1.29699707e-03,  1.20544434e-03, ...,\n",
      "           2.80761719e-03, -3.50952148e-04,  4.15802002e-04],\n",
      "         [ 1.79290771e-04,  5.03540039e-04, -6.21795654e-04, ...,\n",
      "          -4.29153442e-05,  1.12056732e-04,  1.60217285e-04],\n",
      "         [ 1.63078308e-04, -2.36511230e-04, -3.86238098e-05, ...,\n",
      "           8.62121582e-04, -2.96020508e-03,  1.59454346e-03]],\n",
      "\n",
      "        [[-2.12669373e-04, -7.32421875e-04, -9.59634781e-06, ...,\n",
      "           1.50299072e-03,  1.81579590e-03, -1.13677979e-03],\n",
      "         [ 9.34600830e-04, -1.15203857e-03,  1.80053711e-03, ...,\n",
      "          -5.49316406e-04,  1.22833252e-03,  7.28607178e-04],\n",
      "         [ 1.25122070e-03, -1.01470947e-03, -1.20544434e-03, ...,\n",
      "           1.13964081e-04, -7.24792480e-04,  2.84194946e-04],\n",
      "         ...,\n",
      "         [-4.74929810e-04,  2.74658203e-04, -4.86373901e-04, ...,\n",
      "           6.67572021e-04, -4.92095947e-04,  1.06048584e-03],\n",
      "         [ 7.51495361e-04,  2.89916992e-04,  3.47137451e-04, ...,\n",
      "           3.43322754e-03, -1.05857849e-04,  3.93676758e-03],\n",
      "         [ 3.89099121e-03, -9.84191895e-04, -9.49859619e-04, ...,\n",
      "          -7.66754150e-04,  7.78198242e-04, -6.94274902e-04]],\n",
      "\n",
      "        [[-9.00268555e-04, -3.21960449e-03, -4.73022461e-04, ...,\n",
      "          -3.32641602e-03, -5.70678711e-03,  8.31604004e-04],\n",
      "         [-5.49316406e-04,  1.56402588e-04,  2.21252441e-03, ...,\n",
      "           1.77001953e-03, -4.61578369e-04,  1.13677979e-03],\n",
      "         [-1.66893005e-04,  4.48226929e-04, -2.27355957e-03, ...,\n",
      "           6.21795654e-04, -5.37872314e-04, -3.29971313e-04],\n",
      "         ...,\n",
      "         [-2.92968750e-03,  3.93676758e-03, -1.83105469e-03, ...,\n",
      "           3.58581543e-03, -2.07901001e-04, -3.43322754e-03],\n",
      "         [-4.74929810e-04, -2.02941895e-03,  3.52859497e-04, ...,\n",
      "           3.01361084e-04,  9.23156738e-04,  2.38037109e-03],\n",
      "         [ 3.84521484e-03,  6.79016113e-04,  3.03649902e-03, ...,\n",
      "          -6.02722168e-04, -1.04427338e-04,  5.52368164e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.94297934e-07, -1.15203857e-03, -6.82473183e-06, ...,\n",
      "           3.06701660e-03,  2.45666504e-03, -4.80651855e-04],\n",
      "         [-1.46484375e-03,  5.76019287e-04, -3.07083130e-04, ...,\n",
      "          -2.59399414e-03,  3.23486328e-03, -1.18255615e-03],\n",
      "         [ 1.51062012e-03,  2.19726562e-03, -4.42504883e-04, ...,\n",
      "           6.29425049e-04, -1.80053711e-03, -1.50680542e-04],\n",
      "         ...,\n",
      "         [ 1.37329102e-03, -7.17163086e-04,  1.48773193e-03, ...,\n",
      "          -1.64031982e-03,  2.86865234e-03, -9.84191895e-04],\n",
      "         [-7.97271729e-04,  3.50952148e-04, -2.25830078e-03, ...,\n",
      "          -2.19726562e-03, -1.13677979e-03, -5.26428223e-04],\n",
      "         [-9.84191895e-04, -1.19209290e-04, -1.95312500e-03, ...,\n",
      "           8.58306885e-04, -7.59124756e-04,  1.02233887e-03]],\n",
      "\n",
      "        [[-2.78472900e-04, -2.28881836e-04,  1.42669678e-03, ...,\n",
      "          -5.73730469e-03, -3.28063965e-03,  1.28173828e-03],\n",
      "         [-1.99317932e-04,  1.72424316e-03, -8.48770142e-05, ...,\n",
      "          -3.49426270e-03, -2.10571289e-03,  5.37872314e-04],\n",
      "         [ 1.89208984e-03, -6.02722168e-04, -1.15966797e-03, ...,\n",
      "           2.97546387e-04, -2.30407715e-03, -5.83648682e-04],\n",
      "         ...,\n",
      "         [ 2.27355957e-03,  3.64780426e-05,  1.28936768e-03, ...,\n",
      "          -9.07897949e-04,  4.11987305e-04, -9.49859619e-04],\n",
      "         [ 1.44958496e-03, -1.43432617e-03, -3.26538086e-03, ...,\n",
      "          -2.47192383e-03,  6.14166260e-04, -1.90734863e-03],\n",
      "         [-1.64794922e-03, -1.11389160e-03,  9.72747803e-04, ...,\n",
      "           1.28173828e-03,  1.58691406e-03, -6.40869141e-04]],\n",
      "\n",
      "        [[ 1.48773193e-03,  2.30789185e-04,  1.47819519e-04, ...,\n",
      "           3.52859497e-04, -1.63269043e-03, -1.99890137e-03],\n",
      "         [ 5.11169434e-04,  1.27410889e-03, -2.46047974e-04, ...,\n",
      "          -8.69750977e-04, -1.03759766e-03,  2.12097168e-03],\n",
      "         [-2.21252441e-04, -1.48773193e-03,  2.45666504e-03, ...,\n",
      "          -3.02124023e-03,  2.16674805e-03,  1.25885010e-03],\n",
      "         ...,\n",
      "         [-9.00268555e-04,  5.22613525e-04,  2.78472900e-04, ...,\n",
      "          -6.21795654e-04,  4.79125977e-03, -1.64794922e-03],\n",
      "         [ 1.00708008e-03, -2.67028809e-03, -4.76837158e-04, ...,\n",
      "           8.81195068e-04,  9.53674316e-04,  1.95312500e-03],\n",
      "         [-1.43432617e-03,  9.49859619e-04, -3.84521484e-03, ...,\n",
      "          -1.11389160e-03, -1.82151794e-04,  3.63159180e-03]]],\n",
      "\n",
      "\n",
      "       [[[ 1.50680542e-04, -1.00708008e-03,  1.66320801e-03, ...,\n",
      "          -6.21795654e-04,  7.43865967e-04, -1.20162964e-04],\n",
      "         [-6.59942627e-04, -1.94549561e-04, -1.34468079e-04, ...,\n",
      "          -1.03759766e-03,  2.80380249e-04, -8.46862793e-04],\n",
      "         [ 1.64985657e-04,  1.18255615e-04, -2.15530396e-04, ...,\n",
      "          -6.94274902e-04, -7.32421875e-04, -8.58306885e-04],\n",
      "         ...,\n",
      "         [-7.17163086e-04, -2.44140625e-04, -8.35418701e-04, ...,\n",
      "          -1.75476074e-04, -1.45721436e-03, -1.36566162e-03],\n",
      "         [ 1.46865845e-04,  3.01361084e-04, -4.08172607e-04, ...,\n",
      "          -7.29560852e-05,  3.50952148e-04,  3.43322754e-04],\n",
      "         [-1.91688538e-04,  2.20298767e-04,  4.69684601e-05, ...,\n",
      "          -2.63977051e-03, -1.38092041e-03,  1.46484375e-03]],\n",
      "\n",
      "        [[-5.18798828e-03, -4.30297852e-03, -6.59179688e-03, ...,\n",
      "           6.79016113e-04,  1.09863281e-03, -1.03759766e-03],\n",
      "         [ 7.28607178e-04, -9.48905945e-05, -2.20298767e-04, ...,\n",
      "          -2.26974487e-04, -5.43594360e-05, -3.16619873e-04],\n",
      "         [ 8.46862793e-04, -7.78198242e-04,  5.07354736e-04, ...,\n",
      "           2.07519531e-03,  2.44140625e-03, -5.64575195e-04],\n",
      "         ...,\n",
      "         [-6.79016113e-04,  2.02178955e-04,  6.67572021e-04, ...,\n",
      "          -4.63485718e-04,  8.08715820e-04, -1.28173828e-03],\n",
      "         [ 6.10351562e-04, -9.38415527e-04, -1.49726868e-04, ...,\n",
      "           2.61306763e-04,  2.78472900e-04, -1.60980225e-03],\n",
      "         [-5.37872314e-04, -6.21795654e-04,  1.83105469e-03, ...,\n",
      "          -3.45230103e-04,  4.57763672e-04, -4.45842743e-05]],\n",
      "\n",
      "        [[-1.64794922e-03, -1.57165527e-03, -1.77764893e-03, ...,\n",
      "           3.43322754e-03,  4.02832031e-03, -2.02941895e-03],\n",
      "         [ 8.50677490e-04,  9.72747803e-04,  1.36566162e-03, ...,\n",
      "          -1.51062012e-03, -6.43730164e-05,  2.20298767e-04],\n",
      "         [ 1.24359131e-03,  1.09863281e-03, -2.97546387e-04, ...,\n",
      "           2.76184082e-03, -2.78472900e-04, -8.88824463e-04],\n",
      "         ...,\n",
      "         [-1.27410889e-03, -2.19726562e-03, -1.09863281e-03, ...,\n",
      "           6.21795654e-04, -6.44683838e-04, -2.09045410e-03],\n",
      "         [ 1.19018555e-03, -9.84191895e-04, -1.46484375e-03, ...,\n",
      "          -1.72424316e-03,  5.30242920e-04,  3.79943848e-03],\n",
      "         [ 2.01416016e-03,  3.84521484e-03, -1.64031982e-03, ...,\n",
      "          -2.67028809e-03, -1.38092041e-03, -4.15039062e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.70025635e-04,  6.07967377e-05, -7.93457031e-04, ...,\n",
      "           4.73022461e-04,  1.26647949e-03,  3.79562378e-04],\n",
      "         [-1.15203857e-03,  9.00268555e-04, -1.46484375e-03, ...,\n",
      "           6.48498535e-04, -2.45666504e-03,  9.46044922e-04],\n",
      "         [ 4.11987305e-04,  1.96456909e-04,  1.11389160e-03, ...,\n",
      "          -3.63159180e-03, -3.40270996e-03,  2.80761719e-03],\n",
      "         ...,\n",
      "         [-1.06811523e-03,  1.02996826e-03, -1.09863281e-03, ...,\n",
      "           1.87683105e-03,  2.60925293e-03, -1.28173828e-03],\n",
      "         [-3.14712524e-04,  1.29699707e-03,  6.79016113e-04, ...,\n",
      "          -3.23486328e-03,  9.61303711e-04,  3.08990479e-04],\n",
      "         [ 5.62667847e-05, -2.05993652e-03,  1.59454346e-03, ...,\n",
      "          -8.85009766e-04, -1.96838379e-03,  6.10351562e-04]],\n",
      "\n",
      "        [[-4.50134277e-04, -8.43048096e-04,  3.70025635e-04, ...,\n",
      "          -1.15203857e-03, -9.11712646e-04,  2.22778320e-03],\n",
      "         [ 6.29425049e-04,  2.10571289e-03,  1.32751465e-03, ...,\n",
      "          -6.14166260e-04,  6.90460205e-04, -3.64685059e-03],\n",
      "         [ 2.21252441e-03, -8.85009766e-04,  7.13348389e-04, ...,\n",
      "           7.28607178e-04, -1.66320801e-03, -1.57356262e-04],\n",
      "         ...,\n",
      "         [-3.73840332e-03, -1.48010254e-03,  1.12915039e-03, ...,\n",
      "          -8.35418701e-04,  2.12097168e-03,  4.55856323e-04],\n",
      "         [ 3.06701660e-03,  1.89971924e-03, -1.57356262e-05, ...,\n",
      "          -1.70135498e-03,  3.54003906e-03, -3.17382812e-03],\n",
      "         [-3.41415405e-04,  1.67083740e-03, -3.50475311e-05, ...,\n",
      "          -8.88824463e-04,  9.34600830e-04, -5.27501106e-06]],\n",
      "\n",
      "        [[-7.00950623e-05, -2.97546387e-04,  2.33650208e-04, ...,\n",
      "           5.91278076e-04,  3.08227539e-03, -2.68554688e-03],\n",
      "         [ 2.04467773e-03, -6.90460205e-04,  3.41415405e-04, ...,\n",
      "           1.61743164e-03,  4.88281250e-04,  4.02832031e-03],\n",
      "         [ 2.72750854e-04, -1.71661377e-03,  7.55310059e-04, ...,\n",
      "          -4.08935547e-03,  1.35803223e-03,  3.58581543e-03],\n",
      "         ...,\n",
      "         [ 7.05718994e-05, -2.48718262e-03, -2.50244141e-03, ...,\n",
      "          -3.18908691e-03, -3.66210938e-03,  8.46862793e-04],\n",
      "         [ 1.10626221e-03,  9.15527344e-05, -3.24249268e-04, ...,\n",
      "          -3.14712524e-04,  2.47955322e-05,  2.22778320e-03],\n",
      "         [ 1.21307373e-03, -5.79833984e-04,  3.02124023e-03, ...,\n",
      "           2.13623047e-03,  1.07765198e-04,  1.07574463e-03]]],\n",
      "\n",
      "\n",
      "       [[[-3.20434570e-04,  6.71386719e-04, -2.76565552e-04, ...,\n",
      "          -6.94274902e-04,  1.16729736e-03, -9.23156738e-04],\n",
      "         [-8.01086426e-05, -2.41279602e-04,  1.82151794e-04, ...,\n",
      "          -4.02450562e-04,  7.85827637e-04, -4.86373901e-04],\n",
      "         [ 1.31607056e-04,  1.37329102e-04,  3.69548798e-05, ...,\n",
      "          -1.95312500e-03, -1.90734863e-03, -1.72424316e-03],\n",
      "         ...,\n",
      "         [-5.64575195e-04, -2.46047974e-04, -4.88281250e-04, ...,\n",
      "           8.85009766e-04,  9.72747803e-04,  1.96456909e-04],\n",
      "         [-3.83853912e-05, -3.16619873e-04,  2.02178955e-04, ...,\n",
      "           1.99317932e-04, -3.14712524e-05, -5.25265932e-07],\n",
      "         [-2.52723694e-05,  8.88109207e-06, -7.91549683e-05, ...,\n",
      "           8.58306885e-04, -1.09100342e-03, -7.28607178e-04]],\n",
      "\n",
      "        [[ 3.14331055e-03,  2.45666504e-03,  2.02941895e-03, ...,\n",
      "           4.97436523e-03,  2.15530396e-04, -6.40869141e-04],\n",
      "         [ 2.89916992e-03,  3.85284424e-04, -1.33514404e-03, ...,\n",
      "          -1.70135498e-03, -1.11389160e-03,  3.66210938e-04],\n",
      "         [ 3.79943848e-03, -1.70135498e-03, -5.49316406e-04, ...,\n",
      "           3.38745117e-03,  2.44140625e-04, -7.32421875e-04],\n",
      "         ...,\n",
      "         [ 8.23974609e-04,  9.10758972e-05, -1.23596191e-03, ...,\n",
      "           3.33786011e-04,  6.10351562e-04, -2.07901001e-04],\n",
      "         [-1.66320801e-03, -6.33239746e-04,  1.00708008e-03, ...,\n",
      "           1.71661377e-03, -1.21593475e-04,  9.26971436e-04],\n",
      "         [-2.33459473e-03,  1.54495239e-04, -3.92913818e-04, ...,\n",
      "          -5.34057617e-04,  5.45501709e-04, -9.84191895e-04]],\n",
      "\n",
      "        [[-6.86645508e-04,  2.39562988e-03,  1.86920166e-03, ...,\n",
      "          -8.08715820e-04, -2.80761719e-03, -1.09863281e-03],\n",
      "         [-8.91685486e-05,  1.22833252e-03, -2.82287598e-03, ...,\n",
      "          -9.07897949e-04,  5.84125519e-05,  4.50134277e-04],\n",
      "         [ 6.67572021e-04,  1.93023682e-03,  9.07897949e-04, ...,\n",
      "          -5.37109375e-03, -7.55310059e-04,  1.06048584e-03],\n",
      "         ...,\n",
      "         [ 6.59942627e-04, -1.83105469e-04, -1.15203857e-03, ...,\n",
      "          -2.34985352e-03,  2.38037109e-03,  5.45501709e-04],\n",
      "         [ 2.33459473e-03, -1.91688538e-04, -2.38037109e-03, ...,\n",
      "           5.45501709e-04,  6.82830811e-04, -5.03540039e-03],\n",
      "         [ 7.43865967e-04, -1.72424316e-03,  8.69750977e-04, ...,\n",
      "          -3.21960449e-03,  1.20544434e-03,  1.92642212e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.67300415e-04, -4.34875488e-04,  1.28936768e-03, ...,\n",
      "           2.01416016e-03,  2.91442871e-03, -4.21524048e-04],\n",
      "         [ 2.22778320e-03, -2.19726562e-03,  1.12152100e-03, ...,\n",
      "           2.63977051e-03,  1.35040283e-03, -1.57356262e-04],\n",
      "         [-7.43865967e-04, -5.56945801e-04, -9.46044922e-04, ...,\n",
      "           2.53295898e-03,  1.55639648e-03,  1.67083740e-03],\n",
      "         ...,\n",
      "         [ 1.31607056e-04, -9.07897949e-04, -1.45721436e-03, ...,\n",
      "          -2.12669373e-04,  1.86920166e-03,  5.64575195e-04],\n",
      "         [ 1.00708008e-03, -6.48498535e-04,  3.79562378e-04, ...,\n",
      "           4.08935547e-03,  1.29699707e-03,  3.11279297e-03],\n",
      "         [ 1.93023682e-03,  3.21960449e-03, -2.27928162e-04, ...,\n",
      "          -2.86865234e-03,  1.94549561e-03, -3.52478027e-03]],\n",
      "\n",
      "        [[-2.10762024e-04, -1.67083740e-03, -2.77709961e-03, ...,\n",
      "          -9.46044922e-04,  9.99450684e-04, -3.26538086e-03],\n",
      "         [ 1.02519989e-04, -1.07288361e-04, -2.80380249e-04, ...,\n",
      "          -2.49862671e-04,  2.30407715e-03, -2.31933594e-03],\n",
      "         [ 6.02722168e-04, -4.99725342e-04,  2.22778320e-03, ...,\n",
      "          -7.59124756e-04, -8.96453857e-04, -1.15203857e-03],\n",
      "         ...,\n",
      "         [ 3.18908691e-03,  2.92968750e-03, -1.22070312e-03, ...,\n",
      "          -4.02832031e-03, -3.06701660e-03,  4.97436523e-03],\n",
      "         [-1.17492676e-03, -1.74713135e-03,  2.42614746e-03, ...,\n",
      "           4.57763672e-03,  2.33459473e-03,  3.60107422e-03],\n",
      "         [ 8.62121582e-04, -2.28881836e-03, -7.05718994e-04, ...,\n",
      "           4.10079956e-04, -2.86865234e-03, -5.18798828e-04]],\n",
      "\n",
      "        [[-1.85394287e-03, -6.48498535e-04,  1.53541565e-04, ...,\n",
      "          -1.18017197e-05,  3.24249268e-04, -7.01904297e-04],\n",
      "         [ 6.10947609e-06,  3.98635864e-04, -2.54821777e-03, ...,\n",
      "          -7.17163086e-04,  8.50677490e-04, -1.50299072e-03],\n",
      "         [-2.55584717e-04,  6.21795654e-04,  1.42097473e-04, ...,\n",
      "          -7.05718994e-04,  9.84191895e-04, -6.33239746e-04],\n",
      "         ...,\n",
      "         [ 8.31604004e-04,  1.22070312e-03,  1.33514404e-03, ...,\n",
      "           7.43865967e-04,  6.14166260e-04,  1.19781494e-03],\n",
      "         [ 1.27410889e-03,  1.96838379e-03,  6.72340393e-05, ...,\n",
      "           1.99890137e-03,  5.85937500e-03,  1.28936768e-03],\n",
      "         [-1.00708008e-03,  6.25610352e-04,  1.83105469e-03, ...,\n",
      "          -2.07519531e-03,  3.01361084e-04,  2.65502930e-03]]]],      dtype=float32)}, 'value': {'kernel': Array([[[[ 7.85827637e-04, -6.86645508e-03,  1.82342529e-03, ...,\n",
      "          -8.17871094e-03,  1.12304688e-02,  8.34465027e-05],\n",
      "         [-1.60217285e-03, -4.94384766e-03, -4.42504883e-03, ...,\n",
      "           6.13403320e-03, -2.24304199e-03, -8.11767578e-03],\n",
      "         [ 1.25122070e-02, -1.23596191e-03,  2.59399414e-03, ...,\n",
      "          -1.00708008e-02,  1.59454346e-03, -1.08337402e-03],\n",
      "         ...,\n",
      "         [ 1.85546875e-02,  2.66113281e-02,  1.67846680e-03, ...,\n",
      "           6.10351562e-03, -2.55126953e-02,  1.67236328e-02],\n",
      "         [-2.94494629e-03,  1.00708008e-02,  4.76074219e-03, ...,\n",
      "          -5.40161133e-03, -1.11694336e-02, -5.24902344e-03],\n",
      "         [ 1.08032227e-02,  6.22558594e-03,  7.32421875e-03, ...,\n",
      "          -6.31713867e-03,  3.05175781e-03,  1.35421753e-04]],\n",
      "\n",
      "        [[ 6.83593750e-03, -3.55529785e-03,  5.92041016e-03, ...,\n",
      "           1.58691406e-03, -6.07299805e-03, -3.38745117e-03],\n",
      "         [ 1.12915039e-03, -3.90625000e-03, -2.51770020e-03, ...,\n",
      "          -1.99890137e-03, -4.91333008e-03,  2.18200684e-03],\n",
      "         [ 5.00488281e-03,  4.94384766e-03, -7.14111328e-03, ...,\n",
      "          -1.87683105e-03,  9.70458984e-03,  3.96728516e-03],\n",
      "         ...,\n",
      "         [ 9.11712646e-04,  3.43322754e-03,  2.89916992e-03, ...,\n",
      "          -7.32421875e-03,  1.92260742e-03, -3.06701660e-03],\n",
      "         [ 1.11694336e-02,  3.20434570e-03,  1.08642578e-02, ...,\n",
      "          -1.68457031e-02,  5.88989258e-03,  8.96453857e-04],\n",
      "         [ 5.64575195e-03, -3.11279297e-03, -9.04083252e-04, ...,\n",
      "          -5.27954102e-03, -6.28662109e-03, -1.02233887e-03]],\n",
      "\n",
      "        [[-4.54711914e-03,  2.83813477e-03, -7.01904297e-04, ...,\n",
      "           5.70678711e-03,  2.94189453e-02,  9.82666016e-03],\n",
      "         [ 2.35595703e-02,  7.81250000e-03, -1.96533203e-02, ...,\n",
      "          -2.22167969e-02,  1.39770508e-02, -9.39941406e-03],\n",
      "         [ 2.53295898e-03, -1.09863281e-03, -9.76562500e-03, ...,\n",
      "          -2.79235840e-03, -1.55029297e-02, -9.09423828e-03],\n",
      "         ...,\n",
      "         [-8.27789307e-04,  1.31225586e-02, -2.10571289e-03, ...,\n",
      "           8.11767578e-03,  4.94384766e-03,  1.10473633e-02],\n",
      "         [ 1.43051147e-04,  4.18090820e-03,  2.14843750e-02, ...,\n",
      "           1.42822266e-02,  4.51660156e-03, -2.64892578e-02],\n",
      "         [-1.30004883e-02,  2.10571289e-03, -2.31933594e-03, ...,\n",
      "           3.70025635e-04, -3.66210938e-02, -1.22070312e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.18408203e-02, -3.03649902e-03, -1.44042969e-02, ...,\n",
      "           2.28271484e-02, -1.39770508e-02, -5.46875000e-02],\n",
      "         [ 1.72424316e-03,  4.36401367e-03, -4.21142578e-03, ...,\n",
      "          -1.25122070e-02, -4.78515625e-02, -3.35693359e-03],\n",
      "         [ 1.87683105e-03,  2.06298828e-02,  7.08007812e-03, ...,\n",
      "          -3.32031250e-02,  8.85009766e-03, -1.03759766e-02],\n",
      "         ...,\n",
      "         [ 3.52859497e-04,  3.09753418e-03, -1.91650391e-02, ...,\n",
      "          -1.19018555e-02, -9.52148438e-03,  3.67736816e-03],\n",
      "         [ 2.07519531e-02, -7.04956055e-03,  2.64892578e-02, ...,\n",
      "          -7.01904297e-03,  1.11083984e-02, -3.12500000e-02],\n",
      "         [-1.08642578e-02, -1.06811523e-02, -9.27734375e-03, ...,\n",
      "           8.72802734e-03,  4.36306000e-05,  2.10762024e-04]],\n",
      "\n",
      "        [[-2.41088867e-03,  1.48315430e-02, -1.84326172e-02, ...,\n",
      "           3.87573242e-03, -1.64794922e-02,  1.05285645e-03],\n",
      "         [ 6.19506836e-03,  1.40991211e-02, -1.68609619e-03, ...,\n",
      "           1.63574219e-02, -9.15527344e-03, -2.74658203e-02],\n",
      "         [ 1.37939453e-02,  6.53076172e-03,  6.14166260e-04, ...,\n",
      "          -1.95312500e-03, -2.29492188e-02, -1.74560547e-02],\n",
      "         ...,\n",
      "         [-7.62939453e-03,  6.82830811e-04,  1.96533203e-02, ...,\n",
      "          -5.12695312e-03,  7.69042969e-03, -3.63769531e-02],\n",
      "         [ 6.31713867e-03, -9.82666016e-03, -2.51770020e-03, ...,\n",
      "           2.18505859e-02,  4.39453125e-02,  1.02539062e-02],\n",
      "         [ 5.98907471e-04, -2.84423828e-02, -2.10571289e-03, ...,\n",
      "           1.86767578e-02, -1.83105469e-03, -1.09863281e-03]],\n",
      "\n",
      "        [[ 1.30004883e-02,  2.13623047e-02,  1.21459961e-02, ...,\n",
      "           1.14746094e-02, -5.24902344e-02,  1.23291016e-02],\n",
      "         [ 3.02734375e-02,  2.03857422e-02,  1.39160156e-02, ...,\n",
      "          -1.38549805e-02, -2.22167969e-02, -9.03320312e-03],\n",
      "         [ 7.35473633e-03,  1.28173828e-02, -7.72094727e-03, ...,\n",
      "           2.65502930e-03,  2.91442871e-03, -1.49536133e-02],\n",
      "         ...,\n",
      "         [ 2.49023438e-02,  2.44140625e-03, -1.68457031e-02, ...,\n",
      "          -1.06811523e-02,  1.44653320e-02,  1.64794922e-02],\n",
      "         [-2.88391113e-03, -5.34057617e-03,  1.26953125e-02, ...,\n",
      "          -9.76562500e-03, -3.99780273e-03, -2.49023438e-02],\n",
      "         [-5.46264648e-03,  4.34570312e-02, -1.05590820e-02, ...,\n",
      "          -2.83813477e-03,  1.09252930e-02, -1.17797852e-02]]],\n",
      "\n",
      "\n",
      "       [[[-5.95092773e-04, -4.57763672e-04,  9.64355469e-03, ...,\n",
      "          -4.82177734e-03,  2.13623047e-03, -1.00135803e-04],\n",
      "         [-3.07559967e-05, -3.79943848e-03,  3.43322754e-03, ...,\n",
      "           1.22070312e-02, -1.20239258e-02, -1.47247314e-03],\n",
      "         [ 6.77490234e-03, -8.23974609e-04,  6.62231445e-03, ...,\n",
      "          -2.71606445e-03,  1.21459961e-02, -5.52368164e-03],\n",
      "         ...,\n",
      "         [-2.22167969e-02, -8.30078125e-03, -9.03320312e-03, ...,\n",
      "           2.50244141e-03,  4.88281250e-02,  9.27734375e-03],\n",
      "         [-4.48608398e-03,  3.82995605e-03, -5.61523438e-03, ...,\n",
      "           7.23266602e-03,  2.42614746e-03,  1.13525391e-02],\n",
      "         [ 3.14331055e-03, -1.09863281e-02,  4.85229492e-03, ...,\n",
      "          -5.70678711e-03,  4.79125977e-03,  2.53295898e-03]],\n",
      "\n",
      "        [[-8.36181641e-03,  2.38037109e-03, -1.18408203e-02, ...,\n",
      "           4.76074219e-03,  1.49536133e-02,  6.89697266e-03],\n",
      "         [-1.78337097e-04, -2.53295898e-03,  1.16729736e-03, ...,\n",
      "          -1.84631348e-03, -3.08227539e-03, -8.81195068e-04],\n",
      "         [-8.97216797e-03, -2.82287598e-03, -1.97753906e-02, ...,\n",
      "           1.37329102e-02, -7.23266602e-03, -1.10473633e-02],\n",
      "         ...,\n",
      "         [ 3.75747681e-04,  7.47680664e-03,  2.41279602e-04, ...,\n",
      "          -5.46264648e-03,  9.27734375e-03, -7.26318359e-03],\n",
      "         [-3.75366211e-03,  2.38037109e-02,  7.32421875e-03, ...,\n",
      "           7.69042969e-03, -1.44004822e-04,  1.18255615e-03],\n",
      "         [-8.11767578e-03,  5.92041016e-03, -1.06811523e-03, ...,\n",
      "           2.70080566e-03, -5.58471680e-03,  1.64031982e-03]],\n",
      "\n",
      "        [[ 7.66754150e-04,  1.40991211e-02,  1.51977539e-02, ...,\n",
      "           2.50244141e-03, -1.26953125e-02, -1.53808594e-02],\n",
      "         [ 2.75878906e-02,  1.64794922e-02, -2.47802734e-02, ...,\n",
      "          -1.62353516e-02,  1.12915039e-02,  5.21850586e-03],\n",
      "         [-6.71386719e-03, -6.07299805e-03,  1.18255615e-03, ...,\n",
      "           1.66015625e-02,  4.02832031e-03, -2.42919922e-02],\n",
      "         ...,\n",
      "         [ 1.53198242e-02, -1.02996826e-03,  1.42822266e-02, ...,\n",
      "           2.53906250e-02, -4.82177734e-03,  2.29492188e-02],\n",
      "         [-9.94873047e-03, -1.09252930e-02,  6.01196289e-03, ...,\n",
      "           7.44628906e-03, -1.55029297e-02,  1.08032227e-02],\n",
      "         [-7.17163086e-03, -2.77099609e-02,  1.34887695e-02, ...,\n",
      "          -3.06396484e-02, -6.40869141e-04,  1.68457031e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.03540039e-03,  2.41699219e-02,  5.73730469e-03, ...,\n",
      "          -2.78320312e-02, -2.90527344e-02, -2.92968750e-02],\n",
      "         [-1.83105469e-02, -1.23901367e-02, -1.13525391e-02, ...,\n",
      "          -2.57568359e-02,  9.58251953e-03,  3.46679688e-02],\n",
      "         [ 5.95092773e-03,  1.26342773e-02, -4.19921875e-02, ...,\n",
      "           3.15856934e-03, -1.85546875e-02,  1.06811523e-02],\n",
      "         ...,\n",
      "         [-7.11059570e-03, -3.28063965e-03, -3.75976562e-02, ...,\n",
      "          -9.39941406e-03, -2.83203125e-02,  1.72119141e-02],\n",
      "         [-3.60107422e-03, -5.37109375e-03, -2.50244141e-02, ...,\n",
      "          -1.47705078e-02, -7.14111328e-03, -3.90625000e-03],\n",
      "         [-2.44140625e-02,  2.12097168e-03, -3.01513672e-02, ...,\n",
      "           2.22167969e-02,  3.46679688e-02, -1.69677734e-02]],\n",
      "\n",
      "        [[-3.54003906e-02,  2.51464844e-02,  1.33056641e-02, ...,\n",
      "           2.60009766e-02, -4.97436523e-03,  3.83300781e-02],\n",
      "         [ 9.33837891e-03, -2.29492188e-02, -4.39453125e-02, ...,\n",
      "           2.00195312e-02, -4.15039062e-02, -2.80761719e-02],\n",
      "         [ 4.02832031e-02, -2.12402344e-02, -1.84326172e-02, ...,\n",
      "          -9.82666016e-03,  1.97753906e-02,  5.85937500e-03],\n",
      "         ...,\n",
      "         [ 1.14746094e-02,  1.25885010e-03, -3.10897827e-04, ...,\n",
      "          -9.21630859e-03, -2.36816406e-02, -2.57873535e-03],\n",
      "         [-2.27050781e-02,  3.34472656e-02,  3.50952148e-03, ...,\n",
      "           1.03149414e-02,  3.41415405e-04,  7.81250000e-03],\n",
      "         [-1.87988281e-02, -3.35693359e-03,  4.10156250e-02, ...,\n",
      "          -5.82885742e-03, -4.68750000e-02,  5.40161133e-03]],\n",
      "\n",
      "        [[-2.78320312e-02,  7.72094727e-03, -3.37219238e-03, ...,\n",
      "           1.35803223e-03, -1.44042969e-02, -1.95312500e-02],\n",
      "         [ 3.23486328e-03,  1.20849609e-02,  3.72314453e-03, ...,\n",
      "           2.13623047e-02, -2.09960938e-02,  1.27563477e-02],\n",
      "         [ 3.49426270e-03, -1.15966797e-02, -4.15039062e-03, ...,\n",
      "           1.01928711e-02, -3.11279297e-02, -9.15527344e-03],\n",
      "         ...,\n",
      "         [ 3.29589844e-02,  4.15039062e-02, -1.04370117e-02, ...,\n",
      "           3.66210938e-03, -1.90429688e-02,  3.70788574e-03],\n",
      "         [ 8.42285156e-03, -2.14843750e-02,  1.00097656e-02, ...,\n",
      "           3.89099121e-03, -2.80761719e-02,  2.31933594e-02],\n",
      "         [-3.22265625e-02, -4.98046875e-02, -3.10058594e-02, ...,\n",
      "          -8.48388672e-03, -2.41699219e-02, -2.55126953e-02]]],\n",
      "\n",
      "\n",
      "       [[[ 1.88446045e-03, -7.72094727e-03,  9.99450684e-04, ...,\n",
      "           8.34465027e-05,  3.34167480e-03,  2.12097168e-03],\n",
      "         [ 1.28936768e-03, -5.00488281e-03,  3.87573242e-03, ...,\n",
      "           4.36401367e-03,  4.33349609e-03,  2.41088867e-03],\n",
      "         [-2.80761719e-03, -3.35693359e-03, -2.86865234e-03, ...,\n",
      "           2.34603882e-04, -4.11987305e-04,  3.81469727e-03],\n",
      "         ...,\n",
      "         [-8.48388672e-03, -1.59912109e-02, -1.50146484e-02, ...,\n",
      "           5.49316406e-03,  1.33666992e-02,  1.58691406e-02],\n",
      "         [-6.62231445e-03,  3.17382812e-03, -6.83593750e-03, ...,\n",
      "          -1.09863281e-02,  1.30462646e-03,  2.30407715e-03],\n",
      "         [-1.88827515e-04,  5.40161133e-03, -5.18798828e-03, ...,\n",
      "           1.02539062e-02, -9.76562500e-04,  5.55419922e-03]],\n",
      "\n",
      "        [[-4.08935547e-03, -2.39562988e-03,  1.08642578e-02, ...,\n",
      "          -2.05993652e-03,  5.09643555e-03, -4.05883789e-03],\n",
      "         [-3.09753418e-03,  2.05993652e-03, -2.04467773e-03, ...,\n",
      "          -2.18200684e-03,  3.35693359e-04,  7.20977783e-04],\n",
      "         [ 1.77001953e-02, -6.74438477e-03,  1.80664062e-02, ...,\n",
      "           2.67333984e-02, -6.83593750e-03, -1.31835938e-02],\n",
      "         ...,\n",
      "         [-3.69262695e-03,  1.00097656e-02, -3.25012207e-03, ...,\n",
      "          -6.16455078e-03,  1.06811523e-03,  4.05883789e-03],\n",
      "         [-5.58471680e-03,  2.52685547e-02,  1.03712082e-05, ...,\n",
      "           7.26318359e-03,  1.92260742e-03, -4.91333008e-03],\n",
      "         [ 3.92913818e-04, -1.60980225e-03,  2.99072266e-03, ...,\n",
      "           4.63867188e-03,  9.39941406e-03,  4.15039062e-03]],\n",
      "\n",
      "        [[ 2.12402344e-02, -2.18505859e-02, -1.26647949e-03, ...,\n",
      "           3.12500000e-02,  4.45556641e-03,  8.78906250e-03],\n",
      "         [ 6.50024414e-03,  2.97851562e-02,  1.26953125e-02, ...,\n",
      "          -2.36816406e-02,  4.95910645e-04,  4.08935547e-03],\n",
      "         [-2.41699219e-02, -7.56835938e-03, -2.14843750e-02, ...,\n",
      "           9.39941406e-03, -1.16729736e-03,  5.61523438e-03],\n",
      "         ...,\n",
      "         [-1.39160156e-02,  1.11083984e-02, -1.86767578e-02, ...,\n",
      "          -7.75146484e-03, -1.98974609e-02, -2.18505859e-02],\n",
      "         [-1.14746094e-02,  2.25830078e-02, -1.02539062e-02, ...,\n",
      "          -1.36718750e-02,  2.44140625e-03,  4.60815430e-03],\n",
      "         [ 1.59912109e-02, -2.74658203e-02,  1.21459961e-02, ...,\n",
      "          -1.90429688e-02,  1.78222656e-02,  1.34277344e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.99072266e-02,  4.07714844e-02, -8.36181641e-03, ...,\n",
      "          -3.19824219e-02,  1.14135742e-02,  3.88183594e-02],\n",
      "         [-2.86865234e-03,  1.09252930e-02,  3.56445312e-02, ...,\n",
      "           5.54199219e-02, -2.45361328e-02, -4.02450562e-04],\n",
      "         [ 1.89208984e-02, -1.20849609e-02,  3.66210938e-02, ...,\n",
      "           2.19726562e-02,  9.11712646e-04, -4.82559204e-04],\n",
      "         ...,\n",
      "         [-2.89306641e-02, -2.25830078e-03, -2.08740234e-02, ...,\n",
      "           1.14135742e-02, -2.24609375e-02, -8.54492188e-03],\n",
      "         [-4.30297852e-03,  1.49536133e-02, -2.06298828e-02, ...,\n",
      "          -4.37011719e-02,  1.75781250e-02,  1.57470703e-02],\n",
      "         [ 1.40380859e-02,  1.43432617e-02,  1.54876709e-03, ...,\n",
      "           1.21459961e-02, -2.42919922e-02,  6.95800781e-03]],\n",
      "\n",
      "        [[ 2.56347656e-02,  3.01513672e-02, -1.70898438e-02, ...,\n",
      "          -2.11181641e-02,  2.11181641e-02,  1.25732422e-02],\n",
      "         [-1.44042969e-02, -2.51770020e-03,  2.81982422e-02, ...,\n",
      "          -2.49023438e-02,  2.58789062e-02, -2.10571289e-03],\n",
      "         [ 8.42285156e-03,  2.05993652e-04, -1.00097656e-02, ...,\n",
      "          -2.88085938e-02, -6.46972656e-03, -2.16064453e-02],\n",
      "         ...,\n",
      "         [ 4.22363281e-02,  1.58691406e-02,  1.28173828e-02, ...,\n",
      "           3.51562500e-02,  1.06201172e-02,  1.09252930e-02],\n",
      "         [ 3.01513672e-02,  1.51367188e-02,  3.32641602e-03, ...,\n",
      "          -1.14746094e-02,  9.27734375e-03,  2.51770020e-03],\n",
      "         [ 8.36181641e-03,  5.09643555e-03,  1.32446289e-02, ...,\n",
      "           6.59942627e-04,  2.18505859e-02,  1.61132812e-02]],\n",
      "\n",
      "        [[-4.88281250e-03,  1.74560547e-02, -1.59912109e-02, ...,\n",
      "           2.64892578e-02,  7.99560547e-03,  2.81982422e-02],\n",
      "         [-2.27050781e-02, -2.33154297e-02,  1.74560547e-02, ...,\n",
      "           7.81250000e-03,  3.71093750e-02,  2.47802734e-02],\n",
      "         [-4.51660156e-03, -6.71386719e-03,  1.33666992e-02, ...,\n",
      "          -1.15966797e-03,  2.30407715e-03, -3.27148438e-02],\n",
      "         ...,\n",
      "         [-1.62353516e-02, -5.06591797e-03, -1.00097656e-02, ...,\n",
      "          -6.89697266e-03, -4.83398438e-02,  4.73022461e-03],\n",
      "         [-3.03955078e-02, -2.01416016e-02, -7.38525391e-03, ...,\n",
      "          -1.34887695e-02, -8.97216797e-03,  1.55029297e-02],\n",
      "         [-3.32031250e-02, -3.27148438e-02,  1.91650391e-02, ...,\n",
      "          -1.40380859e-02,  1.64794922e-02,  2.74658203e-02]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[ 5.85937500e-03, -1.06201172e-02,  4.79125977e-03, ...,\n",
      "          -8.91113281e-03,  1.26647949e-03,  6.16455078e-03],\n",
      "         [-1.00708008e-02, -1.85394287e-03,  3.73840332e-03, ...,\n",
      "          -7.05718994e-04, -4.39453125e-03,  3.91006470e-04],\n",
      "         [ 2.22778320e-03,  3.52478027e-03,  1.09863281e-02, ...,\n",
      "          -7.56835938e-03,  4.27246094e-03,  1.10626221e-04],\n",
      "         ...,\n",
      "         [-3.83300781e-02,  3.17382812e-02, -2.55126953e-02, ...,\n",
      "           2.64892578e-02,  2.22167969e-02, -3.41796875e-02],\n",
      "         [-2.21252441e-03,  1.04522705e-03, -5.07354736e-04, ...,\n",
      "          -3.44848633e-03,  3.00598145e-03, -1.25732422e-02],\n",
      "         [ 2.41088867e-03,  1.85546875e-02, -2.47192383e-03, ...,\n",
      "           3.09753418e-03,  5.37109375e-03, -6.86645508e-04]],\n",
      "\n",
      "        [[-6.13403320e-03,  7.78198242e-04,  4.13894653e-04, ...,\n",
      "           2.12097168e-03, -9.88769531e-03,  2.51464844e-02],\n",
      "         [-5.95092773e-03,  3.79943848e-03,  9.76562500e-04, ...,\n",
      "           5.61523438e-03, -2.19345093e-04, -8.12530518e-04],\n",
      "         [-1.55448914e-04, -6.19506836e-03,  1.44958496e-03, ...,\n",
      "           2.72216797e-02, -9.94873047e-03, -3.89099121e-03],\n",
      "         ...,\n",
      "         [-1.48773193e-03,  4.92095947e-04,  6.22558594e-03, ...,\n",
      "          -1.13525391e-02, -1.09863281e-03, -4.48608398e-03],\n",
      "         [ 9.52148438e-03,  1.98364258e-03, -2.62451172e-02, ...,\n",
      "           3.58581543e-04,  3.31115723e-03,  1.13010406e-04],\n",
      "         [ 4.36401367e-03, -4.30297852e-03,  1.27410889e-03, ...,\n",
      "          -1.74713135e-03, -8.05664062e-03,  1.87683105e-03]],\n",
      "\n",
      "        [[ 3.95507812e-02, -9.33837891e-03, -1.62353516e-02, ...,\n",
      "          -1.18408203e-02, -8.97216797e-03, -1.72119141e-02],\n",
      "         [-2.10571289e-03,  4.05883789e-03,  3.25012207e-03, ...,\n",
      "          -3.63769531e-02,  4.76074219e-03,  1.25732422e-02],\n",
      "         [ 1.58691406e-02, -9.82666016e-03, -3.22341919e-04, ...,\n",
      "          -1.86767578e-02, -1.05590820e-02,  2.73132324e-03],\n",
      "         ...,\n",
      "         [ 4.51660156e-03, -7.56835938e-03,  9.88769531e-03, ...,\n",
      "          -8.36181641e-03, -1.59912109e-02,  1.95312500e-02],\n",
      "         [ 1.81579590e-03, -7.93457031e-03, -1.90734863e-03, ...,\n",
      "          -1.07421875e-02, -1.90429688e-02,  2.07519531e-02],\n",
      "         [-1.10626221e-03,  2.50244141e-03,  2.13623047e-02, ...,\n",
      "          -1.30615234e-02, -7.59887695e-03, -5.21850586e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.36816406e-02, -2.03857422e-02, -6.29425049e-04, ...,\n",
      "           2.01416016e-02, -3.85742188e-02,  1.85546875e-02],\n",
      "         [ 7.53784180e-03, -5.92041016e-03, -1.44195557e-03, ...,\n",
      "          -3.27148438e-02, -4.36401367e-03, -8.30078125e-03],\n",
      "         [ 7.47680664e-03,  7.08007812e-03,  1.58691406e-02, ...,\n",
      "          -1.15356445e-02, -2.62451172e-02,  2.46582031e-02],\n",
      "         ...,\n",
      "         [-2.90527344e-02, -1.57470703e-02, -1.49536133e-02, ...,\n",
      "          -1.36718750e-02, -3.17382812e-02,  2.12402344e-02],\n",
      "         [-6.46972656e-03,  3.17382812e-02, -2.21252441e-03, ...,\n",
      "          -2.82287598e-03, -1.53198242e-02,  4.97436523e-03],\n",
      "         [ 2.58789062e-02,  1.18408203e-02, -1.50756836e-02, ...,\n",
      "          -2.55126953e-02,  1.81884766e-02,  3.57055664e-03]],\n",
      "\n",
      "        [[-3.07617188e-02, -1.18408203e-02,  5.76782227e-03, ...,\n",
      "           1.03759766e-02, -5.88378906e-02, -3.22265625e-02],\n",
      "         [-8.30078125e-03, -1.27563477e-02,  1.68457031e-02, ...,\n",
      "          -1.06811523e-02, -3.56445312e-02,  8.17871094e-03],\n",
      "         [-8.23974609e-03, -3.19824219e-02, -6.25610352e-03, ...,\n",
      "          -1.51824951e-03, -2.19726562e-02,  2.68554688e-02],\n",
      "         ...,\n",
      "         [-1.09252930e-02, -1.30615234e-02,  4.21142578e-03, ...,\n",
      "           1.14746094e-02, -3.96728516e-03, -3.51562500e-02],\n",
      "         [-2.20947266e-02,  3.46679688e-02, -2.30712891e-02, ...,\n",
      "           2.58789062e-02,  4.19921875e-02,  5.73730469e-03],\n",
      "         [ 1.38549805e-02, -1.68457031e-02, -1.80664062e-02, ...,\n",
      "          -2.57568359e-02,  3.17382812e-02, -1.07421875e-02]],\n",
      "\n",
      "        [[ 8.66699219e-03,  5.67626953e-03,  1.39770508e-02, ...,\n",
      "           8.05664062e-03,  1.06811523e-02,  1.57470703e-02],\n",
      "         [ 1.94091797e-02,  4.07695770e-05,  1.74560547e-02, ...,\n",
      "           3.63159180e-03,  4.91333008e-03,  9.82666016e-03],\n",
      "         [-2.60925293e-03, -2.89306641e-02, -3.78417969e-03, ...,\n",
      "           2.12402344e-02,  1.68457031e-02,  4.45556641e-03],\n",
      "         ...,\n",
      "         [ 1.11083984e-02,  1.03149414e-02,  5.82885742e-03, ...,\n",
      "          -4.18090820e-03,  4.52041626e-04, -3.96728516e-04],\n",
      "         [-1.21459961e-02,  1.40991211e-02,  2.70996094e-02, ...,\n",
      "           5.52368164e-03,  7.32421875e-03, -3.34472656e-02],\n",
      "         [ 4.24804688e-02,  6.89697266e-03, -1.77001953e-02, ...,\n",
      "          -1.78222656e-02, -9.70458984e-03,  2.06298828e-02]]],\n",
      "\n",
      "\n",
      "       [[[-6.02722168e-04,  1.25732422e-02, -1.39160156e-02, ...,\n",
      "          -2.01416016e-03,  8.11767578e-03, -3.90625000e-03],\n",
      "         [ 2.02941895e-03,  5.30242920e-04, -6.74438477e-03, ...,\n",
      "          -6.59179688e-03,  8.97216797e-03,  2.24304199e-03],\n",
      "         [ 7.72094727e-03, -1.12304688e-02, -4.45556641e-03, ...,\n",
      "           1.15356445e-02,  6.04248047e-03, -5.46264648e-03],\n",
      "         ...,\n",
      "         [-2.47802734e-02,  3.06396484e-02, -4.52041626e-04, ...,\n",
      "           1.14135742e-02, -1.06201172e-02,  1.57470703e-02],\n",
      "         [ 1.41601562e-02, -8.23974609e-03, -4.27246094e-03, ...,\n",
      "           1.18255615e-03, -3.46374512e-03,  3.12805176e-03],\n",
      "         [ 6.22558594e-03, -4.60815430e-03, -1.99890137e-03, ...,\n",
      "           3.99780273e-03,  1.56250000e-02, -6.82830811e-04]],\n",
      "\n",
      "        [[-3.12500000e-02,  1.10473633e-02, -9.38415527e-04, ...,\n",
      "           7.69042969e-03, -2.20947266e-02,  7.50732422e-03],\n",
      "         [ 2.34985352e-03,  8.12530518e-04,  9.46044922e-03, ...,\n",
      "           9.61303711e-04,  4.45556641e-03,  1.89781189e-04],\n",
      "         [ 1.35040283e-03,  8.85009766e-03,  1.89208984e-02, ...,\n",
      "          -1.70898438e-02, -2.19726562e-02, -1.52587891e-02],\n",
      "         ...,\n",
      "         [ 2.09045410e-03,  2.02941895e-03, -2.19726562e-03, ...,\n",
      "          -4.76074219e-03,  1.01928711e-02, -1.96838379e-03],\n",
      "         [ 7.29370117e-03,  2.42919922e-02,  2.78320312e-02, ...,\n",
      "           3.29589844e-03, -1.06201172e-02, -7.14111328e-03],\n",
      "         [ 4.69970703e-03,  3.08227539e-03, -5.40161133e-03, ...,\n",
      "          -9.84191895e-04,  2.99072266e-03, -5.79833984e-03]],\n",
      "\n",
      "        [[-5.95092773e-03,  5.31005859e-03, -2.92968750e-02, ...,\n",
      "          -2.86865234e-03, -5.34057617e-03,  4.73022461e-03],\n",
      "         [ 1.05590820e-02,  1.16577148e-02, -3.27148438e-02, ...,\n",
      "           5.03540039e-03,  1.55639648e-02,  2.68554688e-03],\n",
      "         [ 1.73950195e-03,  8.97216797e-03,  4.63867188e-03, ...,\n",
      "           1.22070312e-02,  1.20849609e-02,  1.67236328e-02],\n",
      "         ...,\n",
      "         [-9.52148438e-03,  1.26953125e-02, -6.92749023e-03, ...,\n",
      "           6.74438477e-03, -5.88989258e-03, -1.51824951e-03],\n",
      "         [-8.23974609e-03, -2.22167969e-02,  2.41088867e-03, ...,\n",
      "          -1.35498047e-02, -1.66015625e-02, -9.52148438e-03],\n",
      "         [ 2.10571289e-03, -8.69750977e-04,  8.85009766e-03, ...,\n",
      "          -1.67846680e-03, -1.03759766e-02,  1.57470703e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.60980225e-03,  3.99780273e-03,  2.89306641e-02, ...,\n",
      "           1.62353516e-02,  6.07299805e-03,  1.14746094e-02],\n",
      "         [ 7.93457031e-03,  4.63867188e-02, -1.04980469e-02, ...,\n",
      "           8.30078125e-03, -1.99890137e-03, -2.70996094e-02],\n",
      "         [ 3.51562500e-02, -1.04980469e-02,  9.70458984e-03, ...,\n",
      "          -1.61132812e-02, -2.64892578e-02, -8.85009766e-03],\n",
      "         ...,\n",
      "         [-1.38854980e-03, -5.07812500e-02, -3.64685059e-03, ...,\n",
      "          -9.39941406e-03,  7.93457031e-03, -7.20214844e-03],\n",
      "         [-7.43865967e-05,  1.64794922e-02, -1.08032227e-02, ...,\n",
      "          -3.12500000e-02,  6.62231445e-03, -4.15039062e-02],\n",
      "         [-3.95507812e-02, -7.44628906e-03,  4.88281250e-03, ...,\n",
      "          -7.17163086e-03,  3.01513672e-02, -1.47705078e-02]],\n",
      "\n",
      "        [[-3.39355469e-02,  3.73535156e-02, -2.60009766e-02, ...,\n",
      "           2.34375000e-02, -1.10473633e-02,  1.00708008e-02],\n",
      "         [ 1.04522705e-03,  1.64794922e-02,  5.73730469e-03, ...,\n",
      "          -6.77490234e-03,  5.31005859e-03,  1.67846680e-03],\n",
      "         [ 1.40991211e-02,  2.22778320e-03,  6.04248047e-03, ...,\n",
      "           1.84326172e-02,  7.26318359e-03, -8.96453857e-05],\n",
      "         ...,\n",
      "         [ 4.05273438e-02, -9.33837891e-03, -1.96533203e-02, ...,\n",
      "           7.93457031e-04,  3.85742188e-02,  5.58471680e-03],\n",
      "         [-3.36914062e-02,  2.14843750e-02,  2.91442871e-03, ...,\n",
      "          -2.73437500e-02, -1.70898438e-02, -1.80664062e-02],\n",
      "         [ 5.43212891e-03, -2.56347656e-02,  4.68750000e-02, ...,\n",
      "          -2.21252441e-03, -4.02832031e-03, -5.61523438e-02]],\n",
      "\n",
      "        [[ 6.48498535e-04,  3.63769531e-02,  1.47819519e-04, ...,\n",
      "           1.51977539e-02, -1.72119141e-02,  7.56835938e-03],\n",
      "         [ 2.47802734e-02,  7.38525391e-03, -1.00097656e-02, ...,\n",
      "           2.79541016e-02,  3.88183594e-02, -1.61132812e-02],\n",
      "         [ 3.02734375e-02,  8.48388672e-03,  9.58251953e-03, ...,\n",
      "          -1.28784180e-02, -3.46679688e-02, -1.62506104e-03],\n",
      "         ...,\n",
      "         [ 1.25122070e-02,  7.59887695e-03, -2.27355957e-03, ...,\n",
      "           1.03149414e-02,  8.11767578e-03,  3.18908691e-03],\n",
      "         [-7.04956055e-03, -2.01416016e-02,  2.97851562e-02, ...,\n",
      "          -1.91650391e-02, -2.08740234e-02,  4.21142578e-03],\n",
      "         [-6.59179688e-03, -1.50146484e-02, -3.06396484e-02, ...,\n",
      "          -2.74658203e-03, -2.91748047e-02, -5.15747070e-03]]],\n",
      "\n",
      "\n",
      "       [[[ 1.03149414e-02,  4.76074219e-03, -1.42211914e-02, ...,\n",
      "           2.83813477e-03,  3.67736816e-03, -6.13403320e-03],\n",
      "         [-8.97216797e-03, -1.01928711e-02,  7.20214844e-03, ...,\n",
      "           1.36375427e-04, -4.02832031e-03,  5.40161133e-03],\n",
      "         [-5.24902344e-03, -7.93457031e-03,  1.63269043e-03, ...,\n",
      "          -8.96453857e-04,  6.77490234e-03, -9.21630859e-03],\n",
      "         ...,\n",
      "         [-4.60815430e-03,  1.42822266e-02,  1.25122070e-02, ...,\n",
      "          -8.85009766e-03,  2.16064453e-02, -1.09252930e-02],\n",
      "         [-8.17871094e-03,  7.56835938e-03, -1.01928711e-02, ...,\n",
      "           4.48608398e-03,  1.53808594e-02, -2.59399414e-03],\n",
      "         [ 3.82995605e-03, -4.94384766e-03,  3.69262695e-03, ...,\n",
      "          -2.24304199e-03,  7.01904297e-04,  1.48010254e-03]],\n",
      "\n",
      "        [[-2.62451172e-02,  3.32031250e-02, -1.94549561e-03, ...,\n",
      "           8.05664062e-03, -1.28784180e-02, -3.15856934e-03],\n",
      "         [-2.34985352e-03,  7.89642334e-04, -1.15966797e-03, ...,\n",
      "           3.32641602e-03,  6.56127930e-03, -4.08935547e-03],\n",
      "         [-1.86920166e-03, -1.89781189e-04,  7.23266602e-03, ...,\n",
      "           7.29370117e-03, -8.60595703e-03,  2.01416016e-02],\n",
      "         ...,\n",
      "         [ 5.70678711e-03, -7.93457031e-04, -5.21850586e-03, ...,\n",
      "          -2.89916992e-03,  1.20849609e-02, -4.27246094e-03],\n",
      "         [-2.01416016e-02,  6.74438477e-03, -3.82995605e-03, ...,\n",
      "          -7.23266602e-03,  2.77709961e-03,  6.50024414e-03],\n",
      "         [ 5.18798828e-03, -1.20544434e-03,  6.10351562e-03, ...,\n",
      "           1.48773193e-03, -1.03759766e-03,  8.66699219e-03]],\n",
      "\n",
      "        [[ 4.79125977e-03, -1.58691406e-02, -2.97851562e-02, ...,\n",
      "          -3.18908691e-03,  1.01928711e-02, -2.05078125e-02],\n",
      "         [ 2.33154297e-02, -1.14135742e-02, -1.62353516e-02, ...,\n",
      "          -5.76019287e-04,  2.02636719e-02,  9.82666016e-03],\n",
      "         [-1.45874023e-02, -6.62231445e-03, -2.06298828e-02, ...,\n",
      "           1.58691406e-02, -3.89099121e-03,  1.89208984e-02],\n",
      "         ...,\n",
      "         [-1.06811523e-02, -1.02539062e-02, -7.35473633e-03, ...,\n",
      "           3.24707031e-02, -1.55029297e-02,  1.58691406e-02],\n",
      "         [ 1.64794922e-02,  2.23388672e-02,  4.85229492e-03, ...,\n",
      "          -9.76562500e-03,  9.33837891e-03, -2.61230469e-02],\n",
      "         [ 9.82666016e-03, -3.29589844e-03,  9.58251953e-03, ...,\n",
      "           1.09863281e-03, -5.95092773e-03, -7.75146484e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.97753906e-02, -5.88989258e-03,  1.22680664e-02, ...,\n",
      "          -3.89099121e-03,  2.60009766e-02, -5.88989258e-03],\n",
      "         [ 2.67333984e-02, -9.19342041e-04, -1.97753906e-02, ...,\n",
      "           1.83868408e-03,  2.03857422e-02,  2.53906250e-02],\n",
      "         [ 9.29832458e-05, -1.50756836e-02,  7.85827637e-04, ...,\n",
      "          -2.36511230e-03, -1.72119141e-02, -9.88769531e-03],\n",
      "         ...,\n",
      "         [ 3.20434570e-03,  2.00195312e-02,  5.37109375e-03, ...,\n",
      "          -1.37939453e-02,  1.18255615e-03, -3.40270996e-03],\n",
      "         [-2.92968750e-02, -2.63671875e-02, -8.30078125e-03, ...,\n",
      "           7.81250000e-03,  1.86767578e-02,  7.26318359e-03],\n",
      "         [-1.69677734e-02,  1.36718750e-02,  1.05590820e-02, ...,\n",
      "          -1.50146484e-02, -1.18408203e-02,  2.51464844e-02]],\n",
      "\n",
      "        [[ 3.14941406e-02,  4.91333008e-03, -1.26342773e-02, ...,\n",
      "           4.24804688e-02,  2.37226486e-05,  4.11987305e-03],\n",
      "         [-1.22070312e-02, -4.18090820e-03, -1.84631348e-03, ...,\n",
      "          -3.03955078e-02, -2.39257812e-02,  1.34277344e-03],\n",
      "         [ 9.64355469e-03, -1.66015625e-02, -1.27563477e-02, ...,\n",
      "          -2.01416016e-03,  6.86645508e-03, -8.66699219e-03],\n",
      "         ...,\n",
      "         [-3.93676758e-03,  3.11279297e-02,  3.27148438e-02, ...,\n",
      "          -3.49121094e-02,  2.22167969e-02, -3.38745117e-03],\n",
      "         [-2.47192383e-03,  2.94189453e-02,  2.67028809e-03, ...,\n",
      "          -3.66210938e-03, -1.50756836e-02,  4.48608398e-03],\n",
      "         [-3.66210938e-02,  1.92871094e-02,  7.14111328e-03, ...,\n",
      "           3.83300781e-02,  2.53906250e-02, -2.41699219e-02]],\n",
      "\n",
      "        [[ 1.31225586e-02,  1.31225586e-02,  3.14941406e-02, ...,\n",
      "          -1.18408203e-02, -7.23266602e-03,  1.68457031e-02],\n",
      "         [ 2.42614746e-03,  1.69677734e-02,  3.58581543e-03, ...,\n",
      "          -4.63867188e-02, -1.16577148e-02,  1.21459961e-02],\n",
      "         [ 5.70678711e-03, -4.45556641e-03,  6.40869141e-03, ...,\n",
      "          -5.34057617e-03,  1.91650391e-02,  3.66210938e-02],\n",
      "         ...,\n",
      "         [ 1.19018555e-03, -2.13623047e-03, -8.54492188e-03, ...,\n",
      "           3.10058594e-02, -1.70898438e-02,  4.27246094e-02],\n",
      "         [-1.90429688e-02, -1.94549561e-03, -3.41796875e-02, ...,\n",
      "          -3.27148438e-02,  1.89208984e-02,  1.51977539e-02],\n",
      "         [-6.04248047e-03, -1.96533203e-02, -1.84631348e-03, ...,\n",
      "           2.60009766e-02,  1.00097656e-02,  3.97949219e-02]]]],      dtype=float32)}}}, 'logits_dense': {'kernel': Array([[-0.00389099, -0.03149414, -0.01245117, ..., -0.02807617,\n",
      "         0.02294922,  0.00799561],\n",
      "       [ 0.00317383,  0.04663086,  0.00360107, ..., -0.01953125,\n",
      "         0.0255127 , -0.00878906],\n",
      "       [-0.00714111, -0.00231934,  0.01953125, ..., -0.00239563,\n",
      "         0.03149414,  0.00634766],\n",
      "       ...,\n",
      "       [ 0.00531006, -0.02111816, -0.02709961, ...,  0.01226807,\n",
      "         0.00668335, -0.02929688],\n",
      "       [-0.00817871,  0.01733398,  0.01428223, ..., -0.01165771,\n",
      "        -0.00921631, -0.02001953],\n",
      "       [ 0.00701904,  0.03344727, -0.00817871, ..., -0.02368164,\n",
      "        -0.00582886,  0.03369141]], dtype=float32)}}, 'token_embedder': {'embedding': Array([[ 1.22934580e-06, -1.81794167e-06, -4.35113907e-06, ...,\n",
      "         8.71717930e-07, -6.52670860e-06,  8.90344381e-07],\n",
      "       [ 1.86157227e-03, -3.37219238e-03,  3.98635864e-04, ...,\n",
      "        -8.30078125e-03,  2.57873535e-03, -3.93676758e-03],\n",
      "       [ 1.09863281e-02,  9.88769531e-03, -5.09643555e-03, ...,\n",
      "         2.51770020e-03,  7.70568848e-04, -5.00488281e-03],\n",
      "       ...,\n",
      "       [-1.39770508e-02, -2.73132324e-03, -1.98974609e-02, ...,\n",
      "        -1.04370117e-02,  9.58251953e-03, -1.80053711e-03],\n",
      "       [-1.07421875e-02,  9.33837891e-03,  1.29394531e-02, ...,\n",
      "        -3.32031250e-02, -1.63574219e-02,  3.38745117e-03],\n",
      "       [-8.30078125e-03, -4.05883789e-03, -1.10626221e-03, ...,\n",
      "         3.47900391e-03, -1.29394531e-02,  3.19480896e-05]],      dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "print(training_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "942dc554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'decoder': {'decoder_norm': {'scale': PartitionSpec('fsdp',)},\n",
       "   'layers': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "     'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "     'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}},\n",
       "    'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp', 'stage')},\n",
       "    'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp', 'stage')},\n",
       "    'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "     'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))},\n",
       "     'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "     'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}},\n",
       "   'logits_dense': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'))}},\n",
       "  'token_embedder': {'embedding': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_state_annotations.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5c008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: params.decoder.decoder_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers.mlp.wi_0.kernel.\n",
      "value shape: (4096, 32, 11008)\n",
      "key: params.decoder.layers.mlp.wi_1.kernel.\n",
      "value shape: (4096, 32, 11008)\n",
      "key: params.decoder.layers.mlp.wo.kernel.\n",
      "value shape: (11008, 32, 4096)\n",
      "key: params.decoder.layers.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096, 32)\n",
      "key: params.decoder.layers.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096, 32)\n",
      "key: params.decoder.layers.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 32, 128)\n",
      "key: params.decoder.layers.self_attention.out.kernel.\n",
      "value shape: (32, 32, 128, 4096)\n",
      "key: params.decoder.layers.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 32, 128)\n",
      "key: params.decoder.layers.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 32, 128)\n",
      "key: params.decoder.logits_dense.kernel.\n",
      "value shape: (4096, 32000)\n",
      "key: params.token_embedder.embedding.\n",
      "value shape: (32000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print_nested_keys(training_state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd2547",
   "metadata": {},
   "source": [
    "## scanned, force unroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b28fa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'force_unroll', 'scan_layers', 'base_output_directory', 'skip_jax_distributed_system']\n",
      "Running Model: llama2-7b\n",
      "Updating following parameters in config\n",
      "\n",
      "base_emb_dim: 4096\n",
      "base_num_query_heads: 32\n",
      "base_num_kv_heads: 32\n",
      "base_mlp_dim: 11008\n",
      "base_num_decoder_layers: 32\n",
      "head_dim: 128\n",
      "mlp_activations: ['silu', 'linear']\n",
      "vocab_size: 32000\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "normalization_layer_epsilon: 1e-05\n",
      "decoder_block: llama2\n",
      "logical_axis_rules: [['norm', 'fsdp']]\n",
      "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'enable_dropout', 'logits_via_embedding', 'normalization_layer_epsilon', 'decoder_block', 'logical_axis_rules']\n",
      "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: autoselected\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 4096\n",
      "Config param base_mlp_dim: 11008\n",
      "Config param base_moe_mlp_dim: 7168\n",
      "Config param base_num_decoder_layers: 32\n",
      "Config param base_num_kv_heads: 32\n",
      "Config param base_num_query_heads: 32\n",
      "Config param base_output_directory: gs://runner-maxtext-logs\n",
      "Config param beta_fast: 32\n",
      "Config param beta_slow: 1\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: gs://runner-maxtext-logs/test3/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_concurrent_gb: 96\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param chunk_attn_window_size: 0\n",
      "Config param collect_stack_trace: False\n",
      "Config param colocated_python_data_input: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param context: remat\n",
      "Config param context_parallel_load_balance: True\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_context_autoregressive_parallelism: 1\n",
      "Config param dcn_context_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param dcn_tensor_transpose_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: DecoderBlockType.LLAMA2\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dtype_mm: float32\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param dump_step: -1\n",
      "Config param emb_dim: 4096\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_gcp_goodput_metrics: True\n",
      "Config param enable_gcp_step_deviation_metrics: True\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_llm_inference_pool: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_padding_causal_mask: True\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_prefix_caching: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param enable_tensorboard: True\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 12.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param first_num_dense_layers: 0\n",
      "Config param float32_logits: False\n",
      "Config param float32_qk_product: False\n",
      "Config param force_unroll: True\n",
      "Config param freeze_vision_encoder_params: True\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param generate_slice: v5e-16\n",
      "Config param global_batch_size_to_eval_on: 12\n",
      "Config param global_batch_size_to_load: 12\n",
      "Config param global_batch_size_to_load_eval: 12\n",
      "Config param global_batch_size_to_train_on: 12\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 30\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_file_type: arrayrecord\n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param grain_worker_count_eval: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param heartbeat_reporting_interval_in_seconds: 5\n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param hidden_size_for_vit: 1408\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_context_autoregressive_parallelism: 1\n",
      "Config param ici_context_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param ici_tensor_transpose_parallelism: 1\n",
      "Config param image_path: \n",
      "Config param image_size_for_vit: 896\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param inhomogeneous_layer_cycle_interval: 1\n",
      "Config param init_weights_seed: 0\n",
      "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
      "Config param interleave_moe_layer_step: 1\n",
      "Config param intermediate_size_for_vit: 5632\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_lora_rank: 512\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 150001\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: gs://shuningjin-multipod-dev/llama2-7b/2025-06-22/scanned/0/items\n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param local_rope_max_timescale: -1\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()), ('norm', 'fsdp'))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param lora_input_adapters_path: \n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_position_embeddings: 163840\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 2048\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: gs://runner-maxtext-logs/test3/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 12\n",
      "Config param micro_batch_size_to_train_on: 12\n",
      "Config param mla_naive_kvcache: True\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 11008\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_name: llama2-7b\n",
      "Config param moe_mlp_dim: 7168\n",
      "Config param monitor_goodput: False\n",
      "Config param monitor_step_time_deviation: True\n",
      "Config param mscale: 1.0\n",
      "Config param mu_dtype: float32\n",
      "Config param multi_sampling: False\n",
      "Config param n_routing_groups: -1\n",
      "Config param nope_layer_interval: -1\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_attention_heads_for_vit: 16\n",
      "Config param num_channels_for_vit: 3\n",
      "Config param num_decoder_layers: 32\n",
      "Config param num_epoch: 1\n",
      "Config param num_experts: 1\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_hidden_layers_for_vit: 34\n",
      "Config param num_kv_heads: 32\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 32\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimize_mesh_for_tpu_v6e: False\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param original_max_position_embeddings: 4096\n",
      "Config param out_proj: remat\n",
      "Config param override_model_config: False\n",
      "Config param packing: True\n",
      "Config param pagedattn_head_dim_alignment: 128\n",
      "Config param pagedattn_max_pages_per_group: 64\n",
      "Config param pagedattn_num_pages: 64\n",
      "Config param pagedattn_pages_per_compute_block: 4\n",
      "Config param pagedattn_tokens_per_page: 32\n",
      "Config param param_scan_axis: 1\n",
      "Config param parameter_memory_host_offload: False\n",
      "Config param patch_size_for_vit: 14\n",
      "Config param per_device_batch_size: 12.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param pipeline_fsdp_ag_once: False\n",
      "Config param pipeline_parallel_layers: -1\n",
      "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param prefill_chunk_size: 256\n",
      "Config param prefill_slice: v5e-16\n",
      "Config param prefix_caching_dram_byte: 100000000000\n",
      "Config param prefix_caching_hbm_byte: 10000000000\n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param projector_dropout_for_vit: 0.0\n",
      "Config param projector_input_dim_for_vit: 4096\n",
      "Config param projector_output_dim_for_vit: 4096\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param q_lora_rank: 0\n",
      "Config param qk_nope_head_dim: 128\n",
      "Config param qk_rope_head_dim: 64\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param remat_policy_for_vit: minimal\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
      "Config param report_performance_metric_for_gcp_monitoring: False\n",
      "Config param reshape_q: False\n",
      "Config param return_log_prob: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_factor: 40\n",
      "Config param rope_max_timescale: 10000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param rope_theta_for_vit: 10000\n",
      "Config param rope_type: default\n",
      "Config param rope_use_scale: True\n",
      "Config param routed_bias: False\n",
      "Config param routed_scaling_factor: 1.0\n",
      "Config param routed_score_func: \n",
      "Config param run_name: test3\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_layers_per_stage: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sft_train_on_completion_only: False\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param shared_experts: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: True\n",
      "Config param sliding_window_size: 0\n",
      "Config param sparse_matmul: True\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param step_deviation_interval_seconds: 30\n",
      "Config param steps: 150001\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param temperature_tuning: False\n",
      "Config param tensorboard_dir: gs://runner-maxtext-logs/test3/tensorboard/\n",
      "Config param tile_activation_dim: 1024\n",
      "Config param tile_batch_seq: 512\n",
      "Config param tile_size_for_vit: 336\n",
      "Config param tile_weight_dim: 1024\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param tokenizer_type: sentencepiece\n",
      "Config param topk_routing_group: -1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param train_split: train\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_chat_template: False\n",
      "Config param use_chunked_prefill: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_multimodal: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_qk_norm: False\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_random_routing: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_sft: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param v_head_dim: 128\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vision_output_dim_for_vit: 4096\n",
      "Config param vocab_size: 32000\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: float32\n"
     ]
    }
   ],
   "source": [
    "cmd=\"python MaxText/configs/base.yml model_name=llama2-7b \\\n",
    "base_output_directory=gs://runner-maxtext-logs run_name=test3 \\\n",
    "load_parameters_path=gs://shuningjin-multipod-dev/llama2-7b/2025-06-22/scanned/0/items scan_layers=true force_unroll=true \\\n",
    "async_checkpointing=false skip_jax_distributed_system=true\"\n",
    "argv = cmd.split()\n",
    "config = pyconfig.initialize(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b971e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "Checkpoint manager created!\n",
      "Read training checkpoint from: \n",
      "checkpoint manager exists so trying to load this run's existing checkpoint\n",
      "restoring params from gs://shuningjin-multipod-dev/llama2-7b/2025-06-22/scanned/0/items\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "{'params': {'decoder': {'decoder_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('fsdp',), memory_kind=unpinned_host), global_shape=(4096,), shape=(4096,), strict=True)}, 'layers': {'mlp': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(4096, 32, 11008), shape=(4096, 32, 11008), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(4096, 32, 11008), shape=(4096, 32, 11008), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(11008, 32, 4096), shape=(11008, 32, 4096), strict=True)}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('fsdp', 'stage'), memory_kind=unpinned_host), global_shape=(4096, 32), shape=(4096, 32), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('fsdp', 'stage'), memory_kind=unpinned_host), global_shape=(4096, 32), shape=(4096, 32), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(4096, 32, 32, 128), shape=(4096, 32, 32, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(32, 32, 128, 4096), shape=(32, 32, 128, 4096), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(4096, 32, 32, 128), shape=(4096, 32, 32, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(4096, 32, 32, 128), shape=(4096, 32, 32, 128), strict=True)}}}, 'logits_dense': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(4096, 32000), shape=(4096, 32000), strict=True)}}, 'token_embedder': {'embedding': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(32000, 4096), shape=(32000, 4096), strict=True)}}}\n",
      "In input checkpoint Number of model params=6.738 billion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 08:21:37.342180: E external/xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %compare.13 = pred[1]{0} compare(%constant.298, %constant.33), direction=GE, metadata={op_name=\"jit(slice_ith)/jit(main)/jit(_take)/ge\" source_file=\"/var/tmp/ipykernel_1106180/2432396246.py\" source_line=74}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n",
      "2025-06-27 08:21:37.531012: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.191921684s\n",
      "Constant folding an instruction is taking > 1s:\n",
      "\n",
      "  %compare.13 = pred[1]{0} compare(%constant.298, %constant.33), direction=GE, metadata={op_name=\"jit(slice_ith)/jit(main)/jit(_take)/ge\" source_file=\"/var/tmp/ipykernel_1106180/2432396246.py\" source_line=74}\n",
      "\n",
      "This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.\n",
      "\n",
      "If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.\n"
     ]
    }
   ],
   "source": [
    "training_state2, training_state_annotations2 = generate_decode_checkpoint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad4a478",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type PartitionSpec is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_state_annotations2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type PartitionSpec is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(training_state_annotations2.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55699525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'decoder': {'decoder_norm': {'scale': PartitionSpec('fsdp',)}, 'logits_dense': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'))}, 'layers_0': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_1': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_2': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_3': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_4': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_5': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_6': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_7': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_8': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_9': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_10': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_11': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_12': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_13': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_14': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_15': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_16': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_17': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_18': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_19': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_20': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_21': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_22': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_23': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_24': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_25': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_26': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_27': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_28': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_29': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_30': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}, 'layers_31': {'mlp': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))}, 'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}, 'post_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('fsdp',)}, 'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}, 'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}, 'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}}, 'token_embedder': {'embedding': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}}}\n"
     ]
    }
   ],
   "source": [
    "print(training_state_annotations2.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc6cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: params.decoder.decoder_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.logits_dense.kernel.\n",
      "value shape: (4096, 32000)\n",
      "key: params.decoder.layers_0.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_0.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_0.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_0.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_0.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_0.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_0.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_0.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_0.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_1.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_1.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_1.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_1.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_1.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_1.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_1.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_1.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_1.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_2.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_2.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_2.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_2.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_2.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_2.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_2.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_2.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_2.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_3.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_3.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_3.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_3.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_3.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_3.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_3.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_3.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_3.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_4.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_4.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_4.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_4.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_4.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_4.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_4.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_4.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_4.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_5.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_5.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_5.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_5.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_5.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_5.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_5.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_5.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_5.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_6.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_6.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_6.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_6.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_6.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_6.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_6.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_6.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_6.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_7.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_7.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_7.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_7.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_7.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_7.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_7.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_7.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_7.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_8.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_8.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_8.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_8.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_8.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_8.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_8.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_8.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_8.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_9.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_9.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_9.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_9.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_9.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_9.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_9.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_9.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_9.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_10.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_10.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_10.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_10.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_10.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_10.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_10.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_10.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_10.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_11.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_11.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_11.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_11.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_11.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_11.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_11.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_11.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_11.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_12.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_12.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_12.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_12.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_12.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_12.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_12.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_12.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_12.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_13.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_13.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_13.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_13.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_13.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_13.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_13.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_13.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_13.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_14.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_14.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_14.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_14.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_14.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_14.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_14.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_14.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_14.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_15.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_15.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_15.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_15.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_15.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_15.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_15.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_15.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_15.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_16.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_16.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_16.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_16.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_16.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_16.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_16.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_16.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_16.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_17.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_17.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_17.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_17.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_17.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_17.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_17.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_17.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_17.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_18.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_18.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_18.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_18.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_18.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_18.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_18.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_18.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_18.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_19.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_19.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_19.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_19.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_19.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_19.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_19.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_19.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_19.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_20.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_20.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_20.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_20.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_20.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_20.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_20.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_20.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_20.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_21.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_21.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_21.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_21.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_21.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_21.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_21.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_21.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_21.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_22.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_22.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_22.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_22.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_22.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_22.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_22.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_22.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_22.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_23.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_23.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_23.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_23.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_23.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_23.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_23.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_23.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_23.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_24.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_24.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_24.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_24.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_24.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_24.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_24.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_24.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_24.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_25.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_25.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_25.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_25.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_25.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_25.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_25.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_25.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_25.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_26.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_26.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_26.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_26.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_26.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_26.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_26.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_26.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_26.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_27.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_27.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_27.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_27.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_27.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_27.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_27.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_27.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_27.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_28.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_28.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_28.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_28.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_28.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_28.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_28.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_28.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_28.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_29.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_29.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_29.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_29.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_29.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_29.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_29.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_29.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_29.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_30.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_30.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_30.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_30.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_30.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_30.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_30.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_30.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_30.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_31.mlp.wi_0.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_31.mlp.wi_1.kernel.\n",
      "value shape: (4096, 11008)\n",
      "key: params.decoder.layers_31.mlp.wo.kernel.\n",
      "value shape: (11008, 4096)\n",
      "key: params.decoder.layers_31.post_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_31.pre_self_attention_layer_norm.scale.\n",
      "value shape: (4096,)\n",
      "key: params.decoder.layers_31.self_attention.key.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_31.self_attention.out.kernel.\n",
      "value shape: (32, 128, 4096)\n",
      "key: params.decoder.layers_31.self_attention.query.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.decoder.layers_31.self_attention.value.kernel.\n",
      "value shape: (4096, 32, 128)\n",
      "key: params.token_embedder.embedding.\n",
      "value shape: (32000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print_nested_keys(training_state2.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85bd98e",
   "metadata": {},
   "source": [
    "## checkpoint1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8602a833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'force_unroll', 'scan_layers', 'base_output_directory', 'skip_jax_distributed_system']\n",
      "Running Model: llama4-17b-16e\n",
      "Updating following parameters in config\n",
      "\n",
      "decoder_block: llama4\n",
      "mlp_activations: ['silu', 'linear']\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "tokenizer_type: huggingface\n",
      "base_emb_dim: 5120\n",
      "base_num_decoder_layers: 48\n",
      "base_num_query_heads: 40\n",
      "base_num_kv_heads: 8\n",
      "base_mlp_dim: 16384\n",
      "base_moe_mlp_dim: 8192\n",
      "vocab_size: 202048\n",
      "normalization_layer_epsilon: 1e-05\n",
      "rope_max_timescale: 500000\n",
      "rope_type: llama3.1\n",
      "num_experts: 16\n",
      "shared_experts: 1\n",
      "num_experts_per_tok: 1\n",
      "use_qk_norm: True\n",
      "nope_layer_interval: 4\n",
      "interleave_moe_layer_step: 1\n",
      "inhomogeneous_layer_cycle_interval: 4\n",
      "temperature_tuning: True\n",
      "chunk_attn_window_size: 8192\n",
      "image_size_for_vit: 336\n",
      "Updating keys from model: ['decoder_block', 'mlp_activations', 'enable_dropout', 'logits_via_embedding', 'tokenizer_type', 'base_emb_dim', 'base_num_decoder_layers', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_moe_mlp_dim', 'vocab_size', 'normalization_layer_epsilon', 'rope_max_timescale', 'rope_type', 'num_experts', 'shared_experts', 'num_experts_per_tok', 'use_qk_norm', 'nope_layer_interval', 'interleave_moe_layer_step', 'inhomogeneous_layer_cycle_interval', 'temperature_tuning', 'chunk_attn_window_size', 'image_size_for_vit']\n",
      "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: autoselected\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 5120\n",
      "Config param base_mlp_dim: 16384\n",
      "Config param base_moe_mlp_dim: 8192\n",
      "Config param base_num_decoder_layers: 48\n",
      "Config param base_num_kv_heads: 8\n",
      "Config param base_num_query_heads: 40\n",
      "Config param base_output_directory: gs://runner-maxtext-logs\n",
      "Config param beta_fast: 32\n",
      "Config param beta_slow: 1\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: gs://runner-maxtext-logs/test3/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_concurrent_gb: 96\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param chunk_attn_window_size: 8192\n",
      "Config param collect_stack_trace: False\n",
      "Config param colocated_python_data_input: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param context: remat\n",
      "Config param context_parallel_load_balance: True\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_context_autoregressive_parallelism: 1\n",
      "Config param dcn_context_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param dcn_tensor_transpose_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: DecoderBlockType.LLAMA4\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dtype_mm: float32\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param dump_step: -1\n",
      "Config param emb_dim: 5120\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_gcp_goodput_metrics: True\n",
      "Config param enable_gcp_step_deviation_metrics: True\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_llm_inference_pool: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_padding_causal_mask: True\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_prefix_caching: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param enable_tensorboard: True\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 12.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param first_num_dense_layers: 0\n",
      "Config param float32_logits: False\n",
      "Config param float32_qk_product: False\n",
      "Config param force_unroll: False\n",
      "Config param freeze_vision_encoder_params: True\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param generate_slice: v5e-16\n",
      "Config param global_batch_size_to_eval_on: 12\n",
      "Config param global_batch_size_to_load: 12\n",
      "Config param global_batch_size_to_load_eval: 12\n",
      "Config param global_batch_size_to_train_on: 12\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 30\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_file_type: arrayrecord\n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param grain_worker_count_eval: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param heartbeat_reporting_interval_in_seconds: 5\n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param hidden_size_for_vit: 1408\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_context_autoregressive_parallelism: 1\n",
      "Config param ici_context_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param ici_tensor_transpose_parallelism: 1\n",
      "Config param image_path: \n",
      "Config param image_size_for_vit: 336\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param inhomogeneous_layer_cycle_interval: 4\n",
      "Config param init_weights_seed: 0\n",
      "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
      "Config param interleave_moe_layer_step: 1\n",
      "Config param intermediate_size_for_vit: 5632\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_lora_rank: 512\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 150001\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: gs://jacobplatin/llama4/v5-scout-instruct-bf16-final-scanned/0/items\n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param local_rope_max_timescale: -1\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param lora_input_adapters_path: \n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_position_embeddings: 163840\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 2048\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: gs://runner-maxtext-logs/test3/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 12\n",
      "Config param micro_batch_size_to_train_on: 12\n",
      "Config param mla_naive_kvcache: True\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 16384\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_name: llama4-17b-16e\n",
      "Config param moe_mlp_dim: 8192\n",
      "Config param monitor_goodput: False\n",
      "Config param monitor_step_time_deviation: True\n",
      "Config param mscale: 1.0\n",
      "Config param mu_dtype: float32\n",
      "Config param multi_sampling: False\n",
      "Config param n_routing_groups: -1\n",
      "Config param nope_layer_interval: 4\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_attention_heads_for_vit: 16\n",
      "Config param num_channels_for_vit: 3\n",
      "Config param num_decoder_layers: 48\n",
      "Config param num_epoch: 1\n",
      "Config param num_experts: 16\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_hidden_layers_for_vit: 34\n",
      "Config param num_kv_heads: 8\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 40\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimize_mesh_for_tpu_v6e: False\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param original_max_position_embeddings: 4096\n",
      "Config param out_proj: remat\n",
      "Config param override_model_config: False\n",
      "Config param packing: True\n",
      "Config param pagedattn_head_dim_alignment: 128\n",
      "Config param pagedattn_max_pages_per_group: 64\n",
      "Config param pagedattn_num_pages: 64\n",
      "Config param pagedattn_pages_per_compute_block: 4\n",
      "Config param pagedattn_tokens_per_page: 32\n",
      "Config param param_scan_axis: 1\n",
      "Config param parameter_memory_host_offload: False\n",
      "Config param patch_size_for_vit: 14\n",
      "Config param per_device_batch_size: 12.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param pipeline_fsdp_ag_once: False\n",
      "Config param pipeline_parallel_layers: -1\n",
      "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param prefill_chunk_size: 256\n",
      "Config param prefill_slice: v5e-16\n",
      "Config param prefix_caching_dram_byte: 100000000000\n",
      "Config param prefix_caching_hbm_byte: 10000000000\n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param projector_dropout_for_vit: 0.0\n",
      "Config param projector_input_dim_for_vit: 4096\n",
      "Config param projector_output_dim_for_vit: 4096\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param q_lora_rank: 0\n",
      "Config param qk_nope_head_dim: 128\n",
      "Config param qk_rope_head_dim: 64\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param remat_policy_for_vit: minimal\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
      "Config param report_performance_metric_for_gcp_monitoring: False\n",
      "Config param reshape_q: False\n",
      "Config param return_log_prob: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_factor: 40\n",
      "Config param rope_max_timescale: 500000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param rope_theta_for_vit: 10000\n",
      "Config param rope_type: llama3.1\n",
      "Config param rope_use_scale: True\n",
      "Config param routed_bias: False\n",
      "Config param routed_scaling_factor: 1.0\n",
      "Config param routed_score_func: \n",
      "Config param run_name: test3\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_layers_per_stage: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sft_train_on_completion_only: False\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param shared_experts: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: True\n",
      "Config param sliding_window_size: 0\n",
      "Config param sparse_matmul: True\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param step_deviation_interval_seconds: 30\n",
      "Config param steps: 150001\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param temperature_tuning: True\n",
      "Config param tensorboard_dir: gs://runner-maxtext-logs/test3/tensorboard/\n",
      "Config param tile_activation_dim: 1024\n",
      "Config param tile_batch_seq: 512\n",
      "Config param tile_size_for_vit: 336\n",
      "Config param tile_weight_dim: 1024\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param tokenizer_type: huggingface\n",
      "Config param topk_routing_group: -1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param train_split: train\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_chat_template: False\n",
      "Config param use_chunked_prefill: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_multimodal: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_qk_norm: True\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_random_routing: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_sft: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param v_head_dim: 128\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vision_output_dim_for_vit: 4096\n",
      "Config param vocab_size: 202048\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: float32\n"
     ]
    }
   ],
   "source": [
    "cmd=\"python MaxText/configs/base.yml model_name=llama4-17b-16e \\\n",
    "base_output_directory=gs://runner-maxtext-logs run_name=test3 \\\n",
    "load_parameters_path=gs://jacobplatin/llama4/v5-scout-instruct-bf16-final-scanned/0/items scan_layers=true force_unroll=false \\\n",
    "async_checkpointing=false skip_jax_distributed_system=true\"\n",
    "argv = cmd.split()\n",
    "config = pyconfig.initialize(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae3316d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "Checkpoint manager created!\n",
      "Read training checkpoint from: \n",
      "checkpoint manager exists so trying to load this run's existing checkpoint\n",
      "restoring params from gs://jacobplatin/llama4/v5-scout-instruct-bf16-final-scanned/0/items\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "{'params': {'decoder': {'decoder_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'),), memory_kind=unpinned_host), global_shape=(5120,), shape=(5120,), strict=True)}, 'layers': {'layers_0': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}, 'layers_1': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}, 'layers_2': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}, 'layers_3': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}}, 'logits_dense': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 202048), shape=(5120, 202048), strict=True)}}, 'token_embedder': {'embedding': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(202048, 5120), shape=(202048, 5120), strict=True)}}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing 4 keys in structure path ('params', 'params', 'decoder', 'layers'), including: ['layers_0', 'layers_1', 'layers_2', 'layers_3']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_state3, training_state_annotations3 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_decode_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 191\u001b[0m, in \u001b[0;36mgenerate_decode_checkpoint\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Read training state from config.load_paramaters_path\u001b[39;00m\n\u001b[1;32m    190\u001b[0m max_logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead training checkpoint from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mload_full_state_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 191\u001b[0m training_state, training_state_annotations \u001b[38;5;241m=\u001b[39m \u001b[43m_read_train_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m _possibly_unroll_params(config, training_state, training_state_annotations, mesh)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_state, training_state_annotations\n",
      "Cell \u001b[0;32mIn[4], line 104\u001b[0m, in \u001b[0;36m_read_train_checkpoint\u001b[0;34m(config, checkpoint_manager, mesh)\u001b[0m\n\u001b[1;32m    102\u001b[0m learning_rate_schedule \u001b[38;5;241m=\u001b[39m maxtext_utils\u001b[38;5;241m.\u001b[39mcreate_learning_rate_schedule(config)\n\u001b[1;32m    103\u001b[0m tx \u001b[38;5;241m=\u001b[39m optimizers\u001b[38;5;241m.\u001b[39mget_optimizer(config, learning_rate_schedule)\n\u001b[0;32m--> 104\u001b[0m state, state_mesh_notations, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmaxtext_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_training_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_manager\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m num_params \u001b[38;5;241m=\u001b[39m max_utils\u001b[38;5;241m.\u001b[39mcalculate_num_params_from_pytree(state\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    108\u001b[0m max_logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn input checkpoint Number of model params=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_params\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m billion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/maxtext/MaxText/maxtext_utils.py:697\u001b[0m, in \u001b[0;36msetup_training_state\u001b[0;34m(model, data_iterator, tx, config, rng, mesh, checkpoint_manager)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msetup_training_state\u001b[39m(model, data_iterator, tx, config, rng, mesh, checkpoint_manager):\n\u001b[1;32m    696\u001b[0m   is_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msetup_initial_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheckpoint_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/maxtext/MaxText/maxtext_utils.py:742\u001b[0m, in \u001b[0;36msetup_initial_state\u001b[0;34m(model, data_iterator, tx, config, rng, mesh, checkpoint_manager, is_training)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;66;03m# Initialization\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m nn_partitioning\u001b[38;5;241m.\u001b[39maxis_rules(config\u001b[38;5;241m.\u001b[39mlogical_axis_rules):\n\u001b[0;32m--> 742\u001b[0m   restored, raw_params \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpointing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_if_possible\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheckpoint_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_parameters_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_full_state_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_storage_concurrent_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m      \u001b[49m\u001b[43munboxed_abstract_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_single_replica_ckpt_restoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_ocdbt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_storage_use_ocdbt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_zarr3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint_storage_use_zarr3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m restored:\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    757\u001b[0m         checkpoint_manager,\n\u001b[1;32m    758\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    761\u001b[0m         ),\n\u001b[1;32m    762\u001b[0m     ):\n",
      "File \u001b[0;32m~/maxtext/MaxText/checkpointing.py:346\u001b[0m, in \u001b[0;36mload_state_if_possible\u001b[0;34m(checkpoint_manager, data_iterator, load_parameters_from_path, load_full_state_from_path, checkpoint_storage_concurrent_gb, abstract_unboxed_pre_state, enable_single_replica_ckpt_restoring, dataset_type, step, use_ocdbt, use_zarr3)\u001b[0m\n\u001b[1;32m    332\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    333\u001b[0m           checkpoint_manager\u001b[38;5;241m.\u001b[39mrestore(\n\u001b[1;32m    334\u001b[0m               step,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    342\u001b[0m           \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    343\u001b[0m       )\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_parameters_from_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 346\u001b[0m   restored_params \u001b[38;5;241m=\u001b[39m \u001b[43mload_params_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m      \u001b[49m\u001b[43mload_parameters_from_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m      \u001b[49m\u001b[43mabstract_unboxed_pre_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheckpoint_storage_concurrent_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_ocdbt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ocdbt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_zarr3\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_zarr3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, restored_params\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m load_full_state_from_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/maxtext/MaxText/checkpointing.py:412\u001b[0m, in \u001b[0;36mload_params_from_path\u001b[0;34m(load_parameters_from_path, abstract_unboxed_params, checkpoint_storage_concurrent_gb, use_ocdbt, use_zarr3)\u001b[0m\n\u001b[1;32m    410\u001b[0m restore_args \u001b[38;5;241m=\u001b[39m ocp\u001b[38;5;241m.\u001b[39mcheckpoint_utils\u001b[38;5;241m.\u001b[39mconstruct_restore_args(abstract_unboxed_params)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28mprint\u001b[39m(restore_args)\n\u001b[0;32m--> 412\u001b[0m restored \u001b[38;5;241m=\u001b[39m \u001b[43mckptr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mabstract_unboxed_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_args\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restored[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/checkpointers/checkpointer.py:304\u001b[0m, in \u001b[0;36mCheckpointer.restore\u001b[0;34m(self, directory, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRestoring checkpoint from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, directory)\n\u001b[1;32m    303\u001b[0m ckpt_args \u001b[38;5;241m=\u001b[39m construct_checkpoint_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 304\u001b[0m restored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m multihost\u001b[38;5;241m.\u001b[39msync_global_processes(\n\u001b[1;32m    306\u001b[0m     multihost\u001b[38;5;241m.\u001b[39munique_barrier_key(\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheckpointer:restore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m     processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_active_processes,\n\u001b[1;32m    311\u001b[0m )\n\u001b[1;32m    312\u001b[0m restore_duration_secs \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m restore_start_time\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/checkpointers/checkpointer.py:323\u001b[0m, in \u001b[0;36mCheckpointer._restore\u001b[0;34m(self, directory, args)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_restore\u001b[39m(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m, directory: epath\u001b[38;5;241m.\u001b[39mPathLike, args: checkpoint_args\u001b[38;5;241m.\u001b[39mCheckpointArgs\n\u001b[1;32m    322\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/handlers/pytree_checkpoint_handler.py:816\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.restore\u001b[0;34m(self, directory, item, restore_args, transforms, transforms_default_to_original, legacy_transform_fn, args)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    806\u001b[0m     (directory \u001b[38;5;241m/\u001b[39m PYTREE_METADATA_FILE)\u001b[38;5;241m.\u001b[39mexists()\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m can_ignore_aggregate_file\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m legacy_transform_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    810\u001b[0m ):\n\u001b[1;32m    811\u001b[0m   args \u001b[38;5;241m=\u001b[39m BasePyTreeRestoreArgs(\n\u001b[1;32m    812\u001b[0m       item,\n\u001b[1;32m    813\u001b[0m       restore_args\u001b[38;5;241m=\u001b[39mrestore_args,\n\u001b[1;32m    814\u001b[0m       partial_restore\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mpartial_restore,\n\u001b[1;32m    815\u001b[0m   )\n\u001b[0;32m--> 816\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m logging\u001b[38;5;241m.\u001b[39mvlog(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectory=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, restore_args=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, directory, restore_args)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mexists():\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/handlers/base_pytree_checkpoint_handler.py:774\u001b[0m, in \u001b[0;36mBasePyTreeCheckpointHandler.restore\u001b[0;34m(self, directory, args)\u001b[0m\n\u001b[1;32m    772\u001b[0m   item \u001b[38;5;241m=\u001b[39m value_metadata_tree\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mpartial_restore:\n\u001b[0;32m--> 774\u001b[0m   value_metadata_tree \u001b[38;5;241m=\u001b[39m \u001b[43mtree_structure_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_trim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_metadata_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    776\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m   restore_args \u001b[38;5;241m=\u001b[39m tree_structure_utils\u001b[38;5;241m.\u001b[39mtree_trim(\n\u001b[1;32m    778\u001b[0m       item, restore_args, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    779\u001b[0m   )\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m   \u001b[38;5;66;03m# is_empty_or_leaf is necessary here to treat empty nodes (e.g. empty\u001b[39;00m\n\u001b[1;32m    782\u001b[0m   \u001b[38;5;66;03m# dicts, lists, custom nodes) as leaves, as they do not contain any\u001b[39;00m\n\u001b[1;32m    783\u001b[0m   \u001b[38;5;66;03m# actual data to be restored, but are needed to maintain the structure.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:235\u001b[0m, in \u001b[0;36mtree_trim\u001b[0;34m(template, structure, trimmed_structure_callback, strict)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtree_trim\u001b[39m(\n\u001b[1;32m    197\u001b[0m     template: PyTreeOf[Any],\n\u001b[1;32m    198\u001b[0m     structure: PyTreeOf[T],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    202\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTreeOf[T] \u001b[38;5;241m|\u001b[39m parts_of\u001b[38;5;241m.\u001b[39mPartsOf[PyTreeOf[T]]:\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Removes nodes in `structure` so that its shape matches that of `template`.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m  Only dictionary entries are trimmed; sequences are unchanged and the length\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m      sequence or dictionary is encountered.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43m_tree_trim_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrimmed_structure_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrimmed_structure_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mfull_structure \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:411\u001b[0m, in \u001b[0;36m_tree_trim_impl\u001b[0;34m(template, structure, trimmed_structure_callback, strict)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# To avoid a self-referential recursion, we create a partial that captures\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# the `trimmed_structure_callback` and `strict` arguments instead of doing an\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# implicit closure.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m tree_trim_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    407\u001b[0m     _tree_trim,\n\u001b[1;32m    408\u001b[0m     trimmed_structure_callback\u001b[38;5;241m=\u001b[39mtrimmed_structure_callback,\n\u001b[1;32m    409\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    410\u001b[0m )\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parts_of\u001b[38;5;241m.\u001b[39mPartsOf(template, \u001b[43mtree_trim_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:289\u001b[0m, in \u001b[0;36m_tree_trim\u001b[0;34m(path, template, structure, trimmed_structure_callback, strict)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, n \u001b[38;5;129;01min\u001b[39;00m drop_items:\n\u001b[1;32m    287\u001b[0m       trimmed_structure_callback((\u001b[38;5;241m*\u001b[39mpath, k), n)\n\u001b[0;32m--> 289\u001b[0m   keep_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    290\u001b[0m       k: _tree_trim(\n\u001b[1;32m    291\u001b[0m           (\u001b[38;5;241m*\u001b[39mpath, k), template[k], v, trimmed_structure_callback, strict\n\u001b[1;32m    292\u001b[0m       )\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m keep_items\n\u001b[1;32m    294\u001b[0m   }\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(template)((\u001b[38;5;241m*\u001b[39mkeep_dict\u001b[38;5;241m.\u001b[39mitems(), \u001b[38;5;241m*\u001b[39mplaceholder_items))  \u001b[38;5;66;03m# pytype:disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m named_tuple \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39misinstance_of_namedtuple(named_tuple):\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:290\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, n \u001b[38;5;129;01min\u001b[39;00m drop_items:\n\u001b[1;32m    287\u001b[0m       trimmed_structure_callback((\u001b[38;5;241m*\u001b[39mpath, k), n)\n\u001b[1;32m    289\u001b[0m   keep_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 290\u001b[0m       k: \u001b[43m_tree_trim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m          \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrimmed_structure_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m keep_items\n\u001b[1;32m    294\u001b[0m   }\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(template)((\u001b[38;5;241m*\u001b[39mkeep_dict\u001b[38;5;241m.\u001b[39mitems(), \u001b[38;5;241m*\u001b[39mplaceholder_items))  \u001b[38;5;66;03m# pytype:disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m named_tuple \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39misinstance_of_namedtuple(named_tuple):\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:289\u001b[0m, in \u001b[0;36m_tree_trim\u001b[0;34m(path, template, structure, trimmed_structure_callback, strict)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, n \u001b[38;5;129;01min\u001b[39;00m drop_items:\n\u001b[1;32m    287\u001b[0m       trimmed_structure_callback((\u001b[38;5;241m*\u001b[39mpath, k), n)\n\u001b[0;32m--> 289\u001b[0m   keep_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    290\u001b[0m       k: _tree_trim(\n\u001b[1;32m    291\u001b[0m           (\u001b[38;5;241m*\u001b[39mpath, k), template[k], v, trimmed_structure_callback, strict\n\u001b[1;32m    292\u001b[0m       )\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m keep_items\n\u001b[1;32m    294\u001b[0m   }\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(template)((\u001b[38;5;241m*\u001b[39mkeep_dict\u001b[38;5;241m.\u001b[39mitems(), \u001b[38;5;241m*\u001b[39mplaceholder_items))  \u001b[38;5;66;03m# pytype:disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m named_tuple \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39misinstance_of_namedtuple(named_tuple):\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:290\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, n \u001b[38;5;129;01min\u001b[39;00m drop_items:\n\u001b[1;32m    287\u001b[0m       trimmed_structure_callback((\u001b[38;5;241m*\u001b[39mpath, k), n)\n\u001b[1;32m    289\u001b[0m   keep_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 290\u001b[0m       k: \u001b[43m_tree_trim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m          \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrimmed_structure_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m keep_items\n\u001b[1;32m    294\u001b[0m   }\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(template)((\u001b[38;5;241m*\u001b[39mkeep_dict\u001b[38;5;241m.\u001b[39mitems(), \u001b[38;5;241m*\u001b[39mplaceholder_items))  \u001b[38;5;66;03m# pytype:disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m named_tuple \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39misinstance_of_namedtuple(named_tuple):\n",
      "    \u001b[0;31m[... skipping similar frames: <dictcomp> at line 290 (1 times), _tree_trim at line 289 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:289\u001b[0m, in \u001b[0;36m_tree_trim\u001b[0;34m(path, template, structure, trimmed_structure_callback, strict)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, n \u001b[38;5;129;01min\u001b[39;00m drop_items:\n\u001b[1;32m    287\u001b[0m       trimmed_structure_callback((\u001b[38;5;241m*\u001b[39mpath, k), n)\n\u001b[0;32m--> 289\u001b[0m   keep_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    290\u001b[0m       k: _tree_trim(\n\u001b[1;32m    291\u001b[0m           (\u001b[38;5;241m*\u001b[39mpath, k), template[k], v, trimmed_structure_callback, strict\n\u001b[1;32m    292\u001b[0m       )\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m keep_items\n\u001b[1;32m    294\u001b[0m   }\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(template)((\u001b[38;5;241m*\u001b[39mkeep_dict\u001b[38;5;241m.\u001b[39mitems(), \u001b[38;5;241m*\u001b[39mplaceholder_items))  \u001b[38;5;66;03m# pytype:disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m named_tuple \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39misinstance_of_namedtuple(named_tuple):\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:290\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, n \u001b[38;5;129;01min\u001b[39;00m drop_items:\n\u001b[1;32m    287\u001b[0m       trimmed_structure_callback((\u001b[38;5;241m*\u001b[39mpath, k), n)\n\u001b[1;32m    289\u001b[0m   keep_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 290\u001b[0m       k: \u001b[43m_tree_trim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m          \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrimmed_structure_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m keep_items\n\u001b[1;32m    294\u001b[0m   }\n\u001b[1;32m    295\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(template)((\u001b[38;5;241m*\u001b[39mkeep_dict\u001b[38;5;241m.\u001b[39mitems(), \u001b[38;5;241m*\u001b[39mplaceholder_items))  \u001b[38;5;66;03m# pytype:disable=wrong-arg-count\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m named_tuple \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39misinstance_of_namedtuple(named_tuple):\n",
      "File \u001b[0;32m~/venv-maxtext/lib/python3.10/site-packages/orbax/checkpoint/_src/tree/structure_utils.py:272\u001b[0m, in \u001b[0;36m_tree_trim\u001b[0;34m(path, template, structure, trimmed_structure_callback, strict)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m:=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m template \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m structure_dict]:\n\u001b[1;32m    271\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m strict:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m keys in structure path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincluding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# Fill the result with placeholders\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     placeholder_items\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    279\u001b[0m         (k, jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, template[k])) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing\n\u001b[1;32m    280\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Missing 4 keys in structure path ('params', 'params', 'decoder', 'layers'), including: ['layers_0', 'layers_1', 'layers_2', 'layers_3']"
     ]
    }
   ],
   "source": [
    "training_state3, training_state_annotations3 = generate_decode_checkpoint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dab548",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_state_annotations3.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747246d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested_keys(training_state3.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd157e",
   "metadata": {},
   "source": [
    "## checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "217bdf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'force_unroll', 'scan_layers', 'base_output_directory', 'skip_jax_distributed_system']\n",
      "Running Model: llama4-17b-16e\n",
      "Updating following parameters in config\n",
      "\n",
      "decoder_block: llama4\n",
      "mlp_activations: ['silu', 'linear']\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "tokenizer_type: huggingface\n",
      "base_emb_dim: 5120\n",
      "base_num_decoder_layers: 48\n",
      "base_num_query_heads: 40\n",
      "base_num_kv_heads: 8\n",
      "base_mlp_dim: 16384\n",
      "base_moe_mlp_dim: 8192\n",
      "vocab_size: 202048\n",
      "normalization_layer_epsilon: 1e-05\n",
      "rope_max_timescale: 500000\n",
      "rope_type: llama3.1\n",
      "num_experts: 16\n",
      "shared_experts: 1\n",
      "num_experts_per_tok: 1\n",
      "use_qk_norm: True\n",
      "nope_layer_interval: 4\n",
      "interleave_moe_layer_step: 1\n",
      "inhomogeneous_layer_cycle_interval: 4\n",
      "temperature_tuning: True\n",
      "chunk_attn_window_size: 8192\n",
      "image_size_for_vit: 336\n",
      "Updating keys from model: ['decoder_block', 'mlp_activations', 'enable_dropout', 'logits_via_embedding', 'tokenizer_type', 'base_emb_dim', 'base_num_decoder_layers', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_moe_mlp_dim', 'vocab_size', 'normalization_layer_epsilon', 'rope_max_timescale', 'rope_type', 'num_experts', 'shared_experts', 'num_experts_per_tok', 'use_qk_norm', 'nope_layer_interval', 'interleave_moe_layer_step', 'inhomogeneous_layer_cycle_interval', 'temperature_tuning', 'chunk_attn_window_size', 'image_size_for_vit']\n",
      "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: autoselected\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 5120\n",
      "Config param base_mlp_dim: 16384\n",
      "Config param base_moe_mlp_dim: 8192\n",
      "Config param base_num_decoder_layers: 48\n",
      "Config param base_num_kv_heads: 8\n",
      "Config param base_num_query_heads: 40\n",
      "Config param base_output_directory: gs://runner-maxtext-logs\n",
      "Config param beta_fast: 32\n",
      "Config param beta_slow: 1\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: gs://runner-maxtext-logs/test3/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_concurrent_gb: 96\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param chunk_attn_window_size: 8192\n",
      "Config param collect_stack_trace: False\n",
      "Config param colocated_python_data_input: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param context: remat\n",
      "Config param context_parallel_load_balance: True\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_context_autoregressive_parallelism: 1\n",
      "Config param dcn_context_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param dcn_tensor_transpose_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: DecoderBlockType.LLAMA4\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dtype_mm: float32\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param dump_step: -1\n",
      "Config param emb_dim: 5120\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_gcp_goodput_metrics: True\n",
      "Config param enable_gcp_step_deviation_metrics: True\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_llm_inference_pool: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_padding_causal_mask: True\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_prefix_caching: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param enable_tensorboard: True\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 12.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param first_num_dense_layers: 0\n",
      "Config param float32_logits: False\n",
      "Config param float32_qk_product: False\n",
      "Config param force_unroll: False\n",
      "Config param freeze_vision_encoder_params: True\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param generate_slice: v5e-16\n",
      "Config param global_batch_size_to_eval_on: 12\n",
      "Config param global_batch_size_to_load: 12\n",
      "Config param global_batch_size_to_load_eval: 12\n",
      "Config param global_batch_size_to_train_on: 12\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 30\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_file_type: arrayrecord\n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param grain_worker_count_eval: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param heartbeat_reporting_interval_in_seconds: 5\n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param hidden_size_for_vit: 1408\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_context_autoregressive_parallelism: 1\n",
      "Config param ici_context_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param ici_tensor_transpose_parallelism: 1\n",
      "Config param image_path: \n",
      "Config param image_size_for_vit: 336\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param inhomogeneous_layer_cycle_interval: 4\n",
      "Config param init_weights_seed: 0\n",
      "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
      "Config param interleave_moe_layer_step: 1\n",
      "Config param intermediate_size_for_vit: 5632\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_lora_rank: 512\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 150001\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: gs://shuningjin-multipod-dev/llama4-17b-16e/conversion/meta-scanned/0/items\n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param local_rope_max_timescale: -1\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param lora_input_adapters_path: \n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_position_embeddings: 163840\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 2048\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: gs://runner-maxtext-logs/test3/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 12\n",
      "Config param micro_batch_size_to_train_on: 12\n",
      "Config param mla_naive_kvcache: True\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 16384\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_name: llama4-17b-16e\n",
      "Config param moe_mlp_dim: 8192\n",
      "Config param monitor_goodput: False\n",
      "Config param monitor_step_time_deviation: True\n",
      "Config param mscale: 1.0\n",
      "Config param mu_dtype: float32\n",
      "Config param multi_sampling: False\n",
      "Config param n_routing_groups: -1\n",
      "Config param nope_layer_interval: 4\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_attention_heads_for_vit: 16\n",
      "Config param num_channels_for_vit: 3\n",
      "Config param num_decoder_layers: 48\n",
      "Config param num_epoch: 1\n",
      "Config param num_experts: 16\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_hidden_layers_for_vit: 34\n",
      "Config param num_kv_heads: 8\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 40\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimize_mesh_for_tpu_v6e: False\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param original_max_position_embeddings: 4096\n",
      "Config param out_proj: remat\n",
      "Config param override_model_config: False\n",
      "Config param packing: True\n",
      "Config param pagedattn_head_dim_alignment: 128\n",
      "Config param pagedattn_max_pages_per_group: 64\n",
      "Config param pagedattn_num_pages: 64\n",
      "Config param pagedattn_pages_per_compute_block: 4\n",
      "Config param pagedattn_tokens_per_page: 32\n",
      "Config param param_scan_axis: 1\n",
      "Config param parameter_memory_host_offload: False\n",
      "Config param patch_size_for_vit: 14\n",
      "Config param per_device_batch_size: 12.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param pipeline_fsdp_ag_once: False\n",
      "Config param pipeline_parallel_layers: -1\n",
      "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param prefill_chunk_size: 256\n",
      "Config param prefill_slice: v5e-16\n",
      "Config param prefix_caching_dram_byte: 100000000000\n",
      "Config param prefix_caching_hbm_byte: 10000000000\n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param projector_dropout_for_vit: 0.0\n",
      "Config param projector_input_dim_for_vit: 4096\n",
      "Config param projector_output_dim_for_vit: 4096\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param q_lora_rank: 0\n",
      "Config param qk_nope_head_dim: 128\n",
      "Config param qk_rope_head_dim: 64\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param remat_policy_for_vit: minimal\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
      "Config param report_performance_metric_for_gcp_monitoring: False\n",
      "Config param reshape_q: False\n",
      "Config param return_log_prob: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_factor: 40\n",
      "Config param rope_max_timescale: 500000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param rope_theta_for_vit: 10000\n",
      "Config param rope_type: llama3.1\n",
      "Config param rope_use_scale: True\n",
      "Config param routed_bias: False\n",
      "Config param routed_scaling_factor: 1.0\n",
      "Config param routed_score_func: \n",
      "Config param run_name: test3\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_layers_per_stage: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sft_train_on_completion_only: False\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param shared_experts: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: True\n",
      "Config param sliding_window_size: 0\n",
      "Config param sparse_matmul: True\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param step_deviation_interval_seconds: 30\n",
      "Config param steps: 150001\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param temperature_tuning: True\n",
      "Config param tensorboard_dir: gs://runner-maxtext-logs/test3/tensorboard/\n",
      "Config param tile_activation_dim: 1024\n",
      "Config param tile_batch_seq: 512\n",
      "Config param tile_size_for_vit: 336\n",
      "Config param tile_weight_dim: 1024\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param tokenizer_type: huggingface\n",
      "Config param topk_routing_group: -1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param train_split: train\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_chat_template: False\n",
      "Config param use_chunked_prefill: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_multimodal: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_qk_norm: True\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_random_routing: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_sft: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param v_head_dim: 128\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vision_output_dim_for_vit: 4096\n",
      "Config param vocab_size: 202048\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: float32\n"
     ]
    }
   ],
   "source": [
    "cmd=\"python MaxText/configs/base.yml model_name=llama4-17b-16e \\\n",
    "base_output_directory=gs://runner-maxtext-logs run_name=test3 \\\n",
    "load_parameters_path=gs://shuningjin-multipod-dev/llama4-17b-16e/conversion/meta-scanned/0/items scan_layers=true force_unroll=false \\\n",
    "async_checkpointing=false skip_jax_distributed_system=true\"\n",
    "argv = cmd.split()\n",
    "config = pyconfig.initialize(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b359e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "Checkpoint manager created!\n",
      "Read training checkpoint from: \n"
     ]
    }
   ],
   "source": [
    "training_state4, training_state_annotations4 = generate_decode_checkpoint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d27ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nested_keys(training_state4.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7f121",
   "metadata": {},
   "source": [
    "## checkpoint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ce7a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'force_unroll', 'scan_layers', 'base_output_directory', 'skip_jax_distributed_system']\n",
      "Running Model: llama4-17b-16e\n",
      "Updating following parameters in config\n",
      "\n",
      "decoder_block: llama4\n",
      "mlp_activations: ['silu', 'linear']\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "tokenizer_type: huggingface\n",
      "base_emb_dim: 5120\n",
      "base_num_decoder_layers: 48\n",
      "base_num_query_heads: 40\n",
      "base_num_kv_heads: 8\n",
      "base_mlp_dim: 16384\n",
      "base_moe_mlp_dim: 8192\n",
      "vocab_size: 202048\n",
      "normalization_layer_epsilon: 1e-05\n",
      "rope_max_timescale: 500000\n",
      "rope_type: llama3.1\n",
      "num_experts: 16\n",
      "shared_experts: 1\n",
      "num_experts_per_tok: 1\n",
      "use_qk_norm: True\n",
      "nope_layer_interval: 4\n",
      "interleave_moe_layer_step: 1\n",
      "inhomogeneous_layer_cycle_interval: 4\n",
      "temperature_tuning: True\n",
      "chunk_attn_window_size: 8192\n",
      "image_size_for_vit: 336\n",
      "Updating keys from model: ['decoder_block', 'mlp_activations', 'enable_dropout', 'logits_via_embedding', 'tokenizer_type', 'base_emb_dim', 'base_num_decoder_layers', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_moe_mlp_dim', 'vocab_size', 'normalization_layer_epsilon', 'rope_max_timescale', 'rope_type', 'num_experts', 'shared_experts', 'num_experts_per_tok', 'use_qk_norm', 'nope_layer_interval', 'interleave_moe_layer_step', 'inhomogeneous_layer_cycle_interval', 'temperature_tuning', 'chunk_attn_window_size', 'image_size_for_vit']\n",
      "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: True\n",
      "Config param add_eos: True\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: autoselected\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 5120\n",
      "Config param base_mlp_dim: 16384\n",
      "Config param base_moe_mlp_dim: 8192\n",
      "Config param base_num_decoder_layers: 48\n",
      "Config param base_num_kv_heads: 8\n",
      "Config param base_num_query_heads: 40\n",
      "Config param base_output_directory: gs://runner-maxtext-logs\n",
      "Config param beta_fast: 32\n",
      "Config param beta_slow: 1\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: gs://runner-maxtext-logs/test3/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_concurrent_gb: 96\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param chunk_attn_window_size: 8192\n",
      "Config param collect_stack_trace: False\n",
      "Config param colocated_python_data_input: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param context: remat\n",
      "Config param context_parallel_load_balance: True\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.1\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: tfds\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_context_autoregressive_parallelism: 1\n",
      "Config param dcn_context_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param dcn_tensor_transpose_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: DecoderBlockType.LLAMA4\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dtype_mm: float32\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param dump_step: -1\n",
      "Config param emb_dim: 5120\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_gcp_goodput_metrics: True\n",
      "Config param enable_gcp_step_deviation_metrics: True\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_llm_inference_pool: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_padding_causal_mask: True\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_prefix_caching: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param enable_tensorboard: True\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.1\n",
      "Config param eval_interval: -1\n",
      "Config param eval_per_device_batch_size: 12.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param first_num_dense_layers: 0\n",
      "Config param float32_logits: False\n",
      "Config param float32_qk_product: False\n",
      "Config param force_unroll: False\n",
      "Config param freeze_vision_encoder_params: True\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param generate_slice: v5e-16\n",
      "Config param global_batch_size_to_eval_on: 12\n",
      "Config param global_batch_size_to_load: 12\n",
      "Config param global_batch_size_to_load_eval: 12\n",
      "Config param global_batch_size_to_train_on: 12\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 30\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_file_type: arrayrecord\n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param grain_worker_count_eval: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param heartbeat_reporting_interval_in_seconds: 5\n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param hidden_size_for_vit: 1408\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_context_autoregressive_parallelism: 1\n",
      "Config param ici_context_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param ici_tensor_transpose_parallelism: 1\n",
      "Config param image_path: \n",
      "Config param image_size_for_vit: 336\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param inhomogeneous_layer_cycle_interval: 4\n",
      "Config param init_weights_seed: 0\n",
      "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
      "Config param interleave_moe_layer_step: 1\n",
      "Config param intermediate_size_for_vit: 5632\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_lora_rank: 512\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 150001\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: gs://shuningjin-multipod-dev/scout_pretrain/shuning-llama4-2025-06-22-23-49-54/checkpoints/0/items\n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param local_rope_max_timescale: -1\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param lora_input_adapters_path: \n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_position_embeddings: 163840\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 2048\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: gs://runner-maxtext-logs/test3/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 12\n",
      "Config param micro_batch_size_to_train_on: 12\n",
      "Config param mla_naive_kvcache: True\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 16384\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_name: llama4-17b-16e\n",
      "Config param moe_mlp_dim: 8192\n",
      "Config param monitor_goodput: False\n",
      "Config param monitor_step_time_deviation: True\n",
      "Config param mscale: 1.0\n",
      "Config param mu_dtype: float32\n",
      "Config param multi_sampling: False\n",
      "Config param n_routing_groups: -1\n",
      "Config param nope_layer_interval: 4\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_attention_heads_for_vit: 16\n",
      "Config param num_channels_for_vit: 3\n",
      "Config param num_decoder_layers: 48\n",
      "Config param num_epoch: 1\n",
      "Config param num_experts: 16\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_hidden_layers_for_vit: 34\n",
      "Config param num_kv_heads: 8\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 40\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimize_mesh_for_tpu_v6e: False\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param original_max_position_embeddings: 4096\n",
      "Config param out_proj: remat\n",
      "Config param override_model_config: False\n",
      "Config param packing: True\n",
      "Config param pagedattn_head_dim_alignment: 128\n",
      "Config param pagedattn_max_pages_per_group: 64\n",
      "Config param pagedattn_num_pages: 64\n",
      "Config param pagedattn_pages_per_compute_block: 4\n",
      "Config param pagedattn_tokens_per_page: 32\n",
      "Config param param_scan_axis: 1\n",
      "Config param parameter_memory_host_offload: False\n",
      "Config param patch_size_for_vit: 14\n",
      "Config param per_device_batch_size: 12.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param pipeline_fsdp_ag_once: False\n",
      "Config param pipeline_parallel_layers: -1\n",
      "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param prefill_chunk_size: 256\n",
      "Config param prefill_slice: v5e-16\n",
      "Config param prefix_caching_dram_byte: 100000000000\n",
      "Config param prefix_caching_hbm_byte: 10000000000\n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param projector_dropout_for_vit: 0.0\n",
      "Config param projector_input_dim_for_vit: 4096\n",
      "Config param projector_output_dim_for_vit: 4096\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param q_lora_rank: 0\n",
      "Config param qk_nope_head_dim: 128\n",
      "Config param qk_rope_head_dim: 64\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param remat_policy_for_vit: minimal\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
      "Config param report_performance_metric_for_gcp_monitoring: False\n",
      "Config param reshape_q: False\n",
      "Config param return_log_prob: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_factor: 40\n",
      "Config param rope_max_timescale: 500000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param rope_theta_for_vit: 10000\n",
      "Config param rope_type: llama3.1\n",
      "Config param rope_use_scale: True\n",
      "Config param routed_bias: False\n",
      "Config param routed_scaling_factor: 1.0\n",
      "Config param routed_score_func: \n",
      "Config param run_name: test3\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_layers_per_stage: False\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sft_train_on_completion_only: False\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param shared_experts: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: True\n",
      "Config param sliding_window_size: 0\n",
      "Config param sparse_matmul: True\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param step_deviation_interval_seconds: 30\n",
      "Config param steps: 150001\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param temperature_tuning: True\n",
      "Config param tensorboard_dir: gs://runner-maxtext-logs/test3/tensorboard/\n",
      "Config param tile_activation_dim: 1024\n",
      "Config param tile_batch_seq: 512\n",
      "Config param tile_size_for_vit: 336\n",
      "Config param tile_weight_dim: 1024\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param tokenizer_type: huggingface\n",
      "Config param topk_routing_group: -1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param train_split: train\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_chat_template: False\n",
      "Config param use_chunked_prefill: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_multimodal: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_qk_norm: True\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_random_routing: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_sft: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param v_head_dim: 128\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vision_output_dim_for_vit: 4096\n",
      "Config param vocab_size: 202048\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: float32\n"
     ]
    }
   ],
   "source": [
    "cmd=\"python MaxText/configs/base.yml model_name=llama4-17b-16e \\\n",
    "base_output_directory=gs://runner-maxtext-logs run_name=test3 \\\n",
    "load_parameters_path=gs://shuningjin-multipod-dev/scout_pretrain/shuning-llama4-2025-06-22-23-49-54/checkpoints/0/items scan_layers=true force_unroll=false \\\n",
    "async_checkpointing=false skip_jax_distributed_system=true\"\n",
    "argv = cmd.split()\n",
    "config = pyconfig.initialize(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ca995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "Checkpoint manager created!\n",
      "Read training checkpoint from: \n",
      "checkpoint manager exists so trying to load this run's existing checkpoint\n",
      "restoring params from gs://shuningjin-multipod-dev/scout_pretrain/shuning-llama4-2025-06-22-23-49-54/checkpoints/0/items\n",
      "Creating checkpoint manager with ocdbt=True and zarr3=True\n",
      "{'params': {'decoder': {'decoder_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'),), memory_kind=unpinned_host), global_shape=(5120,), shape=(5120,), strict=True)}, 'layers': {'layers_0': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}, 'layers_1': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}, 'layers_2': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}, 'layers_3': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None), memory_kind=unpinned_host), global_shape=(5120, 12, 16), shape=(5120, 12, 16), strict=True)}, 'wi_0': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wi_1': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(16, 12, 5120, 8192), shape=(16, 12, 5120, 8192), strict=True), 'wo': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context')), memory_kind=unpinned_host), global_shape=(16, 12, 8192, 5120), shape=(16, 12, 8192, 5120), strict=True)}, 'shared_experts': {'wi_0': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wi_1': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 12, 8192), shape=(5120, 12, 8192), strict=True)}, 'wo': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(8192, 12, 5120), shape=(8192, 12, 5120), strict=True)}}}, 'post_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'pre_self_attention_layer_norm': {'scale': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage'), memory_kind=unpinned_host), global_shape=(5120, 12), shape=(5120, 12), strict=True)}, 'self_attention': {'key': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}, 'out': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(40, 12, 128, 5120), shape=(40, 12, 128, 5120), strict=True)}, 'query': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 40, 128), shape=(5120, 12, 40, 128), strict=True)}, 'value': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None), memory_kind=unpinned_host), global_shape=(5120, 12, 8, 128), shape=(5120, 12, 8, 128), strict=True)}}}}, 'logits_dense': {'kernel': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), memory_kind=unpinned_host), global_shape=(5120, 202048), shape=(5120, 202048), strict=True)}}, 'token_embedder': {'embedding': ArrayRestoreArgs(restore_type=<class 'jax.Array'>, dtype=dtype('float32'), mesh=None, mesh_axes=None, sharding=NamedSharding(mesh=Mesh('data': 1, 'stage': 1, 'fsdp': 1, 'fsdp_transpose': 1, 'sequence': 1, 'context': 1, 'context_autoregressive': 1, 'tensor': 1, 'tensor_transpose': 1, 'tensor_sequence': 1, 'expert': 1, 'autoregressive': 1, axis_types=(Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto, Auto)), spec=PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), memory_kind=unpinned_host), global_shape=(202048, 5120), shape=(202048, 5120), strict=True)}}}\n"
     ]
    }
   ],
   "source": [
    "training_state5, training_state_annotations5 = generate_decode_checkpoint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b0fc10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: params.decoder.decoder_norm.scale.\n",
      "value shape: (5120,)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.MoeBlock_0.gate.kernel.\n",
      "value shape: (5120, 12, 16)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.MoeBlock_0.wi_0.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.MoeBlock_0.wi_1.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.MoeBlock_0.wo.\n",
      "value shape: (16, 12, 8192, 5120)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.shared_experts.wi_0.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.shared_experts.wi_1.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_0.Llama4MoEBlock_0.shared_experts.wo.kernel.\n",
      "value shape: (8192, 12, 5120)\n",
      "key: params.decoder.layers.layers_0.post_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_0.pre_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_0.self_attention.key.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_0.self_attention.out.kernel.\n",
      "value shape: (40, 12, 128, 5120)\n",
      "key: params.decoder.layers.layers_0.self_attention.query.kernel.\n",
      "value shape: (5120, 12, 40, 128)\n",
      "key: params.decoder.layers.layers_0.self_attention.value.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.MoeBlock_0.gate.kernel.\n",
      "value shape: (5120, 12, 16)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.MoeBlock_0.wi_0.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.MoeBlock_0.wi_1.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.MoeBlock_0.wo.\n",
      "value shape: (16, 12, 8192, 5120)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.shared_experts.wi_0.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.shared_experts.wi_1.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_1.Llama4MoEBlock_0.shared_experts.wo.kernel.\n",
      "value shape: (8192, 12, 5120)\n",
      "key: params.decoder.layers.layers_1.post_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_1.pre_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_1.self_attention.key.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_1.self_attention.out.kernel.\n",
      "value shape: (40, 12, 128, 5120)\n",
      "key: params.decoder.layers.layers_1.self_attention.query.kernel.\n",
      "value shape: (5120, 12, 40, 128)\n",
      "key: params.decoder.layers.layers_1.self_attention.value.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.MoeBlock_0.gate.kernel.\n",
      "value shape: (5120, 12, 16)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.MoeBlock_0.wi_0.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.MoeBlock_0.wi_1.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.MoeBlock_0.wo.\n",
      "value shape: (16, 12, 8192, 5120)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.shared_experts.wi_0.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.shared_experts.wi_1.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_2.Llama4MoEBlock_0.shared_experts.wo.kernel.\n",
      "value shape: (8192, 12, 5120)\n",
      "key: params.decoder.layers.layers_2.post_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_2.pre_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_2.self_attention.key.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_2.self_attention.out.kernel.\n",
      "value shape: (40, 12, 128, 5120)\n",
      "key: params.decoder.layers.layers_2.self_attention.query.kernel.\n",
      "value shape: (5120, 12, 40, 128)\n",
      "key: params.decoder.layers.layers_2.self_attention.value.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.MoeBlock_0.gate.kernel.\n",
      "value shape: (5120, 12, 16)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.MoeBlock_0.wi_0.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.MoeBlock_0.wi_1.\n",
      "value shape: (16, 12, 5120, 8192)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.MoeBlock_0.wo.\n",
      "value shape: (16, 12, 8192, 5120)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.shared_experts.wi_0.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.shared_experts.wi_1.kernel.\n",
      "value shape: (5120, 12, 8192)\n",
      "key: params.decoder.layers.layers_3.Llama4MoEBlock_0.shared_experts.wo.kernel.\n",
      "value shape: (8192, 12, 5120)\n",
      "key: params.decoder.layers.layers_3.post_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_3.pre_self_attention_layer_norm.scale.\n",
      "value shape: (5120, 12)\n",
      "key: params.decoder.layers.layers_3.self_attention.key.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.layers.layers_3.self_attention.out.kernel.\n",
      "value shape: (40, 12, 128, 5120)\n",
      "key: params.decoder.layers.layers_3.self_attention.query.kernel.\n",
      "value shape: (5120, 12, 40, 128)\n",
      "key: params.decoder.layers.layers_3.self_attention.value.kernel.\n",
      "value shape: (5120, 12, 8, 128)\n",
      "key: params.decoder.logits_dense.kernel.\n",
      "value shape: (5120, 202048)\n",
      "key: params.token_embedder.embedding.\n",
      "value shape: (202048, 5120)\n"
     ]
    }
   ],
   "source": [
    "print_nested_keys(training_state5.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c566069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'decoder': {'decoder_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'),)},\n",
       "   'layers': {'layers_0': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None)},\n",
       "       'wi_0': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wi_1': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wo': PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context'))},\n",
       "      'shared_experts': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}},\n",
       "     'post_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'pre_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))},\n",
       "      'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}},\n",
       "    'layers_1': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None)},\n",
       "       'wi_0': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wi_1': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wo': PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context'))},\n",
       "      'shared_experts': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}},\n",
       "     'post_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'pre_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))},\n",
       "      'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}},\n",
       "    'layers_2': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None)},\n",
       "       'wi_0': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wi_1': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wo': PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context'))},\n",
       "      'shared_experts': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}},\n",
       "     'post_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'pre_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))},\n",
       "      'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}},\n",
       "    'layers_3': {'Llama4MoEBlock_0': {'MoeBlock_0': {'gate': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', None)},\n",
       "       'wi_0': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wi_1': PartitionSpec('expert', 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context'), ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')),\n",
       "       'wo': PartitionSpec('expert', 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), ('fsdp', 'sequence', 'tensor_transpose', 'context'))},\n",
       "      'shared_experts': {'wi_0': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wi_1': {'kernel': PartitionSpec(('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'), 'stage', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'))},\n",
       "       'wo': {'kernel': PartitionSpec(('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive'), 'stage', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert'))}}},\n",
       "     'post_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'pre_self_attention_layer_norm': {'scale': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence'), 'stage')},\n",
       "     'self_attention': {'key': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'out': {'kernel': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), 'stage', None, ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))},\n",
       "      'query': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)},\n",
       "      'value': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), 'stage', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), None)}}}},\n",
       "   'logits_dense': {'kernel': PartitionSpec(('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'), ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'))}},\n",
       "  'token_embedder': {'embedding': PartitionSpec(('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive'), ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert'))}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_state_annotations5.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a758bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxtext",
   "language": "python",
   "name": "maxtext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
