export TOKENIZER_PATH=assets/tokenizer_llama3.tiktoken
export LOAD_PARAMETERS_PATH=gs://wyzhang-dev/ckpt/maxtext/llama-3.1-8b-int8
export MAX_PREFILL_PREDICT_LENGTH=1024
export MAX_TARGET_LENGTH=2048
export MODEL_NAME=llama3.1-8b
export ICI_FSDP_PARALLELISM=1
export ICI_AUTOREGRESSIVE_PARALLELISM=1
export ICI_TENSOR_PARALLELISM=-1
export SCAN_LAYERS=false
export WEIGHT_DTYPE=bfloat16
export PER_DEVICE_BATCH_SIZE=1
export INFERENCE_SERVER=MaxtextInterleavedServer

python3 MaxText/maxengine_server.py \
  MaxText/configs/base.yml \
  tokenizer_path=${TOKENIZER_PATH} \
  load_parameters_path=${LOAD_PARAMETERS_PATH} \
  max_prefill_predict_length=${MAX_PREFILL_PREDICT_LENGTH} \
  max_target_length=${MAX_TARGET_LENGTH} \
  model_name=${MODEL_NAME} \
  ici_fsdp_parallelism=${ICI_FSDP_PARALLELISM} \
  ici_autoregressive_parallelism=${ICI_AUTOREGRESSIVE_PARALLELISM} \
  ici_tensor_parallelism=${ICI_TENSOR_PARALLELISM} \
  scan_layers=${SCAN_LAYERS} \
  weight_dtype=${WEIGHT_DTYPE} \
  per_device_batch_size=${PER_DEVICE_BATCH_SIZE} \
  checkpoint_is_quantized=True \
  quantization=int8 \
  quantize_kvcache=True \
  compute_axis_order=0,2,1,3 \
  ar_cache_axis_order=0,2,1,3 \
  enable_jax_profiler=True \
  stack_prefill_result_cache=True \
  inference_benchmark_test=True \
  inference_server=${INFERENCE_SERVER} \
