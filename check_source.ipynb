{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# example cmd: python3 ckpt.py --ckpt_dir=/home/ranran/ran_ckpt/hf-16b/hf/ --checkpoint_type=safetensors\n",
    "\n",
    "CHECKPOINT_TYPES = (\"pth\", \"safetensors\")\n",
    "\n",
    "def print_nested_keys(data, prefix=\"\"):\n",
    "  \"\"\"\n",
    "  Prints nested keys of a dictionary-like structure in a directory-like format.\n",
    "  Args:\n",
    "      data: The dictionary-like structure to traverse.\n",
    "      prefix: The current path prefix.\n",
    "  \"\"\"\n",
    "  if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "      current_path = f\"{prefix}{key}.\"\n",
    "      print_nested_keys(value, current_path)\n",
    "  else:\n",
    "    print(f\"key: {prefix}\")\n",
    "    print(f\"value shape: {data.shape}\")\n",
    "\n",
    "\n",
    "def load_pth_checkpoint(ckpt_paths):\n",
    "  chkpt_vars_raw = {}\n",
    "  for i, ckpt_path in enumerate(ckpt_paths):\n",
    "    print(f\"Loading checkpointpath {i+1} of {len(ckpt_paths)} ...\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    chkpt_vars_raw[int(ckpt_path.name.split(\".\", maxsplit=2)[1])] = checkpoint  \n",
    "  print_nested_keys(chkpt_vars_raw)\n",
    "\n",
    "\n",
    "def load_safetensors_checkpoint(ckpt_paths):\n",
    "  chkpt_vars_raw = {}\n",
    "  for i, ckpt_path in enumerate(ckpt_paths):\n",
    "    print(f\"Loading checkpoint path {i+1} of {len(ckpt_paths)} ...\")\n",
    "    with safe_open(ckpt_path, framework=\"pt\") as f:\n",
    "      for k in f.keys():\n",
    "        chkpt_vars_raw[k] = f.get_tensor(k)\n",
    "  print_nested_keys(chkpt_vars_raw)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "  parser = argparse.ArgumentParser(description=\"Print the contents (keys and shapes) of safetensors and PyTorch checkpoint files.\")\n",
    "  parser.add_argument(\"--ckpt_dir\", type=str, required=True)\n",
    "  parser.add_argument(\"--checkpoint_type\", type=str, required=True)\n",
    "  args = parser.parse_args(argv)\n",
    "  print(args)\n",
    "\n",
    "  if args.checkpoint_type not in CHECKPOINT_TYPES:\n",
    "    raise NotImplementedError\n",
    "\n",
    "  ckpt_paths = sorted(pathlib.Path(args.ckpt_dir).glob(f\"[!.]*.{args.checkpoint_type}\"))\n",
    "  if args.checkpoint_type == \"safetensors\":\n",
    "    load_safetensors_checkpoint(ckpt_paths)\n",
    "  else:\n",
    "    assert args.checkpoint_type == \"pth\"\n",
    "    load_pth_checkpoint(ckpt_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f8b54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ckpt_dir='/home/shuningjin/llama4-17b-16e/meta-bf16', checkpoint_type='pth')\n",
      "hi\n",
      "Loading checkpointpath 1 of 8 ...\n",
      "Loading checkpointpath 2 of 8 ...\n",
      "Loading checkpointpath 3 of 8 ...\n",
      "Loading checkpointpath 4 of 8 ...\n",
      "Loading checkpointpath 5 of 8 ...\n",
      "Loading checkpointpath 6 of 8 ...\n",
      "Loading checkpointpath 7 of 8 ...\n",
      "Loading checkpointpath 8 of 8 ...\n",
      "key: 0.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 0.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 0.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 0.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 0.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 0.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 0.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 0.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 0.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 0.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 0.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 0.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 0.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 1.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 1.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 1.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 1.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 1.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 1.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 1.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 1.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 1.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 1.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 1.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 1.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 2.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 2.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 2.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 2.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 2.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 2.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 2.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 2.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 2.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 2.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 2.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 2.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 3.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 3.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 3.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 3.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 3.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 3.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 3.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 3.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 3.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 3.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 3.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 3.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 4.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 4.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 4.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 4.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 4.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 4.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 4.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 4.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 4.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 4.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 4.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 4.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 5.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 5.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 5.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 5.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 5.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 5.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 5.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 5.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 5.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 5.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 5.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 5.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 6.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 6.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 6.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 6.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 6.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 6.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 6.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 6.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 6.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 6.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 6.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 6.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_projection.weight.\n",
      "value shape: torch.Size([640, 4096])\n",
      "key: 7.tok_embeddings.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 7.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.output.weight.\n",
      "value shape: torch.Size([25256, 5120])\n",
      "key: 7.layers.0.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.0.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.0.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.0.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.0.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.0.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.0.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.0.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.0.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.0.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.0.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.0.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.0.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.0.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.1.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.1.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.1.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.1.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.1.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.1.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.1.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.1.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.1.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.1.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.1.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.1.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.1.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.1.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.2.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.2.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.2.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.2.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.2.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.2.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.2.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.2.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.2.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.2.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.2.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.2.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.2.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.2.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.3.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.3.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.3.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.3.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.3.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.3.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.3.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.3.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.3.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.3.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.3.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.3.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.3.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.3.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.4.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.4.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.4.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.4.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.4.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.4.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.4.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.4.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.4.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.4.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.4.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.4.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.4.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.4.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.5.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.5.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.5.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.5.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.5.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.5.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.5.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.5.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.5.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.5.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.5.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.5.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.5.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.5.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.6.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.6.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.6.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.6.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.6.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.6.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.6.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.6.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.6.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.6.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.6.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.6.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.6.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.6.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.7.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.7.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.7.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.7.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.7.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.7.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.7.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.7.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.7.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.7.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.7.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.7.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.7.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.7.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.8.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.8.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.8.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.8.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.8.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.8.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.8.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.8.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.8.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.8.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.8.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.8.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.8.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.8.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.9.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.9.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.9.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.9.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.9.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.9.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.9.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.9.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.9.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.9.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.9.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.9.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.9.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.9.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.10.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.10.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.10.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.10.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.10.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.10.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.10.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.10.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.10.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.10.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.10.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.10.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.10.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.10.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.11.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.11.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.11.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.11.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.11.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.11.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.11.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.11.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.11.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.11.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.11.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.11.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.11.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.11.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.12.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.12.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.12.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.12.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.12.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.12.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.12.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.12.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.12.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.12.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.12.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.12.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.12.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.12.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.13.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.13.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.13.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.13.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.13.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.13.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.13.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.13.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.13.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.13.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.13.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.13.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.13.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.13.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.14.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.14.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.14.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.14.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.14.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.14.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.14.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.14.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.14.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.14.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.14.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.14.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.14.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.14.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.15.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.15.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.15.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.15.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.15.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.15.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.15.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.15.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.15.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.15.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.15.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.15.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.15.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.15.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.16.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.16.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.16.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.16.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.16.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.16.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.16.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.16.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.16.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.16.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.16.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.16.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.16.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.16.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.17.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.17.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.17.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.17.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.17.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.17.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.17.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.17.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.17.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.17.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.17.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.17.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.17.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.17.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.18.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.18.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.18.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.18.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.18.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.18.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.18.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.18.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.18.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.18.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.18.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.18.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.18.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.18.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.19.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.19.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.19.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.19.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.19.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.19.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.19.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.19.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.19.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.19.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.19.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.19.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.19.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.19.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.20.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.20.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.20.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.20.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.20.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.20.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.20.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.20.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.20.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.20.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.20.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.20.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.20.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.20.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.21.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.21.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.21.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.21.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.21.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.21.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.21.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.21.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.21.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.21.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.21.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.21.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.21.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.21.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.22.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.22.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.22.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.22.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.22.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.22.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.22.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.22.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.22.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.22.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.22.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.22.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.22.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.22.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.23.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.23.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.23.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.23.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.23.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.23.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.23.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.23.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.23.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.23.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.23.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.23.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.23.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.23.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.24.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.24.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.24.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.24.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.24.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.24.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.24.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.24.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.24.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.24.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.24.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.24.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.24.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.24.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.25.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.25.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.25.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.25.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.25.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.25.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.25.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.25.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.25.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.25.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.25.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.25.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.25.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.25.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.26.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.26.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.26.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.26.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.26.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.26.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.26.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.26.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.26.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.26.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.26.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.26.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.26.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.26.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.27.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.27.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.27.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.27.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.27.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.27.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.27.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.27.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.27.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.27.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.27.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.27.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.27.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.27.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.28.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.28.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.28.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.28.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.28.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.28.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.28.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.28.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.28.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.28.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.28.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.28.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.28.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.28.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.29.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.29.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.29.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.29.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.29.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.29.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.29.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.29.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.29.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.29.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.29.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.29.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.29.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.29.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.30.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.30.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.30.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.30.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.30.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.30.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.30.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.30.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.30.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.30.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.30.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.30.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.30.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.30.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.31.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.31.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.31.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.31.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.31.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.31.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.31.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.31.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.31.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.31.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.31.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.31.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.31.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.31.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.32.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.32.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.32.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.32.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.32.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.32.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.32.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.32.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.32.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.32.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.32.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.32.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.32.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.32.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.33.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.33.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.33.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.33.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.33.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.33.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.33.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.33.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.33.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.33.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.33.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.33.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.33.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.33.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.34.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.34.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.34.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.34.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.34.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.34.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.34.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.34.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.34.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.34.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.34.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.34.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.34.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.34.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.35.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.35.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.35.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.35.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.35.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.35.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.35.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.35.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.35.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.35.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.35.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.35.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.35.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.35.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.36.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.36.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.36.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.36.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.36.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.36.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.36.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.36.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.36.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.36.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.36.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.36.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.36.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.36.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.37.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.37.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.37.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.37.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.37.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.37.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.37.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.37.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.37.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.37.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.37.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.37.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.37.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.37.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.38.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.38.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.38.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.38.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.38.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.38.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.38.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.38.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.38.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.38.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.38.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.38.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.38.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.38.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.39.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.39.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.39.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.39.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.39.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.39.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.39.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.39.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.39.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.39.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.39.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.39.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.39.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.39.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.40.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.40.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.40.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.40.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.40.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.40.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.40.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.40.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.40.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.40.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.40.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.40.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.40.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.40.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.41.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.41.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.41.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.41.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.41.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.41.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.41.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.41.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.41.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.41.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.41.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.41.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.41.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.41.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.42.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.42.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.42.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.42.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.42.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.42.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.42.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.42.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.42.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.42.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.42.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.42.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.42.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.42.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.43.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.43.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.43.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.43.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.43.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.43.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.43.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.43.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.43.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.43.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.43.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.43.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.43.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.43.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.44.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.44.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.44.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.44.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.44.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.44.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.44.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.44.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.44.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.44.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.44.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.44.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.44.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.44.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.45.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.45.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.45.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.45.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.45.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.45.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.45.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.45.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.45.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.45.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.45.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.45.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.45.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.45.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.46.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.46.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.46.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.46.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.46.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.46.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.46.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.46.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.46.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.46.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.46.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.46.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.46.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.46.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.47.feed_forward.norm.weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.layers.47.attention.wo.weight.\n",
      "value shape: torch.Size([5120, 640])\n",
      "key: 7.layers.47.feed_forward.expert_activation_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.47.feed_forward.running_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.47.feed_forward.global_gate_stats_3E.\n",
      "value shape: torch.Size([3, 16])\n",
      "key: 7.layers.47.feed_forward.router_DE.\n",
      "value shape: torch.Size([5120, 16])\n",
      "key: 7.layers.47.feed_forward.w_in_shared_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.47.feed_forward.w_swiglu_FD.weight.\n",
      "value shape: torch.Size([1024, 5120])\n",
      "key: 7.layers.47.feed_forward.w_out_shared_DF.weight.\n",
      "value shape: torch.Size([5120, 1024])\n",
      "key: 7.layers.47.feed_forward.experts.moe_w_in_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.47.feed_forward.experts.moe_w_swiglu_eD_F.\n",
      "value shape: torch.Size([81920, 1024])\n",
      "key: 7.layers.47.feed_forward.experts.moe_w_out_eF_D.\n",
      "value shape: torch.Size([16384, 5120])\n",
      "key: 7.layers.47.attention.wqkv.weight.\n",
      "value shape: torch.Size([896, 5120])\n",
      "key: 7.layers.47.attention.wqkv.layer_norm_weight.\n",
      "value shape: torch.Size([5120])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.class_embedding.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.ln_post.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_adapter.mlp.c_proj.weight.\n",
      "value shape: torch.Size([4096, 512])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.positional_embedding_vlm.\n",
      "value shape: torch.Size([577, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.ln_pre.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.ln_pre.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_adapter.mlp.c_fc.weight.\n",
      "value shape: torch.Size([512, 5632])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.33.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.ln_post.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.11.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.30.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.13.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.2.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.7.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.9.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.conv1._linear.weight.\n",
      "value shape: torch.Size([176, 588])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.10.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.24.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.32.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.14.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.3.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.6.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.23.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.4.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.attn.wo.weight.\n",
      "value shape: torch.Size([1408, 176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.31.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.27.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wv.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.0.mlp.c_proj.weight.\n",
      "value shape: torch.Size([1408, 704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.1.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.15.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.17.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.21.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.20.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.12.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.29.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.mlp.c_proj.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.mlp.c_fc.weight.\n",
      "value shape: torch.Size([704, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.ln_2.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.ln_2.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.16.attn.wq.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.26.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.5.ln_1.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.28.mlp.c_fc.bias.\n",
      "value shape: torch.Size([704])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.19.attn.wq.bias.\n",
      "value shape: torch.Size([176])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.18.attn.wo.bias.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.25.attn.wk.weight.\n",
      "value shape: torch.Size([176, 1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.22.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n",
      "key: 7.vision_embeddings.vision_encoder.transformer.resblocks.8.ln_1.weight.\n",
      "value shape: torch.Size([1408])\n"
     ]
    }
   ],
   "source": [
    "cmd = \"--ckpt_dir=/home/shuningjin/llama4-17b-16e/meta-bf16 --checkpoint_type=pth\".split()\n",
    "main(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40236c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxtext",
   "language": "python",
   "name": "maxtext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
