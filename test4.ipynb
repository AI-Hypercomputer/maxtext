{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889937f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jupyter Python Executable: /home/shuningjin_google_com/venv-rl/bin/python3\n",
      "2. Current Working Directory: /home/shuningjin_google_com/maxtext\n",
      "3. VLLM is importing from:    /home/shuningjin_google_com/vllm/vllm/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('/home/shuningjin_google_com/maxtext')\n",
    "\n",
    "print(f\"1. Jupyter Python Executable: {sys.executable}\")\n",
    "print(f\"2. Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    import vllm\n",
    "    print(f\"3. VLLM is importing from:    {vllm.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"3. VLLM import failed completely.\")\n",
    "except AttributeError:\n",
    "    print(\"3. VLLM imported but has no __file__ (Namespace or directory shadowing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fd0f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 02:07:26 [utils.py:253] non-default args: {'max_model_len': 128, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'model': 'unsloth/gpt-oss-20b-BF16'}\n",
      "WARNING 11-13 02:07:26 [model.py:437] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.\n",
      "INFO 11-13 02:07:26 [model.py:630] Resolved architecture: GptOssForCausalLM\n",
      "INFO 11-13 02:07:26 [model.py:1735] Using max model len 128\n",
      "INFO 11-13 02:07:26 [scheduler.py:254] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-13 02:07:26 [config.py:272] Overriding max cuda graph capture size to 1024 for performance.\n",
      "INFO 11-13 02:07:26 [tpu_platform.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=4, sharding_strategy=ShardingStrategy(tensor_parallelism=4, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
      "WARNING 11-13 02:07:26 [tpu_platform.py:154] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16\n",
      "INFO 11-13 02:07:26 [tpu_platform.py:187] Force using UniProcExecutor for JAX on single host.\n",
      "INFO 11-13 02:07:28 [core.py:94] Initializing a V1 LLM engine (v0.11.1rc7.dev83+g64d57c3be) with config: model='unsloth/gpt-oss-20b-BF16', speculative_config=None, tokenizer='unsloth/gpt-oss-20b-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=None, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gpt-oss-20b-BF16, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.DYNAMO_TRACE_ONCE: 2>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'openxla', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': True, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 1024, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 02:07:28.139453  467351 b295d63588a.cc:762] Linux version 5.19.0-1027-gcp (buildd@lcy02-amd64-078) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.1.0-2ubuntu1~22.04) 12.1.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #29~22.04.1-Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023\n",
      "I1113 02:07:28.146753  467351 b295d63588a.cc:844] Process id 467351\n",
      "I1113 02:07:28.146770  467351 b295d63588a.cc:849] Current working directory /home/shuningjin_google_com/maxtext\n",
      "I1113 02:07:28.146772  467351 b295d63588a.cc:851] Current timezone is UTC (currently UTC +00:00)\n",
      "I1113 02:07:28.146776  467351 b295d63588a.cc:855] Built on Sep 11 2025 15:57:19 (1757631439)\n",
      "I1113 02:07:28.146778  467351 b295d63588a.cc:856]  at rbex-enqueue-targets@lgje25.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "I1113 02:07:28.146779  467351 b295d63588a.cc:857]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "I1113 02:07:28.146781  467351 b295d63588a.cc:858]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "I1113 02:07:28.146786  467351 b295d63588a.cc:861]  from changelist 804429027 with baseline 804273004 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/804273004.1/g3     \n",
      "I1113 02:07:28.146787  467351 b295d63588a.cc:865] Build label: libtpu_lts_20250908_a_RC01\n",
      "I1113 02:07:28.146789  467351 b295d63588a.cc:867] Build tool: Bazel, release r4rca-2025.08.25-1 (mainline @798895491)\n",
      "I1113 02:07:28.146791  467351 b295d63588a.cc:868] Build target: \n",
      "I1113 02:07:28.146795  467351 b295d63588a.cc:875] Command line arguments:\n",
      "I1113 02:07:28.146797  467351 b295d63588a.cc:877] argv[0]: './tpu_driver'\n",
      "I1113 02:07:28.146800  467351 b295d63588a.cc:877] argv[1]: '--minloglevel=0'\n",
      "I1113 02:07:28.146801  467351 b295d63588a.cc:877] argv[2]: '--stderrthreshold=0'\n",
      "I1113 02:07:28.146803  467351 b295d63588a.cc:877] argv[3]: '--v=0'\n",
      "I1113 02:07:28.146805  467351 b295d63588a.cc:877] argv[4]: '--vmodule='\n",
      "I1113 02:07:28.146807  467351 b295d63588a.cc:877] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "I1113 02:07:28.146808  467351 b295d63588a.cc:877] argv[6]: '--max_log_size=1024'\n",
      "I1113 02:07:28.146810  467351 b295d63588a.cc:877] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "I1113 02:07:28.146812  467351 b295d63588a.cc:877] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "I1113 02:07:28.146813  467351 b295d63588a.cc:877] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "I1113 02:07:28.146815  467351 b295d63588a.cc:877] argv[10]: '--2a886c8_twist=false'\n",
      "I1113 02:07:28.146817  467351 b295d63588a.cc:877] argv[11]: '--2a886c8_chip_config_name=megachip_tccontrol'\n",
      "I1113 02:07:28.146818  467351 b295d63588a.cc:877] argv[12]: '--2a886c8_chips_per_host_bounds=2,2,1'\n",
      "I1113 02:07:28.146820  467351 b295d63588a.cc:877] argv[13]: '--2a886c8_host_bounds=1,1,1'\n",
      "I1113 02:07:28.146822  467351 b295d63588a.cc:877] argv[14]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "I1113 02:07:28.146824  467351 b295d63588a.cc:877] argv[15]: '--2a886c8_slice_builder_worker_addresses=10.164.0.41:8471'\n",
      "I1113 02:07:28.146825  467351 b295d63588a.cc:877] argv[16]: '--tpu_slice_builder_dump_chip=true'\n",
      "I1113 02:07:28.146827  467351 b295d63588a.cc:877] argv[17]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "I1113 02:07:28.146829  467351 b295d63588a.cc:877] argv[18]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "I1113 02:07:28.146830  467351 b295d63588a.cc:877] argv[19]: '--bypass_vbar_control_service=0'\n",
      "I1113 02:07:28.146832  467351 b295d63588a.cc:877] argv[20]: '--2a886c8_ici_resilient=false'\n",
      "I1113 02:07:28.146834  467351 b295d63588a.cc:877] argv[21]: '--xla_tpu_use_resilient_collective_emitter=false'\n",
      "I1113 02:07:28.146835  467351 b295d63588a.cc:877] argv[22]: '--tpu_slice_builder_topology_discovery_fault_injection='\n",
      "I1113 02:07:28.146837  467351 b295d63588a.cc:877] argv[23]: '--runtime_metric_service_port=8431'\n",
      "I1113 02:07:28.146838  467351 b295d63588a.cc:877] argv[24]: '--tpu_hbm_report_enable=1'\n",
      "I1113 02:07:28.146840  467351 b295d63588a.cc:877] argv[25]: '--tpu_hbm_report_frequency=5s'\n",
      "I1113 02:07:28.146842  467351 b295d63588a.cc:877] argv[26]: '--enable_runtime_uptime_telemetry=true'\n",
      "I1113 02:07:28.146843  467351 b295d63588a.cc:877] argv[27]: ''\n",
      "I1113 02:07:28.146845  467351 b295d63588a.cc:877] argv[28]: '--xla_tpu_use_enhanced_launch_barrier=true'\n",
      "I1113 02:07:28.147092  467351 init.cc:78] Remote crash gathering hook installed.\n",
      "I1113 02:07:28.147177  467351 tpu_runtime_type_flags.cc:79] --tpu_use_tfrt not specified. Using default value: true\n",
      "I1113 02:07:28.179699  467351 tpu_hal.cc:429] Registered plugin from module: breakpoint_debugger_server\n",
      "I1113 02:07:28.179976  467351 log_message_host_command_handler.cc:70] Registering LogMessageHostCommandHandler\n",
      "I1113 02:07:28.179986  467351 host_command_handler_factory.cc:31] Skipping registration of host command handler for opcode Log because it is not in the allowlist.\n",
      "I1113 02:07:28.180009  467351 tpu_hal.cc:429] Registered plugin from module: megascale_sync_flag_logger\n",
      "I1113 02:07:28.181864  467351 tpu_hal.cc:429] Registered plugin from module: RuntimeMetricHelper\n",
      "I1113 02:07:28.181951  467351 tf_tpu_flags.cc:63] 2a886c8Platform is NOT registered.\n",
      "I1113 02:07:28.182140  467351 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "I1113 02:07:28.182164  467351 tpu_hal.cc:429] Registered plugin from module: sdc_checker_callback\n",
      "I1113 02:07:28.182230  467351 tpu_hal.cc:429] Registered plugin from module: xsc_explicit_checksum_tracing_callback\n",
      "I1113 02:07:28.182316  467351 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "I1113 02:07:28.203391  467351 config.cc:256] gRPC experiments enabled: error_flatten, event_engine_callback_cq, event_engine_client, event_engine_dns, event_engine_dns_non_client_channel, event_engine_listener, google_no_envelope_resolver, monitoring_experiment, tsi_frame_protector_without_locks\n",
      "I1113 02:07:28.212274  467351 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 228, prefix = futex-default\n",
      "I1113 02:07:28.212478  467351 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <E0-27 BA-D8 4E-56 00-00 A0-28 BA-D8 4E-56 00-00 80-5C 9E-6E 45-7F 00-00 40-6A 3D-70 45-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = 0, clock = 0x564ed5f14148\n",
      "I1113 02:07:28.221659  478518 cachednslookup.cc:391] TTL not found in response, not caching response\n",
      "I1113 02:07:28.469228  467351 debug_manager.cc:220] Registering error handler with name: libtpu_telemetry_handler\n",
      "W1113 02:07:28.469882  467351 uptime_telemetry.cc:187] UptimeMetric attributes are updated.\n",
      "Previous Attributes: go/debugstr  \n",
      "key: \"uptime_attributes\"\n",
      "value {\n",
      "  kvlist_attr {\n",
      "    attributes {\n",
      "      key: \"ml_framework_name\"\n",
      "      value {\n",
      "        string_attr: \"jax\"\n",
      "      }\n",
      "    }\n",
      "    attributes {\n",
      "      key: \"ml_framework_version\"\n",
      "      value {\n",
      "        string_attr: \"jax-0.7.2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "New Attributes: go/debugstr  \n",
      "key: \"uptime_attributes\"\n",
      "value {\n",
      "  kvlist_attr {\n",
      "    attributes {\n",
      "      key: \"ml_framework_name\"\n",
      "      value {\n",
      "        string_attr: \"pytorch/xla2\"\n",
      "      }\n",
      "    }\n",
      "    attributes {\n",
      "      key: \"ml_framework_version\"\n",
      "      value {\n",
      "        string_attr: \"pytorch/xla2-v0.0.1\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "I1113 02:07:28.479664  467351 singleton_tpu_states_manager.cc:73] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "I1113 02:07:28.479678  467351 singleton_tpu_states_manager.cc:96] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "I1113 02:07:28.480835  467351 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 02:07:28.480845  467351 tpu_version_flag.cc:54] Using auto-detected TPU version TPU v5\n",
      "I1113 02:07:28.481670  467351 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 02:07:28.482484  467351 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 02:07:28.483317  467351 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 02:07:28.483669  467351 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 02:07:28.483675  467351 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 02:07:28.483676  467351 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 02:07:28.483679  467351 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 02:07:28.487296  478782 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 02:07:28.487316  478782 flags_util.cc:318] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "I1113 02:07:28.488177  478782 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 02:07:28.488185  478782 tpu_network_factory.cc:67] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "I1113 02:07:28.498071  478784 futex.cc:68] RAW: Futex::Swap(): using FUTEX_WAKE + FUTEX_WAIT\n",
      "I1113 02:07:29.563126  478785 async_driver.cc:454] [/dev/vfio/0 tpu575:pe0:0] vf_id:0 Driver opened.\n",
      "I1113 02:07:29.568874  478788 async_driver.cc:454] [/dev/vfio/3 tpu575:pe0:3] vf_id:0 Driver opened.\n",
      "I1113 02:07:29.579075  478786 async_driver.cc:454] [/dev/vfio/1 tpu575:pe0:1] vf_id:0 Driver opened.\n",
      "I1113 02:07:29.635655  478787 async_driver.cc:454] [/dev/vfio/2 tpu575:pe0:2] vf_id:0 Driver opened.\n",
      "W1113 02:07:29.690709  478788 async_driver.cc:1736] All cores not supported.\n",
      "W1113 02:07:29.691126  478785 async_driver.cc:1736] All cores not supported.\n",
      "W1113 02:07:29.691690  478786 async_driver.cc:1736] All cores not supported.\n",
      "W1113 02:07:29.691962  478787 async_driver.cc:1736] All cores not supported.\n",
      "I1113 02:07:29.692047  478782 slice_builder_helper.cc:99] Current host is used as SliceBuilder master.\n",
      "I1113 02:07:29.693011  478782 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "I1113 02:07:29.697435  478782 legacy_topology_discoverer.cc:55] Target Topology: (2, 2, 1)\n",
      "I1113 02:07:30.724629  478782 master.cc:219] Successfully initialized SliceBuilder master session 100c8000af84235f with expected topology (2, 2, 1)\n",
      "I1113 02:07:30.725869  478782 tpu_hal.cc:200] Starting premapped memory manager initialization...\n",
      "W1113 02:07:30.726781  479113 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.726783  479111 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.726778  479112 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.726804  479111 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 02:07:30.726809  479112 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 02:07:30.726799  479113 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 02:07:30.727214  479110 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.727228  479110 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 02:07:30.727253  479112 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.727261  479112 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 02:07:30.727266  479113 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.727271  479113 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 02:07:30.727272  479111 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 02:07:30.727277  479111 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 11-13 02:07:35 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 02:07:35.430713  478782 runtime_metric_service.cc:159] Successfully started Runtime Metric Service on port: 8431\n",
      "I1113 02:07:35.430813  478782 system.cc:1091] tpu::System initialized, current host id: 0, logical device ids: 0,1,2,3\n",
      "I1113 02:07:35.430847  467351 tpu_system_state.cc:217] CreateTpuSystemState: TPU initialization is successful and it took 6.944741369s\n",
      "I1113 02:07:35.430869  467351 tpu_system_state.cc:221] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "I1113 02:07:35.430876  467351 tpu_host_allocator.cc:64] Premapped buffer is using alignment 64\n",
      "I1113 02:07:35.431344  467351 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "I1113 02:07:35.565675  467351 autofdo_agent.cc:198] xla_tpu_autofdo_profile_dir updated to \n",
      "W1113 02:07:35.565690  467351 autofdo_agent.cc:201] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "I1113 02:07:35.568960  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1113 02:07:35.568977  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1113 02:07:35.569174  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.242396ms\n",
      "I1113 02:07:35.595474  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:35.600572  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.3691575ms\n",
      "I1113 02:07:35.609662  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.02574175ms\n",
      "I1113 02:07:35.609761  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:13d169e8f0cb52132827f3458861645b2ad7cf1a3afcb1127a6444697d9ec7eb\n",
      "I1113 02:07:35.609765  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):7274c5d1d7ca793892a3a02db907049e506cf3ed81768774f478da0a45758e78\n",
      "I1113 02:07:35.609767  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:35.609865  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.000504ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 02:07:35 [tpu_runner.py:273] Init mesh | mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto))\n",
      "INFO 11-13 02:07:35 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
      "INFO 11-13 02:07:35 [utils.py:59] Prepared request paddings: [8, 16, 32, 64, 128, 256]\n",
      "INFO 11-13 02:07:35 [compilation_manager.py:34] Enabling JAX compile cache.\n",
      "INFO 11-13 02:07:35 [tpu_worker.py:151] Init worker | rank=0 | node_id=0 | is_driver_worker=True | hbm=[(0.0, 95.74), (0.0, 95.74), (0.0, 95.74), (0.0, 95.74)]GiB\n",
      "INFO 11-13 02:07:35 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=vllm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 02:07:35.632204  467351 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 8 instructions, modules: jit__threefry_seed\n",
      "I1113 02:07:35.632219  467351 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__threefry_seed instructions: 8 fingerprint: \n",
      "I1113 02:07:35.632353  467351 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__threefry_seed instructions: 8\n",
      "I1113 02:07:35.632356  467351 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1113 02:07:35.634574  467351 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1113 02:07:35.635058  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663111577 bytes.\n",
      "I1113 02:07:35.635070  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883155578 bytes.\n",
      "I1113 02:07:35.635346  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.28457425ms\n",
      "I1113 02:07:35.635673  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:35.641082  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.48365675ms\n",
      "I1113 02:07:35.641724  478783 2a886c8_compiler_base.cc:3045] final program bundle count: 205 note this count does not reflect cycles spent executing delays.\n",
      "I1113 02:07:35.646278  478783 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1113 02:07:35.647320  478783 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (50.0K).\n",
      "I1113 02:07:35.647492  478783 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__threefry_seed\n",
      "I1113 02:07:35.647501  478783 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 50.0K / 95.74G\n",
      "I1113 02:07:35.647504  478783 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 4.0K / 64.00M\n",
      "I1113 02:07:35.647518  478783 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.05M:\n",
      "I1113 02:07:35.647520  478783 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1113 02:07:35.647521  478783 2a886c8_compiler_base.cc:3549]     program           50.0K \n",
      "I1113 02:07:35.647522  478783 2a886c8_compiler_base.cc:3549]     arguments          512B \n",
      "I1113 02:07:35.647523  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:35.647525  478783 2a886c8_compiler_base.cc:3549] Output size 512B; shares 0B with arguments.\n",
      "I1113 02:07:35.647526  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:35.647527  478783 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1113 02:07:35.647528  478783 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1113 02:07:35.647530  478783 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1113 02:07:35.647531  478783 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1113 02:07:35.647532  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:35.647533  478783 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1113 02:07:35.647534  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:35.647542  478783 2a886c8_compiler_base.cc:3553] Program sflag requirement 212B:\n",
      "I1113 02:07:35.647544  478783 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1113 02:07:35.647545  478783 2a886c8_compiler_base.cc:3553]     scoped               8B\n",
      "I1113 02:07:35.647546  478783 2a886c8_compiler_base.cc:3553] Program vmem requirement 4.0K:\n",
      "I1113 02:07:35.647547  478783 2a886c8_compiler_base.cc:3553]     scoped             4.0K\n",
      "I1113 02:07:35.647548  478783 2a886c8_compiler_base.cc:3553] Program smem requirement 32B:\n",
      "I1113 02:07:35.647549  478783 2a886c8_compiler_base.cc:3553]     scoped              32B\n",
      "I1113 02:07:35.647550  478783 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1113 02:07:35.647551  478783 2a886c8_compiler_base.cc:3553] Program hbm requirement 50.0K:\n",
      "I1113 02:07:35.647552  478783 2a886c8_compiler_base.cc:3553]     overlays          50.0K\n",
      "I1113 02:07:35.647553  478783 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 1.9K / 1.00M (1 parameters)\n",
      "I1113 02:07:35.647571  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.449635ms\n",
      "I1113 02:07:35.647653  478783 isa_program_util_common.cc:510] (HLO module jit__threefry_seed): Executable fingerprint:4e5a39cda394a06cc48348ff5409ccb2e1723f45f094264066ae3490e8cf3524\n",
      "I1113 02:07:35.647656  478783 isa_program_util_common.cc:514] (HLO module jit__threefry_seed): Executable fingerprint (including data segments):dc57a40245541f0780852f287f4f5dd66c906d0a6bbfcaaaf69cdd4b47aa3406\n",
      "I1113 02:07:35.647658  478783 isa_program_util_common.cc:517] (HLO module jit__threefry_seed): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:35.647752  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 16.7398365ms\n",
      "I1113 02:07:35.666021  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1113 02:07:35.666037  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1113 02:07:35.666162  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.0910575ms\n",
      "I1113 02:07:35.666427  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:35.670710  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.32938525ms\n",
      "I1113 02:07:35.676965  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.217137ms\n",
      "I1113 02:07:35.677053  478783 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:05accf46df728591c609ef1ad06662677f25037e3433b8cac821e0dcdaf0f76f\n",
      "I1113 02:07:35.677057  478783 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):7824ca0711ee26b12f60d723ce497e4d5f85ef619be6170fdaefc1ff4fabd785\n",
      "I1113 02:07:35.677059  478783 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:35.677138  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.0999865ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-13 02:07:37 [rocm.py:34] Failed to import from amdsmi with ModuleNotFoundError(\"No module named 'amdsmi'\")\n",
      "WARNING 11-13 02:07:37 [rocm.py:39] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "WARNING 11-13 02:07:37 [rocm.py:45] Failed to import from vllm._rocm_C with ModuleNotFoundError(\"No module named 'vllm._rocm_C'\")\n",
      "WARNING 11-13 02:07:37 [selector.py:147] use_v1 parameter for get_attn_backend_cls is deprecated and will be removed in v0.13.0 or v1.0.0, whichever is soonest. Please remove it from your plugin code.\n",
      "WARNING 11-13 02:07:37 [registry.py:172] _Backend has been renamed to AttentionBackendEnum. Please update your code to use AttentionBackendEnum instead. _Backend will be removed in a future release.\n",
      "INFO 11-13 02:07:37 [tpu_platform.py:63] Cannot use None backend on TPU.\n",
      "INFO 11-13 02:07:37 [tpu_platform.py:66] Using Pallas V1 backend.\n",
      "INFO 11-13 02:07:37 [layer.py:331] Disabling MoE shared_experts cuda stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.68it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:03,  1.78it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.62it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:03,  1.55it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:03<00:02,  1.51it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:03<00:02,  1.49it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:04<00:01,  1.77it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.60it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 02:07:43 [default_loader.py:314] Loading weights took 5.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "I1113 02:07:43.516075  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97604262809 bytes.\n",
      "I1113 02:07:43.516121  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4880213140 bytes.\n",
      "I1113 02:07:43.516286  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.10222775ms\n",
      "I1113 02:07:43.516769  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.532138  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 15.49147975ms\n",
      "I1113 02:07:43.540094  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.88115975ms\n",
      "I1113 02:07:43.540217  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:58ce4f458d4d91ce939f7ee5091646b64aeb1a68fd8efdab8061ad95bbaddf87\n",
      "I1113 02:07:43.540221  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):18f7c1b8fd973d02086a83949d38540a9d7f9dbb1d5a788703d778cded7ed379\n",
      "I1113 02:07:43.540224  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.540313  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 29.20239125ms\n",
      "I1113 02:07:43.559293  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633754009 bytes.\n",
      "I1113 02:07:43.559308  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881687700 bytes.\n",
      "I1113 02:07:43.559426  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.6381125ms\n",
      "I1113 02:07:43.559731  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.573805  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 14.1341695ms\n",
      "I1113 02:07:43.580924  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.08443725ms\n",
      "I1113 02:07:43.581010  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:83c9b85902891a356e046bce166bbebc8b722649a4016ff1a5ed790cb880920f\n",
      "I1113 02:07:43.581014  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):9d44370a088ad526935beac2fe617e198dfbd05a897a1873189d5c50c87ee597\n",
      "I1113 02:07:43.581016  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.581092  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 25.3369365ms\n",
      "I1113 02:07:43.597543  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639631769 bytes.\n",
      "I1113 02:07:43.597557  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881981588 bytes.\n",
      "I1113 02:07:43.597688  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.5586495ms\n",
      "I1113 02:07:43.597970  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.654822  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 56.90412275ms\n",
      "I1113 02:07:43.664902  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 10.03124425ms\n",
      "I1113 02:07:43.665041  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:3b50d8789a9e3f3b104c2fe8b9665f4dd580a856d5403fa2bc2892ffac36a079\n",
      "I1113 02:07:43.665045  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):16c4f414a0fd71cbbe8010ed33820bd803ee115de9eea563548473828e9d09e3\n",
      "I1113 02:07:43.665047  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.665132  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 71.03254325ms\n",
      "I1113 02:07:43.680720  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633754009 bytes.\n",
      "I1113 02:07:43.680736  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881687700 bytes.\n",
      "I1113 02:07:43.680849  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.05584775ms\n",
      "I1113 02:07:43.681132  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.711605  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 30.52315775ms\n",
      "I1113 02:07:43.719470  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.83254725ms\n",
      "I1113 02:07:43.719569  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:fc335a3c5bf3b8c128fad17e9d39750689ec7457cd1f16995f2ab69be07b5363\n",
      "I1113 02:07:43.719572  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):b852271f2d648d94251143e950d67c8dff8b284b202c0dfdeb68447ac400212d\n",
      "I1113 02:07:43.719574  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.719661  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 41.89846925ms\n",
      "I1113 02:07:43.735394  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97660275609 bytes.\n",
      "I1113 02:07:43.735410  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883013780 bytes.\n",
      "I1113 02:07:43.735541  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.43766875ms\n",
      "I1113 02:07:43.735827  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.791143  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 55.37289175ms\n",
      "I1113 02:07:43.801331  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 10.11404ms\n",
      "I1113 02:07:43.801490  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:80401440c3e445335d0a724a91707db758d5b1df92516e58389cd2b69a275eaf\n",
      "I1113 02:07:43.801494  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):73047d5057bf00f806325fb160fd1cabf80cdc2fc81197e6b9243ab036fa6e7d\n",
      "I1113 02:07:43.801501  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.801579  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 69.5069635ms\n",
      "I1113 02:07:43.817546  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633754009 bytes.\n",
      "I1113 02:07:43.817561  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881687700 bytes.\n",
      "I1113 02:07:43.817674  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.10786175ms\n",
      "I1113 02:07:43.817954  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.848334  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 30.43074275ms\n",
      "I1113 02:07:43.856315  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.95159475ms\n",
      "I1113 02:07:43.856416  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:2b8d1ef8493cb66ae64170f631621a69c22d11275f3afa31e81a2ab6734bb0ce\n",
      "I1113 02:07:43.856420  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):043857375c40b5feac96d179845108a72c454d11b2f5454afe5d2250a9c53f47\n",
      "I1113 02:07:43.856422  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.856509  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 41.97500175ms\n",
      "I1113 02:07:43.872963  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633641369 bytes.\n",
      "I1113 02:07:43.872979  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881682068 bytes.\n",
      "I1113 02:07:43.873187  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.683288ms\n",
      "I1113 02:07:43.873559  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:43.968792  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 95.3150885ms\n",
      "I1113 02:07:43.977586  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.7380905ms\n",
      "I1113 02:07:43.977698  478783 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:16979283a871623a3d281b0b9270f21a28d117ef6286b74f3b14537619d0e6a4\n",
      "I1113 02:07:43.977702  478783 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):aa1c1352a30ccbc748da1495b023d0f426ef9f5809a474fe304cb1843a7237d8\n",
      "I1113 02:07:43.977704  478783 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:43.977781  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 108.3126865ms\n",
      "I1113 02:07:43.994206  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633733529 bytes.\n",
      "I1113 02:07:43.994228  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881686676 bytes.\n",
      "I1113 02:07:43.994360  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.3570605ms\n",
      "I1113 02:07:43.994649  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.042149  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 47.54556825ms\n",
      "I1113 02:07:44.050579  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.37511925ms\n",
      "I1113 02:07:44.050697  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:b6b5711b78edbdc54e201ac43c0011040a219857e17516c8454b4dad2cc257b5\n",
      "I1113 02:07:44.050701  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):6c15272f7c6d961dc3e4f8333a985091535d7a42d35e0a284b1f2d90e516239c\n",
      "I1113 02:07:44.050703  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.050779  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 59.80669125ms\n",
      "I1113 02:07:44.069683  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633641369 bytes.\n",
      "I1113 02:07:44.069699  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881682068 bytes.\n",
      "I1113 02:07:44.069940  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.774082ms\n",
      "I1113 02:07:44.070262  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.134422  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 64.21210975ms\n",
      "I1113 02:07:44.144336  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.8826295ms\n",
      "I1113 02:07:44.144468  478783 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:e2a77f982dbc41759bad6a5a62299f4620e3798b88aa19ea929e0f57f8137da1\n",
      "I1113 02:07:44.144472  478783 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):940544b6d226901a6b0bd42f7dc7ddf307f6efba6c7299b79c158e3344f9e507\n",
      "I1113 02:07:44.144473  478783 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.144551  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 78.42566375ms\n",
      "[2025-11-13 02:07:44] WARNING ops_registry.py:36: Duplicate op registration for aten.__and__\n",
      "[2025-11-13 02:07:44] WARNING unquantized.py:92: Bias might return incorrect value.\n",
      "I1113 02:07:44.181323  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663224729 bytes.\n",
      "I1113 02:07:44.181339  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161236 bytes.\n",
      "I1113 02:07:44.181453  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.11316475ms\n",
      "I1113 02:07:44.181747  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.185551  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.852625ms\n",
      "I1113 02:07:44.191837  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.2536955ms\n",
      "I1113 02:07:44.191923  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:f837605375cecab62e1ff72ed75fe7b28165e0d79c899767a5bc8f6476af2d2a\n",
      "I1113 02:07:44.191928  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):aecf3f58c7b2675f7884fcff74aa873715b81dd70db441546fdb446f5900914e\n",
      "I1113 02:07:44.191930  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.192003  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.6944865ms\n",
      "I1113 02:07:44.207682  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 02:07:44.207698  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 02:07:44.207812  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.08230325ms\n",
      "I1113 02:07:44.208094  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.211812  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.7649295ms\n",
      "I1113 02:07:44.217957  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.1155155ms\n",
      "I1113 02:07:44.218036  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:48026d4012d5d2122b9e5f367958091460957b5e663a1e87573333a3e84e4d6d\n",
      "I1113 02:07:44.218040  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):be1a781d7df97912a673fe3dd504335d63cf8d2ce663b383bd42bfe21cfe073f\n",
      "I1113 02:07:44.218042  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.218117  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.416704ms\n",
      "I1113 02:07:44.233733  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663237017 bytes.\n",
      "I1113 02:07:44.233749  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161850 bytes.\n",
      "I1113 02:07:44.233862  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.06536625ms\n",
      "I1113 02:07:44.234148  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.239496  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.39831225ms\n",
      "I1113 02:07:44.245762  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.23536ms\n",
      "I1113 02:07:44.245849  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:28dc6e32f760358a2f5b441f5c9afa012c9f9b6c26cb66c54f3be6a599da3e18\n",
      "I1113 02:07:44.245853  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):a88d004cdfdf9c929882e26986008d6e4c652320e3aa8d23c7f17b63a3277f22\n",
      "I1113 02:07:44.245855  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.245955  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.187444ms\n",
      "I1113 02:07:44.261433  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 02:07:44.261448  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 02:07:44.261559  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.0176315ms\n",
      "I1113 02:07:44.261853  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.265580  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.7802125ms\n",
      "I1113 02:07:44.271907  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.297534ms\n",
      "I1113 02:07:44.271988  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:4ef068d35e80b2e033add7759c098be8cf72b04e78bb8548808e8ebd3e4e8622\n",
      "I1113 02:07:44.271993  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):d047164d75dfc1dcc351397c19f3f5aa95deb1bb9519a4ac877bf5500b07fdc8\n",
      "I1113 02:07:44.271995  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.272072  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.562065ms\n",
      "I1113 02:07:44.287411  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663233945 bytes.\n",
      "I1113 02:07:44.287427  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161697 bytes.\n",
      "I1113 02:07:44.287569  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.07736175ms\n",
      "I1113 02:07:44.287845  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.292454  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.64836475ms\n",
      "I1113 02:07:44.298737  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.253397ms\n",
      "I1113 02:07:44.298824  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:c61e0273795e22aff2e61154c8ddd754db0e38b9eab82c9941a71a55a05134f8\n",
      "I1113 02:07:44.298828  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):fa2236215bed88d7b9721ab7e2025530211a2ad392e63626bf4030552e61ec48\n",
      "I1113 02:07:44.298830  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.298911  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.4499925ms\n",
      "I1113 02:07:44.314502  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 02:07:44.314517  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 02:07:44.314631  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.9931145ms\n",
      "I1113 02:07:44.314928  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.318676  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.791594ms\n",
      "I1113 02:07:44.324842  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.13689175ms\n",
      "I1113 02:07:44.324929  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:f1544e8c1fb68fd467ed660e22e249aeece16ea525dd5af09e305f26c95d2f76\n",
      "I1113 02:07:44.324933  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):414380cf261e4d585c25b9331ec8ede941ab7f71e4a6baa0b0dc22588b60ffa1\n",
      "I1113 02:07:44.324935  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.325012  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.404956ms\n",
      "I1113 02:07:44.341152  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663122329 bytes.\n",
      "I1113 02:07:44.341171  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883156116 bytes.\n",
      "I1113 02:07:44.341371  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.567105ms\n",
      "I1113 02:07:44.341675  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.349058  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 7.4290665ms\n",
      "I1113 02:07:44.355601  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.48061575ms\n",
      "I1113 02:07:44.355700  478783 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:2b5beae33971884e11365a65b0df96d93c9f282836ecd632b0910b0514923d03\n",
      "I1113 02:07:44.355705  478783 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):50b4121e50d8a39a31e63d3d51087be900e11e3cff92c19560b9302821c5c9d2\n",
      "I1113 02:07:44.355707  478783 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.355790  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 18.0211755ms\n",
      "I1113 02:07:44.371859  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 02:07:44.371878  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 02:07:44.371995  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.04406775ms\n",
      "I1113 02:07:44.372268  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.378229  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.01087925ms\n",
      "I1113 02:07:44.384699  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.432975ms\n",
      "I1113 02:07:44.384785  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:4c6eeef9e2b841a73fca00c9dce95cf6f080326ddbeaa0ee8a5c6f88cf5cefaa\n",
      "I1113 02:07:44.384789  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):06dc08928344621928fab216265b49ac6d8628351fa7db9fb383046afc29ca9d\n",
      "I1113 02:07:44.384792  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.384876  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.9674295ms\n",
      "I1113 02:07:44.402630  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663122329 bytes.\n",
      "I1113 02:07:44.402645  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883156116 bytes.\n",
      "I1113 02:07:44.402879  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.68404425ms\n",
      "I1113 02:07:44.403222  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.409713  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.5447965ms\n",
      "I1113 02:07:44.416246  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.49813575ms\n",
      "I1113 02:07:44.416341  478783 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:3f993349989336765464972db6f2e86c714ef7e8b57143aa814fcb8f920da975\n",
      "I1113 02:07:44.416345  478783 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):1ffa025bd48190bb8a79e303cb1e8b54c0c520b5488306cbca057d7051768344\n",
      "I1113 02:07:44.416347  478783 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.416420  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.28924625ms\n",
      "I1113 02:07:44.435023  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97616059289 bytes.\n",
      "I1113 02:07:44.435039  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4880802964 bytes.\n",
      "I1113 02:07:44.435178  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.09003225ms\n",
      "I1113 02:07:44.435469  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.449059  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 13.640565ms\n",
      "I1113 02:07:44.455835  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.74505575ms\n",
      "I1113 02:07:44.455941  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:c2323f7624e6a4c72d12ec85203491fb7e1e1fb0d447274c7a9980b201fe2528\n",
      "I1113 02:07:44.455945  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):f47c59589b6be179b62e2396607a7ab1535818c9ef58e1db9648b9ff31af8c57\n",
      "I1113 02:07:44.455947  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.456037  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 23.98194125ms\n",
      "I1113 02:07:44.471974  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639652249 bytes.\n",
      "I1113 02:07:44.471989  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881982612 bytes.\n",
      "I1113 02:07:44.472120  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.017281ms\n",
      "I1113 02:07:44.472429  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.518554  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 46.18621675ms\n",
      "I1113 02:07:44.526494  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.9064405ms\n",
      "I1113 02:07:44.526600  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:4e464aba26b0d6307ab048d9a85ddf000be8e75cc5d2caf3e397af0b8dfe9a68\n",
      "I1113 02:07:44.526604  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):2aed48c8df49d2215e3dceb8146a3e44631ae66de0e61e5e88d7710da076feb6\n",
      "I1113 02:07:44.526606  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.526684  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 57.60610375ms\n",
      "I1113 02:07:44.542226  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639642009 bytes.\n",
      "I1113 02:07:44.542242  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881982100 bytes.\n",
      "I1113 02:07:44.542381  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.156987ms\n",
      "I1113 02:07:44.542667  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.589765  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 47.1483185ms\n",
      "I1113 02:07:44.597911  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.11459175ms\n",
      "I1113 02:07:44.598019  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:119d30f0866fdd213428108db8264e175a498b187efe94835bf59e79d2f41f53\n",
      "I1113 02:07:44.598022  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2215c3920519e491819185135fc53d7bc85d4ce0bd50a45e8dca3b2c3bb8db94\n",
      "I1113 02:07:44.598024  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.598102  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 58.9114005ms\n",
      "I1113 02:07:44.613611  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639642009 bytes.\n",
      "I1113 02:07:44.613627  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881982100 bytes.\n",
      "I1113 02:07:44.613763  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.2060575ms\n",
      "I1113 02:07:44.614057  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.660501  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 46.4933775ms\n",
      "I1113 02:07:44.668652  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.121028ms\n",
      "I1113 02:07:44.668760  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:119d30f0866fdd213428108db8264e175a498b187efe94835bf59e79d2f41f53\n",
      "I1113 02:07:44.668764  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2215c3920519e491819185135fc53d7bc85d4ce0bd50a45e8dca3b2c3bb8db94\n",
      "I1113 02:07:44.668766  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.668841  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 58.314284ms\n",
      "I1113 02:07:44.686645  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639539609 bytes.\n",
      "I1113 02:07:44.686660  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881976980 bytes.\n",
      "I1113 02:07:44.686907  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.76082875ms\n",
      "I1113 02:07:44.687254  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.767946  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 80.74233425ms\n",
      "I1113 02:07:44.777617  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.638565ms\n",
      "I1113 02:07:44.777751  478783 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:0df6078050e3ded44bfdd701a62d20ab164c5626d55ac14b26ebb497e1b1c2cf\n",
      "I1113 02:07:44.777755  478783 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):08473352e61faa66a5a90f591a990391af3fbd7ad113c58b8dff0c58d4ef628f\n",
      "I1113 02:07:44.777756  478783 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.777836  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 94.7330675ms\n",
      "I1113 02:07:44.794877  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663232921 bytes.\n",
      "I1113 02:07:44.794892  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161646 bytes.\n",
      "I1113 02:07:44.795013  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1083455ms\n",
      "I1113 02:07:44.795311  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.799046  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.783782ms\n",
      "I1113 02:07:44.805269  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.1936445ms\n",
      "I1113 02:07:44.805352  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:a060efa22e5c5844e61fd4dfc7ec7c41341484484d11e0d8880b79ed9e55305f\n",
      "I1113 02:07:44.805355  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):5fa5df802b6a203886959eadc2d310d9c6ee6e93759e4c5c8aa5487a91a34a4e\n",
      "I1113 02:07:44.805357  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.805431  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.5570485ms\n",
      "I1113 02:07:44.820930  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663239065 bytes.\n",
      "I1113 02:07:44.820946  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161953 bytes.\n",
      "I1113 02:07:44.821076  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.948733ms\n",
      "I1113 02:07:44.821354  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.825121  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.8121495ms\n",
      "I1113 02:07:44.831345  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.19509625ms\n",
      "I1113 02:07:44.831427  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:7833fc97f1e913958e6f6f567f42cbf7f7f42b2d70d349f0609cfa73490154a7\n",
      "I1113 02:07:44.831431  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):78125be834049c20907f5c87a2e3261275fa5bd6454ba95900533555788da7e2\n",
      "I1113 02:07:44.831433  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.831508  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.40674775ms\n",
      "I1113 02:07:44.846936  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663239065 bytes.\n",
      "I1113 02:07:44.846951  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161953 bytes.\n",
      "I1113 02:07:44.847063  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.98342025ms\n",
      "I1113 02:07:44.847368  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.853424  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.1025815ms\n",
      "I1113 02:07:44.859833  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.38026075ms\n",
      "I1113 02:07:44.859937  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:748f23e329d68bd81b522d2f76e34d3f052263462207b73b06bc8e76e502a842\n",
      "I1113 02:07:44.859941  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):7a6dcfd917b6e26f88a9b730b1d18ff97e6a23fac81ca936e674f0429de19d02\n",
      "I1113 02:07:44.859942  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.860019  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.9689265ms\n",
      "I1113 02:07:44.875604  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663192473 bytes.\n",
      "I1113 02:07:44.875619  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883159623 bytes.\n",
      "I1113 02:07:44.875788  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.39013375ms\n",
      "I1113 02:07:44.876089  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:44.884805  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 8.76506825ms\n",
      "I1113 02:07:44.891446  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.610718ms\n",
      "I1113 02:07:44.891542  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:4dfc6c15fd1934b65f1dda9c62b27b2011a8c0b71be6a1cf96dbbe6343e044b1\n",
      "I1113 02:07:44.891546  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):1236b371dce705646644e1ada82a9bf475545dc91996f0b1c6d317391bba4e45\n",
      "I1113 02:07:44.891548  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:44.891622  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 19.25461875ms\n",
      "I1113 02:07:44.977652  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 95539878809 bytes.\n",
      "I1113 02:07:44.977691  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4776993940 bytes.\n",
      "I1113 02:07:44.977886  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.59467375ms\n",
      "I1113 02:07:44.978481  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.034805  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 56.435678ms\n",
      "I1113 02:07:45.042559  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.679425ms\n",
      "I1113 02:07:45.042680  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:b3ba6325a053ecbb6f9addbc4449554c9cf58e2ff7a22dbb2dca310c38b21420\n",
      "I1113 02:07:45.042684  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):24f89d50687e0e6689ba857e5672f04955a820bf9d7ba9210b477b0ee4185088\n",
      "I1113 02:07:45.042686  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.042810  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 70.5779935ms\n",
      "I1113 02:07:45.178422  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577969049 bytes.\n",
      "I1113 02:07:45.178443  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828898452 bytes.\n",
      "I1113 02:07:45.178590  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.8249755ms\n",
      "I1113 02:07:45.178970  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.219849  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 40.9606585ms\n",
      "I1113 02:07:45.227298  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.40896575ms\n",
      "I1113 02:07:45.227395  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:10ec1d0e39903987b226b392e3ae0a6b8326e701ac61b21ab79063ec858f5ac4\n",
      "I1113 02:07:45.227399  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):16e939837fc9dcac1cac38cf27f8d8b496076876605f0b309082e6a19a4f423e\n",
      "I1113 02:07:45.227401  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.227500  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 52.780845ms\n",
      "I1113 02:07:45.246671  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662507929 bytes.\n",
      "I1113 02:07:45.246686  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883125396 bytes.\n",
      "I1113 02:07:45.246801  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.17832475ms\n",
      "I1113 02:07:45.247181  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.254721  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 7.6195235ms\n",
      "I1113 02:07:45.261294  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.53648675ms\n",
      "I1113 02:07:45.261379  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:661b06f4f697007005242bb8e0c92a014b6271a707633661de22b6b79703c9e9\n",
      "I1113 02:07:45.261383  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):f1c228e24b6e0981d545569cf87e2530d5e5bf65819bb00da0d09d1393c633c7\n",
      "I1113 02:07:45.261385  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.261464  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.87383225ms\n",
      "I1113 02:07:45.277752  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662868377 bytes.\n",
      "I1113 02:07:45.277774  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143418 bytes.\n",
      "I1113 02:07:45.277888  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.08232475ms\n",
      "I1113 02:07:45.278202  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.283877  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.7338555ms\n",
      "I1113 02:07:45.290193  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.262337ms\n",
      "I1113 02:07:45.290277  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:9be621fdff47b4326661798b415522ffb365d75067437e9bfa2eebcf11bfe103\n",
      "I1113 02:07:45.290281  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):ed0f8738fdcede926890b9ab0a3b476a8f6aeb3aad2c6d889944505f3b7046e6\n",
      "I1113 02:07:45.290283  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.290353  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.5779855ms\n",
      "I1113 02:07:45.306879  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663255449 bytes.\n",
      "I1113 02:07:45.306894  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162772 bytes.\n",
      "I1113 02:07:45.306996  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.89273575ms\n",
      "I1113 02:07:45.307320  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.310960  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.69006425ms\n",
      "I1113 02:07:45.317174  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.1843615ms\n",
      "I1113 02:07:45.317252  478783 isa_program_util_common.cc:510] (HLO module jit_iota): Executable fingerprint:57280a4beddf7678a596a27f40caa733a67b820f9649b94003f3c9518fc87ba9\n",
      "I1113 02:07:45.317256  478783 isa_program_util_common.cc:514] (HLO module jit_iota): Executable fingerprint (including data segments):262569d1fe4ffd105c8c2d1bf625e744c2ab8f6a95289cb0b9894d7c9e56f41b\n",
      "I1113 02:07:45.317258  478783 isa_program_util_common.cc:517] (HLO module jit_iota): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.317333  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.2566085ms\n",
      "I1113 02:07:45.333574  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663170969 bytes.\n",
      "I1113 02:07:45.333592  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883158548 bytes.\n",
      "I1113 02:07:45.333765  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.51877025ms\n",
      "I1113 02:07:45.334086  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.338817  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.78016775ms\n",
      "I1113 02:07:45.345123  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.272009ms\n",
      "I1113 02:07:45.345211  478783 isa_program_util_common.cc:510] (HLO module jit_multiply): Executable fingerprint:190754de3fd259b600c4db676b2c114e15ab6eb5ea56043b52cdc120001bdf66\n",
      "I1113 02:07:45.345215  478783 isa_program_util_common.cc:514] (HLO module jit_multiply): Executable fingerprint (including data segments):1c24fc1e1a6b38bced32a9c341eccfc774976694799298676a3480dee2e87fa7\n",
      "I1113 02:07:45.345217  478783 isa_program_util_common.cc:517] (HLO module jit_multiply): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.345291  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.091575ms\n",
      "I1113 02:07:45.361605  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663170969 bytes.\n",
      "I1113 02:07:45.361624  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883158548 bytes.\n",
      "I1113 02:07:45.361797  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.4160715ms\n",
      "I1113 02:07:45.362138  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.366800  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.70866025ms\n",
      "I1113 02:07:45.373225  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.39547575ms\n",
      "I1113 02:07:45.373316  478783 isa_program_util_common.cc:510] (HLO module jit_add): Executable fingerprint:17408244a2de47c6d73e33643c8125edbb692a7cb8acf62010dc670a27dbc0d1\n",
      "I1113 02:07:45.373320  478783 isa_program_util_common.cc:514] (HLO module jit_add): Executable fingerprint (including data segments):2c52dbfbae6f7ca42a13d24049ab242eaca1fc0030abcd52b5e7d2d3c6f960e5\n",
      "I1113 02:07:45.373322  478783 isa_program_util_common.cc:517] (HLO module jit_add): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.373396  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.051095ms\n",
      "I1113 02:07:45.389392  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663232921 bytes.\n",
      "I1113 02:07:45.389410  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161646 bytes.\n",
      "I1113 02:07:45.389522  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.07603525ms\n",
      "I1113 02:07:45.389810  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.394980  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.21659525ms\n",
      "I1113 02:07:45.401293  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.27972675ms\n",
      "I1113 02:07:45.401376  478783 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:9227d72cd9baf1554241ff5d8296facec42fc4e1d717002d69cbdf7c50be761b\n",
      "I1113 02:07:45.401380  478783 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):2883b206079e1401df8a6d073c19dcf7b8db54b3b335f10601db08c5161ea85a\n",
      "I1113 02:07:45.401382  478783 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.401454  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.05010175ms\n",
      "I1113 02:07:45.418924  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601273753 bytes.\n",
      "I1113 02:07:45.418951  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830063687 bytes.\n",
      "I1113 02:07:45.419346  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.8709115ms\n",
      "I1113 02:07:45.419717  479672 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.616996  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 197.328994ms\n",
      "I1113 02:07:45.634602  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 17.57272925ms\n",
      "I1113 02:07:45.634882  478783 isa_program_util_common.cc:510] (HLO module jit_gather): Executable fingerprint:09250165b686b46e38a36cc99b32f56719aab4db2ae462fb294fc37d748bb1e0\n",
      "I1113 02:07:45.634888  478783 isa_program_util_common.cc:514] (HLO module jit_gather): Executable fingerprint (including data segments):ba7abf873dc4e628cc16da527ed3e7c4971db32707fb08aa0a80c7209c95a99b\n",
      "I1113 02:07:45.634889  478783 isa_program_util_common.cc:517] (HLO module jit_gather): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.635044  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 220.601135ms\n",
      "I1113 02:07:45.652346  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120607129 bytes.\n",
      "I1113 02:07:45.652362  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856030356 bytes.\n",
      "I1113 02:07:45.652499  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.0675145ms\n",
      "I1113 02:07:45.652792  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:45.752313  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 99.57189875ms\n",
      "I1113 02:07:45.760794  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.4506075ms\n",
      "I1113 02:07:45.760910  478783 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:6776705d872a6a419d682a20ca9f99ac30293a6ee6a8e9eafc6d7ba0d9bb1426\n",
      "I1113 02:07:45.760915  478783 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):1301a464cc50019a4037786e265539be0957c2d8bd621c8881a9e23dd30c4d3a\n",
      "I1113 02:07:45.760917  478783 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:45.760993  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 111.5918905ms\n",
      "I1113 02:07:45.781479  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577876889 bytes.\n",
      "I1113 02:07:45.781496  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828893844 bytes.\n",
      "I1113 02:07:45.781738  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.15556675ms\n",
      "I1113 02:07:45.782132  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.121632  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 339.55567775ms\n",
      "I1113 02:07:46.139586  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 17.903639ms\n",
      "I1113 02:07:46.139821  478783 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:49174e3b7b00b43f371d4a45c04fc60596b08e4c7a15ea96eb5cdd5ca33630c6\n",
      "I1113 02:07:46.139831  478783 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):9b1bc7eeff20235a16f9bfcef95c8203deadb4c7599d857281d0acb4c5fd39b0\n",
      "I1113 02:07:46.139833  478783 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.139948  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 362.4119735ms\n",
      "I1113 02:07:46.163650  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662588313 bytes.\n",
      "I1113 02:07:46.163668  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883129415 bytes.\n",
      "I1113 02:07:46.164051  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.24214625ms\n",
      "I1113 02:07:46.164481  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.217001  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 52.578747ms\n",
      "I1113 02:07:46.229781  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 12.7469905ms\n",
      "I1113 02:07:46.230032  478783 isa_program_util_common.cc:510] (HLO module jit_gather): Executable fingerprint:2c9aa7d52b976bd6e1d355e6b401a95e5542e7a604c9901931d41ba0472ef04f\n",
      "I1113 02:07:46.230037  478783 isa_program_util_common.cc:514] (HLO module jit_gather): Executable fingerprint (including data segments):c2aa889a5799b707ce674d3de95c301dc3cc36c417c79138290ab5f93685cd37\n",
      "I1113 02:07:46.230039  478783 isa_program_util_common.cc:517] (HLO module jit_gather): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.230126  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 71.36291675ms\n",
      "I1113 02:07:46.248412  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1113 02:07:46.248428  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1113 02:07:46.248564  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.171144ms\n",
      "I1113 02:07:46.248896  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.254648  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.8071355ms\n",
      "I1113 02:07:46.260914  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.236414ms\n",
      "I1113 02:07:46.260997  478783 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:36863f80d229aa82f961a43d1551de01b46350b844dad8d226cc9c5cd413204d\n",
      "I1113 02:07:46.261001  478783 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):dd347b26c2bc1b49ea22dbfc0d22da881488711b8bb2dcad31c0d6b4b3b1b2c8\n",
      "I1113 02:07:46.261003  478783 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.261080  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.7167915ms\n",
      "I1113 02:07:46.278656  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662796697 bytes.\n",
      "I1113 02:07:46.278672  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883139834 bytes.\n",
      "I1113 02:07:46.278849  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.450919ms\n",
      "I1113 02:07:46.279198  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.293068  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 13.93635675ms\n",
      "I1113 02:07:46.299879  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.77987725ms\n",
      "I1113 02:07:46.299971  478783 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:862c6c0ad8df8b43a3f58bd93a5f7f096e7e67db184184e45557cb91ad099dd1\n",
      "I1113 02:07:46.299975  478783 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):ed5eb524be4a1623e28a210a74740fb316c3cd58bde02430e600c129dd72919e\n",
      "I1113 02:07:46.299977  478783 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.300055  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 24.69092ms\n",
      "I1113 02:07:46.316443  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601551769 bytes.\n",
      "I1113 02:07:46.316459  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830077588 bytes.\n",
      "I1113 02:07:46.316584  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.2261805ms\n",
      "I1113 02:07:46.316881  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.375638  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 58.803762ms\n",
      "I1113 02:07:46.384387  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.71545275ms\n",
      "I1113 02:07:46.384510  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:678ce0c95e46b45f32a99aad9554b5dae6a6acb5945f6b11b73c47a3aa45f324\n",
      "I1113 02:07:46.384514  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):dd4f0f5ba99c0d52e0631b0719e2d74b90f94f42ec3ecc50f6d2360e7a0078f1\n",
      "I1113 02:07:46.384516  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.384595  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 71.26893825ms\n",
      "I1113 02:07:46.400848  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120596889 bytes.\n",
      "I1113 02:07:46.400864  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856029844 bytes.\n",
      "I1113 02:07:46.401011  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.2536345ms\n",
      "I1113 02:07:46.401325  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.551014  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 149.74040725ms\n",
      "I1113 02:07:46.560146  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.0816685ms\n",
      "I1113 02:07:46.560271  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:7c6411dec771d952b15c6969ad4368c559b7dd92eab7ede3682b368c3c5d81d8\n",
      "I1113 02:07:46.560275  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):5820e82e0ee3f5519b7c218f4f9ad4212128d3921661e3cd92d51ee63e1fd61c\n",
      "I1113 02:07:46.560277  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.560391  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 162.665603ms\n",
      "I1113 02:07:46.580839  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601551769 bytes.\n",
      "I1113 02:07:46.580856  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830077588 bytes.\n",
      "I1113 02:07:46.581020  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.741579ms\n",
      "I1113 02:07:46.581355  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.645131  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 63.830169ms\n",
      "I1113 02:07:46.654287  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.12090325ms\n",
      "I1113 02:07:46.654417  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:364c7e6e4b7d3d7b86773283e7f4a3268806d3011042d377be7a9430b204c4f6\n",
      "I1113 02:07:46.654421  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):5a39aa896b46ab09b63e510471eb6f4e729ec58d6095a0c7cd9abdbbb6be8a6a\n",
      "I1113 02:07:46.654423  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.654509  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 77.274747ms\n",
      "I1113 02:07:46.672467  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577897369 bytes.\n",
      "I1113 02:07:46.672483  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828894868 bytes.\n",
      "I1113 02:07:46.672672  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.6413405ms\n",
      "I1113 02:07:46.673017  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.892882  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 219.91371325ms\n",
      "I1113 02:07:46.903831  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 10.915049ms\n",
      "I1113 02:07:46.903967  478783 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:f3bcc3bc248fe1f4c10f9ff7416cb2afaaf95bd09a2376ad65ddb3d25df63d29\n",
      "I1113 02:07:46.903972  478783 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):5f5119da77ec1401e59b41fdb1a440d356002503b570cde3c32db5966cfd4a9f\n",
      "I1113 02:07:46.903974  478783 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:46.904051  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 235.05682825ms\n",
      "I1113 02:07:46.921085  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577958809 bytes.\n",
      "I1113 02:07:46.921101  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828897940 bytes.\n",
      "I1113 02:07:46.921228  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.50497825ms\n",
      "I1113 02:07:46.921529  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:46.991617  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 70.13724825ms\n",
      "I1113 02:07:46.999825  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.17671775ms\n",
      "I1113 02:07:46.999962  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:370bf6a664e975e1f371e910fdd416e0e8d28da07cc37c5501c9be4574f3052f\n",
      "I1113 02:07:46.999966  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2857ad7ab3cd3b5b2d0d0237fd1602baaac8aec9dbde47bf28d0883d6f308226\n",
      "I1113 02:07:46.999968  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.000044  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 82.35234975ms\n",
      "W1113 02:07:47.014905  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:47.016012  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:47.019899  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97391926169 bytes.\n",
      "I1113 02:07:47.019915  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4869596308 bytes.\n",
      "I1113 02:07:47.020052  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.68602675ms\n",
      "I1113 02:07:47.020391  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.104544  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 84.19754725ms\n",
      "I1113 02:07:47.113808  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.23073375ms\n",
      "I1113 02:07:47.113950  478783 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:4ec9574a3186f0ffb26f288cc5af3621746ad8c1d4991c67b4bdb87b270e0864\n",
      "I1113 02:07:47.113954  478783 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):866d8ffa0dcb20f9fbb98cd3fbfa331dea30e2d3bf56faea2f4301d0c4f92bac\n",
      "I1113 02:07:47.113956  478783 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.114032  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 100.70930225ms\n",
      "I1113 02:07:47.132934  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601439129 bytes.\n",
      "I1113 02:07:47.132951  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830071956 bytes.\n",
      "I1113 02:07:47.133205  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.0066415ms\n",
      "I1113 02:07:47.133550  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.277135  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 143.64363475ms\n",
      "I1113 02:07:47.289038  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.87088125ms\n",
      "I1113 02:07:47.289213  478783 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:20eead6a6510050c6a06c55122b8125f131bb038ab560287921e1408b4cfd7d9\n",
      "I1113 02:07:47.289217  478783 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):04755dd2a83101e584b1b12e15ff4349abacea6104b328fa99bbddd5f8c7afdf\n",
      "I1113 02:07:47.289219  478783 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.289297  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 160.14460575ms\n",
      "W1113 02:07:47.306741  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:47.307825  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:47.311575  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97527585689 bytes.\n",
      "I1113 02:07:47.311591  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4876379284 bytes.\n",
      "I1113 02:07:47.311708  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.5355245ms\n",
      "I1113 02:07:47.312021  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.365818  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 53.84316225ms\n",
      "I1113 02:07:47.374800  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.94788325ms\n",
      "I1113 02:07:47.374955  478783 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:15c2192ba3aab672e2441fcdcb5dd0fd27e9db3557c987a18c3fc1ddad3f4c7f\n",
      "I1113 02:07:47.374960  478783 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):9d2205b41ad0dfa013595d4516038ea0438d82d20afa73629bebb193bacb8121\n",
      "I1113 02:07:47.374962  478783 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.375039  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 69.906986ms\n",
      "I1113 02:07:47.393818  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120484249 bytes.\n",
      "I1113 02:07:47.393834  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856024212 bytes.\n",
      "I1113 02:07:47.394084  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.98662375ms\n",
      "I1113 02:07:47.394431  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.514769  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 120.38867675ms\n",
      "I1113 02:07:47.526463  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.66085ms\n",
      "I1113 02:07:47.526634  478783 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:0b8c4880d23345669941a617135b96ba768a456ede951744195f175a759afdb0\n",
      "I1113 02:07:47.526638  478783 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):268fe7ac84d0ca5d34860663e970705125da83ee54688b8709aa7c3fdff9e183\n",
      "I1113 02:07:47.526641  478783 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.526719  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 136.6643885ms\n",
      "I1113 02:07:47.545830  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662876569 bytes.\n",
      "I1113 02:07:47.545846  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143828 bytes.\n",
      "I1113 02:07:47.545962  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.19163825ms\n",
      "I1113 02:07:47.546277  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.551484  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.25535675ms\n",
      "I1113 02:07:47.557683  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.17026675ms\n",
      "I1113 02:07:47.557769  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:40f0cccc83b28e24379e13c409b003ccad4ccc778086a097887634a9ee5403dd\n",
      "I1113 02:07:47.557773  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):ee1a3428c3f31f125fa99f61c620127bcb3685b1b77f124b4e32b507ade2f66d\n",
      "I1113 02:07:47.557775  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.557849  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.11332175ms\n",
      "I1113 02:07:47.573724  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1113 02:07:47.573740  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1113 02:07:47.573853  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.10077075ms\n",
      "I1113 02:07:47.574156  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.635171  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 61.064045ms\n",
      "I1113 02:07:47.646401  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.19469ms\n",
      "I1113 02:07:47.646543  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:0dde0b84570d4f8b311fbcbd7ee430f7919a3793377646a02f6711db334be04a\n",
      "I1113 02:07:47.646547  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):9a0eb996550b3c697d53556710a42baa13eea2d2f3345cf7f3b7507136739d51\n",
      "I1113 02:07:47.646549  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.646625  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 75.90724425ms\n",
      "I1113 02:07:47.663057  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662876569 bytes.\n",
      "I1113 02:07:47.663072  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143828 bytes.\n",
      "I1113 02:07:47.663214  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.17058925ms\n",
      "I1113 02:07:47.663533  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.675097  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 11.617868ms\n",
      "I1113 02:07:47.681641  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.462395ms\n",
      "I1113 02:07:47.681727  478783 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:fe2efdcb3d52d0c9503a29b1fd9d8498cc7667239778b75c81aad27049feb87e\n",
      "I1113 02:07:47.681731  478783 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):920a107875ed240ea0640763eaf114f4317f940c6737c362f05502e10852b963\n",
      "I1113 02:07:47.681732  478783 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.681808  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 21.79455225ms\n",
      "I1113 02:07:47.698099  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662780313 bytes.\n",
      "I1113 02:07:47.698114  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883139015 bytes.\n",
      "I1113 02:07:47.698297  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.43431975ms\n",
      "I1113 02:07:47.698617  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.736580  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 38.00821875ms\n",
      "I1113 02:07:47.745013  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.4009645ms\n",
      "I1113 02:07:47.745118  478783 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:b24c7be130ac8e12437a95a6cd7ecef069e71d4550e553c3b4b8122e2c8964de\n",
      "I1113 02:07:47.745122  478783 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):762f9655e81f3a87d7f24e295dea3151ad562ea2dc123a7bf39dcd65ced3fa04\n",
      "I1113 02:07:47.745124  478783 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.745201  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.386524ms\n",
      "I1113 02:07:47.761385  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662851993 bytes.\n",
      "I1113 02:07:47.761400  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883142599 bytes.\n",
      "I1113 02:07:47.761512  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.08720175ms\n",
      "I1113 02:07:47.761806  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.946331  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 184.5733005ms\n",
      "I1113 02:07:47.963272  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 16.9084705ms\n",
      "I1113 02:07:47.963443  478783 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:c10c422fe9795af2a983e3fbe2b1ecc9ca4f388bf744c3ef546de1d2c94bb1d7\n",
      "I1113 02:07:47.963447  478783 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):1bc38b4ce2c53b8b9cc6361d15490201ae59dd6487e03a820d4ed55c7c99a084\n",
      "I1113 02:07:47.963449  478783 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.963527  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 205.1328805ms\n",
      "W1113 02:07:47.978447  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:47.979468  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:47.983039  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663146905 bytes.\n",
      "I1113 02:07:47.983054  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883157345 bytes.\n",
      "I1113 02:07:47.983210  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.24878ms\n",
      "I1113 02:07:47.983526  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:47.992724  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.24154925ms\n",
      "I1113 02:07:47.999678  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.92566025ms\n",
      "I1113 02:07:47.999785  478783 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:aaf05ebded97981a3587ddd4f9bbc84f9643aeaf3c797f818835483d598e1dc1\n",
      "I1113 02:07:47.999788  478783 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):2d51af4641d8d01a339154a7f5c2762e2c452cbecf4787fa69aa7b718d1c2faa\n",
      "I1113 02:07:47.999791  478783 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:47.999869  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 22.94873125ms\n",
      "I1113 02:07:48.018418  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662763929 bytes.\n",
      "I1113 02:07:48.018434  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883138196 bytes.\n",
      "I1113 02:07:48.018672  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.692317ms\n",
      "I1113 02:07:48.018996  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:48.034522  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 15.5753655ms\n",
      "I1113 02:07:48.041830  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.27965375ms\n",
      "I1113 02:07:48.041934  478783 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:8685cfc07fa56f2461e517612b8269a1d1a3d31761d46a417de0fd787c225930\n",
      "I1113 02:07:48.041938  478783 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):fd21cb59c09ca83a49ec9b16477e055f65a0dfb690d750753d3800d0814646f5\n",
      "I1113 02:07:48.041940  478783 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:48.042014  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 27.08172975ms\n",
      "W1113 02:07:48.058406  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:48.059353  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:48.062939  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1113 02:07:48.062954  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1113 02:07:48.063081  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.13626375ms\n",
      "I1113 02:07:48.063435  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:48.072603  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.2134625ms\n",
      "I1113 02:07:48.079715  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.0826285ms\n",
      "I1113 02:07:48.079823  478783 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:95fc20b3d91139158e400b9720ffa468a4909097f9a8e569f181ab7417efc621\n",
      "I1113 02:07:48.079827  478783 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):992271494bf12f27ce7deca100ad97747b4b2eaa47779baf466c5bbf7a133266\n",
      "I1113 02:07:48.079829  478783 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:48.079908  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 23.0075935ms\n",
      "I1113 02:07:53.756885  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663244697 bytes.\n",
      "I1113 02:07:53.756913  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162234 bytes.\n",
      "I1113 02:07:53.757063  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.99046075ms\n",
      "I1113 02:07:53.757520  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:53.762033  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.5955905ms\n",
      "I1113 02:07:53.769184  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.11182925ms\n",
      "I1113 02:07:53.769274  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:7075cbfbd75beffcf249e571e6a4bddb8379e01aac94c4d7c47a7f9ca1704518\n",
      "I1113 02:07:53.769278  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):b8441eb9ca29849d647247aaa9c78c1a597aabdd6aa0aa919b29de261bbda7f2\n",
      "I1113 02:07:53.769280  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:53.769414  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 16.3905185ms\n",
      "I1113 02:07:53.786313  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663244697 bytes.\n",
      "I1113 02:07:53.786329  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162234 bytes.\n",
      "I1113 02:07:53.786444  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.076024ms\n",
      "I1113 02:07:53.786740  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:53.790664  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.9651295ms\n",
      "I1113 02:07:53.797126  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.4310245ms\n",
      "I1113 02:07:53.797207  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:375f551ac122c4c1443de2d6757a8e6ee695c8bd10bcc876cb4acb15627a65ca\n",
      "I1113 02:07:53.797211  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):4c8eb4dd58c79142733686c9b511662531ea17d536c2a918c926dac69fcf2fbe\n",
      "I1113 02:07:53.797213  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:53.797289  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.95445175ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 02:07:54 [tpu_runner.py:497] Init model | hbm=[(10.11, 95.74), (10.11, 95.74), (10.11, 95.74), (10.11, 95.74)]GiB\n",
      "INFO 11-13 02:07:54 [tpu_worker.py:174] Memory statistics | total_hbm_limit_gb=382.97GiB | total_hbm_limit_cap_gb=344.68GiB | total_hbm_used_gb=40.43GiB | total_hbm_avail_gb=304.24GiB\n",
      "WARNING 11-13 02:07:54 [kv_cache_utils.py:1095] Hybrid KV cache manager is disabled for this hybrid model, This means we do not enable any optimizations for saving KV cache memory (e.g., dropping the KV cache outside the sliding window). The compute of layers like sliding window is still saved.\n",
      "INFO 11-13 02:07:54 [kv_cache_utils.py:1229] GPU KV cache size: 6,646,224 tokens\n",
      "INFO 11-13 02:07:54 [kv_cache_utils.py:1234] Maximum concurrency for 128 tokens per request: 51923.62x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 02:07:54.305650  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1113 02:07:54.305728  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1113 02:07:54.305894  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.9869695ms\n",
      "I1113 02:07:54.306322  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.311584  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.33923525ms\n",
      "I1113 02:07:54.319056  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.434647ms\n",
      "I1113 02:07:54.319206  478783 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:13d169e8f0cb52132827f3458861645b2ad7cf1a3afcb1127a6444697d9ec7eb\n",
      "I1113 02:07:54.319211  478783 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):7274c5d1d7ca793892a3a02db907049e506cf3ed81768774f478da0a45758e78\n",
      "I1113 02:07:54.319213  478783 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.319339  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.4757715ms\n",
      "I1113 02:07:54.389599  467351 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 166 instructions, modules: jit__threefry_fold_in\n",
      "I1113 02:07:54.389614  467351 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__threefry_fold_in instructions: 166 fingerprint: \n",
      "I1113 02:07:54.390273  467351 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__threefry_fold_in instructions: 153\n",
      "I1113 02:07:54.390279  467351 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1113 02:07:54.401556  467351 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1113 02:07:54.403652  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97661667225 bytes.\n",
      "I1113 02:07:54.403665  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883083361 bytes.\n",
      "I1113 02:07:54.404909  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 16.515629ms\n",
      "I1113 02:07:54.405564  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.424695  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 19.1818705ms\n",
      "I1113 02:07:54.427136  478783 2a886c8_compiler_base.cc:3045] final program bundle count: 633 note this count does not reflect cycles spent executing delays.\n",
      "I1113 02:07:54.432068  478783 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1113 02:07:54.433607  478783 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (76.5K).\n",
      "I1113 02:07:54.433873  478783 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__threefry_fold_in\n",
      "I1113 02:07:54.433879  478783 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 156.5K / 95.74G\n",
      "I1113 02:07:54.433883  478783 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 4.5K / 64.00M\n",
      "I1113 02:07:54.433893  478783 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.16M:\n",
      "I1113 02:07:54.433895  478783 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1113 02:07:54.433896  478783 2a886c8_compiler_base.cc:3549]     program          156.5K \n",
      "I1113 02:07:54.433897  478783 2a886c8_compiler_base.cc:3549]     arguments          1.0K \n",
      "I1113 02:07:54.433899  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:54.433900  478783 2a886c8_compiler_base.cc:3549] Output size 512B; shares 0B with arguments.\n",
      "I1113 02:07:54.433902  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:54.433903  478783 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1113 02:07:54.433904  478783 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1113 02:07:54.433905  478783 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1113 02:07:54.433907  478783 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1113 02:07:54.433908  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:54.433909  478783 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1113 02:07:54.433910  478783 2a886c8_compiler_base.cc:3549] \n",
      "I1113 02:07:54.433922  478783 2a886c8_compiler_base.cc:3553] Program sflag requirement 220B:\n",
      "I1113 02:07:54.433923  478783 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1113 02:07:54.433924  478783 2a886c8_compiler_base.cc:3553]     scoped              12B\n",
      "I1113 02:07:54.433925  478783 2a886c8_compiler_base.cc:3553]     HLO temp             4B (100.0% utilization: Unpadded (4B) Padded (4B), 0.0% fragmentation (0B))\n",
      "I1113 02:07:54.433926  478783 2a886c8_compiler_base.cc:3553] Program hbm requirement 156.5K:\n",
      "I1113 02:07:54.433927  478783 2a886c8_compiler_base.cc:3553]     HLO temp          80.0K (4.8% utilization: Unpadded (76B) Padded (1.6K), 98.0% fragmentation (78.4K))\n",
      "I1113 02:07:54.433929  478783 2a886c8_compiler_base.cc:3553]     overlays          76.5K\n",
      "I1113 02:07:54.433930  478783 2a886c8_compiler_base.cc:3553] Program vmem requirement 4.5K:\n",
      "I1113 02:07:54.433931  478783 2a886c8_compiler_base.cc:3553]     scoped             4.0K\n",
      "I1113 02:07:54.433932  478783 2a886c8_compiler_base.cc:3553]     HLO temp           512B (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (512B))\n",
      "I1113 02:07:54.433933  478783 2a886c8_compiler_base.cc:3553] Program smem requirement 40B:\n",
      "I1113 02:07:54.433934  478783 2a886c8_compiler_base.cc:3553]     scoped              40B\n",
      "I1113 02:07:54.433935  478783 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1113 02:07:54.433936  478783 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 1.9K / 1.00M (2 parameters)\n",
      "I1113 02:07:54.433949  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.220401ms\n",
      "I1113 02:07:54.434124  478783 isa_program_util_common.cc:510] (HLO module jit__threefry_fold_in): Executable fingerprint:9bb788d9827517b4bb3d64a21746b99ef4adaed7811785123bfd9d2b576d9501\n",
      "I1113 02:07:54.434127  478783 isa_program_util_common.cc:514] (HLO module jit__threefry_fold_in): Executable fingerprint (including data segments):5a622cfb5e76d5a9edfea31742dad4d10b548b95d9f0f1a078ff626a5ee2b500\n",
      "I1113 02:07:54.434128  478783 isa_program_util_common.cc:517] (HLO module jit__threefry_fold_in): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.434207  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.09300325ms\n",
      "I1113 02:07:54.454519  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663203225 bytes.\n",
      "I1113 02:07:54.454534  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883160161 bytes.\n",
      "I1113 02:07:54.454678  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.16341825ms\n",
      "I1113 02:07:54.455037  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.460112  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.122616ms\n",
      "I1113 02:07:54.466806  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.66375575ms\n",
      "I1113 02:07:54.466898  478783 isa_program_util_common.cc:510] (HLO module jit_add): Executable fingerprint:1d37c80f028627082acffcad0f66d83660c0f7d233cd5625ea0f63148a6071a2\n",
      "I1113 02:07:54.466903  478783 isa_program_util_common.cc:514] (HLO module jit_add): Executable fingerprint (including data segments):762e771bcbd93219353261aa62fb966518be5395f8616ec2362defda1b7ad905\n",
      "I1113 02:07:54.466905  478783 isa_program_util_common.cc:517] (HLO module jit_add): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.466979  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.4996255ms\n",
      "W1113 02:07:54.488220  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.489317  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.493650  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.493672  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.493784  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 7.0686365ms\n",
      "I1113 02:07:54.494101  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.527232  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.18249125ms\n",
      "I1113 02:07:54.535292  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.02878875ms\n",
      "I1113 02:07:54.535407  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.535411  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.535413  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.535488  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.814525ms\n",
      "W1113 02:07:54.552926  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.553907  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.557849  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.557864  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.557978  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.51734525ms\n",
      "I1113 02:07:54.558332  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.591743  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.464409ms\n",
      "I1113 02:07:54.600241  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.4662445ms\n",
      "I1113 02:07:54.600358  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.600362  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.600364  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.600440  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.02432425ms\n",
      "W1113 02:07:54.617606  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.618595  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.622617  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.622632  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.622746  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.6373795ms\n",
      "I1113 02:07:54.623100  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.656336  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.28620525ms\n",
      "I1113 02:07:54.664962  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.5937995ms\n",
      "I1113 02:07:54.665077  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.665081  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.665084  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.665162  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.09929175ms\n",
      "W1113 02:07:54.682520  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.683533  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.687635  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.687651  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.687762  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.71892325ms\n",
      "I1113 02:07:54.688107  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.721302  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.2467445ms\n",
      "I1113 02:07:54.730096  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.76194325ms\n",
      "I1113 02:07:54.730212  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.730215  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.730217  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.730290  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.29072ms\n",
      "W1113 02:07:54.747488  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.748490  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.752599  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.752615  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.752726  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.791204ms\n",
      "I1113 02:07:54.753085  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.786211  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.17457775ms\n",
      "I1113 02:07:54.795044  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.8019455ms\n",
      "I1113 02:07:54.795180  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.795185  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.795187  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.795262  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.375806ms\n",
      "W1113 02:07:54.812587  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.813584  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.817741  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.817757  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.817868  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.78976625ms\n",
      "I1113 02:07:54.818234  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.851621  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.43789575ms\n",
      "I1113 02:07:54.860665  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.01147575ms\n",
      "I1113 02:07:54.860781  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.860785  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.860787  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.860862  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.82932025ms\n",
      "W1113 02:07:54.878028  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.879034  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.883093  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.883108  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.883244  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.709826ms\n",
      "I1113 02:07:54.883610  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.916879  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.31936525ms\n",
      "I1113 02:07:54.925830  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.91950325ms\n",
      "I1113 02:07:54.925952  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.925957  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.925958  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.926034  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.548492ms\n",
      "W1113 02:07:54.943695  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:54.944708  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:54.948845  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:54.948861  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:54.948973  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.82645875ms\n",
      "I1113 02:07:54.949348  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:54.982537  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.2395815ms\n",
      "I1113 02:07:54.991606  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.9337825ms\n",
      "I1113 02:07:54.991722  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:54.991726  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:54.991728  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:54.991803  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.705269ms\n",
      "W1113 02:07:55.009170  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.010175  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.014391  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.014407  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.014519  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8366105ms\n",
      "I1113 02:07:55.014892  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.048203  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.3687335ms\n",
      "I1113 02:07:55.057241  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.005804ms\n",
      "I1113 02:07:55.057359  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.057363  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.057365  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.057441  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.8046475ms\n",
      "W1113 02:07:55.074666  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.075690  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.079887  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.079902  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.080014  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8548085ms\n",
      "I1113 02:07:55.080390  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.115747  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 35.40852375ms\n",
      "I1113 02:07:55.124821  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.96460625ms\n",
      "I1113 02:07:55.124978  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.124982  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.124984  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.125060  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 51.94823875ms\n",
      "W1113 02:07:55.142486  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.143512  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.147636  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.147652  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.147766  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.79865725ms\n",
      "I1113 02:07:55.148135  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.181487  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.4053735ms\n",
      "I1113 02:07:55.190510  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.99234625ms\n",
      "I1113 02:07:55.190661  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.190666  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.190667  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.190748  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.82918675ms\n",
      "W1113 02:07:55.208204  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.209204  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.213463  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.213479  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.213590  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.93820175ms\n",
      "I1113 02:07:55.213971  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.247418  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.501901ms\n",
      "I1113 02:07:55.256498  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.04891ms\n",
      "I1113 02:07:55.256613  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.256617  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.256619  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.256695  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.087515ms\n",
      "W1113 02:07:55.274108  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.275122  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.279344  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.279361  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.279474  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.84740175ms\n",
      "I1113 02:07:55.279847  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.313179  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.3826165ms\n",
      "I1113 02:07:55.322305  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.093473ms\n",
      "I1113 02:07:55.322421  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.322425  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.322427  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.322501  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.9199765ms\n",
      "W1113 02:07:55.339954  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.340968  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.345233  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.345249  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.345364  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.93061425ms\n",
      "I1113 02:07:55.345743  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.379091  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.40060175ms\n",
      "I1113 02:07:55.388166  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.04347725ms\n",
      "I1113 02:07:55.388282  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.388285  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.388287  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.388362  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.980435ms\n",
      "W1113 02:07:55.405705  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.406826  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.410934  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.410950  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.411064  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.84456475ms\n",
      "I1113 02:07:55.411491  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.444822  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.41417125ms\n",
      "I1113 02:07:55.454028  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.06398925ms\n",
      "I1113 02:07:55.454142  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.454146  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.454148  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.454224  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.05892325ms\n",
      "W1113 02:07:55.471685  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.472681  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.476919  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.476934  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.477048  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.89135575ms\n",
      "I1113 02:07:55.477621  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.511485  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.942796ms\n",
      "I1113 02:07:55.520529  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.01084925ms\n",
      "I1113 02:07:55.520646  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.520650  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.520651  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.520728  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.61948625ms\n",
      "W1113 02:07:55.538522  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.539551  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.543839  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.543853  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.543965  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.9692535ms\n",
      "I1113 02:07:55.544514  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.577875  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.422744ms\n",
      "I1113 02:07:55.586972  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.06470025ms\n",
      "I1113 02:07:55.587085  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.587088  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.587090  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.587190  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.24237225ms\n",
      "W1113 02:07:55.604839  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.605827  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.610087  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.610101  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.610220  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.89123725ms\n",
      "I1113 02:07:55.610619  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.644046  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.49538975ms\n",
      "I1113 02:07:55.653109  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.032913ms\n",
      "I1113 02:07:55.653223  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.653226  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.653228  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.653304  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.02554475ms\n",
      "W1113 02:07:55.671080  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.672108  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.676301  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.676330  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.676446  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8484425ms\n",
      "I1113 02:07:55.676975  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.710373  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.459009ms\n",
      "I1113 02:07:55.719576  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.17200925ms\n",
      "I1113 02:07:55.719690  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.719694  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.719695  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.719770  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.22163725ms\n",
      "W1113 02:07:55.737521  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.738531  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.742753  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.742768  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.742884  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8808165ms\n",
      "I1113 02:07:55.743445  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.776911  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.52782325ms\n",
      "I1113 02:07:55.786041  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.09959725ms\n",
      "I1113 02:07:55.786154  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.786158  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.786160  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.786236  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.28032325ms\n",
      "W1113 02:07:55.804044  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.805043  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.809286  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.809300  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.809418  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.94107225ms\n",
      "I1113 02:07:55.809959  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.843358  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.462712ms\n",
      "I1113 02:07:55.852512  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.1224085ms\n",
      "I1113 02:07:55.852626  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.852630  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.852632  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.852709  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.28124475ms\n",
      "W1113 02:07:55.871141  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.872155  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.876442  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.876457  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.876569  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.92840025ms\n",
      "I1113 02:07:55.877105  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.910589  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.54744175ms\n",
      "I1113 02:07:55.919778  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.15856475ms\n",
      "I1113 02:07:55.919895  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.919900  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.919902  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.919977  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.380779ms\n",
      "W1113 02:07:55.937160  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 02:07:55.938163  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:55.942429  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:55.942443  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:55.942557  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.922247ms\n",
      "I1113 02:07:55.943106  479673 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:55.976378  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.3344335ms\n",
      "I1113 02:07:55.985573  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.1634945ms\n",
      "I1113 02:07:55.985689  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:55.985693  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:55.985695  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:55.985770  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.184013ms\n",
      "W1113 02:07:56.003495  478783 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 02:07:56 [kv_cache_manager.py:216] Init kv-cache | num_layers=24 | shape=(num_blocks, (16, 4, 2, 128)) | num_blocks=[415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389] | sharding=NamedSharding(mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto)), spec=PartitionSpec('data', None, 'model'), memory_kind=device) | dtype=bfloat16 | hbm=[(86.17, 95.74), (86.17, 95.74), (86.17, 95.74), (86.17, 95.74)]Gb\n",
      "INFO 11-13 02:07:56 [core.py:247] init engine (profile, create kv cache, warmup model) took 1.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 02:07:56.004492  478783 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 02:07:56.008654  467351 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 02:07:56.008668  467351 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 02:07:56.008780  467351 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.80513275ms\n",
      "I1113 02:07:56.009324  479674 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 02:07:56.042834  478783 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.571921ms\n",
      "I1113 02:07:56.052045  478783 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.17695225ms\n",
      "I1113 02:07:56.052159  478783 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 02:07:56.052163  478783 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 02:07:56.052165  478783 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 02:07:56.052240  467351 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.31468475ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 02:07:58 [llm.py:353] Supported tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "import os\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"\n",
    "os.environ[\"JAX_RANDOM_WEIGHTS\"] = \"False\"\n",
    "os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n",
    "\n",
    "os.environ[\"TPU_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TPU_STDERR_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"VLLM_MLA_DISABLE\"] = \"1\"\n",
    "#os.environ[\"MODEL_IMPL_TYPE\"] = \"vllm\"\n",
    "\n",
    "MODEL = \"unsloth/gpt-oss-20b-BF16\"\n",
    "\n",
    "golden_llm = LLM(\n",
    "    MODEL,\n",
    "    max_model_len=128,\n",
    "    tensor_parallel_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8016b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_state = golden_llm.llm_engine.model_executor.driver_worker.model_runner.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c753071",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'flat_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m flat_state = \u001b[43mgolden_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_state\u001b[49m()\n\u001b[32m      2\u001b[39m flat_state\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'flat_state'"
     ]
    }
   ],
   "source": [
    "flat_state = golden_state.flat_state()\n",
    "flat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c019ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'flax.nnx.variablelib.Param'> <class 'jaxlib._jax.ArrayImpl'>\n",
      "embedder.input_embedding_table_VD\n",
      "\tshape: (201088, 2880)\n",
      "\tsharding: (('data', 'model'), None)\n",
      "final_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.0.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.0.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.0.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.0.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.0.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.0.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.0.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.0.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.0.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.0.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.0.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.0.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.0.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.0.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.0.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.0.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.0.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.1.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.1.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.1.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.1.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.1.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.1.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.1.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.1.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.1.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.1.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.1.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.1.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.1.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.1.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.1.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.1.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.1.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.2.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.2.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.2.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.2.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.2.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.2.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.2.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.2.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.2.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.2.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.2.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.2.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.2.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.2.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.2.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.2.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.2.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.3.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.3.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.3.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.3.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.3.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.3.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.3.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.3.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.3.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.3.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.3.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.3.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.3.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.3.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.3.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.3.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.3.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.4.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.4.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.4.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.4.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.4.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.4.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.4.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.4.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.4.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.4.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.4.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.4.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.4.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.4.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.4.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.4.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.4.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.5.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.5.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.5.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.5.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.5.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.5.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.5.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.5.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.5.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.5.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.5.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.5.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.5.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.5.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.5.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.5.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.5.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.6.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.6.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.6.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.6.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.6.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.6.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.6.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.6.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.6.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.6.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.6.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.6.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.6.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.6.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.6.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.6.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.6.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.7.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.7.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.7.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.7.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.7.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.7.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.7.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.7.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.7.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.7.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.7.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.7.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.7.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.7.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.7.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.7.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.7.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.8.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.8.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.8.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.8.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.8.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.8.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.8.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.8.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.8.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.8.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.8.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.8.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.8.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.8.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.8.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.8.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.8.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.9.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.9.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.9.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.9.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.9.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.9.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.9.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.9.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.9.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.9.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.9.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.9.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.9.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.9.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.9.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.9.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.9.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.10.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.10.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.10.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.10.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.10.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.10.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.10.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.10.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.10.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.10.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.10.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.10.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.10.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.10.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.10.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.10.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.10.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.11.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.11.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.11.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.11.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.11.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.11.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.11.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.11.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.11.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.11.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.11.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.11.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.11.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.11.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.11.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.11.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.11.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.12.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.12.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.12.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.12.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.12.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.12.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.12.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.12.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.12.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.12.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.12.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.12.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.12.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.12.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.12.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.12.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.12.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.13.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.13.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.13.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.13.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.13.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.13.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.13.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.13.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.13.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.13.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.13.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.13.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.13.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.13.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.13.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.13.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.13.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.14.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.14.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.14.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.14.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.14.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.14.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.14.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.14.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.14.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.14.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.14.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.14.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.14.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.14.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.14.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.14.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.14.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.15.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.15.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.15.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.15.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.15.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.15.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.15.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.15.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.15.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.15.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.15.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.15.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.15.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.15.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.15.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.15.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.15.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.16.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.16.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.16.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.16.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.16.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.16.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.16.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.16.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.16.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.16.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.16.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.16.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.16.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.16.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.16.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.16.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.16.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.17.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.17.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.17.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.17.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.17.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.17.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.17.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.17.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.17.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.17.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.17.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.17.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.17.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.17.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.17.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.17.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.17.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.18.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.18.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.18.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.18.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.18.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.18.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.18.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.18.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.18.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.18.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.18.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.18.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.18.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.18.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.18.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.18.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.18.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.19.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.19.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.19.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.19.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.19.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.19.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.19.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.19.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.19.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.19.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.19.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.19.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.19.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.19.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.19.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.19.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.19.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.20.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.20.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.20.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.20.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.20.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.20.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.20.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.20.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.20.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.20.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.20.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.20.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.20.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.20.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.20.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.20.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.20.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.21.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.21.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.21.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.21.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.21.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.21.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.21.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.21.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.21.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.21.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.21.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.21.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.21.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.21.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.21.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.21.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.21.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.22.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.22.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.22.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.22.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.22.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.22.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.22.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.22.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.22.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.22.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.22.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.22.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.22.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.22.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.22.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.22.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.22.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.23.attn.bias_k_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.23.attn.bias_o_D\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.23.attn.bias_q_NH\n",
      "\tshape: (64, 64)\n",
      "\tsharding: ()\n",
      "layers.23.attn.bias_v_KH\n",
      "\tshape: (8, 64)\n",
      "\tsharding: ()\n",
      "layers.23.attn.kernel_k_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.23.attn.kernel_o_proj_NHD\n",
      "\tshape: (64, 64, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.23.attn.kernel_q_DNH\n",
      "\tshape: (2880, 64, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.23.attn.kernel_v_DKH\n",
      "\tshape: (2880, 8, 64)\n",
      "\tsharding: (None, 'model', None)\n",
      "layers.23.attn.sinks_N\n",
      "\tshape: (64,)\n",
      "\tsharding: ()\n",
      "layers.23.custom_module.mlp1_bias_EF2\n",
      "\tshape: (32, 5760)\n",
      "\tsharding: ('model', None)\n",
      "layers.23.custom_module.mlp1_weight_EDF2\n",
      "\tshape: (32, 2880, 5760)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.23.custom_module.mlp2_bias_ED\n",
      "\tshape: (32, 2880)\n",
      "\tsharding: ('model', None)\n",
      "layers.23.custom_module.mlp2_weight_EFD\n",
      "\tshape: (32, 2880, 2880)\n",
      "\tsharding: ('model', None, None)\n",
      "layers.23.custom_module.router.bias_E\n",
      "\tshape: (32,)\n",
      "\tsharding: ('model',)\n",
      "layers.23.custom_module.router.kernel_DE\n",
      "\tshape: (2880, 32)\n",
      "\tsharding: ('model', None)\n",
      "layers.23.pre_attention_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "layers.23.pre_mlp_norm.scale\n",
      "\tshape: (2880,)\n",
      "\tsharding: ()\n",
      "lm_head.input_embedding_table_DV\n",
      "\tshape: (2880, 201088)\n",
      "\tsharding: (None, ('data', 'model'))\n",
      "rng.default.count\n",
      "\tshape: ()\n",
      "\tsharding: NamedSharding(mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto)), spec=PartitionSpec(), memory_kind=device)\n",
      "rng.default.key\n",
      "\tshape: ()\n",
      "\tsharding: NamedSharding(mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto)), spec=PartitionSpec(), memory_kind=device)\n"
     ]
    }
   ],
   "source": [
    "j = flat_state[0][1]\n",
    "print(\"type:\",  type(j), type(j.value))\n",
    "\n",
    "for i, j in flat_state:\n",
    "  print(\".\".join(str(k) for k in i))\n",
    "  print(\"\\tshape:\", j.value.shape)\n",
    "  print(\"\\tsharding:\", j.sharding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315fba15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl",
   "language": "python",
   "name": "venv-rl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
