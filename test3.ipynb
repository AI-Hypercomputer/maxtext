{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "889937f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Jupyter Python Executable: /home/shuningjin_google_com/venv-rl/bin/python3\n",
      "2. Current Working Directory: /home/shuningjin_google_com/maxtext\n",
      "3. VLLM is importing from:    /home/shuningjin_google_com/vllm/vllm/__init__.py\n",
      "4. MaxText import failed completely.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir('/home/shuningjin_google_com/maxtext')\n",
    "\n",
    "print(f\"1. Jupyter Python Executable: {sys.executable}\")\n",
    "print(f\"2. Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    import vllm\n",
    "    print(f\"3. VLLM is importing from:    {vllm.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"3. VLLM import failed completely.\")\n",
    "except AttributeError:\n",
    "    print(\"3. VLLM imported but has no __file__ (Namespace or directory shadowing).\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import MaxText\n",
    "    print(f\"4. MaxText is importing from:    {MaxText.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"4. MaxText import failed completely.\")\n",
    "except AttributeError:\n",
    "    print(\"4. MaxText imported but has no __file__ (Namespace or directory shadowing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79fd0f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuningjin_google_com/venv-rl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:09:34 [__init__.py:26] TPU info: node_name=shuningjin-tpu-v3 | tpu_type=v5p-8 | worker_id=0 | num_chips=4 | num_cores_per_chip=2\n",
      "INFO 11-13 01:09:34 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 11-13 01:09:34 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 11-13 01:09:34 [interface.py:197] Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 11-13 01:09:35 [utils.py:253] non-default args: {'max_model_len': 128, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'model': 'unsloth/gpt-oss-20b-BF16'}\n",
      "WARNING 11-13 01:09:36 [model.py:437] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.\n",
      "INFO 11-13 01:09:36 [model.py:630] Resolved architecture: GptOssForCausalLM\n",
      "INFO 11-13 01:09:36 [model.py:1735] Using max model len 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 01:09:36,722\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:09:36 [scheduler.py:254] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-13 01:09:36 [config.py:272] Overriding max cuda graph capture size to 1024 for performance.\n",
      "INFO 11-13 01:09:36 [tpu_platform.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=4, sharding_strategy=ShardingStrategy(tensor_parallelism=4, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
      "WARNING 11-13 01:09:36 [tpu_platform.py:154] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16\n",
      "INFO 11-13 01:09:36 [tpu_platform.py:187] Force using UniProcExecutor for JAX on single host.\n",
      "INFO 11-13 01:09:38 [core.py:94] Initializing a V1 LLM engine (v0.11.1rc7.dev83+g64d57c3be) with config: model='unsloth/gpt-oss-20b-BF16', speculative_config=None, tokenizer='unsloth/gpt-oss-20b-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=None, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gpt-oss-20b-BF16, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.DYNAMO_TRACE_ONCE: 2>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'openxla', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': True, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 1024, 'local_cache_dir': None}\n",
      "WARNING 11-13 01:09:38 [tpu_platform.py:223] Pin memory is not supported on TPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 01:09:38.430731  471069 b295d63588a.cc:762] Linux version 5.19.0-1027-gcp (buildd@lcy02-amd64-078) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.1.0-2ubuntu1~22.04) 12.1.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #29~22.04.1-Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023\n",
      "I1113 01:09:38.437992  471069 b295d63588a.cc:844] Process id 471069\n",
      "I1113 01:09:38.438011  471069 b295d63588a.cc:849] Current working directory /home/shuningjin_google_com/maxtext\n",
      "I1113 01:09:38.438013  471069 b295d63588a.cc:851] Current timezone is UTC (currently UTC +00:00)\n",
      "I1113 01:09:38.438018  471069 b295d63588a.cc:855] Built on Sep 11 2025 15:57:19 (1757631439)\n",
      "I1113 01:09:38.438019  471069 b295d63588a.cc:856]  at rbex-enqueue-targets@lgje25.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "I1113 01:09:38.438021  471069 b295d63588a.cc:857]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "I1113 01:09:38.438022  471069 b295d63588a.cc:858]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "I1113 01:09:38.438027  471069 b295d63588a.cc:861]  from changelist 804429027 with baseline 804273004 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/804273004.1/g3     \n",
      "I1113 01:09:38.438029  471069 b295d63588a.cc:865] Build label: libtpu_lts_20250908_a_RC01\n",
      "I1113 01:09:38.438031  471069 b295d63588a.cc:867] Build tool: Bazel, release r4rca-2025.08.25-1 (mainline @798895491)\n",
      "I1113 01:09:38.438032  471069 b295d63588a.cc:868] Build target: \n",
      "I1113 01:09:38.438037  471069 b295d63588a.cc:875] Command line arguments:\n",
      "I1113 01:09:38.438038  471069 b295d63588a.cc:877] argv[0]: './tpu_driver'\n",
      "I1113 01:09:38.438041  471069 b295d63588a.cc:877] argv[1]: '--minloglevel=0'\n",
      "I1113 01:09:38.438043  471069 b295d63588a.cc:877] argv[2]: '--stderrthreshold=0'\n",
      "I1113 01:09:38.438045  471069 b295d63588a.cc:877] argv[3]: '--v=0'\n",
      "I1113 01:09:38.438047  471069 b295d63588a.cc:877] argv[4]: '--vmodule='\n",
      "I1113 01:09:38.438048  471069 b295d63588a.cc:877] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "I1113 01:09:38.438050  471069 b295d63588a.cc:877] argv[6]: '--max_log_size=1024'\n",
      "I1113 01:09:38.438052  471069 b295d63588a.cc:877] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "I1113 01:09:38.438053  471069 b295d63588a.cc:877] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "I1113 01:09:38.438055  471069 b295d63588a.cc:877] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "I1113 01:09:38.438057  471069 b295d63588a.cc:877] argv[10]: '--2a886c8_twist=false'\n",
      "I1113 01:09:38.438058  471069 b295d63588a.cc:877] argv[11]: '--2a886c8_chip_config_name=megachip_tccontrol'\n",
      "I1113 01:09:38.438060  471069 b295d63588a.cc:877] argv[12]: '--2a886c8_chips_per_host_bounds=2,2,1'\n",
      "I1113 01:09:38.438062  471069 b295d63588a.cc:877] argv[13]: '--2a886c8_host_bounds=1,1,1'\n",
      "I1113 01:09:38.438064  471069 b295d63588a.cc:877] argv[14]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "I1113 01:09:38.438065  471069 b295d63588a.cc:877] argv[15]: '--2a886c8_slice_builder_worker_addresses=10.164.0.41:8471'\n",
      "I1113 01:09:38.438067  471069 b295d63588a.cc:877] argv[16]: '--tpu_slice_builder_dump_chip=true'\n",
      "I1113 01:09:38.438068  471069 b295d63588a.cc:877] argv[17]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "I1113 01:09:38.438070  471069 b295d63588a.cc:877] argv[18]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "I1113 01:09:38.438072  471069 b295d63588a.cc:877] argv[19]: '--bypass_vbar_control_service=0'\n",
      "I1113 01:09:38.438073  471069 b295d63588a.cc:877] argv[20]: '--2a886c8_ici_resilient=false'\n",
      "I1113 01:09:38.438075  471069 b295d63588a.cc:877] argv[21]: '--xla_tpu_use_resilient_collective_emitter=false'\n",
      "I1113 01:09:38.438077  471069 b295d63588a.cc:877] argv[22]: '--tpu_slice_builder_topology_discovery_fault_injection='\n",
      "I1113 01:09:38.438078  471069 b295d63588a.cc:877] argv[23]: '--runtime_metric_service_port=8431'\n",
      "I1113 01:09:38.438080  471069 b295d63588a.cc:877] argv[24]: '--tpu_hbm_report_enable=1'\n",
      "I1113 01:09:38.438082  471069 b295d63588a.cc:877] argv[25]: '--tpu_hbm_report_frequency=5s'\n",
      "I1113 01:09:38.438083  471069 b295d63588a.cc:877] argv[26]: '--enable_runtime_uptime_telemetry=true'\n",
      "I1113 01:09:38.438085  471069 b295d63588a.cc:877] argv[27]: ''\n",
      "I1113 01:09:38.438092  471069 b295d63588a.cc:877] argv[28]: '--xla_tpu_use_enhanced_launch_barrier=true'\n",
      "I1113 01:09:38.438318  471069 init.cc:78] Remote crash gathering hook installed.\n",
      "I1113 01:09:38.438350  471069 tpu_runtime_type_flags.cc:79] --tpu_use_tfrt not specified. Using default value: true\n",
      "I1113 01:09:38.463529  471069 tpu_hal.cc:429] Registered plugin from module: breakpoint_debugger_server\n",
      "I1113 01:09:38.463782  471069 log_message_host_command_handler.cc:70] Registering LogMessageHostCommandHandler\n",
      "I1113 01:09:38.463792  471069 host_command_handler_factory.cc:31] Skipping registration of host command handler for opcode Log because it is not in the allowlist.\n",
      "I1113 01:09:38.463813  471069 tpu_hal.cc:429] Registered plugin from module: megascale_sync_flag_logger\n",
      "I1113 01:09:38.465507  471069 tpu_hal.cc:429] Registered plugin from module: RuntimeMetricHelper\n",
      "I1113 01:09:38.465581  471069 tf_tpu_flags.cc:63] 2a886c8Platform is NOT registered.\n",
      "I1113 01:09:38.465756  471069 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "I1113 01:09:38.465779  471069 tpu_hal.cc:429] Registered plugin from module: sdc_checker_callback\n",
      "I1113 01:09:38.465837  471069 tpu_hal.cc:429] Registered plugin from module: xsc_explicit_checksum_tracing_callback\n",
      "I1113 01:09:38.465915  471069 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "I1113 01:09:38.486701  471069 config.cc:256] gRPC experiments enabled: error_flatten, event_engine_callback_cq, event_engine_client, event_engine_dns, event_engine_dns_non_client_channel, event_engine_listener, google_no_envelope_resolver, monitoring_experiment, tsi_frame_protector_without_locks\n",
      "I1113 01:09:38.495470  471069 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 228, prefix = futex-default\n",
      "I1113 01:09:38.495668  471069 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <60-26 61-54 47-56 00-00 A0-25 61-54 47-56 00-00 80-5C 1E-11 48-7F 00-00 40-6A BD-12 48-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = 0, clock = 0x56474e624108\n",
      "I1113 01:09:38.503838  471801 cachednslookup.cc:391] TTL not found in response, not caching response\n",
      "I1113 01:09:38.523751  471069 debug_manager.cc:220] Registering error handler with name: libtpu_telemetry_handler\n",
      "W1113 01:09:38.524298  471069 uptime_telemetry.cc:187] UptimeMetric attributes are updated.\n",
      "Previous Attributes: go/debugproto  \n",
      "key: \"uptime_attributes\"\n",
      "value {\n",
      "  kvlist_attr {\n",
      "    attributes {\n",
      "      key: \"ml_framework_name\"\n",
      "      value {\n",
      "        string_attr: \"jax\"\n",
      "      }\n",
      "    }\n",
      "    attributes {\n",
      "      key: \"ml_framework_version\"\n",
      "      value {\n",
      "        string_attr: \"jax-0.7.2\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "New Attributes: go/debugproto  \n",
      "key: \"uptime_attributes\"\n",
      "value {\n",
      "  kvlist_attr {\n",
      "    attributes {\n",
      "      key: \"ml_framework_name\"\n",
      "      value {\n",
      "        string_attr: \"pytorch/xla2\"\n",
      "      }\n",
      "    }\n",
      "    attributes {\n",
      "      key: \"ml_framework_version\"\n",
      "      value {\n",
      "        string_attr: \"pytorch/xla2-v0.0.1\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "I1113 01:09:38.534160  471069 singleton_tpu_states_manager.cc:73] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "I1113 01:09:38.534173  471069 singleton_tpu_states_manager.cc:96] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "I1113 01:09:38.535355  471069 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 01:09:38.535365  471069 tpu_version_flag.cc:54] Using auto-detected TPU version TPU v5\n",
      "I1113 01:09:38.536192  471069 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 01:09:38.536994  471069 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 01:09:38.537798  471069 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 01:09:38.538129  471069 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 01:09:38.538134  471069 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 01:09:38.538136  471069 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 01:09:38.538137  471069 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1113 01:09:38.541700  472070 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 01:09:38.541717  472070 flags_util.cc:318] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "I1113 01:09:38.542549  472070 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1113 01:09:38.542556  472070 tpu_network_factory.cc:67] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "I1113 01:09:38.551967  472072 futex.cc:68] RAW: Futex::Swap(): using FUTEX_WAKE + FUTEX_WAIT\n",
      "I1113 01:09:39.577863  472075 async_driver.cc:454] [/dev/vfio/2 tpu575:pe0:2] vf_id:0 Driver opened.\n",
      "I1113 01:09:39.631741  472073 async_driver.cc:454] [/dev/vfio/0 tpu575:pe0:0] vf_id:0 Driver opened.\n",
      "I1113 01:09:39.637361  472074 async_driver.cc:454] [/dev/vfio/1 tpu575:pe0:1] vf_id:0 Driver opened.\n",
      "I1113 01:09:39.637582  472076 async_driver.cc:454] [/dev/vfio/3 tpu575:pe0:3] vf_id:0 Driver opened.\n",
      "W1113 01:09:39.655960  472075 async_driver.cc:1736] All cores not supported.\n",
      "W1113 01:09:39.707632  472073 async_driver.cc:1736] All cores not supported.\n",
      "W1113 01:09:39.708147  472074 async_driver.cc:1736] All cores not supported.\n",
      "W1113 01:09:39.708379  472076 async_driver.cc:1736] All cores not supported.\n",
      "I1113 01:09:39.708449  472070 slice_builder_helper.cc:99] Current host is used as SliceBuilder master.\n",
      "I1113 01:09:39.709398  472070 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "I1113 01:09:39.718274  472070 legacy_topology_discoverer.cc:55] Target Topology: (2, 2, 1)\n",
      "I1113 01:09:40.747727  472070 master.cc:219] Successfully initialized SliceBuilder master session d14223af0efe3a8a with expected topology (2, 2, 1)\n",
      "I1113 01:09:40.748995  472070 tpu_hal.cc:200] Starting premapped memory manager initialization...\n",
      "W1113 01:09:40.749900  472400 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.749906  472399 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.749910  472401 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.749916  472400 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 01:09:40.749919  472399 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 01:09:40.749922  472401 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 01:09:40.750291  472398 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.750305  472398 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 01:09:40.750338  472400 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.750345  472400 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 01:09:40.750346  472399 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.750347  472401 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1113 01:09:40.750350  472399 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1113 01:09:40.750351  472401 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 11-13 01:09:44 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 01:09:44.692936  472070 runtime_metric_service.cc:159] Successfully started Runtime Metric Service on port: 8431\n",
      "I1113 01:09:44.693033  472070 system.cc:1091] tpu::System initialized, current host id: 0, logical device ids: 0,1,2,3\n",
      "I1113 01:09:44.693050  471069 tpu_system_state.cc:217] CreateTpuSystemState: TPU initialization is successful and it took 6.152580191s\n",
      "I1113 01:09:44.693071  471069 tpu_system_state.cc:221] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "I1113 01:09:44.693078  471069 tpu_host_allocator.cc:64] Premapped buffer is using alignment 64\n",
      "I1113 01:09:44.693524  471069 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "I1113 01:09:44.818082  471069 autofdo_agent.cc:198] xla_tpu_autofdo_profile_dir updated to \n",
      "W1113 01:09:44.818096  471069 autofdo_agent.cc:201] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "I1113 01:09:44.821393  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1113 01:09:44.821404  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1113 01:09:44.821603  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.26657325ms\n",
      "I1113 01:09:44.866614  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:44.871722  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.38458225ms\n",
      "I1113 01:09:44.880669  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.8916785ms\n",
      "I1113 01:09:44.880767  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:13d169e8f0cb52132827f3458861645b2ad7cf1a3afcb1127a6444697d9ec7eb\n",
      "I1113 01:09:44.880771  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):7274c5d1d7ca793892a3a02db907049e506cf3ed81768774f478da0a45758e78\n",
      "I1113 01:09:44.880773  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:44.880862  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 64.58677275ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:09:44 [tpu_runner.py:273] Init mesh | mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto))\n",
      "INFO 11-13 01:09:44 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
      "INFO 11-13 01:09:44 [utils.py:59] Prepared request paddings: [8, 16, 32, 64, 128, 256]\n",
      "INFO 11-13 01:09:44 [compilation_manager.py:34] Enabling JAX compile cache.\n",
      "INFO 11-13 01:09:44 [tpu_worker.py:151] Init worker | rank=0 | node_id=0 | is_driver_worker=True | hbm=[(0.0, 95.74), (0.0, 95.74), (0.0, 95.74), (0.0, 95.74)]GiB\n",
      "INFO 11-13 01:09:44 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=vllm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 01:09:44.902540  471069 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 8 instructions, modules: jit__threefry_seed\n",
      "I1113 01:09:44.902557  471069 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__threefry_seed instructions: 8 fingerprint: \n",
      "I1113 01:09:44.902691  471069 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__threefry_seed instructions: 8\n",
      "I1113 01:09:44.902693  471069 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1113 01:09:44.904886  471069 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1113 01:09:44.905384  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663111577 bytes.\n",
      "I1113 01:09:44.905393  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883155578 bytes.\n",
      "I1113 01:09:44.905633  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.18476625ms\n",
      "I1113 01:09:44.905969  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:44.911364  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.469515ms\n",
      "I1113 01:09:44.912022  472071 2a886c8_compiler_base.cc:3045] final program bundle count: 205 note this count does not reflect cycles spent executing delays.\n",
      "I1113 01:09:44.916506  472071 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1113 01:09:44.917509  472071 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (50.0K).\n",
      "I1113 01:09:44.917683  472071 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__threefry_seed\n",
      "I1113 01:09:44.917692  472071 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 50.0K / 95.74G\n",
      "I1113 01:09:44.917696  472071 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 4.0K / 64.00M\n",
      "I1113 01:09:44.917708  472071 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.05M:\n",
      "I1113 01:09:44.917710  472071 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1113 01:09:44.917712  472071 2a886c8_compiler_base.cc:3549]     program           50.0K \n",
      "I1113 01:09:44.917713  472071 2a886c8_compiler_base.cc:3549]     arguments          512B \n",
      "I1113 01:09:44.917714  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:09:44.917715  472071 2a886c8_compiler_base.cc:3549] Output size 512B; shares 0B with arguments.\n",
      "I1113 01:09:44.917716  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:09:44.917718  472071 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1113 01:09:44.917719  472071 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1113 01:09:44.917720  472071 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1113 01:09:44.917721  472071 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1113 01:09:44.917722  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:09:44.917723  472071 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1113 01:09:44.917724  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:09:44.917732  472071 2a886c8_compiler_base.cc:3553] Program sflag requirement 212B:\n",
      "I1113 01:09:44.917734  472071 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1113 01:09:44.917735  472071 2a886c8_compiler_base.cc:3553]     scoped               8B\n",
      "I1113 01:09:44.917736  472071 2a886c8_compiler_base.cc:3553] Program vmem requirement 4.0K:\n",
      "I1113 01:09:44.917737  472071 2a886c8_compiler_base.cc:3553]     scoped             4.0K\n",
      "I1113 01:09:44.917738  472071 2a886c8_compiler_base.cc:3553] Program smem requirement 32B:\n",
      "I1113 01:09:44.917739  472071 2a886c8_compiler_base.cc:3553]     scoped              32B\n",
      "I1113 01:09:44.917740  472071 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1113 01:09:44.917741  472071 2a886c8_compiler_base.cc:3553] Program hbm requirement 50.0K:\n",
      "I1113 01:09:44.917742  472071 2a886c8_compiler_base.cc:3553]     overlays          50.0K\n",
      "I1113 01:09:44.917743  472071 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 1.9K / 1.00M (1 parameters)\n",
      "I1113 01:09:44.917761  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.362833ms\n",
      "I1113 01:09:44.917842  472071 isa_program_util_common.cc:510] (HLO module jit__threefry_seed): Executable fingerprint:4e5a39cda394a06cc48348ff5409ccb2e1723f45f094264066ae3490e8cf3524\n",
      "I1113 01:09:44.917845  472071 isa_program_util_common.cc:514] (HLO module jit__threefry_seed): Executable fingerprint (including data segments):dc57a40245541f0780852f287f4f5dd66c906d0a6bbfcaaaf69cdd4b47aa3406\n",
      "I1113 01:09:44.917846  472071 isa_program_util_common.cc:517] (HLO module jit__threefry_seed): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:44.917934  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 16.53530525ms\n",
      "I1113 01:09:44.935577  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1113 01:09:44.935594  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1113 01:09:44.935719  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.158698ms\n",
      "I1113 01:09:44.935973  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:44.940285  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.3588955ms\n",
      "I1113 01:09:44.946393  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.068652ms\n",
      "I1113 01:09:44.946474  472071 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:05accf46df728591c609ef1ad06662677f25037e3433b8cac821e0dcdaf0f76f\n",
      "I1113 01:09:44.946478  472071 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):7824ca0711ee26b12f60d723ce497e4d5f85ef619be6170fdaefc1ff4fabd785\n",
      "I1113 01:09:44.946481  472071 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:44.946559  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.02966575ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-13 01:09:46 [rocm.py:34] Failed to import from amdsmi with ModuleNotFoundError(\"No module named 'amdsmi'\")\n",
      "WARNING 11-13 01:09:46 [rocm.py:39] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "WARNING 11-13 01:09:46 [rocm.py:45] Failed to import from vllm._rocm_C with ModuleNotFoundError(\"No module named 'vllm._rocm_C'\")\n",
      "WARNING 11-13 01:09:46 [selector.py:147] use_v1 parameter for get_attn_backend_cls is deprecated and will be removed in v0.13.0 or v1.0.0, whichever is soonest. Please remove it from your plugin code.\n",
      "WARNING 11-13 01:09:46 [registry.py:172] _Backend has been renamed to AttentionBackendEnum. Please update your code to use AttentionBackendEnum instead. _Backend will be removed in a future release.\n",
      "INFO 11-13 01:09:46 [tpu_platform.py:63] Cannot use None backend on TPU.\n",
      "INFO 11-13 01:09:46 [tpu_platform.py:66] Using Pallas V1 backend.\n",
      "INFO 11-13 01:09:46 [layer.py:331] Disabling MoE shared_experts cuda stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:04,  1.72it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:04,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.63it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:03,  1.51it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:03<00:02,  1.50it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:03<00:02,  1.45it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:04<00:01,  1.74it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  1.68it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:09:52 [default_loader.py:314] Loading weights took 5.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "I1113 01:09:52.849347  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97604262809 bytes.\n",
      "I1113 01:09:52.849391  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4880213140 bytes.\n",
      "I1113 01:09:52.849547  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.1861975ms\n",
      "I1113 01:09:52.849987  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:52.865323  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 15.45388925ms\n",
      "I1113 01:09:52.872918  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.5364075ms\n",
      "I1113 01:09:52.873016  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:58ce4f458d4d91ce939f7ee5091646b64aeb1a68fd8efdab8061ad95bbaddf87\n",
      "I1113 01:09:52.873020  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):18f7c1b8fd973d02086a83949d38540a9d7f9dbb1d5a788703d778cded7ed379\n",
      "I1113 01:09:52.873022  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:52.873145  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 27.8324595ms\n",
      "I1113 01:09:52.891223  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633754009 bytes.\n",
      "I1113 01:09:52.891238  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881687700 bytes.\n",
      "I1113 01:09:52.891359  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.3287515ms\n",
      "I1113 01:09:52.891690  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:52.905519  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 13.8906375ms\n",
      "I1113 01:09:52.912229  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.67935025ms\n",
      "I1113 01:09:52.912317  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:83c9b85902891a356e046bce166bbebc8b722649a4016ff1a5ed790cb880920f\n",
      "I1113 01:09:52.912321  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):9d44370a088ad526935beac2fe617e198dfbd05a897a1873189d5c50c87ee597\n",
      "I1113 01:09:52.912324  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:52.912398  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 24.40368125ms\n",
      "I1113 01:09:52.930009  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639631769 bytes.\n",
      "I1113 01:09:52.930025  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881981588 bytes.\n",
      "I1113 01:09:52.930158  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.57454725ms\n",
      "I1113 01:09:52.930443  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:52.995381  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 64.99167625ms\n",
      "I1113 01:09:53.005908  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 10.47468075ms\n",
      "I1113 01:09:53.006046  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:3b50d8789a9e3f3b104c2fe8b9665f4dd580a856d5403fa2bc2892ffac36a079\n",
      "I1113 01:09:53.006050  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):16c4f414a0fd71cbbe8010ed33820bd803ee115de9eea563548473828e9d09e3\n",
      "I1113 01:09:53.006052  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.006152  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 79.598521ms\n",
      "I1113 01:09:53.022518  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633754009 bytes.\n",
      "I1113 01:09:53.022534  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881687700 bytes.\n",
      "I1113 01:09:53.022648  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.07963575ms\n",
      "I1113 01:09:53.022945  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.053514  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 30.62165175ms\n",
      "I1113 01:09:53.062215  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.669847ms\n",
      "I1113 01:09:53.062325  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:fc335a3c5bf3b8c128fad17e9d39750689ec7457cd1f16995f2ab69be07b5363\n",
      "I1113 01:09:53.062330  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):b852271f2d648d94251143e950d67c8dff8b284b202c0dfdeb68447ac400212d\n",
      "I1113 01:09:53.062331  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.062406  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 42.8708425ms\n",
      "I1113 01:09:53.077997  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97660275609 bytes.\n",
      "I1113 01:09:53.078012  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883013780 bytes.\n",
      "I1113 01:09:53.078143  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.4635665ms\n",
      "I1113 01:09:53.078444  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.135786  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 57.40228875ms\n",
      "I1113 01:09:53.145546  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.69338875ms\n",
      "I1113 01:09:53.145710  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:80401440c3e445335d0a724a91707db758d5b1df92516e58389cd2b69a275eaf\n",
      "I1113 01:09:53.145714  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):73047d5057bf00f806325fb160fd1cabf80cdc2fc81197e6b9243ab036fa6e7d\n",
      "I1113 01:09:53.145722  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.145802  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 71.1549385ms\n",
      "I1113 01:09:53.161270  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633754009 bytes.\n",
      "I1113 01:09:53.161287  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881687700 bytes.\n",
      "I1113 01:09:53.161402  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.10568375ms\n",
      "I1113 01:09:53.161689  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.192249  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 30.6123895ms\n",
      "I1113 01:09:53.200015  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.7091025ms\n",
      "I1113 01:09:53.200121  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:2b8d1ef8493cb66ae64170f631621a69c22d11275f3afa31e81a2ab6734bb0ce\n",
      "I1113 01:09:53.200124  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):043857375c40b5feac96d179845108a72c454d11b2f5454afe5d2250a9c53f47\n",
      "I1113 01:09:53.200126  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.200208  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 41.944052ms\n",
      "I1113 01:09:53.216338  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633641369 bytes.\n",
      "I1113 01:09:53.216354  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881682068 bytes.\n",
      "I1113 01:09:53.216557  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.72424375ms\n",
      "I1113 01:09:53.216886  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.314338  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 97.5091455ms\n",
      "I1113 01:09:53.323231  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.85974125ms\n",
      "I1113 01:09:53.323346  472071 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:16979283a871623a3d281b0b9270f21a28d117ef6286b74f3b14537619d0e6a4\n",
      "I1113 01:09:53.323350  472071 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):aa1c1352a30ccbc748da1495b023d0f426ef9f5809a474fe304cb1843a7237d8\n",
      "I1113 01:09:53.323353  472071 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.323430  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 110.63778275ms\n",
      "I1113 01:09:53.339301  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633733529 bytes.\n",
      "I1113 01:09:53.339324  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881686676 bytes.\n",
      "I1113 01:09:53.339459  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.395639ms\n",
      "I1113 01:09:53.339765  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.387026  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 47.317129ms\n",
      "I1113 01:09:53.395407  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.349506ms\n",
      "I1113 01:09:53.395518  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:b6b5711b78edbdc54e201ac43c0011040a219857e17516c8454b4dad2cc257b5\n",
      "I1113 01:09:53.395522  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):6c15272f7c6d961dc3e4f8333a985091535d7a42d35e0a284b1f2d90e516239c\n",
      "I1113 01:09:53.395524  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.395601  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 59.568779ms\n",
      "I1113 01:09:53.414061  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97633641369 bytes.\n",
      "I1113 01:09:53.414077  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881682068 bytes.\n",
      "I1113 01:09:53.414330  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.783672ms\n",
      "I1113 01:09:53.414668  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.479125  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 64.5194845ms\n",
      "I1113 01:09:53.489093  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.915961ms\n",
      "I1113 01:09:53.489229  472071 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:e2a77f982dbc41759bad6a5a62299f4620e3798b88aa19ea929e0f57f8137da1\n",
      "I1113 01:09:53.489233  472071 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):940544b6d226901a6b0bd42f7dc7ddf307f6efba6c7299b79c158e3344f9e507\n",
      "I1113 01:09:53.489235  472071 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.489311  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 78.808098ms\n",
      "WARNING:root:Duplicate op registration for aten.__and__\n",
      "WARNING:tpu_inference.layers.vllm.quantization.unquantized:Bias might return incorrect value.\n",
      "I1113 01:09:53.525224  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663224729 bytes.\n",
      "I1113 01:09:53.525240  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161236 bytes.\n",
      "I1113 01:09:53.525355  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.12463275ms\n",
      "I1113 01:09:53.525655  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.529520  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.91614975ms\n",
      "I1113 01:09:53.535914  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.3620775ms\n",
      "I1113 01:09:53.536011  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:f837605375cecab62e1ff72ed75fe7b28165e0d79c899767a5bc8f6476af2d2a\n",
      "I1113 01:09:53.536016  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):aecf3f58c7b2675f7884fcff74aa873715b81dd70db441546fdb446f5900914e\n",
      "I1113 01:09:53.536018  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.536095  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.89548925ms\n",
      "I1113 01:09:53.551176  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 01:09:53.551191  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 01:09:53.551309  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.09593575ms\n",
      "I1113 01:09:53.551586  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.555291  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.745737ms\n",
      "I1113 01:09:53.561547  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.209189ms\n",
      "I1113 01:09:53.561630  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:48026d4012d5d2122b9e5f367958091460957b5e663a1e87573333a3e84e4d6d\n",
      "I1113 01:09:53.561635  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):be1a781d7df97912a673fe3dd504335d63cf8d2ce663b383bd42bfe21cfe073f\n",
      "I1113 01:09:53.561637  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.561713  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.53279125ms\n",
      "I1113 01:09:53.576979  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663237017 bytes.\n",
      "I1113 01:09:53.576997  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161850 bytes.\n",
      "I1113 01:09:53.577116  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.127326ms\n",
      "I1113 01:09:53.577419  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.582776  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.411152ms\n",
      "I1113 01:09:53.589148  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.34084925ms\n",
      "I1113 01:09:53.589238  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:28dc6e32f760358a2f5b441f5c9afa012c9f9b6c26cb66c54f3be6a599da3e18\n",
      "I1113 01:09:53.589242  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):a88d004cdfdf9c929882e26986008d6e4c652320e3aa8d23c7f17b63a3277f22\n",
      "I1113 01:09:53.589244  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.589316  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.36061525ms\n",
      "I1113 01:09:53.604397  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 01:09:53.604413  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 01:09:53.604530  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.13826525ms\n",
      "I1113 01:09:53.604825  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.608562  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.78649925ms\n",
      "I1113 01:09:53.614764  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.17228475ms\n",
      "I1113 01:09:53.614857  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:4ef068d35e80b2e033add7759c098be8cf72b04e78bb8548808e8ebd3e4e8622\n",
      "I1113 01:09:53.614861  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):d047164d75dfc1dcc351397c19f3f5aa95deb1bb9519a4ac877bf5500b07fdc8\n",
      "I1113 01:09:53.614863  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.614934  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.57307725ms\n",
      "I1113 01:09:53.629907  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663233945 bytes.\n",
      "I1113 01:09:53.629922  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161697 bytes.\n",
      "I1113 01:09:53.630066  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1674105ms\n",
      "I1113 01:09:53.630375  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.635037  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.7178795ms\n",
      "I1113 01:09:53.641460  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.3914025ms\n",
      "I1113 01:09:53.641549  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:c61e0273795e22aff2e61154c8ddd754db0e38b9eab82c9941a71a55a05134f8\n",
      "I1113 01:09:53.641553  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):fa2236215bed88d7b9721ab7e2025530211a2ad392e63626bf4030552e61ec48\n",
      "I1113 01:09:53.641555  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.641647  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.7817325ms\n",
      "I1113 01:09:53.656842  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 01:09:53.656859  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 01:09:53.656973  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.077995ms\n",
      "I1113 01:09:53.657271  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.661044  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.8251815ms\n",
      "I1113 01:09:53.667344  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.26997075ms\n",
      "I1113 01:09:53.667427  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:f1544e8c1fb68fd467ed660e22e249aeece16ea525dd5af09e305f26c95d2f76\n",
      "I1113 01:09:53.667431  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):414380cf261e4d585c25b9331ec8ede941ab7f71e4a6baa0b0dc22588b60ffa1\n",
      "I1113 01:09:53.667433  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.667505  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.6405735ms\n",
      "I1113 01:09:53.685020  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663122329 bytes.\n",
      "I1113 01:09:53.685037  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883156116 bytes.\n",
      "I1113 01:09:53.685245  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.5822835ms\n",
      "I1113 01:09:53.685572  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.692994  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 7.4780385ms\n",
      "I1113 01:09:53.699586  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.56096575ms\n",
      "I1113 01:09:53.699689  472071 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:2b5beae33971884e11365a65b0df96d93c9f282836ecd632b0910b0514923d03\n",
      "I1113 01:09:53.699693  472071 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):50b4121e50d8a39a31e63d3d51087be900e11e3cff92c19560b9302821c5c9d2\n",
      "I1113 01:09:53.699695  472071 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.699769  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 18.14278525ms\n",
      "I1113 01:09:53.715368  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234969 bytes.\n",
      "I1113 01:09:53.715393  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161748 bytes.\n",
      "I1113 01:09:53.715508  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.17083725ms\n",
      "I1113 01:09:53.715806  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.721815  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.064426ms\n",
      "I1113 01:09:53.728161  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.31402775ms\n",
      "I1113 01:09:53.728260  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:4c6eeef9e2b841a73fca00c9dce95cf6f080326ddbeaa0ee8a5c6f88cf5cefaa\n",
      "I1113 01:09:53.728264  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):06dc08928344621928fab216265b49ac6d8628351fa7db9fb383046afc29ca9d\n",
      "I1113 01:09:53.728265  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.728339  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 16.03343875ms\n",
      "I1113 01:09:53.745594  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663122329 bytes.\n",
      "I1113 01:09:53.745610  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883156116 bytes.\n",
      "I1113 01:09:53.745847  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.70031325ms\n",
      "I1113 01:09:53.746174  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.752542  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.42379825ms\n",
      "I1113 01:09:53.759056  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.48279525ms\n",
      "I1113 01:09:53.759183  472071 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:3f993349989336765464972db6f2e86c714ef7e8b57143aa814fcb8f920da975\n",
      "I1113 01:09:53.759188  472071 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):1ffa025bd48190bb8a79e303cb1e8b54c0c520b5488306cbca057d7051768344\n",
      "I1113 01:09:53.759190  472071 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.759265  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.163688ms\n",
      "I1113 01:09:53.776210  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97616059289 bytes.\n",
      "I1113 01:09:53.776226  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4880802964 bytes.\n",
      "I1113 01:09:53.776345  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1846275ms\n",
      "I1113 01:09:53.776647  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.790179  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 13.58307575ms\n",
      "I1113 01:09:53.796982  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.77190075ms\n",
      "I1113 01:09:53.797072  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:c2323f7624e6a4c72d12ec85203491fb7e1e1fb0d447274c7a9980b201fe2528\n",
      "I1113 01:09:53.797076  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):f47c59589b6be179b62e2396607a7ab1535818c9ef58e1db9648b9ff31af8c57\n",
      "I1113 01:09:53.797078  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.797175  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 24.04724825ms\n",
      "I1113 01:09:53.813449  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639652249 bytes.\n",
      "I1113 01:09:53.813464  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881982612 bytes.\n",
      "I1113 01:09:53.813594  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.98117325ms\n",
      "I1113 01:09:53.813885  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.860501  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 46.669492ms\n",
      "I1113 01:09:53.868553  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.019819ms\n",
      "I1113 01:09:53.868662  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:4e464aba26b0d6307ab048d9a85ddf000be8e75cc5d2caf3e397af0b8dfe9a68\n",
      "I1113 01:09:53.868666  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):2aed48c8df49d2215e3dceb8146a3e44631ae66de0e61e5e88d7710da076feb6\n",
      "I1113 01:09:53.868667  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.868737  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 58.1495235ms\n",
      "I1113 01:09:53.884013  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639642009 bytes.\n",
      "I1113 01:09:53.884029  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881982100 bytes.\n",
      "I1113 01:09:53.884175  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1861135ms\n",
      "I1113 01:09:53.884453  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:53.933627  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 49.22964875ms\n",
      "I1113 01:09:53.941394  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.692469ms\n",
      "I1113 01:09:53.941503  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:119d30f0866fdd213428108db8264e175a498b187efe94835bf59e79d2f41f53\n",
      "I1113 01:09:53.941507  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2215c3920519e491819185135fc53d7bc85d4ce0bd50a45e8dca3b2c3bb8db94\n",
      "I1113 01:09:53.941509  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:53.941584  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 60.628897ms\n",
      "I1113 01:09:53.956862  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639642009 bytes.\n",
      "I1113 01:09:53.956878  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881982100 bytes.\n",
      "I1113 01:09:53.957020  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.189541ms\n",
      "I1113 01:09:53.957320  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.003763  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 46.49923725ms\n",
      "I1113 01:09:54.011857  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.06120275ms\n",
      "I1113 01:09:54.011970  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:119d30f0866fdd213428108db8264e175a498b187efe94835bf59e79d2f41f53\n",
      "I1113 01:09:54.011973  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2215c3920519e491819185135fc53d7bc85d4ce0bd50a45e8dca3b2c3bb8db94\n",
      "I1113 01:09:54.011976  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.012046  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 58.24576725ms\n",
      "I1113 01:09:54.029279  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97639539609 bytes.\n",
      "I1113 01:09:54.029296  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4881976980 bytes.\n",
      "I1113 01:09:54.029539  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.766023ms\n",
      "I1113 01:09:54.029888  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.111774  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 81.96927075ms\n",
      "I1113 01:09:54.121142  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.30442025ms\n",
      "I1113 01:09:54.121277  472071 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:0df6078050e3ded44bfdd701a62d20ab164c5626d55ac14b26ebb497e1b1c2cf\n",
      "I1113 01:09:54.121281  472071 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):08473352e61faa66a5a90f591a990391af3fbd7ad113c58b8dff0c58d4ef628f\n",
      "I1113 01:09:54.121283  472071 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.121362  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 95.6304005ms\n",
      "I1113 01:09:54.137994  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663232921 bytes.\n",
      "I1113 01:09:54.138010  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161646 bytes.\n",
      "I1113 01:09:54.138131  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.177215ms\n",
      "I1113 01:09:54.138445  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.142255  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.86425875ms\n",
      "I1113 01:09:54.148583  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.29685225ms\n",
      "I1113 01:09:54.148668  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:a060efa22e5c5844e61fd4dfc7ec7c41341484484d11e0d8880b79ed9e55305f\n",
      "I1113 01:09:54.148672  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):5fa5df802b6a203886959eadc2d310d9c6ee6e93759e4c5c8aa5487a91a34a4e\n",
      "I1113 01:09:54.148675  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.148747  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.8269655ms\n",
      "I1113 01:09:54.163862  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663239065 bytes.\n",
      "I1113 01:09:54.163879  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161953 bytes.\n",
      "I1113 01:09:54.164008  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.00961925ms\n",
      "I1113 01:09:54.164301  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.168006  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.74839775ms\n",
      "I1113 01:09:54.174294  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.257567ms\n",
      "I1113 01:09:54.174378  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:7833fc97f1e913958e6f6f567f42cbf7f7f42b2d70d349f0609cfa73490154a7\n",
      "I1113 01:09:54.174381  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):78125be834049c20907f5c87a2e3261275fa5bd6454ba95900533555788da7e2\n",
      "I1113 01:09:54.174383  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.174455  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.48465475ms\n",
      "I1113 01:09:54.189457  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663239065 bytes.\n",
      "I1113 01:09:54.189473  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161953 bytes.\n",
      "I1113 01:09:54.189584  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.04778075ms\n",
      "I1113 01:09:54.189867  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.195944  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.12426825ms\n",
      "I1113 01:09:54.202536  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.56157475ms\n",
      "I1113 01:09:54.202623  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:748f23e329d68bd81b522d2f76e34d3f052263462207b73b06bc8e76e502a842\n",
      "I1113 01:09:54.202626  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):7a6dcfd917b6e26f88a9b730b1d18ff97e6a23fac81ca936e674f0429de19d02\n",
      "I1113 01:09:54.202628  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.202702  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 16.196285ms\n",
      "I1113 01:09:54.217914  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663192473 bytes.\n",
      "I1113 01:09:54.217930  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883159623 bytes.\n",
      "I1113 01:09:54.218111  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.45025675ms\n",
      "I1113 01:09:54.218420  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.227069  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 8.69927825ms\n",
      "I1113 01:09:54.233898  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.7962015ms\n",
      "I1113 01:09:54.234011  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:4dfc6c15fd1934b65f1dda9c62b27b2011a8c0b71be6a1cf96dbbe6343e044b1\n",
      "I1113 01:09:54.234015  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):1236b371dce705646644e1ada82a9bf475545dc91996f0b1c6d317391bba4e45\n",
      "I1113 01:09:54.234017  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.234088  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 19.457131ms\n",
      "I1113 01:09:54.339140  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 95539878809 bytes.\n",
      "I1113 01:09:54.339190  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4776993940 bytes.\n",
      "I1113 01:09:54.339347  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.068701ms\n",
      "I1113 01:09:54.339746  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.396041  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 56.38377725ms\n",
      "I1113 01:09:54.403571  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.468946ms\n",
      "I1113 01:09:54.403689  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:b3ba6325a053ecbb6f9addbc4449554c9cf58e2ff7a22dbb2dca310c38b21420\n",
      "I1113 01:09:54.403694  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):24f89d50687e0e6689ba857e5672f04955a820bf9d7ba9210b477b0ee4185088\n",
      "I1113 01:09:54.403696  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.403823  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 68.593273ms\n",
      "I1113 01:09:54.510936  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577969049 bytes.\n",
      "I1113 01:09:54.510963  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828898452 bytes.\n",
      "I1113 01:09:54.511141  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.30772875ms\n",
      "I1113 01:09:54.511551  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.552939  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 41.4654565ms\n",
      "I1113 01:09:54.560270  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.24755975ms\n",
      "I1113 01:09:54.560369  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:10ec1d0e39903987b226b392e3ae0a6b8326e701ac61b21ab79063ec858f5ac4\n",
      "I1113 01:09:54.560373  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):16e939837fc9dcac1cac38cf27f8d8b496076876605f0b309082e6a19a4f423e\n",
      "I1113 01:09:54.560375  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.560461  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 53.703262ms\n",
      "I1113 01:09:54.578096  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662507929 bytes.\n",
      "I1113 01:09:54.578112  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883125396 bytes.\n",
      "I1113 01:09:54.578229  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.16226325ms\n",
      "I1113 01:09:54.578543  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.586066  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 7.58102225ms\n",
      "I1113 01:09:54.592675  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.534482ms\n",
      "I1113 01:09:54.592774  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:661b06f4f697007005242bb8e0c92a014b6271a707633661de22b6b79703c9e9\n",
      "I1113 01:09:54.592778  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):f1c228e24b6e0981d545569cf87e2530d5e5bf65819bb00da0d09d1393c633c7\n",
      "I1113 01:09:54.592780  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.592862  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.82749775ms\n",
      "I1113 01:09:54.608614  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662868377 bytes.\n",
      "I1113 01:09:54.608629  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143418 bytes.\n",
      "I1113 01:09:54.608743  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.094095ms\n",
      "I1113 01:09:54.609059  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.614483  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.47561875ms\n",
      "I1113 01:09:54.620703  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.1614365ms\n",
      "I1113 01:09:54.620788  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:9be621fdff47b4326661798b415522ffb365d75067437e9bfa2eebcf11bfe103\n",
      "I1113 01:09:54.620791  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):ed0f8738fdcede926890b9ab0a3b476a8f6aeb3aad2c6d889944505f3b7046e6\n",
      "I1113 01:09:54.620794  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.620866  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.24883575ms\n",
      "I1113 01:09:54.636793  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663255449 bytes.\n",
      "I1113 01:09:54.636809  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162772 bytes.\n",
      "I1113 01:09:54.636912  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.869262ms\n",
      "I1113 01:09:54.637211  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.640882  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.72320675ms\n",
      "I1113 01:09:54.646948  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.03672075ms\n",
      "I1113 01:09:54.647026  472071 isa_program_util_common.cc:510] (HLO module jit_iota): Executable fingerprint:57280a4beddf7678a596a27f40caa733a67b820f9649b94003f3c9518fc87ba9\n",
      "I1113 01:09:54.647030  472071 isa_program_util_common.cc:514] (HLO module jit_iota): Executable fingerprint (including data segments):262569d1fe4ffd105c8c2d1bf625e744c2ab8f6a95289cb0b9894d7c9e56f41b\n",
      "I1113 01:09:54.647032  472071 isa_program_util_common.cc:517] (HLO module jit_iota): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.647107  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.0902915ms\n",
      "I1113 01:09:54.662597  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663170969 bytes.\n",
      "I1113 01:09:54.662613  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883158548 bytes.\n",
      "I1113 01:09:54.662789  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.3794855ms\n",
      "I1113 01:09:54.663098  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.667734  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.68118175ms\n",
      "I1113 01:09:54.673953  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.18759125ms\n",
      "I1113 01:09:54.674042  472071 isa_program_util_common.cc:510] (HLO module jit_multiply): Executable fingerprint:190754de3fd259b600c4db676b2c114e15ab6eb5ea56043b52cdc120001bdf66\n",
      "I1113 01:09:54.674046  472071 isa_program_util_common.cc:514] (HLO module jit_multiply): Executable fingerprint (including data segments):1c24fc1e1a6b38bced32a9c341eccfc774976694799298676a3480dee2e87fa7\n",
      "I1113 01:09:54.674048  472071 isa_program_util_common.cc:517] (HLO module jit_multiply): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.674123  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.75138725ms\n",
      "I1113 01:09:54.689752  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663170969 bytes.\n",
      "I1113 01:09:54.689768  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883158548 bytes.\n",
      "I1113 01:09:54.689945  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.3508315ms\n",
      "I1113 01:09:54.690250  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.694809  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.60886075ms\n",
      "I1113 01:09:54.701030  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.17926475ms\n",
      "I1113 01:09:54.701120  472071 isa_program_util_common.cc:510] (HLO module jit_add): Executable fingerprint:17408244a2de47c6d73e33643c8125edbb692a7cb8acf62010dc670a27dbc0d1\n",
      "I1113 01:09:54.701124  472071 isa_program_util_common.cc:514] (HLO module jit_add): Executable fingerprint (including data segments):2c52dbfbae6f7ca42a13d24049ab242eaca1fc0030abcd52b5e7d2d3c6f960e5\n",
      "I1113 01:09:54.701126  472071 isa_program_util_common.cc:517] (HLO module jit_add): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.701200  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.64045075ms\n",
      "I1113 01:09:54.716485  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663232921 bytes.\n",
      "I1113 01:09:54.716501  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161646 bytes.\n",
      "I1113 01:09:54.716612  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.00047325ms\n",
      "I1113 01:09:54.716887  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.721889  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.047556ms\n",
      "I1113 01:09:54.728063  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.144723ms\n",
      "I1113 01:09:54.728145  472071 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:9227d72cd9baf1554241ff5d8296facec42fc4e1d717002d69cbdf7c50be761b\n",
      "I1113 01:09:54.728150  472071 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):2883b206079e1401df8a6d073c19dcf7b8db54b3b335f10601db08c5161ea85a\n",
      "I1113 01:09:54.728151  472071 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.728221  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.64003675ms\n",
      "I1113 01:09:54.745019  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601273753 bytes.\n",
      "I1113 01:09:54.745036  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830063687 bytes.\n",
      "I1113 01:09:54.745402  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.7482965ms\n",
      "I1113 01:09:54.745769  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:54.945388  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 199.6679925ms\n",
      "I1113 01:09:54.963035  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 17.6070255ms\n",
      "I1113 01:09:54.963317  472071 isa_program_util_common.cc:510] (HLO module jit_gather): Executable fingerprint:09250165b686b46e38a36cc99b32f56719aab4db2ae462fb294fc37d748bb1e0\n",
      "I1113 01:09:54.963323  472071 isa_program_util_common.cc:514] (HLO module jit_gather): Executable fingerprint (including data segments):ba7abf873dc4e628cc16da527ed3e7c4971db32707fb08aa0a80c7209c95a99b\n",
      "I1113 01:09:54.963325  472071 isa_program_util_common.cc:517] (HLO module jit_gather): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:54.963411  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 222.7916835ms\n",
      "I1113 01:09:54.980427  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120607129 bytes.\n",
      "I1113 01:09:54.980443  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856030356 bytes.\n",
      "I1113 01:09:54.980583  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.194984ms\n",
      "I1113 01:09:54.980922  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.080791  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 99.92870625ms\n",
      "I1113 01:09:55.089208  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.38780675ms\n",
      "I1113 01:09:55.089318  472071 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:6776705d872a6a419d682a20ca9f99ac30293a6ee6a8e9eafc6d7ba0d9bb1426\n",
      "I1113 01:09:55.089322  472071 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):1301a464cc50019a4037786e265539be0957c2d8bd621c8881a9e23dd30c4d3a\n",
      "I1113 01:09:55.089324  472071 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.089401  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 112.0429005ms\n",
      "I1113 01:09:55.107145  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577876889 bytes.\n",
      "I1113 01:09:55.107186  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828893844 bytes.\n",
      "I1113 01:09:55.107386  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.71481325ms\n",
      "I1113 01:09:55.107730  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.445165  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 337.49384675ms\n",
      "I1113 01:09:55.462524  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 17.32322975ms\n",
      "I1113 01:09:55.462745  472071 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:49174e3b7b00b43f371d4a45c04fc60596b08e4c7a15ea96eb5cdd5ca33630c6\n",
      "I1113 01:09:55.462750  472071 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):9b1bc7eeff20235a16f9bfcef95c8203deadb4c7599d857281d0acb4c5fd39b0\n",
      "I1113 01:09:55.462753  472071 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.462833  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 359.1972985ms\n",
      "I1113 01:09:55.483939  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662588313 bytes.\n",
      "I1113 01:09:55.483955  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883129415 bytes.\n",
      "I1113 01:09:55.484315  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.81684025ms\n",
      "I1113 01:09:55.484718  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.539333  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 54.669863ms\n",
      "I1113 01:09:55.552715  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 13.316519ms\n",
      "I1113 01:09:55.552914  472071 isa_program_util_common.cc:510] (HLO module jit_gather): Executable fingerprint:2c9aa7d52b976bd6e1d355e6b401a95e5542e7a604c9901931d41ba0472ef04f\n",
      "I1113 01:09:55.552919  472071 isa_program_util_common.cc:514] (HLO module jit_gather): Executable fingerprint (including data segments):c2aa889a5799b707ce674d3de95c301dc3cc36c417c79138290ab5f93685cd37\n",
      "I1113 01:09:55.552921  472071 isa_program_util_common.cc:517] (HLO module jit_gather): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.552999  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 73.5378875ms\n",
      "I1113 01:09:55.569202  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1113 01:09:55.569217  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1113 01:09:55.569351  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.06682075ms\n",
      "I1113 01:09:55.569667  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.575300  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.6850545ms\n",
      "I1113 01:09:55.582062  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.721817ms\n",
      "I1113 01:09:55.582156  472071 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:36863f80d229aa82f961a43d1551de01b46350b844dad8d226cc9c5cd413204d\n",
      "I1113 01:09:55.582160  472071 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):dd347b26c2bc1b49ea22dbfc0d22da881488711b8bb2dcad31c0d6b4b3b1b2c8\n",
      "I1113 01:09:55.582162  472071 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.582238  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.9820425ms\n",
      "I1113 01:09:55.599512  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662796697 bytes.\n",
      "I1113 01:09:55.599527  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883139834 bytes.\n",
      "I1113 01:09:55.599702  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.45995225ms\n",
      "I1113 01:09:55.600029  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.614704  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 14.722773ms\n",
      "I1113 01:09:55.621881  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.145516ms\n",
      "I1113 01:09:55.621972  472071 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:862c6c0ad8df8b43a3f58bd93a5f7f096e7e67db184184e45557cb91ad099dd1\n",
      "I1113 01:09:55.621976  472071 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):ed5eb524be4a1623e28a210a74740fb316c3cd58bde02430e600c129dd72919e\n",
      "I1113 01:09:55.621978  472071 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.622053  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 25.847967ms\n",
      "I1113 01:09:55.638516  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601551769 bytes.\n",
      "I1113 01:09:55.638532  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830077588 bytes.\n",
      "I1113 01:09:55.638661  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.18686675ms\n",
      "I1113 01:09:55.638998  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.703146  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 64.20797775ms\n",
      "I1113 01:09:55.712739  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.471268ms\n",
      "I1113 01:09:55.712869  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:678ce0c95e46b45f32a99aad9554b5dae6a6acb5945f6b11b73c47a3aa45f324\n",
      "I1113 01:09:55.712873  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):dd4f0f5ba99c0d52e0631b0719e2d74b90f94f42ec3ecc50f6d2360e7a0078f1\n",
      "I1113 01:09:55.712875  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.712960  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 77.51943075ms\n",
      "I1113 01:09:55.729743  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120596889 bytes.\n",
      "I1113 01:09:55.729759  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856029844 bytes.\n",
      "I1113 01:09:55.729900  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.28801725ms\n",
      "I1113 01:09:55.730321  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.867611  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 137.3976545ms\n",
      "I1113 01:09:55.876240  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.5972425ms\n",
      "I1113 01:09:55.876350  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:7c6411dec771d952b15c6969ad4368c559b7dd92eab7ede3682b368c3c5d81d8\n",
      "I1113 01:09:55.876354  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):5820e82e0ee3f5519b7c218f4f9ad4212128d3921661e3cd92d51ee63e1fd61c\n",
      "I1113 01:09:55.876356  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.876429  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 149.854586ms\n",
      "I1113 01:09:55.892424  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601551769 bytes.\n",
      "I1113 01:09:55.892440  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830077588 bytes.\n",
      "I1113 01:09:55.892570  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.23275875ms\n",
      "I1113 01:09:55.892883  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:55.962777  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 69.9454345ms\n",
      "I1113 01:09:55.972833  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 10.022461ms\n",
      "I1113 01:09:55.972970  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:364c7e6e4b7d3d7b86773283e7f4a3268806d3011042d377be7a9430b204c4f6\n",
      "I1113 01:09:55.972975  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):5a39aa896b46ab09b63e510471eb6f4e729ec58d6095a0c7cd9abdbbb6be8a6a\n",
      "I1113 01:09:55.972977  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:55.973054  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 83.74936075ms\n",
      "I1113 01:09:55.990267  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577897369 bytes.\n",
      "I1113 01:09:55.990284  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828894868 bytes.\n",
      "I1113 01:09:55.990464  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.56066875ms\n",
      "I1113 01:09:55.990874  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.211235  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 220.4094515ms\n",
      "I1113 01:09:56.224768  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 13.498689ms\n",
      "I1113 01:09:56.224947  472071 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:f3bcc3bc248fe1f4c10f9ff7416cb2afaaf95bd09a2376ad65ddb3d25df63d29\n",
      "I1113 01:09:56.224951  472071 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):5f5119da77ec1401e59b41fdb1a440d356002503b570cde3c32db5966cfd4a9f\n",
      "I1113 01:09:56.224953  472071 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.225032  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 238.16613725ms\n",
      "I1113 01:09:56.243706  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577958809 bytes.\n",
      "I1113 01:09:56.243722  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828897940 bytes.\n",
      "I1113 01:09:56.243845  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.4774635ms\n",
      "I1113 01:09:56.244146  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.320087  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 75.9934345ms\n",
      "I1113 01:09:56.328432  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.31378375ms\n",
      "I1113 01:09:56.328542  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:370bf6a664e975e1f371e910fdd416e0e8d28da07cc37c5501c9be4574f3052f\n",
      "I1113 01:09:56.328546  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2857ad7ab3cd3b5b2d0d0237fd1602baaac8aec9dbde47bf28d0883d6f308226\n",
      "I1113 01:09:56.328548  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.328626  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 88.29222375ms\n",
      "W1113 01:09:56.343780  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:09:56.344822  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:09:56.348600  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97391926169 bytes.\n",
      "I1113 01:09:56.348616  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4869596308 bytes.\n",
      "I1113 01:09:56.348747  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.6173265ms\n",
      "I1113 01:09:56.349084  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.433708  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 84.6712245ms\n",
      "I1113 01:09:56.442885  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.1447595ms\n",
      "I1113 01:09:56.443021  472071 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:4ec9574a3186f0ffb26f288cc5af3621746ad8c1d4991c67b4bdb87b270e0864\n",
      "I1113 01:09:56.443025  472071 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):866d8ffa0dcb20f9fbb98cd3fbfa331dea30e2d3bf56faea2f4301d0c4f92bac\n",
      "I1113 01:09:56.443027  472071 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.443104  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 101.02581625ms\n",
      "I1113 01:09:56.461315  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601439129 bytes.\n",
      "I1113 01:09:56.461331  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830071956 bytes.\n",
      "I1113 01:09:56.461584  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.90011875ms\n",
      "I1113 01:09:56.461925  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.604626  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 142.7523175ms\n",
      "I1113 01:09:56.617159  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 12.49973225ms\n",
      "I1113 01:09:56.617332  472071 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:20eead6a6510050c6a06c55122b8125f131bb038ab560287921e1408b4cfd7d9\n",
      "I1113 01:09:56.617336  472071 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):04755dd2a83101e584b1b12e15ff4349abacea6104b328fa99bbddd5f8c7afdf\n",
      "I1113 01:09:56.617339  472071 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.617417  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 159.776904ms\n",
      "W1113 01:09:56.633546  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:09:56.634614  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:09:56.638347  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97527585689 bytes.\n",
      "I1113 01:09:56.638363  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4876379284 bytes.\n",
      "I1113 01:09:56.638485  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.44470425ms\n",
      "I1113 01:09:56.638823  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.693054  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 54.2843005ms\n",
      "I1113 01:09:56.702008  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.9237755ms\n",
      "I1113 01:09:56.702147  472071 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:15c2192ba3aab672e2441fcdcb5dd0fd27e9db3557c987a18c3fc1ddad3f4c7f\n",
      "I1113 01:09:56.702151  472071 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):9d2205b41ad0dfa013595d4516038ea0438d82d20afa73629bebb193bacb8121\n",
      "I1113 01:09:56.702153  472071 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.702231  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 70.2336155ms\n",
      "I1113 01:09:56.721323  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120484249 bytes.\n",
      "I1113 01:09:56.721339  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856024212 bytes.\n",
      "I1113 01:09:56.721590  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.93228675ms\n",
      "I1113 01:09:56.721935  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.841459  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 119.5755755ms\n",
      "I1113 01:09:56.853269  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.77677175ms\n",
      "I1113 01:09:56.853439  472071 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:0b8c4880d23345669941a617135b96ba768a456ede951744195f175a759afdb0\n",
      "I1113 01:09:56.853443  472071 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):268fe7ac84d0ca5d34860663e970705125da83ee54688b8709aa7c3fdff9e183\n",
      "I1113 01:09:56.853445  472071 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.853521  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 135.90882925ms\n",
      "I1113 01:09:56.871062  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662876569 bytes.\n",
      "I1113 01:09:56.871078  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143828 bytes.\n",
      "I1113 01:09:56.871226  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.187706ms\n",
      "I1113 01:09:56.871537  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.876862  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.37386025ms\n",
      "I1113 01:09:56.883076  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.18367875ms\n",
      "I1113 01:09:56.883184  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:40f0cccc83b28e24379e13c409b003ccad4ccc778086a097887634a9ee5403dd\n",
      "I1113 01:09:56.883189  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):ee1a3428c3f31f125fa99f61c620127bcb3685b1b77f124b4e32b507ade2f66d\n",
      "I1113 01:09:56.883191  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.883264  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.2602795ms\n",
      "I1113 01:09:56.898688  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1113 01:09:56.898703  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1113 01:09:56.898814  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.0755245ms\n",
      "I1113 01:09:56.899111  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.962138  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 63.0782335ms\n",
      "I1113 01:09:56.973386  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.21700225ms\n",
      "I1113 01:09:56.973570  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:0dde0b84570d4f8b311fbcbd7ee430f7919a3793377646a02f6711db334be04a\n",
      "I1113 01:09:56.973575  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):9a0eb996550b3c697d53556710a42baa13eea2d2f3345cf7f3b7507136739d51\n",
      "I1113 01:09:56.973577  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:56.973654  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 77.94765425ms\n",
      "I1113 01:09:56.989279  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662876569 bytes.\n",
      "I1113 01:09:56.989295  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143828 bytes.\n",
      "I1113 01:09:56.989414  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1410165ms\n",
      "I1113 01:09:56.989721  472936 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:56.998983  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.31003075ms\n",
      "I1113 01:09:57.005569  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.55750425ms\n",
      "I1113 01:09:57.005655  472071 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:fe2efdcb3d52d0c9503a29b1fd9d8498cc7667239778b75c81aad27049feb87e\n",
      "I1113 01:09:57.005659  472071 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):920a107875ed240ea0640763eaf114f4317f940c6737c362f05502e10852b963\n",
      "I1113 01:09:57.005661  472071 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:57.005731  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 19.4915705ms\n",
      "I1113 01:09:57.021741  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662780313 bytes.\n",
      "I1113 01:09:57.021757  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883139015 bytes.\n",
      "I1113 01:09:57.021939  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.4481115ms\n",
      "I1113 01:09:57.022278  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:57.060018  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 37.7971315ms\n",
      "I1113 01:09:57.068534  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.43860675ms\n",
      "I1113 01:09:57.068640  472071 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:b24c7be130ac8e12437a95a6cd7ecef069e71d4550e553c3b4b8122e2c8964de\n",
      "I1113 01:09:57.068644  472071 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):762f9655e81f3a87d7f24e295dea3151ad562ea2dc123a7bf39dcd65ced3fa04\n",
      "I1113 01:09:57.068646  472071 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:57.068747  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.289632ms\n",
      "I1113 01:09:57.084655  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662851993 bytes.\n",
      "I1113 01:09:57.084671  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883142599 bytes.\n",
      "I1113 01:09:57.084785  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.126346ms\n",
      "I1113 01:09:57.085086  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:57.266660  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 181.625245ms\n",
      "I1113 01:09:57.283496  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 16.803621ms\n",
      "I1113 01:09:57.283669  472071 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:c10c422fe9795af2a983e3fbe2b1ecc9ca4f388bf744c3ef546de1d2c94bb1d7\n",
      "I1113 01:09:57.283673  472071 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):1bc38b4ce2c53b8b9cc6361d15490201ae59dd6487e03a820d4ed55c7c99a084\n",
      "I1113 01:09:57.283676  472071 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:57.283757  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 202.1300815ms\n",
      "W1113 01:09:57.298185  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:09:57.299112  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:09:57.302684  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663146905 bytes.\n",
      "I1113 01:09:57.302701  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883157345 bytes.\n",
      "I1113 01:09:57.302835  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.20799275ms\n",
      "I1113 01:09:57.303196  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:57.312431  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.308564ms\n",
      "I1113 01:09:57.319315  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.85336975ms\n",
      "I1113 01:09:57.319422  472071 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:aaf05ebded97981a3587ddd4f9bbc84f9643aeaf3c797f818835483d598e1dc1\n",
      "I1113 01:09:57.319426  472071 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):2d51af4641d8d01a339154a7f5c2762e2c452cbecf4787fa69aa7b718d1c2faa\n",
      "I1113 01:09:57.319428  472071 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:57.319495  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 22.911782ms\n",
      "I1113 01:09:57.337302  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662763929 bytes.\n",
      "I1113 01:09:57.337318  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883138196 bytes.\n",
      "I1113 01:09:57.337557  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.706963ms\n",
      "I1113 01:09:57.337890  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:57.355445  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 17.6102085ms\n",
      "I1113 01:09:57.362601  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.125464ms\n",
      "I1113 01:09:57.362702  472071 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:8685cfc07fa56f2461e517612b8269a1d1a3d31761d46a417de0fd787c225930\n",
      "I1113 01:09:57.362706  472071 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):fd21cb59c09ca83a49ec9b16477e055f65a0dfb690d750753d3800d0814646f5\n",
      "I1113 01:09:57.362708  472071 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:57.362785  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 28.97815325ms\n",
      "W1113 01:09:57.377904  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:09:57.378827  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:09:57.382312  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1113 01:09:57.382327  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1113 01:09:57.382456  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.98260375ms\n",
      "I1113 01:09:57.382778  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:09:57.391915  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.1801855ms\n",
      "I1113 01:09:57.398954  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.008744ms\n",
      "I1113 01:09:57.399060  472071 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:95fc20b3d91139158e400b9720ffa468a4909097f9a8e569f181ab7417efc621\n",
      "I1113 01:09:57.399064  472071 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):992271494bf12f27ce7deca100ad97747b4b2eaa47779baf466c5bbf7a133266\n",
      "I1113 01:09:57.399066  472071 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:09:57.399142  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 22.70941425ms\n",
      "I1113 01:10:03.652248  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663244697 bytes.\n",
      "I1113 01:10:03.652276  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162234 bytes.\n",
      "I1113 01:10:03.652428  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.225655ms\n",
      "I1113 01:10:03.652886  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:03.657554  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.74516475ms\n",
      "I1113 01:10:03.665395  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.804113ms\n",
      "I1113 01:10:03.665481  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:7075cbfbd75beffcf249e571e6a4bddb8379e01aac94c4d7c47a7f9ca1704518\n",
      "I1113 01:10:03.665485  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):b8441eb9ca29849d647247aaa9c78c1a597aabdd6aa0aa919b29de261bbda7f2\n",
      "I1113 01:10:03.665488  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:03.665631  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 19.47964975ms\n",
      "I1113 01:10:03.682307  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663244697 bytes.\n",
      "I1113 01:10:03.682326  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162234 bytes.\n",
      "I1113 01:10:03.682442  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.0745015ms\n",
      "I1113 01:10:03.682765  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:03.686779  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.05732625ms\n",
      "I1113 01:10:03.693829  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.01902625ms\n",
      "I1113 01:10:03.693908  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:375f551ac122c4c1443de2d6757a8e6ee695c8bd10bcc876cb4acb15627a65ca\n",
      "I1113 01:10:03.693912  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):4c8eb4dd58c79142733686c9b511662531ea17d536c2a918c926dac69fcf2fbe\n",
      "I1113 01:10:03.693914  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:03.694016  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.68032575ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:10:04 [tpu_runner.py:497] Init model | hbm=[(10.11, 95.74), (10.11, 95.74), (10.11, 95.74), (10.11, 95.74)]GiB\n",
      "INFO 11-13 01:10:04 [tpu_worker.py:174] Memory statistics | total_hbm_limit_gb=382.97GiB | total_hbm_limit_cap_gb=344.68GiB | total_hbm_used_gb=40.43GiB | total_hbm_avail_gb=304.24GiB\n",
      "WARNING 11-13 01:10:04 [kv_cache_utils.py:1095] Hybrid KV cache manager is disabled for this hybrid model, This means we do not enable any optimizations for saving KV cache memory (e.g., dropping the KV cache outside the sliding window). The compute of layers like sliding window is still saved.\n",
      "INFO 11-13 01:10:04 [kv_cache_utils.py:1229] GPU KV cache size: 6,646,224 tokens\n",
      "INFO 11-13 01:10:04 [kv_cache_utils.py:1234] Maximum concurrency for 128 tokens per request: 51923.62x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 01:10:04.111294  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1113 01:10:04.111319  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1113 01:10:04.111499  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.23508425ms\n",
      "I1113 01:10:04.111932  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.117554  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.71103225ms\n",
      "I1113 01:10:04.124729  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.13755825ms\n",
      "I1113 01:10:04.124848  472071 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:13d169e8f0cb52132827f3458861645b2ad7cf1a3afcb1127a6444697d9ec7eb\n",
      "I1113 01:10:04.124853  472071 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):7274c5d1d7ca793892a3a02db907049e506cf3ed81768774f478da0a45758e78\n",
      "I1113 01:10:04.124855  472071 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.125016  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.80077025ms\n",
      "I1113 01:10:04.198530  471069 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 166 instructions, modules: jit__threefry_fold_in\n",
      "I1113 01:10:04.198546  471069 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__threefry_fold_in instructions: 166 fingerprint: \n",
      "I1113 01:10:04.199238  471069 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__threefry_fold_in instructions: 153\n",
      "I1113 01:10:04.199244  471069 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1113 01:10:04.210424  471069 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1113 01:10:04.212497  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97661667225 bytes.\n",
      "I1113 01:10:04.212509  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883083361 bytes.\n",
      "I1113 01:10:04.213742  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 16.40667625ms\n",
      "I1113 01:10:04.214486  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.233493  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 19.0566255ms\n",
      "I1113 01:10:04.236103  472071 2a886c8_compiler_base.cc:3045] final program bundle count: 633 note this count does not reflect cycles spent executing delays.\n",
      "I1113 01:10:04.241028  472071 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1113 01:10:04.242571  472071 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (76.5K).\n",
      "I1113 01:10:04.242845  472071 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__threefry_fold_in\n",
      "I1113 01:10:04.242850  472071 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 156.5K / 95.74G\n",
      "I1113 01:10:04.242854  472071 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 4.5K / 64.00M\n",
      "I1113 01:10:04.242864  472071 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.16M:\n",
      "I1113 01:10:04.242867  472071 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1113 01:10:04.242868  472071 2a886c8_compiler_base.cc:3549]     program          156.5K \n",
      "I1113 01:10:04.242869  472071 2a886c8_compiler_base.cc:3549]     arguments          1.0K \n",
      "I1113 01:10:04.242870  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:10:04.242872  472071 2a886c8_compiler_base.cc:3549] Output size 512B; shares 0B with arguments.\n",
      "I1113 01:10:04.242873  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:10:04.242874  472071 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1113 01:10:04.242875  472071 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1113 01:10:04.242877  472071 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1113 01:10:04.242878  472071 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1113 01:10:04.242879  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:10:04.242880  472071 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1113 01:10:04.242881  472071 2a886c8_compiler_base.cc:3549] \n",
      "I1113 01:10:04.242893  472071 2a886c8_compiler_base.cc:3553] Program sflag requirement 220B:\n",
      "I1113 01:10:04.242894  472071 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1113 01:10:04.242895  472071 2a886c8_compiler_base.cc:3553]     scoped              12B\n",
      "I1113 01:10:04.242896  472071 2a886c8_compiler_base.cc:3553]     HLO temp             4B (100.0% utilization: Unpadded (4B) Padded (4B), 0.0% fragmentation (0B))\n",
      "I1113 01:10:04.242898  472071 2a886c8_compiler_base.cc:3553] Program hbm requirement 156.5K:\n",
      "I1113 01:10:04.242899  472071 2a886c8_compiler_base.cc:3553]     HLO temp          80.0K (4.8% utilization: Unpadded (76B) Padded (1.6K), 98.0% fragmentation (78.4K))\n",
      "I1113 01:10:04.242901  472071 2a886c8_compiler_base.cc:3553]     overlays          76.5K\n",
      "I1113 01:10:04.242902  472071 2a886c8_compiler_base.cc:3553] Program vmem requirement 4.5K:\n",
      "I1113 01:10:04.242903  472071 2a886c8_compiler_base.cc:3553]     scoped             4.0K\n",
      "I1113 01:10:04.242904  472071 2a886c8_compiler_base.cc:3553]     HLO temp           512B (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (512B))\n",
      "I1113 01:10:04.242905  472071 2a886c8_compiler_base.cc:3553] Program smem requirement 40B:\n",
      "I1113 01:10:04.242906  472071 2a886c8_compiler_base.cc:3553]     scoped              40B\n",
      "I1113 01:10:04.242907  472071 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1113 01:10:04.242908  472071 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 1.9K / 1.00M (2 parameters)\n",
      "I1113 01:10:04.242921  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.3943975ms\n",
      "I1113 01:10:04.243095  472071 isa_program_util_common.cc:510] (HLO module jit__threefry_fold_in): Executable fingerprint:9bb788d9827517b4bb3d64a21746b99ef4adaed7811785123bfd9d2b576d9501\n",
      "I1113 01:10:04.243098  472071 isa_program_util_common.cc:514] (HLO module jit__threefry_fold_in): Executable fingerprint (including data segments):5a622cfb5e76d5a9edfea31742dad4d10b548b95d9f0f1a078ff626a5ee2b500\n",
      "I1113 01:10:04.243100  472071 isa_program_util_common.cc:517] (HLO module jit__threefry_fold_in): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.243204  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.154593ms\n",
      "I1113 01:10:04.263872  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663203225 bytes.\n",
      "I1113 01:10:04.263888  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883160161 bytes.\n",
      "I1113 01:10:04.264046  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.27788325ms\n",
      "I1113 01:10:04.264543  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.269573  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.08491775ms\n",
      "I1113 01:10:04.276347  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.74532375ms\n",
      "I1113 01:10:04.276458  472071 isa_program_util_common.cc:510] (HLO module jit_add): Executable fingerprint:1d37c80f028627082acffcad0f66d83660c0f7d233cd5625ea0f63148a6071a2\n",
      "I1113 01:10:04.276462  472071 isa_program_util_common.cc:514] (HLO module jit_add): Executable fingerprint (including data segments):762e771bcbd93219353261aa62fb966518be5395f8616ec2362defda1b7ad905\n",
      "I1113 01:10:04.276464  472071 isa_program_util_common.cc:517] (HLO module jit_add): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.276538  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.80360375ms\n",
      "W1113 01:10:04.297498  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.298600  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.303019  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.303044  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.303187  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 7.2021545ms\n",
      "I1113 01:10:04.303509  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.336649  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.184264ms\n",
      "I1113 01:10:04.344765  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.08181325ms\n",
      "I1113 01:10:04.344879  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.344883  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.344885  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.344957  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.02036025ms\n",
      "W1113 01:10:04.362118  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.363101  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.367188  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.367205  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.367319  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.6876445ms\n",
      "I1113 01:10:04.367800  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.400828  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.08102175ms\n",
      "I1113 01:10:04.409425  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.56570475ms\n",
      "I1113 01:10:04.409540  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.409544  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.409546  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.409625  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.04004175ms\n",
      "W1113 01:10:04.426084  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.427062  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.431135  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.431170  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.431292  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.6963755ms\n",
      "I1113 01:10:04.431815  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.465226  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.47034425ms\n",
      "I1113 01:10:04.474177  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.91917775ms\n",
      "I1113 01:10:04.474292  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.474296  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.474298  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.474372  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.82189525ms\n",
      "W1113 01:10:04.493756  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.494762  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.498836  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.498853  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.498970  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.74710225ms\n",
      "I1113 01:10:04.499379  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.532371  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.04439725ms\n",
      "I1113 01:10:04.541257  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.760076ms\n",
      "I1113 01:10:04.541372  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.541375  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.541377  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.541451  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.27936875ms\n",
      "W1113 01:10:04.558601  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.559868  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.563867  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.563883  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.564000  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.89857425ms\n",
      "I1113 01:10:04.564535  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.597822  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.343667ms\n",
      "I1113 01:10:04.606837  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.98310975ms\n",
      "I1113 01:10:04.606954  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.606959  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.606961  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.607031  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.974642ms\n",
      "W1113 01:10:04.623710  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.624709  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.628816  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.628832  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.628961  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.81999775ms\n",
      "I1113 01:10:04.629644  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.662920  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.32714475ms\n",
      "I1113 01:10:04.671862  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.9115755ms\n",
      "I1113 01:10:04.671980  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.671984  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.671986  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.672061  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.97168425ms\n",
      "W1113 01:10:04.689509  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.690516  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.694661  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.694676  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.694790  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.79930075ms\n",
      "I1113 01:10:04.695388  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.728748  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.427019ms\n",
      "I1113 01:10:04.738124  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.34298625ms\n",
      "I1113 01:10:04.738241  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.738246  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.738248  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.738324  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.38262475ms\n",
      "W1113 01:10:04.757608  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.758632  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.763324  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.763340  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.763464  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 7.447928ms\n",
      "I1113 01:10:04.764034  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.797814  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.8441625ms\n",
      "I1113 01:10:04.805921  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.0761555ms\n",
      "I1113 01:10:04.806034  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.806038  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.806040  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.806121  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.16442975ms\n",
      "W1113 01:10:04.823783  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.824790  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.829025  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.829041  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.829157  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.91446975ms\n",
      "I1113 01:10:04.829676  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.862413  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.79445325ms\n",
      "I1113 01:10:04.870563  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.11893625ms\n",
      "I1113 01:10:04.870711  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.870715  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.870717  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.870794  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.6013325ms\n",
      "W1113 01:10:04.887351  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.888334  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.892344  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.892365  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.892642  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.82277425ms\n",
      "I1113 01:10:04.893036  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.925428  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.448927ms\n",
      "I1113 01:10:04.933626  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.07757025ms\n",
      "I1113 01:10:04.933741  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.933745  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.933747  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.933820  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.0493105ms\n",
      "W1113 01:10:04.952188  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:04.953166  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:04.957181  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:04.957356  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:04.957478  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8272ms\n",
      "I1113 01:10:04.957869  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:04.990368  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.56090475ms\n",
      "I1113 01:10:04.998474  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.0752525ms\n",
      "I1113 01:10:04.998589  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:04.998593  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:04.998595  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:04.998665  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.0627645ms\n",
      "W1113 01:10:05.015880  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.016878  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.020941  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.020957  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.021075  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.88667275ms\n",
      "I1113 01:10:05.021464  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.053903  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.495521ms\n",
      "I1113 01:10:05.061732  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.79778175ms\n",
      "I1113 01:10:05.061845  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.061849  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.061851  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.061924  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.78504125ms\n",
      "W1113 01:10:05.079006  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.080025  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.084283  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.084300  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.084413  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.91643725ms\n",
      "I1113 01:10:05.084939  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.117452  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.56519525ms\n",
      "I1113 01:10:05.125335  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.85237375ms\n",
      "I1113 01:10:05.125451  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.125455  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.125457  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.125531  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.0795365ms\n",
      "W1113 01:10:05.143267  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.144285  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.148292  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.148308  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.148424  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.99558925ms\n",
      "I1113 01:10:05.148844  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.181284  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.503993ms\n",
      "I1113 01:10:05.189214  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.89937375ms\n",
      "I1113 01:10:05.189327  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.189331  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.189333  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.189401  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.08855725ms\n",
      "W1113 01:10:05.206597  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.207629  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.211747  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.211763  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.211882  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8149305ms\n",
      "I1113 01:10:05.212271  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.245037  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.81572875ms\n",
      "I1113 01:10:05.252905  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.8369855ms\n",
      "I1113 01:10:05.253018  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.253022  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.253024  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.253097  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.07787425ms\n",
      "W1113 01:10:05.270409  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.271519  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.275534  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.275551  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.275666  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.93467875ms\n",
      "I1113 01:10:05.276076  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.308578  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.55777525ms\n",
      "I1113 01:10:05.316575  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.9668945ms\n",
      "I1113 01:10:05.316689  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.316693  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.316695  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.316765  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.08291575ms\n",
      "W1113 01:10:05.335974  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.336963  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.341136  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.341151  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.341269  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8659995ms\n",
      "I1113 01:10:05.341801  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.374342  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.5964375ms\n",
      "I1113 01:10:05.382353  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.97982575ms\n",
      "I1113 01:10:05.382468  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.382472  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.382474  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.382547  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.1938085ms\n",
      "W1113 01:10:05.399749  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.400736  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.404751  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.404767  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.404881  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8096445ms\n",
      "I1113 01:10:05.405288  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.437713  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.48263875ms\n",
      "I1113 01:10:05.445714  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.8745005ms\n",
      "I1113 01:10:05.445827  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.445831  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.445833  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.445911  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.88644075ms\n",
      "W1113 01:10:05.462614  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.463621  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.467682  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.467853  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.467983  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.88442975ms\n",
      "I1113 01:10:05.468358  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.500741  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.43300625ms\n",
      "I1113 01:10:05.508732  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.86936175ms\n",
      "I1113 01:10:05.508848  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.508852  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.508854  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.508926  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.87401025ms\n",
      "W1113 01:10:05.525533  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.526534  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.530589  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.530604  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.530717  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.91261725ms\n",
      "I1113 01:10:05.531119  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.563586  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.526321ms\n",
      "I1113 01:10:05.571509  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.8922885ms\n",
      "I1113 01:10:05.571623  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.571627  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.571628  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.571697  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.9377615ms\n",
      "W1113 01:10:05.588886  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.589910  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.594199  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.594216  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.594336  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.96301725ms\n",
      "I1113 01:10:05.594927  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.627433  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.55951825ms\n",
      "I1113 01:10:05.635379  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.91569ms\n",
      "I1113 01:10:05.635495  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.635498  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.635500  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.635573  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.2464685ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:10:05 [kv_cache_manager.py:216] Init kv-cache | num_layers=24 | shape=(num_blocks, (16, 4, 2, 128)) | num_blocks=[415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389, 415389] | sharding=NamedSharding(mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto)), spec=PartitionSpec('data', None, 'model'), memory_kind=device) | dtype=bfloat16 | hbm=[(86.17, 95.74), (86.17, 95.74), (86.17, 95.74), (86.17, 95.74)]Gb\n",
      "INFO 11-13 01:10:05 [core.py:247] init engine (profile, create kv cache, warmup model) took 1.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1113 01:10:05.652745  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.653743  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.657830  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.657881  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.658000  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.95848625ms\n",
      "I1113 01:10:05.658409  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.690725  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.372089ms\n",
      "I1113 01:10:05.698675  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.91884775ms\n",
      "I1113 01:10:05.698787  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.698791  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.698793  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.698865  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.87402825ms\n",
      "W1113 01:10:05.717593  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.718595  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.722713  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.722729  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.722849  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 8.7076115ms\n",
      "I1113 01:10:05.723252  472934 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.755661  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.45585575ms\n",
      "I1113 01:10:05.763586  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.89378125ms\n",
      "I1113 01:10:05.763702  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.763706  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.763708  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.763779  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.68411425ms\n",
      "W1113 01:10:05.780778  472071 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1113 01:10:05.781769  472071 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1113 01:10:05.785783  471069 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1113 01:10:05.785799  471069 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1113 01:10:05.785921  471069 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.79124675ms\n",
      "I1113 01:10:05.786329  472935 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1113 01:10:05.818706  472071 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.4341455ms\n",
      "I1113 01:10:05.826622  472071 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.88517ms\n",
      "I1113 01:10:05.826735  472071 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a1e709ee866ff56f07b7c2a13f2166de39d9c05ba54e9cc5a627d730d7ea3bd3\n",
      "I1113 01:10:05.826739  472071 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):58c25d513f2fd8fa025cedd69c3133645dc72f06a688e259b9be9b986b56d238\n",
      "I1113 01:10:05.826741  472071 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1113 01:10:05.826815  471069 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.74036425ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 01:10:08 [llm.py:353] Supported tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "import os\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"\n",
    "os.environ[\"JAX_RANDOM_WEIGHTS\"] = \"False\"\n",
    "os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n",
    "\n",
    "os.environ[\"TPU_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TPU_STDERR_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"VLLM_MLA_DISABLE\"] = \"1\"\n",
    "os.environ[\"MODEL_IMPL_TYPE\"] = \"vllm\"\n",
    "\n",
    "MODEL = \"unsloth/gpt-oss-20b-BF16\"\n",
    "\n",
    "golden_llm = LLM(\n",
    "    MODEL,\n",
    "    max_model_len=128,\n",
    "    tensor_parallel_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8016b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_state = golden_llm.llm_engine.model_executor.driver_worker.model_runner.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c753071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vllm_model.lm_head.weight': Array([[-0.00601196, 0.000736237, 0.00131226, ..., 0.0057373,\n",
       "         -0.00405884, -0.00408936],\n",
       "        [-0.0134888, -0.00188446, -0.00390625, ..., 0.00224304,\n",
       "         0.00366211, -0.00656128],\n",
       "        [-0.00445557, -0.00765991, 0.00671387, ..., -0.00271606,\n",
       "         0.00457764, -0.010376],\n",
       "        ...,\n",
       "        [0.000774384, 6.07967e-05, 0.000255585, ..., 9.05991e-05,\n",
       "         0.000225067, -0.000237465],\n",
       "        [-2.46763e-05, -0.000463486, 0.000134468, ..., -0.000155449,\n",
       "         -6.34193e-05, -7.34329e-05],\n",
       "        [-8.01086e-05, 0.000132561, -7.86781e-06, ..., -5.34058e-05,\n",
       "         -0.000188828, -0.000339508]], dtype=bfloat16),\n",
       " 'vllm_model.model.embedding.weight': Array([[-0.298828, -0.695312, 1.91406, ..., -0.100586, -0.0289307,\n",
       "         0.101074],\n",
       "        [-0.332031, -0.363281, 1.91406, ..., 0.059082, 0.0235596,\n",
       "         0.198242],\n",
       "        [0.208008, 0.000225067, 0.474609, ..., 0.00708008, 0.0334473,\n",
       "         0.365234],\n",
       "        ...,\n",
       "        [-0.0161133, -0.0168457, 0.0722656, ..., -0.00238037, 0.0017395,\n",
       "         0.00361633],\n",
       "        [-0.00494385, 0.0224609, -0.0327148, ..., 0.000774384, 0.00125885,\n",
       "         -0.00448608],\n",
       "        [0.00540161, -0.00811768, 0.0072937, ..., -0.00260925, -0.0039978,\n",
       "         -0.00830078]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.attn.o_proj.bias': Array([0.0219727, -0.0117188, 0.0213623, ..., -0.00184631, -0.0405273,\n",
       "        -0.043457], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.attn.o_proj.weight': Array([[-0.000663757, -0.00162506, -0.00198364, ..., -0.00531006,\n",
       "         -0.00254822, -0.00328064],\n",
       "        [0.000350952, 0.00512695, 0.00180054, ..., 0.0127563, 0.0039978,\n",
       "         0.00576782],\n",
       "        [0.00744629, 0.00259399, 0.00842285, ..., -0.000724792,\n",
       "         -0.00506592, 0.000167847],\n",
       "        ...,\n",
       "        [0.00308228, 0.0035553, 0.00302124, ..., 0.0043335, 0.0015564,\n",
       "         0.00201416],\n",
       "        [0.00265503, -0.000564575, -0.00216675, ..., 0.000488281,\n",
       "         -0.0038147, -0.00367737],\n",
       "        [0.00172424, -0.00241089, 0.000537872, ..., -0.00175476,\n",
       "         -0.00653076, 8.01086e-05]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.attn.qkv_proj.bias': Array([-0.03125, -0.0175781, -0.248047, ..., -0.0556641, 0.373047,\n",
       "        0.0649414], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.attn.qkv_proj.weight': Array([[-0.00011301, 0.00509644, 0.00549316, ..., -0.0201416, -0.036377,\n",
       "         -0.0129395],\n",
       "        [-0.000926971, -0.000431061, 0.000778198, ..., 0.00180817,\n",
       "         -0.0164795, -0.00111389],\n",
       "        [0.00179291, -0.012207, 0.00257874, ..., -0.0019455, -0.00830078,\n",
       "         -0.0202637],\n",
       "        ...,\n",
       "        [0.00144196, 0.0256348, -0.0167236, ..., 0.0825195, -0.0292969,\n",
       "         0.0683594],\n",
       "        [-0.0478516, 0.00427246, 0.0349121, ..., -0.0634766, 0.103516,\n",
       "         -0.0424805],\n",
       "        [-0.0117798, -0.0164795, -0.0349121, ..., -0.125977, 0.0488281,\n",
       "         0.0251465]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.attn.rotary_emb.cos_sin_cache': Array([[ 1.34657359e+00,  1.34657359e+00,  1.34657359e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 7.27556884e-01,  1.03935814e+00,  1.19763231e+00, ...,\n",
       "          8.57526231e-07,  5.90873583e-07,  4.07138060e-07],\n",
       "        [-5.60372353e-01,  2.57892013e-01,  7.83756495e-01, ...,\n",
       "          1.71505246e-06,  1.18174717e-06,  8.14276120e-07],\n",
       "        ...,\n",
       "        [-2.45971248e-01, -7.50365615e-01,  1.15514159e+00, ...,\n",
       "          1.12264656e-01,  7.74025247e-02,  5.33492155e-02],\n",
       "        [-1.24693739e+00,  1.40130743e-01,  7.08978295e-01, ...,\n",
       "          1.12265497e-01,  7.74031132e-02,  5.33496216e-02],\n",
       "        [-1.10147500e+00,  9.58203316e-01,  1.10065356e-01, ...,\n",
       "          1.12266362e-01,  7.74037018e-02,  5.33500277e-02]],      dtype=float32),\n",
       " 'vllm_model.model.layers.0.attn.sinks': Array([2.51562, 0.558594, 1.71875, 0.910156, 0.96875, 0.695312, -0.988281,\n",
       "        1.42188, 0.992188, 1.55469, 1.84375, 0.988281, 1.59375, 0.6875,\n",
       "        1.34375, 1.26562, -0.078125, 0.326172, -0.0458984, 0.589844,\n",
       "        0.691406, 0.427734, 0.554688, 1.0625, 2.92188, 0.269531, 0.628906,\n",
       "        1.3125, 2.92188, 3.03125, 2.14062, 1.90625, 1.58594, 2.5, 0.605469,\n",
       "        -2.45312, 0.445312, 2.51562, 1.09375, -0.589844, -0.554688,\n",
       "        0.326172, 0.710938, 0.714844, 0.457031, 0.964844, 0.691406,\n",
       "        0.691406, 0.193359, 1.46094, 2, 4.0625, -1.59375, 1.3125, 2.17188,\n",
       "        2.14062, 3.51562, 1.21875, 0.9375, 1.57812, 1.72656, 1.19531,\n",
       "        2.35938, 0.714844], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.input_layernorm.weight': Array([1.39062, 1.25781, 1.07812, ..., 5.46875, 4.78125, 2], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.mlp.experts.w13_bias': Array([[-0.742188, -0.417969, -0.488281, ..., -0.890625, -0.75,\n",
       "         -0.917969],\n",
       "        [-0.507812, -0.6875, -0.621094, ..., -0.847656, -0.859375,\n",
       "         -0.882812],\n",
       "        [-0.757812, -0.351562, -0.416016, ..., -0.820312, -0.859375,\n",
       "         -0.90625],\n",
       "        ...,\n",
       "        [-0.554688, -0.742188, -0.730469, ..., -0.886719, -0.953125,\n",
       "         -0.898438],\n",
       "        [-1.78125, -1.89844, -0.365234, ..., -0.488281, -1.05469,\n",
       "         -0.800781],\n",
       "        [-0.53125, -0.279297, -0.628906, ..., -0.707031, -1.0625,\n",
       "         -0.820312]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.mlp.experts.w13_weight': Array([[[0, 0, 0, ..., 0.03125, -0, 0.0625],\n",
       "         [0.0234375, 0.015625, 0.0234375, ..., -0, -0, -0.015625],\n",
       "         [-0.03125, 0.015625, 0.03125, ..., 0.0625, 0.09375, -0.03125],\n",
       "         ...,\n",
       "         [-0.015625, 0.0234375, -0.0078125, ..., 0.03125, -0.03125,\n",
       "          0.09375],\n",
       "         [0.0625, -0.0625, 0.0078125, ..., 0.03125, 0.0625, -0.046875],\n",
       "         [-0.0078125, -0, 0.03125, ..., 0.046875, 0.046875, -0.09375]],\n",
       " \n",
       "        [[-0.015625, 0, 0.015625, ..., -0.03125, 0.125, 0],\n",
       "         [0, 0.03125, -0.015625, ..., 0.125, -0.09375, -0.015625],\n",
       "         [-0.03125, 0.03125, 0.015625, ..., -0.0625, -0, 0],\n",
       "         ...,\n",
       "         [0.015625, 0.03125, 0.015625, ..., -0.125, 0.046875, 0.0625],\n",
       "         [-0.046875, -0.015625, 0, ..., 0.0625, -0.03125, -0.0625],\n",
       "         [0, -0.03125, 0, ..., -0.09375, -0, 0.03125]],\n",
       " \n",
       "        [[-0.03125, -0.0078125, -0.03125, ..., 0.03125, -0.0625,\n",
       "          0.015625],\n",
       "         [-0.015625, 0.0234375, -0.0078125, ..., -0.0234375, -0,\n",
       "          0.015625],\n",
       "         [-0.046875, -0.015625, 0.0078125, ..., -0.046875, -0.03125,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [0, 0.015625, -0.015625, ..., -0.03125, 0.0078125, -0.03125],\n",
       "         [0.015625, -0.03125, -0, ..., -0.03125, -0, 0],\n",
       "         [0.03125, 0.03125, 0.03125, ..., 0, 0.03125, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0, -0.03125, 0.015625, ..., -0.0625, 0.015625, -0.0078125],\n",
       "         [-0.0234375, -0.0078125, -0.015625, ..., 0.03125, -0.03125,\n",
       "          -0.015625],\n",
       "         [0.03125, 0.0078125, 0, ..., -0.09375, 0.03125, -0.0625],\n",
       "         ...,\n",
       "         [-0.03125, -0, -0.03125, ..., 0, 0.03125, -0.03125],\n",
       "         [0.0234375, -0.0078125, -0.015625, ..., 0.015625, -0.09375,\n",
       "          -0.03125],\n",
       "         [-0.046875, -0.015625, 0.015625, ..., -0.1875, -0.015625,\n",
       "          -0.0625]],\n",
       " \n",
       "        [[0.0234375, -0.0078125, -0.015625, ..., 0.046875, 0.125,\n",
       "          0.046875],\n",
       "         [0.03125, 0.015625, 0.015625, ..., 0.046875, 0.015625,\n",
       "          -0.046875],\n",
       "         [0.015625, -0.0078125, 0.0078125, ..., -0.0625, 0.015625,\n",
       "          -0.03125],\n",
       "         ...,\n",
       "         [-0.015625, -0, 0.015625, ..., 0.09375, -0.03125, 0.03125],\n",
       "         [0.015625, -0, 0.015625, ..., -0.0625, -0.015625, -0],\n",
       "         [-0.015625, -0.015625, 0.015625, ..., 0.0625, 0.1875, 0.03125]],\n",
       " \n",
       "        [[0, 0.046875, -0.0078125, ..., -0.0625, -0.046875, 0.046875],\n",
       "         [-0.00390625, -0.0234375, 0.0078125, ..., 0.0234375, 0.015625,\n",
       "          0.015625],\n",
       "         [0.015625, 0.015625, 0, ..., -0.0625, -0.046875, 0.03125],\n",
       "         ...,\n",
       "         [0.03125, 0.046875, -0.0078125, ..., -0, 0.046875, -0],\n",
       "         [-0.015625, -0.0625, -0, ..., 0.0625, 0.046875, -0.015625],\n",
       "         [-0, 0.0078125, 0.0078125, ..., 0, 0.03125, 0]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.mlp.experts.w2_bias': Array([[0.0324707, -0.0148926, -0.0349121, ..., 0.010376, -0.00512695,\n",
       "         -0.0197754],\n",
       "        [0.0393066, 0.0307617, -0.0859375, ..., 0.0108643, 0.0163574,\n",
       "         0.0272217],\n",
       "        [0.0366211, 0.00561523, 0.0444336, ..., 0.0203857, -0.012146,\n",
       "         -0.0488281],\n",
       "        ...,\n",
       "        [0.0498047, 0.00466919, 0.0022583, ..., 0.0159912, -0.0249023,\n",
       "         -0.0678711],\n",
       "        [-0.0230713, -0.185547, -0.207031, ..., 0.000220299, -0.0050354,\n",
       "         0.000314713],\n",
       "        [-0.0122681, -0.0874023, -0.109863, ..., 0.00132751, 0.019165,\n",
       "         -0.0115356]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.mlp.experts.w2_weight': Array([[[-0.0234375, 0.09375, -0.03125, ..., 0.015625, -0.015625,\n",
       "          0.0234375],\n",
       "         [0, 0.0078125, -0.03125, ..., 0, -0.03125, 0.0234375],\n",
       "         [-0.03125, -0.015625, -0, ..., 0.015625, 0.0234375, 0.046875],\n",
       "         ...,\n",
       "         [-0.0078125, 0.0078125, 0.00195312, ..., 0.00585938, 0.00195312,\n",
       "          0.0078125],\n",
       "         [-0.0078125, -0.00195312, -0.00585938, ..., 0, -0.00390625,\n",
       "          -0.00390625],\n",
       "         [-0.015625, 0.03125, -0.0117188, ..., 0.015625, -0.0078125,\n",
       "          0.046875]],\n",
       " \n",
       "        [[0.00390625, -0.015625, -0.0234375, ..., -0.0078125, 0.015625,\n",
       "          -0],\n",
       "         [-0.015625, 0.03125, -0.0234375, ..., -0.0625, 0, 0.03125],\n",
       "         [-0.03125, -0.03125, -0.0078125, ..., 0.03125, -0.0078125,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [0.00390625, -0.00585938, 0, ..., 0.0078125, -0.00390625,\n",
       "          0.0078125],\n",
       "         [0.0117188, -0.00195312, -0.0078125, ..., -0.00390625,\n",
       "          0.00390625, 0],\n",
       "         [0.00390625, -0.00390625, 0.015625, ..., 0.00390625, 0.015625,\n",
       "          0.015625]],\n",
       " \n",
       "        [[-0.0117188, -0.03125, -0.03125, ..., -0.015625, 0.015625,\n",
       "          0.0078125],\n",
       "         [0.015625, -0.046875, -0.0078125, ..., -0, -0.03125, 0.03125],\n",
       "         [-0.015625, -0.0078125, 0.046875, ..., 0.0234375, 0.015625,\n",
       "          0.0234375],\n",
       "         ...,\n",
       "         [-0.00195312, -0.0078125, 0.0078125, ..., -0.0078125, 0.0078125,\n",
       "          -0.00195312],\n",
       "         [-0.00390625, -0.00195312, 0.00390625, ..., -0.00195312,\n",
       "          -0.00390625, -0.0117188],\n",
       "         [0.00390625, 0.00390625, -0.0117188, ..., 0.00390625,\n",
       "          -0.0117188, 0.0117188]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.015625, -0.015625, 0.0625, ..., -0.0234375, 0.00390625,\n",
       "          0.00390625],\n",
       "         [-0.0078125, -0.015625, -0.046875, ..., 0.015625, 0.015625,\n",
       "          0.0625],\n",
       "         [0.0078125, 0.0078125, -0.0234375, ..., -0.03125, -0.0234375,\n",
       "          0.046875],\n",
       "         ...,\n",
       "         [0.0117188, 0.0078125, -0.0078125, ..., -0.00585938,\n",
       "          -0.00390625, 0.00195312],\n",
       "         [-0.00195312, -0.00195312, -0.00195312, ..., 0.00195312, -0,\n",
       "          0.00585938],\n",
       "         [-0.00390625, 0.03125, -0.015625, ..., -0, -0.00390625,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[-0.0234375, 0.0078125, 0.0234375, ..., -0.00390625, -0.0078125,\n",
       "          0.0078125],\n",
       "         [0.0078125, 0.0234375, 0.0625, ..., -0, -0.0078125, -0.015625],\n",
       "         [-0.015625, 0.0078125, -0.0234375, ..., 0.0234375, -0,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [-0.00195312, 0.00390625, 0.00390625, ..., -0.000976562,\n",
       "          0.00292969, -0.0078125],\n",
       "         [0.0078125, 0.00292969, 0, ..., -0.00390625, -0.00195312,\n",
       "          0.00195312],\n",
       "         [-0.00390625, 0, -0.0078125, ..., 0.0078125, -0.0078125,\n",
       "          0.00585938]],\n",
       " \n",
       "        [[-0.0078125, 0.0078125, -0.0117188, ..., -0, -0.00390625,\n",
       "          0.015625],\n",
       "         [0.015625, 0.0234375, 0, ..., -0, -0, 0.0078125],\n",
       "         [0.0078125, 0.0078125, 0, ..., -0.0234375, -0.0234375, -0],\n",
       "         ...,\n",
       "         [0.00390625, -0, 0.00390625, ..., -0.00585938, 0.0078125,\n",
       "          -0.00585938],\n",
       "         [0.00292969, 0.00195312, -0.000976562, ..., 0.00195312,\n",
       "          0.00390625, 0.00292969],\n",
       "         [0, -0.00195312, -0.00390625, ..., 0.00195312, 0.00585938,\n",
       "          -0.00195312]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.mlp.router.bias': Array([0.134766, -0.0708008, 0.109375, 0.148438, 0.0117798, -0.229492,\n",
       "        0.213867, 0.0771484, 0.0908203, -0.378906, 0.0162354, 0.133789,\n",
       "        0.0761719, -0.15625, -0.0617676, 0.0559082, -0.0639648, 0.0544434,\n",
       "        -0.147461, -0.0874023, 0.0490723, -0.043457, 0.0217285, -0.222656,\n",
       "        0.117676, -0.330078, 0.0554199, 0.171875, 0.0644531, 0.0500488,\n",
       "        -0.138672, -0.306641], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.mlp.router.weight': Array([[-0.0274658, 0.0148926, -0.00357056, ..., -0.0103149, 0.0127563,\n",
       "         -0.00756836],\n",
       "        [-0.0336914, -0.0390625, 0.0144043, ..., 0.000759125, -0.0187988,\n",
       "         -0.0240479],\n",
       "        [0.0371094, 0.00257874, -0.0488281, ..., -0.0100708, 0.00134277,\n",
       "         0.00787354],\n",
       "        ...,\n",
       "        [-0.024292, -0.00238037, -0.00744629, ..., -0.00350952,\n",
       "         -0.00213623, 0.00891113],\n",
       "        [0.00494385, 0.0505371, 0.0229492, ..., 0.0090332, 0.0230713,\n",
       "         0.000136375],\n",
       "        [0.0380859, 0.00238037, 0.0206299, ..., 0.00439453, 0.0120239,\n",
       "         -0.0206299]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.0.post_attention_layernorm.weight': Array([1.74219, 1.32812, 1.21875, ..., 6.96875, 6.5, 3.28125], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.attn.o_proj.bias': Array([-0.00717163, -0.0114746, -0.0185547, ..., 0.0205078, -0.0424805,\n",
       "        -0.0317383], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.attn.o_proj.weight': Array([[0.0206299, -0.00466919, -0.00411987, ..., 0.00830078, 0.00408936,\n",
       "         0.00370789],\n",
       "        [-0.0088501, -0.010376, -0.0281982, ..., 0.00157166, -0.0169678,\n",
       "         -0.000843048],\n",
       "        [-0.00787354, -0.00842285, -0.00466919, ..., 0.00601196,\n",
       "         0.00430298, 0.00723267],\n",
       "        ...,\n",
       "        [0.000343323, 0.00213623, -0.00613403, ..., -0.00273132,\n",
       "         -0.0107422, 0.00174713],\n",
       "        [0.00170898, -0.00291443, -0.00216675, ..., -0.00175476,\n",
       "         -0.000486374, -0.000343323],\n",
       "        [-0.0200195, -0.00668335, 0.00337219, ..., 0.00180817,\n",
       "         -0.00616455, 0.00250244]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.attn.qkv_proj.bias': Array([-0.113281, -0.0281982, -0.257812, ..., -0.00531006, 0.237305,\n",
       "        0.209961], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.attn.qkv_proj.weight': Array([[-0.0145264, -0.000907898, 0.00302124, ..., 0.00708008,\n",
       "         -0.0388184, 0.0170898],\n",
       "        [-0.0195312, 0.0128784, 0.0037384, ..., 0.0275879, -0.0308838,\n",
       "         0.0322266],\n",
       "        [-0.012146, -0.00415039, 0.00427246, ..., 0.0170898, -0.024292,\n",
       "         0.0155029],\n",
       "        ...,\n",
       "        [0.022583, 0.0476074, -0.0361328, ..., -0.0576172, -0.0629883,\n",
       "         -0.0869141],\n",
       "        [0.0336914, -0.0554199, 0.0136108, ..., -0.0311279, -0.0664062,\n",
       "         0.0231934],\n",
       "        [0.00527954, -0.0179443, 0.0220947, ..., -0.0864258, -0.147461,\n",
       "         0.0639648]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.attn.sinks': Array([3.98438, 3.26562, 2.35938, 2.28125, 3.04688, 4.34375, 2.125,\n",
       "        2.60938, 0.992188, 2.0625, 3.35938, 1.84375, 3.4375, 2.125, 3,\n",
       "        0.515625, 2.51562, -0.337891, -0.480469, 2.625, 7.875, 7.5625,\n",
       "        3.73438, 4.625, 2.64062, 2.79688, 1.57812, 0.972656, 1.23438,\n",
       "        1.78125, 1.57031, 1.63281, 4.65625, 4.4375, 3.625, 4.0625, 3.0625,\n",
       "        3.71875, 4.40625, 5.6875, 5.09375, 3.9375, 4.34375, 4.15625,\n",
       "        4.46875, 3.89062, 3.28125, 2.35938, 6.75, 1.875, 6.5, 5.8125,\n",
       "        5.03125, 6.875, 6.90625, 6.78125, 1.32031, 2.20312, 5.53125, 2.25,\n",
       "        1.375, 5, 2.03125, 1.76562], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.input_layernorm.weight': Array([1.60938, 1.39844, 1.17188, ..., 3.39062, 3.40625, 2.0625],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.mlp.experts.w13_bias': Array([[-0.217773, -0.15625, -0.644531, ..., -0.960938, -0.691406,\n",
       "         -1.35938],\n",
       "        [-0.423828, -0.231445, -0.851562, ..., -1.08594, -0.828125,\n",
       "         -0.761719],\n",
       "        [-0.314453, -0.19043, -0.314453, ..., -0.855469, -0.960938,\n",
       "         -0.933594],\n",
       "        ...,\n",
       "        [-0.163086, -1.10938, -0.326172, ..., -0.988281, -0.941406,\n",
       "         -0.447266],\n",
       "        [-0.375, -1.00781, -0.223633, ..., -0.914062, -0.878906,\n",
       "         -0.867188],\n",
       "        [-0.636719, -0.253906, -0.460938, ..., -0.96875, -0.867188,\n",
       "         -0.960938]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.mlp.experts.w13_weight': Array([[[0.015625, 0.00390625, 0.0078125, ..., 0.0078125, 0.03125,\n",
       "          0.0117188],\n",
       "         [-0, 0.0078125, 0.015625, ..., -0.015625, -0.015625, 0.0117188],\n",
       "         [0, 0.015625, 0.0078125, ..., 0.03125, -0.0625, -0.0078125],\n",
       "         ...,\n",
       "         [-0.03125, -0.015625, 0.046875, ..., 0.03125, -0.03125, -0],\n",
       "         [-0.0625, -0.015625, -0.015625, ..., 0.015625, -0.015625,\n",
       "          -0.0625],\n",
       "         [-0, -0, 0.015625, ..., 0.046875, -0.0625, -0.0625]],\n",
       " \n",
       "        [[-0.015625, -0.03125, -0.015625, ..., -0.015625, -0.09375,\n",
       "          -0.015625],\n",
       "         [-0.015625, 0, 0.00390625, ..., 0.00390625, 0.015625, 0.0078125],\n",
       "         [-0.015625, 0.015625, -0.03125, ..., 0.03125, -0.015625,\n",
       "          -0.03125],\n",
       "         ...,\n",
       "         [-0.015625, -0, -0.015625, ..., 0.03125, -0.046875, -0.046875],\n",
       "         [0.046875, -0.0234375, 0.015625, ..., -0.09375, -0.03125,\n",
       "          -0.03125],\n",
       "         [0.046875, 0, 0.046875, ..., -0.03125, -0.0625, -0]],\n",
       " \n",
       "        [[-0, -0.0117188, 0.0078125, ..., 0.015625, -0.046875, 0.0078125],\n",
       "         [-0.015625, -0.0234375, 0.03125, ..., 0.0078125, 0.0234375, -0],\n",
       "         [0.03125, 0.0078125, -0.0234375, ..., 0.03125, -0.0234375,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [-0.0625, -0.03125, 0.015625, ..., 0.015625, 0, 0.046875],\n",
       "         [-0.0078125, 0.0234375, 0, ..., -0.015625, 0.03125, -0.015625],\n",
       "         [0.03125, -0, 0.046875, ..., -0.0078125, 0.0234375, -0.0234375]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.015625, -0.0078125, 0, ..., 0.0078125, -0.0625, 0.015625],\n",
       "         [0.015625, -0.0078125, 0.0078125, ..., 0.03125, -0.09375,\n",
       "          -0.0078125],\n",
       "         [0, 0, 0.0078125, ..., -0.015625, -0.03125, 0.03125],\n",
       "         ...,\n",
       "         [-0, -0, 0.015625, ..., 0.046875, 0.0625, -0.03125],\n",
       "         [-0.0078125, -0.015625, -0.0078125, ..., 0.09375, 0.015625, -0],\n",
       "         [0, -0.0078125, -0.0078125, ..., -0.015625, 0.03125, -0.046875]],\n",
       " \n",
       "        [[0, -0.0078125, -0.015625, ..., 0.0234375, 0.0078125, 0.0234375],\n",
       "         [0.015625, 0.03125, -0.0234375, ..., -0.03125, 0, -0.015625],\n",
       "         [-0.00390625, 0.0078125, -0.0078125, ..., -0.0625, -0.0078125,\n",
       "          -0.046875],\n",
       "         ...,\n",
       "         [-0.03125, 0.0234375, -0, ..., 0.0078125, 0.03125, -0.0625],\n",
       "         [0.0078125, -0.03125, -0.0078125, ..., 0.046875, -0.046875,\n",
       "          0.09375],\n",
       "         [0.03125, 0.03125, 0.0625, ..., 0.09375, -0.0625, -0.015625]],\n",
       " \n",
       "        [[0.015625, 0, -0.0078125, ..., 0.0625, 0.0625, 0.03125],\n",
       "         [-0.015625, -0, -0.0078125, ..., 0.0625, 0.03125, -0.0078125],\n",
       "         [-0.015625, 0.015625, 0.0078125, ..., 0.015625, 0.09375,\n",
       "          0.046875],\n",
       "         ...,\n",
       "         [-0.015625, 0.015625, -0.015625, ..., 0.03125, 0.03125,\n",
       "          0.015625],\n",
       "         [0.015625, -0.046875, 0.03125, ..., -0.046875, 0.03125, 0],\n",
       "         [-0.03125, 0, -0.015625, ..., -0.015625, 0.015625, -0.03125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.mlp.experts.w2_bias': Array([[-0.0537109, -0.155273, -0.0593262, ..., -0.000427246, 0.0100098,\n",
       "         -0.109375],\n",
       "        [0.0195312, -0.0429688, -0.0361328, ..., -0.0986328, 0.0155029,\n",
       "         0.0161133],\n",
       "        [-0.0183105, 0.0410156, -0.0223389, ..., 0.0158691, -0.0593262,\n",
       "         -0.0038147],\n",
       "        ...,\n",
       "        [0.0349121, -0.0116577, -0.141602, ..., 0.0102539, -0.0114746,\n",
       "         -0.000161171],\n",
       "        [-0.0022583, 0.204102, 0.0471191, ..., -0.00830078, -0.0336914,\n",
       "         -0.0281982],\n",
       "        [-0.0194092, 0.0300293, -0.170898, ..., 0.026001, -0.0157471,\n",
       "         -0.0375977]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.mlp.experts.w2_weight': Array([[[0.0234375, 0.015625, -0.03125, ..., -0.03125, 0.046875,\n",
       "          -0.0234375],\n",
       "         [-0.046875, -0.03125, -0.046875, ..., -0.03125, -0.015625,\n",
       "          -0.0625],\n",
       "         [0.0234375, 0.09375, 0.03125, ..., -0.03125, -0.09375, 0],\n",
       "         ...,\n",
       "         [-0, 0.03125, -0.046875, ..., -0.0078125, 0.015625, 0],\n",
       "         [0.015625, -0.0234375, 0.015625, ..., 0.0234375, 0, 0.00390625],\n",
       "         [0.0234375, 0.0117188, 0.0078125, ..., 0.0234375, 0.0078125,\n",
       "          -0.0117188]],\n",
       " \n",
       "        [[-0.015625, 0.03125, 0.0234375, ..., 0, 0.0234375, 0.0234375],\n",
       "         [-0.015625, 0.046875, -0.015625, ..., -0.015625, 0.09375,\n",
       "          0.0234375],\n",
       "         [-0.015625, -0.015625, -0.015625, ..., -0.03125, 0.03125,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [0.0078125, 0.046875, 0.0234375, ..., -0, -0.03125, -0.0234375],\n",
       "         [-0.00390625, 0, -0.0117188, ..., -0.015625, 0.015625, 0.015625],\n",
       "         [0.0078125, -0.03125, -0, ..., -0.00390625, -0.015625, -0.03125]],\n",
       " \n",
       "        [[0.015625, 0, -0.046875, ..., -0.015625, 0, 0.0078125],\n",
       "         [0.0625, -0.0625, -0.015625, ..., -0.046875, -0.015625,\n",
       "          -0.015625],\n",
       "         [-0.09375, 0.125, 0.015625, ..., 0.046875, -0.0625, -0.03125],\n",
       "         ...,\n",
       "         [0.015625, -0.015625, 0.015625, ..., -0.00390625, 0, -0.015625],\n",
       "         [-0.00390625, 0.0117188, 0.0117188, ..., 0.00390625, -0,\n",
       "          -0.0117188],\n",
       "         [-0.0234375, 0.015625, 0, ..., 0.0234375, -0.03125, -0.046875]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.046875, 0.046875, 0, ..., 0, -0.0078125, -0.03125],\n",
       "         [0, -0.046875, 0.0234375, ..., -0.0625, 0.0625, -0.046875],\n",
       "         [-0.015625, -0, 0.015625, ..., -0.046875, -0.03125, -0.0234375],\n",
       "         ...,\n",
       "         [-0.0117188, -0.0078125, -0.00390625, ..., 0, 0.0117188,\n",
       "          -0.0117188],\n",
       "         [-0.0078125, -0.0117188, 0.0117188, ..., 0.0078125, 0.00390625,\n",
       "          -0.015625],\n",
       "         [-0.00390625, 0.0234375, 0.0078125, ..., -0.015625, 0.0234375,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[-0.0234375, 0.03125, 0, ..., -0.125, -0, 0.015625],\n",
       "         [0.015625, 0.03125, 0.046875, ..., 0.03125, 0.03125, 0],\n",
       "         [-0, -0, -0.09375, ..., 0.015625, 0.03125, 0.03125],\n",
       "         ...,\n",
       "         [0.0078125, -0, -0.0234375, ..., 0.046875, -0, -0.0078125],\n",
       "         [-0.0078125, -0.0078125, -0.00390625, ..., 0.00390625,\n",
       "          -0.0117188, 0.015625],\n",
       "         [-0.03125, -0.0078125, 0.03125, ..., -0.015625, -0.0078125,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[0.015625, -0.0234375, -0.0078125, ..., 0.0234375, -0.0234375,\n",
       "          0.0234375],\n",
       "         [0.046875, -0.015625, 0.03125, ..., -0.0625, -0.03125,\n",
       "          -0.046875],\n",
       "         [0.046875, -0.09375, 0, ..., 0.015625, 0.046875, 0.09375],\n",
       "         ...,\n",
       "         [-0.015625, 0.0117188, 0.0078125, ..., 0.0078125, -0.00390625,\n",
       "          0.00390625],\n",
       "         [-0.0117188, 0.00390625, -0.0117188, ..., -0.0078125,\n",
       "          0.00390625, 0.0078125],\n",
       "         [0.03125, 0.0078125, 0.0078125, ..., -0.0117188, 0.0078125,\n",
       "          0.015625]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.mlp.router.bias': Array([-0.265625, -0.148438, 0.143555, 0.0549316, -0.0314941, -0.0180664,\n",
       "        0.000984192, 0.151367, 0.0952148, 0.0461426, -0.365234, 0.337891,\n",
       "        -0.115723, -0.294922, -0.182617, 0.107422, -0.25, 0.139648,\n",
       "        -0.332031, 0.0424805, -0.125977, 0.0927734, 0.0712891, -0.138672,\n",
       "        0.0178223, 0.185547, 0.0154419, -0.224609, -0.0693359, -0.0649414,\n",
       "        0.0449219, 0.081543], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.mlp.router.weight': Array([[-0.00231934, -0.00668335, -0.0141602, ..., -0.0181885,\n",
       "         -0.00466919, -0.00772095],\n",
       "        [-0.0177002, -0.00297546, 0.00811768, ..., 0.012207, -0.00312805,\n",
       "         0.00521851],\n",
       "        [-0.00427246, -0.0202637, -0.00549316, ..., -0.00357056,\n",
       "         -0.0100708, -0.00276184],\n",
       "        ...,\n",
       "        [0.0145264, -0.0198975, 0.0122681, ..., 0.00205994, 0.00300598,\n",
       "         -0.00717163],\n",
       "        [0.0296631, -0.0119019, -0.0255127, ..., 0.00402832, 0.00418091,\n",
       "         -0.00291443],\n",
       "        [-0.00958252, 0.020874, 0.0454102, ..., -0.00205994, -0.000227928,\n",
       "         0.000492096]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.1.post_attention_layernorm.weight': Array([2.14062, 1.53906, 1.48438, ..., 4.90625, 5.15625, 3.26562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.attn.o_proj.bias': Array([-0.059082, -1.45312, -0.357422, ..., -1.27344, 0.339844, -0.765625],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.attn.o_proj.weight': Array([[-0.0571289, 0.0170898, 0.0568848, ..., 0.390625, -0.090332,\n",
       "         -0.0201416],\n",
       "        [0.00817871, -0.0098877, -0.0644531, ..., -0.0461426, 0.117188,\n",
       "         -0.143555],\n",
       "        [0.0294189, -0.0424805, -0.00102234, ..., -0.161133, -0.125977,\n",
       "         0.0766602],\n",
       "        ...,\n",
       "        [-0.050293, 0.0366211, 0.0585938, ..., -0.11377, -0.0090332,\n",
       "         0.0107422],\n",
       "        [-0.00402832, 0.0332031, -0.0228271, ..., 0.0598145, -0.0664062,\n",
       "         0.0115967],\n",
       "        [-0.00244141, -0.0419922, 0.0291748, ..., 0.181641, -0.015625,\n",
       "         -0.107422]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.attn.qkv_proj.bias': Array([-0.125, 0.412109, 0.554688, ..., 0.168945, -0.0839844, 0.029541],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.attn.qkv_proj.weight': Array([[0.00128174, -0.00915527, 0.00524902, ..., -0.0189209, 0.0162354,\n",
       "         0.012207],\n",
       "        [-0.0148926, -0.00257874, 0.0158691, ..., 0.0198975, -0.00958252,\n",
       "         0.00817871],\n",
       "        [-0.00402832, -0.00634766, 0.00193787, ..., 0.00518799, 0.0231934,\n",
       "         0.0181885],\n",
       "        ...,\n",
       "        [0.0393066, -0.0410156, -0.11084, ..., -0.00708008, 0.0556641,\n",
       "         -0.0834961],\n",
       "        [0.0480957, -0.00564575, 0.0184326, ..., 0.020752, -0.0354004,\n",
       "         -0.0888672],\n",
       "        [-0.03125, -0.0356445, -0.0524902, ..., -0.0105591, -0.0629883,\n",
       "         0.00442505]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.attn.sinks': Array([1.875, 2.71875, 2.09375, 4.8125, 2.35938, 2.42188, 1.42969,\n",
       "        3.29688, 2.70312, 2.28125, 2.14062, 2.65625, 2.39062, 2.70312,\n",
       "        2.48438, 2.42188, 2.09375, 1.53125, 2.625, 2.32812, 2.17188, 3,\n",
       "        2.65625, 1.96875, 2.21875, 1.82031, 2.6875, 2.17188, 2.09375,\n",
       "        1.48438, 1.90625, 2.67188, 2.5, 2.5, 2.54688, 2.40625, 2.79688,\n",
       "        2.64062, 2.375, 2, 2.85938, 2.71875, 3.10938, 2.3125, 2.82812,\n",
       "        2.625, 2.60938, 3.39062, 1.60938, 1.69531, 2.29688, 1.72656,\n",
       "        3.07812, 2.45312, 2.5, 2.76562, 2.46875, 3.01562, 3.65625, 2.85938,\n",
       "        0.361328, 2.8125, 2.98438, 3.29688], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.input_layernorm.weight': Array([1.67969, 1.5, 1.80469, ..., 1.70312, 1.91406, 1.82812], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.mlp.experts.w13_bias': Array([[-0.0559082, -0.032959, -0.734375, ..., -0.894531, -1.09375,\n",
       "         -0.714844],\n",
       "        [0.00891113, -0.496094, -0.046875, ..., -0.671875, -0.8125,\n",
       "         -0.863281],\n",
       "        [-0.953125, -0.742188, -0.324219, ..., -1.27344, -0.519531,\n",
       "         -0.867188],\n",
       "        ...,\n",
       "        [-0.832031, -0.152344, -1.55469, ..., -0.941406, -0.511719,\n",
       "         -0.9375],\n",
       "        [-0.257812, -1.625, -0.464844, ..., -0.625, -0.910156, -0.976562],\n",
       "        [-0.192383, -0.960938, -0.490234, ..., -1.10156, -0.734375,\n",
       "         -0.8125]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.mlp.experts.w13_weight': Array([[[-0.00585938, -0.00195312, -0.0078125, ..., -0.0117188,\n",
       "          0.0117188, 0.00195312],\n",
       "         [-0.00585938, -0.000976562, 0.00292969, ..., -0.00390625, 0,\n",
       "          0.0117188],\n",
       "         [-0.015625, 0.015625, 0.00390625, ..., -0.0234375, 0.015625,\n",
       "          0.0078125],\n",
       "         ...,\n",
       "         [0.03125, 0, -0.046875, ..., 0.015625, 0.0078125, -0.046875],\n",
       "         [-0.0234375, -0.0078125, -0.015625, ..., 0.03125, 0, 0.015625],\n",
       "         [0.03125, 0.03125, -0.046875, ..., 0.046875, 0.015625, 0.046875]],\n",
       " \n",
       "        [[-0.00195312, 0.00195312, 0, ..., -0.00292969, -0.00585938,\n",
       "          -0.000976562],\n",
       "         [-0.0117188, -0.00585938, -0.0117188, ..., -0.0078125, -0,\n",
       "          -0.00390625],\n",
       "         [-0, -0.00585938, -0.00195312, ..., -0.000976562, 0.00195312,\n",
       "          -0.00585938],\n",
       "         ...,\n",
       "         [-0.0078125, 0.0078125, -0.046875, ..., -0.0234375, -0.046875,\n",
       "          -0.03125],\n",
       "         [-0.03125, 0.00390625, -0.0078125, ..., 0.03125, 0.0078125,\n",
       "          0.015625],\n",
       "         [0.0234375, 0.0078125, -0.0234375, ..., 0.0234375, -0.046875,\n",
       "          0.0078125]],\n",
       " \n",
       "        [[0, -0.00390625, 0.0234375, ..., 0, -0.0234375, 0.0117188],\n",
       "         [-0.00585938, 0.0078125, 0.00195312, ..., 0.00390625,\n",
       "          -0.00390625, -0.0078125],\n",
       "         [-0.00390625, -0.0078125, 0.00390625, ..., -0.0078125, -0, 0],\n",
       "         ...,\n",
       "         [0.00390625, -0, -0, ..., 0.015625, 0.0078125, -0],\n",
       "         [0.015625, -0.0078125, -0.03125, ..., -0.0078125, 0.0078125,\n",
       "          -0.03125],\n",
       "         [-0.046875, -0.015625, 0.03125, ..., 0.0625, 0.0625, -0.03125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.015625, -0.0078125, -0.0078125, ..., -0, -0.0078125,\n",
       "          -0.00390625],\n",
       "         [0.00195312, -0, -0.00585938, ..., -0, -0.0078125, 0.00390625],\n",
       "         [-0.03125, 0.0078125, -0, ..., -0.0234375, 0.03125, 0.0234375],\n",
       "         ...,\n",
       "         [-0.046875, -0, 0.0117188, ..., 0.015625, -0.00390625,\n",
       "          -0.0078125],\n",
       "         [-0, -0.046875, 0.015625, ..., 0.03125, 0.09375, -0],\n",
       "         [-0, -0.0078125, -0.015625, ..., -0.03125, 0.0078125,\n",
       "          -0.0234375]],\n",
       " \n",
       "        [[0.0117188, -0.00195312, 0.00390625, ..., 0.0117188, -0.0117188,\n",
       "          -0.0078125],\n",
       "         [0, -0.0078125, -0.00390625, ..., -0.0078125, -0.0117188,\n",
       "          0.015625],\n",
       "         [-0, 0.03125, 0.015625, ..., -0, -0.015625, 0.0078125],\n",
       "         ...,\n",
       "         [-0.046875, -0.015625, -0.0078125, ..., 0.0078125, -0, -0.0625],\n",
       "         [-0.00390625, -0, 0.0117188, ..., -0.0078125, -0.015625, 0.0625],\n",
       "         [0.03125, 0, 0.0234375, ..., -0.015625, -0, 0.0234375]],\n",
       " \n",
       "        [[-0, 0.00195312, -0.0078125, ..., -0.00390625, 0.0078125,\n",
       "          -0.0117188],\n",
       "         [-0.00390625, -0.0078125, 0.015625, ..., 0.00195312, -0.0078125,\n",
       "          -0.00390625],\n",
       "         [0.0234375, 0.0117188, -0, ..., 0.0117188, -0.00390625,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [0.03125, -0.0078125, -0.046875, ..., -0.0234375, 0, -0.0078125],\n",
       "         [0.015625, 0.015625, 0.046875, ..., 0.0078125, 0.046875, -0],\n",
       "         [-0.0234375, 0.0078125, -0, ..., -0.015625, -0.046875,\n",
       "          -0.046875]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.mlp.experts.w2_bias': Array([[1.85156, 0.53125, -2.01562, ..., -0.163086, 1.15625, -0.535156],\n",
       "        [-0.914062, -0.447266, -1.38281, ..., 0.263672, 1.02344,\n",
       "         -0.640625],\n",
       "        [1.125, -0.574219, 0.476562, ..., -1.28906, 2.1875, -0.242188],\n",
       "        ...,\n",
       "        [0.445312, -0.992188, -0.683594, ..., 0.0869141, 0.150391,\n",
       "         -0.235352],\n",
       "        [0.25, 0.458984, 0.351562, ..., -0.542969, 0.839844, -0.910156],\n",
       "        [0.9375, -0.738281, -0.0913086, ..., -0.816406, 1.09375, -1.09375]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.mlp.experts.w2_weight': Array([[[-0, 0.5, -0, ..., -1, 1, 0.5],\n",
       "         [-0.125, -0.75, -0, ..., 0, -0.25, 0.5],\n",
       "         [-0.25, -0.125, -0.375, ..., -0.125, 0.25, -0.25],\n",
       "         ...,\n",
       "         [-0.75, -0.5, -0.5, ..., -0.75, 0.5, 0.25],\n",
       "         [0.375, -0.5, -0.25, ..., -0.75, -0.375, -0.125],\n",
       "         [0, 0.125, 0.375, ..., 0.25, -0.5, 0.25]],\n",
       " \n",
       "        [[-0.25, 0.25, -1, ..., 0.25, -0.25, 0],\n",
       "         [-2, 1, -0.5, ..., -0.25, 0.125, 0.5],\n",
       "         [-0.75, -0.125, -0, ..., -0, -0.75, 0.5],\n",
       "         ...,\n",
       "         [0.75, -0.375, 0.375, ..., -0.125, -0.5, 0.75],\n",
       "         [-0.5, 0.125, 0.25, ..., -0.375, -0.125, -0.375],\n",
       "         [0.375, -0.5, 0.25, ..., -0.375, -0.25, 0.375]],\n",
       " \n",
       "        [[0.125, -0.5, -0.5, ..., 0.125, 0.5, -0.375],\n",
       "         [-0.25, -1, 0.5, ..., 0.125, 0.375, 0.75],\n",
       "         [0.25, 0.375, -0.125, ..., 0.5, -0.25, 1.5],\n",
       "         ...,\n",
       "         [-0.25, 0.0625, -0.375, ..., 0.375, 0.25, 0.75],\n",
       "         [-0.125, -0.125, 0, ..., 0.1875, -0.125, -0.5],\n",
       "         [0.375, -0.125, -0.125, ..., -0.375, -0.125, 0.375]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0, 0.125, 0.375, ..., -0.5, -0.5, 0.25],\n",
       "         [-0, -0, -0.75, ..., 0, -0.5, 0],\n",
       "         [-0.375, 0.25, -0.75, ..., -0, 0.25, 0.5],\n",
       "         ...,\n",
       "         [0.125, -0.375, -1, ..., -0, 0.125, -0],\n",
       "         [0.5, 0.75, -0.5, ..., -0.125, 0.5, 0.5],\n",
       "         [-0, 0.375, 0.75, ..., 0.25, 0.75, -0.125]],\n",
       " \n",
       "        [[-0.25, -0.75, -0.5, ..., -0.25, 0.25, -0],\n",
       "         [0.25, -0.75, 0.25, ..., 0.5, 0.125, -1],\n",
       "         [-0.25, 0.75, 0.375, ..., 0.125, -0.75, -0.5],\n",
       "         ...,\n",
       "         [0, -0.25, 0, ..., -0.25, -1, 0.5],\n",
       "         [-0.125, 0.25, 0.25, ..., 0.125, -0.125, -0.5],\n",
       "         [1, -0.25, -0.125, ..., -1, 1, 0.375]],\n",
       " \n",
       "        [[0.75, -0.375, 0.25, ..., 0.75, 0.5, 0.5],\n",
       "         [-0.375, 0.125, 0.25, ..., -0.375, 0.1875, -0.5],\n",
       "         [0.75, 0, -0.5, ..., -1.5, 0.25, 0.25],\n",
       "         ...,\n",
       "         [0.5, 0.25, 0.75, ..., -0.75, 0.25, 0],\n",
       "         [0.5, 0.25, 0.375, ..., -0.25, 0.75, -0],\n",
       "         [0.375, -0.25, -0.125, ..., 0.375, 0.5, -0.375]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.mlp.router.bias': Array([-0.0668945, -0.0544434, -0.00476074, 0.0385742, -0.0266113,\n",
       "        -0.078125, 0.013855, 0.00273132, 0.0698242, -0.0107422, 0.0163574,\n",
       "        -0.19043, 0.0214844, 0.0471191, 0.00418091, 0.00823975, -0.0766602,\n",
       "        -0.0561523, 0.0581055, 0.147461, -0.00854492, -0.0717773,\n",
       "        0.0154419, -0.0615234, 0.0463867, -0.0402832, -0.145508,\n",
       "        -0.0883789, 0.0620117, 0.0922852, 0.0284424, 0.0169678],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.mlp.router.weight': Array([[0.00297546, -8.24928e-05, 0.00811768, ..., 0.00267029,\n",
       "         -0.00289917, -0.00254822],\n",
       "        [0.0020752, 0.000314713, 0.00509644, ..., 0.00300598, 0.0016861,\n",
       "         -0.00231934],\n",
       "        [0.00133514, -0.00245667, -0.00671387, ..., 0.000610352,\n",
       "         -0.00150299, 0.00402832],\n",
       "        ...,\n",
       "        [0.00482178, -0.00253296, 0.012146, ..., 0.012146, 0.00270081,\n",
       "         0.00424194],\n",
       "        [-0.00921631, 0.00994873, 0.000220299, ..., -0.00334167,\n",
       "         0.00500488, 0.000295639],\n",
       "        [-0.0015564, -0.00167084, 0.00686646, ..., 0.000263214,\n",
       "         -0.00114441, 0.0045166]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.10.post_attention_layernorm.weight': Array([1.82812, 1.35938, 2.10938, ..., 1.78125, 2.42188, 2.28125],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.attn.o_proj.bias': Array([0.263672, -0.0134888, 0.192383, ..., -0.738281, -0.400391, 0.78125],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.attn.o_proj.weight': Array([[-0.0302734, 0.0874023, -0.0361328, ..., 0.0478516, -0.0366211,\n",
       "         0.0356445],\n",
       "        [0.0727539, 0.0103149, -0.143555, ..., 0.109375, -0.0057373,\n",
       "         0.00964355],\n",
       "        [-0.164062, -0.227539, 0.0800781, ..., -0.0218506, 0.0727539,\n",
       "         -0.102051],\n",
       "        ...,\n",
       "        [0.0610352, -0.000556946, 0.0593262, ..., 0.0349121, -0.0252686,\n",
       "         -0.263672],\n",
       "        [-0.0111084, 0.163086, 0.0255127, ..., 0.00750732, -0.0834961,\n",
       "         0.0834961],\n",
       "        [0.0270996, 0.185547, -0.107422, ..., -0.0281982, 0.0480957,\n",
       "         0.128906]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.attn.qkv_proj.bias': Array([0.632812, 1.13281, 0.929688, ..., 0.193359, 0.53125, 0.333984],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.attn.qkv_proj.weight': Array([[0.0554199, 0.0537109, -0.0100098, ..., 0.000242233, -0.0136108,\n",
       "         0.0230713],\n",
       "        [-0.0180664, -0.0088501, -0.0493164, ..., -0.0174561, 0.0512695,\n",
       "         0.0358887],\n",
       "        [0.0137939, 0.0255127, -0.0257568, ..., -0.0110474, -0.0383301,\n",
       "         -0.0147095],\n",
       "        ...,\n",
       "        [0.00317383, -0.0317383, 0.00723267, ..., -0.00976562, 0.00708008,\n",
       "         0.00805664],\n",
       "        [-0.00238037, -0.0311279, 0.00897217, ..., 0.0216064, 0.0227051,\n",
       "         -0.0354004],\n",
       "        [-0.0515137, -0.0257568, 0.0310059, ..., 0.0849609, 0.00921631,\n",
       "         -0.00762939]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.attn.sinks': Array([1.30469, 5.40625, 3.60938, 4.8125, 6.5625, 5.625, 5.0625, 6.3125,\n",
       "        5.4375, 3.07812, 4.0625, 4.3125, 3.65625, 3.17188, 3.03125,\n",
       "        3.85938, 3.76562, 3.65625, 3.15625, 2.14062, 2.46875, 4.90625,\n",
       "        2.98438, 4.46875, 5.21875, 5.28125, 4.90625, 5.25, 4.625, 3.32812,\n",
       "        5.28125, 4.96875, 4.5, 4.3125, 3.6875, 3.89062, 4.21875, 5,\n",
       "        4.03125, 4.5, 3.95312, 4.71875, 4.53125, 4.25, 4.46875, 4.09375,\n",
       "        4.75, 4.0625, 5.625, 4.90625, 5.0625, 5.21875, 4.96875, 5.25,\n",
       "        5.15625, 5.78125, 1.03125, 0.291016, 5.3125, 1.32812, 1.22656, 2,\n",
       "        -0.214844, 3.17188], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.input_layernorm.weight': Array([1.71094, 1.33594, 1.625, ..., 1.6875, 2.03125, 1.91406], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.mlp.experts.w13_bias': Array([[-0.225586, -0.238281, -0.355469, ..., -1.07031, -0.882812,\n",
       "         -0.816406],\n",
       "        [-0.163086, -0.263672, -1.28125, ..., -0.640625, -1.32031,\n",
       "         -0.451172],\n",
       "        [-0.0996094, -0.398438, -0.0761719, ..., -1.09375, -0.777344,\n",
       "         -0.828125],\n",
       "        ...,\n",
       "        [-0.238281, -0.0893555, -0.244141, ..., -0.859375, -0.523438,\n",
       "         -0.410156],\n",
       "        [-0.294922, -0.18457, -0.210938, ..., -0.5, -0.820312, -0.453125],\n",
       "        [-0.445312, 0.00915527, -0.0654297, ..., -0.291016, -0.691406,\n",
       "         -0.554688]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.mlp.experts.w13_weight': Array([[[0.00390625, -0.0078125, 0.0078125, ..., 0.00390625, 0.00195312,\n",
       "          -0.00390625],\n",
       "         [-0.00585938, 0.00390625, 0.0117188, ..., -0.0078125,\n",
       "          0.00195312, 0.00390625],\n",
       "         [-0.0117188, -0.0117188, -0.00195312, ..., -0.0078125,\n",
       "          -0.0117188, -0.00390625],\n",
       "         ...,\n",
       "         [0.00390625, -0.015625, -0, ..., 0.03125, -0.015625, -0.0078125],\n",
       "         [-0.0078125, 0.03125, -0.0234375, ..., 0.0117188, -0.015625,\n",
       "          0.03125],\n",
       "         [-0.0078125, -0.0234375, 0.0625, ..., 0.0234375, 0.0234375,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[-0.00390625, -0.00585938, -0, ..., -0.0078125, -0.00585938,\n",
       "          -0.0117188],\n",
       "         [-0.0078125, 0.03125, -0.015625, ..., -0.0078125, 0.00390625,\n",
       "          -0],\n",
       "         [-0.0078125, -0.015625, 0.00390625, ..., 0.0078125, -0.03125,\n",
       "          0.03125],\n",
       "         ...,\n",
       "         [0.03125, -0.0234375, 0.015625, ..., 0.0078125, -0.0234375,\n",
       "          0.03125],\n",
       "         [-0.0234375, -0.015625, -0.015625, ..., -0.0234375, -0, 0],\n",
       "         [0, -0, 0.0625, ..., -0.0625, -0.0625, 0.0234375]],\n",
       " \n",
       "        [[-0, -0.00195312, 0.0078125, ..., 0.00195312, -0.00390625,\n",
       "          -0.0117188],\n",
       "         [0, -0.00585938, 0.0234375, ..., 0.0117188, -0.0078125,\n",
       "          -0.00585938],\n",
       "         [-0.0078125, -0.0078125, -0, ..., 0.0078125, 0.0234375,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [-0.015625, 0.0625, 0.0078125, ..., 0.015625, 0.015625,\n",
       "          0.0117188],\n",
       "         [-0.015625, 0.0078125, -0, ..., 0.046875, 0.046875, -0.046875],\n",
       "         [-0.00390625, 0.00390625, 0.0234375, ..., -0.015625, 0.015625,\n",
       "          0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00390625, -0.00390625, 0.00390625, ..., -0.00390625,\n",
       "          0.0078125, -0],\n",
       "         [0.000976562, 0.00390625, 0.000976562, ..., -0.00292969,\n",
       "          0.00195312, -0.000976562],\n",
       "         [-0.015625, 0.00390625, 0.00390625, ..., -0.00390625,\n",
       "          -0.0078125, 0.00195312],\n",
       "         ...,\n",
       "         [-0.015625, -0.00390625, 0.0078125, ..., 0.03125, 0.0234375,\n",
       "          0.015625],\n",
       "         [-0.03125, 0.0078125, -0.015625, ..., -0.0234375, 0.046875,\n",
       "          -0.0078125],\n",
       "         [-0.046875, 0.03125, 0.046875, ..., 0, 0, -0.015625]],\n",
       " \n",
       "        [[-0.015625, 0.00390625, -0.00195312, ..., 0.0078125, -0.0078125,\n",
       "          0],\n",
       "         [0.00585938, 0.00195312, -0.00585938, ..., 0.000976562,\n",
       "          -0.0078125, -0.00390625],\n",
       "         [0.0078125, -0.00390625, 0.0078125, ..., 0.00195312, 0.0078125,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [0.046875, 0.0234375, 0.0234375, ..., 0.046875, -0.0078125,\n",
       "          -0.03125],\n",
       "         [0.046875, 0.015625, -0.0078125, ..., 0.046875, -0.015625,\n",
       "          -0.0234375],\n",
       "         [0.046875, 0.0234375, -0.0078125, ..., 0.0234375, -0, -0]],\n",
       " \n",
       "        [[-0.000976562, -0.0117188, 0.0078125, ..., 0.00390625,\n",
       "          -0.0078125, -0.00195312],\n",
       "         [0.00390625, -0.000976562, -0.00292969, ..., 0.000976562,\n",
       "          -0.000976562, 0],\n",
       "         [0.00585938, -0.0078125, 0.00390625, ..., -0.00195312,\n",
       "          -0.00195312, -0.00195312],\n",
       "         ...,\n",
       "         [-0.03125, 0.0078125, 0.03125, ..., -0.03125, 0.0234375,\n",
       "          0.03125],\n",
       "         [0.0625, -0.046875, 0.046875, ..., 0.09375, 0.046875, -0.03125],\n",
       "         [-0.0078125, 0.0078125, 0, ..., -0.015625, 0, -0.046875]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.mlp.experts.w2_bias': Array([[1.46094, 0.116699, 0.300781, ..., -0.835938, 1.48438, -0.227539],\n",
       "        [1.58594, -0.667969, -0.855469, ..., 0.933594, 1.30469, 0.886719],\n",
       "        [3.28125, -1.19531, -1.10156, ..., -1.52344, 2.25, -0.433594],\n",
       "        ...,\n",
       "        [1.84375, -0.113281, 0.851562, ..., 1.94531, 1.44531, -0.773438],\n",
       "        [2.23438, 0.314453, 0.960938, ..., 2.4375, 1.98438, -1.33594],\n",
       "        [2.53125, -0.585938, -2.3125, ..., 0.144531, 3.875, -1.875]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.mlp.experts.w2_weight': Array([[[-2, 1.5, 1, ..., 0.75, -0.5, -1],\n",
       "         [0.5, 0.25, -0.5, ..., 1, 0.25, -0.25],\n",
       "         [0.75, -1, 0.5, ..., -0.25, 0.25, -0.25],\n",
       "         ...,\n",
       "         [-0.75, -0.75, 0.75, ..., 0.75, 0.25, 0.25],\n",
       "         [0.125, -0.125, -0.25, ..., -0.25, 0, 0.5],\n",
       "         [-0.375, -0.375, -0.375, ..., 0.125, 0.5, -1]],\n",
       " \n",
       "        [[-0.125, 1.5, 0.5, ..., 1.5, 0.75, 0.25],\n",
       "         [-0.75, -1, -0.25, ..., -1.5, 0.125, 0],\n",
       "         [0.5, -0.25, -0.75, ..., -0, -0.5, -0.25],\n",
       "         ...,\n",
       "         [-1.5, -0.375, -0.375, ..., -0.125, -0.375, -0.375],\n",
       "         [0.125, -0.5, -0.25, ..., -0.125, 0.25, -0.5],\n",
       "         [0.125, 0.375, -0.5, ..., -0.375, 0.125, 0.375]],\n",
       " \n",
       "        [[-0, 0.5, -1, ..., 0.5, -2, 0],\n",
       "         [-1.5, -0.25, 0.25, ..., 0.25, 2, 0.25],\n",
       "         [-0.75, 0.5, -0, ..., 0, 0.125, 1],\n",
       "         ...,\n",
       "         [0, 0.75, 1, ..., 0, -0, -0.25],\n",
       "         [1, -0.125, 0, ..., 0.5, 0.125, -0.375],\n",
       "         [-0.25, -0.5, -0.75, ..., 0, 0, -0.5]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[1, 1.5, -0.5, ..., -0, -1.5, -0.5],\n",
       "         [-0, -1, -0.75, ..., 0.5, -0.5, -0],\n",
       "         [-0, 0, 0.25, ..., -0.5, -1.5, -0],\n",
       "         ...,\n",
       "         [0.5, 1, -0.25, ..., 0, 0.5, -0.25],\n",
       "         [-1.5, -0.75, 0.25, ..., 0, 0.25, 0],\n",
       "         [-0.75, 0.25, -1, ..., -0.75, 1, -0.5]],\n",
       " \n",
       "        [[-1.5, 0, -1.5, ..., -0.25, 1, 1],\n",
       "         [0.5, 0.5, 0.25, ..., 0.25, -0.375, 0.375],\n",
       "         [-0.5, -1, 0.25, ..., -0.75, -0.5, -0.5],\n",
       "         ...,\n",
       "         [-1.5, 0.75, -0.25, ..., 0.5, 0.75, 0.75],\n",
       "         [-0.375, 0.25, -0.25, ..., 0.25, 0.25, 0.5],\n",
       "         [0.75, 0.5, -1, ..., -0.75, -0.75, -0.125]],\n",
       " \n",
       "        [[0.25, -3, 1, ..., -1, 2, -0.25],\n",
       "         [0.5, 0, 1, ..., 0, -0.25, 0.5],\n",
       "         [-0.375, -0.25, -0.5, ..., 0.125, -0.75, 0.125],\n",
       "         ...,\n",
       "         [0, 0.25, -0, ..., -1, -2, 0.25],\n",
       "         [0, -0.75, 0.25, ..., 0.75, 1.5, -0.125],\n",
       "         [0.5, -0, 0.75, ..., 0.75, -1.5, -1]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.mlp.router.bias': Array([-0.0223389, -0.0483398, 0.0167236, -0.0161133, -0.019043,\n",
       "        -0.00823975, 0.0390625, 0.078125, -0.0878906, -0.0561523,\n",
       "        0.0250244, -0.197266, 0.0334473, 0.0698242, 0.0368652, 0.0449219,\n",
       "        0.00878906, -0.0830078, 0.0124512, -0.00150299, 0.0439453,\n",
       "        0.00424194, 0.0625, 0.0473633, -0.0639648, 0.0820312, -0.0150146,\n",
       "        -0.0500488, 0.0495605, -0.0976562, 0.0181885, -0.104004],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.mlp.router.weight': Array([[-0.00389099, -0.00120544, 0.00415039, ..., -0.000598907,\n",
       "         -0.00396729, -0.00564575],\n",
       "        [0.00436401, -0.00817871, 0.00390625, ..., -0.00482178,\n",
       "         0.000869751, -0.00460815],\n",
       "        [0.00128174, 0.000286102, 0.00205994, ..., -6.34193e-05,\n",
       "         -0.00344849, 0.00408936],\n",
       "        ...,\n",
       "        [-0.00107574, 0.00546265, 0.000667572, ..., -0.00442505,\n",
       "         -0.00363159, -0.00415039],\n",
       "        [-0.00482178, 0.00363159, -0.00276184, ..., -0.00469971,\n",
       "         -0.00204468, -0.00242615],\n",
       "        [0.00405884, -0.000314713, 0.00244141, ..., -0.0001688,\n",
       "         -0.00379944, -0.000291824]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.11.post_attention_layernorm.weight': Array([1.63281, 1.32031, 2, ..., 1.625, 2.21875, 2.03125], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.attn.o_proj.bias': Array([0.597656, -0.757812, -0.554688, ..., -1.57812, -0.0202637,\n",
       "        -0.921875], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.attn.o_proj.weight': Array([[-0.142578, 0.0634766, -0.0229492, ..., 0.0220947, -0.0106201,\n",
       "         -0.0022583],\n",
       "        [-0.0134888, 0.0169678, -0.130859, ..., -0.00204468, -0.0603027,\n",
       "         0.0174561],\n",
       "        [-0.0358887, 0.081543, 0.0327148, ..., -0.0834961, 0.0400391,\n",
       "         0.0495605],\n",
       "        ...,\n",
       "        [0.125977, 0.198242, -0.0153198, ..., 0.0952148, 0.0603027,\n",
       "         -0.0014801],\n",
       "        [-0.0512695, -0.0419922, -0.00866699, ..., 0.0405273, -0.107422,\n",
       "         -0.0240479],\n",
       "        [-0.0429688, -0.300781, 0.0563965, ..., 0.202148, -0.181641,\n",
       "         -0.036377]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.attn.qkv_proj.bias': Array([0.318359, 0.330078, -0.671875, ..., 0.175781, -0.100098, -0.195312],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.attn.qkv_proj.weight': Array([[0.00424194, -0.00799561, 0.000946045, ..., 0.00112915,\n",
       "         0.00634766, 0.0065918],\n",
       "        [0.00769043, 0.00242615, 0.000442505, ..., -0.000320435,\n",
       "         0.00521851, 0.00354004],\n",
       "        [-0.0151367, 0.0123901, -0.00634766, ..., -0.00411987, -0.0113525,\n",
       "         -0.0211182],\n",
       "        ...,\n",
       "        [-0.0185547, -0.00393677, -0.0256348, ..., 0.0141602, 0.0217285,\n",
       "         -0.0378418],\n",
       "        [-0.0189209, 0.0893555, 0.126953, ..., 0.0366211, -0.0791016,\n",
       "         0.0129395],\n",
       "        [0.0118408, -0.0400391, 0.117188, ..., -0.010498, 0.0296631,\n",
       "         0.0388184]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.attn.sinks': Array([3.03125, 2.6875, 2.34375, 1.15625, 2.64062, 1.85938, 2.17188,\n",
       "        2.54688, 3.07812, 2.20312, 2.84375, 2.96875, 1.89062, 1.8125,\n",
       "        1.27344, 1.60938, 1.79688, 2.5, 2.48438, 3.26562, 2.01562, 2.15625,\n",
       "        2.65625, 2.60938, 3.04688, 2.76562, 2.79688, 2.90625, 3.20312,\n",
       "        2.4375, 2.9375, 2.9375, 2.07812, 1.72656, 1.875, 1.73438, 1.71094,\n",
       "        1.55469, 2.70312, 2.40625, 0.304688, 1.36719, 0.804688, 2.45312,\n",
       "        1.90625, 0.578125, -2.63453e-05, 0.40625, 2.67188, 2.57812,\n",
       "        2.48438, 2.42188, 2.39062, 2.75, 2.78125, 2.29688, 2.89062,\n",
       "        2.79688, 1.8125, 2.17188, 2.5, 3.10938, 2.26562, 2.76562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.input_layernorm.weight': Array([1.67188, 1.48438, 1.72656, ..., 1.71094, 1.96094, 1.83594],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.mlp.experts.w13_bias': Array([[-0.248047, 0.0105591, 0.0461426, ..., -1.05469, -0.929688,\n",
       "         -1.42969],\n",
       "        [-0.0766602, -0.914062, 0.414062, ..., -0.3125, -0.0771484,\n",
       "         -0.269531],\n",
       "        [-0.125977, -0.519531, -0.229492, ..., -0.460938, -0.449219,\n",
       "         -0.667969],\n",
       "        ...,\n",
       "        [-0.152344, -0.193359, -0.507812, ..., -0.761719, -0.699219,\n",
       "         -0.578125],\n",
       "        [0.0634766, -0.261719, -0.0756836, ..., -1.34375, -0.691406,\n",
       "         -0.851562],\n",
       "        [-0.178711, -0.507812, -0.0917969, ..., -0.847656, -0.949219,\n",
       "         -0.84375]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.mlp.experts.w13_weight': Array([[[0.0078125, -0, 0.00195312, ..., 0, 0.0078125, -0.0078125],\n",
       "         [0, 0.00390625, -0.00390625, ..., -0.00585938, 0.00195312,\n",
       "          -0.00585938],\n",
       "         [-0.00146484, 0.00390625, 0.00292969, ..., 0.00292969, -0,\n",
       "          0.00195312],\n",
       "         ...,\n",
       "         [0.0078125, -0.015625, -0.015625, ..., 0.0078125, -0, -0.015625],\n",
       "         [-0.03125, -0, 0.0078125, ..., 0.03125, 0.0078125, -0],\n",
       "         [-0.046875, -0.0234375, -0.0078125, ..., 0.00390625, 0.015625,\n",
       "          -0.0117188]],\n",
       " \n",
       "        [[0.00292969, 0, -0, ..., 0.000976562, -0.00195312, -0.000488281],\n",
       "         [0.00390625, -0.00390625, -0.015625, ..., -0, 0.0234375,\n",
       "          0.0117188],\n",
       "         [0.00195312, -0, -0.000976562, ..., -0.000976562, 0.00390625,\n",
       "          0.00292969],\n",
       "         ...,\n",
       "         [0.0078125, 0.0234375, -0.015625, ..., -0.0078125, 0.0078125,\n",
       "          -0.015625],\n",
       "         [-0.03125, 0.015625, -0.015625, ..., 0.015625, -0.0234375,\n",
       "          -0.015625],\n",
       "         [0.00390625, 0, 0.0078125, ..., 0.0078125, -0.0078125,\n",
       "          0.0078125]],\n",
       " \n",
       "        [[0.00390625, -0.0078125, -0.00585938, ..., 0.0117188,\n",
       "          -0.00195312, -0.00195312],\n",
       "         [0.015625, -0.015625, 0.00390625, ..., 0.00195312, -0.00195312,\n",
       "          -0.015625],\n",
       "         [-0.0078125, 0.0078125, -0, ..., -0.0117188, 0.00195312, -0],\n",
       "         ...,\n",
       "         [0.03125, 0.0625, 0, ..., 0.015625, 0.0078125, 0.015625],\n",
       "         [0.03125, -0.0117188, 0.0234375, ..., 0.0078125, -0.0078125,\n",
       "          0.015625],\n",
       "         [-0.0078125, -0.015625, -0, ..., 0.03125, 0.015625, 0.03125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.0117188, 0.00195312, -0.00195312, ..., -0.00195312,\n",
       "          0.00390625, -0.0117188],\n",
       "         [-0.00390625, 0, 0.0078125, ..., -0.0078125, -0.00292969,\n",
       "          0.000976562],\n",
       "         [0.0234375, 0.015625, 0.0117188, ..., -0.0117188, -0.015625,\n",
       "          0.0234375],\n",
       "         ...,\n",
       "         [0.0078125, 0.015625, 0.015625, ..., 0.03125, 0.0234375,\n",
       "          0.0078125],\n",
       "         [-0.00390625, -0.03125, -0.03125, ..., -0.03125, 0.0625,\n",
       "          0.0234375],\n",
       "         [-0.0234375, -0.0078125, 0, ..., -0.015625, 0.0078125,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[-0.00195312, 0.00390625, -0.0078125, ..., -0.00195312,\n",
       "          0.00390625, -0.00195312],\n",
       "         [0.0078125, -0.00390625, 0.0078125, ..., -0.0234375, -0.0234375,\n",
       "          -0.0234375],\n",
       "         [-0.000976562, 0, 0.0078125, ..., 0.00195312, -0.00585938,\n",
       "          0.0117188],\n",
       "         ...,\n",
       "         [-0.0078125, 0, 0.015625, ..., -0.046875, -0.0625, 0.046875],\n",
       "         [-0.03125, 0.0234375, -0.046875, ..., -0, -0.0078125, 0],\n",
       "         [0.046875, 0.015625, -0.0078125, ..., 0.0078125, 0.0234375,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[0, 0.00195312, -0.00195312, ..., -0.0078125, -0.00195312,\n",
       "          0.00390625],\n",
       "         [0.015625, -0.0078125, -0, ..., -0.0078125, 0.00390625,\n",
       "          0.00390625],\n",
       "         [-0.00292969, 0.00390625, -0.0078125, ..., 0.00390625,\n",
       "          -0.00390625, 0],\n",
       "         ...,\n",
       "         [0.0078125, -0.0234375, -0.0078125, ..., -0.03125, -0.0234375,\n",
       "          0.015625],\n",
       "         [-0.0234375, -0.0234375, -0.046875, ..., -0.015625, 0.0234375,\n",
       "          0.0078125],\n",
       "         [-0.0234375, 0, 0.015625, ..., -0.046875, -0.015625, 0]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.mlp.experts.w2_bias': Array([[-1.86719, -0.373047, -0.941406, ..., 1.76562, 1.25781, -0.933594],\n",
       "        [-1.42188, 0.519531, 0.28125, ..., -1.73438, 2.09375, -0.40625],\n",
       "        [-0.96875, -0.0368652, 1.84375, ..., -0.227539, 1.25, -0.984375],\n",
       "        ...,\n",
       "        [-1.05469, -0.273438, -0.558594, ..., -2.4375, 2.42188, -0.882812],\n",
       "        [0.00366211, -1.40625, 1, ..., 1.24219, 3.48438, -2.875],\n",
       "        [2.78125, -0.12793, 0.213867, ..., -0.273438, 0.574219, 0.414062]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.mlp.experts.w2_weight': Array([[[-2, -0.25, 2, ..., -1, 0.25, 0.25],\n",
       "         [-0.75, -0.375, -1, ..., -0.75, -0.25, 0],\n",
       "         [1.5, 0.25, -1.5, ..., -0, 1.5, -0.25],\n",
       "         ...,\n",
       "         [2, -0.75, -1, ..., 0, 0.5, -0.75],\n",
       "         [1, 0.375, 1.5, ..., -0.5, 0.25, 0.25],\n",
       "         [1, 2, -1, ..., -0.5, -1.5, -0.5]],\n",
       " \n",
       "        [[0.5, -0.5, -1, ..., -0.5, -2, 1.5],\n",
       "         [0.25, 0.25, 1, ..., -0.25, -1.5, 0.25],\n",
       "         [-1.5, -0.25, -0.5, ..., 0.25, 0.5, -1],\n",
       "         ...,\n",
       "         [0.25, 0.5, 0.25, ..., 0.25, 0, 0.75],\n",
       "         [-0.5, 1, -0.25, ..., -0.25, 0.25, -0.25],\n",
       "         [0.5, 1.5, 0.5, ..., 0.5, -1, -0.5]],\n",
       " \n",
       "        [[0.5, -1.5, 1.5, ..., 1, 0.5, 0.25],\n",
       "         [0.25, -0.75, -0.25, ..., 0.25, -1.5, 0.5],\n",
       "         [-0.25, 0.125, -0.125, ..., 0.5, -0.75, 0.5],\n",
       "         ...,\n",
       "         [-1, -0, 0.5, ..., -0.375, -0.25, 1],\n",
       "         [0.375, -0.125, 0.25, ..., 0.25, 0.75, 0.5],\n",
       "         [0.5, 0.25, -1, ..., 0.25, -0.5, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1, 1.5, 0.25, ..., -2, -0, -0.25],\n",
       "         [-0.25, 1, -0.75, ..., -0.5, -1, -0],\n",
       "         [-0.75, 0.375, -0.125, ..., -1.5, -0.5, 0.5],\n",
       "         ...,\n",
       "         [-0.25, 1, 0.5, ..., 0.5, -0.5, -0.5],\n",
       "         [-0.5, -0.5, 0.5, ..., -0.5, 0.25, -1.5],\n",
       "         [0.75, 0.125, -0.25, ..., 1.5, 0.25, -1]],\n",
       " \n",
       "        [[2, 0.5, 2, ..., 1, 1, -0.5],\n",
       "         [1, -0.25, 0.5, ..., 0.75, 1, 0.5],\n",
       "         [1, 0.5, 0.25, ..., 0.5, -0, 0.25],\n",
       "         ...,\n",
       "         [0.5, -0.5, -0.75, ..., -1, -2, 0.25],\n",
       "         [0.25, 0.5, 2, ..., -0, 0.75, 0.25],\n",
       "         [-1, -1, 0.5, ..., 1, -0.5, 0.75]],\n",
       " \n",
       "        [[-1, -0.25, 0.25, ..., -0.75, -0, -0],\n",
       "         [-2, 0.25, -0.25, ..., 0.5, 0, -0.25],\n",
       "         [-1, 0.25, 1.5, ..., 0.25, -0.5, -1.5],\n",
       "         ...,\n",
       "         [-0, -0, 0.25, ..., 0, 1.5, -1],\n",
       "         [-0.25, -0.25, 0, ..., 0.5, -0.25, 0.25],\n",
       "         [1, -0.75, 0.5, ..., 0.25, -0.125, -0.5]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.mlp.router.bias': Array([-0.0588379, -0.0859375, 0.0422363, -0.0678711, -0.0255127,\n",
       "        -0.0227051, 0.0913086, 0.0159912, 0.020874, 0.0090332, -0.00817871,\n",
       "        -0.142578, -0.0810547, 0.0219727, 0.0708008, 0.0551758, 0.00732422,\n",
       "        0.114258, -0.0512695, -0.0546875, -0.0839844, 0.119629,\n",
       "        0.000303268, -0.0693359, -0.0272217, 0.0913086, 0.000919342,\n",
       "        -0.0639648, -0.0327148, 0.0410156, -0.0270996, 0.0344238],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.mlp.router.weight': Array([[-0.00317383, 0.000976562, 0.001297, ..., 0.00242615, 0.000984192,\n",
       "         -0.00631714],\n",
       "        [0.000862122, 0.00340271, 0.000823975, ..., -0.0045166,\n",
       "         0.000835419, -0.00228882],\n",
       "        [-0.00299072, -0.00576782, -0.00254822, ..., 0.00195312,\n",
       "         0.0106201, -0.000850677],\n",
       "        ...,\n",
       "        [0.00189972, 0.0017395, -0.000774384, ..., -0.00305176,\n",
       "         0.00282288, 5.31673e-05],\n",
       "        [0.00674438, 0.00263977, -0.00242615, ..., -0.00285339,\n",
       "         -0.00622559, 0.00439453],\n",
       "        [-0.00994873, -0.0129395, -0.00454712, ..., 0.00628662,\n",
       "         -0.00457764, -0.00363159]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.12.post_attention_layernorm.weight': Array([1.47656, 1.48438, 1.875, ..., 1.63281, 2.15625, 1.96094], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.attn.o_proj.bias': Array([0.318359, -0.527344, -0.12793, ..., 0.470703, -0.570312, 0.851562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.attn.o_proj.weight': Array([[-0.359375, -0.126953, -0.539062, ..., 0.449219, -0.219727,\n",
       "         0.0223389],\n",
       "        [0.443359, 0.0678711, -0.100586, ..., 0.210938, 0.125977,\n",
       "         -0.0795898],\n",
       "        [-0.0427246, -0.104004, -0.0456543, ..., 0.122559, 0.204102,\n",
       "         0.0649414],\n",
       "        ...,\n",
       "        [-0.0283203, 0.192383, 0.363281, ..., -0.15625, -0.451172,\n",
       "         0.11084],\n",
       "        [0.103516, 0.0927734, 0.28125, ..., 0.00994873, 0.0932617,\n",
       "         -0.0664062],\n",
       "        [-0.322266, -0.0507812, 0.0976562, ..., -0.0415039, 0.131836,\n",
       "         -0.206055]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.attn.qkv_proj.bias': Array([-0.166992, 0.097168, -0.0356445, ..., -0.217773, 0.277344,\n",
       "        0.289062], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.attn.qkv_proj.weight': Array([[-0.00717163, -0.0117188, -0.0115967, ..., -0.00231934,\n",
       "         -0.0135498, -0.00799561],\n",
       "        [0.00143433, 0.0183105, -0.00156403, ..., 0.010376, 0.0153809,\n",
       "         -0.00185394],\n",
       "        [-0.00656128, -0.0151367, 0.00732422, ..., -0.00561523,\n",
       "         -0.0123291, 0.00230408],\n",
       "        ...,\n",
       "        [-0.00315857, -0.0437012, 0.0137939, ..., 0.0150757, 0.0131836,\n",
       "         -0.0205078],\n",
       "        [-0.0771484, 0.0522461, 0.0483398, ..., 0.0456543, 0.0461426,\n",
       "         -0.045166],\n",
       "        [0.0245361, 0.00190735, -0.0214844, ..., 0.0157471, 0.0566406,\n",
       "         -0.0432129]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.attn.sinks': Array([5.125, 5.3125, 5.28125, 5.78125, 5.21875, 5.625, 5.84375, 5.28125,\n",
       "        3.1875, 2.42188, 2.59375, 3.4375, 2.90625, 3.14062, 3.4375,\n",
       "        2.92188, 2.5625, 2.78125, 2.65625, 2.64062, 2.28125, 2.71875,\n",
       "        2.95312, 2.67188, 4.4375, 4.09375, 4.28125, 2.75, 4.96875, 5,\n",
       "        4.78125, 4.4375, 5, 5.03125, 5.125, 5.0625, 5.65625, 5.125,\n",
       "        5.03125, 4.21875, 4.25, 4.625, 5.15625, 4.0625, 2.34375, 5.65625,\n",
       "        3.71875, 4.59375, 0.84375, 2.65625, 1.32031, 0.691406, 0.380859,\n",
       "        1.07031, 4, 0.507812, 0.400391, 0.5, 3.64062, 3.76562, 2.90625,\n",
       "        1.21875, 1.74219, 2.57812], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.input_layernorm.weight': Array([1.70312, 1.67188, 1.5625, ..., 1.63281, 1.71094, 1.80469],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.mlp.experts.w13_bias': Array([[-0.412109, -0.207031, -0.214844, ..., -0.964844, -0.208008,\n",
       "         -0.621094],\n",
       "        [-0.230469, -0.777344, -0.0605469, ..., -0.683594, -0.472656,\n",
       "         -0.519531],\n",
       "        [-0.316406, -0.648438, -0.0805664, ..., -0.652344, -0.161133,\n",
       "         -0.796875],\n",
       "        ...,\n",
       "        [-0.121582, -1.05469, -0.126953, ..., -0.625, -0.482422,\n",
       "         -0.855469],\n",
       "        [-0.589844, -0.578125, -0.208008, ..., -0.9375, -0.535156,\n",
       "         -0.636719],\n",
       "        [-1.53906, -0.376953, -0.539062, ..., -0.345703, -0.359375,\n",
       "         -1.42188]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.mlp.experts.w13_weight': Array([[[-0.0078125, 0.00390625, 0, ..., -0.00390625, -0.0117188,\n",
       "          0.03125],\n",
       "         [-0.00195312, 0.00585938, 0.0234375, ..., 0.00195312,\n",
       "          -0.00195312, 0.00585938],\n",
       "         [-0.00585938, -0.00390625, 0.00390625, ..., 0.00585938, 0,\n",
       "          0.00390625],\n",
       "         ...,\n",
       "         [-0, -0.0078125, 0, ..., -0.0625, 0, 0.0234375],\n",
       "         [0.015625, -0.0234375, -0.046875, ..., -0.0625, 0.015625,\n",
       "          -0.0078125],\n",
       "         [-0.046875, -0.046875, 0.0078125, ..., -0.046875, 0.03125,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[0.00390625, -0.00195312, 0.00390625, ..., 0.00195312,\n",
       "          -0.00195312, 0.00585938],\n",
       "         [0.0117188, -0.0078125, -0, ..., -0.015625, -0.0117188,\n",
       "          -0.0117188],\n",
       "         [0.00195312, 0.0078125, 0.00292969, ..., 0.00585938,\n",
       "          -0.00195312, 0.0078125],\n",
       "         ...,\n",
       "         [-0.09375, 0.03125, -0.125, ..., -0.09375, 0.03125, -0],\n",
       "         [0.03125, 0.03125, 0.015625, ..., 0.015625, 0.015625, 0.0117188],\n",
       "         [0.0078125, 0.0078125, -0.03125, ..., 0.015625, 0.03125,\n",
       "          0.015625]],\n",
       " \n",
       "        [[0.0234375, 0.00390625, 0.00390625, ..., -0.0117188, -0.0234375,\n",
       "          -0.00390625],\n",
       "         [0.00195312, -0.00195312, -0.00195312, ..., 0.0117188,\n",
       "          -0.015625, 0.00390625],\n",
       "         [0.000976562, 0, 0.00292969, ..., -0.00585938, -0.00390625,\n",
       "          0.00390625],\n",
       "         ...,\n",
       "         [0.0078125, -0, -0.0234375, ..., 0.0078125, 0, 0.0234375],\n",
       "         [0.03125, 0.00390625, 0.0078125, ..., 0.00390625, -0.0078125,\n",
       "          -0.0234375],\n",
       "         [0.046875, 0.015625, 0.0078125, ..., -0.0234375, -0.0078125,\n",
       "          -0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00195312, -0.00390625, 0.0078125, ..., -0.00585938,\n",
       "          0.00390625, -0.0078125],\n",
       "         [-0.00390625, 0.0234375, -0.0234375, ..., -0.0117188,\n",
       "          -0.0078125, -0.0117188],\n",
       "         [0.00195312, 0.0117188, -0.0078125, ..., 0.00195312, -0.0078125,\n",
       "          -0.00390625],\n",
       "         ...,\n",
       "         [0.03125, 0.0078125, -0, ..., 0.015625, -0, -0.0234375],\n",
       "         [-0.0078125, 0.046875, 0.0078125, ..., -0.0078125, -0.0078125,\n",
       "          0.0234375],\n",
       "         [-0.015625, 0.0078125, 0.015625, ..., -0, -0, 0]],\n",
       " \n",
       "        [[0.0234375, -0.046875, -0.00390625, ..., 0.0117188, 0, 0],\n",
       "         [0.015625, -0.0078125, 0.00195312, ..., -0.015625, -0.00390625,\n",
       "          0.0117188],\n",
       "         [-0.00585938, 0, 0.00390625, ..., -0, -0.00195312, -0.0078125],\n",
       "         ...,\n",
       "         [-0.0234375, -0.0078125, 0, ..., -0.015625, 0.015625, -0.015625],\n",
       "         [-0.0117188, 0.0078125, 0.046875, ..., 0.0078125, -0.015625,\n",
       "          -0.046875],\n",
       "         [0.015625, -0.0078125, -0.0078125, ..., -0.0078125, -0.03125,\n",
       "          -0.0234375]],\n",
       " \n",
       "        [[0.0117188, -0.015625, -0.0234375, ..., -0, -0.0078125,\n",
       "          0.015625],\n",
       "         [0.0078125, 0.0078125, 0.015625, ..., 0.00195312, 0.0078125,\n",
       "          -0.00195312],\n",
       "         [0, 0.015625, -0.015625, ..., 0.0078125, -0.0234375, 0.015625],\n",
       "         ...,\n",
       "         [-0.015625, 0.0625, -0.03125, ..., 0.125, 0.0625, 0],\n",
       "         [-0.015625, 0.015625, -0, ..., 0.046875, -0.015625, -0.0625],\n",
       "         [-0.0078125, -0, -0, ..., -0.015625, 0.0078125, -0.0078125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.mlp.experts.w2_bias': Array([[-0.0588379, 1.14062, -1.66406, ..., -1.04688, -0.151367, -4.4375],\n",
       "        [-2.8125, -1.5, 0.0230713, ..., -1.13281, 2.85938, 0.855469],\n",
       "        [-0.451172, 0.921875, -1.71875, ..., -0.941406, 1.65625,\n",
       "         -0.0400391],\n",
       "        ...,\n",
       "        [0.800781, 0.195312, -0.255859, ..., -1.65625, 0.443359,\n",
       "         -0.753906],\n",
       "        [1.125, 0.738281, -0.757812, ..., 0.585938, 4.15625, -3.75],\n",
       "        [-0.800781, 1.17969, 1.24219, ..., 2.67188, 1.03125, -3]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.mlp.experts.w2_weight': Array([[[0.25, 0.5, 2, ..., 0, -2, 0.5],\n",
       "         [-0.75, -1.5, 1, ..., 1, 0.5, -0.75],\n",
       "         [-1.5, 0.75, 0, ..., -0.5, -1, 2],\n",
       "         ...,\n",
       "         [0.25, 0.5, 0, ..., -1.5, -1, 0.375],\n",
       "         [-1, -0, 0.5, ..., -0.5, -0, 0.25],\n",
       "         [-0.75, -0.5, 0.75, ..., -0.25, -2, -0.25]],\n",
       " \n",
       "        [[-2, 0.25, -1.5, ..., -1, -0.5, -0.5],\n",
       "         [1, 0.75, -0.25, ..., 0.5, 0.5, -0.25],\n",
       "         [0.75, -0.25, 0.75, ..., -0.75, 1, -1],\n",
       "         ...,\n",
       "         [0.25, -0, 0.75, ..., -0.5, 0.25, -0.25],\n",
       "         [-0.25, 0.5, 0.25, ..., 0.25, -0.5, 1],\n",
       "         [0.75, -0.5, 0, ..., 0.5, -0, -0.5]],\n",
       " \n",
       "        [[0.5, -0.5, 1, ..., -2, -0, -0.5],\n",
       "         [0.25, -1.5, 1, ..., -0.5, 0.5, 0.25],\n",
       "         [-0.5, -0.5, 0.75, ..., 0.25, -0.25, 1],\n",
       "         ...,\n",
       "         [0, -0, -1, ..., -0.25, 0.25, 2],\n",
       "         [-0.75, 1.5, -0.5, ..., -0.5, 0.75, 0.75],\n",
       "         [0, -0.5, -0.5, ..., 0.5, -0.5, 1.5]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.5, 0.25, -2, ..., -1.5, 0.5, 1],\n",
       "         [-1, 0.75, -0.5, ..., -0.5, 1, 0.25],\n",
       "         [0, 0.5, -1.5, ..., -1, 1, -0.25],\n",
       "         ...,\n",
       "         [0.25, -1, 0.75, ..., 0, -0.25, 0.25],\n",
       "         [0.25, -0.25, -0, ..., -0.5, 0.5, 0],\n",
       "         [-0.75, -0.5, 0.25, ..., 0.5, -0.5, -1]],\n",
       " \n",
       "        [[-0, 0.5, 0.25, ..., 0.5, -1.5, -0.5],\n",
       "         [1, 0.125, 0.75, ..., -0.25, -1, -0.25],\n",
       "         [0.25, 0.25, 0.5, ..., -0.25, 0.25, -0.75],\n",
       "         ...,\n",
       "         [0.5, -0.75, 0.5, ..., 0.25, 1.5, -0.25],\n",
       "         [0.25, 1.5, -0.25, ..., 0.75, 0.25, 0.75],\n",
       "         [0.25, 0.25, -0.25, ..., 0.75, -0.25, 1]],\n",
       " \n",
       "        [[1, 2, -3, ..., 0, 2, -0],\n",
       "         [-0.25, -0.5, -0, ..., 0.5, 0.5, 0.5],\n",
       "         [0.25, -1, 0.75, ..., 0.25, -0.5, 0.25],\n",
       "         ...,\n",
       "         [0.75, 0.5, 0.5, ..., -0, -1, 0.5],\n",
       "         [-0.25, 0.25, -0.25, ..., -0.25, 0.5, -0],\n",
       "         [0.5, 1.5, -0, ..., 1, -0.25, -0.5]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.mlp.router.bias': Array([0.00720215, 0.0405273, -0.116699, -0.0246582, -0.000911713,\n",
       "        0.0556641, 0.0150757, 0.0101318, 0.0310059, 0.0524902, -0.0180664,\n",
       "        -0.0991211, -0.052002, 0.00515747, 0.0634766, 0.0108643,\n",
       "        -0.00369263, 0.00631714, 0.0332031, 0.0371094, 0.0439453,\n",
       "        -0.0106201, -0.0598145, -0.00643921, 0.0717773, 0.0289307,\n",
       "        -0.0771484, -0.00396729, -0.0859375, 0.0100098, 0.0327148,\n",
       "        -0.0839844], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.mlp.router.weight': Array([[-0.00549316, -0.0043335, -0.00427246, ..., -0.00349426,\n",
       "         0.00218201, 0.00726318],\n",
       "        [-0.00836182, -0.00172424, 0.00360107, ..., 0.00421143,\n",
       "         0.00402832, -0.00466919],\n",
       "        [0.00561523, -0.00123596, -0.00297546, ..., -0.00253296,\n",
       "         -0.00315857, 0.00128937],\n",
       "        ...,\n",
       "        [0.00494385, 0.0125122, 0.00358582, ..., 0.00811768, 0.00193787,\n",
       "         -0.00215149],\n",
       "        [-0.000400543, -0.00117493, -0.00166321, ..., 0.00271606,\n",
       "         -0.00376892, 0.0065918],\n",
       "        [-0.0035553, -0.0090332, 0.00139618, ..., 0.00167084, 0.00105286,\n",
       "         -0.00315857]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.13.post_attention_layernorm.weight': Array([1.34375, 1.52344, 1.88281, ..., 1.58594, 2.04688, 1.82031],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.attn.o_proj.bias': Array([0.457031, -0.488281, 0.21875, ..., 0.789062, 0.388672, -0.976562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.attn.o_proj.weight': Array([[-0.199219, 0.15332, -0.0120239, ..., 0.0559082, 0.0270996,\n",
       "         -0.189453],\n",
       "        [-0.00195312, 0.0917969, 0.0205078, ..., 0.0869141, -0.0103149,\n",
       "         0.000850677],\n",
       "        [-0.132812, -0.0927734, 0.0786133, ..., 0.0212402, -0.138672,\n",
       "         -0.0908203],\n",
       "        ...,\n",
       "        [0.0688477, 0.108887, 0.0732422, ..., -0.0664062, 0.131836,\n",
       "         0.124023],\n",
       "        [0.134766, 0.0678711, -0.0957031, ..., 0.0233154, 0.00343323,\n",
       "         0.0301514],\n",
       "        [-0.129883, -0.0195312, 0.224609, ..., -0.0112915, 0.0270996,\n",
       "         0.136719]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.attn.qkv_proj.bias': Array([0.0708008, -0.0996094, -0.0449219, ..., 0.382812, 0.625, -0.116699],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.attn.qkv_proj.weight': Array([[0.00331116, 0.00610352, -0.00479126, ..., 0.00643921, 0.00738525,\n",
       "         0.000850677],\n",
       "        [-0.00546265, -0.000644684, -0.000101089, ..., -0.00219727,\n",
       "         -0.00732422, -0.00239563],\n",
       "        [0.000629425, 0.00952148, 0.0101318, ..., -0.00274658,\n",
       "         -0.000169754, -0.0067749],\n",
       "        ...,\n",
       "        [0.0505371, -0.0107422, -0.00241089, ..., 0.0698242, 0.100098,\n",
       "         -0.024292],\n",
       "        [0.060791, -0.0249023, 0.0639648, ..., -0.00717163, 0.146484,\n",
       "         -0.277344],\n",
       "        [0.0266113, 0.0917969, -0.000484467, ..., -0.00488281, -0.03125,\n",
       "         0.106934]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.attn.sinks': Array([0.785156, 0.964844, 1.82031, 1.42188, 1.57031, 0.84375, 1.38281,\n",
       "        1.14844, 2.15625, 0.890625, 2.07812, -0.0854492, 1.28906, 2.46875,\n",
       "        0.746094, 2.375, 1.77344, 1.95312, 2.28125, 1.44531, 1.24219,\n",
       "        1.52344, 1.47656, 2.0625, 1.96875, 1.64844, 2.20312, 2.10938,\n",
       "        1.76562, 1.75, 1.64844, 2.09375, 0.648438, 1.25781, 1.25, 0.960938,\n",
       "        0.660156, 1.4375, 0.12793, 1.08594, 0.730469, 1.98438, 0.124023,\n",
       "        0.396484, -0.835938, 1.89062, 0.878906, 2.39062, 1.42188, 2.39062,\n",
       "        1.51562, 1.64062, 2.40625, 2.07812, 1.83594, 2.09375, 2.375,\n",
       "        1.55469, 1.86719, 1.91406, 0.902344, 1.875, 1.46875, 1.92969],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.input_layernorm.weight': Array([1.69531, 1.86719, 1.97656, ..., 1.82031, 2.10938, 1.85938],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.mlp.experts.w13_bias': Array([[0.00619507, -0.722656, -0.804688, ..., 0.0512695, -0.0375977,\n",
       "         -0.789062],\n",
       "        [-0.197266, -0.126953, 0.176758, ..., -0.753906, -0.0410156,\n",
       "         -0.394531],\n",
       "        [0.0898438, -1.17188, -0.189453, ..., -0.664062, -0.914062,\n",
       "         -0.824219],\n",
       "        ...,\n",
       "        [-2.14062, -0.0942383, -0.59375, ..., -0.396484, -0.108887,\n",
       "         -0.100586],\n",
       "        [-0.318359, -0.267578, -0.185547, ..., -0.527344, -0.443359,\n",
       "         -0.671875],\n",
       "        [-0.0090332, -0.00787354, -0.195312, ..., -0.832031, -0.365234,\n",
       "         -0.167969]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.mlp.experts.w13_weight': Array([[[0, 0.00390625, 0.00195312, ..., -0.00390625, 0.00195312,\n",
       "          0.00292969],\n",
       "         [-0.015625, 0.0078125, 0.00585938, ..., 0.0078125, 0.0117188,\n",
       "          0.00390625],\n",
       "         [0.015625, 0.0078125, -0.0078125, ..., -0.015625, 0,\n",
       "          -0.00390625],\n",
       "         ...,\n",
       "         [-0.0234375, -0.00390625, -0.03125, ..., 0.03125, 0.00390625,\n",
       "          -0.015625],\n",
       "         [-0.0078125, -0, 0.0234375, ..., 0.0078125, 0.0117188, -0.03125],\n",
       "         [0.0234375, 0.0078125, -0.0078125, ..., -0.0117188, -0.015625,\n",
       "          0.03125]],\n",
       " \n",
       "        [[-0.000976562, -0.000976562, 0.00292969, ..., -0.00292969,\n",
       "          -0.00390625, 0.00195312],\n",
       "         [0.00195312, 0.0117188, -0.00390625, ..., -0, 0, -0.0117188],\n",
       "         [-0.00390625, 0.00292969, 0.00195312, ..., 0.000976562,\n",
       "          0.00195312, -0.00195312],\n",
       "         ...,\n",
       "         [0.09375, -0.046875, 0, ..., 0, -0.046875, 0.03125],\n",
       "         [0.03125, -0.0625, -0.03125, ..., -0, 0.03125, -0.03125],\n",
       "         [0.0234375, 0.0625, -0.0078125, ..., 0.03125, 0.0078125,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[0.000976562, -0.00585938, 0.000976562, ..., -0.00390625,\n",
       "          0.00292969, -0.00390625],\n",
       "         [-0.015625, 0.0078125, 0.03125, ..., -0, 0, 0.0117188],\n",
       "         [0.015625, 0.00390625, 0.015625, ..., -0.015625, 0.0234375, -0],\n",
       "         ...,\n",
       "         [0.03125, 0, -0.0625, ..., -0.0078125, 0.046875, -0.015625],\n",
       "         [-0.0234375, -0.046875, 0, ..., 0.0234375, -0.0078125,\n",
       "          -0.0234375],\n",
       "         [0.0625, -0.0234375, 0.0625, ..., 0.015625, -0.015625, -0.03125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00390625, -0.0078125, -0.00390625, ..., -0.03125, -0, -0],\n",
       "         [-0.00195312, -0.0078125, -0.00195312, ..., -0.0078125,\n",
       "          -0.00195312, -0.00195312],\n",
       "         [-0.0078125, -0.00390625, -0.0117188, ..., -0.00195312,\n",
       "          0.015625, -0],\n",
       "         ...,\n",
       "         [0.0078125, -0, -0.03125, ..., 0.0234375, -0.0234375, 0.0117188],\n",
       "         [-0.0234375, 0, 0.046875, ..., 0.015625, -0.03125, 0.09375],\n",
       "         [0.09375, 0.015625, -0.03125, ..., 0.09375, -0.015625,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[-0.00390625, 0.00390625, 0, ..., 0.00390625, -0, -0.00390625],\n",
       "         [0.00585938, 0.015625, -0.0078125, ..., 0.0078125, -0.0117188,\n",
       "          0.0117188],\n",
       "         [0.00195312, -0.00390625, 0.00585938, ..., -0.00585938,\n",
       "          -0.00195312, -0.00195312],\n",
       "         ...,\n",
       "         [0.0234375, -0.03125, -0.00390625, ..., 0.0234375, -0.0234375,\n",
       "          0.015625],\n",
       "         [0, -0.03125, -0.0078125, ..., 0.046875, -0.03125, -0.0078125],\n",
       "         [0, 0.0117188, 0.0234375, ..., 0.0234375, -0.03125, 0.015625]],\n",
       " \n",
       "        [[0.015625, -0.0117188, -0, ..., -0.00195312, 0.0117188,\n",
       "          -0.0117188],\n",
       "         [0, 0.000976562, 0.00390625, ..., 0.000976562, -0.00195312,\n",
       "          0.00390625],\n",
       "         [0.00195312, 0.00585938, 0.00585938, ..., -0.00292969,\n",
       "          0.0078125, 0.00292969],\n",
       "         ...,\n",
       "         [0.03125, 0.03125, 0, ..., 0.046875, 0.0234375, 0.0234375],\n",
       "         [-0.00390625, -0.0078125, -0.0078125, ..., 0.00390625,\n",
       "          -0.00390625, -0],\n",
       "         [-0.0078125, 0.0234375, 0.015625, ..., -0.03125, 0.046875,\n",
       "          -0.0625]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.mlp.experts.w2_bias': Array([[-3.54688, 1.4375, 1.26562, ..., -0.394531, 3.09375, -1.3125],\n",
       "        [1.6875, -4.0625, -2.40625, ..., -0.170898, 0.785156, -3.25],\n",
       "        [1.74219, 1.67188, -0.222656, ..., -0.953125, 5.0625, -5.125],\n",
       "        ...,\n",
       "        [-2.67188, -1.30469, -2.0625, ..., 0.859375, 0.90625, -0.308594],\n",
       "        [-0.550781, 2.54688, 0.667969, ..., -1.625, 2.64062, -1.17969],\n",
       "        [-6.46875, 2.625, 0.3125, ..., -2.09375, 0.582031, -4.21875]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.mlp.experts.w2_weight': Array([[[1, -0.5, 3, ..., -1, 0, 0],\n",
       "         [0.5, -0.25, 0.5, ..., -0.75, 0.75, -1],\n",
       "         [1.5, -1, 0.5, ..., -0.75, -0, 1],\n",
       "         ...,\n",
       "         [1.5, 0.25, -0.5, ..., -2, -1, 1],\n",
       "         [1.5, 0.5, 1, ..., 0, -0.5, -1],\n",
       "         [-2, -0.75, -1.5, ..., 1.5, 1.5, 1]],\n",
       " \n",
       "        [[-1, 2, 0.5, ..., 1, 0.5, -0.5],\n",
       "         [-0.5, 0.25, 2, ..., -1, -1.5, 3],\n",
       "         [-1.5, 0.75, -0, ..., 0.25, 0.25, -0.5],\n",
       "         ...,\n",
       "         [-1.5, -2, -0.5, ..., -0, -0.75, -0.25],\n",
       "         [-0.5, -1.5, 1.5, ..., 0, 2, -2],\n",
       "         [-1.5, 2, 3, ..., 1, -2, -0.5]],\n",
       " \n",
       "        [[-1, -4, 1, ..., 0.5, -2, 4],\n",
       "         [-0.25, 1, 0.25, ..., 0.25, 0.5, 0.75],\n",
       "         [0.25, 0.5, 0.25, ..., -0.5, -0.5, 2],\n",
       "         ...,\n",
       "         [1.5, 0.5, 0.25, ..., 1, 1, -0.25],\n",
       "         [-3, -0.5, -1, ..., 1.5, 0.25, 0],\n",
       "         [3, -1.5, -1, ..., 0, 0.5, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 1, -1, ..., 3, 1, 0],\n",
       "         [-0.25, 0.25, 0, ..., 0.75, -1.5, -0.5],\n",
       "         [-1.5, -1, 0.5, ..., 0.75, 0.5, -1.5],\n",
       "         ...,\n",
       "         [-0.5, 2, -0.75, ..., -0.5, -0, 3],\n",
       "         [-1, -1, -0.5, ..., 0, -3, -2],\n",
       "         [-0, 1, 2, ..., 2, 8, -0]],\n",
       " \n",
       "        [[-1.5, 1.5, 1, ..., -0.5, 2, -1],\n",
       "         [0.5, -2, -0.25, ..., 0.75, 0.25, 0.5],\n",
       "         [1, 1.5, 0.25, ..., 0.5, -0.5, -0],\n",
       "         ...,\n",
       "         [0.25, 0.5, 1.5, ..., 0.5, 1.5, -0.25],\n",
       "         [0.25, 0.25, -0.5, ..., -0, -0.25, 0.25],\n",
       "         [-0, 2, 0, ..., 0.25, 0.25, -0.5]],\n",
       " \n",
       "        [[1, -0.5, -1, ..., 1, 2, -6],\n",
       "         [-0.5, 1, 0, ..., -2, 0, -1.5],\n",
       "         [0.25, 0.75, -1, ..., 2, 0.75, -1],\n",
       "         ...,\n",
       "         [0.5, -0.5, -0, ..., 1.5, -0.5, -1.5],\n",
       "         [-0.25, -0, 0, ..., 0.75, 0.25, 1.5],\n",
       "         [-0.5, -1, -1, ..., 3, -1, 3]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.mlp.router.bias': Array([-0.00378418, -0.0197754, -0.0219727, 0.0742188, -0.177734,\n",
       "        0.0327148, 0.0622559, 0.0512695, 0.0251465, 0.0212402, -0.0446777,\n",
       "        -0.179688, -0.0568848, -0.0358887, 0.00631714, 0.074707, 0.0593262,\n",
       "        0.162109, 0.0358887, -0.0786133, 0.0544434, -0.0810547, -0.0583496,\n",
       "        0.00762939, -0.0869141, -0.0113525, -0.0712891, -0.0622559,\n",
       "        0.0141602, -0.00891113, 0.0230713, 0.0192871], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.mlp.router.weight': Array([[-0.00273132, -0.00366211, 0.00213623, ..., -0.0035553,\n",
       "         -0.000159264, -0.00163269],\n",
       "        [0.00173187, -0.00280762, 0.00136566, ..., 0.00358582,\n",
       "         -0.00213623, -0.000564575],\n",
       "        [-0.000667572, 0.00427246, -0.00138092, ..., -0.00144196,\n",
       "         0.00273132, -0.000312805],\n",
       "        ...,\n",
       "        [0.00221252, -0.00512695, 0.000341415, ..., 0.00186157,\n",
       "         0.00418091, 0.0018158],\n",
       "        [-0.000183105, 0.000366211, 0.0035553, ..., -0.0017395,\n",
       "         0.00515747, 0.000284195],\n",
       "        [-0.00288391, 0.0045166, 0.00256348, ..., 0.00389099, 0.000801086,\n",
       "         0.00157166]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.14.post_attention_layernorm.weight': Array([1.22656, 1.5625, 1.75781, ..., 1.63281, 2.01562, 1.73438],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.attn.o_proj.bias': Array([1.46875, -0.0252686, -0.492188, ..., -0.149414, 0.460938, 0.263672],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.attn.o_proj.weight': Array([[-0.125, 0.574219, 0.158203, ..., 0.154297, -0.106445, 0.375],\n",
       "        [-0.292969, 0.253906, -0.402344, ..., 0.157227, 0.0205078,\n",
       "         0.0761719],\n",
       "        [-0.570312, 0.201172, -0.277344, ..., 0.0908203, 0.0917969,\n",
       "         0.036377],\n",
       "        ...,\n",
       "        [-0.554688, -0.100586, 0.19043, ..., 0.0113525, -0.0649414,\n",
       "         -0.0649414],\n",
       "        [0.194336, -0.283203, 0.0654297, ..., -0.0133667, -0.0625,\n",
       "         0.117188],\n",
       "        [0.131836, -0.351562, 0.447266, ..., 0.104004, 0.527344, 0.287109]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.attn.qkv_proj.bias': Array([0.404297, -0.433594, 0.0844727, ..., -0.0209961, 0.0625, 0.223633],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.attn.qkv_proj.weight': Array([[-0.00108337, -0.0402832, 0.00509644, ..., 0.0397949, 0.046875,\n",
       "         0.0566406],\n",
       "        [0.00341797, 0.000576019, 0.0336914, ..., -0.0527344, -0.010376,\n",
       "         -0.0488281],\n",
       "        [-0.006073, -0.0296631, -0.003479, ..., 0.00927734, -0.0128784,\n",
       "         0.0405273],\n",
       "        ...,\n",
       "        [-0.0119019, 0.00848389, 0.0088501, ..., 0.00476074, 0.0111084,\n",
       "         -0.0205078],\n",
       "        [-0.0688477, -0.0216064, 0.0135498, ..., -0.00836182, 0.0319824,\n",
       "         -0.015564],\n",
       "        [-0.0153198, 0.0510254, 0.0332031, ..., 0.0148926, -0.0869141,\n",
       "         -0.0424805]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.attn.sinks': Array([3.25, 3.65625, 3.25, 3.09375, 3.39062, 3.60938, 3.10938, 3.34375,\n",
       "        2.53125, 1.64844, 2.42188, 2.3125, 2.07812, 3.75, 1.71875, 2.4375,\n",
       "        3.10938, 2.9375, 3.07812, 2.5625, 2.75, 3.39062, 2.71875, 3.09375,\n",
       "        1.60938, 3.0625, 3.6875, 4.03125, 3.28125, 2.8125, 3.10938,\n",
       "        2.79688, 3.71875, 3.76562, 3.0625, 1.99219, 3.67188, 3.5625,\n",
       "        3.85938, 5.15625, 3.28125, 2.625, 2.64062, 2.03125, 1.60156,\n",
       "        2.8125, 3.17188, 2.51562, 3.5, 1.86719, 0.404297, 1.28125, 3.65625,\n",
       "        2.65625, 1.75, 2.48438, 1.71875, 1.63281, 2.35938, 2.42188,\n",
       "        1.74219, 1.875, 2.0625, 2.90625], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.input_layernorm.weight': Array([1.28906, 1.60156, 1.41406, ..., 1.4375, 1.67188, 1.60938],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.mlp.experts.w13_bias': Array([[0.134766, -0.152344, -0.0825195, ..., -0.671875, -0.519531,\n",
       "         -0.574219],\n",
       "        [-1.25, -0.796875, -0.0154419, ..., -0.535156, -0.460938,\n",
       "         -0.558594],\n",
       "        [0.125, -0.0991211, 0.0957031, ..., -0.273438, -0.472656,\n",
       "         -0.558594],\n",
       "        ...,\n",
       "        [0.339844, 0.138672, 0.259766, ..., -0.628906, -0.314453,\n",
       "         -0.167969],\n",
       "        [-0.113281, -0.135742, -0.0947266, ..., -0.135742, -0.462891,\n",
       "         -0.5625],\n",
       "        [-0.384766, 0.213867, 0.0830078, ..., -0.15332, -0.443359,\n",
       "         -0.398438]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.mlp.experts.w13_weight': Array([[[0.00292969, 0, -0.00585938, ..., 0.000976562, 0, 0.00390625],\n",
       "         [0.00292969, 0.000976562, 0.00292969, ..., 0.00195312,\n",
       "          -0.00195312, 0],\n",
       "         [0.00390625, -0.000976562, 0.00585938, ..., -0.00585938,\n",
       "          0.000976562, -0.00195312],\n",
       "         ...,\n",
       "         [0.0078125, -0.03125, 0.0078125, ..., 0.0625, 0.0078125,\n",
       "          0.015625],\n",
       "         [0.03125, -0.0625, -0.0078125, ..., -0.015625, 0.046875,\n",
       "          -0.0078125],\n",
       "         [-0.015625, -0.0625, -0.046875, ..., -0.0234375, 0.0078125, 0]],\n",
       " \n",
       "        [[0.0117188, -0.00390625, -0.0078125, ..., -0.0078125,\n",
       "          -0.0117188, 0.0117188],\n",
       "         [-0.0117188, -0.0078125, -0.0117188, ..., 0.0078125,\n",
       "          -0.00390625, 0.0078125],\n",
       "         [-0.0078125, -0.00585938, -0.0078125, ..., 0.00390625,\n",
       "          0.00585938, -0.00195312],\n",
       "         ...,\n",
       "         [0.015625, -0.0234375, -0.00390625, ..., -0.0234375, 0.015625,\n",
       "          0.0234375],\n",
       "         [0.015625, 0.0234375, -0.0234375, ..., 0.0234375, 0.0117188,\n",
       "          0.03125],\n",
       "         [-0.046875, -0.0234375, -0.0078125, ..., 0.0078125, -0.03125,\n",
       "          -0]],\n",
       " \n",
       "        [[0, -0.00195312, 0.00195312, ..., 0, 0.000976562, -0.00390625],\n",
       "         [0.00390625, 0.00195312, 0.00195312, ..., 0.00585938,\n",
       "          0.000976562, 0.000976562],\n",
       "         [0.000976562, -0.0078125, 0.000976562, ..., -0.00195312,\n",
       "          0.00390625, -0.0078125],\n",
       "         ...,\n",
       "         [-0.0234375, 0.0234375, 0.0234375, ..., 0.046875, -0,\n",
       "          -0.0078125],\n",
       "         [-0.03125, 0.0078125, 0, ..., -0.015625, -0.015625, 0],\n",
       "         [-0.015625, 0, 0.09375, ..., -0.03125, 0.0234375, 0.03125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00195312, -0, 0.00195312, ..., 0.00585938, -0.0078125,\n",
       "          -0.000976562],\n",
       "         [0, -0.00195312, 0.000976562, ..., -0.00585938, -0.00292969,\n",
       "          0.00195312],\n",
       "         [0.0117188, -0.00585938, -0.00195312, ..., -0.00585938,\n",
       "          -0.00195312, -0.0078125],\n",
       "         ...,\n",
       "         [0.0078125, -0.03125, 0.015625, ..., 0.015625, 0.015625,\n",
       "          0.09375],\n",
       "         [-0.046875, -0.0078125, -0.015625, ..., -0.015625, -0.015625,\n",
       "          -0.00390625],\n",
       "         [0.0078125, 0.03125, 0.015625, ..., 0.0078125, -0.00390625,\n",
       "          0.00390625]],\n",
       " \n",
       "        [[-0.0078125, 0.015625, 0.00585938, ..., 0.0117188, -0.0117188,\n",
       "          0.00195312],\n",
       "         [-0.00390625, 0.00390625, 0.00195312, ..., -0.00390625,\n",
       "          -0.0117188, 0.0078125],\n",
       "         [0.00390625, 0.00195312, 0.0078125, ..., -0.00390625,\n",
       "          -0.00195312, 0.00195312],\n",
       "         ...,\n",
       "         [0.0234375, 0.00390625, -0.0117188, ..., -0.0078125, -0.0117188,\n",
       "          0.00390625],\n",
       "         [0.0117188, 0.0234375, 0.0234375, ..., 0.0078125, 0.0234375,\n",
       "          0.0234375],\n",
       "         [-0.0234375, -0.03125, 0.03125, ..., -0.09375, 0.03125, 0.03125]],\n",
       " \n",
       "        [[0.0078125, -0.0117188, 0.0117188, ..., -0.00390625, -0.0234375,\n",
       "          -0.015625],\n",
       "         [-0.0117188, -0.0078125, -0.00390625, ..., -0.00390625,\n",
       "          -0.0117188, -0.00390625],\n",
       "         [-0.000976562, -0.00292969, 0, ..., -0, -0.00390625,\n",
       "          -0.000976562],\n",
       "         ...,\n",
       "         [-0.015625, -0.0234375, 0.0234375, ..., -0.0117188, -0.0078125,\n",
       "          0.0117188],\n",
       "         [-0.03125, -0.015625, -0.0078125, ..., 0.0078125, 0.0234375,\n",
       "          0.0625],\n",
       "         [0.015625, 0.03125, 0.0625, ..., 0.03125, 0.09375, 0.046875]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.mlp.experts.w2_bias': Array([[-2.21875, -0.0388184, 1.71094, ..., -1.48438, 9.125, -4.625],\n",
       "        [-0.808594, 1.5, 0.929688, ..., 2.6875, 0.960938, 0.53125],\n",
       "        [5.90625, -0.375, -0.0147095, ..., 0.380859, 0.640625, 2.21875],\n",
       "        ...,\n",
       "        [-4.1875, 3.96875, 0.511719, ..., 0.5, -3.4375, -1.99219],\n",
       "        [13.25, 4.78125, 2.625, ..., 3.40625, 5.8125, -4.46875],\n",
       "        [-3.96875, 6.3125, 3.20312, ..., -1.96875, 5.375, -4.21875]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.mlp.experts.w2_weight': Array([[[-1.5, -4, 2, ..., 2, 1, -1],\n",
       "         [-1, -0, 0.5, ..., -1, -1, 1],\n",
       "         [1.5, -1.5, -1, ..., -3, 2, -0],\n",
       "         ...,\n",
       "         [-0.5, 1, -1, ..., 1, -1.5, -2],\n",
       "         [1.5, -1, -0.5, ..., -2, -1, -3],\n",
       "         [-6, 6, 1.5, ..., 2, 4, 0]],\n",
       " \n",
       "        [[-3, -0, -1, ..., 2, 1, 2],\n",
       "         [-0.25, -0.25, 1, ..., -0.25, 0.5, -0.75],\n",
       "         [-1, 1.5, 0.5, ..., -1, -0.75, -1.5],\n",
       "         ...,\n",
       "         [-0.75, -0, -0.75, ..., -0, 0.5, 0.5],\n",
       "         [-0.75, -0.5, -0.5, ..., -0, 0, 1],\n",
       "         [-0.25, 0.75, 0, ..., 2, -2, -1]],\n",
       " \n",
       "        [[-12, 4, -3, ..., -6, 4, -2],\n",
       "         [0.5, 0, -1, ..., -0.5, -1.5, 2],\n",
       "         [-1, 2, 2, ..., 0, 0, 3],\n",
       "         ...,\n",
       "         [-1, -0, -0.5, ..., 0.5, -1, -0.5],\n",
       "         [0, 0, -3, ..., 3, 0, 1],\n",
       "         [-0, 3, 3, ..., -6, 6, -6]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[4, 2, -4, ..., 2, 3, 1],\n",
       "         [0.5, 4, -1, ..., -2, -0.75, -0.75],\n",
       "         [-0.25, 1.5, -0.25, ..., 1.5, 0, 0.25],\n",
       "         ...,\n",
       "         [0.25, 1.5, 0.5, ..., -1, -1, -1],\n",
       "         [3, 3, 0.5, ..., -2, 0.75, -1],\n",
       "         [-1, -2, 6, ..., -0, 1, -1]],\n",
       " \n",
       "        [[-2, 2, -1, ..., -4, -3, 4],\n",
       "         [2, 1.5, 2, ..., -2, 1.5, -1.5],\n",
       "         [1, -0, -1, ..., 1, 1.5, 0.5],\n",
       "         ...,\n",
       "         [3, 0.5, -0.5, ..., -3, 1, -0.5],\n",
       "         [-0.5, -1.5, -0.75, ..., 0.5, 2, 0.5],\n",
       "         [-1.5, -0.5, 4, ..., 1.5, -1, -2]],\n",
       " \n",
       "        [[-2, 2, 4, ..., -3, -6, 4],\n",
       "         [-1.5, 0.5, 1.5, ..., 1.5, 1, 3],\n",
       "         [-1.5, -0, -0, ..., -2, 1.5, 1],\n",
       "         ...,\n",
       "         [-0.5, -0.5, 1, ..., 0.5, -1.5, 1],\n",
       "         [-0.5, 1.5, 1.5, ..., 1.5, 0.5, 2],\n",
       "         [0, -0, -2, ..., 1, 6, 4]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.mlp.router.bias': Array([0.046875, 0.090332, -0.0120239, -0.0849609, 0.013855, 0.0252686,\n",
       "        -0.144531, 0.0334473, -0.0252686, -0.1875, -0.109863, -0.00393677,\n",
       "        0.0324707, 0.0393066, 0.0222168, -0.00260925, 0.00964355,\n",
       "        0.0441895, -0.0281982, -0.065918, -0.0673828, 0.15332, 0.0162354,\n",
       "        -0.0186768, 0.0098877, -0.0181885, -0.171875, 0.0844727, 0.0437012,\n",
       "        -0.0196533, -0.0227051, -0.0206299], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.mlp.router.weight': Array([[0.00283813, 0.00257874, 0.000189781, ..., 0.000843048,\n",
       "         0.000598907, 0.00238037],\n",
       "        [0.000534058, -0.00598145, -0.000713348, ..., 0.00170135,\n",
       "         0.00267029, 7.62939e-06],\n",
       "        [-0.000991821, 0.00738525, 0.00122833, ..., -0.00117493,\n",
       "         0.00069809, -0.00219727],\n",
       "        ...,\n",
       "        [-0.00601196, -0.0146484, -0.00379944, ..., -0.00349426,\n",
       "         -0.00222778, 0.00102234],\n",
       "        [-0.000862122, 0.00372314, -0.0020752, ..., -0.00224304,\n",
       "         -0.000408173, 0.000659943],\n",
       "        [0.00043869, 0.00308228, 0.00436401, ..., -0.001091, 0.000629425,\n",
       "         -0.000915527]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.15.post_attention_layernorm.weight': Array([1.03125, 1.53125, 1.75, ..., 1.52344, 1.78125, 1.5], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.attn.o_proj.bias': Array([1.46875, 0.108398, -0.535156, ..., 0.75, 0.163086, -0.703125],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.attn.o_proj.weight': Array([[-0.671875, -0.103027, 1.21875, ..., -0.0893555, -0.355469,\n",
       "         -0.478516],\n",
       "        [-0.0742188, -0.103027, 0.0771484, ..., -0.0761719, 0.285156,\n",
       "         0.200195],\n",
       "        [0.289062, 0.145508, 0.423828, ..., 0.251953, 0.12207, 0.0732422],\n",
       "        ...,\n",
       "        [-0.0568848, 0.283203, -0.777344, ..., -0.114258, -0.18457,\n",
       "         -0.371094],\n",
       "        [-0.182617, 0.542969, -0.25, ..., -0.057373, -0.136719, 0.0179443],\n",
       "        [0.117188, -0.0148315, 0.667969, ..., 0.392578, -0.101562,\n",
       "         0.703125]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.attn.qkv_proj.bias': Array([0.238281, -0.382812, -0.1875, ..., 0.117188, 0.104004, -0.423828],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.attn.qkv_proj.weight': Array([[-0.00518799, -0.0154419, -0.00759888, ..., -0.0163574,\n",
       "         0.00976562, -0.00891113],\n",
       "        [2.74181e-05, -0.00683594, -0.0140381, ..., 0.00445557,\n",
       "         -0.0378418, 0.00714111],\n",
       "        [-0.00567627, -0.0146484, 0.00201416, ..., -0.0123291,\n",
       "         -0.00592041, 0.00108337],\n",
       "        ...,\n",
       "        [-0.0098877, 0.0308838, -0.0537109, ..., -0.0471191, -0.00491333,\n",
       "         -0.0568848],\n",
       "        [0.0117798, -0.104004, -0.0537109, ..., 0.0280762, 0.0158691,\n",
       "         0.00521851],\n",
       "        [0.0161133, -0.103516, -0.0174561, ..., 0.138672, -0.00387573,\n",
       "         -0.0537109]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.attn.sinks': Array([1.83594, 1.83594, 2.1875, 2.6875, 1.47656, 2.125, 2.17188, 2.46875,\n",
       "        1.79688, 2.01562, 1.82812, 1.51562, 1.61719, 1.58594, 1.57812,\n",
       "        1.95312, 2.01562, 1.70312, 1.90625, 2.625, 2.03125, 1.69531,\n",
       "        2.76562, 2.45312, 1.00781, 1.46875, 1.76562, -0.316406, 1.38281,\n",
       "        0.929688, 0.679688, 0.347656, 1.38281, 2.01562, 1.60938, 2.09375,\n",
       "        0.0800781, 1.58594, 2.20312, 1.20312, 1.85938, 1.58594, 1.875,\n",
       "        1.55469, 1.72656, 1.19531, 0.472656, 2.01562, 1.71094, 2.0625,\n",
       "        2.26562, 1.89062, 1.375, 1.11719, 1.91406, 1.58594, 1.4375,\n",
       "        1.21094, 1.42969, 1.60938, 1.39062, 1.92969, 1.14844, -0.263672],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.input_layernorm.weight': Array([1.625, 1.86719, 1.72656, ..., 1.71094, 1.98438, 1.70312], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.mlp.experts.w13_bias': Array([[-0.111328, -0.167969, -0.21582, ..., -0.259766, -0.478516,\n",
       "         -0.523438],\n",
       "        [-0.695312, -0.71875, 0.101074, ..., -0.34375, -0.796875,\n",
       "         -0.380859],\n",
       "        [-0.123535, -0.988281, 0.0257568, ..., -0.287109, -0.369141,\n",
       "         -1.07812],\n",
       "        ...,\n",
       "        [0.0228271, -0.416016, -0.34375, ..., -0.984375, -0.625,\n",
       "         -0.746094],\n",
       "        [-0.0500488, 0.40625, 0.0314941, ..., -0.400391, -0.464844,\n",
       "         -0.546875],\n",
       "        [-0.318359, -0.0629883, -0.182617, ..., -0.585938, -0.451172,\n",
       "         -0.427734]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.mlp.experts.w13_weight': Array([[[0.0117188, 0.0078125, 0.0117188, ..., 0.0078125, -0.00390625,\n",
       "          -0.00195312],\n",
       "         [-0.00390625, -0.00195312, -0.00390625, ..., 0, -0.00195312,\n",
       "          -0.00195312],\n",
       "         [-0.0078125, 0.0078125, 0.0117188, ..., 0.00195312, -0.00195312,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [0.09375, -0.03125, -0.0078125, ..., -0, 0.0625, 0],\n",
       "         [-0.046875, 0, 0.0625, ..., -0.046875, 0.015625, 0.015625],\n",
       "         [0.0234375, 0.0078125, -0.046875, ..., -0.0078125, 0.03125, -0]],\n",
       " \n",
       "        [[-0.0078125, -0.0234375, 0, ..., 0.015625, -0.0234375, 0],\n",
       "         [0.00390625, -0.0078125, 0, ..., -0.00390625, 0.0234375,\n",
       "          -0.0078125],\n",
       "         [-0, 0.0117188, -0.00390625, ..., -0.00585938, -0.00585938,\n",
       "          -0.00585938],\n",
       "         ...,\n",
       "         [-0.046875, 0.0234375, 0.03125, ..., -0.0625, 0, 0.0078125],\n",
       "         [-0.09375, 0.0625, -0.0625, ..., -0.03125, -0.046875, 0.03125],\n",
       "         [-0.046875, 0, -0.015625, ..., -0.0078125, 0.0078125, 0.015625]],\n",
       " \n",
       "        [[-0.00585938, -0.00195312, 0.0078125, ..., -0.00195312,\n",
       "          0.0078125, 0.0078125],\n",
       "         [-0.0078125, -0.015625, 0.0117188, ..., 0.0078125, -0.00390625,\n",
       "          0.00390625],\n",
       "         [-0.0078125, 0.00390625, 0.00195312, ..., -0.00195312,\n",
       "          -0.00195312, -0.0117188],\n",
       "         ...,\n",
       "         [0.0234375, -0.0078125, 0.046875, ..., 0.015625, 0.03125, -0],\n",
       "         [0.0234375, 0.03125, -0.0078125, ..., -0.0078125, 0.00390625,\n",
       "          0.0234375],\n",
       "         [0.015625, -0.03125, -0.015625, ..., -0, -0, -0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.000976562, -0.00195312, 0.00195312, ..., 0.00195312, 0,\n",
       "          0.00585938],\n",
       "         [0.00195312, -0.0117188, 0.0117188, ..., -0.00585938, 0.0078125,\n",
       "          -0.0117188],\n",
       "         [-0.0078125, -0.00390625, 0.00390625, ..., -0.0078125,\n",
       "          -0.00585938, 0],\n",
       "         ...,\n",
       "         [0, 0.03125, 0.015625, ..., -0, -0.0234375, -0.0078125],\n",
       "         [0.03125, 0.015625, 0.0625, ..., 0.03125, -0.03125, 0],\n",
       "         [-0.0078125, 0.0078125, 0.03125, ..., -0.0078125, -0.0625,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[-0.00292969, -0.0078125, -0, ..., 0.00195312, 0.00390625, -0],\n",
       "         [-0, -0.00195312, -0.00585938, ..., 0.00292969, -0.000976562,\n",
       "          -0.00195312],\n",
       "         [0.00585938, -0.00195312, -0.0078125, ..., -0.0078125,\n",
       "          -0.00390625, -0.00390625],\n",
       "         ...,\n",
       "         [0.015625, -0.046875, 0.03125, ..., 0.0234375, -0.0078125,\n",
       "          -0.046875],\n",
       "         [-0.046875, 0.0078125, 0.015625, ..., -0, -0.0078125, 0.0234375],\n",
       "         [-0.09375, -0.03125, 0.03125, ..., -0.046875, 0, -0.09375]],\n",
       " \n",
       "        [[0.00390625, -0.00585938, -0.00390625, ..., 0.0078125,\n",
       "          0.00195312, -0.0117188],\n",
       "         [-0.00195312, 0.0078125, 0.00292969, ..., -0.000976562,\n",
       "          -0.00390625, 0.00585938],\n",
       "         [0.015625, 0.00390625, -0.00390625, ..., -0.00292969,\n",
       "          0.00585938, 0.000976562],\n",
       "         ...,\n",
       "         [-0.0078125, 0.03125, -0.00390625, ..., 0.0234375, -0.03125,\n",
       "          -0.015625],\n",
       "         [-0, 0.015625, 0.0078125, ..., -0.046875, 0.0078125, 0.0078125],\n",
       "         [-0.0078125, -0.03125, -0.015625, ..., 0.00390625, 0.015625,\n",
       "          0.00390625]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.mlp.experts.w2_bias': Array([[4.65625, -3.21875, 4.1875, ..., 0.597656, 7.375, -3.54688],\n",
       "        [-9.8125, -0.992188, -1.71875, ..., 2, 9.3125, -2.4375],\n",
       "        [-6.875, 1.05469, -0.386719, ..., -1.54688, 2.48438, -3.84375],\n",
       "        ...,\n",
       "        [9.1875, 3.79688, 0.96875, ..., 5.90625, 7.4375, -5.46875],\n",
       "        [6.28125, 3.85938, 5.78125, ..., 0.0116577, -1.53125, -1.83594],\n",
       "        [-3.03125, 1.67969, 3.14062, ..., 4.65625, 1.625, -4.84375]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.mlp.experts.w2_weight': Array([[[0, 3, 3, ..., -3, -2, 8],\n",
       "         [1.5, 2, 0.5, ..., 2, 2, -0.5],\n",
       "         [1, 6, -0, ..., -1, 4, -1.5],\n",
       "         ...,\n",
       "         [2, -0, -0, ..., -2, 2, 4],\n",
       "         [0, 3, -1, ..., -3, 3, -4],\n",
       "         [-0, 6, -2, ..., 4, -3, 6]],\n",
       " \n",
       "        [[1, -2, 6, ..., -6, -1, 1],\n",
       "         [0.5, -1.5, 0, ..., -2, 0, 3],\n",
       "         [-1.5, -1, 0, ..., 2, 0.5, 3],\n",
       "         ...,\n",
       "         [1.5, 0, -0.5, ..., -1, -0.5, -1],\n",
       "         [0.5, 1, 0.5, ..., 0.5, -1.5, 1.5],\n",
       "         [3, -1.5, 4, ..., 1, 1, -1]],\n",
       " \n",
       "        [[-3, -3, 1, ..., 6, 8, -2],\n",
       "         [-1, 0.5, -1, ..., -1.5, 2, -0.25],\n",
       "         [-1, 2, -0.5, ..., 2, 4, 0],\n",
       "         ...,\n",
       "         [-0.5, 1, -0.5, ..., -0, 1, -0.5],\n",
       "         [-1, -2, -2, ..., -3, 0.5, 1.5],\n",
       "         [0, -2, 1, ..., -4, 3, 2]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[4, -3, 4, ..., 2, -4, -0],\n",
       "         [4, 0.5, 0, ..., 0.5, 2, -0.5],\n",
       "         [2, 0, 1.5, ..., -0, -0, -1.5],\n",
       "         ...,\n",
       "         [-1, 2, 1, ..., -1, 0.5, -1.5],\n",
       "         [-3, 1, -2, ..., 3, -1, -1],\n",
       "         [-8, 2, -2, ..., 1, -1, 1]],\n",
       " \n",
       "        [[3, -2, -8, ..., -4, 8, 4],\n",
       "         [-2, 1, 1, ..., 4, -0, -1],\n",
       "         [-0, -0.5, 2, ..., -3, -2, -0],\n",
       "         ...,\n",
       "         [-0.5, 1.5, 3, ..., -2, -2, -2],\n",
       "         [1, 1, 2, ..., -2, -2, -0],\n",
       "         [-8, -0, -4, ..., -0, 3, -1]],\n",
       " \n",
       "        [[-0, -4, -4, ..., 3, -2, 4],\n",
       "         [-0, 1.5, -1.5, ..., -2, 1.5, 2],\n",
       "         [3, 1.5, 3, ..., 0, -1, -3],\n",
       "         ...,\n",
       "         [-0, -4, -0.5, ..., -1, 1, 0],\n",
       "         [0.5, 3, -2, ..., 0, 0, 4],\n",
       "         [-4, 2, 0, ..., 3, -0, -2]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.mlp.router.bias': Array([-0.0771484, 0.0249023, 0.0324707, 0.0175781, 0.186523, -0.0942383,\n",
       "        -0.117676, 0.0358887, -0.0187988, -0.00643921, -0.0800781,\n",
       "        -0.0140991, 0.0439453, 0.0698242, 0.0302734, -0.167969, 0.0161133,\n",
       "        -0.150391, 0.046875, 0.0227051, -0.0756836, -0.166992, -0.006073,\n",
       "        0.00946045, 0.0446777, 0.000598907, 0.0383301, 0.0495605,\n",
       "        0.0262451, 0.0250244, -0.0620117, 0.0546875], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.mlp.router.weight': Array([[0.00239563, -0.000743866, -0.000537872, ..., 0.00332642,\n",
       "         -0.000379562, 0.000240326],\n",
       "        [-0.0038147, 0.000728607, 0.000953674, ..., -0.000297546,\n",
       "         0.00358582, -0.00028038],\n",
       "        [-0.00198364, -0.000587463, 0.0050354, ..., 0.0039978,\n",
       "         0.000701904, 0.00442505],\n",
       "        ...,\n",
       "        [0.00257874, -0.00186157, -0.00205994, ..., -0.00286865,\n",
       "         -0.00236511, 0.000144005],\n",
       "        [-0.00164032, -0.0115967, -0.010498, ..., -0.00564575,\n",
       "         -0.00183105, -0.00056839],\n",
       "        [0.00112152, 0.00463867, 0.00344849, ..., -0.00210571, 0.0016098,\n",
       "         0.00031662]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.16.post_attention_layernorm.weight': Array([0.824219, 1.53906, 1.57031, ..., 1.54688, 1.61719, 1.19531],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.attn.o_proj.bias': Array([1.60938, -1.09375, 0.777344, ..., 2.59375, 0.78125, -0.941406],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.attn.o_proj.weight': Array([[0.160156, 0.726562, -0.015564, ..., 0.257812, -0.726562,\n",
       "         0.143555],\n",
       "        [0.0576172, 0.233398, -0.0908203, ..., -0.00191498, -0.373047,\n",
       "         -0.0227051],\n",
       "        [0.0825195, 0.578125, -0.257812, ..., -0.0927734, -0.289062,\n",
       "         0.474609],\n",
       "        ...,\n",
       "        [-0.0429688, 0.0385742, -0.316406, ..., 0.332031, -0.204102,\n",
       "         0.251953],\n",
       "        [0.00765991, 0.124512, 0.265625, ..., -0.902344, -0.279297,\n",
       "         -0.0378418],\n",
       "        [0.00817871, -0.275391, 0.322266, ..., 0.367188, -0.53125,\n",
       "         -0.175781]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.attn.qkv_proj.bias': Array([-0.855469, 0.208984, -0.824219, ..., -0.100098, -0.136719,\n",
       "        -0.00418091], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.attn.qkv_proj.weight': Array([[0.00196838, -0.0324707, -0.0122681, ..., -0.0141602, 0.00540161,\n",
       "         0.0169678],\n",
       "        [-0.0189209, -0.0152588, -0.0551758, ..., -0.0568848, 0.034668,\n",
       "         -0.015564],\n",
       "        [-0.00075531, -0.0196533, 0.00427246, ..., -0.0791016, 0.00549316,\n",
       "         -0.0407715],\n",
       "        ...,\n",
       "        [-0.00289917, 0.0257568, 0.0148926, ..., 0.103027, 0.0493164,\n",
       "         -0.00289917],\n",
       "        [0.0214844, -0.0222168, -0.0116577, ..., 0.0090332, -0.0437012,\n",
       "         -0.0169678],\n",
       "        [-0.0305176, -0.0913086, -0.0422363, ..., 0.0116577, 0.050293,\n",
       "         0.0169678]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.attn.sinks': Array([-1.02344, 1.58594, 1.44531, 1.30469, 1.45312, 1.21094, 1.04688,\n",
       "        1.76562, 3.39062, 3.34375, 3.28125, 3.79688, 3.28125, 2.53125,\n",
       "        3.28125, 2.6875, 1.48438, 2, 1.89844, 1.14844, 2.60938, 1.77344,\n",
       "        1.53125, 2.48438, 2.15625, 1.92188, 1.61719, 1.9375, 1.09375,\n",
       "        1.21875, 1.69531, 2.15625, 2.48438, 2.8125, 2.54688, 2.375,\n",
       "        2.32812, 2.46875, 2.65625, 2.73438, 2.875, 3.84375, 3.42188,\n",
       "        2.57812, 2.65625, 2.9375, 3.9375, 1.35938, 2.65625, 2.26562,\n",
       "        2.01562, 2.125, 2.09375, 1.72656, 2.23438, 2.26562, 1.61719,\n",
       "        1.66406, 1.80469, 1.60938, 1.36719, 1.70312, 1.61719, 1.42188],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.input_layernorm.weight': Array([1.14062, 1.92188, 1.53906, ..., 1.53906, 1.57812, 1.42188],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.mlp.experts.w13_bias': Array([[-0.0332031, -0.24707, -0.11084, ..., -0.742188, -0.601562,\n",
       "         -0.298828],\n",
       "        [-0.208008, -0.84375, -0.0898438, ..., -0.324219, -0.691406,\n",
       "         -0.0373535],\n",
       "        [-1.85938, -0.828125, 0.019043, ..., -0.550781, -0.542969,\n",
       "         -0.761719],\n",
       "        ...,\n",
       "        [-0.925781, 0.0441895, -0.632812, ..., -0.738281, -0.359375,\n",
       "         -0.511719],\n",
       "        [-0.208984, -0.0957031, -0.542969, ..., 0.0490723, 0.0742188,\n",
       "         -0.328125],\n",
       "        [-0.234375, -0.425781, 0.168945, ..., -0.703125, -0.447266,\n",
       "         -0.523438]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.mlp.experts.w13_weight': Array([[[-0.000976562, -0.00195312, 0.00195312, ..., 0.00390625,\n",
       "          -0.00390625, 0.00195312],\n",
       "         [-0.00585938, -0.00292969, 0.000976562, ..., -0.00195312,\n",
       "          0.00585938, 0.00390625],\n",
       "         [-0.00585938, 0.0078125, -0, ..., 0.00390625, 0.0078125,\n",
       "          -0.0117188],\n",
       "         ...,\n",
       "         [-0, -0.015625, -0.0078125, ..., 0.0078125, -0.0234375,\n",
       "          0.0078125],\n",
       "         [0.015625, 0.03125, -0.046875, ..., -0.03125, -0.015625,\n",
       "          -0.046875],\n",
       "         [-0.03125, -0.0078125, 0.03125, ..., 0.0117188, 0.0117188,\n",
       "          -0.00390625]],\n",
       " \n",
       "        [[-0.0078125, 0.00585938, 0.00585938, ..., -0.015625, -0.0078125,\n",
       "          0.0117188],\n",
       "         [-0.0117188, 0.0117188, 0.015625, ..., -0.015625, 0.00390625,\n",
       "          0.0234375],\n",
       "         [0.00195312, 0.00195312, 0.00195312, ..., -0.00585938,\n",
       "          0.0078125, 0.00195312],\n",
       "         ...,\n",
       "         [0, -0.0234375, -0.0234375, ..., 0.0625, -0.015625, -0.03125],\n",
       "         [-0.0078125, -0, -0, ..., -0.046875, -0.015625, -0.015625],\n",
       "         [-0.03125, -0.03125, -0.03125, ..., 0.015625, 0.046875, -0.0625]],\n",
       " \n",
       "        [[0.046875, -0.0625, -0.0625, ..., 0.015625, -0.03125, 0.015625],\n",
       "         [-0.0078125, -0.015625, -0, ..., -0.00390625, 0.0117188,\n",
       "          0.0078125],\n",
       "         [-0, 0.0078125, -0, ..., -0.00390625, 0.0078125, 0.0117188],\n",
       "         ...,\n",
       "         [-0.00390625, 0, -0.0234375, ..., -0.0078125, -0.00390625,\n",
       "          -0.0078125],\n",
       "         [0.0078125, 0.03125, -0.046875, ..., 0.03125, 0.0078125,\n",
       "          -0.0234375],\n",
       "         [0.0234375, 0.015625, 0.0078125, ..., -0.0078125, -0.0234375,\n",
       "          -0.0234375]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00390625, 0.015625, 0.015625, ..., 0.03125, 0.0234375,\n",
       "          -0.0078125],\n",
       "         [-0.00390625, -0.000976562, -0.00195312, ..., -0.00195312,\n",
       "          -0.000976562, 0.00292969],\n",
       "         [0.0117188, -0.00585938, -0.015625, ..., -0.0078125, 0.0078125,\n",
       "          0.0234375],\n",
       "         ...,\n",
       "         [-0.046875, 0.03125, -0.015625, ..., -0.015625, 0.015625,\n",
       "          0.046875],\n",
       "         [0.015625, -0.015625, 0.015625, ..., -0.03125, -0.03125, -0],\n",
       "         [0, 0.1875, 0.015625, ..., 0.0625, 0.09375, 0.0625]],\n",
       " \n",
       "        [[0.0117188, -0.00195312, 0.0078125, ..., 0.00195312, -0.0078125,\n",
       "          -0.00195312],\n",
       "         [0.0078125, 0.00390625, -0, ..., 0.00390625, -0.00195312,\n",
       "          -0.0078125],\n",
       "         [-0.0078125, -0.00390625, 0.0234375, ..., 0.00390625,\n",
       "          -0.0117188, 0.015625],\n",
       "         ...,\n",
       "         [-0.0117188, -0.03125, -0.0117188, ..., -0.0078125, -0.0078125,\n",
       "          0.0234375],\n",
       "         [-0, 0.046875, -0.0234375, ..., 0.015625, 0, 0.0234375],\n",
       "         [-0.015625, 0.0625, -0.0078125, ..., -0, 0.015625, 0.0625]],\n",
       " \n",
       "        [[-0.00195312, -0.00585938, -0.00195312, ..., -0.00390625,\n",
       "          0.00195312, 0.0078125],\n",
       "         [-0.0117188, -0.0078125, -0.0078125, ..., 0.0117188,\n",
       "          -0.00195312, -0.00585938],\n",
       "         [0.00195312, 0, 0.00585938, ..., -0.0078125, -0.000976562,\n",
       "          -0.00195312],\n",
       "         ...,\n",
       "         [0, 0.0234375, -0.0078125, ..., -0.0234375, -0.0078125,\n",
       "          -0.015625],\n",
       "         [-0.0078125, -0.015625, 0.0234375, ..., -0.03125, -0.0078125,\n",
       "          0.0078125],\n",
       "         [0.015625, -0, 0.0234375, ..., -0.0078125, -0, 0.046875]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.mlp.experts.w2_bias': Array([[21.375, 4.9375, 7.65625, ..., 8.75, 6.09375, -3.8125],\n",
       "        [-13.4375, -3.5, 9.75, ..., 6.0625, -1.04688, -11.625],\n",
       "        [11.3125, 3.70312, -0.601562, ..., 6.4375, -5.71875, -6.96875],\n",
       "        ...,\n",
       "        [11.125, 2.45312, 8.4375, ..., 11.3125, -2.17188, -18],\n",
       "        [-0.886719, 1.57031, 4.65625, ..., -4.90625, -10.0625, 0.203125],\n",
       "        [1.82031, 6.6875, 5.09375, ..., 5.75, -1.70312, 0.451172]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.mlp.experts.w2_weight': Array([[[-1, -0, 4, ..., 12, 6, 0],\n",
       "         [1.5, 3, 0.5, ..., -1, 0.5, 2],\n",
       "         [6, 1, -3, ..., 4, -0, -2],\n",
       "         ...,\n",
       "         [3, 1, -0, ..., -2, -0, -2],\n",
       "         [3, -1, 2, ..., -4, -3, -2],\n",
       "         [-1, 4, 4, ..., -1, -6, 2]],\n",
       " \n",
       "        [[-2, 6, 0, ..., 8, -6, 6],\n",
       "         [-1, 3, 3, ..., 1, 1, 3],\n",
       "         [1, 1, 0, ..., 6, 0, -4],\n",
       "         ...,\n",
       "         [0.5, -1.5, -2, ..., -1.5, 1, 3],\n",
       "         [-6, -1, 2, ..., 6, -6, 2],\n",
       "         [-2, 2, -4, ..., -4, 4, 2]],\n",
       " \n",
       "        [[-0, -4, 0, ..., 1, -12, -0],\n",
       "         [-1.5, -2, 0, ..., -0, 0, -1.5],\n",
       "         [-0.5, 0, 1, ..., 1, 4, 1.5],\n",
       "         ...,\n",
       "         [1.5, -0, 1.5, ..., -1, -2, -0.5],\n",
       "         [-0.5, -1.5, 0.5, ..., 0.5, -0, -2],\n",
       "         [-1, -12, 1, ..., -1, 1, -2]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-6, 4, -2, ..., -2, 2, -0],\n",
       "         [-0.5, -2, 0.5, ..., -0.5, -2, 0],\n",
       "         [3, 0.5, -4, ..., 3, -3, 2],\n",
       "         ...,\n",
       "         [-0.5, -3, -1, ..., 6, 1, -1.5],\n",
       "         [-1, -0, 6, ..., 0.5, 4, 0.5],\n",
       "         [6, -8, 2, ..., -1, 3, 6]],\n",
       " \n",
       "        [[-2, 12, 6, ..., -0, 4, 4],\n",
       "         [-4, 4, -4, ..., 2, -0, 0],\n",
       "         [0, 1, 3, ..., 1, -2, 4],\n",
       "         ...,\n",
       "         [0, 0.5, 1, ..., 0, -1, 1],\n",
       "         [1, -4, -0, ..., -2, 1, -4],\n",
       "         [-8, -3, 4, ..., 1, -4, -8]],\n",
       " \n",
       "        [[-2, 4, 2, ..., -12, -6, 6],\n",
       "         [-0.5, -2, -2, ..., 3, 1, -3],\n",
       "         [-4, 4, 1, ..., 3, 3, 1.5],\n",
       "         ...,\n",
       "         [-1, -3, -0, ..., -0.5, 6, -1],\n",
       "         [-4, 0.5, 3, ..., 1.5, -3, 2],\n",
       "         [2, -6, 2, ..., -0, 6, -4]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.mlp.router.bias': Array([-0.0571289, -0.001091, 0.0717773, 0.19043, 0.0549316, 0.28125,\n",
       "        0.0217285, -0.0393066, -0.0712891, 0.001091, -0.048584, 0.0133057,\n",
       "        -0.100586, -0.213867, -0.0668945, 0.171875, 0.0235596, -0.175781,\n",
       "        -0.170898, 0.0107422, -0.0600586, 0.122559, -0.0131226,\n",
       "        -0.00582886, 0.0834961, -0.122559, 0.124023, 0.0151978, -0.135742,\n",
       "        -0.0952148, 0.0067749, 0.00239563], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.mlp.router.weight': Array([[0.00799561, 0.0043335, 0.0017395, ..., -0.00576782, -0.0032959,\n",
       "         -3.67165e-05],\n",
       "        [-0.00393677, 0.00273132, 0.00141907, ..., 0.00224304,\n",
       "         -0.000610352, -0.00147247],\n",
       "        [0.000583649, -0.000850677, 0.00704956, ..., -0.0020752,\n",
       "         0.00149536, 0.000869751],\n",
       "        ...,\n",
       "        [-0.00537109, -0.00102234, 0.00445557, ..., -0.0045166,\n",
       "         0.00147247, -0.00262451],\n",
       "        [0.00012207, -0.00604248, -0.00720215, ..., 0.0043335, 0.00193787,\n",
       "         -0.00151825],\n",
       "        [-0.000234604, -0.00921631, -0.000694275, ..., -0.00460815,\n",
       "         0.00268555, 0.00460815]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.17.post_attention_layernorm.weight': Array([0.699219, 1.60938, 1.53125, ..., 1.5, 1.47656, 1.04688], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.attn.o_proj.bias': Array([0.898438, -0.318359, 0.546875, ..., 1.03125, 0.24707, 0.726562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.attn.o_proj.weight': Array([[-0.0673828, 0.154297, -0.851562, ..., 0.308594, -0.535156,\n",
       "         0.202148],\n",
       "        [-0.0432129, -0.150391, -0.18457, ..., -0.00439453, -0.133789,\n",
       "         0.259766],\n",
       "        [0.644531, 0.269531, -0.255859, ..., -0.0231934, -0.236328,\n",
       "         0.306641],\n",
       "        ...,\n",
       "        [0.369141, 0.032959, 0.117676, ..., 0.0639648, -0.0908203,\n",
       "         -0.154297],\n",
       "        [-0.105469, 0.114746, 0.246094, ..., 0.120117, 0.173828, -0.34375],\n",
       "        [-0.0664062, 0.378906, -0.11084, ..., 0.0761719, 0.168945,\n",
       "         -0.734375]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.attn.qkv_proj.bias': Array([0.0231934, 0.201172, -0.0932617, ..., -0.275391, 0.365234,\n",
       "        0.115723], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.attn.qkv_proj.weight': Array([[0.00144958, -0.0139771, 0.00138855, ..., 0.00933838, 0.00698853,\n",
       "         0.00125885],\n",
       "        [-0.00332642, -0.00296021, 0.00570679, ..., 0.00463867, 0.0124512,\n",
       "         0.00135803],\n",
       "        [-0.00201416, 0.00830078, -0.000103474, ..., -0.00100708,\n",
       "         -0.00872803, -0.00512695],\n",
       "        ...,\n",
       "        [-0.0139771, 0.0698242, 0.074707, ..., -0.0300293, 0.078125,\n",
       "         0.027832],\n",
       "        [0.0153198, -0.0393066, 0.0233154, ..., -0.00138855, -0.0622559,\n",
       "         -0.015625],\n",
       "        [-0.00260925, -0.0336914, -0.0256348, ..., 0.0390625, -0.0834961,\n",
       "         0.0322266]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.attn.sinks': Array([1.83594, 1.76562, 1.67188, 1.63281, 1.19531, 1.17969, 0.0849609,\n",
       "        1.80469, 0.988281, 0.78125, 1.28125, -0.0139771, 1.02344, 1.57031,\n",
       "        1.03125, 1.16406, 0.894531, 0.176758, 1.19531, 0.498047, 2,\n",
       "        2.57812, 1.22656, 0.5, 1.38281, 1.8125, 1.00781, 1.85156, 1.4375,\n",
       "        1.64062, 1.70312, 1.5625, 1.91406, 0.660156, 0.683594, 1.09375,\n",
       "        1.07812, 1.1875, 0.601562, -1.42188, 1.53125, 1.90625, 1.58594,\n",
       "        1.35156, 1.17188, 1.85938, 1.59375, 1.22656, 0.816406, 1.48438,\n",
       "        0.652344, 1.01562, 1.96094, -0.163086, 1.22656, -0.640625, 1.25781,\n",
       "        1.71094, 1.11719, 1.60156, 1.16406, 0.386719, 1.86719, 1.17188],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.input_layernorm.weight': Array([1.25, 1.88281, 1.625, ..., 1.61719, 1.79688, 1.47656], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.mlp.experts.w13_bias': Array([[-0.231445, -0.486328, -0.0703125, ..., -0.361328, -0.367188,\n",
       "         -0.566406],\n",
       "        [-0.390625, -0.0981445, -1.34375, ..., -0.458984, -0.439453,\n",
       "         -0.427734],\n",
       "        [-0.141602, -0.0751953, -0.0512695, ..., -0.523438, -0.196289,\n",
       "         -0.761719],\n",
       "        ...,\n",
       "        [-0.146484, -0.0888672, 0.109375, ..., -0.361328, -0.330078,\n",
       "         -0.447266],\n",
       "        [-0.0688477, -0.589844, -0.162109, ..., -0.279297, -0.425781,\n",
       "         -0.392578],\n",
       "        [-0.808594, -0.585938, -0.0986328, ..., -0.396484, -0.589844,\n",
       "         -0.376953]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.mlp.experts.w13_weight': Array([[[0.0078125, 0.0078125, -0.0078125, ..., 0.0078125, -0.0117188,\n",
       "          0.00390625],\n",
       "         [0.0117188, -0.00390625, -0.0078125, ..., -0.00390625,\n",
       "          -0.00195312, 0.0078125],\n",
       "         [-0.00390625, -0, 0.0078125, ..., -0.00195312, 0.00585938,\n",
       "          -0.0117188],\n",
       "         ...,\n",
       "         [-0.0078125, -0.00390625, 0.0234375, ..., -0.015625, 0.015625,\n",
       "          -0.0078125],\n",
       "         [-0.015625, -0.015625, -0.015625, ..., 0, -0.03125, 0],\n",
       "         [-0.0234375, -0.046875, 0.0234375, ..., 0.046875, 0.0078125,\n",
       "          0.0078125]],\n",
       " \n",
       "        [[-0.00390625, 0.0078125, 0.00390625, ..., -0.00195312,\n",
       "          0.00195312, 0.00195312],\n",
       "         [-0.00195312, -0.00585938, -0.00195312, ..., 0.00390625,\n",
       "          -0.00390625, 0.00390625],\n",
       "         [-0.015625, -0.0078125, -0.0234375, ..., -0.03125, -0.0078125,\n",
       "          -0.00390625],\n",
       "         ...,\n",
       "         [-0.0234375, 0.0234375, 0.00390625, ..., -0.015625, -0.03125,\n",
       "          -0.0078125],\n",
       "         [0.015625, -0.015625, 0, ..., -0.0078125, -0.046875, 0.015625],\n",
       "         [0.015625, -0.0625, -0.015625, ..., -0.00390625, 0.015625,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[0.00195312, 0.00390625, 0.00585938, ..., 0.00585938,\n",
       "          0.00390625, 0.000976562],\n",
       "         [0.00585938, 0.00390625, -0.00390625, ..., 0.000976562,\n",
       "          0.00195312, 0.000976562],\n",
       "         [0.0078125, -0.00390625, 0.00195312, ..., 0.00195312, 0,\n",
       "          0.00585938],\n",
       "         ...,\n",
       "         [0.09375, 0.015625, -0.046875, ..., -0.046875, 0.09375, 0.03125],\n",
       "         [0.03125, 0.03125, 0.03125, ..., -0.03125, -0, 0.015625],\n",
       "         [0.03125, -0.0625, -0.0625, ..., -0.09375, 0.046875, -0.015625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00292969, 0, -0.0078125, ..., -0.00390625, 0.00390625,\n",
       "          -0.00292969],\n",
       "         [0, 0.03125, 0.015625, ..., -0, 0.00390625, -0.015625],\n",
       "         [-0, 0.00195312, 0, ..., -0.000976562, -0.00585938, -0.00195312],\n",
       "         ...,\n",
       "         [0, 0.0078125, 0.0234375, ..., 0.0078125, 0.015625, 0.0234375],\n",
       "         [-0.0078125, 0.0078125, -0.015625, ..., -0.015625, -0.00390625,\n",
       "          0.015625],\n",
       "         [-0.0078125, -0.0234375, -0.0625, ..., -0.03125, 0.0234375, 0]],\n",
       " \n",
       "        [[-0, -0, -0.00585938, ..., -0.00390625, 0, 0],\n",
       "         [0.0234375, -0.0078125, 0.015625, ..., -0.015625, -0.00390625,\n",
       "          -0],\n",
       "         [-0.00390625, -0.00390625, 0.00390625, ..., -0.00390625,\n",
       "          0.0078125, 0.00195312],\n",
       "         ...,\n",
       "         [-0.0117188, 0.0078125, -0, ..., 0.015625, 0.015625, 0.00390625],\n",
       "         [0.00390625, -0.0117188, 0.0234375, ..., 0.0117188, -0.00390625,\n",
       "          0],\n",
       "         [0.0234375, -0.0117188, 0.03125, ..., 0.0078125, 0.015625,\n",
       "          -0.0234375]],\n",
       " \n",
       "        [[0.0078125, -0.0117188, 0.015625, ..., -0.0234375, 0.015625,\n",
       "          -0.00390625],\n",
       "         [-0.015625, 0.0078125, 0, ..., -0.0078125, -0.0117188,\n",
       "          0.0078125],\n",
       "         [0.0078125, -0.00195312, -0, ..., -0.015625, 0.00195312,\n",
       "          -0.00585938],\n",
       "         ...,\n",
       "         [-0.0625, 0.03125, -0.0234375, ..., -0.046875, -0.0078125,\n",
       "          0.0234375],\n",
       "         [0, -0.0078125, 0.015625, ..., 0.03125, 0.0078125, 0.046875],\n",
       "         [-0.0234375, -0.03125, 0.0078125, ..., 0.015625, 0.015625, 0]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.mlp.experts.w2_bias': Array([[44.5, 2.89062, 4.34375, ..., -5.34375, 0.722656, -12.6875],\n",
       "        [-15, -2.20312, 4.5625, ..., -10.875, -8.625, 4.28125],\n",
       "        [17.75, 1.95312, 10.3125, ..., 3.64062, 10.875, 3.4375],\n",
       "        ...,\n",
       "        [-5.125, -0.200195, 6.09375, ..., -0.753906, -8.0625, 0.392578],\n",
       "        [-13.6875, -11.625, -2.54688, ..., -17, -8.75, -1.50781],\n",
       "        [19.25, 4.9375, 1.69531, ..., 3.32812, -12.25, 8.3125]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.mlp.experts.w2_weight': Array([[[-8, -4, -2, ..., -6, 12, 2],\n",
       "         [-0.5, 0.5, -2, ..., -0, 1, -1],\n",
       "         [-1, -3, -1.5, ..., -1, 1.5, 1],\n",
       "         ...,\n",
       "         [-1, 2, -4, ..., -0.5, 1.5, -0.5],\n",
       "         [-3, 0.5, 1, ..., -6, -4, -2],\n",
       "         [-6, 8, -8, ..., -8, -4, -2]],\n",
       " \n",
       "        [[12, 12, 4, ..., -0, -2, 8],\n",
       "         [-4, -4, -4, ..., 3, 0, 3],\n",
       "         [-2, 0, -1, ..., -6, 2, -1],\n",
       "         ...,\n",
       "         [3, -0.5, -2, ..., -1, 1, 1],\n",
       "         [-2, -4, -4, ..., 2, -4, -3],\n",
       "         [-12, -2, -4, ..., 6, 2, 6]],\n",
       " \n",
       "        [[-8, 2, -6, ..., -12, 12, 12],\n",
       "         [-2, 8, 1, ..., 1, 2, -3],\n",
       "         [-1, 2, -1, ..., -2, 6, -2],\n",
       "         ...,\n",
       "         [-3, -2, 6, ..., -2, -6, -0],\n",
       "         [0, -12, -0, ..., 6, 6, 0],\n",
       "         [8, -2, -0, ..., -6, -16, 8]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[8, -0, -12, ..., -8, 4, 2],\n",
       "         [0, -3, -4, ..., -6, 0, -2],\n",
       "         [0, -1, -12, ..., -4, 3, -6],\n",
       "         ...,\n",
       "         [3, 2, 3, ..., 6, -0, 4],\n",
       "         [-2, -2, 3, ..., 3, 0.5, -1.5],\n",
       "         [8, 6, 2, ..., -6, 0, 8]],\n",
       " \n",
       "        [[-4, -0, -6, ..., -6, 0, -8],\n",
       "         [1, -1, -2, ..., -4, 1, 6],\n",
       "         [4, 1, 3, ..., 2, -2, -1.5],\n",
       "         ...,\n",
       "         [-4, 0, 3, ..., 3, 3, -1.5],\n",
       "         [2, 2, -8, ..., -1, -1, -4],\n",
       "         [-1, -2, 8, ..., -4, -2, 6]],\n",
       " \n",
       "        [[0, 8, 0, ..., 6, 2, 4],\n",
       "         [1, 2, -1, ..., 2, -4, 2],\n",
       "         [-1, 2, -4, ..., 8, -2, 1],\n",
       "         ...,\n",
       "         [-2, 8, 1, ..., -0, -1, -2],\n",
       "         [3, 3, 2, ..., -4, -3, -3],\n",
       "         [-8, -2, 3, ..., -8, 2, 2]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.mlp.router.bias': Array([0.116211, 0.0786133, -0.201172, 0.0324707, 0.00315857, 0.0693359,\n",
       "        -0.125, 0.171875, 0.0159912, -0.0532227, -0.0893555, 0.113281,\n",
       "        0.113281, -0.206055, 0.0559082, 0.074707, -0.0189209, 0.142578,\n",
       "        0.00854492, 0.0673828, -0.0908203, -0.0305176, -0.0375977,\n",
       "        -0.0712891, -0.119629, -0.00448608, -0.0354004, -0.277344,\n",
       "        -0.0678711, 0.0297852, 0.0249023, 0.0120239], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.mlp.router.weight': Array([[0.00680542, 0.000237465, -0.00170135, ..., 0.0045166, 0.00257874,\n",
       "         -0.00147247],\n",
       "        [-0.000278473, -0.00976562, 0.000553131, ..., 0.00288391,\n",
       "         -0.00292969, 0.00280762],\n",
       "        [0.00656128, 0.00564575, -0.00218201, ..., -0.00063324,\n",
       "         -0.00245667, -0.0012207],\n",
       "        ...,\n",
       "        [-0.00512695, -0.00427246, -6.53267e-05, ..., 0.00576782,\n",
       "         -0.000163078, 0.00494385],\n",
       "        [-0.0045166, -0.0043335, 0.00222778, ..., 0.00598145, -0.00323486,\n",
       "         0.00239563],\n",
       "        [-0.00182343, -0.000701904, -0.00149536, ..., -0.00854492,\n",
       "         0.000453949, -0.000999451]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.18.post_attention_layernorm.weight': Array([0.6875, 1.65625, 1.55469, ..., 1.5625, 1.47656, 0.953125],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.attn.o_proj.bias': Array([1.52344, -0.664062, 0.53125, ..., 0.172852, -0.621094, 1.86719],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.attn.o_proj.weight': Array([[0.53125, 0.0334473, -1.07812, ..., -0.898438, 1.11719, 0.103516],\n",
       "        [0.0140381, -0.439453, -0.192383, ..., -0.204102, -0.742188,\n",
       "         -0.0480957],\n",
       "        [0.193359, -1.03906, 0.0756836, ..., -0.141602, -0.675781,\n",
       "         -0.032959],\n",
       "        ...,\n",
       "        [-0.335938, -0.223633, 0.664062, ..., 0.173828, 0.640625,\n",
       "         0.464844],\n",
       "        [-0.441406, -0.239258, -0.205078, ..., 0.00297546, 0.466797,\n",
       "         -0.121094],\n",
       "        [0.132812, -1.15625, -0.0629883, ..., 1.33594, -0.558594,\n",
       "         0.523438]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.attn.qkv_proj.bias': Array([-0.0544434, 0.0150146, -0.0534668, ..., -0.214844, 0.0649414,\n",
       "        -0.0294189], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.attn.qkv_proj.weight': Array([[0.00265503, 0.0203857, 0.000459671, ..., -0.00543213,\n",
       "         -0.00671387, -0.00469971],\n",
       "        [0.00479126, -0.00479126, 0.0001297, ..., -0.00219727,\n",
       "         -0.000579834, 0.000999451],\n",
       "        [-0.00811768, 0.00476074, 0.00308228, ..., 0.0127563, -0.00958252,\n",
       "         -0.00628662],\n",
       "        ...,\n",
       "        [-0.00479126, 0.0466309, -0.0227051, ..., 0.0124512, -0.00750732,\n",
       "         0.0524902],\n",
       "        [0.0373535, 0.0126953, -0.0249023, ..., 0.00271606, 0.00958252,\n",
       "         0.0412598],\n",
       "        [-0.0432129, -0.065918, 0.0703125, ..., 0.102539, 0.0203857,\n",
       "         0.0339355]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.attn.sinks': Array([1.40625, 1.64062, 1.70312, 1.23438, 1.89062, 1.70312, 1.5, 1.10938,\n",
       "        2.73438, 3.73438, 1.40625, 3.09375, 2.59375, 2.57812, 2.98438,\n",
       "        3.07812, 4.15625, 3.42188, 2.75, 2.39062, 2.92188, 1.17969,\n",
       "        2.03125, 3.92188, 1.36719, 1.71094, 1.53125, 2.5, 1.42969, 1.8125,\n",
       "        2.125, 1.86719, 2.67188, 2.57812, 3.04688, 3.73438, 2.45312,\n",
       "        2.96875, 2.32812, 3.59375, 2.42188, 1.67969, 1.32031, 1.54688,\n",
       "        2.03125, 1.625, 1.77344, 1.99219, 2.23438, 2.42188, 2.46875,\n",
       "        2.35938, 1.96875, 2.17188, 1.79688, 2.0625, 1.6875, 1.82031,\n",
       "        1.61719, 1.39844, 1.35156, 2.125, 1.76562, 1.64062],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.input_layernorm.weight': Array([0.886719, 1.78125, 1.35938, ..., 1.6875, 1.57031, 1.10156],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.mlp.experts.w13_bias': Array([[-0.675781, -0.482422, -0.515625, ..., -0.0976562, -0.625,\n",
       "         -0.5625],\n",
       "        [-0.0830078, -2.03125, -1.75, ..., -0.652344, -0.53125, -1.48438],\n",
       "        [-0.0771484, -0.00125885, -0.125977, ..., -0.18457, -0.369141,\n",
       "         -0.640625],\n",
       "        ...,\n",
       "        [0.0189209, -0.199219, -0.273438, ..., -0.451172, -0.777344,\n",
       "         -0.457031],\n",
       "        [-0.458984, -1.13281, -0.707031, ..., -0.244141, -0.484375,\n",
       "         -0.671875],\n",
       "        [-0.0908203, -0.0668945, -0.53125, ..., -0.621094, 0.213867,\n",
       "         -0.761719]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.mlp.experts.w13_weight': Array([[[-0.0117188, 0.00390625, -0.0078125, ..., 0.00390625,\n",
       "          -0.0078125, 0.0078125],\n",
       "         [0.0117188, -0.00390625, -0.0117188, ..., -0.0078125, 0.0117188,\n",
       "          0.0117188],\n",
       "         [-0.0117188, -0.0078125, 0.0117188, ..., 0.015625, -0.00390625,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [0.0078125, 0.046875, 0.015625, ..., 0.015625, 0.03125,\n",
       "          -0.0078125],\n",
       "         [-0, 0, 0.0078125, ..., 0.015625, 0.0625, 0.03125],\n",
       "         [0.0234375, -0.09375, 0.046875, ..., 0.09375, -0.015625,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[0.00195312, -0.00390625, 0.00195312, ..., -0.00585938,\n",
       "          0.00195312, -0.0078125],\n",
       "         [0.015625, 0.0078125, -0.0078125, ..., -0.015625, 0, -0.0078125],\n",
       "         [-0.0078125, -0.00390625, -0.00390625, ..., -0.0117188, 0.03125,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [-0.0117188, -0.00390625, 0.03125, ..., 0.0078125, 0.046875,\n",
       "          0.015625],\n",
       "         [-0.00390625, -0.0078125, -0.00390625, ..., 0.0078125, 0.015625,\n",
       "          0.0234375],\n",
       "         [0.0078125, 0.0078125, -0.015625, ..., 0, 0.0078125, -0]],\n",
       " \n",
       "        [[0.00292969, 0.00292969, 0.00585938, ..., 0.00390625,\n",
       "          0.00390625, 0.00292969],\n",
       "         [0.000976562, 0.00585938, -0, ..., 0, -0.000976562, 0.00390625],\n",
       "         [-0.00195312, -0.00195312, -0.00195312, ..., 0.00292969,\n",
       "          -0.000976562, 0.0078125],\n",
       "         ...,\n",
       "         [-0.015625, 0.015625, -0.015625, ..., -0.03125, 0.03125, -0],\n",
       "         [0.03125, 0.015625, -0.015625, ..., 0.015625, -0.015625,\n",
       "          0.015625],\n",
       "         [-0.0625, 0.015625, 0, ..., -0.03125, -0.125, 0.046875]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00585938, 0.00390625, 0.00195312, ..., -0.00585938, -0, -0],\n",
       "         [0.00195312, 0.00195312, 0.0078125, ..., -0.00195312, 0.0078125,\n",
       "          -0.00585938],\n",
       "         [-0.00585938, 0.0234375, 0.00585938, ..., 0.00390625,\n",
       "          -0.00195312, 0.00390625],\n",
       "         ...,\n",
       "         [-0.0234375, 0.015625, -0, ..., -0, -0.0234375, -0.0078125],\n",
       "         [-0.0078125, 0.0234375, -0.015625, ..., -0, -0.015625, -0],\n",
       "         [0.0234375, 0.046875, 0.0234375, ..., -0.046875, 0.046875,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[-0.0078125, 0.00390625, -0.00390625, ..., -0.0117188,\n",
       "          0.0078125, 0.0117188],\n",
       "         [0.00390625, -0.015625, 0.0234375, ..., 0.0117188, -0.015625,\n",
       "          -0.015625],\n",
       "         [-0.0078125, 0.00390625, 0.0078125, ..., 0.0078125, -0.00390625,\n",
       "          -0.00390625],\n",
       "         ...,\n",
       "         [-0.015625, 0.0078125, -0.046875, ..., 0, 0.0078125, 0.0078125],\n",
       "         [-0.0234375, 0, 0.0078125, ..., -0.0234375, -0.0078125, 0],\n",
       "         [0.0078125, 0.015625, -0.015625, ..., 0.0078125, -0.0234375,\n",
       "          0.0234375]],\n",
       " \n",
       "        [[0.00292969, -0.000976562, -0.00292969, ..., 0.000976562,\n",
       "          -0.00195312, -0.00390625],\n",
       "         [-0.00585938, -0.00390625, -0, ..., -0.00585938, -0.00390625,\n",
       "          -0.000976562],\n",
       "         [0.0117188, -0.0117188, -0.00390625, ..., -0.00390625,\n",
       "          -0.0078125, -0.0078125],\n",
       "         ...,\n",
       "         [0, 0.0078125, 0.0625, ..., 0, -0.0078125, -0.03125],\n",
       "         [-0.0234375, 0.0078125, -0, ..., -0, -0.0234375, 0.015625],\n",
       "         [-0.015625, 0, -0, ..., -0.0625, 0.0625, 0.03125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.mlp.experts.w2_bias': Array([[28.5, 15.75, 11.625, ..., -3, -12.3125, 3.78125],\n",
       "        [3.90625, -1.75, 6.84375, ..., -11.3125, 3.04688, -3.5],\n",
       "        [-11.625, -0.824219, 19.875, ..., -0.679688, 7.90625, -14.8125],\n",
       "        ...,\n",
       "        [-0.882812, 4.6875, -3.51562, ..., -3.42188, 6.125, -8.875],\n",
       "        [5.8125, 10.3125, 0.211914, ..., 4.9375, 6.21875, 11.6875],\n",
       "        [-7.09375, 5.8125, 9.75, ..., -17.125, 8.9375, 6.65625]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.mlp.experts.w2_weight': Array([[[8, 4, 6, ..., 6, -8, -8],\n",
       "         [-4, -4, -6, ..., -3, 2, 1],\n",
       "         [6, -0, -0, ..., -0, 4, 1],\n",
       "         ...,\n",
       "         [4, -6, 6, ..., 6, -1, 6],\n",
       "         [-2, 3, 1, ..., 3, 2, 4],\n",
       "         [4, 4, 8, ..., -0, 2, 2]],\n",
       " \n",
       "        [[6, 2, -6, ..., 0, 8, -12],\n",
       "         [-2, 4, 0, ..., -3, -0, 3],\n",
       "         [-3, 1, 8, ..., 1, 4, 2],\n",
       "         ...,\n",
       "         [-2, -4, -4, ..., 1, -1, 8],\n",
       "         [-6, -16, 2, ..., 2, -6, 0],\n",
       "         [-12, -0, -6, ..., -8, -2, 16]],\n",
       " \n",
       "        [[-2, 2, -8, ..., -2, 24, -4],\n",
       "         [1, -1, -0, ..., 0, 4, 2],\n",
       "         [-6, 0, 4, ..., 1, 8, 2],\n",
       "         ...,\n",
       "         [3, -3, 0, ..., 2, -4, -2],\n",
       "         [1, -1, 1, ..., -4, 6, 0],\n",
       "         [16, -2, -0, ..., -0, -0, 4]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-4, -4, -24, ..., 8, -12, 12],\n",
       "         [2, 4, 3, ..., -8, 6, -2],\n",
       "         [6, -3, -1, ..., 3, 0, 1],\n",
       "         ...,\n",
       "         [1, -0, 3, ..., 2, 1, 3],\n",
       "         [6, -1, 1, ..., 3, -1, 6],\n",
       "         [-6, 16, 2, ..., -4, -8, -0]],\n",
       " \n",
       "        [[12, 4, -16, ..., 12, -2, -2],\n",
       "         [3, 0, -3, ..., 2, 6, 3],\n",
       "         [0, 1, -6, ..., 6, -6, -0],\n",
       "         ...,\n",
       "         [1, 2, 1, ..., -2, -8, 4],\n",
       "         [0, -1, 4, ..., 6, 6, 4],\n",
       "         [8, -4, 6, ..., 12, 0, 8]],\n",
       " \n",
       "        [[2, 4, -12, ..., 0, 16, -4],\n",
       "         [3, 3, 2, ..., 3, 2, 4],\n",
       "         [-4, -3, -1, ..., 6, 4, -1],\n",
       "         ...,\n",
       "         [8, -0, 12, ..., 8, -2, 2],\n",
       "         [1, 4, 6, ..., -4, -0, 2],\n",
       "         [8, 4, -6, ..., -2, -6, 0]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.mlp.router.bias': Array([0.123535, -0.0544434, -0.214844, -0.0546875, -0.0175781,\n",
       "        -0.0106812, 0.116699, 0.0639648, 0.0185547, -0.0284424, -0.0708008,\n",
       "        -0.0366211, -0.0810547, -0.134766, 0.115723, 0.120605, -0.0722656,\n",
       "        -0.121094, 0.0251465, -0.0200195, -0.202148, 0.0255127, 0.171875,\n",
       "        0.0222168, 0.00811768, 0.15918, 0.0205078, -0.166992, 0.108887,\n",
       "        0.0405273, 0.0145264, -0.188477], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.mlp.router.weight': Array([[0.00325012, -0.00164032, 0.00288391, ..., -0.00256348,\n",
       "         -0.00180817, 0.00692749],\n",
       "        [0.00680542, 0.00598145, 0.00212097, ..., 0.00378418, 0.00582886,\n",
       "         -0.00193024],\n",
       "        [0.00131989, 0.0129395, 0.00257874, ..., 0.00349426, 0.00056839,\n",
       "         -0.00338745],\n",
       "        ...,\n",
       "        [-0.00521851, 0.00778198, 0.00552368, ..., -0.0030365,\n",
       "         -0.00196838, -0.000862122],\n",
       "        [-0.00622559, -0.00069809, 0.000295639, ..., -0.000667572,\n",
       "         -0.0038147, -0.000484467],\n",
       "        [0.00202942, 0.00576782, 0.00424194, ..., 0.00386047, 0.00157928,\n",
       "         -0.00179291]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.19.post_attention_layernorm.weight': Array([0.644531, 1.72656, 1.625, ..., 1.5625, 1.41406, 0.824219],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.attn.o_proj.bias': Array([-0.0593262, 0.0825195, -0.0776367, ..., -0.149414, 0.00701904,\n",
       "        -0.034668], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.attn.o_proj.weight': Array([[-0.00622559, -0.00205994, 0.00543213, ..., 0.00518799,\n",
       "         0.00379944, -0.00619507],\n",
       "        [0.0272217, -0.00717163, 0.00854492, ..., 0.00131989, -0.00616455,\n",
       "         0.0197754],\n",
       "        [0.00610352, 0.00292969, -0.00592041, ..., 0.00958252, 0.00552368,\n",
       "         0.00698853],\n",
       "        ...,\n",
       "        [0.00288391, -0.00741577, 0.00411987, ..., -0.0039978,\n",
       "         -0.00405884, 0.00219727],\n",
       "        [-0.00491333, 0.00108337, -0.00247192, ..., -0.00244141,\n",
       "         -0.000789642, 0.000606537],\n",
       "        [0.00512695, -0.00204468, 4.19617e-05, ..., -0.0178223,\n",
       "         -0.00228882, 0.000930786]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.attn.qkv_proj.bias': Array([0.00308228, -0.0341797, 0.0397949, ..., -0.12793, -0.0180664,\n",
       "        0.139648], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.attn.qkv_proj.weight': Array([[-0.000511169, 8.53539e-05, 0.000602722, ..., 0.00317383,\n",
       "         -0.00524902, -0.000617981],\n",
       "        [0.00062561, -0.000549316, 1.41859e-05, ..., -0.000896454,\n",
       "         -0.00111389, -0.00025177],\n",
       "        [0.00259399, -0.00180817, 0.00100708, ..., -0.0109253, 0.010437,\n",
       "         -0.00497437],\n",
       "        ...,\n",
       "        [0.00364685, -0.0311279, -0.00622559, ..., 0.0595703, 0.0883789,\n",
       "         0.0639648],\n",
       "        [0.0317383, 0.00982666, 0.0332031, ..., -0.00118256, -0.0380859,\n",
       "         0.0683594],\n",
       "        [0.0163574, -0.0256348, -0.0114746, ..., -0.145508, -0.0947266,\n",
       "         0.0634766]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.attn.sinks': Array([3.1875, 2.78125, 3.79688, 2.5625, 3.59375, 3.09375, 3.14062,\n",
       "        1.55469, 1.09375, 1.07031, 1.92969, 2.15625, 3.9375, 0.90625,\n",
       "        0.46875, 1.33594, 2.03125, 3.09375, 2.92188, 1.69531, 1.86719,\n",
       "        2.26562, 1.78125, 2.15625, 2.6875, 2.85938, 3.01562, 1.96094,\n",
       "        2.23438, 2.75, 2.64062, 2.71875, 1.79688, 2.89062, 1.5625, 3.09375,\n",
       "        2.42188, 3.48438, 2, 1.55469, 2.14062, 2.125, 2.70312, 2.25,\n",
       "        2.10938, 1.91406, 2.21875, 3.09375, 2.57812, 2.375, 2.25, 3.04688,\n",
       "        3.28125, 4.1875, 2.53125, 3.26562, 3.14062, 3.07812, 4.375,\n",
       "        2.70312, 2.9375, 2.5625, 2.48438, 3.20312], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.input_layernorm.weight': Array([1.78906, 1.46875, 1.35156, ..., 2.8125, 2.6875, 2.10938], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.mlp.experts.w13_bias': Array([[-0.171875, -0.204102, -0.0976562, ..., -0.628906, -0.976562,\n",
       "         -0.953125],\n",
       "        [-0.341797, -0.466797, -1.40625, ..., -0.964844, -0.910156,\n",
       "         -0.90625],\n",
       "        [-0.300781, -0.237305, -0.253906, ..., -0.726562, -0.941406,\n",
       "         -0.84375],\n",
       "        ...,\n",
       "        [-0.25, -0.519531, -0.267578, ..., -0.839844, -0.318359, -0.5625],\n",
       "        [-0.0185547, -0.597656, -0.554688, ..., -0.859375, -0.984375,\n",
       "         -0.960938],\n",
       "        [0.0303955, -0.542969, -0.960938, ..., -0.953125, -0.910156,\n",
       "         -0.867188]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.mlp.experts.w13_weight': Array([[[0.015625, -0.00195312, 0.00195312, ..., -0.0117188, 0.0078125,\n",
       "          -0.015625],\n",
       "         [0.00390625, 0.0117188, 0.0117188, ..., -0, -0.03125,\n",
       "          -0.00390625],\n",
       "         [0.015625, -0.00390625, -0, ..., -0.015625, 0.00390625,\n",
       "          -0.0234375],\n",
       "         ...,\n",
       "         [-0.0625, -0.015625, 0, ..., -0.03125, 0.03125, -0.03125],\n",
       "         [-0.015625, 0.046875, -0.046875, ..., -0.015625, 0.046875,\n",
       "          0.0625],\n",
       "         [0.0625, 0.0234375, 0, ..., -0.0078125, 0.03125, -0.0078125]],\n",
       " \n",
       "        [[0.03125, 0.0078125, -0, ..., -0.015625, -0.015625, 0.03125],\n",
       "         [-0.0078125, -0.03125, 0.0117188, ..., 0.0625, -0.015625,\n",
       "          0.03125],\n",
       "         [0, 0.0234375, 0.0234375, ..., -0.03125, 0.015625, 0.03125],\n",
       "         ...,\n",
       "         [0.015625, 0.015625, -0.03125, ..., -0, 0.046875, -0.015625],\n",
       "         [-0.046875, 0.03125, -0.0234375, ..., 0.0078125, 0.046875,\n",
       "          -0.015625],\n",
       "         [-0, 0.015625, -0.0078125, ..., 0, 0.046875, 0.03125]],\n",
       " \n",
       "        [[-0.0078125, 0.0078125, -0.0078125, ..., 0, 0.0078125,\n",
       "          0.0078125],\n",
       "         [0.03125, 0.0078125, 0.0234375, ..., -0.0078125, 0.03125, 0],\n",
       "         [-0.00390625, -0.00390625, -0.015625, ..., -0.0078125, 0.015625,\n",
       "          0.0078125],\n",
       "         ...,\n",
       "         [0.0625, -0.0078125, 0, ..., -0.0625, -0.03125, 0.0625],\n",
       "         [0.015625, -0.015625, 0.03125, ..., 0.03125, 0.03125, 0.046875],\n",
       "         [-0.0234375, -0.0078125, 0, ..., 0.0625, 0.046875, 0.03125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0, -0.03125, 0.0078125, ..., 0.015625, -0.046875, -0.0078125],\n",
       "         [0.0078125, -0.0234375, 0, ..., 0.03125, 0.015625, -0.0078125],\n",
       "         [-0.0234375, 0.00390625, 0.015625, ..., -0.015625, 0.046875,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [-0.0234375, 0.015625, -0.0234375, ..., 0.0625, -0.015625,\n",
       "          0.03125],\n",
       "         [-0.0078125, -0.03125, 0.0078125, ..., 0.015625, -0.0234375,\n",
       "          0.03125],\n",
       "         [-0.015625, -0.015625, -0.046875, ..., 0.0078125, 0.0234375,\n",
       "          0.0078125]],\n",
       " \n",
       "        [[-0.0078125, 0.015625, -0.0078125, ..., 0, 0.0078125,\n",
       "          0.00390625],\n",
       "         [-0.00390625, 0.0234375, -0.0078125, ..., -0.00390625, 0.03125,\n",
       "          0.03125],\n",
       "         [-0.03125, 0.0234375, -0.0078125, ..., -0.046875, 0.046875,\n",
       "          0.0078125],\n",
       "         ...,\n",
       "         [-0.03125, -0, -0.0078125, ..., -0.015625, 0.0625, 0.0625],\n",
       "         [-0.015625, -0.046875, 0.046875, ..., 0.03125, -0.03125,\n",
       "          0.015625],\n",
       "         [-0.046875, 0.015625, -0.015625, ..., 0.03125, 0.046875,\n",
       "          0.0234375]],\n",
       " \n",
       "        [[-0.0078125, -0, -0.0078125, ..., 0.046875, 0.015625, 0.0078125],\n",
       "         [0.0117188, 0.0234375, -0.03125, ..., -0.0234375, 0.0078125,\n",
       "          -0.0078125],\n",
       "         [-0.0078125, 0, 0.0234375, ..., -0.0117188, -0.015625,\n",
       "          0.00390625],\n",
       "         ...,\n",
       "         [0.0078125, 0.0078125, 0.03125, ..., 0, 0.03125, 0.046875],\n",
       "         [0.03125, -0.0625, 0.03125, ..., 0.015625, 0.03125, 0.03125],\n",
       "         [0.0234375, -0, -0, ..., -0.0078125, -0.0078125, 0.0234375]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.mlp.experts.w2_bias': Array([[-0.0299072, -0.189453, 0.0668945, ..., 0.189453, 0.144531,\n",
       "         -0.0187988],\n",
       "        [-0.0253906, -0.0825195, -0.131836, ..., 0.0198975, -0.0388184,\n",
       "         -0.0356445],\n",
       "        [-0.0410156, 0.0693359, -0.108398, ..., 0.0883789, 0.0712891,\n",
       "         -0.0227051],\n",
       "        ...,\n",
       "        [-0.0147095, 0.00187683, -0.010437, ..., 0.0524902, -0.0218506,\n",
       "         0.00732422],\n",
       "        [-0.0544434, -0.0756836, 0.0264893, ..., 0.0874023, 0.0568848,\n",
       "         0.0756836],\n",
       "        [-0.0270996, -0.107422, -0.078125, ..., 0.171875, 0.00982666,\n",
       "         -0.019165]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.mlp.experts.w2_weight': Array([[[-0.046875, 0.046875, -0.046875, ..., -0.09375, -0, 0.03125],\n",
       "         [0.125, -0.0625, -0, ..., -0.03125, 0.0625, 0.0625],\n",
       "         [0.09375, -0.0625, -0.015625, ..., 0.09375, -0.03125, -0.015625],\n",
       "         ...,\n",
       "         [0.0625, -0.0625, -0.03125, ..., 0.015625, -0.03125, -0.03125],\n",
       "         [0.03125, -0.046875, 0.0234375, ..., 0.0234375, 0.046875,\n",
       "          -0.015625],\n",
       "         [-0.015625, 0.015625, -0.03125, ..., 0.0078125, -0.0078125,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[-0.03125, 0.046875, 0.0625, ..., -0.015625, 0.015625,\n",
       "          0.0234375],\n",
       "         [0.09375, 0.015625, 0.1875, ..., 0.09375, 0.03125, 0],\n",
       "         [0, -0.015625, -0.125, ..., -0.015625, 0.046875, 0.015625],\n",
       "         ...,\n",
       "         [-0.0234375, 0, -0.015625, ..., 0.0625, -0, 0.0078125],\n",
       "         [0.015625, 0.0078125, 0.046875, ..., 0.0117188, -0.0234375,\n",
       "          -0.0234375],\n",
       "         [0.046875, 0.0078125, -0.0078125, ..., -0.0078125, -0,\n",
       "          0.0234375]],\n",
       " \n",
       "        [[0.03125, 0.015625, -0, ..., 0.03125, -0.03125, 0.0078125],\n",
       "         [-0, -0.03125, 0.015625, ..., -0.03125, 0.015625, -0],\n",
       "         [-0.0078125, -0.03125, -0.0234375, ..., -0.03125, -0.0625,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [-0.03125, -0.046875, 0.046875, ..., -0.0234375, 0.0078125,\n",
       "          -0.0234375],\n",
       "         [-0.0234375, 0.0078125, -0.0234375, ..., 0.046875, 0.015625,\n",
       "          0.015625],\n",
       "         [-0.015625, 0.015625, 0.0234375, ..., -0.0078125, -0.0078125,\n",
       "          0.046875]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.0078125, 0.03125, -0.015625, ..., -0.0625, 0.015625,\n",
       "          -0.0078125],\n",
       "         [-0, 0.015625, -0.015625, ..., -0.046875, 0.0078125, 0.0625],\n",
       "         [-0.015625, -0.0625, 0.03125, ..., 0.03125, 0.03125, 0.015625],\n",
       "         ...,\n",
       "         [-0, 0, -0.03125, ..., 0.03125, 0.015625, -0.015625],\n",
       "         [0.0078125, 0.0234375, -0.0078125, ..., 0.0078125, 0.0078125, 0],\n",
       "         [0.03125, -0.0078125, -0.015625, ..., -0, 0.0234375, 0.0234375]],\n",
       " \n",
       "        [[-0.0234375, -0.0078125, -0.0078125, ..., 0.0234375, -0.03125,\n",
       "          -0.046875],\n",
       "         [0.0078125, -0.0078125, -0.03125, ..., 0.09375, -0.0234375,\n",
       "          0.0234375],\n",
       "         [-0.03125, 0.046875, -0.0078125, ..., -0.0234375, -0.0078125,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [-0.015625, 0, -0.0078125, ..., 0.03125, 0.015625, -0],\n",
       "         [0.00390625, -0.0078125, 0.015625, ..., 0.0234375, -0.015625,\n",
       "          0.0234375],\n",
       "         [-0.00390625, -0.0078125, 0.0117188, ..., -0.046875, 0.0117188,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[-0, -0.015625, -0, ..., 0.046875, -0, -0],\n",
       "         [-0, -0.09375, -0.0625, ..., -0.09375, 0.03125, -0.09375],\n",
       "         [-0.03125, -0.03125, 0.015625, ..., -0.03125, 0.03125, 0],\n",
       "         ...,\n",
       "         [0.015625, -0.0234375, 0.015625, ..., 0.015625, 0.015625,\n",
       "          0.0078125],\n",
       "         [0.00390625, 0, 0.0117188, ..., 0.0078125, -0.00390625,\n",
       "          0.0078125],\n",
       "         [-0.046875, -0, 0, ..., 0.03125, -0, -0.0078125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.mlp.router.bias': Array([-0.267578, 0.0644531, 0.0639648, -0.300781, 0.109863, -0.0185547,\n",
       "        -0.0129395, -0.0771484, -0.527344, 0.130859, 0.0810547, 0.0825195,\n",
       "        -0.048584, 0.169922, 0.135742, -0.134766, 0.0703125, 0.0766602,\n",
       "        0.0595703, 0.0639648, -0.03125, -0.0644531, -0.585938, -0.186523,\n",
       "        0.0407715, 0.0834961, 0.00738525, 0.0617676, -0.0942383, -0.202148,\n",
       "        0.15918, 0.12793], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.mlp.router.weight': Array([[0.00424194, -0.00358582, -0.0143433, ..., -0.0101929,\n",
       "         -0.00512695, -0.0131836],\n",
       "        [0.0200195, 0.00656128, 0.0125122, ..., 0.00717163, 0.00793457,\n",
       "         0.00793457],\n",
       "        [-0.00344849, 0.0112305, 0.00643921, ..., 0.00311279,\n",
       "         -0.000827789, -0.00662231],\n",
       "        ...,\n",
       "        [-0.00184631, -0.0124512, 0.0264893, ..., -0.000268936,\n",
       "         0.00534058, 0.00382996],\n",
       "        [0.0175781, 0.00927734, -0.00756836, ..., 0.00213623, -0.00379944,\n",
       "         0.00221252],\n",
       "        [0.00640869, -0.00274658, 0.00106049, ..., -0.00463867,\n",
       "         0.00570679, -0.00698853]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.2.post_attention_layernorm.weight': Array([2.73438, 1.70312, 1.97656, ..., 3.92188, 4.5625, 3.51562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.attn.o_proj.bias': Array([1.46094, -0.441406, 0.222656, ..., -0.980469, -1.45312, -0.065918],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.attn.o_proj.weight': Array([[1.0625, 1.10156, 0.554688, ..., 0.710938, -1.15625, 1.28125],\n",
       "        [0.78125, 0.232422, -0.679688, ..., -0.151367, -0.957031,\n",
       "         -0.216797],\n",
       "        [0.0563965, -0.242188, -0.742188, ..., -0.147461, 0.15625,\n",
       "         -0.196289],\n",
       "        ...,\n",
       "        [-0.181641, -0.470703, 0.365234, ..., -0.394531, 0.0217285,\n",
       "         0.671875],\n",
       "        [-0.167969, 0.154297, 0.324219, ..., 0.125977, -0.679688, 0.9375],\n",
       "        [-1.64062, 0.324219, 0.6875, ..., 0.269531, 0.142578, 0.648438]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.attn.qkv_proj.bias': Array([0.236328, -0.131836, -0.182617, ..., -0.235352, -0.433594,\n",
       "        -0.699219], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.attn.qkv_proj.weight': Array([[0.00100708, -0.000133514, -0.00286865, ..., 0.0027771,\n",
       "         0.00656128, 0.00021553],\n",
       "        [-0.00221252, 0.000553131, 0.0100098, ..., -0.00384521,\n",
       "         -0.00314331, 0.00107574],\n",
       "        [0.00216675, -0.00196838, 0.000789642, ..., 0.00570679,\n",
       "         -0.00479126, -4.24385e-05],\n",
       "        ...,\n",
       "        [-0.0327148, 0.0461426, 0.0913086, ..., 0.0727539, -0.0234375,\n",
       "         -0.0351562],\n",
       "        [-0.0194092, -0.0957031, 0.106934, ..., -0.0717773, -0.0493164,\n",
       "         -0.00482178],\n",
       "        [-0.00485229, -0.0427246, -0.0634766, ..., 0.0220947, 0.0524902,\n",
       "         0.00872803]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.attn.sinks': Array([1.77344, 1.42188, 1.14844, 1.71875, 1.3125, 1.09375, 1.76562,\n",
       "        -0.0117798, -1.69531, 1.16406, 0.984375, 1.36719, 0.847656,\n",
       "        1.28125, 0.515625, 1.94531, 1.42188, 1.25, 1.22656, 1.0625,\n",
       "        0.957031, -0.0649414, 1.38281, 1.54688, 1.21875, 1.25, 1.01562,\n",
       "        1.82812, 0.714844, 1.34375, 0.953125, 1.10156, 2.04688, 1.625,\n",
       "        1.83594, 1.92188, 1.15625, 1.77344, 1.64844, 1.17969, 1.375,\n",
       "        1.26562, 0.503906, 1.04688, 1.625, 1.03906, 1.21094, 0.314453,\n",
       "        2.54688, 2.04688, 2.34375, 2.375, 1.21094, 2.5, 1.97656, 2.25,\n",
       "        1.9375, 0.482422, 1.28906, 1.63281, 1.625, 1.67188, 1.84375,\n",
       "        1.57812], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.input_layernorm.weight': Array([1.22656, 1.74219, 1.57031, ..., 1.5625, 1.53125, 1.16406],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.mlp.experts.w13_bias': Array([[-0.139648, -0.0585938, -0.201172, ..., -0.302734, -0.488281,\n",
       "         -0.180664],\n",
       "        [-0.816406, 0.0498047, -0.238281, ..., -0.636719, 0.246094,\n",
       "         0.255859],\n",
       "        [-0.177734, -0.375, -1.00781, ..., -0.542969, -0.597656,\n",
       "         -0.589844],\n",
       "        ...,\n",
       "        [-0.400391, -0.150391, -0.589844, ..., -0.601562, -0.566406,\n",
       "         -0.589844],\n",
       "        [-0.777344, -1.5625, -0.300781, ..., -0.617188, -0.582031,\n",
       "         -0.511719],\n",
       "        [-1.99219, 0.0649414, -0.326172, ..., 1.10156, -0.828125,\n",
       "         -0.640625]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.mlp.experts.w13_weight': Array([[[0.0117188, 0.00585938, 0.0078125, ..., -0.015625, -0.0117188,\n",
       "          0.00195312],\n",
       "         [-0.00292969, -0.00195312, 0.00195312, ..., 0.00195312,\n",
       "          -0.00585938, -0.000976562],\n",
       "         [-0.00585938, 0.00585938, -0.00390625, ..., -0.00390625,\n",
       "          -0.00195312, 0.00585938],\n",
       "         ...,\n",
       "         [0.0078125, -0.03125, 0.03125, ..., 0.0078125, -0.0078125,\n",
       "          -0.015625],\n",
       "         [-0.00390625, 0.0234375, -0.0117188, ..., 0.03125, 0.015625,\n",
       "          -0.0234375],\n",
       "         [-0.0078125, 0.015625, -0.015625, ..., -0.015625, 0.046875,\n",
       "          0.03125]],\n",
       " \n",
       "        [[-0.00390625, -0.00390625, -0, ..., -0.015625, 0.0117188,\n",
       "          -0.0117188],\n",
       "         [0.00390625, -0.0117188, -0, ..., 0, 0.0234375, -0.0078125],\n",
       "         [-0, 0.00390625, -0.0078125, ..., -0.00195312, -0.00585938,\n",
       "          -0.00390625],\n",
       "         ...,\n",
       "         [0.015625, 0.03125, 0.015625, ..., 0.03125, 0.046875, -0.03125],\n",
       "         [-0.0078125, 0.0078125, -0.0234375, ..., 0, 0.015625,\n",
       "          -0.0078125],\n",
       "         [0.015625, -0.0234375, -0.0078125, ..., 0.00390625, -0.0078125,\n",
       "          0]],\n",
       " \n",
       "        [[0.00390625, 0, 0.00390625, ..., -0.0117188, 0.00195312,\n",
       "          -0.015625],\n",
       "         [-0, 0.0234375, -0.0078125, ..., -0.0078125, -0.00390625,\n",
       "          0.0117188],\n",
       "         [-0.015625, 0.00390625, -0.015625, ..., -0.015625, 0.0234375,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [0, 0, 0.03125, ..., 0.0078125, 0.0234375, -0.046875],\n",
       "         [-0, -0.0078125, 0.0234375, ..., 0.0234375, 0.0078125,\n",
       "          0.0234375],\n",
       "         [-0.015625, 0.015625, -0.015625, ..., 0.0078125, 0.046875,\n",
       "          -0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00390625, 0.00390625, -0.015625, ..., -0.0117188,\n",
       "          -0.00390625, 0.0117188],\n",
       "         [-0.00585938, 0.0078125, -0.00390625, ..., 0.00585938,\n",
       "          -0.0078125, -0.00195312],\n",
       "         [-0.0078125, -0.00585938, 0.00195312, ..., 0.0234375,\n",
       "          0.00585938, 0.0078125],\n",
       "         ...,\n",
       "         [-0, -0.046875, 0.03125, ..., 0.0078125, 0.03125, -0.0234375],\n",
       "         [-0.03125, -0.0078125, -0.015625, ..., 0.0234375, -0, -0.03125],\n",
       "         [-0.0078125, -0.0625, 0.015625, ..., 0.0078125, -0.046875,\n",
       "          0.046875]],\n",
       " \n",
       "        [[0.015625, 0.0117188, -0.00390625, ..., -0.0117188, -0.015625,\n",
       "          0.015625],\n",
       "         [-0.0078125, 0.015625, -0.015625, ..., -0.00390625, 0,\n",
       "          0.0078125],\n",
       "         [-0, 0, 0.00390625, ..., -0.0078125, -0.0117188, -0.00585938],\n",
       "         ...,\n",
       "         [-0, -0.015625, -0.03125, ..., 0.03125, 0.015625, -0.0625],\n",
       "         [0.0234375, -0.015625, 0.03125, ..., -0.03125, -0.015625,\n",
       "          -0.03125],\n",
       "         [-0.046875, -0.0078125, 0.015625, ..., 0.0078125, -0.0078125,\n",
       "          0.03125]],\n",
       " \n",
       "        [[0.125, -0, 0, ..., 0, 0, 0.03125],\n",
       "         [-0.00390625, -0.00390625, 0.00390625, ..., 0, 0.00195312,\n",
       "          -0.000976562],\n",
       "         [0, -0.0078125, 0, ..., -0.00390625, 0.00195312, -0],\n",
       "         ...,\n",
       "         [0, 0.03125, -0.00390625, ..., -0.00195312, 0.0078125,\n",
       "          -0.00195312],\n",
       "         [0.015625, -0.0234375, -0.046875, ..., 0.0078125, -0.015625,\n",
       "          0.0078125],\n",
       "         [0.03125, 0.046875, -0.0625, ..., -0.015625, -0.046875, -0]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.mlp.experts.w2_bias': Array([[16.125, 28.625, 19.25, ..., 7.375, 6.5625, -17.125],\n",
       "        [-4.71875, -7.1875, 3.60938, ..., -6.3125, -15.5, -19.375],\n",
       "        [1.29688, 1.17969, 15, ..., -10.9375, -17.625, -1.17969],\n",
       "        ...,\n",
       "        [25, 11.6875, 14.625, ..., -11.9375, 2.85938, 14.0625],\n",
       "        [17, 3.35938, 14.5625, ..., -3.60938, -24.875, 7.5],\n",
       "        [10.5, 12.5, -8.625, ..., -14, -5.09375, 6.40625]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.mlp.experts.w2_weight': Array([[[-8, -6, -6, ..., 4, -12, 4],\n",
       "         [-0, -4, 0, ..., 4, 6, 1],\n",
       "         [0, -8, -12, ..., 16, 4, -6],\n",
       "         ...,\n",
       "         [2, 8, 8, ..., -2, -2, -2],\n",
       "         [-8, 2, -8, ..., 2, 2, 4],\n",
       "         [16, 0, 8, ..., -0, -8, -0]],\n",
       " \n",
       "        [[-6, -4, 2, ..., 8, -4, -2],\n",
       "         [2, 2, -2, ..., 4, -2, 2],\n",
       "         [4, -3, 3, ..., 4, 2, -2],\n",
       "         ...,\n",
       "         [2, -0, 4, ..., -6, -0, 6],\n",
       "         [-2, -2, 8, ..., 12, -2, 4],\n",
       "         [2, 8, 0, ..., -12, 2, -2]],\n",
       " \n",
       "        [[4, -4, -24, ..., -6, 4, -6],\n",
       "         [-8, 3, -0, ..., 1, -8, -8],\n",
       "         [-8, -8, 6, ..., 4, -2, 2],\n",
       "         ...,\n",
       "         [-0, -2, -0, ..., 2, 8, -2],\n",
       "         [-1, 1, -3, ..., -2, -12, -6],\n",
       "         [2, 12, 8, ..., 4, 8, 4]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-6, 12, 0, ..., -12, 12, 12],\n",
       "         [2, 3, 3, ..., -1, -6, -2],\n",
       "         [-0, -4, 2, ..., 4, -4, 4],\n",
       "         ...,\n",
       "         [6, -3, -4, ..., -0, -6, 0],\n",
       "         [-4, -6, -1, ..., -0, -12, -8],\n",
       "         [6, -6, -12, ..., 8, -8, -6]],\n",
       " \n",
       "        [[-4, 4, -2, ..., 12, 0, -8],\n",
       "         [3, -8, -3, ..., 1, -3, 0],\n",
       "         [8, -2, -4, ..., 1, 0, -4],\n",
       "         ...,\n",
       "         [-0, -8, 4, ..., -3, -0, -8],\n",
       "         [-6, -0, 1, ..., 4, 2, -6],\n",
       "         [-8, 0, -4, ..., 4, -6, 0]],\n",
       " \n",
       "        [[12, 6, 2, ..., -0, -12, -12],\n",
       "         [1, 4, 8, ..., -3, -3, -2],\n",
       "         [-6, -0, 2, ..., -4, 6, 1],\n",
       "         ...,\n",
       "         [16, -0, -2, ..., -2, 6, 8],\n",
       "         [4, -6, 8, ..., -4, 8, -2],\n",
       "         [8, 16, 4, ..., -12, 0, 8]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.mlp.router.bias': Array([-0.181641, 0.0507812, 0.00454712, 0.0800781, 0.00576782, 0.132812,\n",
       "        -0.043457, 0.114746, 0.0397949, 0.0527344, 0.0255127, 0.0250244,\n",
       "        -0.147461, -0.199219, 0.0219727, 0.0534668, -0.275391, -0.00762939,\n",
       "        -0.00115967, 0.0209961, 0.0132446, -0.0893555, 0.112305, -0.148438,\n",
       "        0.09375, -0.164062, 0.204102, -0.000663757, 0.0922852, -0.0203857,\n",
       "        -0.0756836, -0.0108032], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.mlp.router.weight': Array([[-0.0078125, 0.00286865, -0.00144958, ..., -0.00127411,\n",
       "         0.00294495, -0.00332642],\n",
       "        [-0.00273132, 0.00500488, 3.3617e-05, ..., -0.00262451, 0.0027771,\n",
       "         -0.000606537],\n",
       "        [0.000112057, 0.0027771, -0.00296021, ..., 0.00245667, 0.0102539,\n",
       "         0.00180054],\n",
       "        ...,\n",
       "        [0.00704956, -0.00265503, 0.00022316, ..., -0.003479, -0.00343323,\n",
       "         -0.00156403],\n",
       "        [0.00250244, 0.00775146, 0.000117779, ..., 0.00361633,\n",
       "         0.000389099, -0.00576782],\n",
       "        [-0.000492096, -0.000576019, -0.001297, ..., -0.00162506,\n",
       "         -0.00150299, 0.00119019]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.20.post_attention_layernorm.weight': Array([0.703125, 1.8125, 1.67969, ..., 1.63281, 1.46094, 0.824219],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.attn.o_proj.bias': Array([3.45312, 4.03125, 0.84375, ..., -0.464844, -3.5, 1.5625], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.attn.o_proj.weight': Array([[0.476562, 0.421875, 0.020752, ..., -1.89844, -1.09375,\n",
       "         -0.0617676],\n",
       "        [0.761719, 0.478516, -0.195312, ..., -1.89844, -0.34375, 0.15332],\n",
       "        [-0.789062, 0.148438, 0.257812, ..., 0.296875, -0.394531,\n",
       "         -0.570312],\n",
       "        ...,\n",
       "        [0.287109, -0.703125, -0.236328, ..., -3.0625, -1.82812, 0.291016],\n",
       "        [0.412109, 1.32031, 0.828125, ..., 2.75, -1.17969, 0.507812],\n",
       "        [0.0563965, -1.05469, -0.652344, ..., -0.46875, -0.667969,\n",
       "         0.691406]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.attn.qkv_proj.bias': Array([0.0732422, 0.0424805, 0.0527344, ..., -0.121094, 0.226562,\n",
       "        -0.119629], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.attn.qkv_proj.weight': Array([[-0.0113525, -0.0222168, 0.013916, ..., 0.017334, -0.0183105,\n",
       "         0.0119629],\n",
       "        [-0.00964355, 0.0137329, -0.000663757, ..., 0.0090332, -0.0250244,\n",
       "         0.00619507],\n",
       "        [-0.00427246, 0.000247955, 0.00744629, ..., 0.00637817,\n",
       "         -0.00488281, 0.00193024],\n",
       "        ...,\n",
       "        [-0.0111694, -0.024292, 0.0649414, ..., 0.0205078, 0.110352,\n",
       "         -0.0268555],\n",
       "        [-0.0422363, 0.0371094, 0.0211182, ..., -0.00285339, -0.0708008,\n",
       "         0.00378418],\n",
       "        [-0.00140381, -0.0405273, -0.0339355, ..., 0.0966797, -0.0234375,\n",
       "         0.0351562]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.attn.sinks': Array([2.4375, 2.84375, 2.45312, 2.45312, 2.53125, 2.20312, 2.15625,\n",
       "        2.64062, 2.04688, 3.9375, 3.04688, 3.54688, 2.73438, 3.34375,\n",
       "        3.95312, 2.89062, 3.01562, 2.53125, 2.5625, 2.59375, 2.53125,\n",
       "        4.46875, 3.40625, 2.45312, 2.32812, 2.67188, 2.375, 2.39062,\n",
       "        3.21875, 2.71875, 2.79688, 2.21875, 2.6875, 1.9375, 3.21875,\n",
       "        2.73438, 3.03125, 3.85938, 3.98438, 2.03125, 1.21094, 3.14062,\n",
       "        2.53125, 2.35938, 2.57812, 3.01562, 2.25, 2.01562, 3.04688,\n",
       "        2.53125, 2.9375, 3.0625, 3.29688, 2.59375, 3.42188, 3.29688,\n",
       "        2.84375, 0.765625, 2.0625, 2, 1.61719, 2.09375, 1.75781, 2.17188],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.input_layernorm.weight': Array([1.0625, 1.60156, 1.25781, ..., 1.39062, 1.20312, 0.945312],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.mlp.experts.w13_bias': Array([[0.167969, -0.714844, 0.0344238, ..., -0.59375, -0.447266,\n",
       "         -0.808594],\n",
       "        [-1.5625, -1.52344, -0.6875, ..., -0.373047, -0.200195, -1.49219],\n",
       "        [0.0732422, -0.267578, -0.578125, ..., -0.773438, -0.353516,\n",
       "         -1.21875],\n",
       "        ...,\n",
       "        [-0.277344, -0.453125, -0.582031, ..., -0.582031, -0.6875,\n",
       "         -0.777344],\n",
       "        [-0.289062, -0.149414, -0.0375977, ..., -0.478516, -1.78125,\n",
       "         -0.394531],\n",
       "        [0.0761719, -0.242188, 0.00836182, ..., -0.898438, 0.882812,\n",
       "         -0.449219]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.mlp.experts.w13_weight': Array([[[0.00195312, -0, -0.00195312, ..., -0, 0.0078125, -0.00195312],\n",
       "         [0.00390625, -0.00195312, 0.0078125, ..., 0.00585938, 0.0078125,\n",
       "          0.00390625],\n",
       "         [0.00390625, 0.00195312, -0.00390625, ..., -0.00585938,\n",
       "          0.0078125, 0.0078125],\n",
       "         ...,\n",
       "         [0.015625, -0.046875, -0.0234375, ..., 0.0078125, -0.0625,\n",
       "          0.0078125],\n",
       "         [0.0078125, 0.015625, 0.03125, ..., -0.0078125, 0.015625, 0],\n",
       "         [0, -0.046875, 0.03125, ..., 0.015625, 0.03125, 0.015625]],\n",
       " \n",
       "        [[0.015625, -0.0117188, 0.015625, ..., -0, -0.0078125,\n",
       "          -0.0078125],\n",
       "         [0.0234375, 0.0078125, 0.0234375, ..., -0, -0.015625, -0],\n",
       "         [-0.046875, 0.0234375, -0, ..., -0, 0.0234375, -0.015625],\n",
       "         ...,\n",
       "         [0.0078125, -0.0078125, -0.046875, ..., -0.03125, -0.0078125,\n",
       "          -0.03125],\n",
       "         [-0.015625, 0.03125, 0.03125, ..., 0.015625, 0.0234375,\n",
       "          -0.015625],\n",
       "         [-0, 0.015625, 0, ..., 0.0078125, -0.0078125, 0.015625]],\n",
       " \n",
       "        [[-0, -0.0078125, 0.00195312, ..., -0.00195312, -0, -0.00195312],\n",
       "         [-0.00585938, -0.0078125, -0.00390625, ..., 0.00390625,\n",
       "          -0.0078125, -0.015625],\n",
       "         [0.00195312, -0, -0.015625, ..., 0, -0.00390625, 0.00390625],\n",
       "         ...,\n",
       "         [0.046875, -0.015625, -0.015625, ..., -0.03125, 0, -0.046875],\n",
       "         [0.0078125, -0.0234375, -0.0234375, ..., -0.0234375, 0.0078125,\n",
       "          -0],\n",
       "         [0.09375, -0.046875, 0.0078125, ..., 0.015625, -0.0625,\n",
       "          0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00390625, 0.00390625, 0.00390625, ..., 0, -0.00390625,\n",
       "          -0.00195312],\n",
       "         [-0.0117188, 0.00390625, -0.0078125, ..., 0.00390625,\n",
       "          0.00390625, -0.015625],\n",
       "         [0, -0, -0.0078125, ..., 0.0078125, -0.0078125, -0.0078125],\n",
       "         ...,\n",
       "         [0, 0.0234375, 0.0234375, ..., 0.0234375, 0.015625, 0],\n",
       "         [-0.00390625, 0.0117188, -0.015625, ..., -0.015625, 0.0078125,\n",
       "          -0.015625],\n",
       "         [0.0078125, -0, -0, ..., -0.03125, -0.046875, 0.0078125]],\n",
       " \n",
       "        [[-0.00390625, 0.00390625, 0.0117188, ..., -0.00195312,\n",
       "          -0.00195312, 0.00195312],\n",
       "         [-0.00195312, -0.0117188, -0.015625, ..., 0.00585938, -0,\n",
       "          0.00390625],\n",
       "         [0.00195312, 0.00585938, -0.0078125, ..., 0, -0.00195312,\n",
       "          0.0078125],\n",
       "         ...,\n",
       "         [0, -0.046875, -0.046875, ..., 0, -0.015625, 0],\n",
       "         [0.015625, 0, 0, ..., 0.00390625, -0, -0.015625],\n",
       "         [0.03125, 0.03125, 0, ..., -0, -0.015625, 0.046875]],\n",
       " \n",
       "        [[-0.00195312, -0.00390625, -0.000976562, ..., 0.00390625,\n",
       "          -0.000976562, 0.000976562],\n",
       "         [0.00195312, -0.015625, 0.0078125, ..., 0.015625, -0, 0.015625],\n",
       "         [-0.00390625, -0.0078125, 0.00195312, ..., 0.00195312,\n",
       "          0.0078125, -0],\n",
       "         ...,\n",
       "         [-0.0234375, 0.046875, 0, ..., 0.03125, -0.03125, -0.0078125],\n",
       "         [-0.0078125, 0.00390625, 0.0078125, ..., -0.00195312,\n",
       "          -0.00195312, 0.0078125],\n",
       "         [-0.0234375, -0.0234375, -0.015625, ..., -0.0078125, -0.015625,\n",
       "          0.03125]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.mlp.experts.w2_bias': Array([[56, -7.90625, 2.3125, ..., -2.125, -43, -1.97656],\n",
       "        [5.34375, -6.84375, -0.15918, ..., -21.5, 16.125, 1.14062],\n",
       "        [16.625, 10.8125, -6.78125, ..., -25.125, 2.23438, 21.75],\n",
       "        ...,\n",
       "        [-17.625, 0.161133, -7.21875, ..., 25.625, -27.375, 26.75],\n",
       "        [6.5625, 19.75, -14.1875, ..., -14.625, 9.3125, 1.97656],\n",
       "        [0.213867, 17.875, 1.38281, ..., -4.59375, -4.0625, -16.875]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.mlp.experts.w2_weight': Array([[[6, -2, -0, ..., 0, 2, 4],\n",
       "         [6, -16, -2, ..., 12, 6, -16],\n",
       "         [-0, 4, -16, ..., -12, -12, -8],\n",
       "         ...,\n",
       "         [-6, 0, -0, ..., 12, -4, 8],\n",
       "         [0, -12, 8, ..., 16, -2, -16],\n",
       "         [0, -8, -16, ..., 4, -6, -4]],\n",
       " \n",
       "        [[-4, -0, 4, ..., -4, -2, 4],\n",
       "         [0, -2, -4, ..., -12, -0, 6],\n",
       "         [8, 32, -0, ..., -2, 2, 6],\n",
       "         ...,\n",
       "         [1, 1, -3, ..., -6, 4, 4],\n",
       "         [2, -6, 6, ..., 4, 3, -2],\n",
       "         [2, -16, 0, ..., -16, -2, 2]],\n",
       " \n",
       "        [[-6, 2, 6, ..., 4, 6, -8],\n",
       "         [2, 0, -2, ..., -4, -6, -8],\n",
       "         [2, 6, 0, ..., -2, -4, -6],\n",
       "         ...,\n",
       "         [2, 2, 0, ..., 6, -8, -2],\n",
       "         [-2, 0, -4, ..., 2, 6, -8],\n",
       "         [-6, -8, 4, ..., -0, -0, -12]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-8, -8, 4, ..., -12, -2, -2],\n",
       "         [4, -0, -8, ..., -12, -8, 2],\n",
       "         [-8, -2, -16, ..., 32, 0, 8],\n",
       "         ...,\n",
       "         [8, -2, 24, ..., 4, 8, -4],\n",
       "         [12, 0, -6, ..., 16, 16, -12],\n",
       "         [8, -12, -8, ..., 4, -0, -2]],\n",
       " \n",
       "        [[8, -6, -8, ..., 12, 2, 4],\n",
       "         [-2, -8, 6, ..., 2, -2, -2],\n",
       "         [-8, 4, -4, ..., 2, -2, 4],\n",
       "         ...,\n",
       "         [12, 2, 4, ..., -2, -0, -4],\n",
       "         [-2, 0, -6, ..., -0, -2, -2],\n",
       "         [6, 2, -12, ..., -4, 4, 4]],\n",
       " \n",
       "        [[-0, -6, 6, ..., 16, 6, 8],\n",
       "         [4, -8, 0, ..., 4, -4, 4],\n",
       "         [-16, 6, 6, ..., -4, 4, 8],\n",
       "         ...,\n",
       "         [6, -4, 16, ..., 16, -4, 16],\n",
       "         [-6, -4, -2, ..., -0, -8, -0],\n",
       "         [-0, 2, -2, ..., -16, -8, 16]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.mlp.router.bias': Array([0.0561523, -0.034668, -0.0158691, 0.0495605, 0.15625, -0.00389099,\n",
       "        0.0698242, -0.11377, -0.108887, -0.0238037, 0.108887, 0.224609,\n",
       "        -0.0163574, 0.0246582, 0.0539551, 0.00546265, -0.00482178,\n",
       "        0.0617676, -0.125977, -0.0429688, 0.026001, 0.0563965, -0.02771,\n",
       "        0.0306396, -0.090332, -0.122559, -0.0480957, 0.12793, -0.0786133,\n",
       "        -0.045166, 0.0917969, -0.137695], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.mlp.router.weight': Array([[0.0161133, -0.000322342, -0.00363159, ..., -0.00257874,\n",
       "         0.00358582, 0.00236511],\n",
       "        [-0.00521851, 0.00793457, 0.000846863, ..., -0.00102234,\n",
       "         -0.000236511, -0.000862122],\n",
       "        [-0.00186157, 0.000205994, -0.000341415, ..., 0.00463867,\n",
       "         -0.00177002, -0.00044632],\n",
       "        ...,\n",
       "        [0.00157166, 0.00726318, 0.0019455, ..., -0.000854492, 0.00188446,\n",
       "         0.0013504],\n",
       "        [-0.0037384, -0.0136108, 0.00686646, ..., -0.000188828,\n",
       "         0.00108337, 0.0151367],\n",
       "        [-0.0108643, 0.0038147, 0.00180817, ..., -0.000576019,\n",
       "         -0.00185394, 0.00306702]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.21.post_attention_layernorm.weight': Array([0.789062, 1.78906, 1.60938, ..., 1.73438, 1.44531, 0.890625],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.attn.o_proj.bias': Array([-0.482422, 1.125, 1.54688, ..., 0.597656, -3.17188, 1.14062],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.attn.o_proj.weight': Array([[-1.39062, 0.0732422, 0.945312, ..., -1.59375, -0.703125,\n",
       "         -0.722656],\n",
       "        [-0.445312, 0.443359, -0.285156, ..., 0.253906, -0.137695, 0.9375],\n",
       "        [-0.416016, -0.0942383, 0.851562, ..., 0.00320435, -0.867188,\n",
       "         0.261719],\n",
       "        ...,\n",
       "        [0.412109, -0.261719, -0.863281, ..., -1.70312, -0.527344,\n",
       "         0.828125],\n",
       "        [0.455078, 1.0625, 1.58594, ..., 0.652344, 3.6875, 0.369141],\n",
       "        [0.25, -0.679688, -0.734375, ..., 0.769531, 0.46875, -0.890625]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.attn.qkv_proj.bias': Array([0.129883, -0.065918, 0.0839844, ..., -0.205078, -0.161133,\n",
       "        -0.19043], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.attn.qkv_proj.weight': Array([[-0.00350952, -0.00268555, 0.00318909, ..., 0.00643921,\n",
       "         0.00146484, 0.00564575],\n",
       "        [0.0178223, -0.00613403, 0.0118408, ..., 0.00138855, 0.00631714,\n",
       "         -0.00024128],\n",
       "        [0.0010376, 0.0236816, 0.0134277, ..., 0.0134277, -0.00689697,\n",
       "         0.0025177],\n",
       "        ...,\n",
       "        [-0.0317383, 0.139648, 0.0264893, ..., 0.0688477, -0.100586,\n",
       "         0.0269775],\n",
       "        [-0.0461426, 0.0322266, -0.0100098, ..., -0.0336914, 0.0230713,\n",
       "         0.0270996],\n",
       "        [-0.0205078, -0.269531, 0.101074, ..., -0.0600586, 0.0246582,\n",
       "         0.0247803]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.attn.sinks': Array([1.07812, 0.890625, 1.34375, 0.800781, 1.07812, 0.933594, 0.460938,\n",
       "        0.691406, 0.408203, 1.05469, 0.894531, 1.04688, 1.03125, -0.139648,\n",
       "        0.925781, 0.851562, 0.566406, 0.835938, 0.832031, 0.941406,\n",
       "        0.859375, 1.05469, 0.515625, 1.10938, 1.35938, 1.76562, 1.5,\n",
       "        1.49219, 1.53906, 1.58594, 1.00781, 1.34375, 0.341797, 1.07812,\n",
       "        1.36719, 1.47656, 1.23438, 0.992188, 1.34375, 1.35938, 0.660156,\n",
       "        1.29688, 0.161133, 0.957031, 1.23438, 0.898438, 0.96875, 0.746094,\n",
       "        0.917969, 1.21094, 1.25, 0.648438, 0.828125, 1.08594, 1.59375, 1.5,\n",
       "        1.75781, 0.235352, 1.9375, 2.09375, 1.78125, 0.933594, 1.64062,\n",
       "        1.84375], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.input_layernorm.weight': Array([1.46875, 1.80469, 1.625, ..., 1.59375, 1.625, 1.59375], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.mlp.experts.w13_bias': Array([[-0.231445, -1.11719, -0.722656, ..., -0.0551758, -0.664062,\n",
       "         -0.318359],\n",
       "        [-0.488281, -0.145508, -0.396484, ..., -0.304688, -0.523438,\n",
       "         -0.347656],\n",
       "        [-0.263672, -0.145508, -0.0878906, ..., -0.151367, -0.480469,\n",
       "         -0.59375],\n",
       "        ...,\n",
       "        [-0.0458984, 0.00125885, -0.0664062, ..., -0.90625, 0.605469,\n",
       "         -0.71875],\n",
       "        [-0.157227, -0.447266, -0.267578, ..., -0.976562, -0.78125,\n",
       "         -0.777344],\n",
       "        [-1.24219, -0.419922, -0.330078, ..., -0.738281, -1.89062,\n",
       "         -0.447266]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.mlp.experts.w13_weight': Array([[[0.00390625, 0.00390625, 0.00390625, ..., 0.0078125, -0, 0],\n",
       "         [0.0078125, -0.015625, 0.0078125, ..., -0.00390625, -0.00390625,\n",
       "          0],\n",
       "         [0.0117188, 0, 0.0078125, ..., -0.0234375, -0.015625,\n",
       "          -0.0117188],\n",
       "         ...,\n",
       "         [-0.015625, -0, -0.046875, ..., 0.046875, -0.0078125, -0.015625],\n",
       "         [-0.015625, -0, 0.0078125, ..., -0.0078125, -0.046875, 0.03125],\n",
       "         [0.015625, 0.03125, -0.0078125, ..., 0.03125, -0.0117188,\n",
       "          0.00390625]],\n",
       " \n",
       "        [[0.015625, 0, 0.00390625, ..., -0.00390625, 0.00390625,\n",
       "          0.0078125],\n",
       "         [0, -0.00195312, -0.0078125, ..., -0, 0.00390625, -0.0078125],\n",
       "         [0.0117188, -0.0078125, -0.00195312, ..., -0.0078125, 0.0078125,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [0.03125, 0.015625, 0.03125, ..., 0.125, -0.046875, -0.015625],\n",
       "         [0.0234375, -0.03125, 0.015625, ..., 0.0078125, 0.015625,\n",
       "          -0.0078125],\n",
       "         [0, -0.03125, 0.03125, ..., 0.0234375, -0.0234375, 0.0078125]],\n",
       " \n",
       "        [[-0.00390625, -0.0078125, -0.0117188, ..., 0.0078125, 0,\n",
       "          -0.00390625],\n",
       "         [-0.00195312, 0, -0.0078125, ..., -0.00390625, -0.0117188, -0],\n",
       "         [-0.00390625, -0.0078125, 0, ..., 0.00390625, 0.00585938,\n",
       "          -0.00585938],\n",
       "         ...,\n",
       "         [0.015625, 0.0078125, 0.015625, ..., -0.0625, 0.0078125,\n",
       "          0.015625],\n",
       "         [0.015625, -0.0234375, 0.046875, ..., -0, -0.046875, 0.03125],\n",
       "         [-0.015625, -0.0234375, -0.015625, ..., -0.0078125, -0.046875,\n",
       "          -0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.000976562, -0.00292969, -0.000976562, ..., -0.00390625,\n",
       "          0.00292969, 0.00195312],\n",
       "         [0.00585938, 0.00195312, -0.00292969, ..., -0.00585938,\n",
       "          0.00195312, 0],\n",
       "         [-0.00195312, -0.00292969, 0.00292969, ..., -0.00195312,\n",
       "          -0.0078125, -0.00585938],\n",
       "         ...,\n",
       "         [-0.0234375, 0.0078125, 0.0078125, ..., -0.0234375, 0.015625,\n",
       "          0.015625],\n",
       "         [0.00390625, -0.0234375, 0.00390625, ..., -0.0234375, 0.0117188,\n",
       "          -0],\n",
       "         [-0.046875, -0.0625, 0.0078125, ..., 0.03125, -0.015625,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[0.0078125, -0.00390625, 0, ..., -0.015625, 0.0078125,\n",
       "          -0.00390625],\n",
       "         [0.015625, -0, 0.0117188, ..., -0.00390625, 0.0117188,\n",
       "          0.0117188],\n",
       "         [-0.00390625, 0.0234375, -0, ..., 0.015625, -0.0117188,\n",
       "          0.0078125],\n",
       "         ...,\n",
       "         [0.0078125, -0.00390625, -0.0234375, ..., -0.0117188,\n",
       "          -0.0117188, -0.00390625],\n",
       "         [0.0078125, -0.0078125, -0.015625, ..., -0.046875, 0, 0.03125],\n",
       "         [-0, 0.0625, 0.0078125, ..., -0.03125, -0.015625, -0.03125]],\n",
       " \n",
       "        [[-0.015625, -0, -0.0078125, ..., 0.0234375, -0.00390625,\n",
       "          0.00390625],\n",
       "         [-0, 0, 0.0078125, ..., 0.00195312, -0.00390625, 0.0078125],\n",
       "         [0.00390625, -0.00195312, -0.0078125, ..., -0.00195312,\n",
       "          -0.0078125, 0],\n",
       "         ...,\n",
       "         [0.03125, 0.03125, 0.046875, ..., 0.03125, 0.046875, 0.03125],\n",
       "         [0.015625, 0, 0.015625, ..., -0.03125, 0.046875, -0.015625],\n",
       "         [-0, -0.09375, -0.015625, ..., 0.046875, 0.0625, -0.015625]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.mlp.experts.w2_bias': Array([[-1.01562, 57, -22.25, ..., 4.03125, 0.757812, -30.375],\n",
       "        [-7.6875, 1.25781, 15.4375, ..., -2.125, -6.6875, -6.6875],\n",
       "        [-29.25, 0.417969, 20.625, ..., -22.75, -15, -3.70312],\n",
       "        ...,\n",
       "        [10.0625, -8.625, 23.375, ..., 4.5, -23.125, 11.125],\n",
       "        [-28.75, 8.5625, 13.5625, ..., -7.125, 15, -1.13281],\n",
       "        [-6.96875, -1.00781, 21.875, ..., -23.75, -2.1875, -2.42188]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.mlp.experts.w2_weight': Array([[[-2, -4, -4, ..., 6, -0, 2],\n",
       "         [8, -4, -0, ..., -4, -4, -8],\n",
       "         [2, 0, -0, ..., 4, 8, 2],\n",
       "         ...,\n",
       "         [4, 4, -0, ..., 8, 2, -8],\n",
       "         [-0, 12, -16, ..., 6, -8, 8],\n",
       "         [-6, 4, -2, ..., -6, -6, 12]],\n",
       " \n",
       "        [[6, 4, -4, ..., 8, -12, -12],\n",
       "         [2, -2, 2, ..., -0, 4, 2],\n",
       "         [6, -4, 2, ..., -0, 4, -4],\n",
       "         ...,\n",
       "         [-4, -8, 8, ..., -4, -16, 16],\n",
       "         [-12, -6, -4, ..., 4, -16, -4],\n",
       "         [-2, 16, -2, ..., 16, -8, -0]],\n",
       " \n",
       "        [[6, 4, -4, ..., -8, 8, -6],\n",
       "         [12, 6, -2, ..., 0, 8, 12],\n",
       "         [-4, 16, 2, ..., 4, 6, 0],\n",
       "         ...,\n",
       "         [-12, -0, 4, ..., -2, -12, 6],\n",
       "         [-8, 4, -16, ..., -4, -8, -4],\n",
       "         [-0, 6, -0, ..., 16, 6, -2]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[4, 4, 2, ..., 2, -2, 16],\n",
       "         [-6, -2, -2, ..., 4, 12, 4],\n",
       "         [-4, -12, -0, ..., 6, -4, 0],\n",
       "         ...,\n",
       "         [-0, 8, 0, ..., -8, 4, -12],\n",
       "         [12, 12, 8, ..., 0, -12, 4],\n",
       "         [-6, -6, -8, ..., 0, 2, 2]],\n",
       " \n",
       "        [[-0, -4, -12, ..., 6, 4, -4],\n",
       "         [8, -8, 0, ..., 16, -4, 4],\n",
       "         [-8, 12, -12, ..., -12, 4, 16],\n",
       "         ...,\n",
       "         [0, 8, -12, ..., -0, -0, 12],\n",
       "         [0, 8, 12, ..., 12, -8, -12],\n",
       "         [6, -12, 0, ..., -0, -2, -0]],\n",
       " \n",
       "        [[6, 12, 4, ..., -8, 2, 16],\n",
       "         [-6, -0, -4, ..., -6, -2, 12],\n",
       "         [16, 8, -4, ..., -8, -8, 0],\n",
       "         ...,\n",
       "         [6, -6, -8, ..., 12, 4, -4],\n",
       "         [0, 4, 24, ..., -12, -12, -4],\n",
       "         [2, -2, 2, ..., 16, -2, 16]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.mlp.router.bias': Array([0.0314941, 0.0194092, -0.0284424, -0.0390625, -0.126953, 0.0583496,\n",
       "        0.0498047, -0.0810547, -0.000310898, -0.0233154, -0.0561523,\n",
       "        -0.059082, -0.0231934, 0.0441895, 0.108887, -0.238281, 0.0639648,\n",
       "        -0.0649414, -0.0830078, -0.0480957, 0.21875, -0.078125, -0.0927734,\n",
       "        0.174805, 0.0159912, 0.0152588, 0.0263672, 0.145508, -0.0639648,\n",
       "        0.00476074, -0.105469, 0.0127563], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.mlp.router.weight': Array([[0.00332642, -0.00369263, -0.00230408, ..., 0.000984192,\n",
       "         -0.00331116, 0.00221252],\n",
       "        [0.000629425, -0.0143433, 0.00379944, ..., 0.00363159,\n",
       "         0.000322342, 0.00082016],\n",
       "        [0.0019989, -0.00933838, 0.00270081, ..., -0.00326538,\n",
       "         -0.000160217, -0.00193787],\n",
       "        ...,\n",
       "        [0.0154419, 0.0035553, 3.86238e-05, ..., -0.00185394, 0.00439453,\n",
       "         0.00262451],\n",
       "        [0.00546265, 0.00360107, -0.000419617, ..., 0.00390625,\n",
       "         0.00242615, -0.00598145],\n",
       "        [0.00860596, 0.00170898, -0.00227356, ..., 0.00169373,\n",
       "         -0.00311279, -0.00248718]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.22.post_attention_layernorm.weight': Array([0.9375, 1.77344, 1.65625, ..., 1.74219, 1.52344, 1.01562],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.attn.o_proj.bias': Array([5.3125, 1.25781, 1.60156, ..., 6.09375, -0.425781, -0.361328],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.attn.o_proj.weight': Array([[-0.539062, 0.304688, 1.54688, ..., -0.636719, -1.74219, 0.945312],\n",
       "        [0.0266113, 0.417969, -0.106445, ..., -0.804688, -2.82812,\n",
       "         -1.1875],\n",
       "        [0.112793, -0.65625, -0.103516, ..., -4.1875, -0.460938, 2.29688],\n",
       "        ...,\n",
       "        [-0.21582, -0.439453, -0.683594, ..., -1.21875, -2.67188,\n",
       "         -2.51562],\n",
       "        [0.102051, 0.0732422, -0.453125, ..., 0.550781, -2.78125,\n",
       "         0.851562],\n",
       "        [0.0571289, 0.707031, 1.21875, ..., 1.32812, -0.134766, -0.503906]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.attn.qkv_proj.bias': Array([0.0108032, -0.0205078, -0.00811768, ..., 0.283203, 0.133789,\n",
       "        0.28125], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.attn.qkv_proj.weight': Array([[0.000431061, -4.29153e-06, 9.82285e-05, ..., 4.1008e-05,\n",
       "         -1.09076e-05, -0.000434875],\n",
       "        [-0.000110626, -9.44138e-05, 0.000274658, ..., 2.90871e-05,\n",
       "         -2.87294e-05, 0.000385284],\n",
       "        [-1.90735e-05, 0.000124931, 0.000341415, ..., 2.74181e-05,\n",
       "         -0.000261307, 0.000154495],\n",
       "        ...,\n",
       "        [-0.00427246, -0.03125, -0.0322266, ..., -0.015625, 0.0233154,\n",
       "         -0.052002],\n",
       "        [-0.0534668, 0.0878906, 0.0280762, ..., -0.0299072, -0.00138092,\n",
       "         -0.00576782],\n",
       "        [0.0683594, -0.0458984, -0.048584, ..., -0.0505371, -0.104004,\n",
       "         0.0132446]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.attn.sinks': Array([1.0625, 1.90625, 2.75, 2.10938, 1.67188, 2.03125, 1.40625, 2.75,\n",
       "        1.52344, 1.76562, 1.04688, 1.21875, 0.636719, 0.679688, 1.625,\n",
       "        1.46875, 1.83594, 2.48438, 2.375, 2.26562, 2.20312, 3.64062, 2.875,\n",
       "        1.76562, 1.64844, 1.47656, 1.67969, 1.17188, 2.35938, 3.17188,\n",
       "        3.125, 1.08594, 1.97656, 1.875, 2, 1.85938, 1.85156, 1.85938,\n",
       "        1.77344, 2.5, 1.39844, 1.64062, 1.64062, 1.95312, 3.59375, 2.20312,\n",
       "        2.71875, 1.96094, 1.54688, 2.29688, 2.03125, 1.79688, 1.09375,\n",
       "        2.125, 0.976562, 1.17969, 1.96094, 2.89062, 1.57031, 2.07812,\n",
       "        1.86719, 2.01562, 1.61719, 1.85156], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.input_layernorm.weight': Array([1.07031, 1.66406, 1.25, ..., 1.34375, 1.26562, 1.19531], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.mlp.experts.w13_bias': Array([[-0.433594, -0.108398, 0.0688477, ..., -0.554688, -0.357422,\n",
       "         -0.330078],\n",
       "        [-0.585938, -0.229492, -0.255859, ..., -0.0255127, -0.314453,\n",
       "         -0.314453],\n",
       "        [-0.065918, -0.0869141, -0.347656, ..., -0.527344, -0.486328,\n",
       "         -0.695312],\n",
       "        ...,\n",
       "        [-0.0883789, -0.773438, -0.157227, ..., -0.400391, -0.275391,\n",
       "         -0.316406],\n",
       "        [-0.386719, -0.253906, -0.429688, ..., -0.466797, -0.386719,\n",
       "         -0.478516],\n",
       "        [0.0888672, -0.160156, 0.111816, ..., -0.00891113, -0.546875,\n",
       "         0.386719]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.mlp.experts.w13_weight': Array([[[0.046875, -0.0117188, -0, ..., 0.00390625, -0.0234375, 0.03125],\n",
       "         [0.00195312, 0, 0.00585938, ..., 0.00195312, 0.00390625,\n",
       "          0.00390625],\n",
       "         [-0.00585938, 0.00195312, -0.00585938, ..., -0.00195312,\n",
       "          0.000976562, -0.00292969],\n",
       "         ...,\n",
       "         [0.0234375, 0, -0.0078125, ..., -0.00390625, 0.046875,\n",
       "          -0.015625],\n",
       "         [0.0078125, 0.0078125, -0.0078125, ..., 0.0234375, -0.0078125,\n",
       "          0.046875],\n",
       "         [-0, -0.046875, -0.0234375, ..., -0.0234375, 0.0234375,\n",
       "          0.0234375]],\n",
       " \n",
       "        [[0.0078125, 0.0078125, -0.0234375, ..., 0.0117188, -0.0117188,\n",
       "          0.0234375],\n",
       "         [-0.015625, 0.0234375, 0.015625, ..., -0.03125, 0.015625,\n",
       "          0.0117188],\n",
       "         [0.0078125, 0.015625, 0.015625, ..., -0.00390625, 0.015625,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [0, 0.0625, 0.03125, ..., 0.03125, 0.0078125, -0],\n",
       "         [-0.046875, -0.0625, 0.09375, ..., 0.015625, 0.03125, 0],\n",
       "         [-0.0625, 0.015625, -0, ..., -0.09375, -0.046875, -0.03125]],\n",
       " \n",
       "        [[-0.0117188, 0.0078125, -0.00195312, ..., -0, -0.00585938,\n",
       "          0.00390625],\n",
       "         [0.00390625, -0.0078125, 0.00195312, ..., 0.0078125, 0.015625,\n",
       "          -0.0078125],\n",
       "         [0, 0.00585938, -0.00585938, ..., 0.00585938, -0, -0.00195312],\n",
       "         ...,\n",
       "         [-0.046875, 0.0234375, -0, ..., 0.046875, 0, -0.046875],\n",
       "         [0.0625, 0.0625, -0.0078125, ..., -0, 0.0234375, 0.015625],\n",
       "         [0.015625, 0.015625, 0.015625, ..., 0.0234375, -0.0078125,\n",
       "          0.015625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0, -0.00390625, 0.0078125, ..., 0.0117188, -0.00195312,\n",
       "          -0.00195312],\n",
       "         [0.0078125, -0.0234375, 0.03125, ..., -0.0078125, -0.015625, 0],\n",
       "         [0.00585938, -0.00390625, 0.00585938, ..., -0.015625,\n",
       "          -0.0078125, 0.0078125],\n",
       "         ...,\n",
       "         [-0, -0.046875, -0.03125, ..., -0.015625, -0.015625, 0.0078125],\n",
       "         [0.015625, -0.046875, -0.03125, ..., 0.03125, 0.03125, 0.015625],\n",
       "         [0.015625, -0.046875, 0, ..., 0.015625, -0.0234375, 0.0234375]],\n",
       " \n",
       "        [[0.0078125, 0.0078125, -0.0078125, ..., 0.00195312, 0.015625,\n",
       "          0.0117188],\n",
       "         [-0, -0.0078125, -0.0234375, ..., -0.0234375, 0.00195312,\n",
       "          -0.0078125],\n",
       "         [0.0078125, 0.0078125, -0.0078125, ..., 0, 0, -0.00390625],\n",
       "         ...,\n",
       "         [-0, 0.0078125, -0, ..., 0.046875, -0.03125, 0.03125],\n",
       "         [-0.046875, 0, -0.015625, ..., 0.015625, 0, -0.046875],\n",
       "         [-0.03125, 0.03125, -0.0625, ..., -0.03125, -0.0078125, 0]],\n",
       " \n",
       "        [[-0.00390625, 0.00195312, -0.00292969, ..., 0, 0.00195312,\n",
       "          0.000976562],\n",
       "         [0, -0.00390625, -0.00390625, ..., -0.0078125, -0.0117188, -0],\n",
       "         [-0.0117188, 0, 0, ..., 0.00195312, 0.015625, -0.0078125],\n",
       "         ...,\n",
       "         [0.0117188, 0.015625, -0.015625, ..., 0.015625, -0.015625,\n",
       "          -0.09375],\n",
       "         [0.0234375, 0.0078125, 0.015625, ..., -0, -0.0078125, -0.046875],\n",
       "         [-0.00390625, 0.015625, -0.0234375, ..., -0.0078125, 0.00390625,\n",
       "          -0.03125]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.mlp.experts.w2_bias': Array([[33, 7.25, 13.4375, ..., -11.6875, 18.125, 4.28125],\n",
       "        [-7.4375, -25.875, -4.90625, ..., 3.03125, -4.5625, 9.125],\n",
       "        [2.15625, 18.5, 4.46875, ..., -3.90625, -5.4375, -3.45312],\n",
       "        ...,\n",
       "        [-5.6875, -35, 21, ..., -10.625, 6.3125, -23.25],\n",
       "        [-28.5, -6.375, 25.75, ..., -5.875, -22.75, -20.625],\n",
       "        [-18.375, 11.25, -3.40625, ..., -13.5625, 5.96875, -15.3125]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.mlp.experts.w2_weight': Array([[[2, -8, 2, ..., -2, -4, 2],\n",
       "         [-0, -2, -8, ..., 8, -6, -2],\n",
       "         [-2, 6, 4, ..., 4, -6, 2],\n",
       "         ...,\n",
       "         [6, -8, -8, ..., 0, -12, 4],\n",
       "         [-2, 6, -0, ..., -4, -2, 0],\n",
       "         [4, -4, 1, ..., 1, -4, -3]],\n",
       " \n",
       "        [[-3, -4, 1, ..., -8, 2, 2],\n",
       "         [-4, 4, 8, ..., 2, 12, 0],\n",
       "         [16, -4, -4, ..., 2, -4, -8],\n",
       "         ...,\n",
       "         [-12, 0, -16, ..., 4, 8, 12],\n",
       "         [12, -12, -8, ..., 2, 8, -2],\n",
       "         [-6, -4, 3, ..., 1, -0, 4]],\n",
       " \n",
       "        [[12, -4, 4, ..., -2, -6, -3],\n",
       "         [4, -6, -6, ..., 2, 0, 2],\n",
       "         [-4, 16, -24, ..., 12, -6, -0],\n",
       "         ...,\n",
       "         [-2, 4, -16, ..., -24, 4, -4],\n",
       "         [16, -2, 8, ..., -0, -4, 4],\n",
       "         [-2, 4, -8, ..., 12, -8, -6]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, -8, -2, ..., 16, 16, -4],\n",
       "         [-8, -8, 4, ..., -8, -4, -16],\n",
       "         [8, 16, 12, ..., 12, -12, 4],\n",
       "         ...,\n",
       "         [-4, 0, -12, ..., 4, -16, 16],\n",
       "         [4, 0, 4, ..., -0, -4, -4],\n",
       "         [4, 0, -8, ..., 2, 4, -6]],\n",
       " \n",
       "        [[-6, 6, -1, ..., -2, 6, 12],\n",
       "         [8, 16, 2, ..., -0, 8, -0],\n",
       "         [8, -8, -2, ..., -16, -12, 2],\n",
       "         ...,\n",
       "         [8, -16, -2, ..., -4, -6, -8],\n",
       "         [4, -4, -4, ..., 12, 2, -4],\n",
       "         [-4, 6, -8, ..., 4, 4, -4]],\n",
       " \n",
       "        [[-3, 4, -1, ..., 0, -4, 0],\n",
       "         [4, 4, 4, ..., -2, 6, -2],\n",
       "         [-16, 0, -4, ..., 4, -8, 4],\n",
       "         ...,\n",
       "         [-8, -2, 2, ..., -2, -12, 6],\n",
       "         [2, -4, 4, ..., 0, 12, -2],\n",
       "         [-2, 1, -0, ..., 4, 4, 0]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.mlp.router.bias': Array([-0.101562, -0.0429688, 0.119629, -0.0439453, 0.00157928, -0.126953,\n",
       "        -0.0698242, -0.0057373, 0.109375, -0.0561523, -0.00318909,\n",
       "        -0.0356445, -0.0544434, 0.0766602, 0.0927734, -0.0211182,\n",
       "        0.0649414, -0.144531, 0.0578613, -0.0488281, -0.0126953,\n",
       "        -0.0174561, 0.131836, 0.0800781, -0.0067749, -0.105469, 0.133789,\n",
       "        -0.0201416, -0.0673828, -0.00222778, -0.0412598, -0.0795898],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.mlp.router.weight': Array([[-0.00958252, -0.000614166, 0.00276184, ..., 0.00320435,\n",
       "         0.00119781, -0.00393677],\n",
       "        [0.00308228, 0.000991821, -0.00318909, ..., -0.00558472,\n",
       "         -0.00527954, 0.00442505],\n",
       "        [-0.00549316, -0.00087738, -0.00165558, ..., -0.000495911,\n",
       "         0.00050354, -0.00370789],\n",
       "        ...,\n",
       "        [-0.00105286, -0.000526428, -0.00325012, ..., 0.0020752,\n",
       "         -0.000274658, 0.0027771],\n",
       "        [0.0109863, -0.000850677, -0.00698853, ..., 0.00025177,\n",
       "         0.00139618, -0.00363159],\n",
       "        [-0.0202637, 0.00866699, -0.00279236, ..., -0.00604248,\n",
       "         0.00162506, -0.00271606]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.23.post_attention_layernorm.weight': Array([1.14844, 1.92969, 1.74219, ..., 1.8125, 1.67188, 1.21875],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.attn.o_proj.bias': Array([-0.0133057, -0.0368652, -0.0698242, ..., -0.433594, 0.0917969,\n",
       "        -0.081543], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.attn.o_proj.weight': Array([[-0.0078125, 0.00326538, -0.0067749, ..., -0.0216064, 0.00260925,\n",
       "         -0.00759888],\n",
       "        [-0.00263977, 0.0246582, 0.0280762, ..., 0.0151978, 0.0383301,\n",
       "         0.0324707],\n",
       "        [0.00927734, 0.0184326, -0.0065918, ..., -0.0205078, 0.0361328,\n",
       "         0.0402832],\n",
       "        ...,\n",
       "        [0.0480957, 0.0136719, 0.0493164, ..., 0.0263672, -0.0150757,\n",
       "         0.00193024],\n",
       "        [0.019165, 0.00473022, 0.00842285, ..., 0.00756836, 0.0134277,\n",
       "         -0.00491333],\n",
       "        [-0.0078125, -0.003479, 0.0233154, ..., -0.00172424, -0.0322266,\n",
       "         -0.00585938]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.attn.qkv_proj.bias': Array([0.871094, 0.984375, -1.07812, ..., -0.0786133, 0.00689697,\n",
       "        -0.0991211], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.attn.qkv_proj.weight': Array([[-0.0148926, 0.0152588, -0.0289307, ..., 0.0252686, 0.00585938,\n",
       "         -0.00279236],\n",
       "        [0.0159912, -0.0140991, 0.00112915, ..., 0.00915527, 0.0146484,\n",
       "         -0.0163574],\n",
       "        [-0.0122681, -0.00476074, -0.000450134, ..., -0.0471191,\n",
       "         0.0264893, 0.00283813],\n",
       "        ...,\n",
       "        [-0.00222778, -0.0344238, 0.078125, ..., -0.0527344, 0.0908203,\n",
       "         0.0142822],\n",
       "        [-0.00340271, 0.000230789, -0.0300293, ..., -0.0263672,\n",
       "         -0.0274658, -0.0224609],\n",
       "        [-0.0629883, -0.0327148, -0.0247803, ..., -0.0134888, 0.0111084,\n",
       "         -0.0145874]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.attn.sinks': Array([1.60156, 5.375, 5.3125, 4.625, 7.4375, 3.71875, 5.84375, 3.6875,\n",
       "        3.79688, 3.73438, 2.23438, 4.625, 5.75, 4.59375, 4.09375, 4.90625,\n",
       "        6.375, 2.17188, 4.90625, 3.67188, 5.28125, 3.29688, 3.57812,\n",
       "        3.45312, 4.71875, 5.09375, 4.625, 5.21875, 5.15625, 4.75, 2.46875,\n",
       "        2.42188, 3.46875, 1.96094, 1.52344, 2.42188, 1.59375, 2.67188,\n",
       "        1.9375, 2.48438, 5.28125, 4.1875, 4.09375, 4.84375, 5.96875,\n",
       "        5.03125, 1.60156, 2.67188, 4.625, 2.45312, 3.23438, 3.8125,\n",
       "        2.45312, 3.07812, 3.4375, 3.45312, 5.90625, 5.25, 4.96875, 5.40625,\n",
       "        5.9375, 5.53125, 6.1875, 5.8125], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.input_layernorm.weight': Array([1.82812, 1.39844, 1.53906, ..., 2.1875, 2.375, 2.125], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.mlp.experts.w13_bias': Array([[-0.400391, -0.382812, -0.357422, ..., -1.00781, -0.871094,\n",
       "         -0.988281],\n",
       "        [-0.761719, -0.229492, -1.32031, ..., -0.757812, -1.09375,\n",
       "         -0.683594],\n",
       "        [0.141602, -0.169922, 0.198242, ..., -0.423828, -0.486328,\n",
       "         -0.703125],\n",
       "        ...,\n",
       "        [-2.35938, -1.14062, -1.91406, ..., -0.898438, -0.867188,\n",
       "         -0.953125],\n",
       "        [-0.462891, -0.306641, -0.546875, ..., -0.570312, -0.753906,\n",
       "         -0.914062],\n",
       "        [-0.835938, -0.386719, -1.23438, ..., -0.660156, -0.703125,\n",
       "         -0.785156]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.mlp.experts.w13_weight': Array([[[-0.0078125, 0, 0.03125, ..., 0, 0.0234375, 0.0078125],\n",
       "         [0, 0.015625, 0, ..., -0.0078125, -0.03125, 0.03125],\n",
       "         [0.0078125, 0, -0.015625, ..., 0, 0.0078125, -0.0234375],\n",
       "         ...,\n",
       "         [-0.015625, -0, 0.0078125, ..., 0.0078125, -0.0625, -0.0078125],\n",
       "         [0, 0.015625, 0.015625, ..., -0, 0.0078125, -0.03125],\n",
       "         [-0.0078125, -0.03125, 0.015625, ..., -0.03125, 0.03125,\n",
       "          -0.015625]],\n",
       " \n",
       "        [[0.0234375, 0.0078125, 0.0234375, ..., 0.0078125, -0.0625,\n",
       "          0.046875],\n",
       "         [0.0078125, -0, -0.0117188, ..., 0, -0.0078125, 0.015625],\n",
       "         [-0, -0.015625, -0.015625, ..., -0.046875, 0.0234375, -0.046875],\n",
       "         ...,\n",
       "         [0.046875, 0, 0, ..., -0.0078125, -0.0234375, -0.03125],\n",
       "         [-0.03125, -0.0078125, -0.015625, ..., -0.015625, -0.046875,\n",
       "          -0.03125],\n",
       "         [-0.0625, -0.0625, -0.0625, ..., 0.03125, -0.015625, 0.046875]],\n",
       " \n",
       "        [[0.0078125, 0.0234375, -0, ..., -0.00390625, 0.00390625,\n",
       "          0.015625],\n",
       "         [-0.00585938, 0.0078125, 0, ..., -0.0078125, 0.00390625, 0],\n",
       "         [-0, 0.0078125, 0.00390625, ..., -0.015625, 0.0117188,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [-0.015625, -0.0625, -0.015625, ..., 0.015625, 0.015625,\n",
       "          -0.03125],\n",
       "         [0.0625, -0.0625, 0.046875, ..., 0.03125, -0.015625, 0.0625],\n",
       "         [0.0234375, 0.0234375, -0.0625, ..., -0.046875, -0.015625,\n",
       "          -0.046875]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.0625, 0, 0.0234375, ..., -0.09375, -0.015625, -0.015625],\n",
       "         [0.00390625, 0.0078125, -0.0234375, ..., 0.0078125, 0.0078125,\n",
       "          -0],\n",
       "         [-0.03125, -0.0078125, 0.0078125, ..., -0.046875, 0.0078125,\n",
       "          -0.0234375],\n",
       "         ...,\n",
       "         [-0.046875, 0.0078125, 0.046875, ..., 0.015625, 0, 0.0625],\n",
       "         [0.0078125, 0.015625, -0.0234375, ..., -0, 0.0625, -0.046875],\n",
       "         [0.0078125, -0.0234375, 0.0234375, ..., 0, -0.0625, 0.0078125]],\n",
       " \n",
       "        [[0.00390625, -0.00585938, -0.00195312, ..., 0.00195312,\n",
       "          -0.015625, 0.0117188],\n",
       "         [-0, 0.015625, 0.0078125, ..., 0.0078125, 0.00390625,\n",
       "          -0.0117188],\n",
       "         [0, -0.0234375, 0.0078125, ..., -0, 0.015625, -0.0078125],\n",
       "         ...,\n",
       "         [0.0078125, -0.046875, 0.0078125, ..., 0, -0.0625, 0.0625],\n",
       "         [0.03125, -0.015625, -0.046875, ..., -0, -0.015625, 0.015625],\n",
       "         [0.0234375, -0.0078125, 0, ..., 0.03125, 0, 0.03125]],\n",
       " \n",
       "        [[-0.0234375, -0.0234375, 0.0078125, ..., -0.015625, -0.0234375,\n",
       "          -0.046875],\n",
       "         [-0.015625, -0.0117188, -0.0117188, ..., -0.015625, -0,\n",
       "          0.0234375],\n",
       "         [0.0234375, 0.0234375, 0.03125, ..., -0, -0.0078125, 0.046875],\n",
       "         ...,\n",
       "         [0.0234375, 0.03125, -0.0234375, ..., 0.015625, -0.046875,\n",
       "          0.09375],\n",
       "         [0.09375, 0.03125, 0.015625, ..., 0.015625, 0.0625, 0.046875],\n",
       "         [0.0078125, 0.03125, -0.03125, ..., -0.0234375, 0.0078125,\n",
       "          -0.0078125]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.mlp.experts.w2_bias': Array([[-0.0673828, 0.271484, 0.057373, ..., -0.0471191, -0.0336914,\n",
       "         0.026123],\n",
       "        [0.0922852, -0.139648, -0.0913086, ..., 0.0668945, -0.129883,\n",
       "         -0.0378418],\n",
       "        [-0.000214577, -0.0888672, 0.0541992, ..., 0.142578, 0.130859,\n",
       "         -0.259766],\n",
       "        ...,\n",
       "        [0.145508, -0.0294189, 0.171875, ..., 0.0218506, -0.0507812,\n",
       "         0.0155029],\n",
       "        [-0.0898438, 0.261719, 0.02771, ..., 0.0849609, 0.0113525,\n",
       "         -0.0400391],\n",
       "        [-0.108887, -0.152344, 0.15332, ..., 0.255859, -0.0712891,\n",
       "         -0.169922]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.mlp.experts.w2_weight': Array([[[-0, 0.03125, 0, ..., -0.0625, -0.046875, 0],\n",
       "         [-0.03125, 0.03125, -0.03125, ..., -0.125, 0.0625, -0],\n",
       "         [0.0625, -0.015625, 0.015625, ..., -0.1875, 0.03125, -0.015625],\n",
       "         ...,\n",
       "         [-0.03125, 0.03125, -0.03125, ..., 0.046875, -0.03125,\n",
       "          -0.015625],\n",
       "         [-0.03125, -0.015625, 0.015625, ..., 0.015625, 0.0078125,\n",
       "          -0.046875],\n",
       "         [-0.03125, 0.046875, 0.015625, ..., 0.015625, 0, -0.015625]],\n",
       " \n",
       "        [[0.015625, 0.015625, -0.015625, ..., -0.015625, -0, 0.046875],\n",
       "         [-0.0625, -0.125, 0, ..., 0.0625, 0.09375, 0],\n",
       "         [-0.015625, 0.0078125, 0.0625, ..., 0.03125, -0.0625, -0.09375],\n",
       "         ...,\n",
       "         [-0.015625, -0.015625, 0.03125, ..., 0.046875, 0.046875,\n",
       "          -0.0234375],\n",
       "         [0.015625, 0.03125, -0.015625, ..., 0.03125, -0.046875,\n",
       "          0.0078125],\n",
       "         [-0.015625, -0.0078125, -0.0078125, ..., 0.015625, 0.03125,\n",
       "          -0.015625]],\n",
       " \n",
       "        [[-0.03125, -0.046875, 0, ..., 0.125, -0.046875, -0.046875],\n",
       "         [0.125, -0.09375, -0.0625, ..., -0.09375, -0.0625, -0.1875],\n",
       "         [-0, 0.046875, 0.015625, ..., 0.125, 0.125, 0.03125],\n",
       "         ...,\n",
       "         [-0.046875, 0.03125, -0.015625, ..., 0.1875, 0.09375, -0.03125],\n",
       "         [-0.0234375, -0.0234375, -0.0078125, ..., 0, -0.0234375,\n",
       "          0.046875],\n",
       "         [0.03125, -0.046875, 0.046875, ..., 0.046875, 0, 0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.03125, -0.0625, -0.015625, ..., -0.015625, -0.03125,\n",
       "          -0.0625],\n",
       "         [0.1875, 0.015625, 0, ..., 0.09375, -0, 0.015625],\n",
       "         [-0.125, 0.09375, 0.03125, ..., -0.015625, -0.0625, 0.0625],\n",
       "         ...,\n",
       "         [0.046875, 0.0625, 0.0625, ..., 0.0625, -0.0625, 0.015625],\n",
       "         [0.046875, 0.0078125, -0.046875, ..., -0, 0.0078125, -0.0234375],\n",
       "         [-0.09375, 0.0625, 0.046875, ..., 0.09375, -0.09375, -0.015625]],\n",
       " \n",
       "        [[0.09375, 0.046875, 0.015625, ..., -0.0078125, -0.0625, 0.03125],\n",
       "         [-0.09375, -0.09375, 0.046875, ..., -0, 0.09375, -0.03125],\n",
       "         [-0.1875, 0.03125, -0.03125, ..., 0.09375, 0.015625, 0.015625],\n",
       "         ...,\n",
       "         [0.046875, 0.015625, -0.046875, ..., 0.0625, -0.0625, -0.03125],\n",
       "         [0.0234375, 0.0234375, 0.0625, ..., 0.0078125, 0, -0.046875],\n",
       "         [0.09375, 0.0078125, 0.0234375, ..., 0.046875, 0.0625, -0.0625]],\n",
       " \n",
       "        [[0.015625, 0.046875, 0.0625, ..., -0.015625, 0.015625, -0.03125],\n",
       "         [-0.1875, -0.09375, 0.0625, ..., -0.046875, 0.03125, 0.03125],\n",
       "         [0.03125, -0.046875, 0.03125, ..., -0, -0.015625, -0.046875],\n",
       "         ...,\n",
       "         [0.09375, -0.09375, -0.015625, ..., 0.015625, 0.046875,\n",
       "          0.015625],\n",
       "         [-0.0625, 0.015625, 0.046875, ..., 0.0234375, 0.0625, 0.0234375],\n",
       "         [-0.09375, -0.015625, -0, ..., -0.0078125, 0.015625, 0.0234375]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.mlp.router.bias': Array([0.0463867, -0.0375977, -0.357422, 0.0422363, 0.00389099, 0.0673828,\n",
       "        0.0286865, -0.316406, 0.0622559, 0.0439453, -0.226562, 0.0198975,\n",
       "        -0.219727, 0.045166, -0.00282288, 0.0155029, 0.0991211, 0.097168,\n",
       "        0.00759888, -0.443359, 0.0429688, 0.0598145, -0.152344, 0.0556641,\n",
       "        0.106445, 0.0922852, 0.0529785, 0.0795898, 0.0385742, -0.00497437,\n",
       "        -0.0908203, -0.0927734], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.mlp.router.weight': Array([[0.0112915, -0.00604248, -0.0146484, ..., -0.00402832, -0.0019989,\n",
       "         -0.00512695],\n",
       "        [0.00379944, -0.00387573, 0.00970459, ..., -0.00242615,\n",
       "         -0.00817871, 0.0043335],\n",
       "        [0.0131226, -0.00274658, -0.0117798, ..., -0.00390625, 0.00683594,\n",
       "         -0.00793457],\n",
       "        ...,\n",
       "        [-0.00421143, -0.00331116, -0.0283203, ..., 0.00153351,\n",
       "         -0.000926971, 0.0017395],\n",
       "        [-0.00552368, -0.0114136, 0.0119629, ..., 8.7738e-05, 0.00747681,\n",
       "         0.00747681],\n",
       "        [-0.000545502, -0.00543213, -0.00315857, ..., 0.00028038,\n",
       "         0.00619507, 0.00292969]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.3.post_attention_layernorm.weight': Array([2.92188, 1.85156, 2.375, ..., 4.09375, 4.0625, 3.42188], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.attn.o_proj.bias': Array([-0.253906, -0.0585938, 0.0187988, ..., -0.691406, 0.0419922,\n",
       "        -0.137695], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.attn.o_proj.weight': Array([[0.00354004, 0.0108643, 0.0177002, ..., -0.0119629, -0.00354004,\n",
       "         0.0317383],\n",
       "        [-0.0244141, -0.00686646, -0.0177002, ..., 0.0480957, -0.0629883,\n",
       "         0.0234375],\n",
       "        [0.00836182, -0.0283203, 0.0101929, ..., -0.0290527, -0.00665283,\n",
       "         0.00309753],\n",
       "        ...,\n",
       "        [0.0397949, -0.0290527, 0.0125732, ..., -0.0213623, 0.0114746,\n",
       "         -0.0322266],\n",
       "        [-0.0157471, -0.0322266, -0.0164795, ..., -0.00747681, 0.00897217,\n",
       "         -0.00408936],\n",
       "        [0.000209808, -0.0314941, -0.0133057, ..., 0.0142822, -0.00442505,\n",
       "         0.00830078]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.attn.qkv_proj.bias': Array([0.0522461, 0.0218506, 0.0703125, ..., 0.410156, -0.394531,\n",
       "        -0.145508], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.attn.qkv_proj.weight': Array([[-0.00860596, 0.0135498, 0.00671387, ..., 0.0299072, -0.000549316,\n",
       "         0.00340271],\n",
       "        [0.0016861, -0.00558472, 0.00964355, ..., -0.00726318, 0.00872803,\n",
       "         -0.00695801],\n",
       "        [0.00497437, -0.00439453, -0.0101318, ..., -0.00123596,\n",
       "         -0.000163078, -0.00631714],\n",
       "        ...,\n",
       "        [-0.0395508, 0.0578613, 0.0415039, ..., -0.0266113, 0.0247803,\n",
       "         0.130859],\n",
       "        [0.0620117, -0.00927734, -0.0332031, ..., 0.0123291, -0.032959,\n",
       "         -0.0241699],\n",
       "        [0.0183105, -0.0244141, -0.059082, ..., 0.00640869, -0.00994873,\n",
       "         -0.00601196]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.attn.sinks': Array([2.5625, 3.3125, 1.76562, 2.40625, 2.29688, 1.14844, 2.0625,\n",
       "        1.89844, 2.20312, 2.46875, 2.39062, 2.42188, 1.46094, 2.76562,\n",
       "        2.1875, 1.49219, 1.85156, 1.75781, 1.08594, 2.29688, 1.4375,\n",
       "        3.09375, 2.79688, 1.21875, 3.45312, 1.46094, 3.65625, 3.32812,\n",
       "        2.6875, 3.67188, 2.9375, 3.34375, 1.98438, 0.382812, 1.28906,\n",
       "        1.17969, 1.73438, 0.118164, 1.45312, 3.07812, 2.54688, 1.86719,\n",
       "        3.29688, 2.29688, 2.3125, 2.96875, 2.40625, 1.19531, 3.82812,\n",
       "        3.0625, 2.82812, 2.60938, 2.23438, 2.07812, 2.65625, 2.59375,\n",
       "        2.26562, 1.29688, 2.07812, 2.15625, 0.996094, 1.14062, 0.898438,\n",
       "        1.64062], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.input_layernorm.weight': Array([1.95312, 1.57812, 1.5, ..., 2.01562, 2.34375, 2.15625], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.mlp.experts.w13_bias': Array([[-1.71875, -0.726562, -0.855469, ..., -0.859375, -0.964844,\n",
       "         -1.02344],\n",
       "        [-0.859375, -0.832031, -1.13281, ..., -0.855469, -0.898438,\n",
       "         -0.921875],\n",
       "        [-0.283203, -0.515625, -0.332031, ..., -0.914062, -0.116699,\n",
       "         -0.925781],\n",
       "        ...,\n",
       "        [-1.47656, -0.582031, -0.988281, ..., -0.875, -0.792969, -0.78125],\n",
       "        [-0.158203, -1.28125, -0.277344, ..., -0.753906, -0.792969,\n",
       "         -0.796875],\n",
       "        [-1.00781, -0.746094, -0.300781, ..., -0.882812, -0.878906,\n",
       "         -0.664062]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.mlp.experts.w13_weight': Array([[[0.0234375, 0, -0.0625, ..., -0.03125, -0.0078125, -0.015625],\n",
       "         [-0.015625, -0.0234375, -0.0234375, ..., 0.0078125, -0.0078125,\n",
       "          -0.046875],\n",
       "         [-0.0625, 0.015625, -0.015625, ..., -0, -0.0234375, -0.0625],\n",
       "         ...,\n",
       "         [0.0234375, 0.0234375, -0.00390625, ..., -0.015625, 0.046875,\n",
       "          0.0078125],\n",
       "         [0.0078125, 0.0234375, -0.015625, ..., -0.0078125, -0.015625,\n",
       "          0.0234375],\n",
       "         [-0.0234375, 0.015625, 0.03125, ..., -0.0078125, 0.046875,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[0.00390625, -0.0234375, 0.0234375, ..., -0.03125, 0.0078125,\n",
       "          -0.03125],\n",
       "         [0.015625, -0.0234375, -0.0117188, ..., 0.0078125, -0.0078125,\n",
       "          0.046875],\n",
       "         [-0, 0.0078125, -0.03125, ..., -0.0234375, 0.03125, 0.03125],\n",
       "         ...,\n",
       "         [0.0234375, 0.046875, -0.046875, ..., -0.046875, 0.0625,\n",
       "          -0.0078125],\n",
       "         [-0.03125, 0, -0.0078125, ..., 0.046875, -0.0078125, 0.0078125],\n",
       "         [-0.046875, 0.00390625, -0.0078125, ..., -0.03125, 0.0078125,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[-0.03125, 0.00390625, 0.03125, ..., 0.0078125, 0.0234375, 0],\n",
       "         [0.0234375, 0.0117188, 0.00390625, ..., -0.015625, 0.0234375,\n",
       "          0.0234375],\n",
       "         [-0.0078125, -0.00390625, 0.00585938, ..., 0.0117188,\n",
       "          -0.0078125, -0.00390625],\n",
       "         ...,\n",
       "         [0.046875, -0.015625, 0.0078125, ..., -0.046875, -0.015625,\n",
       "          0.046875],\n",
       "         [0.03125, 0.015625, -0.0078125, ..., 0.015625, -0.015625,\n",
       "          -0.015625],\n",
       "         [0, -0.015625, -0, ..., -0.0625, -0.015625, 0.0625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.046875, -0.015625, 0.0078125, ..., -0.03125, -0.015625,\n",
       "          -0.015625],\n",
       "         [-0.03125, -0, 0.015625, ..., -0.015625, 0.0078125, -0.0234375],\n",
       "         [0.0078125, 0, 0.0078125, ..., -0.0078125, -0.00390625,\n",
       "          0.00390625],\n",
       "         ...,\n",
       "         [-0.015625, -0.03125, -0, ..., 0.0078125, -0, 0.046875],\n",
       "         [0.015625, -0.03125, -0.046875, ..., 0.046875, -0.046875, -0],\n",
       "         [0.03125, 0.015625, 0.0234375, ..., 0.046875, -0.015625, 0]],\n",
       " \n",
       "        [[0.00390625, 0.0078125, 0.00585938, ..., 0.00390625, -0.0078125,\n",
       "          0],\n",
       "         [-0.0078125, 0.03125, 0.0625, ..., 0.015625, 0, 0.046875],\n",
       "         [-0.0078125, -0.0078125, 0.00585938, ..., -0.00585938,\n",
       "          -0.0117188, 0.00195312],\n",
       "         ...,\n",
       "         [0.03125, 0.0234375, 0.015625, ..., -0.03125, 0, 0.0625],\n",
       "         [0.015625, 0.03125, 0.03125, ..., -0.03125, -0.0625, -0.1875],\n",
       "         [0, -0.046875, -0.015625, ..., 0.015625, 0.0234375, 0.03125]],\n",
       " \n",
       "        [[0.0078125, 0.0234375, -0.00390625, ..., -0.015625, 0.015625,\n",
       "          -0.0078125],\n",
       "         [0.015625, 0, -0.03125, ..., -0.0078125, 0.015625, 0.0234375],\n",
       "         [0, -0.015625, -0.00195312, ..., 0.00390625, -0.00390625,\n",
       "          -0.00195312],\n",
       "         ...,\n",
       "         [0.015625, -0.015625, -0.0234375, ..., 0.0078125, 0.0078125,\n",
       "          -0.046875],\n",
       "         [-0.0625, -0.0234375, 0.0078125, ..., 0.0078125, 0.046875,\n",
       "          -0.03125],\n",
       "         [-0.046875, -0.015625, -0.0078125, ..., 0.03125, 0.0234375, -0]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.mlp.experts.w2_bias': Array([[0.0546875, 0.388672, 0.142578, ..., -0.181641, 0.0110474,\n",
       "         -0.00668335],\n",
       "        [-0.0598145, -0.12793, -0.102539, ..., -0.283203, 0.0505371,\n",
       "         0.09375],\n",
       "        [-0.0732422, -0.133789, -0.292969, ..., 0.0167236, 0.101562,\n",
       "         -0.0151367],\n",
       "        ...,\n",
       "        [0.103027, -0.287109, 0.0327148, ..., -0.236328, -0.0563965,\n",
       "         0.0461426],\n",
       "        [0.161133, -0.0136108, 0.00686646, ..., 0.160156, 0.118164,\n",
       "         -0.119141],\n",
       "        [0.0761719, -0.0517578, 0.248047, ..., -0.198242, 0.0197754,\n",
       "         -0.100586]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.mlp.experts.w2_weight': Array([[[-0.09375, -0, 0.03125, ..., -0.015625, -0.03125, 0.015625],\n",
       "         [-0.03125, 0.1875, -0.09375, ..., -0.125, 0.0625, -0.1875],\n",
       "         [-0.015625, 0.0625, 0.125, ..., 0.03125, -0.25, -0.03125],\n",
       "         ...,\n",
       "         [0.046875, 0.046875, 0, ..., 0, 0.015625, -0.0625],\n",
       "         [0.125, 0.03125, 0.015625, ..., 0, 0.015625, 0.046875],\n",
       "         [0.015625, 0, 0.015625, ..., 0.09375, 0.0625, -0]],\n",
       " \n",
       "        [[-0.03125, 0.0625, -0.03125, ..., 0.125, 0, -0.046875],\n",
       "         [-0.25, -0, 0.0625, ..., 0.03125, -0.125, -0.03125],\n",
       "         [0.03125, 0.0625, 0, ..., -0.046875, 0.015625, 0.015625],\n",
       "         ...,\n",
       "         [-0.015625, 0.015625, 0.046875, ..., -0.125, -0.03125, 0.125],\n",
       "         [0.03125, 0.015625, 0.03125, ..., 0.03125, 0.03125, 0],\n",
       "         [-0.09375, 0.0625, -0.015625, ..., -0.03125, -0.0625, -0.0625]],\n",
       " \n",
       "        [[-0.015625, 0.0625, -0.03125, ..., 0.125, -0.046875, -0.046875],\n",
       "         [0.03125, -0.1875, 0.09375, ..., 0, -0.125, 0.03125],\n",
       "         [-0.0625, -0, 0, ..., -0.015625, 0.09375, -0.03125],\n",
       "         ...,\n",
       "         [-0.0625, 0.09375, -0.046875, ..., -0.046875, -0.0625, -0.0625],\n",
       "         [0.015625, 0, -0.015625, ..., -0.0078125, 0.0625, -0.0625],\n",
       "         [0.046875, -0.03125, 0.09375, ..., 0.015625, 0.09375, 0.0625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.09375, 0.09375, -0, ..., -0.09375, -0.0625, 0.0625],\n",
       "         [-0.03125, 0.1875, 0.03125, ..., 0.03125, -0, -0.125],\n",
       "         [-0.015625, 0.03125, -0.015625, ..., 0.1875, -0.125, 0.0625],\n",
       "         ...,\n",
       "         [0.1875, 0.0625, 0.03125, ..., 0.125, 0.09375, -0],\n",
       "         [-0.0625, 0.09375, -0.03125, ..., 0.0625, -0.0625, 0.015625],\n",
       "         [0.09375, -0.015625, 0.046875, ..., -0.0625, -0.03125, -0.0625]],\n",
       " \n",
       "        [[0.0625, 0.09375, -0.09375, ..., 0.03125, 0.0625, -0],\n",
       "         [-0.0625, -0.09375, -0.125, ..., 0, -0, 0],\n",
       "         [0.09375, 0.03125, 0.09375, ..., -0.1875, -0, 0.0625],\n",
       "         ...,\n",
       "         [0.046875, 0.125, 0.0625, ..., 0.0625, 0, -0.0625],\n",
       "         [0, 0.03125, -0.0625, ..., 0.09375, -0.015625, 0.09375],\n",
       "         [-0.0625, 0.046875, -0.125, ..., -0.0625, -0.0625, 0.03125]],\n",
       " \n",
       "        [[-0.03125, 0.125, 0.09375, ..., 0.046875, -0.125, 0.03125],\n",
       "         [0.25, 0.03125, -0.1875, ..., -0.03125, 0.03125, -0.03125],\n",
       "         [-0.125, 0, -0.1875, ..., 0.0625, -0, -0.09375],\n",
       "         ...,\n",
       "         [0.09375, -0.1875, -0, ..., -0.03125, 0.03125, -0.25],\n",
       "         [-0.03125, 0, -0.046875, ..., -0.03125, 0.015625, 0.015625],\n",
       "         [0.09375, -0.09375, -0, ..., -0.125, 0.0625, 0.03125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.mlp.router.bias': Array([0.0197754, -0.0615234, -0.0476074, 0.0493164, 0.0483398,\n",
       "        -0.0196533, -0.203125, 0.065918, 0.0515137, -0.0179443, -0.240234,\n",
       "        0.0361328, -0.00515747, 0.0620117, -0.0111084, -0.136719, 0.107422,\n",
       "        0.0568848, -0.287109, 0.0375977, -0.251953, 0.125, 0.0742188,\n",
       "        0.0559082, 0.0593262, 0.0419922, 0.0437012, -0.0664062, -0.0115967,\n",
       "        0.0722656, -0.435547, 0.0172119], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.mlp.router.weight': Array([[0.00230408, -0.00830078, -0.00619507, ..., -0.00309753,\n",
       "         0.00860596, 0.00860596],\n",
       "        [-0.000289917, -0.000343323, 0.0109253, ..., 0.00927734,\n",
       "         -0.00479126, -0.00315857],\n",
       "        [-0.00488281, 0.00616455, 0.0142822, ..., -0.00653076,\n",
       "         -0.00842285, -0.00111389],\n",
       "        ...,\n",
       "        [-0.0163574, 0.00939941, -0.00488281, ..., -0.00382996,\n",
       "         0.00531006, -0.00250244],\n",
       "        [0.00331116, -0.00300598, -0.0065918, ..., -0.00209045,\n",
       "         -0.00238037, -0.00180817],\n",
       "        [0.000934601, -0.000564575, -0.00738525, ..., -0.000413895,\n",
       "         -0.00114441, 0.000648499]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.4.post_attention_layernorm.weight': Array([3.07812, 1.76562, 2.625, ..., 3.625, 3.82812, 3.48438], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.attn.o_proj.bias': Array([-0.0375977, 0.00491333, 0.0125122, ..., -0.398438, 0.154297,\n",
       "        -0.176758], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.attn.o_proj.weight': Array([[-0.0314941, 0.00601196, -0.00765991, ..., -0.00570679,\n",
       "         -0.00671387, -0.050293],\n",
       "        [-0.0154419, 0.0383301, -0.0281982, ..., -0.0361328, -0.0473633,\n",
       "         0.125],\n",
       "        [0.0140991, 0.0480957, -0.0559082, ..., -0.0133057, 0.0349121,\n",
       "         0.0578613],\n",
       "        ...,\n",
       "        [0.0344238, 0.0864258, -0.00341797, ..., 0.0032196, -0.00299072,\n",
       "         0.0444336],\n",
       "        [0.0322266, 0.0639648, -0.0098877, ..., -0.012085, -0.00485229,\n",
       "         -0.0517578],\n",
       "        [0.0247803, -0.000335693, -0.0688477, ..., 0.00180054, -0.0132446,\n",
       "         -0.0217285]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.attn.qkv_proj.bias': Array([0.102539, 0.0766602, 0.0756836, ..., -0.132812, -0.679688, -0.3125],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.attn.qkv_proj.weight': Array([[-0.0283203, -0.00686646, -0.0266113, ..., -0.00120544,\n",
       "         0.00656128, -0.00265503],\n",
       "        [-0.00253296, 0.019165, -0.0072937, ..., 0.0211182, -0.027832,\n",
       "         0.0134277],\n",
       "        [0.0137939, -0.0246582, 0.0100708, ..., -0.0415039, 0.0088501,\n",
       "         -0.0108643],\n",
       "        ...,\n",
       "        [0.0546875, 0.0170898, 0.00567627, ..., -0.0130005, 0.0830078,\n",
       "         -0.0219727],\n",
       "        [-0.0016861, 0.0419922, 0.00799561, ..., 0.043457, 0.0045166,\n",
       "         0.0549316],\n",
       "        [0.0236816, -0.0319824, 0.0219727, ..., -0.0612793, 0.0683594,\n",
       "         0.0620117]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.attn.sinks': Array([3.5, 3.60938, 5.0625, 4.5, 2, 5.03125, 3.90625, 2.98438, 1.97656,\n",
       "        4.28125, 3.9375, 4.84375, 4.625, 4.9375, 4.28125, 6.25, 3.26562,\n",
       "        3.85938, 1.94531, 3.65625, 3.6875, 3.64062, 5.09375, 4.65625,\n",
       "        3.39062, 1.73438, 1.71875, 1.125, 1.77344, 2.28125, 2.96875,\n",
       "        2.04688, 3.21875, 3.51562, 2.59375, 0.976562, 2.84375, 1.75,\n",
       "        5.90625, 3.21875, 3.96875, 4.4375, 5.53125, 5.40625, 2.73438,\n",
       "        5.34375, 4.09375, 4.46875, 3.84375, 2.57812, 4.34375, 1.72656,\n",
       "        3.64062, 5.0625, 6.59375, 2.21875, 5.71875, 4.46875, 2.5, 0.6875,\n",
       "        5.25, 8.1875, 5.125, 4.71875], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.input_layernorm.weight': Array([1.89062, 1.49219, 1.60156, ..., 1.77344, 2.125, 2], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.mlp.experts.w13_bias': Array([[-0.306641, -1.03906, 0.0683594, ..., -0.027832, -0.242188,\n",
       "         -0.722656],\n",
       "        [-0.5625, -0.341797, -0.230469, ..., -1.00781, -1.05469, -1],\n",
       "        [-0.257812, -0.0839844, -0.566406, ..., -0.792969, -0.847656,\n",
       "         -0.691406],\n",
       "        ...,\n",
       "        [-2.1875, -1.61719, -0.328125, ..., -1.01562, -0.964844, -1.01562],\n",
       "        [-0.138672, -0.306641, -0.298828, ..., -1, -0.761719, -0.761719],\n",
       "        [-1.28125, -2.09375, -0.273438, ..., -0.714844, -1.09375,\n",
       "         -1.21875]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.mlp.experts.w13_weight': Array([[[0.015625, -0.0078125, 0.00195312, ..., -0.0078125, 0.015625,\n",
       "          -0.00390625],\n",
       "         [-0.0234375, 0, -0.03125, ..., -0.0234375, 0, 0],\n",
       "         [0.0078125, -0.03125, 0.0078125, ..., 0, -0.0078125, 0],\n",
       "         ...,\n",
       "         [0.03125, 0.03125, -0, ..., -0.0078125, -0.046875, 0.0625],\n",
       "         [0.03125, 0.0078125, 0.015625, ..., -0.0078125, -0.0625,\n",
       "          -0.046875],\n",
       "         [0.09375, 0.046875, 0, ..., -0.03125, -0, -0.015625]],\n",
       " \n",
       "        [[-0.0117188, -0.0117188, -0, ..., 0.00390625, -0.0117188,\n",
       "          -0.0117188],\n",
       "         [-0.0117188, 0.00390625, -0.015625, ..., 0.015625, 0.00390625,\n",
       "          -0],\n",
       "         [0.0117188, -0.0117188, -0.0078125, ..., 0.0234375, -0.00390625,\n",
       "          -0.00390625],\n",
       "         ...,\n",
       "         [-0.0078125, 0.0234375, -0.0078125, ..., -0.015625, -0.03125,\n",
       "          -0.0078125],\n",
       "         [0.0234375, -0.046875, -0.0234375, ..., -0.0234375, -0.03125, 0],\n",
       "         [-0.03125, 0.015625, 0.03125, ..., 0, 0, -0.0625]],\n",
       " \n",
       "        [[0.0117188, -0.00390625, 0, ..., 0.00390625, 0.0117188,\n",
       "          0.00195312],\n",
       "         [-0.00390625, -0.00585938, -0.00585938, ..., 0.0117188, -0,\n",
       "          -0.00585938],\n",
       "         [-0, 0.0234375, 0.03125, ..., -0.00390625, -0.0234375,\n",
       "          -0.0234375],\n",
       "         ...,\n",
       "         [0.046875, 0.03125, -0.015625, ..., 0, -0.015625, -0.015625],\n",
       "         [-0.03125, -0.0078125, -0, ..., -0.015625, 0.015625, 0.046875],\n",
       "         [-0.0234375, 0.015625, 0.0234375, ..., -0.03125, 0.0078125,\n",
       "          -0.046875]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.015625, 0.0234375, 0, ..., -0.0234375, 0.03125, 0.046875],\n",
       "         [0, 0.0078125, 0.0078125, ..., 0.03125, 0, -0.03125],\n",
       "         [0.00390625, -0.015625, -0.015625, ..., -0.015625, 0.0234375,\n",
       "          -0.0234375],\n",
       "         ...,\n",
       "         [-0.046875, 0.015625, 0, ..., 0.046875, -0.046875, 0.015625],\n",
       "         [0.09375, -0.015625, -0.03125, ..., 0.0625, -0.046875, 0.046875],\n",
       "         [0.0234375, -0.03125, -0.03125, ..., 0.0234375, 0, -0]],\n",
       " \n",
       "        [[-0.00585938, 0.015625, 0.00390625, ..., 0.00585938, -0.0117188,\n",
       "          -0.00195312],\n",
       "         [0.0117188, -0.0078125, -0.00390625, ..., 0, 0.00390625,\n",
       "          0.0117188],\n",
       "         [-0.015625, 0.015625, -0.00390625, ..., -0.00390625, -0.0078125,\n",
       "          0.0234375],\n",
       "         ...,\n",
       "         [0.0078125, 0.0078125, -0.03125, ..., -0.03125, -0.03125,\n",
       "          0.046875],\n",
       "         [-0.015625, -0.0078125, 0.015625, ..., 0.015625, -0.015625,\n",
       "          0.0625],\n",
       "         [-0.0078125, -0.0234375, 0.09375, ..., -0.0234375, -0.03125,\n",
       "          0.015625]],\n",
       " \n",
       "        [[-0.0078125, -0.0078125, 0.0078125, ..., -0.03125, -0.0078125,\n",
       "          -0.0078125],\n",
       "         [0.015625, -0.0078125, 0.015625, ..., -0.0625, -0.015625,\n",
       "          -0.015625],\n",
       "         [0, 0, 0.00390625, ..., -0.0078125, 0.0234375, -0.0117188],\n",
       "         ...,\n",
       "         [-0.046875, 0.015625, 0.015625, ..., 0.0078125, 0.0078125,\n",
       "          -0.0625],\n",
       "         [0.046875, -0.046875, -0.015625, ..., -0.046875, -0.0234375,\n",
       "          -0.09375],\n",
       "         [0.03125, 0.03125, -0.046875, ..., 0.046875, -0.0234375,\n",
       "          0.0234375]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.mlp.experts.w2_bias': Array([[0.353516, 0.248047, -0.367188, ..., 0.136719, 0.112793,\n",
       "         -0.00302124],\n",
       "        [0.116699, 0.0844727, 0.172852, ..., 0.00521851, -0.045166,\n",
       "         -0.0830078],\n",
       "        [-0.164062, -0.202148, 0.00448608, ..., 0.337891, 0.238281,\n",
       "         -0.118652],\n",
       "        ...,\n",
       "        [-0.3125, -0.0825195, 0.166992, ..., 0.100098, 0.0917969,\n",
       "         0.0551758],\n",
       "        [-0.0529785, -0.174805, 0.248047, ..., 0.130859, 0.265625,\n",
       "         -0.0571289],\n",
       "        [-0.00376892, -0.332031, -0.0253906, ..., 0.0615234, 0.0257568,\n",
       "         -0.255859]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.mlp.experts.w2_weight': Array([[[0, 0.09375, -0.046875, ..., -0.03125, 0.03125, -0.03125],\n",
       "         [0.125, -0.0625, -0.0625, ..., 0.125, -0.0625, 0.1875],\n",
       "         [-0.03125, -0.0625, 0.0625, ..., -0.125, 0.0625, -0.03125],\n",
       "         ...,\n",
       "         [-0.09375, 0.03125, 0.09375, ..., 0, 0, 0.0625],\n",
       "         [0.03125, -0.03125, -0.03125, ..., 0.03125, -0, -0.0625],\n",
       "         [-0, 0.03125, -0.03125, ..., 0.09375, 0.03125, -0]],\n",
       " \n",
       "        [[-0, 0, 0.03125, ..., -0.09375, -0.09375, -0.0625],\n",
       "         [0, 0.1875, -0.03125, ..., 0.03125, -0.1875, -0.09375],\n",
       "         [0.03125, -0.25, -0, ..., -0.015625, -0.015625, -0.1875],\n",
       "         ...,\n",
       "         [-0.09375, -0.09375, -0.125, ..., 0.09375, 0.0625, -0.125],\n",
       "         [-0.0625, 0.015625, -0.015625, ..., 0, 0, 0],\n",
       "         [-0, -0.03125, -0.0625, ..., -0.09375, 0.015625, -0.046875]],\n",
       " \n",
       "        [[0.03125, -0.125, -0.125, ..., 0.125, -0.09375, 0.0625],\n",
       "         [-0.25, 0.1875, -0, ..., 0.25, -0, 0.1875],\n",
       "         [0.1875, 0.1875, 0.1875, ..., 0.0625, -0.03125, -0],\n",
       "         ...,\n",
       "         [0.25, -0.03125, 0.09375, ..., -0.0625, 0.125, -0.125],\n",
       "         [0.03125, -0.03125, 0.046875, ..., -0, 0.1875, -0.0625],\n",
       "         [-0.046875, -0.125, -0.1875, ..., -0.015625, -0.125, -0.0625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.046875, -0.046875, -0.03125, ..., -0, 0.125, 0],\n",
       "         [-0.25, 0.09375, -0.125, ..., 0.0625, -0.1875, 0],\n",
       "         [-0.0625, -0.09375, -0.0625, ..., 0.03125, -0.125, 0],\n",
       "         ...,\n",
       "         [-0.03125, 0.125, 0.03125, ..., 0.0625, -0, 0.125],\n",
       "         [-0.03125, -0.125, -0.015625, ..., 0.03125, -0.015625, -0.03125],\n",
       "         [0.1875, -0.03125, 0.015625, ..., 0, 0.015625, -0.015625]],\n",
       " \n",
       "        [[-0.03125, -0.09375, 0.0625, ..., -0.09375, -0, -0.125],\n",
       "         [-0.0625, 0.0625, -0.0625, ..., -0.25, 0.0625, 0.03125],\n",
       "         [-0.03125, -0, -0.125, ..., -0.09375, 0.0625, -0.09375],\n",
       "         ...,\n",
       "         [-0.0625, 0, 0.125, ..., 0.125, 0.125, 0.125],\n",
       "         [0.0625, -0.125, -0.046875, ..., -0.03125, -0.09375, 0.09375],\n",
       "         [0, 0.09375, 0.03125, ..., 0.0625, -0.0625, 0.09375]],\n",
       " \n",
       "        [[0.015625, -0, -0.046875, ..., 0.03125, 0.125, 0.03125],\n",
       "         [0.0625, -0.0625, 0.03125, ..., -0, -0.03125, -0],\n",
       "         [-0.125, -0.0625, 0.015625, ..., -0, 0, 0],\n",
       "         ...,\n",
       "         [0.03125, -0.09375, 0.125, ..., -0.0625, 0.0625, 0.09375],\n",
       "         [0.046875, 0.03125, 0.125, ..., 0.03125, -0.09375, -0.0625],\n",
       "         [0, 0.03125, -0.03125, ..., 0.015625, -0.0625, -0.015625]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.mlp.router.bias': Array([-0.335938, 0.166992, 0.0214844, 0.00842285, 0.0593262, -0.00817871,\n",
       "        -0.170898, 0.0795898, 0.0649414, -0.0189209, -0.0756836, 0.0145874,\n",
       "        0.0240479, 0.00970459, -0.263672, 0.174805, 0.0227051, -0.4375,\n",
       "        -0.0102539, 0.0300293, 0.0109253, 0.0395508, 0.00631714, 0.0385742,\n",
       "        0.114746, -0.103027, -0.0673828, 0.0693359, 0.0132446, 0.052002,\n",
       "        -0.0771484, -0.0018158], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.mlp.router.weight': Array([[-0.0071106, 0.000667572, 0.00375366, ..., -0.00180054,\n",
       "         -0.00463867, -6.77109e-05],\n",
       "        [0.00143433, 0.00683594, 0.00424194, ..., -0.00469971,\n",
       "         -0.00369263, -0.00267029],\n",
       "        [-0.00595093, -0.00512695, 0.00872803, ..., -0.00160217,\n",
       "         -0.00946045, -0.00236511],\n",
       "        ...,\n",
       "        [0.0178223, -0.00279236, 0.000276566, ..., 0.00418091,\n",
       "         -0.000968933, 0.00878906],\n",
       "        [-0.00500488, -0.00537109, 0.00120544, ..., 0.0019455,\n",
       "         -0.00171661, -0.00448608],\n",
       "        [-0.000770569, 0.00219727, 0.00460815, ..., 0.00326538,\n",
       "         0.00628662, 0.0011673]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.5.post_attention_layernorm.weight': Array([2.82812, 1.64062, 2.46875, ..., 2.5, 3.42188, 3.1875], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.attn.o_proj.bias': Array([-0.261719, -0.289062, 0.158203, ..., -0.34375, 0.12207, -0.048584],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.attn.o_proj.weight': Array([[-0.0105591, -0.02771, 0.00244141, ..., 0.0498047, -0.00939941,\n",
       "         -0.0361328],\n",
       "        [-0.0588379, 0.0239258, -0.0388184, ..., -0.0306396, -0.0495605,\n",
       "         0.00056076],\n",
       "        [-0.0148315, -0.0187988, -0.0157471, ..., -0.00830078,\n",
       "         -0.00405884, 0.015625],\n",
       "        ...,\n",
       "        [0.0147095, 0.0166016, -0.00114441, ..., 0.045166, -0.0128174,\n",
       "         -0.0117188],\n",
       "        [0.0322266, 0.000413895, 0.0098877, ..., 0.00524902, -0.00497437,\n",
       "         -0.0407715],\n",
       "        [0.0117188, -0.0334473, -0.0257568, ..., -0.0283203, -0.0179443,\n",
       "         0.00732422]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.attn.qkv_proj.bias': Array([-0.863281, -0.890625, -0.734375, ..., -0.423828, 1.14062,\n",
       "        -0.332031], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.attn.qkv_proj.weight': Array([[-0.00552368, -0.00349426, -0.00817871, ..., 0.0119629,\n",
       "         -0.00836182, 0.0174561],\n",
       "        [-0.00964355, -0.0179443, -0.00421143, ..., 0.0227051, -0.0155029,\n",
       "         0.0263672],\n",
       "        [-0.0203857, -0.00765991, 0.00210571, ..., 0.0100708, -0.0181885,\n",
       "         0.00196838],\n",
       "        ...,\n",
       "        [-0.11377, 0.0213623, -0.0144043, ..., -0.0306396, 0.0522461,\n",
       "         0.119141],\n",
       "        [-0.0238037, 0.00738525, 0.0292969, ..., -0.00726318, 0.0461426,\n",
       "         0.050293],\n",
       "        [0.0529785, 0.00257874, -0.027832, ..., 0.0148315, 0.0839844,\n",
       "         -0.00415039]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.attn.sinks': Array([2.75, 2.65625, 1.92969, 3.35938, 2.9375, 3.01562, 1.34375, 2.21875,\n",
       "        2.20312, 1.17969, 3.42188, 2.5625, 1.90625, 2.95312, 2.04688,\n",
       "        1.13281, 2.1875, 2.71875, 2.14062, 3.04688, 2.375, 1.625, 2.9375,\n",
       "        2.79688, 2.0625, 1.85156, 2.42188, 1.03125, 1.59375, 1.41406,\n",
       "        1.95312, 2.1875, 2.29688, 2.5, 4.1875, 1.96875, 3.125, 1.08594,\n",
       "        1.39062, 2.29688, 2.375, 2.29688, 1.61719, 1.97656, 1.49219, 2.25,\n",
       "        1.94531, 2.14062, 1.82812, 2.40625, 1.33594, 0.320312, 0.408203,\n",
       "        0.8125, 0.574219, -2.29688, 1.36719, 2.48438, 2.04688, 2.51562,\n",
       "        1.92188, 2.90625, 1.98438, 1.10938], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.input_layernorm.weight': Array([1.9375, 1.49219, 1.64062, ..., 1.82031, 2.14062, 1.95312],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.mlp.experts.w13_bias': Array([[-0.914062, -0.625, -0.464844, ..., -0.921875, -0.929688,\n",
       "         -0.824219],\n",
       "        [-1.92188, -0.384766, -1.28906, ..., -0.847656, -0.957031,\n",
       "         -0.566406],\n",
       "        [-1.11719, -0.12793, -0.808594, ..., -0.796875, -0.851562,\n",
       "         -0.777344],\n",
       "        ...,\n",
       "        [-0.78125, -0.765625, -0.196289, ..., -0.925781, -0.773438,\n",
       "         -0.921875],\n",
       "        [-0.201172, -0.222656, -0.75, ..., -0.839844, -0.921875,\n",
       "         -0.910156],\n",
       "        [-0.182617, -0.19043, -0.107422, ..., -0.785156, -0.566406,\n",
       "         -0.832031]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.mlp.experts.w13_weight': Array([[[0.0078125, 0.0117188, -0.0078125, ..., 0.015625, -0.015625,\n",
       "          0.03125],\n",
       "         [0.03125, 0, 0.015625, ..., 0.0234375, -0, 0.0078125],\n",
       "         [0.0234375, 0.015625, 0.0234375, ..., -0.0078125, 0.0234375,\n",
       "          -0.0117188],\n",
       "         ...,\n",
       "         [-0.0234375, -0, -0.015625, ..., 0.015625, 0.046875, 0.046875],\n",
       "         [-0.0234375, -0.0234375, 0.0078125, ..., -0.0078125, 0, -0],\n",
       "         [-0.0234375, 0.0117188, -0.03125, ..., 0.0234375, 0.0625,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[0.015625, -0.03125, -0.015625, ..., -0.046875, -0.015625,\n",
       "          -0.015625],\n",
       "         [-0.015625, 0.0117188, 0.00390625, ..., 0.00390625, 0.0234375,\n",
       "          -0.015625],\n",
       "         [0.00390625, 0.0117188, -0.00390625, ..., -0.0625, 0.0078125,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [-0.0234375, -0.00390625, -0.0234375, ..., 0.0078125, -0.015625,\n",
       "          -0.0234375],\n",
       "         [-0.0625, 0, -0.015625, ..., 0, 0, -0.03125],\n",
       "         [0.046875, -0.015625, 0.015625, ..., 0.046875, -0, 0.015625]],\n",
       " \n",
       "        [[0.0078125, -0.015625, 0, ..., 0.015625, 0.0078125, 0.0234375],\n",
       "         [0.00195312, 0.0078125, 0.00585938, ..., 0, -0.000976562,\n",
       "          -0.0078125],\n",
       "         [0.0625, -0.046875, 0, ..., -0.03125, 0.0234375, 0.0117188],\n",
       "         ...,\n",
       "         [0.015625, -0.015625, -0.03125, ..., 0.015625, 0.0625, -0.03125],\n",
       "         [-0, -0.03125, -0, ..., -0, -0.0625, -0.0234375],\n",
       "         [-0.015625, -0.0234375, 0.03125, ..., 0.0234375, 0.0078125,\n",
       "          0.015625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.03125, 0, 0.015625, ..., -0, -0.03125, -0.015625],\n",
       "         [0.0078125, 0.015625, 0.015625, ..., -0.046875, -0, -0.0234375],\n",
       "         [-0.0078125, -0.0234375, -0.00390625, ..., -0.03125, 0.0078125,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [0.015625, -0.015625, -0.09375, ..., 0.0625, 0.0625, -0.046875],\n",
       "         [-0.046875, 0.0078125, -0.03125, ..., -0.03125, -0.0078125, -0],\n",
       "         [0.03125, -0.015625, 0.0234375, ..., -0.0234375, 0.046875,\n",
       "          -0.0078125]],\n",
       " \n",
       "        [[0.015625, 0.0078125, -0.0078125, ..., 0.00390625, -0.00390625,\n",
       "          0],\n",
       "         [-0.015625, 0, -0.0234375, ..., -0.0117188, -0.00390625,\n",
       "          -0.015625],\n",
       "         [-0.015625, -0.0078125, 0.0234375, ..., -0.015625, 0.0234375,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [-0.015625, 0.0078125, -0.015625, ..., -0, 0.046875, 0.046875],\n",
       "         [0.015625, -0.03125, 0.046875, ..., 0.0234375, -0, 0.015625],\n",
       "         [-0.015625, -0, 0.0078125, ..., 0.015625, 0.046875, -0.03125]],\n",
       " \n",
       "        [[-0.00195312, -0.00390625, 0.00390625, ..., 0.00585938,\n",
       "          -0.00390625, -0.00292969],\n",
       "         [0.015625, 0.0078125, -0.0234375, ..., 0.015625, 0.03125,\n",
       "          -0.00390625],\n",
       "         [0.0078125, 0.00585938, -0.0078125, ..., -0.0078125, 0.00195312,\n",
       "          -0.0117188],\n",
       "         ...,\n",
       "         [0.125, 0.0625, 0.03125, ..., 0, 0.09375, -0.03125],\n",
       "         [0.03125, 0.015625, 0, ..., 0.0625, 0, -0.0625],\n",
       "         [-0.046875, 0.03125, -0.0234375, ..., 0.03125, 0.0078125,\n",
       "          0.0078125]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.mlp.experts.w2_bias': Array([[-0.203125, -0.326172, 0.201172, ..., 0.103027, 0.048584,\n",
       "         -0.199219],\n",
       "        [0.104492, 0.00482178, 0.279297, ..., -0.324219, 0.390625,\n",
       "         -0.0610352],\n",
       "        [0.197266, 0.12207, 0.146484, ..., 0.189453, 0.0107422, -0.347656],\n",
       "        ...,\n",
       "        [0.0942383, 0.0375977, 0.0722656, ..., 0.0981445, -0.238281,\n",
       "         -0.09375],\n",
       "        [0.0196533, -0.206055, -0.291016, ..., 0.165039, 0.00311279,\n",
       "         -0.0371094],\n",
       "        [-0.0869141, -0.271484, -0.113281, ..., 0.0473633, 0.269531,\n",
       "         -0.239258]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.mlp.experts.w2_weight': Array([[[-0.03125, 0.09375, 0.03125, ..., 0.03125, -0.03125, -0],\n",
       "         [-0.125, -0.125, -0.0625, ..., 0.1875, 0.0625, 0.25],\n",
       "         [-0, 0.125, 0.25, ..., -0.1875, 0.1875, 0],\n",
       "         ...,\n",
       "         [0.25, -0.09375, -0.03125, ..., 0.0625, -0.0625, 0.0625],\n",
       "         [0.046875, 0.0625, 0.046875, ..., -0.0625, -0.09375, -0.09375],\n",
       "         [-0, 0.03125, 0.125, ..., -0.125, -0.0625, 0.0625]],\n",
       " \n",
       "        [[0.1875, 0.09375, 0.1875, ..., -0.0625, -0.03125, 0.0625],\n",
       "         [-0.25, -0.1875, 0, ..., 0.1875, -0.375, -0.125],\n",
       "         [-0.1875, -0.0625, 0.125, ..., 0.09375, -0.09375, -0.25],\n",
       "         ...,\n",
       "         [-0.25, 0.1875, 0.125, ..., -0.25, 0.0625, 0.0625],\n",
       "         [-0.0625, -0.09375, -0, ..., -0.125, -0.0625, -0.09375],\n",
       "         [-0.1875, 0.1875, 0.015625, ..., 0.0625, -0.125, -0.25]],\n",
       " \n",
       "        [[0.0625, -0.125, -0.0625, ..., -0.125, -0, 0.125],\n",
       "         [-0.0625, -0, -0.25, ..., 0, -0.1875, 0.0625],\n",
       "         [-0.0625, 0.1875, 0.1875, ..., -0, -0.09375, -0.125],\n",
       "         ...,\n",
       "         [0.25, 0.125, -0.09375, ..., 0.09375, 0.0625, 0.0625],\n",
       "         [0.1875, -0, 0.03125, ..., 0.03125, -0.0625, -0.03125],\n",
       "         [-0, 0.375, 0.0625, ..., 0, -0.0625, -0.09375]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.0625, -0.0625, -0.125, ..., -0.09375, -0.1875, 0.0625],\n",
       "         [-0.25, 0.125, -0.25, ..., 0.1875, 0.375, -0.25],\n",
       "         [0.0625, 0.125, 0.0625, ..., 0.125, -0.375, -0.125],\n",
       "         ...,\n",
       "         [0.09375, -0.125, 0.03125, ..., 0, 0.375, -0.0625],\n",
       "         [-0.015625, 0.0625, 0.03125, ..., 0.03125, 0.09375, -0.0625],\n",
       "         [0.03125, -0.0625, -0.125, ..., -0.0625, -0.0625, -0.0625]],\n",
       " \n",
       "        [[-0.03125, 0.03125, -0.0625, ..., 0.125, 0.1875, 0],\n",
       "         [-0.1875, 0.1875, 0.375, ..., 0.125, -0.375, -0.125],\n",
       "         [-0.375, -0.125, 0.0625, ..., 0.25, 0.0625, 0.0625],\n",
       "         ...,\n",
       "         [0.09375, -0.25, 0.25, ..., -0.25, -0, 0.0625],\n",
       "         [-0.1875, 0.09375, -0.0625, ..., -0, -0.125, 0.0625],\n",
       "         [0.03125, 0.03125, 0.03125, ..., 0.1875, 0.03125, 0.0625]],\n",
       " \n",
       "        [[0.0625, 0, 0.1875, ..., 0.5, 0.125, 0.125],\n",
       "         [-0.125, -0.125, -0.75, ..., -0.1875, -0.25, -0.375],\n",
       "         [-0.0625, 0.09375, -0.25, ..., 0.125, 0.25, 0.09375],\n",
       "         ...,\n",
       "         [-0.25, -0.0625, 0.0625, ..., 0.125, 0.0625, -0.0625],\n",
       "         [0.046875, -0.015625, 0.046875, ..., 0.125, -0.0625, -0.03125],\n",
       "         [-0, 0.0625, -0.1875, ..., -0.1875, -0.125, 0.0625]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.mlp.router.bias': Array([0.0273438, 0.0410156, -0.152344, -0.255859, -0.0913086, 0.0727539,\n",
       "        0.0644531, 0.0991211, -0.0134888, 0.0150146, -0.304688, -0.199219,\n",
       "        0.0771484, 0.0439453, 0.0581055, 0.0898438, -0.244141, -0.0507812,\n",
       "        0.029541, 0.0305176, -0.166992, -0.204102, 0.105957, 0.052002,\n",
       "        0.0869141, 0.0131836, -0.0751953, 0.162109, 0.0397949, 0.0483398,\n",
       "        0.155273, -0.353516], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.mlp.router.weight': Array([[0.0137939, 0.00221252, -0.00909424, ..., -0.00167084, 0.00500488,\n",
       "         -0.00167847],\n",
       "        [-0.0143433, 0.0106201, -0.0115356, ..., 0.0116577, -0.0146484,\n",
       "         -0.000249863],\n",
       "        [-0.0123901, -0.0133057, 0.00177765, ..., -0.0062561, -0.006073,\n",
       "         -0.00236511],\n",
       "        ...,\n",
       "        [-0.00331116, -0.00189972, -0.0098877, ..., -0.00521851,\n",
       "         0.00558472, 0.00836182],\n",
       "        [0.00939941, -0.00034523, 0.0196533, ..., -0.00683594, 0.00598145,\n",
       "         -0.00245667],\n",
       "        [-0.000827789, -0.00427246, -0.00302124, ..., -0.0030365,\n",
       "         0.0010376, 0.00640869]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.6.post_attention_layernorm.weight': Array([2.67188, 1.61719, 2.48438, ..., 2.23438, 3.15625, 2.9375],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.attn.o_proj.bias': Array([0.0581055, -0.00927734, 0.138672, ..., -0.453125, -0.0634766,\n",
       "        -0.0952148], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.attn.o_proj.weight': Array([[0.0385742, -0.0253906, 0.00622559, ..., 0.0327148, -0.00479126,\n",
       "         -0.0284424],\n",
       "        [0.00549316, -0.0111084, 0.0351562, ..., -0.0124512, -0.0805664,\n",
       "         -0.0174561],\n",
       "        [-0.0217285, -0.00445557, 0.0390625, ..., 0.0229492, -0.00151062,\n",
       "         -0.0539551],\n",
       "        ...,\n",
       "        [-0.0354004, -0.0147095, -0.0480957, ..., -0.0498047, -0.0124512,\n",
       "         0.0476074],\n",
       "        [0.02771, 0.0101929, 0.0395508, ..., -0.012146, -0.0146484,\n",
       "         -0.00753784],\n",
       "        [-0.0017395, -0.00817871, 0.0161133, ..., -0.0153809, -0.0263672,\n",
       "         -0.00653076]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.attn.qkv_proj.bias': Array([-0.5625, 0.00726318, 0.369141, ..., 0.0488281, -0.267578,\n",
       "        -0.216797], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.attn.qkv_proj.weight': Array([[-0.052002, -0.041748, -0.0145264, ..., 0.0251465, 0.0385742,\n",
       "         -0.0483398],\n",
       "        [-0.0111084, -0.010376, 0.0109253, ..., -0.00257874, -0.00376892,\n",
       "         -0.0108643],\n",
       "        [0.00234985, -0.0158691, 0.0125122, ..., -0.00094986, 0.000640869,\n",
       "         0.0131836],\n",
       "        ...,\n",
       "        [0.0703125, 0.0183105, -0.0142822, ..., -0.116211, -0.0219727,\n",
       "         0.0427246],\n",
       "        [-0.0336914, 0.0830078, 0.0209961, ..., -0.00247192, -0.0976562,\n",
       "         -0.0529785],\n",
       "        [-0.0109863, -0.0300293, -0.0272217, ..., -0.010498, 0.0859375,\n",
       "         0.0227051]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.attn.sinks': Array([4.34375, 5.21875, 4.03125, 4.1875, 5.21875, 3.71875, 4.3125,\n",
       "        2.89062, 1.17969, 0.960938, 2.01562, 2.6875, 3.1875, 1.79688,\n",
       "        2.28125, 2.65625, 1.35156, 1.85156, 3.5625, 2.98438, 2.3125,\n",
       "        2.76562, 1.8125, 2.39062, 3.48438, 4.8125, 1.71094, 5.3125,\n",
       "        3.17188, 3.875, 3.54688, 3.0625, 2.375, 4.40625, 4, 4.34375,\n",
       "        4.6875, 3.98438, 3.75, 3.67188, 2.10938, 1.09375, 2, 2.76562,\n",
       "        1.27344, 0.373047, 0.875, 2.01562, 4.75, 4.34375, 4.15625, 3.84375,\n",
       "        3.98438, 3.20312, 4.78125, 5.125, 1.85938, 4.09375, 1.90625,\n",
       "        0.433594, 1.33594, 1.64062, 2.95312, 1.72656], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.input_layernorm.weight': Array([1.83594, 1.44531, 1.60156, ..., 1.67188, 2.10938, 1.875], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.mlp.experts.w13_bias': Array([[-0.158203, -0.511719, -1.07812, ..., -0.351562, -0.0986328,\n",
       "         -0.789062],\n",
       "        [-0.146484, -1.15625, -0.910156, ..., -0.8125, -0.648438,\n",
       "         -0.746094],\n",
       "        [-1.23438, -1.07031, -0.714844, ..., -0.863281, -0.886719,\n",
       "         -0.558594],\n",
       "        ...,\n",
       "        [-0.6875, -0.242188, -0.542969, ..., -0.800781, -0.455078,\n",
       "         -0.863281],\n",
       "        [-0.145508, -0.345703, -0.753906, ..., -1.03906, -0.671875,\n",
       "         -0.535156],\n",
       "        [-0.0172119, -0.365234, -1.42969, ..., -0.617188, -1.03906,\n",
       "         -0.535156]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.mlp.experts.w13_weight': Array([[[-0, -0.00585938, -0.0234375, ..., 0.00195312, 0.00585938,\n",
       "          -0.015625],\n",
       "         [0, 0.015625, -0.015625, ..., 0.015625, 0.015625, -0.0078125],\n",
       "         [0.015625, 0.03125, 0.0234375, ..., -0.015625, 0.0078125,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [-0.0078125, 0.015625, 0.046875, ..., 0.03125, 0.015625,\n",
       "          -0.0234375],\n",
       "         [0.0234375, -0.03125, 0.03125, ..., 0.0078125, -0.0078125,\n",
       "          0.015625],\n",
       "         [-0.0078125, 0.03125, -0.046875, ..., 0.015625, -0.046875,\n",
       "          -0.046875]],\n",
       " \n",
       "        [[-0.00390625, 0, -0.00390625, ..., 0.00585938, 0.00390625,\n",
       "          0.00390625],\n",
       "         [-0.0117188, 0.0117188, 0.0078125, ..., -0, 0.0078125, -0],\n",
       "         [0.015625, -0.0234375, -0.0117188, ..., -0.0078125, 0.015625,\n",
       "          0.0234375],\n",
       "         ...,\n",
       "         [0.0078125, 0.015625, -0.03125, ..., 0.015625, 0.015625,\n",
       "          0.046875],\n",
       "         [0.046875, 0.0078125, -0.03125, ..., 0.0625, 0.046875, 0.046875],\n",
       "         [-0.0234375, -0.046875, -0, ..., 0.03125, -0.015625, -0.046875]],\n",
       " \n",
       "        [[-0.0234375, -0.015625, 0.0234375, ..., -0.03125, -0.046875,\n",
       "          -0.046875],\n",
       "         [-0.046875, -0.03125, -0.0117188, ..., -0.0078125, 0.0234375,\n",
       "          0.015625],\n",
       "         [-0.0078125, 0, 0.0234375, ..., 0.03125, -0.03125, -0],\n",
       "         ...,\n",
       "         [0.015625, 0.015625, 0.0078125, ..., 0.046875, -0.03125,\n",
       "          -0.0078125],\n",
       "         [0.0234375, 0.0078125, -0.0078125, ..., 0.0078125, 0.0234375,\n",
       "          -0.0078125],\n",
       "         [0.015625, 0.0078125, -0.0078125, ..., 0.0078125, 0.046875,\n",
       "          -0.03125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.015625, -0.00390625, 0.03125, ..., 0.0078125, -0.0078125,\n",
       "          -0.0078125],\n",
       "         [-0.0078125, 0.0117188, 0.0117188, ..., 0.015625, -0.0234375, 0],\n",
       "         [0.03125, 0.0117188, -0.0078125, ..., 0.00390625, -0.0117188,\n",
       "          0.00390625],\n",
       "         ...,\n",
       "         [0.015625, -0.03125, 0, ..., 0.0234375, 0.0078125, 0.0234375],\n",
       "         [-0.0078125, -0.015625, -0.0234375, ..., 0.015625, 0.046875,\n",
       "          -0.03125],\n",
       "         [-0.015625, 0.0078125, -0.0078125, ..., 0.015625, -0.0078125,\n",
       "          -0.03125]],\n",
       " \n",
       "        [[-0.0117188, -0.0078125, 0.03125, ..., 0.015625, 0.0234375,\n",
       "          -0.0078125],\n",
       "         [0.0078125, 0, 0.00390625, ..., 0.00390625, -0.00390625,\n",
       "          0.0078125],\n",
       "         [-0.0234375, 0.015625, -0.00390625, ..., -0.0078125, -0.015625,\n",
       "          0.015625],\n",
       "         ...,\n",
       "         [0.0234375, -0.046875, 0, ..., 0.0625, 0.0625, 0.03125],\n",
       "         [0.0234375, 0, 0.03125, ..., 0, -0.015625, -0.0078125],\n",
       "         [-0.0078125, 0.046875, -0.0078125, ..., 0.03125, -0.015625,\n",
       "          -0.015625]],\n",
       " \n",
       "        [[-0.00195312, 0.00390625, 0.00195312, ..., -0, -0.00390625,\n",
       "          -0.00195312],\n",
       "         [-0.0117188, -0.015625, -0.015625, ..., 0.015625, -0.046875,\n",
       "          -0.03125],\n",
       "         [0.0234375, 0, -0.0078125, ..., 0, 0, 0.0234375],\n",
       "         ...,\n",
       "         [0.015625, -0, -0.015625, ..., -0.015625, -0.03125, 0.046875],\n",
       "         [0.015625, -0.0234375, -0.0234375, ..., -0.015625, 0, 0.0234375],\n",
       "         [-0.0234375, 0.015625, -0.0078125, ..., -0.046875, 0, 0.03125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.mlp.experts.w2_bias': Array([[0.542969, -0.730469, -0.143555, ..., -0.330078, -0.057373,\n",
       "         -0.015564],\n",
       "        [0.142578, -0.765625, 0.320312, ..., -0.302734, -0.447266,\n",
       "         -0.200195],\n",
       "        [0.0605469, -0.515625, 0.136719, ..., 0.0119019, 0.310547,\n",
       "         -0.145508],\n",
       "        ...,\n",
       "        [-0.5625, -0.175781, 0.0334473, ..., 0.386719, -0.152344,\n",
       "         -0.490234],\n",
       "        [-0.11377, 0.390625, -0.117676, ..., 0.132812, 0.453125,\n",
       "         -0.527344],\n",
       "        [0.263672, -0.373047, -0.0588379, ..., -0.263672, 0.198242,\n",
       "         -0.457031]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.mlp.experts.w2_weight': Array([[[-0.125, 0.0625, 0.1875, ..., -0.25, 0.375, 0.0625],\n",
       "         [-0, -0.125, 0, ..., 0.0625, -0.25, 0.25],\n",
       "         [0.25, -0.125, 0, ..., 0.125, 0.125, -0.125],\n",
       "         ...,\n",
       "         [0.0625, 0.125, 0.1875, ..., 0.375, -0.125, 0.1875],\n",
       "         [-0, -0.0625, 0.125, ..., 0.09375, 0.1875, 0.0625],\n",
       "         [-0.125, 0.0625, -0.125, ..., 0.1875, 0.09375, 0.125]],\n",
       " \n",
       "        [[0.1875, 0.1875, 0.5, ..., 0.25, 0.5, -0.0625],\n",
       "         [-0.25, 0.1875, -0.0625, ..., -0.0625, 0.125, 0.125],\n",
       "         [0.25, -0.125, 0.0625, ..., -0.25, 0, 0.5],\n",
       "         ...,\n",
       "         [-0.25, 0.1875, -0.1875, ..., 0.25, -0.375, 0.125],\n",
       "         [-0.25, -0.03125, 0.125, ..., -0.125, 0.1875, 0.125],\n",
       "         [0.25, 0.03125, -0.0625, ..., -0.25, 0.375, -0.1875]],\n",
       " \n",
       "        [[-0.0625, 0.0625, -0.0625, ..., -0.125, -0.0625, 0.25],\n",
       "         [-0.5, 0.375, -0.125, ..., -0.25, 0.375, -0.25],\n",
       "         [-0.125, -0, 0.125, ..., 0.5, 0.1875, -0.1875],\n",
       "         ...,\n",
       "         [-0.5, 0.1875, -0, ..., -0.03125, 0.1875, -0.09375],\n",
       "         [-0.03125, -0.03125, -0.0625, ..., 0.125, 0, 0.1875],\n",
       "         [0.125, -0.03125, -0.1875, ..., -0, 0.03125, -0.1875]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.125, 0.1875, -0.375, ..., 0.125, 0.1875, -0],\n",
       "         [-0.1875, 0.0625, 0.25, ..., 0.1875, 0.0625, 0.125],\n",
       "         [0.375, -0.125, 0.1875, ..., -0.0625, 0.375, -0.0625],\n",
       "         ...,\n",
       "         [-0.1875, -0.25, 0.0625, ..., 0, 0.0625, -0.25],\n",
       "         [0.03125, -0.03125, 0.125, ..., -0.09375, -0, 0.09375],\n",
       "         [-0.125, -0.375, -0.1875, ..., 0.125, -0.125, -0]],\n",
       " \n",
       "        [[-0.0625, 0.375, 0.0625, ..., -0.5, 0.0625, 0.375],\n",
       "         [-0.25, -0, -0.25, ..., 0.25, -0.1875, 0.375],\n",
       "         [-0.5, -0.0625, 0.125, ..., 0.375, 0.125, 0.1875],\n",
       "         ...,\n",
       "         [0.0625, -0.125, -0, ..., -0.25, 0.125, 0.375],\n",
       "         [0, 0.125, -0, ..., 0.1875, 0, -0.09375],\n",
       "         [-0, -0.125, -0.09375, ..., -0.0625, 0.09375, -0.25]],\n",
       " \n",
       "        [[-0.375, -0.0625, 0.125, ..., -0.1875, 0.1875, 0.5],\n",
       "         [-0, 0.375, -0.375, ..., -0.125, -0.125, 0.125],\n",
       "         [-0, -0.125, -0, ..., 0.25, 0.25, 0.125],\n",
       "         ...,\n",
       "         [0.125, 0.0625, -0, ..., -0.0625, -0.375, -0],\n",
       "         [-0.0625, -0.03125, -0.0625, ..., -0.125, -0.0625, 0.125],\n",
       "         [0.03125, -0.25, 0.0625, ..., 0.03125, 0.09375, -0.03125]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.mlp.router.bias': Array([-0.19043, 0.0130005, 0.050293, 0.041748, -0.230469, 0.0488281,\n",
       "        0.11084, 0.104492, 0.0229492, 0.0854492, 0.0117188, 0.00622559,\n",
       "        -0.000476837, -0.0834961, -0.081543, 0.142578, -0.150391,\n",
       "        -0.188477, 0.0942383, -0.175781, 0.0766602, -0.197266, 0.0615234,\n",
       "        0.0717773, -0.249023, 0.0164795, 0.0373535, 0.101074, -0.257812,\n",
       "        0.09375, -0.0561523, 0.0541992], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.mlp.router.weight': Array([[-0.0057373, -0.000862122, -0.00830078, ..., 0.00221252,\n",
       "         0.00257874, -0.00460815],\n",
       "        [0.00570679, -0.000717163, -0.0112915, ..., 0.0062561, 0.00315857,\n",
       "         0.00291443],\n",
       "        [-0.00811768, 0.00531006, 0.00805664, ..., 0.00189972,\n",
       "         -0.00247192, -0.00164032],\n",
       "        ...,\n",
       "        [0.0163574, 0.00588989, 0.00491333, ..., -0.00309753, 0.00524902,\n",
       "         0.00619507],\n",
       "        [0.0137939, -0.000709534, -0.00185394, ..., -0.00354004,\n",
       "         0.00640869, -0.00138092],\n",
       "        [0.00848389, -0.00175476, -0.00448608, ..., 0.0055542,\n",
       "         -0.00744629, 0.00689697]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.7.post_attention_layernorm.weight': Array([2.5, 1.55469, 2.35938, ..., 2.10938, 2.9375, 2.79688], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.attn.o_proj.bias': Array([0.0612793, -0.375, -0.0578613, ..., -0.597656, 0.202148, -0.239258],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.attn.o_proj.weight': Array([[-0.0600586, -0.019165, -0.0167236, ..., -0.0222168, -0.00958252,\n",
       "         -0.0302734],\n",
       "        [0.150391, -0.0307617, -0.0693359, ..., 0.140625, 0.0131836,\n",
       "         0.0412598],\n",
       "        [0.000333786, -0.0134277, 0.0229492, ..., 0.0424805, 0.00162506,\n",
       "         0.00109863],\n",
       "        ...,\n",
       "        [0.0456543, -0.0336914, -0.0142212, ..., -0.0419922, 0.00500488,\n",
       "         0.0349121],\n",
       "        [0.0375977, 0.0649414, 0.015625, ..., 0.0668945, 0.0128784,\n",
       "         0.0458984],\n",
       "        [-0.0344238, -0.0444336, -0.00817871, ..., -0.0247803, 0.00921631,\n",
       "         -0.0187988]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.attn.qkv_proj.bias': Array([-0.671875, 0.875, 0.355469, ..., 0.357422, 0.279297, -0.122559],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.attn.qkv_proj.weight': Array([[-0.00830078, 0.0117798, 0.00460815, ..., 0.000249863,\n",
       "         -0.00512695, -0.0032959],\n",
       "        [0.00323486, -0.043457, -0.000301361, ..., 0.0137329, 0.0385742,\n",
       "         -0.0186768],\n",
       "        [-0.0143433, 0.0136719, 0.00393677, ..., -0.00328064, -0.00334167,\n",
       "         0.00494385],\n",
       "        ...,\n",
       "        [-0.0383301, 0.00527954, 0.0771484, ..., 0.0654297, 0.074707,\n",
       "         -0.0133057],\n",
       "        [-0.0390625, -0.0125732, 0.0761719, ..., -0.0712891, 0.0522461,\n",
       "         -0.0644531],\n",
       "        [-0.046875, -0.0195312, -0.00567627, ..., -0.0419922, -0.131836,\n",
       "         -0.0505371]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.attn.sinks': Array([1.89844, 1.44531, 2.23438, 1.23438, 2.1875, 1.45312, 2.5, 2.79688,\n",
       "        1.64844, 1.36719, 1.69531, 1.00781, 1.13281, 1.9375, 1.57812,\n",
       "        1.74219, 2.0625, 2.125, 1.8125, 2.25, 3.03125, 2.625, 1.61719,\n",
       "        2.9375, 0.1875, 3.51562, 2.34375, 1.66406, 2.09375, 1.25781,\n",
       "        0.726562, 0.671875, 2.3125, 1.15625, 1.97656, 1.92969, 1.92969,\n",
       "        1.44531, 2.46875, 2.15625, 2.625, 2.75, 2.625, 2.84375, 2.53125,\n",
       "        2.84375, 3, 2.20312, 3.35938, 1.9375, 2.03125, 2.8125, 3.07812,\n",
       "        2.125, 2.34375, 2.96875, 2.96875, 1.5, 2.64062, 2.6875, 2.6875,\n",
       "        3.46875, 2.95312, 1.46094], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.input_layernorm.weight': Array([1.85938, 1.45312, 1.77344, ..., 1.77344, 2.125, 2.0625], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.mlp.experts.w13_bias': Array([[-0.090332, -0.34375, -0.0664062, ..., -0.511719, -1.10156,\n",
       "         -0.484375],\n",
       "        [-0.166992, -0.267578, -0.052002, ..., -0.742188, -0.554688,\n",
       "         -0.613281],\n",
       "        [-0.0791016, 0.523438, -0.417969, ..., -0.429688, -0.683594,\n",
       "         -1.0625],\n",
       "        ...,\n",
       "        [-0.769531, -0.134766, -0.160156, ..., -1.09375, -0.847656,\n",
       "         -0.824219],\n",
       "        [0.00100708, -0.855469, -0.0341797, ..., -0.746094, -0.455078,\n",
       "         -0.59375],\n",
       "        [-0.0493164, -0.271484, 0.0480957, ..., -0.582031, -0.0556641,\n",
       "         -0.554688]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.mlp.experts.w13_weight': Array([[[0.0234375, -0.0117188, 0.0234375, ..., 0.015625, -0.015625,\n",
       "          0.0078125],\n",
       "         [0.00390625, 0.0078125, -0, ..., 0.00390625, 0.0117188,\n",
       "          -0.015625],\n",
       "         [0.00292969, 0.00585938, -0.000976562, ..., 0.00195312,\n",
       "          -0.00585938, 0.0078125],\n",
       "         ...,\n",
       "         [0.03125, 0.015625, 0.0234375, ..., -0.0234375, -0.046875,\n",
       "          0.03125],\n",
       "         [0.015625, -0.03125, -0.046875, ..., -0.015625, -0.015625, -0],\n",
       "         [0.046875, -0.03125, 0.0625, ..., -0.125, -0.0625, 0]],\n",
       " \n",
       "        [[-0.00390625, -0.00195312, 0.00195312, ..., -0.0078125,\n",
       "          -0.00195312, 0.0078125],\n",
       "         [-0.0078125, -0.0117188, 0.00390625, ..., -0.0117188,\n",
       "          0.00585938, 0.00585938],\n",
       "         [-0.00195312, -0.0117188, -0.0078125, ..., 0.00585938,\n",
       "          0.00390625, 0.0078125],\n",
       "         ...,\n",
       "         [0.0078125, -0.03125, 0.015625, ..., 0.0234375, -0.0078125,\n",
       "          -0.0117188],\n",
       "         [-0.00390625, -0.00390625, 0.0078125, ..., 0.0117188,\n",
       "          -0.0117188, 0],\n",
       "         [-0, -0.0117188, -0.00390625, ..., -0.0078125, -0.00390625,\n",
       "          0.0078125]],\n",
       " \n",
       "        [[0.0078125, -0, 0.015625, ..., 0.00585938, 0.00195312,\n",
       "          -0.00390625],\n",
       "         [-0.0078125, 0.00195312, -0.00195312, ..., 0.0078125,\n",
       "          -0.00390625, -0.00195312],\n",
       "         [-0.00195312, -0.00390625, 0.00585938, ..., -0.0117188,\n",
       "          0.015625, 0.0117188],\n",
       "         ...,\n",
       "         [0.03125, 0.015625, 0.015625, ..., 0.0625, 0, -0.0234375],\n",
       "         [-0.00390625, 0.00390625, -0.0078125, ..., -0.015625, 0.0078125,\n",
       "          -0.0234375],\n",
       "         [0, -0.0078125, 0.0117188, ..., -0.0234375, -0.0078125,\n",
       "          0.0078125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.0117188, -0.015625, 0.03125, ..., -0.0234375, -0.015625,\n",
       "          0.015625],\n",
       "         [-0.00390625, -0.0117188, 0.0078125, ..., 0, -0.00585938,\n",
       "          -0.0117188],\n",
       "         [-0.015625, -0.0117188, -0.0078125, ..., 0.0078125, -0.00390625,\n",
       "          0.00390625],\n",
       "         ...,\n",
       "         [0.0078125, 0.03125, -0.0234375, ..., -0.03125, -0.015625,\n",
       "          0.03125],\n",
       "         [0.0234375, 0.015625, 0.0234375, ..., 0.0078125, -0.015625,\n",
       "          -0.015625],\n",
       "         [0.0625, 0.0078125, -0.03125, ..., 0, 0, 0.03125]],\n",
       " \n",
       "        [[-0.000976562, 0.00292969, -0.0078125, ..., 0.0078125,\n",
       "          -0.00390625, -0.0117188],\n",
       "         [0.0078125, -0.015625, 0.03125, ..., -0.015625, 0.046875, 0],\n",
       "         [0.0078125, -0.0117188, 0.0078125, ..., 0.00390625, -0.00585938,\n",
       "          0],\n",
       "         ...,\n",
       "         [-0, -0.03125, -0.015625, ..., 0.015625, 0.0078125, 0.0078125],\n",
       "         [-0.015625, 0.0625, -0.0234375, ..., 0.03125, -0.03125,\n",
       "          0.0234375],\n",
       "         [-0, 0.015625, 0, ..., -0.015625, -0, 0.015625]],\n",
       " \n",
       "        [[0.0078125, 0.00390625, -0.00195312, ..., 0.0117188, 0.0117188,\n",
       "          0],\n",
       "         [0.0117188, 0.0117188, 0.015625, ..., -0.0234375, -0.015625,\n",
       "          0.015625],\n",
       "         [0.00585938, -0.000976562, 0.000976562, ..., -0, 0.000976562,\n",
       "          -0.00195312],\n",
       "         ...,\n",
       "         [0.03125, 0, 0.09375, ..., -0.09375, -0.03125, 0],\n",
       "         [-0, -0.09375, 0.03125, ..., -0.0625, 0.09375, 0.03125],\n",
       "         [0.015625, -0.09375, 0.125, ..., -0.09375, 0.046875, 0.0625]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.mlp.experts.w2_bias': Array([[-0.291016, -0.289062, -0.0216064, ..., -0.6875, 0.259766,\n",
       "         -0.146484],\n",
       "        [-0.361328, -0.578125, -1.03906, ..., -0.25, -0.988281, -0.404297],\n",
       "        [-0.22168, -1.23438, -1.28906, ..., 0.119629, -0.519531,\n",
       "         0.0126343],\n",
       "        ...,\n",
       "        [0.921875, 0.394531, 0.369141, ..., 0.0603027, 0.882812, -1.00781],\n",
       "        [0.306641, -0.165039, -0.146484, ..., 0.867188, 0.175781,\n",
       "         0.0112305],\n",
       "        [-0.753906, -1.01562, -0.644531, ..., -0.384766, -0.0273438,\n",
       "         0.135742]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.mlp.experts.w2_weight': Array([[[-0.25, 0.25, 0.125, ..., 0.5, -0, 0.5],\n",
       "         [0, 0.375, 0.125, ..., 0.375, 0.375, 0.5],\n",
       "         [-0.125, -0.125, 0.5, ..., 0.125, 0.125, 0.375],\n",
       "         ...,\n",
       "         [0.125, 0, 0.375, ..., 0.125, 0.125, -0.25],\n",
       "         [0.125, 0, 0.25, ..., -0.5, 0.1875, 0.125],\n",
       "         [-0.375, -0.25, 0.75, ..., -0, 0.125, 0.25]],\n",
       " \n",
       "        [[-0.1875, 0.03125, 0.03125, ..., 0, 0.125, -0],\n",
       "         [0.03125, 0.03125, -0.1875, ..., -0.09375, -0.09375, -0.1875],\n",
       "         [0.125, 0, 0.03125, ..., 0.03125, 0.09375, 0.09375],\n",
       "         ...,\n",
       "         [0.03125, 0.03125, -0.0625, ..., -0.015625, 0.0625, -0.0625],\n",
       "         [-0.0625, -0.125, -0.03125, ..., -0.015625, 0.03125, 0.0625],\n",
       "         [-0.125, -0.09375, 0.0625, ..., 0.03125, 0.125, 0]],\n",
       " \n",
       "        [[0.125, 0.0625, -0.25, ..., 0.75, -0, -0.0625],\n",
       "         [0.25, 0.25, 0.25, ..., 0.125, 0.25, 0.375],\n",
       "         [-0, -0.125, 0.125, ..., 0.1875, 0, 0.25],\n",
       "         ...,\n",
       "         [0.0625, 0.1875, 0.1875, ..., 0.75, 0.125, -0],\n",
       "         [0.0625, -0.1875, -0.0625, ..., -0.125, -0.03125, 0.125],\n",
       "         [-0.0625, 0.25, -0.125, ..., -0.0625, 0.125, -0.125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.0625, -0.75, 0.1875, ..., -0.125, 0.125, 0],\n",
       "         [-0.75, 0.25, -0.5, ..., -0.375, 0, -0.375],\n",
       "         [-0, -0.0625, -0.5, ..., 0.125, -0.125, -0.125],\n",
       "         ...,\n",
       "         [-0.125, -0.75, 0.125, ..., 0.125, 0.125, -0.125],\n",
       "         [-0.1875, -0.1875, -0.25, ..., 0.375, -0.5, -0.125],\n",
       "         [-0.0625, -0.1875, -0.375, ..., -0, -0.1875, 0.25]],\n",
       " \n",
       "        [[-0.25, -0.1875, 0.375, ..., 0.125, 0.5, -0.25],\n",
       "         [-0.375, -0.25, -0.25, ..., 0.125, 0.5, 0.375],\n",
       "         [-0.5, 0, -0, ..., 0.0625, 0.125, 0.0625],\n",
       "         ...,\n",
       "         [0.75, 0.25, 0.25, ..., -0.25, -0, 0.25],\n",
       "         [-0.25, -0.0625, 0.375, ..., 0.1875, 0.0625, 0.1875],\n",
       "         [-0.75, 0.125, 0.0625, ..., -0.125, -0, -0]],\n",
       " \n",
       "        [[-0.375, -0.125, 0.375, ..., -0.5, 0, 0.5],\n",
       "         [0, -0.25, 0, ..., 0, -0.75, -1],\n",
       "         [0.25, -0, 0.25, ..., 0.25, -0, -0.375],\n",
       "         ...,\n",
       "         [-0.125, -0.25, -0.5, ..., -1, -0.25, 0],\n",
       "         [-0, 0, 0.5, ..., -0.125, 0.5, -0.125],\n",
       "         [0, -0.125, -0.125, ..., 0.5, 0.5, 0.375]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.mlp.router.bias': Array([-0.196289, -0.0742188, -0.261719, -0.132812, 0.0220947, -0.0395508,\n",
       "        0.0913086, 0.00683594, 0.0825195, 0.0805664, -0.175781, 0.0639648,\n",
       "        0.0878906, -0.223633, -0.00112152, -0.0795898, 0.0859375,\n",
       "        0.0559082, 0.0576172, -0.0844727, -0.0634766, -0.101074, 0.0786133,\n",
       "        -0.0179443, -0.0349121, 0.0927734, 0.0620117, 0.0625, -0.244141,\n",
       "        0.145508, 0.166016, -0.154297], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.mlp.router.weight': Array([[-0.00167847, 0.00326538, 0.0102539, ..., 0.00296021,\n",
       "         -0.000904083, 0.00250244],\n",
       "        [-0.000471115, 0.000732422, -0.00915527, ..., -0.0114746,\n",
       "         0.000930786, -0.00228882],\n",
       "        [-0.0037384, 0.00110626, -0.00646973, ..., -0.00196838,\n",
       "         -0.00686646, 0.00193787],\n",
       "        ...,\n",
       "        [0.000545502, -0.00485229, 0.00494385, ..., 0.00349426, -0.006073,\n",
       "         -0.000679016],\n",
       "        [0.001297, 0.00340271, 0.0055542, ..., 0.00201416, 0.00582886,\n",
       "         -0.00311279],\n",
       "        [0.000267029, 0.00891113, 7.53403e-05, ..., -0.00154877,\n",
       "         0.00318909, -0.00112915]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.8.post_attention_layernorm.weight': Array([2.28125, 1.46094, 2.25, ..., 1.99219, 2.75, 2.65625], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.attn.o_proj.bias': Array([-0.1875, 0.0732422, 0.211914, ..., -0.878906, 0.129883, 0.365234],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.attn.o_proj.weight': Array([[0.00872803, -0.0878906, -0.0532227, ..., 0.0231934, 0.119141,\n",
       "         0.0273438],\n",
       "        [-0.0286865, 0.0878906, -0.15918, ..., 0.00424194, 0.109863,\n",
       "         0.119629],\n",
       "        [0.130859, -0.0142212, -0.136719, ..., 0.0168457, -0.0117188,\n",
       "         -0.000827789],\n",
       "        ...,\n",
       "        [-0.0830078, -0.022583, 0.0125732, ..., -0.0135498, -0.0192871,\n",
       "         0.0849609],\n",
       "        [-0.0341797, 0.0311279, -0.020752, ..., -0.010376, 0.0245361,\n",
       "         0.0180664],\n",
       "        [-0.132812, 0.0179443, 0.0800781, ..., 0.0742188, -0.0174561,\n",
       "         0.000522614]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.attn.qkv_proj.bias': Array([-0.421875, 0.287109, 0.519531, ..., 0.170898, 0.00469971, 0.032959],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.attn.qkv_proj.weight': Array([[0.0241699, 0.0098877, -0.00592041, ..., -0.0161133, -0.0422363,\n",
       "         -0.0229492],\n",
       "        [0.0202637, -0.0275879, -0.0275879, ..., -0.0202637, 0.00494385,\n",
       "         0.045166],\n",
       "        [0.0126343, 0.000770569, 0.00421143, ..., 0.027832, -0.0708008,\n",
       "         0.0522461],\n",
       "        ...,\n",
       "        [-0.0119629, -0.00714111, -0.00170135, ..., 0.017334, -0.0373535,\n",
       "         0.0400391],\n",
       "        [-0.0218506, 0.0140991, -0.0358887, ..., 0.0256348, 0.0449219,\n",
       "         -0.0395508],\n",
       "        [0.0253906, -0.0132446, -0.0476074, ..., 0.032959, -0.0322266,\n",
       "         -0.0256348]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.attn.sinks': Array([2.6875, 4.28125, 4.875, 3.625, 4.1875, 3.48438, 5.09375, 3.8125,\n",
       "        2.1875, 4.40625, 4.15625, 3.98438, 6.875, 4.25, 3.6875, 3.40625,\n",
       "        2.53125, 3.25, 3.54688, 3.57812, 3.67188, 3.3125, 3.67188, 3.125,\n",
       "        5, 5.125, 5.65625, 4.4375, 4.90625, 5.6875, 5.28125, 5.34375,\n",
       "        3.96875, 3.57812, 6.5625, 4.375, 5.09375, 5.28125, 3.65625,\n",
       "        4.53125, 3, 3.375, 5.03125, 4.03125, 2.21875, 2.57812, 4.46875,\n",
       "        3.1875, 3.39062, 6.84375, 5.25, 3.29688, 4.1875, 1.74219, 2.5625,\n",
       "        3.8125, 1.14062, 2.73438, 3.78125, 3.82812, 3.51562, 3.90625,\n",
       "        3.6875, 3.5625], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.input_layernorm.weight': Array([1.79688, 1.5, 1.69531, ..., 1.6875, 2.01562, 1.9375], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.mlp.experts.w13_bias': Array([[-0.151367, 0.192383, -0.941406, ..., -1.17188, -0.546875, -0.875],\n",
       "        [-0.738281, -0.550781, -1.03906, ..., -0.921875, -0.742188,\n",
       "         -0.945312],\n",
       "        [-0.116699, -0.166016, -0.0115967, ..., -0.632812, -1.27344,\n",
       "         -1.41406],\n",
       "        ...,\n",
       "        [-0.124512, -0.165039, 0.117188, ..., -0.5625, -0.96875, -1.14062],\n",
       "        [-0.0229492, -0.129883, -0.128906, ..., 0.367188, -0.96875,\n",
       "         -0.664062],\n",
       "        [-0.65625, -0.609375, -0.0106201, ..., -0.609375, -0.796875,\n",
       "         -0.679688]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.mlp.experts.w13_weight': Array([[[0, -0.0078125, -0.00390625, ..., -0.00390625, 0.00390625,\n",
       "          -0.00195312],\n",
       "         [-0.000976562, 0.00292969, 0.00292969, ..., 0.00585938,\n",
       "          0.00292969, -0.00292969],\n",
       "         [0.0078125, -0.015625, 0.0117188, ..., -0.0234375, -0.00390625,\n",
       "          -0.015625],\n",
       "         ...,\n",
       "         [0.015625, -0.0234375, -0.0078125, ..., -0.046875, -0, 0.03125],\n",
       "         [-0.03125, -0.0078125, -0, ..., -0.0078125, -0.046875,\n",
       "          -0.015625],\n",
       "         [-0.046875, 0.046875, 0.0234375, ..., 0.03125, -0.03125,\n",
       "          -0.015625]],\n",
       " \n",
       "        [[0.0078125, -0.015625, -0.015625, ..., -0.046875, -0.046875,\n",
       "          -0.0117188],\n",
       "         [-0.00390625, 0.0117188, -0.00390625, ..., -0.015625,\n",
       "          0.00585938, 0.0078125],\n",
       "         [-0.0234375, -0.00390625, -0.015625, ..., 0.015625, -0,\n",
       "          0.0078125],\n",
       "         ...,\n",
       "         [-0.046875, -0.0234375, 0.03125, ..., -0, -0.015625, 0.0234375],\n",
       "         [0.015625, 0.015625, -0.015625, ..., -0.0625, -0.03125,\n",
       "          -0.046875],\n",
       "         [0.015625, -0.0078125, -0.015625, ..., 0, 0.046875, 0.015625]],\n",
       " \n",
       "        [[-0.00195312, 0.015625, 0, ..., 0.015625, 0.00195312, 0.0078125],\n",
       "         [-0.0078125, 0.0078125, 0.00195312, ..., -0.00585938,\n",
       "          -0.00390625, -0.00585938],\n",
       "         [-0.0078125, 0.0078125, 0.0078125, ..., 0.0078125, 0.00390625,\n",
       "          -0.0078125],\n",
       "         ...,\n",
       "         [-0, -0.0078125, -0.046875, ..., -0, -0.0234375, 0.0625],\n",
       "         [0.0625, 0, -0.015625, ..., -0.0078125, 0.015625, -0.015625],\n",
       "         [-0.0234375, 0.0078125, -0.00390625, ..., -0.0078125, 0.0234375,\n",
       "          -0.015625]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.00585938, -0.00585938, 0.0117188, ..., 0.0078125,\n",
       "          0.00390625, -0.00195312],\n",
       "         [0.0117188, 0, -0.00390625, ..., 0.00195312, -0.00390625,\n",
       "          0.00585938],\n",
       "         [-0.00585938, 0.00585938, 0.00195312, ..., -0.00195312,\n",
       "          0.00195312, -0.0117188],\n",
       "         ...,\n",
       "         [0.03125, -0.015625, -0, ..., -0.0625, 0.015625, -0],\n",
       "         [0.0078125, -0.046875, 0, ..., -0.046875, 0, 0.046875],\n",
       "         [-0.0078125, 0.0234375, -0.0078125, ..., 0.03125, 0.015625,\n",
       "          -0.015625]],\n",
       " \n",
       "        [[-0.00390625, 0.00292969, 0.00195312, ..., 0.00292969,\n",
       "          0.00146484, -0.00292969],\n",
       "         [0.0078125, 0.000976562, 0.00390625, ..., -0.0078125,\n",
       "          0.00390625, 0.00390625],\n",
       "         [0.00390625, 0.00585938, 0.00390625, ..., 0, -0.0078125, 0],\n",
       "         ...,\n",
       "         [-0.046875, 0.09375, 0.0234375, ..., -0.0625, 0.046875, -0],\n",
       "         [0.0234375, 0.015625, -0.0078125, ..., -0.09375, 0, -0.03125],\n",
       "         [0, -0.015625, -0.03125, ..., 0.046875, -0.0078125, -0.0234375]],\n",
       " \n",
       "        [[-0, -0.0078125, 0.03125, ..., 0.00390625, -0.0078125, -0],\n",
       "         [-0.03125, -0.015625, 0.0117188, ..., -0.015625, 0.00390625,\n",
       "          -0.03125],\n",
       "         [-0.0117188, -0.00585938, 0.00390625, ..., 0.015625,\n",
       "          -0.00585938, 0.015625],\n",
       "         ...,\n",
       "         [-0.0078125, -0.015625, 0.0117188, ..., 0.015625, 0.0078125, -0],\n",
       "         [-0.015625, -0.0117188, -0.0078125, ..., -0.015625, -0.0078125,\n",
       "          -0.0078125],\n",
       "         [0.0078125, 0.0078125, -0.00390625, ..., -0.015625, -0,\n",
       "          -0.03125]]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.mlp.experts.w2_bias': Array([[0.373047, -1.77344, -0.0512695, ..., -0.703125, 0.78125,\n",
       "         -0.949219],\n",
       "        [0.8125, -1.47656, -0.275391, ..., -0.11377, 0.742188, -0.139648],\n",
       "        [1.75, -0.703125, 0.1875, ..., -0.386719, 1.79688, -0.84375],\n",
       "        ...,\n",
       "        [0.494141, -0.170898, -1.03125, ..., -0.314453, 0.419922,\n",
       "         -0.0422363],\n",
       "        [-1.95312, -0.773438, -0.0568848, ..., -0.757812, -0.660156,\n",
       "         -0.00421143],\n",
       "        [0.353516, -0.0556641, 0.351562, ..., -1, -0.0727539, 0.410156]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.mlp.experts.w2_weight': Array([[[0.75, -0.25, 0, ..., -0.375, -0.25, -0.75],\n",
       "         [-0.125, -0.25, 0.375, ..., -0.125, -0.5, 0.375],\n",
       "         [-0.5, 0.375, -0.5, ..., 0, 0.375, 0.5],\n",
       "         ...,\n",
       "         [0.25, -0, -0.5, ..., -0.25, -0.25, 0.5],\n",
       "         [0.25, -0.25, 0.375, ..., -0.125, -0.75, -0.75],\n",
       "         [-0.1875, -0.25, -0.75, ..., -0, 0.5, -0.75]],\n",
       " \n",
       "        [[0.5, 0.5, -0.5, ..., -0.0625, 0.5, 0.1875],\n",
       "         [0.25, -0, 0.5, ..., 0.5, -0, -0.75],\n",
       "         [-0.1875, -0.375, 0.1875, ..., 0.375, 0.25, -0.5],\n",
       "         ...,\n",
       "         [-0, -0.75, 0, ..., 0.375, 0, -0.25],\n",
       "         [-0.125, -0.5, -0.25, ..., -0.375, -0.25, 0.125],\n",
       "         [0.1875, 0.5, -0.25, ..., -0.5, -0, -0.25]],\n",
       " \n",
       "        [[-0.375, -0.125, 0.375, ..., 0.125, -0, -0],\n",
       "         [0.125, 0.25, 0.125, ..., 0.125, 0.375, 0.125],\n",
       "         [-0, -0.25, -0.125, ..., -1, 0.375, 0.125],\n",
       "         ...,\n",
       "         [0.25, -0, -0.5, ..., -0.75, -0.25, -0],\n",
       "         [-0.125, -0.25, -0.125, ..., 0.25, -0.25, -0.375],\n",
       "         [0.25, -0.5, -0.5, ..., 0.5, 0.125, -0.25]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.5, -0.125, 0.25, ..., 0.5, 0.25, 0.375],\n",
       "         [-0, 0.5, -0.5, ..., 0.5, -0.5, -0.25],\n",
       "         [-1, -0.75, -0.125, ..., 0.375, 0.125, -0.25],\n",
       "         ...,\n",
       "         [-0.5, 0.375, 0.5, ..., -0.5, 0.25, 0.75],\n",
       "         [-0.5, 0.75, 0, ..., -0.25, -0.125, -0.125],\n",
       "         [-0, -0.375, 0.5, ..., 0.5, -0, 0.125]],\n",
       " \n",
       "        [[0.75, -0.5, -0.5, ..., 0.25, -0.25, -0.25],\n",
       "         [0.25, -0.25, 0.5, ..., 1.5, -0, -1],\n",
       "         [0.125, 0.375, -0.25, ..., -0.125, -0.125, 0.75],\n",
       "         ...,\n",
       "         [0, -0.5, 0.5, ..., -2, -0.5, -0.5],\n",
       "         [-0.5, -0, -0.375, ..., 0.75, 1, 0.25],\n",
       "         [-0.75, 0.125, 0.375, ..., -0.5, 0.5, 0.25]],\n",
       " \n",
       "        [[-0.75, 0.375, 0, ..., 0.25, 0.25, 0.125],\n",
       "         [-0.5, 0.125, -0, ..., -0.25, -0.125, 0.125],\n",
       "         [0.125, -0, 0, ..., -0.25, -0.375, -0.5],\n",
       "         ...,\n",
       "         [-0.375, 0.5, 0.125, ..., -0.5, -0.125, 0],\n",
       "         [0.125, 0.25, 0.375, ..., 0.25, -0, 0.1875],\n",
       "         [-0.125, -0.375, -0.125, ..., -0.125, -0.125, 0]]],      dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.mlp.router.bias': Array([0.0498047, 0.0825195, 0.0314941, -0.261719, -0.0244141, -0.0859375,\n",
       "        -0.0307617, 0.0241699, 0.0230713, -0.00341797, -0.0200195,\n",
       "        0.00753784, 0.0422363, -0.188477, 0.0109253, 0.0791016, -0.0454102,\n",
       "        -0.0634766, 0.0429688, -0.0532227, 0.057373, -0.126953, -0.186523,\n",
       "        0.0471191, 0.0878906, 0.0727539, 0.105469, 0.0446777, -0.0698242,\n",
       "        -0.0303955, -0.065918, 0.0698242], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.mlp.router.weight': Array([[-0.00457764, 0.0065918, 0.00311279, ..., 0.000793457, 0.00341797,\n",
       "         0.00595093],\n",
       "        [4.45843e-05, 6.19888e-05, 0.00823975, ..., -0.00457764,\n",
       "         0.00260925, -0.00378418],\n",
       "        [-0.000106335, 0.00163269, -0.000583649, ..., 0.00296021,\n",
       "         -0.00219727, 0.00102997],\n",
       "        ...,\n",
       "        [0.00860596, 0.00274658, 0.00189972, ..., -0.00350952,\n",
       "         -0.00720215, -0.00157928],\n",
       "        [-0.00265503, 0.00674438, -0.00421143, ..., 0.00312805,\n",
       "         0.00389099, -0.00192261],\n",
       "        [-0.00204468, 0.0148926, -0.00389099, ..., 0.00331116, 0.00247192,\n",
       "         -0.00854492]], dtype=bfloat16),\n",
       " 'vllm_model.model.layers.9.post_attention_layernorm.weight': Array([2.01562, 1.34375, 2.1875, ..., 1.83594, 2.5625, 2.40625], dtype=bfloat16),\n",
       " 'vllm_model.model.norm.weight': Array([10.8125, 10.25, 9.75, ..., 8.9375, 9.3125, 10.875], dtype=bfloat16)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_state = golden_state\n",
    "flat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c019ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm_model.lm_head.weight | (201088, 2880)\n",
      "vllm_model.model.embedding.weight | (201088, 2880)\n",
      "vllm_model.model.layers.0.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.0.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.0.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.0.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.0.attn.rotary_emb.cos_sin_cache | (131072, 64)\n",
      "vllm_model.model.layers.0.attn.sinks | (64,)\n",
      "vllm_model.model.layers.0.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.0.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.0.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.0.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.0.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.0.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.0.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.0.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.1.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.1.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.1.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.1.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.1.attn.sinks | (64,)\n",
      "vllm_model.model.layers.1.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.1.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.1.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.1.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.1.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.1.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.1.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.1.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.10.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.10.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.10.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.10.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.10.attn.sinks | (64,)\n",
      "vllm_model.model.layers.10.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.10.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.10.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.10.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.10.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.10.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.10.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.10.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.11.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.11.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.11.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.11.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.11.attn.sinks | (64,)\n",
      "vllm_model.model.layers.11.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.11.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.11.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.11.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.11.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.11.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.11.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.11.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.12.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.12.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.12.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.12.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.12.attn.sinks | (64,)\n",
      "vllm_model.model.layers.12.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.12.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.12.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.12.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.12.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.12.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.12.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.12.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.13.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.13.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.13.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.13.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.13.attn.sinks | (64,)\n",
      "vllm_model.model.layers.13.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.13.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.13.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.13.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.13.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.13.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.13.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.13.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.14.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.14.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.14.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.14.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.14.attn.sinks | (64,)\n",
      "vllm_model.model.layers.14.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.14.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.14.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.14.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.14.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.14.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.14.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.14.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.15.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.15.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.15.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.15.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.15.attn.sinks | (64,)\n",
      "vllm_model.model.layers.15.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.15.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.15.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.15.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.15.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.15.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.15.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.15.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.16.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.16.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.16.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.16.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.16.attn.sinks | (64,)\n",
      "vllm_model.model.layers.16.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.16.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.16.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.16.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.16.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.16.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.16.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.16.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.17.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.17.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.17.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.17.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.17.attn.sinks | (64,)\n",
      "vllm_model.model.layers.17.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.17.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.17.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.17.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.17.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.17.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.17.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.17.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.18.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.18.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.18.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.18.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.18.attn.sinks | (64,)\n",
      "vllm_model.model.layers.18.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.18.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.18.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.18.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.18.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.18.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.18.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.18.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.19.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.19.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.19.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.19.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.19.attn.sinks | (64,)\n",
      "vllm_model.model.layers.19.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.19.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.19.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.19.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.19.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.19.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.19.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.19.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.2.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.2.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.2.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.2.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.2.attn.sinks | (64,)\n",
      "vllm_model.model.layers.2.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.2.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.2.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.2.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.2.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.2.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.2.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.2.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.20.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.20.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.20.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.20.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.20.attn.sinks | (64,)\n",
      "vllm_model.model.layers.20.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.20.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.20.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.20.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.20.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.20.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.20.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.20.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.21.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.21.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.21.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.21.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.21.attn.sinks | (64,)\n",
      "vllm_model.model.layers.21.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.21.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.21.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.21.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.21.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.21.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.21.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.21.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.22.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.22.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.22.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.22.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.22.attn.sinks | (64,)\n",
      "vllm_model.model.layers.22.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.22.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.22.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.22.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.22.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.22.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.22.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.22.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.23.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.23.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.23.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.23.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.23.attn.sinks | (64,)\n",
      "vllm_model.model.layers.23.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.23.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.23.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.23.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.23.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.23.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.23.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.23.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.3.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.3.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.3.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.3.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.3.attn.sinks | (64,)\n",
      "vllm_model.model.layers.3.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.3.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.3.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.3.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.3.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.3.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.3.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.3.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.4.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.4.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.4.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.4.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.4.attn.sinks | (64,)\n",
      "vllm_model.model.layers.4.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.4.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.4.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.4.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.4.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.4.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.4.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.4.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.5.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.5.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.5.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.5.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.5.attn.sinks | (64,)\n",
      "vllm_model.model.layers.5.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.5.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.5.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.5.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.5.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.5.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.5.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.5.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.6.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.6.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.6.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.6.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.6.attn.sinks | (64,)\n",
      "vllm_model.model.layers.6.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.6.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.6.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.6.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.6.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.6.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.6.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.6.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.7.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.7.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.7.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.7.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.7.attn.sinks | (64,)\n",
      "vllm_model.model.layers.7.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.7.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.7.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.7.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.7.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.7.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.7.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.7.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.8.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.8.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.8.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.8.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.8.attn.sinks | (64,)\n",
      "vllm_model.model.layers.8.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.8.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.8.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.8.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.8.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.8.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.8.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.8.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.9.attn.o_proj.bias | (2880,)\n",
      "vllm_model.model.layers.9.attn.o_proj.weight | (2880, 4096)\n",
      "vllm_model.model.layers.9.attn.qkv_proj.bias | (5120,)\n",
      "vllm_model.model.layers.9.attn.qkv_proj.weight | (5120, 2880)\n",
      "vllm_model.model.layers.9.attn.sinks | (64,)\n",
      "vllm_model.model.layers.9.input_layernorm.weight | (2880,)\n",
      "vllm_model.model.layers.9.mlp.experts.w13_bias | (32, 5760)\n",
      "vllm_model.model.layers.9.mlp.experts.w13_weight | (32, 5760, 2880)\n",
      "vllm_model.model.layers.9.mlp.experts.w2_bias | (32, 2880)\n",
      "vllm_model.model.layers.9.mlp.experts.w2_weight | (32, 2880, 2880)\n",
      "vllm_model.model.layers.9.mlp.router.bias | (32,)\n",
      "vllm_model.model.layers.9.mlp.router.weight | (32, 2880)\n",
      "vllm_model.model.layers.9.post_attention_layernorm.weight | (2880,)\n",
      "vllm_model.model.norm.weight | (2880,)\n",
      "type: <class 'jaxlib._jax.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "# j = flat_state[flat_state.keys()[0]]\n",
    "# print(\"type:\",  type(j), type(j.value))\n",
    "\n",
    "for i, j in flat_state.items():\n",
    "  print(i, \"|\", j.shape)\n",
    "  #print(\".\".join(str(k) for k in i))\n",
    "  #print(\"\\tshape:\", j.shape)\n",
    "  #print(\"\\tsharding:\", j.sharding)\n",
    "\n",
    "print(\"type:\",  type(j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "315fba15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n",
      "vllm_model.lm_head.weight\n",
      "vllm_model.model.embedding.weight\n",
      "vllm_model.model.layers.0.attn.o_proj.bias\n",
      "vllm_model.model.layers.0.attn.o_proj.weight\n",
      "vllm_model.model.layers.0.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.0.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.0.attn.rotary_emb.cos_sin_cache\n",
      "vllm_model.model.layers.0.attn.sinks\n",
      "vllm_model.model.layers.0.input_layernorm.weight\n",
      "vllm_model.model.layers.0.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.0.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.0.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.0.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.0.mlp.router.bias\n",
      "vllm_model.model.layers.0.mlp.router.weight\n",
      "vllm_model.model.layers.0.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.1.attn.o_proj.bias\n",
      "vllm_model.model.layers.1.attn.o_proj.weight\n",
      "vllm_model.model.layers.1.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.1.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.1.attn.sinks\n",
      "vllm_model.model.layers.1.input_layernorm.weight\n",
      "vllm_model.model.layers.1.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.1.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.1.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.1.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.1.mlp.router.bias\n",
      "vllm_model.model.layers.1.mlp.router.weight\n",
      "vllm_model.model.layers.1.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.10.attn.o_proj.bias\n",
      "vllm_model.model.layers.10.attn.o_proj.weight\n",
      "vllm_model.model.layers.10.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.10.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.10.attn.sinks\n",
      "vllm_model.model.layers.10.input_layernorm.weight\n",
      "vllm_model.model.layers.10.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.10.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.10.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.10.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.10.mlp.router.bias\n",
      "vllm_model.model.layers.10.mlp.router.weight\n",
      "vllm_model.model.layers.10.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.11.attn.o_proj.bias\n",
      "vllm_model.model.layers.11.attn.o_proj.weight\n",
      "vllm_model.model.layers.11.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.11.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.11.attn.sinks\n",
      "vllm_model.model.layers.11.input_layernorm.weight\n",
      "vllm_model.model.layers.11.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.11.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.11.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.11.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.11.mlp.router.bias\n",
      "vllm_model.model.layers.11.mlp.router.weight\n",
      "vllm_model.model.layers.11.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.12.attn.o_proj.bias\n",
      "vllm_model.model.layers.12.attn.o_proj.weight\n",
      "vllm_model.model.layers.12.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.12.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.12.attn.sinks\n",
      "vllm_model.model.layers.12.input_layernorm.weight\n",
      "vllm_model.model.layers.12.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.12.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.12.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.12.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.12.mlp.router.bias\n",
      "vllm_model.model.layers.12.mlp.router.weight\n",
      "vllm_model.model.layers.12.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.13.attn.o_proj.bias\n",
      "vllm_model.model.layers.13.attn.o_proj.weight\n",
      "vllm_model.model.layers.13.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.13.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.13.attn.sinks\n",
      "vllm_model.model.layers.13.input_layernorm.weight\n",
      "vllm_model.model.layers.13.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.13.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.13.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.13.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.13.mlp.router.bias\n",
      "vllm_model.model.layers.13.mlp.router.weight\n",
      "vllm_model.model.layers.13.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.14.attn.o_proj.bias\n",
      "vllm_model.model.layers.14.attn.o_proj.weight\n",
      "vllm_model.model.layers.14.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.14.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.14.attn.sinks\n",
      "vllm_model.model.layers.14.input_layernorm.weight\n",
      "vllm_model.model.layers.14.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.14.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.14.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.14.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.14.mlp.router.bias\n",
      "vllm_model.model.layers.14.mlp.router.weight\n",
      "vllm_model.model.layers.14.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.15.attn.o_proj.bias\n",
      "vllm_model.model.layers.15.attn.o_proj.weight\n",
      "vllm_model.model.layers.15.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.15.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.15.attn.sinks\n",
      "vllm_model.model.layers.15.input_layernorm.weight\n",
      "vllm_model.model.layers.15.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.15.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.15.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.15.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.15.mlp.router.bias\n",
      "vllm_model.model.layers.15.mlp.router.weight\n",
      "vllm_model.model.layers.15.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.16.attn.o_proj.bias\n",
      "vllm_model.model.layers.16.attn.o_proj.weight\n",
      "vllm_model.model.layers.16.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.16.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.16.attn.sinks\n",
      "vllm_model.model.layers.16.input_layernorm.weight\n",
      "vllm_model.model.layers.16.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.16.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.16.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.16.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.16.mlp.router.bias\n",
      "vllm_model.model.layers.16.mlp.router.weight\n",
      "vllm_model.model.layers.16.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.17.attn.o_proj.bias\n",
      "vllm_model.model.layers.17.attn.o_proj.weight\n",
      "vllm_model.model.layers.17.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.17.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.17.attn.sinks\n",
      "vllm_model.model.layers.17.input_layernorm.weight\n",
      "vllm_model.model.layers.17.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.17.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.17.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.17.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.17.mlp.router.bias\n",
      "vllm_model.model.layers.17.mlp.router.weight\n",
      "vllm_model.model.layers.17.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.18.attn.o_proj.bias\n",
      "vllm_model.model.layers.18.attn.o_proj.weight\n",
      "vllm_model.model.layers.18.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.18.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.18.attn.sinks\n",
      "vllm_model.model.layers.18.input_layernorm.weight\n",
      "vllm_model.model.layers.18.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.18.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.18.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.18.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.18.mlp.router.bias\n",
      "vllm_model.model.layers.18.mlp.router.weight\n",
      "vllm_model.model.layers.18.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.19.attn.o_proj.bias\n",
      "vllm_model.model.layers.19.attn.o_proj.weight\n",
      "vllm_model.model.layers.19.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.19.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.19.attn.sinks\n",
      "vllm_model.model.layers.19.input_layernorm.weight\n",
      "vllm_model.model.layers.19.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.19.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.19.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.19.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.19.mlp.router.bias\n",
      "vllm_model.model.layers.19.mlp.router.weight\n",
      "vllm_model.model.layers.19.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.2.attn.o_proj.bias\n",
      "vllm_model.model.layers.2.attn.o_proj.weight\n",
      "vllm_model.model.layers.2.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.2.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.2.attn.sinks\n",
      "vllm_model.model.layers.2.input_layernorm.weight\n",
      "vllm_model.model.layers.2.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.2.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.2.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.2.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.2.mlp.router.bias\n",
      "vllm_model.model.layers.2.mlp.router.weight\n",
      "vllm_model.model.layers.2.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.20.attn.o_proj.bias\n",
      "vllm_model.model.layers.20.attn.o_proj.weight\n",
      "vllm_model.model.layers.20.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.20.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.20.attn.sinks\n",
      "vllm_model.model.layers.20.input_layernorm.weight\n",
      "vllm_model.model.layers.20.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.20.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.20.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.20.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.20.mlp.router.bias\n",
      "vllm_model.model.layers.20.mlp.router.weight\n",
      "vllm_model.model.layers.20.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.21.attn.o_proj.bias\n",
      "vllm_model.model.layers.21.attn.o_proj.weight\n",
      "vllm_model.model.layers.21.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.21.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.21.attn.sinks\n",
      "vllm_model.model.layers.21.input_layernorm.weight\n",
      "vllm_model.model.layers.21.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.21.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.21.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.21.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.21.mlp.router.bias\n",
      "vllm_model.model.layers.21.mlp.router.weight\n",
      "vllm_model.model.layers.21.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.22.attn.o_proj.bias\n",
      "vllm_model.model.layers.22.attn.o_proj.weight\n",
      "vllm_model.model.layers.22.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.22.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.22.attn.sinks\n",
      "vllm_model.model.layers.22.input_layernorm.weight\n",
      "vllm_model.model.layers.22.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.22.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.22.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.22.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.22.mlp.router.bias\n",
      "vllm_model.model.layers.22.mlp.router.weight\n",
      "vllm_model.model.layers.22.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.23.attn.o_proj.bias\n",
      "vllm_model.model.layers.23.attn.o_proj.weight\n",
      "vllm_model.model.layers.23.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.23.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.23.attn.sinks\n",
      "vllm_model.model.layers.23.input_layernorm.weight\n",
      "vllm_model.model.layers.23.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.23.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.23.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.23.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.23.mlp.router.bias\n",
      "vllm_model.model.layers.23.mlp.router.weight\n",
      "vllm_model.model.layers.23.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.3.attn.o_proj.bias\n",
      "vllm_model.model.layers.3.attn.o_proj.weight\n",
      "vllm_model.model.layers.3.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.3.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.3.attn.sinks\n",
      "vllm_model.model.layers.3.input_layernorm.weight\n",
      "vllm_model.model.layers.3.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.3.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.3.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.3.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.3.mlp.router.bias\n",
      "vllm_model.model.layers.3.mlp.router.weight\n",
      "vllm_model.model.layers.3.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.4.attn.o_proj.bias\n",
      "vllm_model.model.layers.4.attn.o_proj.weight\n",
      "vllm_model.model.layers.4.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.4.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.4.attn.sinks\n",
      "vllm_model.model.layers.4.input_layernorm.weight\n",
      "vllm_model.model.layers.4.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.4.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.4.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.4.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.4.mlp.router.bias\n",
      "vllm_model.model.layers.4.mlp.router.weight\n",
      "vllm_model.model.layers.4.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.5.attn.o_proj.bias\n",
      "vllm_model.model.layers.5.attn.o_proj.weight\n",
      "vllm_model.model.layers.5.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.5.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.5.attn.sinks\n",
      "vllm_model.model.layers.5.input_layernorm.weight\n",
      "vllm_model.model.layers.5.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.5.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.5.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.5.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.5.mlp.router.bias\n",
      "vllm_model.model.layers.5.mlp.router.weight\n",
      "vllm_model.model.layers.5.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.6.attn.o_proj.bias\n",
      "vllm_model.model.layers.6.attn.o_proj.weight\n",
      "vllm_model.model.layers.6.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.6.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.6.attn.sinks\n",
      "vllm_model.model.layers.6.input_layernorm.weight\n",
      "vllm_model.model.layers.6.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.6.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.6.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.6.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.6.mlp.router.bias\n",
      "vllm_model.model.layers.6.mlp.router.weight\n",
      "vllm_model.model.layers.6.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.7.attn.o_proj.bias\n",
      "vllm_model.model.layers.7.attn.o_proj.weight\n",
      "vllm_model.model.layers.7.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.7.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.7.attn.sinks\n",
      "vllm_model.model.layers.7.input_layernorm.weight\n",
      "vllm_model.model.layers.7.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.7.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.7.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.7.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.7.mlp.router.bias\n",
      "vllm_model.model.layers.7.mlp.router.weight\n",
      "vllm_model.model.layers.7.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.8.attn.o_proj.bias\n",
      "vllm_model.model.layers.8.attn.o_proj.weight\n",
      "vllm_model.model.layers.8.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.8.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.8.attn.sinks\n",
      "vllm_model.model.layers.8.input_layernorm.weight\n",
      "vllm_model.model.layers.8.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.8.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.8.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.8.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.8.mlp.router.bias\n",
      "vllm_model.model.layers.8.mlp.router.weight\n",
      "vllm_model.model.layers.8.post_attention_layernorm.weight\n",
      "vllm_model.model.layers.9.attn.o_proj.bias\n",
      "vllm_model.model.layers.9.attn.o_proj.weight\n",
      "vllm_model.model.layers.9.attn.qkv_proj.bias\n",
      "vllm_model.model.layers.9.attn.qkv_proj.weight\n",
      "vllm_model.model.layers.9.attn.sinks\n",
      "vllm_model.model.layers.9.input_layernorm.weight\n",
      "vllm_model.model.layers.9.mlp.experts.w13_bias\n",
      "vllm_model.model.layers.9.mlp.experts.w13_weight\n",
      "vllm_model.model.layers.9.mlp.experts.w2_bias\n",
      "vllm_model.model.layers.9.mlp.experts.w2_weight\n",
      "vllm_model.model.layers.9.mlp.router.bias\n",
      "vllm_model.model.layers.9.mlp.router.weight\n",
      "vllm_model.model.layers.9.post_attention_layernorm.weight\n",
      "vllm_model.model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "print(len(flat_state))\n",
    "# j = flat_state[flat_state.keys()[0]]\n",
    "# print(\"type:\",  type(j), type(j.value))\n",
    "\n",
    "for i, j in flat_state.items():\n",
    "  print(i)\n",
    "  #print(\".\".join(str(k) for k in i))\n",
    "  # print(\"\\tshape:\", j.shape)\n",
    "  # print(\"\\tsharding:\", j.sharding)\n",
    "\n",
    "#print(\"type:\",  type(j))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a351dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl",
   "language": "python",
   "name": "venv-rl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
