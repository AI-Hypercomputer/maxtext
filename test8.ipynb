{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7912b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuningjin_google_com/venv-rl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-14 20:59:08.131992: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-14 20:59:08.146452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763153948.161303  724997 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763153948.166289  724997 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763153948.180558  724997 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763153948.180572  724997 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763153948.180575  724997 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763153948.180576  724997 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-14 20:59:08.185694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:11 [__init__.py:26] TPU info: node_name=shuningjin-tpu-v3 | tpu_type=v5p-8 | worker_id=0 | num_chips=4 | num_cores_per_chip=2\n",
      "INFO 11-14 20:59:11 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 11-14 20:59:11 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 11-14 20:59:12 [interface.py:171] Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import jax.numpy as jnp\n",
    "from typing import Dict, Any\n",
    "\n",
    "import os\n",
    "os.chdir('/home/shuningjin_google_com/maxtext')\n",
    "\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"\n",
    "os.environ[\"JAX_RANDOM_WEIGHTS\"] = \"False\"\n",
    "os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n",
    "#os.environ[\"HF_TOKEN\"] = \"\"\n",
    "os.environ[\"TPU_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"TPU_STDERR_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"VLLM_MLA_DISABLE\"] = \"1\"\n",
    "os.environ[\"MODEL_IMPL_TYPE\"] = \"vllm\"\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"tpu\"\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from vllm import LLM\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e3a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:21 [utils.py:253] non-default args: {'max_model_len': 64, 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'unsloth/gpt-oss-20b-BF16'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-14 20:59:21 [model.py:437] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.\n",
      "INFO 11-14 20:59:21 [model.py:630] Resolved architecture: GptOssForCausalLM\n",
      "INFO 11-14 20:59:21 [model.py:1728] Using max model len 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 20:59:22,254\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:22 [scheduler.py:226] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-14 20:59:22 [config.py:273] Overriding max cuda graph capture size to 992 for performance.\n",
      "INFO 11-14 20:59:22 [tpu_platform.py:118] Initialized sharding configuration: ShardingConfigManager(total_devices=4, sharding_strategy=ShardingStrategy(tensor_parallelism=4, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
      "WARNING 11-14 20:59:22 [tpu_platform.py:154] The model dtype is not properly set for JAX backend. Overwriting it to jnp.bfloat16\n",
      "INFO 11-14 20:59:22 [tpu_platform.py:187] Force using UniProcExecutor for JAX on single host.\n",
      "INFO 11-14 20:59:23 [core.py:93] Initializing a V1 LLM engine (v0.11.1rc7.dev83+g64d57c3be) with config: model='unsloth/gpt-oss-20b-BF16', speculative_config=None, tokenizer='unsloth/gpt-oss-20b-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=64, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=None, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/gpt-oss-20b-BF16, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': None, 'mode': 2, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'openxla', 'custom_ops': ['all'], 'splitting_ops': None, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 992, 'local_cache_dir': None}\n",
      "WARNING 11-14 20:59:24 [tpu_platform.py:223] Pin memory is not supported on TPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 20:59:24.177373  724997 b295d63588a.cc:762] Linux version 5.19.0-1027-gcp (buildd@lcy02-amd64-078) (x86_64-linux-gnu-gcc-12 (Ubuntu 12.1.0-2ubuntu1~22.04) 12.1.0, GNU ld (GNU Binutils for Ubuntu) 2.38) #29~22.04.1-Ubuntu SMP Thu Jun 22 05:13:17 UTC 2023\n",
      "I1114 20:59:24.185461  724997 b295d63588a.cc:844] Process id 724997\n",
      "I1114 20:59:24.185481  724997 b295d63588a.cc:849] Current working directory /home/shuningjin_google_com/maxtext\n",
      "I1114 20:59:24.185483  724997 b295d63588a.cc:851] Current timezone is UTC (currently UTC +00:00)\n",
      "I1114 20:59:24.185488  724997 b295d63588a.cc:855] Built on Sep 11 2025 15:57:19 (1757631439)\n",
      "I1114 20:59:24.185490  724997 b295d63588a.cc:856]  at rbex-enqueue-targets@lgje25.prod.google.com:/google/src/cloud/buildrabbit-username/buildrabbit-client/g3     \n",
      "I1114 20:59:24.185491  724997 b295d63588a.cc:857]  as //learning/45eac/tfrc/executor:_libtpu.so.native\n",
      "I1114 20:59:24.185493  724997 b295d63588a.cc:858]  for gcc-4.X.Y-crosstool-v18-llvm-grtev4-k8.k8\n",
      "I1114 20:59:24.185499  724997 b295d63588a.cc:861]  from changelist 804429027 with baseline 804273004 in a mint client based on __ar56t/branches/libtpu_lts_release_branch/804273004.1/g3     \n",
      "I1114 20:59:24.185501  724997 b295d63588a.cc:865] Build label: libtpu_lts_20250908_a_RC01\n",
      "I1114 20:59:24.185503  724997 b295d63588a.cc:867] Build tool: Bazel, release r4rca-2025.08.25-1 (mainline @798895491)\n",
      "I1114 20:59:24.185504  724997 b295d63588a.cc:868] Build target: \n",
      "I1114 20:59:24.185510  724997 b295d63588a.cc:875] Command line arguments:\n",
      "I1114 20:59:24.185511  724997 b295d63588a.cc:877] argv[0]: './tpu_driver'\n",
      "I1114 20:59:24.185515  724997 b295d63588a.cc:877] argv[1]: '--minloglevel=0'\n",
      "I1114 20:59:24.185517  724997 b295d63588a.cc:877] argv[2]: '--stderrthreshold=0'\n",
      "I1114 20:59:24.185518  724997 b295d63588a.cc:877] argv[3]: '--v=0'\n",
      "I1114 20:59:24.185520  724997 b295d63588a.cc:877] argv[4]: '--vmodule='\n",
      "I1114 20:59:24.185521  724997 b295d63588a.cc:877] argv[5]: '--log_dir=/tmp/tpu_logs'\n",
      "I1114 20:59:24.185523  724997 b295d63588a.cc:877] argv[6]: '--max_log_size=1024'\n",
      "I1114 20:59:24.185525  724997 b295d63588a.cc:877] argv[7]: '--enforce_kernel_ipv6_support=0'\n",
      "I1114 20:59:24.185526  724997 b295d63588a.cc:877] argv[8]: '--next_pluggable_device_use_c_api=0'\n",
      "I1114 20:59:24.185528  724997 b295d63588a.cc:877] argv[9]: '--2a886c8_wrap=false,false,false'\n",
      "I1114 20:59:24.185530  724997 b295d63588a.cc:877] argv[10]: '--2a886c8_twist=false'\n",
      "I1114 20:59:24.185532  724997 b295d63588a.cc:877] argv[11]: '--2a886c8_chip_config_name=megachip_tccontrol'\n",
      "I1114 20:59:24.185533  724997 b295d63588a.cc:877] argv[12]: '--2a886c8_chips_per_host_bounds=2,2,1'\n",
      "I1114 20:59:24.185535  724997 b295d63588a.cc:877] argv[13]: '--2a886c8_host_bounds=1,1,1'\n",
      "I1114 20:59:24.185537  724997 b295d63588a.cc:877] argv[14]: '--2a886c8_slice_builder_worker_port=8471'\n",
      "I1114 20:59:24.185539  724997 b295d63588a.cc:877] argv[15]: '--2a886c8_slice_builder_worker_addresses=10.164.0.41:8471'\n",
      "I1114 20:59:24.185540  724997 b295d63588a.cc:877] argv[16]: '--tpu_slice_builder_dump_chip=true'\n",
      "I1114 20:59:24.185542  724997 b295d63588a.cc:877] argv[17]: '--tpu_slice_builder_dump_chip_force=false'\n",
      "I1114 20:59:24.185544  724997 b295d63588a.cc:877] argv[18]: '--tpu_slice_builder_dump_to_localhost=1'\n",
      "I1114 20:59:24.185545  724997 b295d63588a.cc:877] argv[19]: '--bypass_vbar_control_service=0'\n",
      "I1114 20:59:24.185547  724997 b295d63588a.cc:877] argv[20]: '--2a886c8_ici_resilient=false'\n",
      "I1114 20:59:24.185549  724997 b295d63588a.cc:877] argv[21]: '--xla_tpu_use_resilient_collective_emitter=false'\n",
      "I1114 20:59:24.185550  724997 b295d63588a.cc:877] argv[22]: '--tpu_slice_builder_topology_discovery_fault_injection='\n",
      "I1114 20:59:24.185552  724997 b295d63588a.cc:877] argv[23]: '--runtime_metric_service_port=8431'\n",
      "I1114 20:59:24.185554  724997 b295d63588a.cc:877] argv[24]: '--tpu_hbm_report_enable=1'\n",
      "I1114 20:59:24.185555  724997 b295d63588a.cc:877] argv[25]: '--tpu_hbm_report_frequency=5s'\n",
      "I1114 20:59:24.185557  724997 b295d63588a.cc:877] argv[26]: '--enable_runtime_uptime_telemetry=true'\n",
      "I1114 20:59:24.185559  724997 b295d63588a.cc:877] argv[27]: ''\n",
      "I1114 20:59:24.185560  724997 b295d63588a.cc:877] argv[28]: '--xla_tpu_use_enhanced_launch_barrier=true'\n",
      "I1114 20:59:24.185812  724997 init.cc:78] Remote crash gathering hook installed.\n",
      "I1114 20:59:24.185847  724997 tpu_runtime_type_flags.cc:79] --tpu_use_tfrt not specified. Using default value: true\n",
      "I1114 20:59:24.203621  724997 tpu_hal.cc:429] Registered plugin from module: breakpoint_debugger_server\n",
      "I1114 20:59:24.203905  724997 log_message_host_command_handler.cc:70] Registering LogMessageHostCommandHandler\n",
      "I1114 20:59:24.203914  724997 host_command_handler_factory.cc:31] Skipping registration of host command handler for opcode Log because it is not in the allowlist.\n",
      "I1114 20:59:24.203936  724997 tpu_hal.cc:429] Registered plugin from module: megascale_sync_flag_logger\n",
      "I1114 20:59:24.205645  724997 tpu_hal.cc:429] Registered plugin from module: RuntimeMetricHelper\n",
      "I1114 20:59:24.205729  724997 tf_tpu_flags.cc:63] 2a886c8Platform is NOT registered.\n",
      "I1114 20:59:24.205920  724997 logger.cc:310] Enabling threaded logging for severity WARNING\n",
      "I1114 20:59:24.205944  724997 tpu_hal.cc:429] Registered plugin from module: sdc_checker_callback\n",
      "I1114 20:59:24.206004  724997 tpu_hal.cc:429] Registered plugin from module: xsc_explicit_checksum_tracing_callback\n",
      "I1114 20:59:24.206095  724997 mlock.cc:219] mlock()-ed 4096 bytes for BuildID, using 1 syscalls.\n",
      "I1114 20:59:24.227576  724997 config.cc:256] gRPC experiments enabled: error_flatten, event_engine_callback_cq, event_engine_client, event_engine_dns, event_engine_dns_non_client_channel, event_engine_listener, google_no_envelope_resolver, monitoring_experiment, tsi_frame_protector_without_locks\n",
      "I1114 20:59:24.236143  724997 init-domain.cc:126] Fiber init: default domain = futex, concurrency = 228, prefix = futex-default\n",
      "I1114 20:59:24.236361  724997 stackdriver_metric_reporter.cc:69] Starting StackdriverMetricReporter fiber loop with options stackdriver_project_name_or_number = \"\", prepare_client_context = 32-byte object <C0-C9 D8-C5 33-56 00-00 00-C9 D8-C5 33-56 00-00 80-5C 5E-97 9F-7F 00-00 40-6A FD-98 9F-7F 00-00>, reporting_interval = 1m, use_borg_stub = false, project_resource_labels = [\"project_id\"], create_time_series = 0, clock = 0x5633c85e7b70\n",
      "I1114 20:59:24.244805  725180 cachednslookup.cc:391] TTL not found in response, not caching response\n",
      "I1114 20:59:24.262439  724997 debug_manager.cc:220] Registering error handler with name: libtpu_telemetry_handler\n",
      "W1114 20:59:24.262819  724997 uptime_telemetry.cc:187] UptimeMetric attributes are updated.\n",
      "Previous Attributes: go/debugstr  \n",
      "key: \"uptime_attributes\"\n",
      "value {\n",
      "  kvlist_attr {\n",
      "    attributes {\n",
      "      key: \"ml_framework_name\"\n",
      "      value {\n",
      "        string_attr: \"tensorflow\"\n",
      "      }\n",
      "    }\n",
      "    attributes {\n",
      "      key: \"ml_framework_version\"\n",
      "      value {\n",
      "        string_attr: \"tensorflow-2.19.1\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "New Attributes: go/debugstr  \n",
      "key: \"uptime_attributes\"\n",
      "value {\n",
      "  kvlist_attr {\n",
      "    attributes {\n",
      "      key: \"ml_framework_name\"\n",
      "      value {\n",
      "        string_attr: \"pytorch/xla2\"\n",
      "      }\n",
      "    }\n",
      "    attributes {\n",
      "      key: \"ml_framework_version\"\n",
      "      value {\n",
      "        string_attr: \"pytorch/xla2-v0.0.1\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "I1114 20:59:24.272899  724997 singleton_tpu_states_manager.cc:73] TPU premapped buffer enabled. Size: 4294967296 Threshold: 4294967296\n",
      "I1114 20:59:24.272912  724997 singleton_tpu_states_manager.cc:96] TpuStatesManager::GetOrCreate(): no tpu system exists. Creating a new tpu system.\n",
      "I1114 20:59:24.274118  724997 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1114 20:59:24.274127  724997 tpu_version_flag.cc:54] Using auto-detected TPU version TPU v5\n",
      "I1114 20:59:24.274966  724997 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1114 20:59:24.275806  724997 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1114 20:59:24.276616  724997 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1114 20:59:24.276974  724997 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1114 20:59:24.276978  724997 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1114 20:59:24.276980  724997 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1114 20:59:24.276981  724997 pending_event_logger.cc:928] Enabling PjRt/TPU event dependency logging\n",
      "I1114 20:59:24.280736  725448 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1114 20:59:24.280753  725448 flags_util.cc:318] Using 8471 from --2a886c8_slice_builder_worker_port as SliceBuilder worker service port.\n",
      "I1114 20:59:24.281593  725448 device_util.cc:145] Found 4 TPU v5 chips.\n",
      "I1114 20:59:24.281600  725448 tpu_network_factory.cc:67] tpunetd either not supported or disabled, falling back to Slice Builder\n",
      "I1114 20:59:24.291371  725450 futex.cc:68] RAW: Futex::Swap(): using FUTEX_WAKE + FUTEX_WAIT\n",
      "I1114 20:59:25.317014  725453 async_driver.cc:454] [/dev/vfio/2 tpu575:pe0:2] vf_id:0 Driver opened.\n",
      "I1114 20:59:25.322970  725451 async_driver.cc:454] [/dev/vfio/0 tpu575:pe0:0] vf_id:0 Driver opened.\n",
      "I1114 20:59:25.346376  725452 async_driver.cc:454] [/dev/vfio/1 tpu575:pe0:1] vf_id:0 Driver opened.\n",
      "I1114 20:59:25.411308  725454 async_driver.cc:454] [/dev/vfio/3 tpu575:pe0:3] vf_id:0 Driver opened.\n",
      "W1114 20:59:25.434001  725452 async_driver.cc:1736] All cores not supported.\n",
      "W1114 20:59:25.439973  725453 async_driver.cc:1736] All cores not supported.\n",
      "W1114 20:59:25.440028  725451 async_driver.cc:1736] All cores not supported.\n",
      "W1114 20:59:25.444343  725454 async_driver.cc:1736] All cores not supported.\n",
      "I1114 20:59:25.444380  725448 slice_builder_helper.cc:99] Current host is used as SliceBuilder master.\n",
      "I1114 20:59:25.445365  725448 hostname.cc:43] Note: we could not read a GMI proto at '/etc/googlemachineidentity/live/machine_identity.pb'. If this is a prod machine, it is probably broken. If it is a non-prod machine (corp, cloudtop etc), this is ok.\n",
      "I1114 20:59:25.450937  725448 legacy_topology_discoverer.cc:55] Target Topology: (2, 2, 1)\n",
      "I1114 20:59:26.485528  725448 master.cc:219] Successfully initialized SliceBuilder master session 9b02e0f1e59860e4 with expected topology (2, 2, 1)\n",
      "I1114 20:59:26.486869  725448 tpu_hal.cc:200] Starting premapped memory manager initialization...\n",
      "W1114 20:59:26.487875  725783 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.487886  725782 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.487882  725780 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.487901  725782 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1114 20:59:26.487899  725783 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1114 20:59:26.487909  725780 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1114 20:59:26.488290  725781 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.488304  725781 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1114 20:59:26.488335  725783 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.488339  725782 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.488343  725783 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1114 20:59:26.488344  725782 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n",
      "W1114 20:59:26.488349  725780 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.execution.timing.distribution.microseconds\n",
      "W1114 20:59:26.488354  725780 libtpu_telemetry_utils.cc:44] Metric already exists: hlo.queue.size.gauge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 11-14 20:59:31 [parallel_state.py:1325] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 20:59:31.260321  725448 runtime_metric_service.cc:159] Successfully started Runtime Metric Service on port: 8431\n",
      "I1114 20:59:31.260445  725448 system.cc:1091] tpu::System initialized, current host id: 0, logical device ids: 0,1,2,3\n",
      "I1114 20:59:31.260475  724997 tpu_system_state.cc:217] CreateTpuSystemState: TPU initialization is successful and it took 6.980946575s\n",
      "I1114 20:59:31.260504  724997 tpu_system_state.cc:221] CreateTpuSystemState: using TPU host premapped buffer of size: 4294967296\n",
      "I1114 20:59:31.260511  724997 tpu_host_allocator.cc:64] Premapped buffer is using alignment 64\n",
      "I1114 20:59:31.260961  724997 allocator_stats_reporter.cc:117] Starting AllocatorStats Reporter with reporting interval: 5s\n",
      "I1114 20:59:31.391071  724997 autofdo_agent.cc:198] xla_tpu_autofdo_profile_dir updated to \n",
      "W1114 20:59:31.391090  724997 autofdo_agent.cc:201] xla_tpu_autofdo_use_remote_repo is overridden to false because xla_tpu_autofdo_profile_dir is not set.\n",
      "I1114 20:59:31.394644  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1114 20:59:31.394658  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1114 20:59:31.394892  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.78431075ms\n",
      "I1114 20:59:31.453246  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:31.458754  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.809765ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:31 [tpu_runner.py:273] Init mesh | mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto))\n",
      "INFO 11-14 20:59:31 [utils.py:93] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
      "INFO 11-14 20:59:31 [utils.py:59] Prepared request paddings: [8, 16, 32, 64, 128, 256]\n",
      "INFO 11-14 20:59:31 [compilation_manager.py:34] Enabling JAX compile cache.\n",
      "INFO 11-14 20:59:31 [tpu_worker.py:151] Init worker | rank=0 | node_id=0 | is_driver_worker=True | hbm=[(0.0, 95.74), (0.0, 95.74), (0.0, 95.74), (0.0, 95.74)]GiB\n",
      "INFO 11-14 20:59:31 [model_loader.py:318] Loading model with MODEL_IMPL_TYPE=vllm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 20:59:31.468316  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.49386175ms\n",
      "I1114 20:59:31.468420  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:13d169e8f0cb52132827f3458861645b2ad7cf1a3afcb1127a6444697d9ec7eb\n",
      "I1114 20:59:31.468423  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):7274c5d1d7ca793892a3a02db907049e506cf3ed81768774f478da0a45758e78\n",
      "I1114 20:59:31.468425  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:31.468519  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 79.482155ms\n",
      "I1114 20:59:31.493583  724997 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 8 instructions, modules: jit__threefry_seed\n",
      "I1114 20:59:31.493599  724997 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__threefry_seed instructions: 8 fingerprint: \n",
      "I1114 20:59:31.493749  724997 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__threefry_seed instructions: 8\n",
      "I1114 20:59:31.493753  724997 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1114 20:59:31.496138  724997 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1114 20:59:31.496665  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663111577 bytes.\n",
      "I1114 20:59:31.496674  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883155578 bytes.\n",
      "I1114 20:59:31.496937  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.4936695ms\n",
      "I1114 20:59:31.497293  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:31.502746  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.5424595ms\n",
      "I1114 20:59:31.503459  725449 2a886c8_compiler_base.cc:3045] final program bundle count: 205 note this count does not reflect cycles spent executing delays.\n",
      "I1114 20:59:31.507986  725449 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1114 20:59:31.509044  725449 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (50.0K).\n",
      "I1114 20:59:31.509266  725449 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__threefry_seed\n",
      "I1114 20:59:31.509277  725449 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 50.0K / 95.74G\n",
      "I1114 20:59:31.509281  725449 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 4.0K / 64.00M\n",
      "I1114 20:59:31.509297  725449 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.05M:\n",
      "I1114 20:59:31.509299  725449 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1114 20:59:31.509301  725449 2a886c8_compiler_base.cc:3549]     program           50.0K \n",
      "I1114 20:59:31.509302  725449 2a886c8_compiler_base.cc:3549]     arguments          512B \n",
      "I1114 20:59:31.509303  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:31.509304  725449 2a886c8_compiler_base.cc:3549] Output size 512B; shares 0B with arguments.\n",
      "I1114 20:59:31.509306  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:31.509310  725449 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1114 20:59:31.509311  725449 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1114 20:59:31.509313  725449 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1114 20:59:31.509314  725449 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1114 20:59:31.509315  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:31.509316  725449 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1114 20:59:31.509317  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:31.509325  725449 2a886c8_compiler_base.cc:3553] Program sflag requirement 212B:\n",
      "I1114 20:59:31.509326  725449 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1114 20:59:31.509327  725449 2a886c8_compiler_base.cc:3553]     scoped               8B\n",
      "I1114 20:59:31.509328  725449 2a886c8_compiler_base.cc:3553] Program vmem requirement 4.0K:\n",
      "I1114 20:59:31.509329  725449 2a886c8_compiler_base.cc:3553]     scoped             4.0K\n",
      "I1114 20:59:31.509331  725449 2a886c8_compiler_base.cc:3553] Program smem requirement 32B:\n",
      "I1114 20:59:31.509332  725449 2a886c8_compiler_base.cc:3553]     scoped              32B\n",
      "I1114 20:59:31.509342  725449 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1114 20:59:31.509343  725449 2a886c8_compiler_base.cc:3553] Program hbm requirement 50.0K:\n",
      "I1114 20:59:31.509344  725449 2a886c8_compiler_base.cc:3553]     overlays          50.0K\n",
      "I1114 20:59:31.509345  725449 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 1.9K / 1.00M (1 parameters)\n",
      "I1114 20:59:31.509364  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.58134925ms\n",
      "I1114 20:59:31.509467  725449 isa_program_util_common.cc:510] (HLO module jit__threefry_seed): Executable fingerprint:4e5a39cda394a06cc48348ff5409ccb2e1723f45f094264066ae3490e8cf3524\n",
      "I1114 20:59:31.509470  725449 isa_program_util_common.cc:514] (HLO module jit__threefry_seed): Executable fingerprint (including data segments):dc57a40245541f0780852f287f4f5dd66c906d0a6bbfcaaaf69cdd4b47aa3406\n",
      "I1114 20:59:31.509472  725449 isa_program_util_common.cc:517] (HLO module jit__threefry_seed): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:31.509564  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.18097675ms\n",
      "I1114 20:59:31.528208  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1114 20:59:31.528224  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1114 20:59:31.528356  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1446335ms\n",
      "I1114 20:59:31.528627  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:31.532968  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.39274875ms\n",
      "I1114 20:59:31.539124  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.1171815ms\n",
      "I1114 20:59:31.539241  725449 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:0dd955e7fff6234828b16b7e584746f4bc8487e7ee70794e4ad701aa63814261\n",
      "I1114 20:59:31.539250  725449 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):d2832ea7e08c57ce23d402eac10e86acfc43c0db821a6ac8f1f93b090700eb9d\n",
      "I1114 20:59:31.539252  725449 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:31.539335  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.15492275ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:33 [tpu_platform.py:63] Cannot use None backend on TPU.\n",
      "INFO 11-14 20:59:33 [tpu_platform.py:66] Using Pallas V1 backend.\n",
      "INFO 11-14 20:59:33 [layer.py:1227] Disabling MoE shared_experts cuda stream\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:05,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:04,  1.71it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:03,  1.46it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:03<00:02,  1.45it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:04<00:02,  1.40it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:04<00:01,  1.67it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:05<00:00,  1.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.52it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:05<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:39 [default_loader.py:314] Loading weights took 6.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "I1114 20:59:39.852841  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 95539878809 bytes.\n",
      "I1114 20:59:39.852885  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4776993940 bytes.\n",
      "I1114 20:59:39.853071  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.45694ms\n",
      "I1114 20:59:39.853465  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:39.909829  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 56.46607475ms\n",
      "I1114 20:59:39.917756  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.85128775ms\n",
      "I1114 20:59:39.917861  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:b3ba6325a053ecbb6f9addbc4449554c9cf58e2ff7a22dbb2dca310c38b21420\n",
      "I1114 20:59:39.917865  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):24f89d50687e0e6689ba857e5672f04955a820bf9d7ba9210b477b0ee4185088\n",
      "I1114 20:59:39.917867  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:39.917956  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 69.41623625ms\n",
      "I1114 20:59:39.995052  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577969049 bytes.\n",
      "I1114 20:59:39.995082  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828898452 bytes.\n",
      "I1114 20:59:39.995286  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.632217ms\n",
      "I1114 20:59:39.995705  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.037224  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 41.61707975ms\n",
      "I1114 20:59:40.045203  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.9396145ms\n",
      "I1114 20:59:40.045319  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:10ec1d0e39903987b226b392e3ae0a6b8326e701ac61b21ab79063ec858f5ac4\n",
      "I1114 20:59:40.045323  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):16e939837fc9dcac1cac38cf27f8d8b496076876605f0b309082e6a19a4f423e\n",
      "I1114 20:59:40.045325  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.045411  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 54.81950725ms\n",
      "I1114 20:59:40.075128  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662507929 bytes.\n",
      "I1114 20:59:40.075144  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883125396 bytes.\n",
      "I1114 20:59:40.075298  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.236778ms\n",
      "I1114 20:59:40.075591  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.083070  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 7.54004ms\n",
      "I1114 20:59:40.089627  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.52699ms\n",
      "I1114 20:59:40.089716  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:661b06f4f697007005242bb8e0c92a014b6271a707633661de22b6b79703c9e9\n",
      "I1114 20:59:40.089720  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):f1c228e24b6e0981d545569cf87e2530d5e5bf65819bb00da0d09d1393c633c7\n",
      "I1114 20:59:40.089722  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.089797  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.77244625ms\n",
      "I1114 20:59:40.105798  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662868377 bytes.\n",
      "I1114 20:59:40.105814  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143418 bytes.\n",
      "I1114 20:59:40.105927  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.0697955ms\n",
      "I1114 20:59:40.106215  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.111597  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.435429ms\n",
      "I1114 20:59:40.117948  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.32077425ms\n",
      "I1114 20:59:40.118035  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:9be621fdff47b4326661798b415522ffb365d75067437e9bfa2eebcf11bfe103\n",
      "I1114 20:59:40.118039  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):ed0f8738fdcede926890b9ab0a3b476a8f6aeb3aad2c6d889944505f3b7046e6\n",
      "I1114 20:59:40.118042  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.118117  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.292549ms\n",
      "I1114 20:59:40.134308  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663255449 bytes.\n",
      "I1114 20:59:40.134324  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162772 bytes.\n",
      "I1114 20:59:40.134422  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 2.93302875ms\n",
      "I1114 20:59:40.134705  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.138309  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.658522ms\n",
      "I1114 20:59:40.144541  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.20326075ms\n",
      "I1114 20:59:40.144619  725449 isa_program_util_common.cc:510] (HLO module jit_iota): Executable fingerprint:57280a4beddf7678a596a27f40caa733a67b820f9649b94003f3c9518fc87ba9\n",
      "I1114 20:59:40.144623  725449 isa_program_util_common.cc:514] (HLO module jit_iota): Executable fingerprint (including data segments):262569d1fe4ffd105c8c2d1bf625e744c2ab8f6a95289cb0b9894d7c9e56f41b\n",
      "I1114 20:59:40.144625  725449 isa_program_util_common.cc:517] (HLO module jit_iota): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.144706  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 13.24442925ms\n",
      "I1114 20:59:40.160767  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663170969 bytes.\n",
      "I1114 20:59:40.160787  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883158548 bytes.\n",
      "I1114 20:59:40.160968  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.57391825ms\n",
      "I1114 20:59:40.161289  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.165973  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.74103875ms\n",
      "I1114 20:59:40.172335  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.32301575ms\n",
      "I1114 20:59:40.172423  725449 isa_program_util_common.cc:510] (HLO module jit_multiply): Executable fingerprint:190754de3fd259b600c4db676b2c114e15ab6eb5ea56043b52cdc120001bdf66\n",
      "I1114 20:59:40.172427  725449 isa_program_util_common.cc:514] (HLO module jit_multiply): Executable fingerprint (including data segments):1c24fc1e1a6b38bced32a9c341eccfc774976694799298676a3480dee2e87fa7\n",
      "I1114 20:59:40.172429  725449 isa_program_util_common.cc:517] (HLO module jit_multiply): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.172503  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.15347775ms\n",
      "I1114 20:59:40.188960  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663170969 bytes.\n",
      "I1114 20:59:40.188982  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883158548 bytes.\n",
      "I1114 20:59:40.189152  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.45147975ms\n",
      "I1114 20:59:40.189475  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.194127  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.7131505ms\n",
      "I1114 20:59:40.200576  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.40574775ms\n",
      "I1114 20:59:40.200666  725449 isa_program_util_common.cc:510] (HLO module jit_add): Executable fingerprint:17408244a2de47c6d73e33643c8125edbb692a7cb8acf62010dc670a27dbc0d1\n",
      "I1114 20:59:40.200671  725449 isa_program_util_common.cc:514] (HLO module jit_add): Executable fingerprint (including data segments):2c52dbfbae6f7ca42a13d24049ab242eaca1fc0030abcd52b5e7d2d3c6f960e5\n",
      "I1114 20:59:40.200673  725449 isa_program_util_common.cc:517] (HLO module jit_add): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.200749  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.08408875ms\n",
      "I1114 20:59:40.216588  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663232921 bytes.\n",
      "I1114 20:59:40.216611  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161646 bytes.\n",
      "I1114 20:59:40.216723  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.092763ms\n",
      "I1114 20:59:40.216999  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.222077  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.12545175ms\n",
      "I1114 20:59:40.228406  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.2989825ms\n",
      "I1114 20:59:40.228489  725449 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:9227d72cd9baf1554241ff5d8296facec42fc4e1d717002d69cbdf7c50be761b\n",
      "I1114 20:59:40.228493  725449 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):2883b206079e1401df8a6d073c19dcf7b8db54b3b335f10601db08c5161ea85a\n",
      "I1114 20:59:40.228495  725449 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.228569  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.971499ms\n",
      "I1114 20:59:40.246147  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601273753 bytes.\n",
      "I1114 20:59:40.246168  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830063687 bytes.\n",
      "I1114 20:59:40.246546  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.93823825ms\n",
      "I1114 20:59:40.246909  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.453136  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 206.2817975ms\n",
      "I1114 20:59:40.471571  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 18.37584ms\n",
      "I1114 20:59:40.471834  725449 isa_program_util_common.cc:510] (HLO module jit_gather): Executable fingerprint:09250165b686b46e38a36cc99b32f56719aab4db2ae462fb294fc37d748bb1e0\n",
      "I1114 20:59:40.471839  725449 isa_program_util_common.cc:514] (HLO module jit_gather): Executable fingerprint (including data segments):ba7abf873dc4e628cc16da527ed3e7c4971db32707fb08aa0a80c7209c95a99b\n",
      "I1114 20:59:40.471840  725449 isa_program_util_common.cc:517] (HLO module jit_gather): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.471956  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 230.3813105ms\n",
      "I1114 20:59:40.488688  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120607129 bytes.\n",
      "I1114 20:59:40.488705  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856030356 bytes.\n",
      "I1114 20:59:40.488854  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1935815ms\n",
      "I1114 20:59:40.489154  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.589040  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 99.940229ms\n",
      "I1114 20:59:40.597618  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.544726ms\n",
      "I1114 20:59:40.597741  725449 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:6776705d872a6a419d682a20ca9f99ac30293a6ee6a8e9eafc6d7ba0d9bb1426\n",
      "I1114 20:59:40.597745  725449 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):1301a464cc50019a4037786e265539be0957c2d8bd621c8881a9e23dd30c4d3a\n",
      "I1114 20:59:40.597747  725449 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.597832  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 112.203986ms\n",
      "I1114 20:59:40.617366  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577876889 bytes.\n",
      "I1114 20:59:40.617382  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828893844 bytes.\n",
      "I1114 20:59:40.617592  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.96300225ms\n",
      "I1114 20:59:40.617936  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:40.953882  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 336.00835225ms\n",
      "I1114 20:59:40.971485  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 17.56073475ms\n",
      "I1114 20:59:40.971714  725449 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:49174e3b7b00b43f371d4a45c04fc60596b08e4c7a15ea96eb5cdd5ca33630c6\n",
      "I1114 20:59:40.971718  725449 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):9b1bc7eeff20235a16f9bfcef95c8203deadb4c7599d857281d0acb4c5fd39b0\n",
      "I1114 20:59:40.971721  725449 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:40.971811  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 358.222077ms\n",
      "I1114 20:59:40.993753  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662588313 bytes.\n",
      "I1114 20:59:40.993771  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883129415 bytes.\n",
      "I1114 20:59:40.994148  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.2063785ms\n",
      "I1114 20:59:40.994544  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.047779  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 53.295694ms\n",
      "I1114 20:59:41.060744  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 12.92836725ms\n",
      "I1114 20:59:41.060959  725449 isa_program_util_common.cc:510] (HLO module jit_gather): Executable fingerprint:2c9aa7d52b976bd6e1d355e6b401a95e5542e7a604c9901931d41ba0472ef04f\n",
      "I1114 20:59:41.060964  725449 isa_program_util_common.cc:514] (HLO module jit_gather): Executable fingerprint (including data segments):c2aa889a5799b707ce674d3de95c301dc3cc36c417c79138290ab5f93685cd37\n",
      "I1114 20:59:41.060965  725449 isa_program_util_common.cc:517] (HLO module jit_gather): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.061052  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 72.15168575ms\n",
      "I1114 20:59:41.078039  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1114 20:59:41.078054  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1114 20:59:41.078194  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.14094825ms\n",
      "I1114 20:59:41.078508  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.084209  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.7572055ms\n",
      "I1114 20:59:41.090679  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.44108925ms\n",
      "I1114 20:59:41.090771  725449 isa_program_util_common.cc:510] (HLO module jit_broadcast_in_dim): Executable fingerprint:36863f80d229aa82f961a43d1551de01b46350b844dad8d226cc9c5cd413204d\n",
      "I1114 20:59:41.090775  725449 isa_program_util_common.cc:514] (HLO module jit_broadcast_in_dim): Executable fingerprint (including data segments):dd347b26c2bc1b49ea22dbfc0d22da881488711b8bb2dcad31c0d6b4b3b1b2c8\n",
      "I1114 20:59:41.090777  725449 isa_program_util_common.cc:517] (HLO module jit_broadcast_in_dim): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.090849  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.8247725ms\n",
      "I1114 20:59:41.108369  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662796697 bytes.\n",
      "I1114 20:59:41.108386  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883139834 bytes.\n",
      "I1114 20:59:41.108564  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.513045ms\n",
      "I1114 20:59:41.108884  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.122766  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 13.93303275ms\n",
      "I1114 20:59:41.129613  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.8041615ms\n",
      "I1114 20:59:41.129707  725449 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:862c6c0ad8df8b43a3f58bd93a5f7f096e7e67db184184e45557cb91ad099dd1\n",
      "I1114 20:59:41.129711  725449 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):ed5eb524be4a1623e28a210a74740fb316c3cd58bde02430e600c129dd72919e\n",
      "I1114 20:59:41.129713  725449 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.129787  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 24.771167ms\n",
      "I1114 20:59:41.148212  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601551769 bytes.\n",
      "I1114 20:59:41.148227  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830077588 bytes.\n",
      "I1114 20:59:41.148359  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.25850325ms\n",
      "I1114 20:59:41.148650  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.208399  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 59.80055ms\n",
      "I1114 20:59:41.217211  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.78149825ms\n",
      "I1114 20:59:41.217334  725449 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:678ce0c95e46b45f32a99aad9554b5dae6a6acb5945f6b11b73c47a3aa45f324\n",
      "I1114 20:59:41.217338  725449 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):dd4f0f5ba99c0d52e0631b0719e2d74b90f94f42ec3ecc50f6d2360e7a0078f1\n",
      "I1114 20:59:41.217340  725449 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.217419  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 72.3495265ms\n",
      "I1114 20:59:41.233875  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120596889 bytes.\n",
      "I1114 20:59:41.233890  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856029844 bytes.\n",
      "I1114 20:59:41.234032  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.25140975ms\n",
      "I1114 20:59:41.234341  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.368311  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 134.020882ms\n",
      "I1114 20:59:41.376910  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.52476525ms\n",
      "I1114 20:59:41.377019  725449 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:7c6411dec771d952b15c6969ad4368c559b7dd92eab7ede3682b368c3c5d81d8\n",
      "I1114 20:59:41.377023  725449 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):5820e82e0ee3f5519b7c218f4f9ad4212128d3921661e3cd92d51ee63e1fd61c\n",
      "I1114 20:59:41.377026  725449 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.377106  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 146.35791175ms\n",
      "I1114 20:59:41.393195  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601551769 bytes.\n",
      "I1114 20:59:41.393211  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830077588 bytes.\n",
      "I1114 20:59:41.393343  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.2298675ms\n",
      "I1114 20:59:41.393637  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.456701  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 63.120174ms\n",
      "I1114 20:59:41.465857  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.1264185ms\n",
      "I1114 20:59:41.466018  725449 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:364c7e6e4b7d3d7b86773283e7f4a3268806d3011042d377be7a9430b204c4f6\n",
      "I1114 20:59:41.466022  725449 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):5a39aa896b46ab09b63e510471eb6f4e729ec58d6095a0c7cd9abdbbb6be8a6a\n",
      "I1114 20:59:41.466024  725449 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.466113  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 76.0321575ms\n",
      "I1114 20:59:41.482669  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577897369 bytes.\n",
      "I1114 20:59:41.482685  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828894868 bytes.\n",
      "I1114 20:59:41.482866  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.503959ms\n",
      "I1114 20:59:41.483281  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.708842  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 225.7061505ms\n",
      "I1114 20:59:41.719823  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 10.94768925ms\n",
      "I1114 20:59:41.719987  725449 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:f3bcc3bc248fe1f4c10f9ff7416cb2afaaf95bd09a2376ad65ddb3d25df63d29\n",
      "I1114 20:59:41.719992  725449 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):5f5119da77ec1401e59b41fdb1a440d356002503b570cde3c32db5966cfd4a9f\n",
      "I1114 20:59:41.719994  725449 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.720074  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 240.74855025ms\n",
      "I1114 20:59:41.736846  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96577958809 bytes.\n",
      "I1114 20:59:41.736862  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4828897940 bytes.\n",
      "I1114 20:59:41.736989  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.55785225ms\n",
      "I1114 20:59:41.737273  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.806904  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 69.68353075ms\n",
      "I1114 20:59:41.815102  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.167968ms\n",
      "I1114 20:59:41.815256  725449 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:370bf6a664e975e1f371e910fdd416e0e8d28da07cc37c5501c9be4574f3052f\n",
      "I1114 20:59:41.815261  725449 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):2857ad7ab3cd3b5b2d0d0237fd1602baaac8aec9dbde47bf28d0883d6f308226\n",
      "I1114 20:59:41.815263  725449 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.815341  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 81.9425425ms\n",
      "W1114 20:59:41.829956  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:41.831031  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:41.835057  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97391926169 bytes.\n",
      "I1114 20:59:41.835073  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4869596308 bytes.\n",
      "I1114 20:59:41.835234  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.82591925ms\n",
      "I1114 20:59:41.835566  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:41.921966  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 86.44467725ms\n",
      "I1114 20:59:41.931279  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 9.274849ms\n",
      "I1114 20:59:41.931419  725449 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:4ec9574a3186f0ffb26f288cc5af3621746ad8c1d4991c67b4bdb87b270e0864\n",
      "I1114 20:59:41.931423  725449 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):866d8ffa0dcb20f9fbb98cd3fbfa331dea30e2d3bf56faea2f4301d0c4f92bac\n",
      "I1114 20:59:41.931424  725449 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:41.931502  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 103.1377135ms\n",
      "I1114 20:59:41.949988  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 96601439129 bytes.\n",
      "I1114 20:59:41.950004  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4830071956 bytes.\n",
      "I1114 20:59:41.950272  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.9829935ms\n",
      "I1114 20:59:41.950607  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.095415  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 144.86531175ms\n",
      "I1114 20:59:42.107366  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.91810925ms\n",
      "I1114 20:59:42.107541  725449 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:20eead6a6510050c6a06c55122b8125f131bb038ab560287921e1408b4cfd7d9\n",
      "I1114 20:59:42.107545  725449 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):04755dd2a83101e584b1b12e15ff4349abacea6104b328fa99bbddd5f8c7afdf\n",
      "I1114 20:59:42.107547  725449 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.107644  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 161.40881375ms\n",
      "W1114 20:59:42.124144  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:42.125210  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:42.128951  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97527585689 bytes.\n",
      "I1114 20:59:42.128967  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4876379284 bytes.\n",
      "I1114 20:59:42.129086  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.438467ms\n",
      "I1114 20:59:42.129413  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.185187  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 55.82601125ms\n",
      "I1114 20:59:42.194172  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.904645ms\n",
      "I1114 20:59:42.194318  725449 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:15c2192ba3aab672e2441fcdcb5dd0fd27e9db3557c987a18c3fc1ddad3f4c7f\n",
      "I1114 20:59:42.194322  725449 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):9d2205b41ad0dfa013595d4516038ea0438d82d20afa73629bebb193bacb8121\n",
      "I1114 20:59:42.194324  725449 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.194403  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 71.7999475ms\n",
      "I1114 20:59:42.212972  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97120484249 bytes.\n",
      "I1114 20:59:42.212989  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4856024212 bytes.\n",
      "I1114 20:59:42.213243  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.0241655ms\n",
      "I1114 20:59:42.213581  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.335770  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 122.24464775ms\n",
      "I1114 20:59:42.347461  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.65606325ms\n",
      "I1114 20:59:42.347633  725449 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:0b8c4880d23345669941a617135b96ba768a456ede951744195f175a759afdb0\n",
      "I1114 20:59:42.347638  725449 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):268fe7ac84d0ca5d34860663e970705125da83ee54688b8709aa7c3fdff9e183\n",
      "I1114 20:59:42.347640  725449 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.347730  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 138.5557975ms\n",
      "I1114 20:59:42.365302  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662876569 bytes.\n",
      "I1114 20:59:42.365318  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143828 bytes.\n",
      "I1114 20:59:42.365436  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1524765ms\n",
      "I1114 20:59:42.365731  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.371004  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.32152075ms\n",
      "I1114 20:59:42.377308  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.27440975ms\n",
      "I1114 20:59:42.377393  725449 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:40f0cccc83b28e24379e13c409b003ccad4ccc778086a097887634a9ee5403dd\n",
      "I1114 20:59:42.377397  725449 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):ee1a3428c3f31f125fa99f61c620127bcb3685b1b77f124b4e32b507ade2f66d\n",
      "I1114 20:59:42.377398  725449 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.377477  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.236649ms\n",
      "I1114 20:59:42.393297  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1114 20:59:42.393313  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1114 20:59:42.393426  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1665365ms\n",
      "I1114 20:59:42.393719  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.454254  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 60.58229375ms\n",
      "I1114 20:59:42.465593  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 11.3091065ms\n",
      "I1114 20:59:42.465763  725449 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:0dde0b84570d4f8b311fbcbd7ee430f7919a3793377646a02f6711db334be04a\n",
      "I1114 20:59:42.465767  725449 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):9a0eb996550b3c697d53556710a42baa13eea2d2f3345cf7f3b7507136739d51\n",
      "I1114 20:59:42.465769  725449 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.465849  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 75.6287645ms\n",
      "I1114 20:59:42.481803  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662876569 bytes.\n",
      "I1114 20:59:42.481819  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143828 bytes.\n",
      "I1114 20:59:42.481934  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.12559625ms\n",
      "I1114 20:59:42.482224  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.491444  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.2744305ms\n",
      "I1114 20:59:42.498070  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.58540425ms\n",
      "I1114 20:59:42.498157  725449 isa_program_util_common.cc:510] (HLO module jit_slice): Executable fingerprint:fe2efdcb3d52d0c9503a29b1fd9d8498cc7667239778b75c81aad27049feb87e\n",
      "I1114 20:59:42.498160  725449 isa_program_util_common.cc:514] (HLO module jit_slice): Executable fingerprint (including data segments):920a107875ed240ea0640763eaf114f4317f940c6737c362f05502e10852b963\n",
      "I1114 20:59:42.498163  725449 isa_program_util_common.cc:517] (HLO module jit_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.498239  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 19.462153ms\n",
      "I1114 20:59:42.514358  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662780313 bytes.\n",
      "I1114 20:59:42.514373  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883139015 bytes.\n",
      "I1114 20:59:42.514558  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.517122ms\n",
      "I1114 20:59:42.514889  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.552559  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 37.72617225ms\n",
      "I1114 20:59:42.561087  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.49793275ms\n",
      "I1114 20:59:42.561190  725449 isa_program_util_common.cc:510] (HLO module jit_concatenate): Executable fingerprint:b24c7be130ac8e12437a95a6cd7ecef069e71d4550e553c3b4b8122e2c8964de\n",
      "I1114 20:59:42.561194  725449 isa_program_util_common.cc:514] (HLO module jit_concatenate): Executable fingerprint (including data segments):762f9655e81f3a87d7f24e295dea3151ad562ea2dc123a7bf39dcd65ced3fa04\n",
      "I1114 20:59:42.561196  725449 isa_program_util_common.cc:517] (HLO module jit_concatenate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.561276  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 50.26850575ms\n",
      "I1114 20:59:42.580415  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662851993 bytes.\n",
      "I1114 20:59:42.580432  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883142599 bytes.\n",
      "I1114 20:59:42.580546  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.1156875ms\n",
      "I1114 20:59:42.580850  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.764747  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 183.94952975ms\n",
      "I1114 20:59:42.781800  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 16.90217125ms\n",
      "I1114 20:59:42.781976  725449 isa_program_util_common.cc:510] (HLO module jit_reshape): Executable fingerprint:c10c422fe9795af2a983e3fbe2b1ecc9ca4f388bf744c3ef546de1d2c94bb1d7\n",
      "I1114 20:59:42.781980  725449 isa_program_util_common.cc:514] (HLO module jit_reshape): Executable fingerprint (including data segments):1bc38b4ce2c53b8b9cc6361d15490201ae59dd6487e03a820d4ed55c7c99a084\n",
      "I1114 20:59:42.781982  725449 isa_program_util_common.cc:517] (HLO module jit_reshape): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.782062  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 204.66330025ms\n",
      "W1114 20:59:42.797415  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:42.798429  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:42.802186  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663146905 bytes.\n",
      "I1114 20:59:42.802201  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883157345 bytes.\n",
      "I1114 20:59:42.802348  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.461007ms\n",
      "I1114 20:59:42.802672  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.811931  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.31161875ms\n",
      "I1114 20:59:42.818864  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.903ms\n",
      "I1114 20:59:42.818973  725449 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:aaf05ebded97981a3587ddd4f9bbc84f9643aeaf3c797f818835483d598e1dc1\n",
      "I1114 20:59:42.818977  725449 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):2d51af4641d8d01a339154a7f5c2762e2c452cbecf4787fa69aa7b718d1c2faa\n",
      "I1114 20:59:42.818979  725449 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.819056  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 23.21343225ms\n",
      "I1114 20:59:42.837517  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662763929 bytes.\n",
      "I1114 20:59:42.837533  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883138196 bytes.\n",
      "I1114 20:59:42.837782  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.79735925ms\n",
      "I1114 20:59:42.838106  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.853622  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 15.56930175ms\n",
      "I1114 20:59:42.860871  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.21855525ms\n",
      "I1114 20:59:42.860972  725449 isa_program_util_common.cc:510] (HLO module jit__multi_slice): Executable fingerprint:8685cfc07fa56f2461e517612b8269a1d1a3d31761d46a417de0fd787c225930\n",
      "I1114 20:59:42.860976  725449 isa_program_util_common.cc:514] (HLO module jit__multi_slice): Executable fingerprint (including data segments):fd21cb59c09ca83a49ec9b16477e055f65a0dfb690d750753d3800d0814646f5\n",
      "I1114 20:59:42.860978  725449 isa_program_util_common.cc:517] (HLO module jit__multi_slice): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.861051  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 27.11046775ms\n",
      "W1114 20:59:42.876893  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:42.877885  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:42.881391  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663056793 bytes.\n",
      "I1114 20:59:42.881406  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883152839 bytes.\n",
      "I1114 20:59:42.881537  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.1101525ms\n",
      "I1114 20:59:42.881868  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:42.891036  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 9.21534ms\n",
      "I1114 20:59:42.898104  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.038061ms\n",
      "I1114 20:59:42.898211  725449 isa_program_util_common.cc:510] (HLO module jit__identity_fn): Executable fingerprint:95fc20b3d91139158e400b9720ffa468a4909097f9a8e569f181ab7417efc621\n",
      "I1114 20:59:42.898215  725449 isa_program_util_common.cc:514] (HLO module jit__identity_fn): Executable fingerprint (including data segments):992271494bf12f27ce7deca100ad97747b4b2eaa47779baf466c5bbf7a133266\n",
      "I1114 20:59:42.898217  725449 isa_program_util_common.cc:517] (HLO module jit__identity_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:42.898293  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 22.909592ms\n",
      "WARNING:root:Duplicate op registration for aten.__and__\n",
      "I1114 20:59:49.067608  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663244697 bytes.\n",
      "I1114 20:59:49.067648  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162234 bytes.\n",
      "I1114 20:59:49.067824  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 4.39091675ms\n",
      "I1114 20:59:49.068312  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:49.073043  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 4.82021875ms\n",
      "I1114 20:59:49.080198  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.01552025ms\n",
      "I1114 20:59:49.080312  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:7075cbfbd75beffcf249e571e6a4bddb8379e01aac94c4d7c47a7f9ca1704518\n",
      "I1114 20:59:49.080316  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):b8441eb9ca29849d647247aaa9c78c1a597aabdd6aa0aa919b29de261bbda7f2\n",
      "I1114 20:59:49.080319  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:49.080453  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 17.0793595ms\n",
      "I1114 20:59:49.098169  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663244697 bytes.\n",
      "I1114 20:59:49.098185  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162234 bytes.\n",
      "I1114 20:59:49.098309  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.20050025ms\n",
      "I1114 20:59:49.098634  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:49.102494  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.90734975ms\n",
      "I1114 20:59:49.108939  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.40969ms\n",
      "I1114 20:59:49.109018  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:375f551ac122c4c1443de2d6757a8e6ee695c8bd10bcc876cb4acb15627a65ca\n",
      "I1114 20:59:49.109022  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):4c8eb4dd58c79142733686c9b511662531ea17d536c2a918c926dac69fcf2fbe\n",
      "I1114 20:59:49.109024  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:49.109108  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 14.0332225ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:49 [tpu_runner.py:497] Init model | hbm=[(11.0, 95.74), (11.0, 95.74), (11.0, 95.74), (11.0, 95.74)]GiB\n",
      "INFO 11-14 20:59:49 [tpu_worker.py:174] Memory statistics | total_hbm_limit_gb=382.97GiB | total_hbm_limit_cap_gb=306.38GiB | total_hbm_used_gb=43.99GiB | total_hbm_avail_gb=262.38GiB\n",
      "WARNING 11-14 20:59:49 [kv_cache_utils.py:1095] Hybrid KV cache manager is disabled for this hybrid model, This means we do not enable any optimizations for saving KV cache memory (e.g., dropping the KV cache outside the sliding window). The compute of layers like sliding window is still saved.\n",
      "INFO 11-14 20:59:49 [kv_cache_utils.py:1229] GPU KV cache size: 5,731,872 tokens\n",
      "INFO 11-14 20:59:49 [kv_cache_utils.py:1234] Maximum concurrency for 64 tokens per request: 89560.50x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 20:59:49.769865  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663234457 bytes.\n",
      "I1114 20:59:49.769907  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883161722 bytes.\n",
      "I1114 20:59:49.770136  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 5.0533605ms\n",
      "I1114 20:59:49.770700  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:49.777075  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 6.4730595ms\n",
      "I1114 20:59:49.785482  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.28016425ms\n",
      "I1114 20:59:49.785607  725449 isa_program_util_common.cc:510] (HLO module jit_convert_element_type): Executable fingerprint:13d169e8f0cb52132827f3458861645b2ad7cf1a3afcb1127a6444697d9ec7eb\n",
      "I1114 20:59:49.785612  725449 isa_program_util_common.cc:514] (HLO module jit_convert_element_type): Executable fingerprint (including data segments):7274c5d1d7ca793892a3a02db907049e506cf3ed81768774f478da0a45758e78\n",
      "I1114 20:59:49.785615  725449 isa_program_util_common.cc:517] (HLO module jit_convert_element_type): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:49.785990  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 20.96864925ms\n",
      "I1114 20:59:49.868318  724997 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 166 instructions, modules: jit__threefry_fold_in\n",
      "I1114 20:59:49.868341  724997 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__threefry_fold_in instructions: 166 fingerprint: \n",
      "I1114 20:59:49.869097  724997 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__threefry_fold_in instructions: 153\n",
      "I1114 20:59:49.869103  724997 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1114 20:59:49.880981  724997 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1114 20:59:49.883210  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97661667225 bytes.\n",
      "I1114 20:59:49.883224  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883083361 bytes.\n",
      "I1114 20:59:49.884579  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 17.61908975ms\n",
      "I1114 20:59:49.885312  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:49.905248  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 20.001724ms\n",
      "I1114 20:59:49.907597  725449 2a886c8_compiler_base.cc:3045] final program bundle count: 633 note this count does not reflect cycles spent executing delays.\n",
      "I1114 20:59:49.912399  725449 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1114 20:59:49.913863  725449 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (76.5K).\n",
      "I1114 20:59:49.914140  725449 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__threefry_fold_in\n",
      "I1114 20:59:49.914146  725449 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 156.5K / 95.74G\n",
      "I1114 20:59:49.914150  725449 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 4.5K / 64.00M\n",
      "I1114 20:59:49.914161  725449 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.16M:\n",
      "I1114 20:59:49.914163  725449 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1114 20:59:49.914164  725449 2a886c8_compiler_base.cc:3549]     program          156.5K \n",
      "I1114 20:59:49.914166  725449 2a886c8_compiler_base.cc:3549]     arguments          1.0K \n",
      "I1114 20:59:49.914169  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:49.914170  725449 2a886c8_compiler_base.cc:3549] Output size 512B; shares 0B with arguments.\n",
      "I1114 20:59:49.914171  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:49.914172  725449 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1114 20:59:49.914174  725449 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1114 20:59:49.914175  725449 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1114 20:59:49.914176  725449 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1114 20:59:49.914177  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:49.914178  725449 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1114 20:59:49.914179  725449 2a886c8_compiler_base.cc:3549] \n",
      "I1114 20:59:49.914193  725449 2a886c8_compiler_base.cc:3553] Program sflag requirement 220B:\n",
      "I1114 20:59:49.914194  725449 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1114 20:59:49.914195  725449 2a886c8_compiler_base.cc:3553]     scoped              12B\n",
      "I1114 20:59:49.914196  725449 2a886c8_compiler_base.cc:3553]     HLO temp             4B (100.0% utilization: Unpadded (4B) Padded (4B), 0.0% fragmentation (0B))\n",
      "I1114 20:59:49.914197  725449 2a886c8_compiler_base.cc:3553] Program hbm requirement 156.5K:\n",
      "I1114 20:59:49.914199  725449 2a886c8_compiler_base.cc:3553]     HLO temp          80.0K (4.8% utilization: Unpadded (76B) Padded (1.6K), 98.0% fragmentation (78.4K))\n",
      "I1114 20:59:49.914200  725449 2a886c8_compiler_base.cc:3553]     overlays          76.5K\n",
      "I1114 20:59:49.914201  725449 2a886c8_compiler_base.cc:3553] Program vmem requirement 4.5K:\n",
      "I1114 20:59:49.914202  725449 2a886c8_compiler_base.cc:3553]     scoped             4.0K\n",
      "I1114 20:59:49.914203  725449 2a886c8_compiler_base.cc:3553]     HLO temp           512B (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (512B))\n",
      "I1114 20:59:49.914204  725449 2a886c8_compiler_base.cc:3553] Program smem requirement 40B:\n",
      "I1114 20:59:49.914205  725449 2a886c8_compiler_base.cc:3553]     scoped              40B\n",
      "I1114 20:59:49.914206  725449 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1114 20:59:49.914207  725449 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 1.9K / 1.00M (2 parameters)\n",
      "I1114 20:59:49.914220  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.93318425ms\n",
      "I1114 20:59:49.914401  725449 isa_program_util_common.cc:510] (HLO module jit__threefry_fold_in): Executable fingerprint:9bb788d9827517b4bb3d64a21746b99ef4adaed7811785123bfd9d2b576d9501\n",
      "I1114 20:59:49.914404  725449 isa_program_util_common.cc:514] (HLO module jit__threefry_fold_in): Executable fingerprint (including data segments):5a622cfb5e76d5a9edfea31742dad4d10b548b95d9f0f1a078ff626a5ee2b500\n",
      "I1114 20:59:49.914405  725449 isa_program_util_common.cc:517] (HLO module jit__threefry_fold_in): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:49.914487  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.83285175ms\n",
      "I1114 20:59:49.940937  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663203225 bytes.\n",
      "I1114 20:59:49.940957  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883160161 bytes.\n",
      "I1114 20:59:49.941133  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.51476425ms\n",
      "I1114 20:59:49.941445  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:49.946670  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 5.27660075ms\n",
      "I1114 20:59:49.952989  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 6.287681ms\n",
      "I1114 20:59:49.953080  725449 isa_program_util_common.cc:510] (HLO module jit_add): Executable fingerprint:1d37c80f028627082acffcad0f66d83660c0f7d233cd5625ea0f63148a6071a2\n",
      "I1114 20:59:49.953085  725449 isa_program_util_common.cc:514] (HLO module jit_add): Executable fingerprint (including data segments):762e771bcbd93219353261aa62fb966518be5395f8616ec2362defda1b7ad905\n",
      "I1114 20:59:49.953087  725449 isa_program_util_common.cc:517] (HLO module jit_add): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:49.953157  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 15.57557725ms\n",
      "W1114 20:59:49.974949  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:49.976007  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:49.980385  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:49.980400  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:49.980515  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 7.0958905ms\n",
      "I1114 20:59:49.980857  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.012952  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.14289575ms\n",
      "I1114 20:59:50.020200  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.21648325ms\n",
      "I1114 20:59:50.020313  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.020317  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.020319  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.020395  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.02020275ms\n",
      "W1114 20:59:50.036944  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.038103  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.042046  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.042062  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.042179  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.70429625ms\n",
      "I1114 20:59:50.042558  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.074383  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.8790785ms\n",
      "I1114 20:59:50.081711  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.29701525ms\n",
      "I1114 20:59:50.081824  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.081828  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.081830  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.081905  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.4759555ms\n",
      "W1114 20:59:50.098062  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.099050  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.102925  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.102941  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.103052  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.46572325ms\n",
      "I1114 20:59:50.103448  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.135182  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.795796ms\n",
      "I1114 20:59:50.142727  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.51444275ms\n",
      "I1114 20:59:50.142839  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.142843  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.142845  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.142924  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.380777ms\n",
      "W1114 20:59:50.159210  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.160210  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.164178  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.164194  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.164311  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.6038235ms\n",
      "I1114 20:59:50.164679  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.196501  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.874651ms\n",
      "I1114 20:59:50.204050  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.51824875ms\n",
      "I1114 20:59:50.204164  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.204168  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.204170  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.204245  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.58560225ms\n",
      "W1114 20:59:50.220508  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.221529  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.225547  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.225564  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.225678  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.71179625ms\n",
      "I1114 20:59:50.226049  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.257812  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.814962ms\n",
      "I1114 20:59:50.265343  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.50096ms\n",
      "I1114 20:59:50.265459  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.265463  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.265465  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.265544  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.62521875ms\n",
      "W1114 20:59:50.284645  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.285735  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.289691  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.289706  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.289819  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.712362ms\n",
      "I1114 20:59:50.290193  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.321898  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.75781975ms\n",
      "I1114 20:59:50.329470  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.54127325ms\n",
      "I1114 20:59:50.329584  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.329589  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.329591  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.329667  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.606623ms\n",
      "W1114 20:59:50.346066  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.347069  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.351121  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.351137  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.351276  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.6995495ms\n",
      "I1114 20:59:50.351653  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.383396  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.79402125ms\n",
      "I1114 20:59:50.390986  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.4971665ms\n",
      "I1114 20:59:50.391102  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.391106  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.391108  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.391205  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.6756765ms\n",
      "W1114 20:59:50.407968  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.408964  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.412995  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.413011  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.413124  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.684162ms\n",
      "I1114 20:59:50.413499  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.445217  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.76937225ms\n",
      "I1114 20:59:50.452805  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.55816075ms\n",
      "I1114 20:59:50.452919  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.452924  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.452926  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.453001  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.6074875ms\n",
      "W1114 20:59:50.469455  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.470536  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.474540  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.474556  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.474671  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.71547475ms\n",
      "I1114 20:59:50.475057  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.506732  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.7267835ms\n",
      "I1114 20:59:50.514343  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.580137ms\n",
      "I1114 20:59:50.514457  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.514461  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.514464  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.514541  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.63095425ms\n",
      "W1114 20:59:50.530903  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.531932  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.536007  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.536023  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.536137  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.734242ms\n",
      "I1114 20:59:50.536518  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.568102  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.63758475ms\n",
      "I1114 20:59:50.575728  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.59388ms\n",
      "I1114 20:59:50.575842  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.575846  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.575848  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.575923  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.56765775ms\n",
      "W1114 20:59:50.592231  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.593279  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.597311  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.597330  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.597447  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.71609325ms\n",
      "I1114 20:59:50.597821  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.630941  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.17189675ms\n",
      "I1114 20:59:50.639877  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.90393525ms\n",
      "I1114 20:59:50.639994  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.639998  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.640000  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.640080  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.398402ms\n",
      "W1114 20:59:50.659720  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.660825  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.665009  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.665025  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.665142  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.972258ms\n",
      "I1114 20:59:50.665524  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.698218  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.74876525ms\n",
      "I1114 20:59:50.707093  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.84423975ms\n",
      "I1114 20:59:50.707240  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.707244  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.707247  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.707329  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.20745375ms\n",
      "W1114 20:59:50.724012  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.725010  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.729231  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.729247  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.729369  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.88719125ms\n",
      "I1114 20:59:50.729745  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.762325  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.632591ms\n",
      "I1114 20:59:50.771251  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.89412275ms\n",
      "I1114 20:59:50.771369  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.771374  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.771375  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.771454  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.0203135ms\n",
      "W1114 20:59:50.788129  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.789291  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.793394  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.793410  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.793524  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.93843575ms\n",
      "I1114 20:59:50.793907  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.826531  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.676594ms\n",
      "I1114 20:59:50.835453  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.8910175ms\n",
      "I1114 20:59:50.835571  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.835575  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.835576  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.835652  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.11480775ms\n",
      "W1114 20:59:50.852424  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.853512  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.857687  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.857702  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.857819  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.91985475ms\n",
      "I1114 20:59:50.858198  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.890791  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 32.64472875ms\n",
      "I1114 20:59:50.899806  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 8.97153325ms\n",
      "I1114 20:59:50.899923  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.899927  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.899930  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.900006  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 49.15198775ms\n",
      "W1114 20:59:50.916912  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.918009  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.922152  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.922168  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.922284  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.9186505ms\n",
      "I1114 20:59:50.922673  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:50.955620  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 33.0011795ms\n",
      "I1114 20:59:50.963448  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.7975075ms\n",
      "I1114 20:59:50.963562  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:50.963566  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:50.963568  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:50.963652  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 48.3463655ms\n",
      "W1114 20:59:50.980294  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:50.981292  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:50.985463  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:50.985478  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:50.985590  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.8271415ms\n",
      "I1114 20:59:50.985957  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.017676  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.766642ms\n",
      "I1114 20:59:51.025328  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.62218675ms\n",
      "I1114 20:59:51.025443  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.025447  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.025449  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.025528  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.81217425ms\n",
      "W1114 20:59:51.044006  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.045071  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.049082  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.049098  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.049217  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.7116035ms\n",
      "I1114 20:59:51.049591  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.081211  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.67471375ms\n",
      "I1114 20:59:51.088920  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.67783275ms\n",
      "I1114 20:59:51.089036  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.089040  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.089042  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.089118  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.65748025ms\n",
      "W1114 20:59:51.105429  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.106415  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.110509  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.110524  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.110636  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.70295025ms\n",
      "I1114 20:59:51.111009  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.142684  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.7303265ms\n",
      "I1114 20:59:51.150337  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.622326ms\n",
      "I1114 20:59:51.150453  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.150458  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.150460  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.150536  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.64874225ms\n",
      "W1114 20:59:51.166894  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.167924  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.172011  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.172026  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.172148  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.7416155ms\n",
      "I1114 20:59:51.172518  726325 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.204209  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.74256775ms\n",
      "I1114 20:59:51.211934  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.6946105ms\n",
      "I1114 20:59:51.212051  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.212055  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.212057  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.212133  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.77641125ms\n",
      "W1114 20:59:51.228481  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.229589  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.233816  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.233831  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.233955  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 7.01574675ms\n",
      "I1114 20:59:51.234350  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:51 [kv_cache_manager.py:216] Init kv-cache | num_layers=24 | shape=(num_blocks, (16, 4, 2, 128)) | num_blocks=[358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242, 358242] | sharding=NamedSharding(mesh=Mesh('data': 1, 'model': 4, axis_types=(Auto, Auto)), spec=PartitionSpec('data', None, 'model'), memory_kind=device) | dtype=bfloat16 | hbm=[(76.6, 95.74), (76.59, 95.74), (76.59, 95.74), (76.59, 95.74)]Gb\n",
      "INFO 11-14 20:59:51 [core.py:258] init engine (profile, create kv cache, warmup model) took 1.51 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1114 20:59:51.266218  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.93008425ms\n",
      "I1114 20:59:51.273947  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.6980015ms\n",
      "I1114 20:59:51.274063  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.274068  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.274070  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.274147  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.2540895ms\n",
      "W1114 20:59:51.290800  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.291824  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.295972  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.295988  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.296110  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.80638525ms\n",
      "I1114 20:59:51.296673  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.328480  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.89889025ms\n",
      "I1114 20:59:51.336147  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.63619775ms\n",
      "I1114 20:59:51.336266  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.336271  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.336273  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.336350  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.09591175ms\n",
      "W1114 20:59:51.352685  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.353678  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.357838  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.357853  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.357968  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.774283ms\n",
      "I1114 20:59:51.358516  726324 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.390270  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.820108ms\n",
      "I1114 20:59:51.397969  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.6689475ms\n",
      "I1114 20:59:51.398085  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.398090  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.398092  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.398167  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 47.048045ms\n",
      "W1114 20:59:51.415514  725449 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 20:59:51.416502  725449 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 20:59:51.420582  724997 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97663245209 bytes.\n",
      "I1114 20:59:51.420597  724997 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883162260 bytes.\n",
      "I1114 20:59:51.420710  724997 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 6.773365ms\n",
      "I1114 20:59:51.421093  726326 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 20:59:51.452581  725449 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 31.5469655ms\n",
      "I1114 20:59:51.460331  725449 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 7.7200615ms\n",
      "I1114 20:59:51.460446  725449 isa_program_util_common.cc:510] (HLO module jit__allocate): Executable fingerprint:a97fa01f12b36ed6417e7c7c3549012e9254d011671b14e0d7ed2e998a1adebb\n",
      "I1114 20:59:51.460450  725449 isa_program_util_common.cc:514] (HLO module jit__allocate): Executable fingerprint (including data segments):00364da3de371d8678258d2667970cc7d925be9fae5ceb20edb6cf1de8e19fd2\n",
      "I1114 20:59:51.460452  725449 isa_program_util_common.cc:517] (HLO module jit__allocate): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 20:59:51.460526  724997 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 46.64234825ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 20:59:53 [llm.py:350] Supported tasks: ('generate',)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/gpt-oss-20b-BF16\"\n",
    "golden_llm = LLM(model_name, max_model_len=64, \n",
    "                 tensor_parallel_size=4,     \n",
    "                 gpu_memory_utilization=0.8,\n",
    "    #enable_expert_parallel=True\n",
    "    )\n",
    "golden_state = golden_llm.llm_engine.model_executor.driver_worker.model_runner.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e26cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|                                                                                                                                                                                             | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 9/9 [00:00<00:00, 175.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n",
      "model.embed_tokens.weight torch.Size([201088, 2880])\n",
      "model.layers.0.self_attn.sinks torch.Size([64])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.0.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.0.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.0.mlp.router.bias torch.Size([32])\n",
      "model.layers.0.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.0.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.0.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.0.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.1.self_attn.sinks torch.Size([64])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.1.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.1.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.1.mlp.router.bias torch.Size([32])\n",
      "model.layers.1.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.1.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.1.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.1.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.2.self_attn.sinks torch.Size([64])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.2.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.2.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.2.mlp.router.bias torch.Size([32])\n",
      "model.layers.2.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.2.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.2.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.2.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.3.self_attn.sinks torch.Size([64])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.3.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.3.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.3.mlp.router.bias torch.Size([32])\n",
      "model.layers.3.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.3.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.3.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.3.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.4.self_attn.sinks torch.Size([64])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.4.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.4.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.4.mlp.router.bias torch.Size([32])\n",
      "model.layers.4.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.4.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.4.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.4.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.5.self_attn.sinks torch.Size([64])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.5.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.5.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.5.mlp.router.bias torch.Size([32])\n",
      "model.layers.5.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.5.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.5.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.5.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.6.self_attn.sinks torch.Size([64])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.6.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.6.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.6.mlp.router.bias torch.Size([32])\n",
      "model.layers.6.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.6.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.6.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.6.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.7.self_attn.sinks torch.Size([64])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.7.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.7.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.7.mlp.router.bias torch.Size([32])\n",
      "model.layers.7.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.7.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.7.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.7.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.8.self_attn.sinks torch.Size([64])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.8.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.8.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.8.mlp.router.bias torch.Size([32])\n",
      "model.layers.8.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.8.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.8.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.8.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.9.self_attn.sinks torch.Size([64])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.9.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.9.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.9.mlp.router.bias torch.Size([32])\n",
      "model.layers.9.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.9.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.9.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.9.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.10.self_attn.sinks torch.Size([64])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.10.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.10.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.10.mlp.router.bias torch.Size([32])\n",
      "model.layers.10.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.10.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.10.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.10.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.11.self_attn.sinks torch.Size([64])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.11.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.11.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.11.mlp.router.bias torch.Size([32])\n",
      "model.layers.11.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.11.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.11.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.11.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.12.self_attn.sinks torch.Size([64])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.12.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.12.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.12.mlp.router.bias torch.Size([32])\n",
      "model.layers.12.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.12.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.12.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.12.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.13.self_attn.sinks torch.Size([64])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.13.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.13.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.13.mlp.router.bias torch.Size([32])\n",
      "model.layers.13.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.13.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.13.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.13.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.14.self_attn.sinks torch.Size([64])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.14.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.14.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.14.mlp.router.bias torch.Size([32])\n",
      "model.layers.14.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.14.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.14.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.14.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.15.self_attn.sinks torch.Size([64])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.15.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.15.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.15.mlp.router.bias torch.Size([32])\n",
      "model.layers.15.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.15.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.15.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.15.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.16.self_attn.sinks torch.Size([64])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.16.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.16.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.16.mlp.router.bias torch.Size([32])\n",
      "model.layers.16.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.16.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.16.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.16.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.16.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.17.self_attn.sinks torch.Size([64])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.17.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.17.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.17.mlp.router.bias torch.Size([32])\n",
      "model.layers.17.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.17.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.17.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.17.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.17.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.18.self_attn.sinks torch.Size([64])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.18.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.18.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.18.mlp.router.bias torch.Size([32])\n",
      "model.layers.18.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.18.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.18.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.18.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.18.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.19.self_attn.sinks torch.Size([64])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.19.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.19.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.19.mlp.router.bias torch.Size([32])\n",
      "model.layers.19.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.19.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.19.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.19.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.19.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.20.self_attn.sinks torch.Size([64])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.20.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.20.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.20.mlp.router.bias torch.Size([32])\n",
      "model.layers.20.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.20.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.20.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.20.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.20.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.21.self_attn.sinks torch.Size([64])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.21.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.21.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.21.mlp.router.bias torch.Size([32])\n",
      "model.layers.21.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.21.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.21.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.21.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.21.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.22.self_attn.sinks torch.Size([64])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.22.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.22.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.22.mlp.router.bias torch.Size([32])\n",
      "model.layers.22.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.22.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.22.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.22.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.22.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.layers.23.self_attn.sinks torch.Size([64])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 2880])\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([512, 2880])\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([512])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([512, 2880])\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([512])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([2880, 4096])\n",
      "model.layers.23.self_attn.o_proj.bias torch.Size([2880])\n",
      "model.layers.23.mlp.router.weight torch.Size([32, 2880])\n",
      "model.layers.23.mlp.router.bias torch.Size([32])\n",
      "model.layers.23.mlp.experts.gate_up_proj torch.Size([32, 2880, 5760])\n",
      "model.layers.23.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])\n",
      "model.layers.23.mlp.experts.down_proj torch.Size([32, 2880, 2880])\n",
      "model.layers.23.mlp.experts.down_proj_bias torch.Size([32, 2880])\n",
      "model.layers.23.input_layernorm.weight torch.Size([2880])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([2880])\n",
      "model.norm.weight torch.Size([2880])\n",
      "lm_head.weight torch.Size([201088, 2880])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/gpt-oss-20b-BF16\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"bfloat16\")\n",
    "print(len(list(model.named_parameters())))\n",
    "for name, val in model.named_parameters():\n",
    "  print(name, val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77b030f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting GPT-OSS-MoE to vLLM Conversion (L=24, E=32) ---\n",
      "--- GPT-OSS-MoE to vLLM Conversion Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Define the target data type (bfloat16 for vLLM/JAX compatibility)\n",
    "TARGET_DTYPE = jnp.bfloat16\n",
    "\n",
    "def to_np_array(weight: Any) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  Safely converts the input (Tensor or Array) to a NumPy array and\n",
    "  explicitly casts it to TARGET_DTYPE (np.bfloat16).\n",
    "  \"\"\"\n",
    "  if isinstance(weight, torch.Tensor):\n",
    "    # Convert PyTorch Tensor to NumPy array\n",
    "    weight = weight.detach().cpu().to(torch.float16).numpy()\n",
    "\n",
    "  # Ensure it is a NumPy array and cast to the target dtype\n",
    "  if isinstance(weight, np.ndarray):\n",
    "    # The .astype(TARGET_DTYPE) performs the bfloat16 casting\n",
    "    return  np.array(weight).astype(TARGET_DTYPE)\n",
    "\n",
    "  # If the input is already a JAX/Numpy bfloat16 array, this is safe.\n",
    "  # Otherwise, this handles conversion from PyTorch/other NumPy dtypes.\n",
    "  return np.array(weight).astype(TARGET_DTYPE)\n",
    "\n",
    "\n",
    "def convert_gptoss_to_vllm_moe_jax(hf_state_dict: Dict[str, Any], num_layers: int = 24, num_experts: int = 32) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts GPT-OSS HuggingFace checkpoint keys and weights/biases to vLLM format, \n",
    "    with fixes for QKV key naming and explicit handling of MoE expert biases.\n",
    "    \n",
    "    The TypeError (2880 vs 5760) is likely a runtime error in vLLM's MoE kernel, \n",
    "    but the mapping itself below is correct based on the key names provided. \n",
    "    The fix for the QKV naming from 'qkv_proj' to 'qkv' is included.\n",
    "\n",
    "    Args:\n",
    "        hf_state_dict: Dictionary containing weights in the GPT-OSS HF format.\n",
    "        num_layers: The total number of decoder layers.\n",
    "        num_experts: The total number of experts.\n",
    "\n",
    "    Returns:\n",
    "        A new dictionary with weights in the vLLM format, as NumPy arrays\n",
    "        with dtype set to np.bfloat16.\n",
    "    \"\"\"\n",
    "    vllm_state_dict = {}\n",
    "    \n",
    "    print(f\"--- Starting GPT-OSS-MoE to vLLM Conversion (L={num_layers}, E={num_experts}) ---\")\n",
    "\n",
    "    # --- 1. Handle Embedding, Head, and Final LayerNorm ---\n",
    "    vllm_state_dict['vllm_model.lm_head.weight'] = to_np_array(hf_state_dict['lm_head.weight'])\n",
    "    vllm_state_dict['vllm_model.model.embedding.weight'] = to_np_array(hf_state_dict['model.embed_tokens.weight'])\n",
    "    vllm_state_dict['vllm_model.model.norm.weight'] = to_np_array(hf_state_dict['model.norm.weight'])\n",
    "    \n",
    "    # --- 2. Iterate through Layers ---\n",
    "    for l in range(num_layers):\n",
    "        hf_prefix = f'model.layers.{l}'\n",
    "        vllm_prefix = f'vllm_model.model.layers.{l}'\n",
    "        \n",
    "        # --- Direct Copies (LayerNorms, O_proj, Router) ---\n",
    "        vllm_state_dict[f'{vllm_prefix}.input_layernorm.weight'] = to_np_array(hf_state_dict[f'{hf_prefix}.input_layernorm.weight'])\n",
    "        vllm_state_dict[f'{vllm_prefix}.post_attention_layernorm.weight'] = to_np_array(hf_state_dict[f'{hf_prefix}.post_attention_layernorm.weight'])\n",
    "        \n",
    "        # O_proj (weight and bias)\n",
    "        vllm_state_dict[f'{vllm_prefix}.attn.o_proj.weight'] = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.o_proj.weight'])\n",
    "        vllm_state_dict[f'{vllm_prefix}.attn.o_proj.bias'] = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.o_proj.bias'])\n",
    "        \n",
    "        # Router (weight and bias)\n",
    "        vllm_state_dict[f'{vllm_prefix}.mlp.router.weight'] = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.router.weight'])\n",
    "        vllm_state_dict[f'{vllm_prefix}.mlp.router.bias'] = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.router.bias'])\n",
    "\n",
    "        # Sinks (Direct Copy for Gpt-oss specific state)\n",
    "        if f'{hf_prefix}.self_attn.sinks' in hf_state_dict:\n",
    "             vllm_state_dict[f'{vllm_prefix}.attn.sinks'] = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.sinks'])\n",
    "        \n",
    "        # --- Fused Attention (QKV) ---\n",
    "        # Weights (Concatenate [Q, K, V] along output dimension 0)\n",
    "        q_weight = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.q_proj.weight'])\n",
    "        k_weight = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.k_proj.weight'])\n",
    "        v_weight = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.v_proj.weight'])\n",
    "        qkv_fused_weight = np.concatenate([q_weight, k_weight, v_weight], axis=0)\n",
    "        \n",
    "        vllm_state_dict[f'{vllm_prefix}.attn.qkv.weight'] = qkv_fused_weight\n",
    "        \n",
    "        # Biases (Concatenate [Q_bias, K_bias, V_bias] along dimension 0)\n",
    "        q_bias = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.q_proj.bias'])\n",
    "        k_bias = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.k_proj.bias'])\n",
    "        v_bias = to_np_array(hf_state_dict[f'{hf_prefix}.self_attn.v_proj.bias'])\n",
    "        qkv_fused_bias = np.concatenate([q_bias, k_bias, v_bias], axis=0)\n",
    "        \n",
    "        vllm_state_dict[f'{vllm_prefix}.attn.qkv.bias'] = qkv_fused_bias\n",
    "\n",
    "\n",
    "        # --- Fused MoE Experts ---\n",
    "\n",
    "        # W1/W3 Fusion: HF's 'gate_up_proj'\n",
    "        # Input shape is (E, H_out, H_in). We need (E, H_in, H_out) based on the error.\n",
    "        # The original shape from HF is  (32, 2880, 5760). The vLLM expects (32, 5760, 2880). \n",
    "        w13_weight = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.experts.gate_up_proj'])\n",
    "        # wi_0 = w13_weight[..., ::2]\n",
    "        # wi_1 = w13_weight[..., 1::2]\n",
    "        # w13_weight = np.concatenate([wi_0, wi_1], axis=-1)\n",
    "        w13_weight = np.transpose(w13_weight, (0, 2, 1))\n",
    "        vllm_state_dict[f'{vllm_prefix}.mlp.experts.w13_weight'] = w13_weight\n",
    "\n",
    "        # W1/W3 Bias (No transpose needed for 1D/2D bias)\n",
    "        vllm_state_dict[f'{vllm_prefix}.mlp.experts.w13_bias'] = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.experts.gate_up_proj_bias'])\n",
    "\n",
    "        # W2 (down_proj)\n",
    "        # Since the shape is (E, H, H), transposing might be necessary if the internal convention differs. \n",
    "        # Let's start by transposing W2 as well, as this is common for all linear layers.\n",
    "        w2_weight = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.experts.down_proj'])\n",
    "        vllm_state_dict[f'{vllm_prefix}.mlp.experts.w2_weight'] = np.transpose(w2_weight, (0, 2, 1)) \n",
    "\n",
    "        # W2 Bias\n",
    "        vllm_state_dict[f'{vllm_prefix}.mlp.experts.w2_bias'] = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.experts.down_proj_bias'])\n",
    "\n",
    "\n",
    "    print(\"--- GPT-OSS-MoE to vLLM Conversion Complete ---\")\n",
    "    return vllm_state_dict\n",
    "\n",
    "\n",
    "converted_state = convert_gptoss_to_vllm_moe_jax(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e650b7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, -0.355469, 0.0703125, ..., 0.140625, 0.234375, 0],\n",
       "       [0, -0.0898438, -0.0664062, ..., -0.0117188, 0.03125, 0],\n",
       "       [0, -0.582031, 0.0644531, ..., -0.0390625, 0.554688, 0],\n",
       "       ...,\n",
       "       [0, -0.125, -0.0117188, ..., -0.0664062, 0.40625, 0],\n",
       "       [0, 1.30469, -1.53125, ..., -0.566406, 0.398438, 0],\n",
       "       [0, -0.494141, 0.349609, ..., -0.355469, 0.566406, 0]],\n",
       "      shape=(32, 5760), dtype=bfloat16)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_state_dict = model.state_dict()\n",
    "a = to_np_array(hf_state_dict[f'model.layers.0.mlp.experts.gate_up_proj_bias'])\n",
    "b = np.array(golden_state[\"vllm_model.model.layers.0.mlp.experts.w13_bias\"])\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81ef7547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.742188, -0.417969, -0.488281, ..., -0.308594, -0.386719,\n",
       "       -0.515625], shape=(2880,), dtype=bfloat16)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94774825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.742188, -0.417969, -0.488281, ..., -0.851562, -0.867188,\n",
       "       -0.917969], shape=(2880,), dtype=bfloat16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0:2880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d129cce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.773438, -0.902344, -0.894531, ..., -0.890625, -0.75, -0.917969],\n",
       "      shape=(2880,), dtype=bfloat16)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0b885b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.832031, -0.601562, -0.419922, ..., -0.890625, -0.75, -0.917969],\n",
       "      shape=(2880,), dtype=bfloat16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][2880:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb3bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a (32, 2880, 5760) <class 'numpy.ndarray'> bfloat16\n",
      "b (32, 2880, 5760) <class 'numpy.ndarray'> bfloat16\n",
      "(32, 2880, 2880)\n",
      "(32, 2880, 5760)\n",
      "2\n",
      "0.434026371487965\n"
     ]
    }
   ],
   "source": [
    "hf_state_dict = model.state_dict()\n",
    "a = hf_state_dict[f'model.layers.0.mlp.experts.gate_up_proj'].to(torch.float32).numpy().astype(jnp.bfloat16)\n",
    "#a = np.transpose(a, (0, 2,1))\n",
    "print(\"a\", a.shape, type(a), a.dtype)\n",
    "b = golden_state[\"vllm_model.model.layers.0.mlp.experts.w13_weight\"]\n",
    "b= np.array(np.transpose(b, (0, 2, 1)))\n",
    "print(\"b\", b.shape, type(b), b.dtype)\n",
    "\n",
    "\n",
    "a1 = a[..., ::2]\n",
    "print(a1.shape)\n",
    "a2 = a[..., 1::2]\n",
    "a3 = np.concatenate([a1,a2], -1)\n",
    "a4 = a3\n",
    "print(a4.shape)\n",
    "\n",
    "\n",
    "print(np.max((a4 - b)))\n",
    "print(np.sum(a4 - b == 0) / np.sum(a4 != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fee9af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vllm_model.model.layers.0.attn.rotary_emb.cos_sin_cache'}\n",
      "vllm_model.model.layers.0.mlp.experts.w13_weight, 2.125\n",
      "vllm_model.model.layers.0.mlp.experts.w13_bias, 2.90625\n",
      "vllm_model.model.layers.1.mlp.experts.w13_weight, 2.96875\n",
      "vllm_model.model.layers.1.mlp.experts.w13_bias, 4.0625\n",
      "vllm_model.model.layers.2.mlp.experts.w13_weight, 1.46875\n",
      "vllm_model.model.layers.2.mlp.experts.w13_bias, 3.15625\n",
      "vllm_model.model.layers.3.mlp.experts.w13_weight, 1.96875\n",
      "vllm_model.model.layers.3.mlp.experts.w13_bias, 3.92188\n",
      "vllm_model.model.layers.4.mlp.experts.w13_weight, 1.52344\n",
      "vllm_model.model.layers.4.mlp.experts.w13_bias, 4.34375\n",
      "vllm_model.model.layers.5.mlp.experts.w13_weight, 1.99219\n",
      "vllm_model.model.layers.5.mlp.experts.w13_bias, 3.875\n",
      "vllm_model.model.layers.6.mlp.experts.w13_weight, 3\n",
      "vllm_model.model.layers.6.mlp.experts.w13_bias, 3.625\n",
      "vllm_model.model.layers.7.mlp.experts.w13_weight, 1.5\n",
      "vllm_model.model.layers.7.mlp.experts.w13_bias, 3.40625\n",
      "vllm_model.model.layers.8.mlp.experts.w13_weight, 1.99219\n",
      "vllm_model.model.layers.8.mlp.experts.w13_bias, 2.95312\n",
      "vllm_model.model.layers.9.mlp.experts.w13_weight, 1.59375\n",
      "vllm_model.model.layers.9.mlp.experts.w13_bias, 3.39062\n",
      "vllm_model.model.layers.10.mlp.experts.w13_weight, 6.0625\n",
      "vllm_model.model.layers.10.mlp.experts.w13_bias, 3.84375\n",
      "vllm_model.model.layers.11.mlp.experts.w13_weight, 4\n",
      "vllm_model.model.layers.11.mlp.experts.w13_bias, 3.65625\n",
      "vllm_model.model.layers.12.mlp.experts.w13_weight, 1.5\n",
      "vllm_model.model.layers.12.mlp.experts.w13_bias, 3.85938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m val.shape == golden_state[key].shape, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgolden_state[key].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#assert jnp.allclose(golden_state[key].mean(), jnp.array(val, dtype=jnp.bfloat16).mean(), atol=1e-5, rtol=1e-5), f\"{key}, {np.max(val - golden_state[key])}\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgolden_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     12\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.max(val\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mgolden_state[key])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-rl/lib/python3.12/site-packages/numpy/_core/numeric.py:2329\u001b[39m, in \u001b[36mallclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_allclose_dispatcher)\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mallclose\u001b[39m(a, b, rtol=\u001b[32m1.e-5\u001b[39m, atol=\u001b[32m1.e-8\u001b[39m, equal_nan=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   2245\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2246\u001b[39m \u001b[33;03m    Returns True if two arrays are element-wise equal within a tolerance.\u001b[39;00m\n\u001b[32m   2247\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2327\u001b[39m \n\u001b[32m   2328\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2329\u001b[39m     res = \u001b[38;5;28mall\u001b[39m(\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builtins.bool(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv-rl/lib/python3.12/site-packages/numpy/_core/numeric.py:2447\u001b[39m, in \u001b[36misclose\u001b[39m\u001b[34m(a, b, rtol, atol, equal_nan)\u001b[39m\n\u001b[32m   2444\u001b[39m     y = \u001b[38;5;28mfloat\u001b[39m(y)\n\u001b[32m   2446\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2447\u001b[39m     result = (\u001b[43mless_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m-\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2448\u001b[39m               & isfinite(y)\n\u001b[32m   2449\u001b[39m               | (x == y))\n\u001b[32m   2450\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m equal_nan:\n\u001b[32m   2451\u001b[39m         result |= isnan(x) & isnan(y)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "missing_keys = golden_state.keys() - converted_state.keys()\n",
    "print(missing_keys)\n",
    "\n",
    "\n",
    "for key, val in converted_state.items():\n",
    "  # print(\n",
    "  #     f\"{golden_state[key].dtype}, {val.dtype} -------- converted state shape:{key} {val.shape} and golden_state[key] shape:  {golden_state[key].shape}\"\n",
    "  # )\n",
    "  assert val.shape == golden_state[key].shape, f\"{key}, {val.shape} {golden_state[key].shape}\"\n",
    "  #assert jnp.allclose(golden_state[key].mean(), jnp.array(val, dtype=jnp.bfloat16).mean(), atol=1e-5, rtol=1e-5), f\"{key}, {np.max(val - golden_state[key])}\"\n",
    "  if not jnp.allclose(golden_state[key], val, atol=1e-5, rtol=1e-5):\n",
    "    print(f\"{key}, {np.max(val - golden_state[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "        vllm_state_dict[f'{vllm_prefix}.mlp.experts.w13_bias'] = to_np_array(hf_state_dict[f'{hf_prefix}.mlp.experts.gate_up_proj_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83210ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vllm_model.model.layers.0.attn.rotary_emb.cos_sin_cache'}\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.lm_head.weight (201088, 2880) and golden_state[key] shape:  (201088, 2880)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.embedding.weight (201088, 2880) and golden_state[key] shape:  (201088, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.norm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.0.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.1.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.2.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.3.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.4.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.5.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.6.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.7.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.8.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.9.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.10.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.11.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.12.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.13.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.14.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.15.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.16.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.17.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.18.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.19.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.20.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.21.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.22.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.input_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.post_attention_layernorm.weight (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.attn.o_proj.weight (2880, 4096) and golden_state[key] shape:  (2880, 4096)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.attn.o_proj.bias (2880,) and golden_state[key] shape:  (2880,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.mlp.router.weight (32, 2880) and golden_state[key] shape:  (32, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.mlp.router.bias (32,) and golden_state[key] shape:  (32,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.attn.sinks (64,) and golden_state[key] shape:  (64,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.attn.qkv.weight (5120, 2880) and golden_state[key] shape:  (5120, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.attn.qkv.bias (5120,) and golden_state[key] shape:  (5120,)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.mlp.experts.w13_weight (32, 5760, 2880) and golden_state[key] shape:  (32, 5760, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.mlp.experts.w13_bias (32, 5760) and golden_state[key] shape:  (32, 5760)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.mlp.experts.w2_weight (32, 2880, 2880) and golden_state[key] shape:  (32, 2880, 2880)\n",
      "bfloat16, bfloat16 -------- converted state shape:vllm_model.model.layers.23.mlp.experts.w2_bias (32, 2880) and golden_state[key] shape:  (32, 2880)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|| 1/1 [00:00<00:00, 647.97it/s]\n",
      "Processed prompts:   0%|                                                                                                                                                 | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]/home/shuningjin_google_com/venv-rl/lib/python3.12/site-packages/torchax/tensor.py:154: UserWarning: Explicitly requested dtype int64 requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/jax-ml/jax#current-gotchas for more.\n",
      "  res = jax_function(self._elem, *args, **kwargs)\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn`t find tuned sizes for the RPA v3 kernel with %s ('TPU v5', 16, 'q_bfloat16_kv_bfloat16', 'q_head-16_kv_head-2_head-64', 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn`t find tuned sizes for the RPA v3 kernel with %s ('TPU v5', 16, 'q_bfloat16_kv_bfloat16', 'q_head-16_kv_head-2_head-64', 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "WARNING:torchax.tensor:In-place to .data modifications still results a copy on TPU\n",
      "I1114 10:11:33.491524  678723 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 4,491 instructions, modules: jit_step_fun\n",
      "I1114 10:11:33.491561  678723 2a886c8_compiler_base.cc:7217] Initial HLO module: jit_step_fun instructions: 4,491 fingerprint: \n",
      "I1114 10:11:33.504304  678723 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit_step_fun instructions: 3,871\n",
      "I1114 10:11:33.504321  678723 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "I1114 10:11:33.985398  679151 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 10:11:36.132684  678723 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1114 10:11:36.575476  678723 latency_hiding_scheduler.cc:3572] [latency-hiding-scheduler] LatencyHidingScheduler current memory usage: 1184104960 bytes. Current limit: 15705672089\n",
      "I1114 10:11:36.589466  678723 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 86138915225 bytes.\n",
      "I1114 10:11:36.589484  678723 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4306945761 bytes.\n",
      "I1114 10:11:36.818549  678723 tpu_plumb_vmem_info.cc:557] PursueNoOptimizationBaselineScavengingVmem is kicking in.\n",
      "I1114 10:11:36.855711  678723 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 3.365881917s\n",
      "I1114 10:11:36.899846  680045 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: T; bytes_for_msa_to_allocate: 67043328\n",
      "E1114 10:11:36.941014  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.24), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.24 to 67043328 bytes.\n",
      "E1114 10:11:36.941377  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.25), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.25 to 67043328 bytes.\n",
      "E1114 10:11:36.941404  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.26), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.26 to 67043328 bytes.\n",
      "E1114 10:11:36.941426  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.27), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.27 to 67043328 bytes.\n",
      "E1114 10:11:36.941447  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.28), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.28 to 67043328 bytes.\n",
      "E1114 10:11:36.941468  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.29), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.29 to 67043328 bytes.\n",
      "E1114 10:11:36.941488  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.30), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.30 to 67043328 bytes.\n",
      "E1114 10:11:36.941508  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.31), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.31 to 67043328 bytes.\n",
      "E1114 10:11:36.941529  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.32), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.32 to 67043328 bytes.\n",
      "E1114 10:11:36.941556  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.33), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.33 to 67043328 bytes.\n",
      "E1114 10:11:36.941577  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.34), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.34 to 67043328 bytes.\n",
      "E1114 10:11:36.941599  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.35), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.35 to 67043328 bytes.\n",
      "E1114 10:11:36.941620  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.36), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.36 to 67043328 bytes.\n",
      "E1114 10:11:36.941641  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.37), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.37 to 67043328 bytes.\n",
      "E1114 10:11:36.941663  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.38), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.38 to 67043328 bytes.\n",
      "E1114 10:11:36.941684  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.39), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.39 to 67043328 bytes.\n",
      "E1114 10:11:36.941706  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.40), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.40 to 67043328 bytes.\n",
      "E1114 10:11:36.941727  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.41), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.41 to 67043328 bytes.\n",
      "E1114 10:11:36.941750  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.42), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.42 to 67043328 bytes.\n",
      "E1114 10:11:36.941772  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.43), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.43 to 67043328 bytes.\n",
      "E1114 10:11:36.941793  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.44), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.44 to 67043328 bytes.\n",
      "E1114 10:11:36.941814  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.45), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.45 to 67043328 bytes.\n",
      "E1114 10:11:36.941836  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.46), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.46 to 67043328 bytes.\n",
      "E1114 10:11:36.941856  680045 memory_space_assignment_util.cc:1382] INVALID_ARGUMENT: 104857600 bytes of scoped Vmem requested (via backend config for RPA-HD_64-bq_16-bkvp_4-p_16.47), but the max valid bytes is 67043328. See go/scoped-vmem for more details.\n",
      "=== Source Location Trace: === \n",
      "platforms/xla/service/ba16c7433/memory_space_assignment_util.cc:178\n",
      " We are lowering the scoped Vmem for RPA-HD_64-bq_16-bkvp_4-p_16.47 to 67043328 bytes.\n",
      "W1114 10:11:39.190292  681267 pipeline_emitter.cc:1630] While compiling: %gmm.48 = bf16[64,1440]{1,0:T(8,128)(2,1)} custom-call(%param.0, %param.1, %param.2, %param.3, %param.4, /*index=5*/%param.5, %param.6), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={s32[], s32[33]{0}, s32[32]{0}, s32[32]{0}, s32[1]{0}, bf16[64,2880]{1,0}, bf16[32,1440,2880]{2,1,0}}, metadata={op_name=\"jit(step_fun)/jit(fused_moe_func_padded)/shard_map/jit(gmm)/pallas_call\" source_file=\"/home/shuningjin_google_com/venv-rl/lib/python3.12/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py\" source_line=547 source_end_line=547 source_column=8 source_end_column=8}, backend_config={\"flag_configs\":[],\"scoped_memory_configs\":[{\"memory_space\":\"1\",\"offset\":\"0\",\"size\":\"16777216\"}],\"custom_call_config\":{\"body\":\"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\",\"cost_estimate\":{\"flops\":\"530841600\",\"transcendentals\":\"0\",\"bytes_accessed\":\"265973760\",\"remote_bytes_transferred\":\"0\"},\"needs_layout_passes\":true,\"allow_input_fusion\":[],\"serialization_format\":\"1\",\"output_memory_colors\":[],\"output_memory_space_colors\":[],\"input_memory_space_colors\":[]},\"used_scoped_memory_configs\":[]}. Software pipelining is disabled since the VMEM resources required for pipelining exceeds the size of VMEM.\n",
      "W1114 10:11:39.441045  681236 pipeline_emitter.cc:1630] While compiling: %gmm.49 = bf16[64,2880]{1,0:T(8,128)(2,1)} custom-call(%param.0, %param.1, %param.2, %param.3, %param.4, /*index=5*/%param.5, %param.6), custom_call_target=\"tpu_custom_call\", operand_layout_constraints={s32[], s32[33]{0}, s32[32]{0}, s32[32]{0}, s32[1]{0}, bf16[64,720]{1,0}, bf16[32,2880,720]{2,1,0}}, metadata={op_name=\"jit(step_fun)/jit(fused_moe_func_padded)/shard_map/jit(gmm)/pallas_call\" source_file=\"/home/shuningjin_google_com/venv-rl/lib/python3.12/site-packages/jax/experimental/pallas/ops/tpu/megablox/gmm.py\" source_line=547 source_end_line=547 source_column=8 source_end_column=8}, backend_config={\"flag_configs\":[],\"scoped_memory_configs\":[{\"memory_space\":\"1\",\"offset\":\"0\",\"size\":\"16777216\"}],\"custom_call_config\":{\"body\":\"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\",\"cost_estimate\":{\"flops\":\"265420800\",\"transcendentals\":\"0\",\"bytes_accessed\":\"133263360\",\"remote_bytes_transferred\":\"0\"},\"needs_layout_passes\":true,\"allow_input_fusion\":[],\"serialization_format\":\"1\",\"output_memory_colors\":[],\"output_memory_space_colors\":[],\"input_memory_space_colors\":[]},\"used_scoped_memory_configs\":[]}. Software pipelining is disabled since the VMEM resources required for pipelining exceeds the size of VMEM.\n",
      "I1114 10:11:40.631040  679151 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 3.7314021075s\n",
      "I1114 10:11:41.097328  679151 2a886c8_compiler_base.cc:3045] final program bundle count: 165,421 note this count does not reflect cycles spent executing delays.\n",
      "I1114 10:11:41.175046  679151 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1114 10:11:41.286906  679151 2a886c8_compiler_base.cc:3310] Program divided into 100 overlays with HLO functions (10.49M).\n",
      "I1114 10:11:41.323999  679151 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit_step_fun\n",
      "I1114 10:11:41.324018  679151 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 1.13G / 95.74G\n",
      "I1114 10:11:41.324025  679151 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 63.94M / 64.00M\n",
      "I1114 10:11:41.324037  679151 2a886c8_compiler_base.cc:3549] Total hbm usage >= 77.50G:\n",
      "I1114 10:11:41.324039  679151 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1114 10:11:41.324040  679151 2a886c8_compiler_base.cc:3549]     program           1.13G \n",
      "I1114 10:11:41.324041  679151 2a886c8_compiler_base.cc:3549]     arguments        76.11G \n",
      "I1114 10:11:41.324043  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:11:41.324044  679151 2a886c8_compiler_base.cc:3549] Output size 65.60G; shares 65.60G with arguments.\n",
      "I1114 10:11:41.324045  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:11:41.324046  679151 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1114 10:11:41.324047  679151 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1114 10:11:41.324049  679151 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1114 10:11:41.324050  679151 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1114 10:11:41.324051  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:11:41.324052  679151 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1114 10:11:41.324053  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:11:41.324077  679151 2a886c8_compiler_base.cc:3553] Program sflag requirement 312B:\n",
      "I1114 10:11:41.324078  679151 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1114 10:11:41.324079  679151 2a886c8_compiler_base.cc:3553]     scoped              44B\n",
      "I1114 10:11:41.324080  679151 2a886c8_compiler_base.cc:3553]     HLO temp            64B (100.0% utilization: Unpadded (64B) Padded (64B), 0.0% fragmentation (0B))\n",
      "I1114 10:11:41.324081  679151 2a886c8_compiler_base.cc:3553] Program hbm requirement 1.13G:\n",
      "I1114 10:11:41.324082  679151 2a886c8_compiler_base.cc:3553]     global            3.02M\n",
      "I1114 10:11:41.324083  679151 2a886c8_compiler_base.cc:3553]     HLO temp          1.12G (97.8% utilization: Unpadded (1.08G) Padded (1.10G), 1.4% fragmentation (15.84M))\n",
      "I1114 10:11:41.324084  679151 2a886c8_compiler_base.cc:3553]     overlays         10.49M\n",
      "I1114 10:11:41.324085  679151 2a886c8_compiler_base.cc:3553] Program vmem requirement 63.94M:\n",
      "I1114 10:11:41.324087  679151 2a886c8_compiler_base.cc:3553]     HLO temp         63.94M (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (63.94M))\n",
      "I1114 10:11:41.324088  679151 2a886c8_compiler_base.cc:3553] Program smem requirement 9.2K:\n",
      "I1114 10:11:41.324089  679151 2a886c8_compiler_base.cc:3553]     scoped             9.2K\n",
      "I1114 10:11:41.324091  679151 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1114 10:11:41.324092  679151 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 12.3K / 1.00M (321 parameters)\n",
      "I1114 10:11:41.324118  679151 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 692.98844925ms\n",
      "I1114 10:11:41.357624  679151 isa_program_util_common.cc:510] (HLO module jit_step_fun): Executable fingerprint:d752be8ab4e59c91df65d6cbe194efc280e005d29bb91c1f4cd062693ac4e350\n",
      "I1114 10:11:41.357638  679151 isa_program_util_common.cc:514] (HLO module jit_step_fun): Executable fingerprint (including data segments):d50b7d94fa64675dde0b5e67ede77f377b24610b8fbf73c0995893c81b31e3b2\n",
      "I1114 10:11:41.357641  679151 isa_program_util_common.cc:517] (HLO module jit_step_fun): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 10:11:41.357779  678723 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 7.87559138575s\n",
      "I1114 10:11:41.378388  678723 tpu_pjrt_executable.cc:361] Source URI is : googlefile:/google_src/files/804429027/depot/branches/libtpu_lts_release_branch/804273004.1/OVERLAY_READONLY/g3     \n",
      "I1114 10:12:55.055188  678723 2a886c8_compiler_base.cc:7160] XLA::TPU running hlo passes for 18 instructions, modules: jit__select_from_array_fn\n",
      "I1114 10:12:55.055225  678723 2a886c8_compiler_base.cc:7217] Initial HLO module: jit__select_from_array_fn instructions: 18 fingerprint: \n",
      "I1114 10:12:55.055497  678723 2a886c8_compiler_base.cc:7289] HLO optimizing module: jit__select_from_array_fn instructions: 18\n",
      "I1114 10:12:55.055501  678723 2a886c8_compiler_base.cc:7304] XLA::TPU HLO optimization\n",
      "W1114 10:12:55.056195  679151 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 10:12:55.057778  679151 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 10:12:55.067198  678723 2a886c8_compiler_base.cc:6277] XLA::TPU HLO PostOptimizationPipeline\n",
      "I1114 10:12:55.067954  678723 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97662863769 bytes.\n",
      "I1114 10:12:55.067963  678723 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4883143188 bytes.\n",
      "I1114 10:12:55.068355  678723 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 14.82301425ms\n",
      "I1114 10:12:55.069185  680045 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 10:12:55.099507  679151 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 30.49532675ms\n",
      "I1114 10:12:55.102578  679151 2a886c8_compiler_base.cc:3045] final program bundle count: 784 note this count does not reflect cycles spent executing delays.\n",
      "I1114 10:12:55.108135  679151 2a886c8_compiler_base.cc:3045] final program bundle count: 273 note this count does not reflect cycles spent executing delays.\n",
      "I1114 10:12:55.111017  679151 2a886c8_compiler_base.cc:3310] Program divided into 2 overlays without HLO functions (86.0K).\n",
      "I1114 10:12:55.111525  679151 2a886c8_compiler_base.cc:3486] XLA::TPU module name: jit__select_from_array_fn\n",
      "I1114 10:12:55.111551  679151 2a886c8_compiler_base.cc:3488] XLA::TPU program HBM usage: 86.0K / 95.74G\n",
      "I1114 10:12:55.111557  679151 2a886c8_compiler_base.cc:3538] XLA::TPU program VMEM usage: 418.5K / 64.00M\n",
      "I1114 10:12:55.111571  679151 2a886c8_compiler_base.cc:3549] Total hbm usage >= 263.18M:\n",
      "I1114 10:12:55.111573  679151 2a886c8_compiler_base.cc:3549]     reserved        263.00M \n",
      "I1114 10:12:55.111575  679151 2a886c8_compiler_base.cc:3549]     program           86.0K \n",
      "I1114 10:12:55.111576  679151 2a886c8_compiler_base.cc:3549]     arguments         92.5K \n",
      "I1114 10:12:55.111577  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:12:55.111578  679151 2a886c8_compiler_base.cc:3549] Output size 46.0K; shares 0B with arguments.\n",
      "I1114 10:12:55.111579  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:12:55.111580  679151 2a886c8_compiler_base.cc:3549] Total host usage >= 0B:\n",
      "I1114 10:12:55.111581  679151 2a886c8_compiler_base.cc:3549]     reserved             0B \n",
      "I1114 10:12:55.111583  679151 2a886c8_compiler_base.cc:3549]     program         unknown size \n",
      "I1114 10:12:55.111584  679151 2a886c8_compiler_base.cc:3549]     arguments            0B \n",
      "I1114 10:12:55.111585  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:12:55.111586  679151 2a886c8_compiler_base.cc:3549] Output size 0B; shares 0B with arguments.\n",
      "I1114 10:12:55.111587  679151 2a886c8_compiler_base.cc:3549] \n",
      "I1114 10:12:55.111599  679151 2a886c8_compiler_base.cc:3553] Program sflag requirement 220B:\n",
      "I1114 10:12:55.111600  679151 2a886c8_compiler_base.cc:3553]     reserved           204B\n",
      "I1114 10:12:55.111601  679151 2a886c8_compiler_base.cc:3553]     scoped               8B\n",
      "I1114 10:12:55.111602  679151 2a886c8_compiler_base.cc:3553]     HLO temp             8B (100.0% utilization: Unpadded (8B) Padded (8B), 0.0% fragmentation (0B))\n",
      "I1114 10:12:55.111604  679151 2a886c8_compiler_base.cc:3553] Program vmem requirement 418.5K:\n",
      "I1114 10:12:55.111605  679151 2a886c8_compiler_base.cc:3553]     scoped           322.0K\n",
      "I1114 10:12:55.111606  679151 2a886c8_compiler_base.cc:3553]     HLO temp          96.5K (0.0% utilization: Unpadded (0B) Padded (0B), 100.0% fragmentation (96.5K))\n",
      "I1114 10:12:55.111607  679151 2a886c8_compiler_base.cc:3553] Program smem requirement 516B:\n",
      "I1114 10:12:55.111608  679151 2a886c8_compiler_base.cc:3553]     scoped             516B\n",
      "I1114 10:12:55.111609  679151 2a886c8_compiler_base.cc:3553] Program host requirement 0B:\n",
      "I1114 10:12:55.111610  679151 2a886c8_compiler_base.cc:3553] Program hbm requirement 86.0K:\n",
      "I1114 10:12:55.111611  679151 2a886c8_compiler_base.cc:3553]     overlays          86.0K\n",
      "I1114 10:12:55.111613  679151 2a886c8_compiler_base.cc:3561] XLA::TPU program SMEM usage: 2.4K / 1.00M (2 parameters)\n",
      "I1114 10:12:55.111633  679151 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 12.0485315ms\n",
      "I1114 10:12:55.111783  679151 isa_program_util_common.cc:510] (HLO module jit__select_from_array_fn): Executable fingerprint:ad982480c1ede5c6e4d1401077e5fb4968282c2c0e6de86e40d5314d4a7de492\n",
      "I1114 10:12:55.111786  679151 isa_program_util_common.cc:514] (HLO module jit__select_from_array_fn): Executable fingerprint (including data segments):7aab9dbb5ef25276fe3f00cce0ec50be187e3ed8a0620b5e5cc9091aed342381\n",
      "I1114 10:12:55.111788  679151 isa_program_util_common.cc:517] (HLO module jit__select_from_array_fn): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 10:12:55.111907  678723 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 58.48750425ms\n",
      "W1114 10:12:59.651665  679151 tpu_spmd_partitioner.cc:413] Collective matmul V1 is enabled. Please consider migrating to V2 since V1 will be deprecated soon. You can enable V2 by setting xla_tpu_all_gather_collective_matmul_mode=post_spmd and xla_tpu_reduce_scatter_collective_matmul_mode=post_spmd.\n",
      "I1114 10:12:59.653137  679151 shardy_xla_pass.cc:316] Using Shardy for XLA SPMD propagation.\n",
      "I1114 10:12:59.659358  678723 sparse_core_compiler.cc:377] SparseCore compiler applying memory pressure reduction threshold to 5% of max shared memory of 97373344665 bytes.\n",
      "I1114 10:12:59.659376  678723 sparse_core_compiler.cc:383] SparseCore compiler initializing memory limit to 4868667233 bytes.\n",
      "I1114 10:12:59.659635  678723 2a886c8_compiler_base.cc:7363] HLO_PASSES stage duration: 9.99799625ms\n",
      "I1114 10:12:59.660291  681292 memory_space_assignment_util.cc:730] MSA Vmem breakdown: default scoped vmem (from flag): 16777216; does_msa_directly_allocate_scoped_vmem: F; bytes_for_msa_to_allocate: 50266112\n",
      "I1114 10:12:59.893610  679151 2a886c8_compiler_base.cc:10636] BACKEND_PASSES stage duration: 233.475428ms\n",
      "I1114 10:12:59.915615  679151 2a886c8_compiler_base.cc:3595] CODE_GENERATION stage duration: 21.9608525ms\n",
      "I1114 10:12:59.915893  679151 isa_program_util_common.cc:510] (HLO module jit_compute_logits_func): Executable fingerprint:e19e9886fe55b7bd828cd7e2e82d3e6bfbe8731ee83a4ab23597d8ade2618d93\n",
      "I1114 10:12:59.915898  679151 isa_program_util_common.cc:514] (HLO module jit_compute_logits_func): Executable fingerprint (including data segments):3365a5a4bfbd0be1a80df542bd3a6e601d4cf7856e56b0b83ba45711153e31fb\n",
      "I1114 10:12:59.915900  679151 isa_program_util_common.cc:517] (HLO module jit_compute_logits_func): Host transfer fingerprint:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "I1114 10:12:59.916001  678723 2a886c8_compiler_base.cc:7788] END_TO_END stage duration: 266.43628525ms\n",
      "Processed prompts: 100%|| 1/1 [08:25<00:00, 505.40s/it, est. speed input: 0.01 toks/s, output: 0.03 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='what is the capital of France?', prompt_token_ids=[13347, 382, 290, 9029, 328, 10128, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' penetr Est/D OP-115aw-1  to To Bur...]?', token_ids=[38043, 457, 302, 26018, 19109, 12, 11999, 1134, 12, 16, 220, 316, 2514, 15602, 64233, 30], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "missing_keys = golden_state.keys() - converted_state.keys()\n",
    "print(missing_keys)\n",
    "\n",
    "\n",
    "for key, val in converted_state.items():\n",
    "  print(\n",
    "      f\"{golden_state[key].dtype}, {val.dtype} -------- converted state shape:{key} {val.shape} and golden_state[key] shape:  {golden_state[key].shape}\"\n",
    "  )\n",
    "  assert val.shape == golden_state[key].shape, f\"{key}, {val.shape} {golden_state[key].shape}\"\n",
    "  golden_state[key] = val\n",
    "\n",
    "del converted_state\n",
    "del model\n",
    "\n",
    "print(golden_llm.generate(\"what is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0866c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-0.0234375 <class 'numpy.ndarray'> bfloat16\n",
      "-0.0344238 <class 'jaxlib._jax.ArrayImpl'> bfloat16\n",
      "-0.0344238 <class 'jaxlib._jax.ArrayImpl'> bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(np.max(golden_state[key] - val))\n",
    "print(val.mean(), type(val), val.dtype)\n",
    "print(golden_state[key].mean(), type(golden_state[key]), golden_state[key].dtype)\n",
    "val_jnp = jnp.array(val)\n",
    "print(val_jnp.mean(), type(val_jnp), val_jnp.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65879a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl",
   "language": "python",
   "name": "venv-rl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
