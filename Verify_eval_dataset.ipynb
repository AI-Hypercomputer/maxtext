{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8bae54-9fda-491e-b72d-daf9c47e403d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lizhiyu/maxtext/MaxText\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhiyu/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd MaxText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cb0c82-dace-4ed0-b52b-fdfb77c9f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\n"
     ]
    }
   ],
   "source": [
    "!ls /home/lizhiyu/maxtext/assets/tokenizer.mistral-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362cec42-d8c4-40f5-a10e-91568e47b370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:52.433167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736498092.448828 1100032 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736498092.453477 1100032 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: ['run_name', 'model_name', 'enable_checkpointing', 'async_checkpointing', 'remat_policy', 'attention', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'dataset_type', 'dataset_path', 'dataset_name', 'eval_dataset_name', 'steps', 'max_target_length', 'eval_interval']\n",
      "Running Model: mixtral-8x22b\n",
      "Updating following parameters in config\n",
      "\n",
      "base_emb_dim: 6144\n",
      "base_num_query_heads: 48\n",
      "base_num_kv_heads: 8\n",
      "base_mlp_dim: 16384\n",
      "base_num_decoder_layers: 56\n",
      "head_dim: 128\n",
      "mlp_activations: ['silu', 'linear']\n",
      "vocab_size: 32768\n",
      "enable_dropout: False\n",
      "logits_via_embedding: False\n",
      "normalization_layer_epsilon: 1e-05\n",
      "num_experts: 8\n",
      "num_experts_per_tok: 2\n",
      "rope_max_timescale: 1000000\n",
      "decoder_block: mistral\n",
      "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'enable_dropout', 'logits_via_embedding', 'normalization_layer_epsilon', 'num_experts', 'num_experts_per_tok', 'rope_max_timescale', 'decoder_block']\n",
      "Override add_bos and add_eos to False when dataset_type=c4_mlperf\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param add_bos: False\n",
      "Config param add_eos: False\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: False\n",
      "Config param attention: flash\n",
      "Config param attention_type: global\n",
      "Config param attn_logits_soft_cap: None\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 6144\n",
      "Config param base_mlp_dim: 16384\n",
      "Config param base_num_decoder_layers: 56\n",
      "Config param base_num_kv_heads: 8\n",
      "Config param base_num_query_heads: 48\n",
      "Config param base_output_directory: /tmp/\n",
      "Config param capacity_factor: -1.0\n",
      "Config param cast_logits_to_fp32: True\n",
      "Config param checkpoint_dir: /tmp/mixtral-mlperf/checkpoints/\n",
      "Config param checkpoint_is_quantized: False\n",
      "Config param checkpoint_period: 10000\n",
      "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
      "Config param checkpoint_storage_use_ocdbt: True\n",
      "Config param checkpoint_storage_use_zarr3: True\n",
      "Config param collect_stack_trace: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param context: remat\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param custom_mesh: \n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 0\n",
      "Config param dataset_name: c4/en:3.0.4\n",
      "Config param dataset_path: gs://mlperf-llm-public2\n",
      "Config param dataset_type: c4_mlperf\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param dcn_tensor_sequence_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: mistral\n",
      "Config param decoder_layer_input: device\n",
      "Config param dpo_beta: 0.1\n",
      "Config param dpo_label_smoothing: 0.0\n",
      "Config param dropout_rate: 0.0\n",
      "Config param dtype: bfloat16\n",
      "Config param dump_hlo: False\n",
      "Config param dump_hlo_delete_local_after: True\n",
      "Config param dump_hlo_gcs_dir: \n",
      "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
      "Config param dump_hlo_module_name: jit_train_step\n",
      "Config param dump_hlo_upload_all: False\n",
      "Config param dump_hlo_xla_flags: \n",
      "Config param emb_dim: 6144\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpointing: False\n",
      "Config param enable_data_shuffling: True\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_goodput_recording: True\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_model_warmup: False\n",
      "Config param enable_pathways_goodput: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param eval_data_columns: ['text']\n",
      "Config param eval_dataset_name: c4/en:3.0.4\n",
      "Config param eval_interval: 1\n",
      "Config param eval_per_device_batch_size: 6.0\n",
      "Config param eval_split: validation\n",
      "Config param eval_steps: -1\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param final_logits_soft_cap: None\n",
      "Config param force_unroll: False\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param global_batch_size_to_eval_on: 24\n",
      "Config param global_batch_size_to_load: 24\n",
      "Config param global_batch_size_to_load_eval: 24\n",
      "Config param global_batch_size_to_train_on: 24\n",
      "Config param global_parameter_scale: 1\n",
      "Config param goodput_upload_interval_seconds: 60\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_eval_files: \n",
      "Config param grain_train_files: \n",
      "Config param grain_worker_count: 1\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param hf_access_token: \n",
      "Config param hf_data_dir: \n",
      "Config param hf_eval_files: \n",
      "Config param hf_eval_split: \n",
      "Config param hf_path: \n",
      "Config param hf_train_files: \n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1]\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param ici_tensor_sequence_parallelism: 1\n",
      "Config param inference_benchmark_test: False\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param inference_server: MaxtextInterleavedServer\n",
      "Config param init_weights_seed: 0\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_debug_log_modules: \n",
      "Config param jax_distributed_initialization_timeout: 300\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param key_proj: remat\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param kv_quant_dtype: int8\n",
      "Config param learning_rate: 3e-05\n",
      "Config param learning_rate_schedule_steps: 10\n",
      "Config param load_balance_loss_weight: 0.01\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: \n",
      "Config param local_checkpoint_directory: \n",
      "Config param local_checkpoint_period: 0\n",
      "Config param log_config: True\n",
      "Config param log_period: 100\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'sequence', 'tensor_sequence')), ('activation_kv_heads', ('tensor', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence',)), ('activation_norm_length', ('tensor_sequence', 'sequence')), ('activation_embed', 'tensor'), ('activation_mlp', ('tensor', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_sequence')), ('activation_vocab', ('tensor', 'sequence', 'tensor_sequence')), ('activation_vocab', 'tensor'), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', 'sequence'), ('activation_stage', 'stage'), ('activation_exp', 'expert'), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'expert')), ('embed', ('fsdp', 'sequence', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence')), ('embed_no_exp', ('fsdp', 'sequence')), ('norm', ('tensor', 'tensor_sequence')), ('q_heads', ('tensor', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_sequence', 'autoregressive')), ('layers', 'stage'), ('kv', ()), ('kv_heads', ('tensor', 'tensor_sequence', 'autoregressive')), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 32768\n",
      "Config param megablox: True\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'tensor_sequence', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: /tmp/mixtral-mlperf/metrics/\n",
      "Config param metrics_file: \n",
      "Config param micro_batch_size_to_eval_on: 24\n",
      "Config param micro_batch_size_to_train_on: 24\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 16384\n",
      "Config param mlpwi: remat\n",
      "Config param mlpwi_0: remat\n",
      "Config param mlpwi_1: remat\n",
      "Config param mlpwo: remat\n",
      "Config param model_call_mode: \n",
      "Config param model_name: mixtral-8x22b\n",
      "Config param monitor_goodput: True\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_decoder_layers: 56\n",
      "Config param num_experts: 8\n",
      "Config param num_experts_per_tok: 2\n",
      "Config param num_kv_heads: 8\n",
      "Config param num_layers_per_pipeline_stage: 1\n",
      "Config param num_pipeline_microbatches: -1\n",
      "Config param num_pipeline_repeats: -1\n",
      "Config param num_query_heads: 48\n",
      "Config param num_slices: 1\n",
      "Config param opt_type: adamw\n",
      "Config param optimizer_memory_host_offload: False\n",
      "Config param out_proj: remat\n",
      "Config param param_scan_axis: 1\n",
      "Config param per_device_batch_size: 6.0\n",
      "Config param pipeline_delay_activation_forwarding: False\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param profile_cleanly: True\n",
      "Config param profile_periodically_period: -1\n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param qkv_proj: remat\n",
      "Config param quant_cfg_path: \n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_proj: remat\n",
      "Config param ragged_block_size: 256\n",
      "Config param record_internal_nn_metrics: 0\n",
      "Config param remat_policy: full\n",
      "Config param replicate_quant_scale: False\n",
      "Config param replicator_backup_interval_minutes: 0\n",
      "Config param reshape_q: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_max_timescale: 1000000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param run_name: mixtral-mlperf\n",
      "Config param sa_block_kv: 512\n",
      "Config param sa_block_kv_compute: 512\n",
      "Config param sa_block_kv_dkv: 512\n",
      "Config param sa_block_kv_dkv_compute: 512\n",
      "Config param sa_block_kv_dq: 512\n",
      "Config param sa_block_q: 512\n",
      "Config param sa_block_q_dkv: 512\n",
      "Config param sa_block_q_dq: 512\n",
      "Config param sa_k_layout: HEAD_DIM_MINOR\n",
      "Config param sa_q_layout: HEAD_DIM_MINOR\n",
      "Config param sa_use_fused_bwd_kernel: False\n",
      "Config param sa_v_layout: HEAD_DIM_MINOR\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_quantized_params_path: \n",
      "Config param scan_layers: True\n",
      "Config param scan_pipeline_iterations: True\n",
      "Config param set_remat_policy_on_layers_per_stage: False\n",
      "Config param set_remat_policy_on_pipeline_iterations: True\n",
      "Config param sharding_tolerance: 0.02\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param skip_jax_distributed_system: False\n",
      "Config param sliding_window_size: 0\n",
      "Config param stack_prefill_result_cache: False\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param steps: 10\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param tensorboard_dir: /tmp/mixtral-mlperf/tensorboard/\n",
      "Config param tokenize_eval_data: True\n",
      "Config param tokenize_train_data: True\n",
      "Config param tokenizer_path: /home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\n",
      "Config param train_data_columns: ['text']\n",
      "Config param trainable_position_size: -1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_dpo: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_post_attn_norm: False\n",
      "Config param use_post_ffw_norm: False\n",
      "Config param use_ragged_attention: False\n",
      "Config param use_replicator_service: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param value_proj: remat\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vocab_size: 32768\n",
      "Config param warmup_steps_fraction: 0.1\n",
      "Config param weight_dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import pyconfig\n",
    "import os\n",
    "\n",
    "\n",
    "argv = [\n",
    "    'MaxText/train.py', '/home/lizhiyu/maxtext/MaxText/configs/base.yml',  # base arg\n",
    "    \"run_name=mixtral-mlperf\",\n",
    "    \"model_name=mixtral-8x22b\",\n",
    "    \"steps=10\",\n",
    "    \"per_device_batch_size=6\",\n",
    "    \"enable_checkpointing=false\", \n",
    "    \"async_checkpointing=false\",\n",
    "    \"remat_policy=full\",\n",
    "    \"dataset_path=gs://mlperf-llm-public2\",\n",
    "    \"dataset_name=c4/en:3.0.4\", \n",
    "    \"eval_dataset_name=c4/en:3.0.4\",  # c4/en:3.0.4 is the untokenized eval dataset\n",
    "    \"max_target_length=32768\", \n",
    "    \"base_output_directory=/tmp/\",\n",
    "    \"dataset_type=c4_mlperf\",\n",
    "    \"tokenizer_path=/home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\",\n",
    "    \"attention=flash\",\n",
    "    \"eval_interval=1\",\n",
    "    ]\n",
    "pyconfig.initialize(argv)\n",
    "cfg = pyconfig.config\n",
    "config = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1267d697-0b79-4339-baeb-eef9657b2f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TFDS_DATA_DIR=gs://mlperf-llm-public2\n"
     ]
    }
   ],
   "source": [
    "%env TFDS_DATA_DIR={cfg.dataset_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1106e617-bf1c-4892-b7ca-9212a8cacb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhiyu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_devices: 4, shape (1, 1, 4, 1, 1, 1, 1, 1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:58.250825: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer path: /home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\n",
      "Tokenizer path: /home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer path: /home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\n",
      "Tokenizer path: /home/lizhiyu/maxtext/assets/tokenizer.mistral-v1\n",
      "Eval data has 388 local entries, padding now with 20 extra entries to get 17 batches.\n",
      "total_weights=Array(12694503, dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from input_pipeline.input_pipeline_interface import create_data_iterator\n",
    "from max_utils import create_device_mesh\n",
    "from jax.sharding import Mesh\n",
    "import jax.numpy as jnp\n",
    "\n",
    "devices_array = create_device_mesh(cfg)\n",
    "mesh = Mesh(devices_array, cfg.mesh_axes)\n",
    "\n",
    "train_iter, eval_iter = create_data_iterator(config, mesh)\n",
    "\n",
    "total_weights = 0\n",
    "for i, batch in enumerate(eval_iter):\n",
    "    total_weights += jnp.sum(batch[\"targets_segmentation\"] != 0)\n",
    "\n",
    "print(f\"{total_weights=}\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3225dbd-6103-4c79-8403-a1001d1f4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/suexu1025/MoE_study/blob/main/clm/clm_datasets.py#L250-L265\n",
    "# 'eval/num_tokens': 12694891: total number of tokens in eval dataset\n",
    "# 'eval/total_weights': 12694503: total number of next tokens prediction in eval dataset\n",
    "assert total_weights == 12694503"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
