{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBmRfde4Kgv4"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Hypercomputer/maxtext/blob/main/src/maxtext/examples/sft_llama3_demo.ipynb)\n",
        "\n",
        "# Llama3.1-8B-Instruct Supervised Fine-Tuning (SFT) Demo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8QDt1zoKgv4"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to perform Supervised Fine-Tuning (SFT) on Llama3.1-8B-Instruct using the Hugging Face ultrachat_200k dataset with MaxText and Tunix integration for efficient training.\n",
        "\n",
        "This notebook can run on **TPU v6e-8** or **v5p-8**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDLlkqKJKgv4"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Change Runtime Type (only if running on Google Colab)\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Change runtime type** from the dropdown menu.\n",
        "4.  Select **v6e-8** or **v5p-8 TPU** as the **Hardware accelerator**.\n",
        "5. Click on **Save**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2WAe9JKgv4"
      },
      "source": [
        "### Get Your Hugging Face Token\n",
        "\n",
        "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
        "\n",
        "**Follow these steps to get your token:**\n",
        "\n",
        "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
        "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "\n",
        "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
        "\n",
        "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
        "\n",
        "4.  **Copy the generated token**. You will need this in the later steps.\n",
        "\n",
        "**Follow these steps to store your token (only if running on Google Colab):**\n",
        "\n",
        "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
        "\n",
        "2. Click **\"+ Add new secret\"**.\n",
        "\n",
        "3. Set the Name as **HF_TOKEN**.\n",
        "\n",
        "4. Paste your token into the Value field.\n",
        "\n",
        "5. Ensure the Notebook access toggle is turned On."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__TGXwlkKgv4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from google.colab import userdata\n",
        "  print(\"Running the notebook on Google Colab\")\n",
        "  IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"Running the notebook on Visual Studio or JupyterLab\")\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuvt9qDPKgv4"
      },
      "source": [
        "### Installation: MaxText & Other Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh72UTvUKgv4"
      },
      "source": [
        "**‚ö†Ô∏è Note:** The installation process in following cell may take a few minutes to complete. Please be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGhCeyj6Kgv4"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    !git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
        "    %cd /content/maxtext\n",
        "\n",
        "    # Install uv, a fast Python package installer\n",
        "    !pip install uv\n",
        "\n",
        "    # Install MaxText and its dependencies\n",
        "    !uv pip install -e .[tpu] --resolution=lowest\n",
        "    !python3 -m MaxText.install_maxtext_extra_deps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xywQLus-Kgv5"
      },
      "source": [
        "### Restart Session (only if running on Google Colab)\n",
        "To apply certain changes, you need to restart the session.\n",
        "\n",
        "**Instructions:**\n",
        "1.  Navigate to the menu at the top of the screen.\n",
        "2.  Click on **Runtime**.\n",
        "3.  Select **Restart session** from the dropdown menu.\n",
        "\n",
        "You will be asked to confirm the action in a pop-up dialog. Click on **Yes**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_w4iLJyKgv5"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXLzWnjUKgv5",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "from maxtext.configs import pyconfig\n",
        "from maxtext.utils.globals import MAXTEXT_PKG_DIR\n",
        "from maxtext.trainers.post_train.sft import train_sft\n",
        "import jax\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "print(f\"MaxText installation path: {MAXTEXT_PKG_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goXDCIB4Kgv5"
      },
      "outputs": [],
      "source": [
        "if not jax.distributed.is_initialized():\n",
        "  jax.distributed.initialize()\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAX devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzMQJ6RXKgv5"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "else:\n",
        "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
        "\n",
        "# If not found in the environment, prompt the user for input securely\n",
        "# getpass function ensures the token is hidden while you type\n",
        "if not HF_TOKEN:\n",
        "    from getpass import getpass\n",
        "    HF_TOKEN = getpass(\"Hugging Face token not found in environment. Please enter it here: \")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Authenticated with Hugging Face successfully!\")\n",
        "else:\n",
        "    print(\"Authentication failed: Hugging Face token is not set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2evOnbOKgv5"
      },
      "source": [
        "## Model Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdvqPIB_Kgv5"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"llama3.1-8b\"\n",
        "TOKENIZER_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# set the path to the model checkpoint or leave empty to download from HuggingFace\n",
        "MODEL_CHECKPOINT_PATH = \"\"\n",
        "if not MODEL_CHECKPOINT_PATH:\n",
        "   MODEL_CHECKPOINT_PATH = f\"{MAXTEXT_PKG_DIR}/llama_checkpoint\"\n",
        "   print(\"Model checkpoint will be downloaded from HuggingFace at: \",  MODEL_CHECKPOINT_PATH)\n",
        "   print(\"Set MODEL_CHECKPOINT_PATH if you do not wish to download the checkpoint.\")\n",
        "\n",
        "BASE_OUTPUT_DIRECTORY = f\"{MAXTEXT_PKG_DIR}/sft_llama3_output\"\n",
        "RUN_NAME = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8hpE43wKgv5"
      },
      "source": [
        "## Download Llama3.1-8B Model Checkpoint from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uupzLQl5Kgv5"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    # Install torch for the conversion script\n",
        "    print(\"Installing torch...\")\n",
        "    subprocess.run(\n",
        "        [\n",
        "            sys.executable, \"-m\", \"pip\", \"install\",\n",
        "            \"torch\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"\n",
        "        ],\n",
        "        check=True\n",
        "    )\n",
        "\n",
        "    # Run checkpoint conversion with environment variables\n",
        "    print(\"Converting checkpoint from HuggingFace...\")\n",
        "    env = os.environ.copy()\n",
        "    env[\"JAX_PLATFORMS\"] = \"cpu\"\n",
        "    # env[\"PYTHONPATH\"] = MAXTEXT_PKG_DIR\n",
        "\n",
        "    subprocess.run(\n",
        "        [\n",
        "            sys.executable,\n",
        "            \"-m\", \"maxtext.checkpoint_conversion.to_maxtext\",\n",
        "            f\"{MAXTEXT_PKG_DIR}/configs/base.yml\",\n",
        "            f\"model_name={MODEL_NAME}\",\n",
        "            f\"base_output_directory={MODEL_CHECKPOINT_PATH}\",\n",
        "            f\"hf_access_token={HF_TOKEN}\",\n",
        "            \"use_multimodal=false\",\n",
        "            \"scan_layers=true\",\n",
        "            \"skip_jax_distributed_system=True\",\n",
        "        ],\n",
        "        check=True,\n",
        "        env=env\n",
        "    )\n",
        "\n",
        "if not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
        "    raise ValueError(\"Model checkpoint conversion failed. Check the logs above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFlbPuOAKgv6"
      },
      "source": [
        "## MaxText Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In-jdp1AAwrL"
      },
      "outputs": [],
      "source": [
        "# Load configuration for SFT training\n",
        "config_argv = [\n",
        "    \"\",\n",
        "    f\"{MAXTEXT_PKG_DIR}/configs/post_train/sft.yml\",\n",
        "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}/0/items\",\n",
        "    f\"model_name={MODEL_NAME}\",\n",
        "    \"steps=100\",\n",
        "    \"per_device_batch_size=1\",\n",
        "    \"max_target_length=1024\",\n",
        "    \"learning_rate=2.0e-5\",\n",
        "    \"weight_dtype=bfloat16\",\n",
        "    \"dtype=bfloat16\",\n",
        "    \"hf_path=HuggingFaceH4/ultrachat_200k\",\n",
        "    f\"hf_access_token={HF_TOKEN}\",\n",
        "    f\"base_output_directory={BASE_OUTPUT_DIRECTORY}\",\n",
        "    f\"run_name={RUN_NAME}\",\n",
        "    f\"tokenizer_path={TOKENIZER_PATH}\",\n",
        "    \"profiler=xplane\",\n",
        "]\n",
        "\n",
        "config = pyconfig.initialize(config_argv)\n",
        "\n",
        "print(\"‚úì SFT configuration loaded:\")\n",
        "print(f\" Model: {config.model_name}\")\n",
        "print(f\" Training Steps: {config.steps}\")\n",
        "print(f\" Output Directory: {config.base_output_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFSQOH3CKgv6"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgwpNgQYCJEd"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"üöÄ Starting SFT Training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    trainer, mesh = train_sft.train(config)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ Training Completed Successfully!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìÅ Checkpoints saved to: {config.checkpoint_dir}\")\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚ùåTraining Failed!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzW1NXX6Kgv6"
      },
      "source": [
        "## üìö Learn More\n",
        "\n",
        "- **CLI Usage**: https://maxtext.readthedocs.io/en/latest/tutorials/posttraining/sft.html\n",
        "- **Configuration**: See `src/maxtext/configs/post_train/sft.yml` for all available options\n",
        "- **Documentation**: Check `src/MaxText/sft/sft_trainer.py` for the `sft_train` function implementation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
