{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7687de92-1dfb-4237-b663-30cda55dc8e1",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning of Llama 3.1-8B on NVIDIA GPUs with JAX and MaxText\n",
    "\n",
    "This tutorial walks you through supervised fine-tuning (SFT) of Llama 3.1-8B on NVIDIA GPUs using JAX and MaxText. You'll learn how to take a pretrained Llama checkpoint, convert it into MaxText's native format, configure an SFT training run, and verify the result with a quick inference test.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Set up the environment and authenticate with Hugging Face\n",
    "2. Download and convert the Llama 3.1-8B checkpoint to MaxText format\n",
    "3. Configure and launch supervised fine-tuning on the UltraChat 200k dataset\n",
    "4. Visualize training metrics with TensorBoard\n",
    "5. Run a quick inference sanity check\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Make sure you have supported hardware\n",
    "\n",
    "**Hardware requirements.** Full-parameter SFT of Llama 3.1-8B is memory-intensive due to optimizer state, activations, and sharded model parameters. We recommend a system with **8 NVIDIA GPUs with at least 80 GB of memory each** (e.g., A100-80GB, H100-80GB, or H200). This allows the model, optimizer state, and activations to be cleanly sharded across devices without aggressive memory tuning.\n",
    "\n",
    "When running `nvidia-smi`, you should see eight or more visible GPUs, each reporting at least 80 GB of total memory, with recent drivers, CUDA 12.x+ support, and minimal memory usage before training starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dhc3l20703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7671875-fea3-4abe-9f01-57c854f50f92",
   "metadata": {},
   "source": [
    "### Get your Hugging Face token\n",
    "\n",
    "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
    "\n",
    "**Follow these steps to get your token:**\n",
    "\n",
    "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
    "\n",
    "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
    "\n",
    "4.  **Copy the generated token**. You will need this in the later steps.\n",
    "\n",
    "**Follow these steps to store your token (only if running on Google Colab):**\n",
    "\n",
    "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
    "\n",
    "2. Click **\"+ Add new secret\"**.\n",
    "\n",
    "3. Set the Name as **HF_TOKEN**.\n",
    "\n",
    "4. Paste your token into the Value field.\n",
    "\n",
    "5. Ensure the Notebook access toggle is turned On."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65244a08-5158-4bff-b3f4-09875613cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ipywidgets import Password, Button, HBox, Output\n",
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import whoami\n",
    "except Exception:\n",
    "    from huggingface_hub import HfApi\n",
    "\n",
    "def _verify_token(token: str) -> str:\n",
    "    try:\n",
    "        return whoami(token=token).get(\"name\", \"unknown\")\n",
    "    except TypeError:\n",
    "        return HfApi(token=token).whoami().get(\"name\", \"unknown\")\n",
    "\n",
    "token_box = Password(description=\"HF Token:\", placeholder=\"paste your token here\", layout={\"width\": \"400px\"})\n",
    "save_btn = Button(description=\"Save\", button_style=\"success\")\n",
    "out = Output()\n",
    "\n",
    "def save_token(_):\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        existing = os.environ.get(\"HF_TOKEN\")\n",
    "        entered = token_box.value.strip()\n",
    "        if existing and not entered:\n",
    "            user = _verify_token(existing)\n",
    "            print(f\"Using existing HF_TOKEN. Logged in as: {user}\")\n",
    "            return\n",
    "        if not entered:\n",
    "            print(\"No token entered.\")\n",
    "            return\n",
    "        os.environ[\"HF_TOKEN\"] = entered\n",
    "        user = _verify_token(entered)\n",
    "        print(f\"Token saved. Logged in as: {user}\")\n",
    "\n",
    "save_btn.on_click(save_token)\n",
    "display(HBox([token_box, save_btn]), out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b691306-88ee-47b3-b841-cd6d072f51eb",
   "metadata": {},
   "source": [
    "### Authenticate with Hugging Face\n",
    "\n",
    "Verify that your Hugging Face token is set and valid by calling the Hub's `whoami` endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0030b-ab25-4d76-a173-1a464c19087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer environment variable if already set\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        user = whoami()[\"name\"]\n",
    "        print(f\"Authenticated with Hugging Face as: {user} (via HF_TOKEN env)\")\n",
    "    except Exception as e:\n",
    "        print(\"HF_TOKEN is set but authentication failed:\", e)\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"HF_TOKEN is not set. Please create a Hugging Face access token \"\n",
    "        \"and export it as an environment variable.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cd139-a776-43ad-aeea-8547fcd8d744",
   "metadata": {},
   "source": [
    "### Acquire permission to use the gated model\n",
    "\n",
    "Llama 3.1-8B is a gated model, so you must explicitly request access before it can be downloaded. Visit the [model page](https://huggingface.co/meta-llama/Llama-3.1-8B) on Hugging Face, log in with the same account linked to your access token, and click **Request access**. You'll need to agree to Meta's license terms; approval is usually granted quickly but is not automatic. Once approved, your Hugging Face token will authorize downloads transparently. If you skip this step, model downloads will fail even with a valid token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b25ee3-1072-4c0f-965d-72e911873d1c",
   "metadata": {},
   "source": [
    "## Get the model and convert it into MaxText format\n",
    "\n",
    "### Import dependencies\n",
    "\n",
    "Import the core libraries needed for this tutorial:\n",
    "- **JAX**: High-performance ML framework with automatic differentiation and XLA compilation\n",
    "- **MaxText**: Google's production-grade training stack for JAX, providing model architectures, checkpoint management, and the SFT training loop\n",
    "\n",
    "The easiest way to get a working environment is the [NVIDIA NGC JAX container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax), which ships with JAX, CUDA, and MaxText preinstalled. To install the dependencies manually:\n",
    "\n",
    "```bash\n",
    "pip install 'jax[cuda13]' maxtext\n",
    "```\n",
    "\n",
    "On top of it, for the model conversion step you will also need **Torch**, the CPU version would be enough:\n",
    "\n",
    "```bash\n",
    "pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86465ed-69d1-4d77-bb47-b29c203e5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import subprocess\n",
    "import logging\n",
    "\n",
    "import transformers\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import MaxText\n",
    "from MaxText import pyconfig\n",
    "from MaxText.sft.sft_trainer import train as sft_train, setup_trainer_state\n",
    "\n",
    "MAXTEXT_REPO_ROOT = os.path.dirname(MaxText.__file__)\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"Number of available devices: {jax.local_device_count()}\")\n",
    "print(f\"MaxText installation path: {MAXTEXT_REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1687aa03-1549-429a-8156-571c7493ca3d",
   "metadata": {},
   "source": [
    "### Define model paths and run configuration\n",
    "\n",
    "This block defines the core paths and identifiers used throughout the tutorial: the model name, tokenizer source, checkpoint location, and output directory. You can override `MODEL_CHECKPOINT_PATH` via an environment variable to point to an existing converted checkpoint and skip the conversion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa133cf-1168-4e87-8c4a-6fc34f1cf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.1-8b\"\n",
    "TOKENIZER_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "WORKSPACE_DIR = Path(os.environ.get(\"WORKSPACE_DIR\", \"/workspace\"))\n",
    "\n",
    "# If set, use it; otherwise default to /workspace/llama_checkpoint\n",
    "MODEL_CHECKPOINT_PATH = os.environ.get(\"MODEL_CHECKPOINT_PATH\")\n",
    "MODEL_CHECKPOINT_PATH = Path(MODEL_CHECKPOINT_PATH) if MODEL_CHECKPOINT_PATH else (WORKSPACE_DIR / \"llama_checkpoint\")\n",
    "\n",
    "print(f\"Model checkpoint directory: {MODEL_CHECKPOINT_PATH}\")\n",
    "print(\"Tip: set MODEL_CHECKPOINT_PATH to a local directory to reuse an existing converted checkpoint.\")\n",
    "\n",
    "BASE_OUTPUT_DIRECTORY = Path(os.environ.get(\"BASE_OUTPUT_DIRECTORY\", str(WORKSPACE_DIR / \"sft_llama3_output\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b762437-1edb-4123-8257-90cb98028e97",
   "metadata": {},
   "source": [
    "### Download and convert the Llama 3.1 checkpoint\n",
    "\n",
    "This block downloads the pretrained Llama 3.1-8B weights from Hugging Face and converts them into MaxText's native checkpoint format. If a converted checkpoint already exists at the target path, this step is skipped entirely.\n",
    "\n",
    "The conversion runs in a CPU-only JAX context (`JAX_PLATFORMS=cpu`) to avoid unnecessary GPU memory allocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028b9c-89ed-4301-9a73-2967694891d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = Path(MODEL_CHECKPOINT_PATH)\n",
    "\n",
    "def run_ckpt_conversion(\n",
    "    *,\n",
    "    maxtext_repo_root: str,\n",
    "    model_name: str,\n",
    "    output_dir: Path,\n",
    "    hf_token: str,\n",
    "    quiet: bool = True,\n",
    ") -> None:\n",
    "    env = os.environ.copy()\n",
    "\n",
    "    # Conversion should not touch GPUs\n",
    "    env[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "    # Reduce verbosity (JAX/XLA/TensorFlow C++ logging)\n",
    "    env.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # 0=all, 1=INFO off, 2=INFO+WARNING off, 3=only FATAL\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"MaxText.utils.ckpt_conversion.to_maxtext\",\n",
    "        f\"{maxtext_repo_root}/configs/base.yml\",\n",
    "        f\"model_name={model_name}\",\n",
    "        f\"base_output_directory={str(output_dir)}\",\n",
    "        f\"hf_access_token={hf_token}\",\n",
    "        \"use_multimodal=false\",\n",
    "        \"scan_layers=true\",\n",
    "        \"skip_jax_distributed_system=True\",\n",
    "    ]\n",
    "\n",
    "    output_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if quiet:\n",
    "        # Capture logs; show only if something goes wrong\n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            env=env,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            print(\"Checkpoint conversion failed. Logs:\\n\")\n",
    "            if result.stdout:\n",
    "                print(\"----- stdout -----\")\n",
    "                print(result.stdout)\n",
    "            if result.stderr:\n",
    "                print(\"----- stderr -----\")\n",
    "                print(result.stderr)\n",
    "            raise RuntimeError(\"Checkpoint conversion failed. See logs above.\")\n",
    "    else:\n",
    "        # Verbose mode (streams logs)\n",
    "        subprocess.run(cmd, env=env, check=True)\n",
    "\n",
    "    print(f\"Checkpoint successfully converted to MaxText format at: {output_dir}\")\n",
    "\n",
    "if ckpt_dir.exists():\n",
    "    print(f\"Converted checkpoint already exists at: {ckpt_dir}\")\n",
    "else:\n",
    "    print(f\"Converting checkpoint to MaxText format â†’ {ckpt_dir}\")\n",
    "    run_ckpt_conversion(\n",
    "        maxtext_repo_root=MAXTEXT_REPO_ROOT,\n",
    "        model_name=MODEL_NAME,\n",
    "        output_dir=ckpt_dir,\n",
    "        hf_token=HF_TOKEN,\n",
    "        quiet=True, \n",
    "    )\n",
    "\n",
    "if not ckpt_dir.exists():\n",
    "    raise RuntimeError(\"Model checkpoint conversion failed. See logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e9555-f026-4bbc-a6fb-7b0cac6bd9da",
   "metadata": {},
   "source": [
    "## Provide the training configuration\n",
    "\n",
    "This block builds the full MaxText SFT training configuration by loading the base `sft.yml` config and applying runtime overrides for the model, dataset, hyperparameters, and output paths. Each run is tagged with a timestamp-based name to keep outputs isolated across experiments. Key settings:\n",
    "\n",
    "- **Dataset:** [UltraChat 200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k), a large instruction-style conversational dataset commonly used for SFT of chat models.\n",
    "- **Training:** 100 steps, learning rate 2e-5, sequence length 1024, bfloat16 precision.\n",
    "- **Checkpoint source:** The converted MaxText checkpoint from the previous step.\n",
    "\n",
    "To use your own dataset, ensure it follows a compatible schema and is accessible via the Hugging Face Hub or a local path. MaxText handles dataset loading, sharding, and batching automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22f54a-efe1-425d-abd4-ceb34561a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_items_path = Path(MODEL_CHECKPOINT_PATH) / \"0\" / \"items\"\n",
    "\n",
    "if not os.environ.get(\"HF_TOKEN\"):\n",
    "    raise RuntimeError(\"HF_TOKEN is not set. Export it before loading the SFT config.\")\n",
    "\n",
    "RUN_NAME = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# Load configuration for SFT training\n",
    "config_argv = [\n",
    "    \"\",\n",
    "    f\"{MAXTEXT_REPO_ROOT}/configs/sft.yml\",\n",
    "    f\"load_parameters_path={ckpt_items_path}\",\n",
    "    f\"model_name={MODEL_NAME}\",\n",
    "    \"steps=100\",\n",
    "    \"per_device_batch_size=1\",\n",
    "    \"max_target_length=1024\",\n",
    "    \"learning_rate=2.0e-5\",\n",
    "    \"weight_dtype=bfloat16\",\n",
    "    \"dtype=bfloat16\",\n",
    "    \"hf_path=HuggingFaceH4/ultrachat_200k\",\n",
    "    f\"hf_access_token={HF_TOKEN}\",\n",
    "    f\"base_output_directory={BASE_OUTPUT_DIRECTORY}\",\n",
    "    f\"run_name={RUN_NAME}\",\n",
    "    f\"tokenizer_path={TOKENIZER_PATH}\",\n",
    "    \"hardware=gpu\",\n",
    "]\n",
    "\n",
    "# Suppress the verbose per-parameter config dump (hundreds of INFO lines)\n",
    "_pyconfig_logger = logging.getLogger(\"MaxText.pyconfig\")\n",
    "_prev_level = _pyconfig_logger.level\n",
    "_pyconfig_logger.setLevel(logging.WARNING)\n",
    "\n",
    "config = pyconfig.initialize(config_argv)\n",
    "\n",
    "_pyconfig_logger.setLevel(_prev_level)\n",
    "\n",
    "print(\"SFT configuration loaded:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Training Steps: {config.steps}\")\n",
    "print(f\"  Max sequence length: {config.max_target_length}\")\n",
    "print(f\"  Output Directory: {config.base_output_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c6100-20e9-4dcb-b9b6-42d68bb03ae7",
   "metadata": {},
   "source": [
    "## Run the SFT training\n",
    "\n",
    "This section launches the SFT training loop. It runs MaxText's `sft_train` with the configuration defined above, reports progress, and saves checkpoints to the output directory. On completion, it prints the checkpoint and TensorBoard log paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13ddd1-57a8-469e-940c-11fe6ae2a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.setdefault(\"LIBTPU_INIT_ARGS\", \"\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting SFT training (run_name={RUN_NAME})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    result = sft_train(config)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training completed successfully\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Checkpoints written to: {config.checkpoint_dir}\")\n",
    "    if hasattr(config, \"tensorboard_dir\"):\n",
    "        print(f\"TensorBoard logs: {config.tensorboard_dir}\")\n",
    "\n",
    "    if isinstance(result, tuple) and len(result) == 2:\n",
    "        trainer, mesh = result\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training failed\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Error details: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3a3e2-027a-4bb7-bb7f-163605010d03",
   "metadata": {},
   "source": [
    "## Visualize training metrics with TensorBoard\n",
    "\n",
    "To monitor training loss and other metrics, launch TensorBoard in a separate terminal:\n",
    "\n",
    "```bash\n",
    "export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "tensorboard --logdir=/workspace/sft_llama3_output --host 0.0.0.0 --port 6006 --load_fast=false\n",
    "```\n",
    "\n",
    "Then open [http://127.0.0.1:6006/](http://127.0.0.1:6006/) in your browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaf26b-72fe-404b-827a-297045547f5b",
   "metadata": {},
   "source": [
    "## Test inference\n",
    "\n",
    "A quick sanity check to verify the fine-tuned model produces coherent output. The code below tokenizes a prompt using the Llama 3.1 chat template, then runs greedy autoregressive generation for up to 10 tokens, stopping early if the model produces an EOS token. This confirms the model loaded correctly and produces reasonable predictions.\n",
    "\n",
    "Note: this is naive autoregressive generation without KV-caching, so each step recomputes attention over the full sequence. For production use, consider a dedicated serving framework with KV-cache support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80390d8-a267-4d63-84c4-9c1c7a5a5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(TOKENIZER_PATH, token=HF_TOKEN)\n",
    "\n",
    "# Get model from trainer\n",
    "model = trainer.model\n",
    "\n",
    "# Format prompt using Llama chat template\n",
    "prompt = \"What is the capital of France?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize\n",
    "tokens = jnp.array(tokenizer(text)[\"input_ids\"])[None, :]\n",
    "\n",
    "# Greedy autoregressive generation\n",
    "max_new_tokens = 10\n",
    "generated_ids = []\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    seq_len = tokens.shape[1]\n",
    "    positions = jnp.arange(seq_len)[None, :]\n",
    "    attention_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))[None, :]\n",
    "\n",
    "    with mesh:\n",
    "        output = model(tokens, positions, None, attention_mask)\n",
    "        logits = output[0] if isinstance(output, tuple) else output\n",
    "\n",
    "    next_token_id = int(jnp.argmax(logits[0, -1]))\n",
    "    generated_ids.append(next_token_id)\n",
    "\n",
    "    if next_token_id == eos_token_id:\n",
    "        break\n",
    "\n",
    "    tokens = jnp.concatenate([tokens, jnp.array([[next_token_id]])], axis=1)\n",
    "\n",
    "# Decode all generated tokens\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated ({len(generated_ids)} tokens): '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a261a8c-55af-47ff-b68f-179abff5b623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
