{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nb_Ppf2ZUQL"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AI-Hypercomputer/maxtext/blob/main/src/maxtext/examples/sft_qwen3_demo.ipynb)\n",
    "\n",
    "# Qwen3-0.6B Supervised Fine-Tuning (SFT) Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGbe4_YQZUQL"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook performs SFT training and evaluation workflow on [OpenAI's GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k).\n",
    "The primary goal is to demonstrate the end-to-end process of:\n",
    "1. Pre-SFT Evaluation: Calcuating baseline accuracy for the model before training.\n",
    "2. SFT Training: Fine-tune the model using MaxText & Tunix SFT trainer.\n",
    "3. Post-SFT Evaluation: Re-running the evaluation loop after training to measure the performance gain achieved by SFT.\n",
    "\n",
    "This notebook can run on the **public TPU v5e-1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zolxPWhQZUQL"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Change Runtime Type (only if running on Google Colab)\n",
    "\n",
    "**Instructions:**\n",
    "1.  Navigate to the menu at the top of the screen.\n",
    "2.  Click on **Runtime**.\n",
    "3.  Select **Change runtime type** from the dropdown menu.\n",
    "4.  Select **v5e-1 TPU** as the **Hardware accelerator**.\n",
    "5. Click on **Save**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rk_QpVVuZUQL"
   },
   "source": [
    "### Get Your Hugging Face Token\n",
    "\n",
    "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
    "\n",
    "**Follow these steps to get your token:**\n",
    "\n",
    "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
    "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
    "\n",
    "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
    "\n",
    "4.  **Copy the generated token**. You will need this in the later steps.\n",
    "\n",
    "**Follow these steps to store your token (only if running on Google Colab):**\n",
    "\n",
    "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
    "\n",
    "2. Click **\"+ Add new secret\"**.\n",
    "\n",
    "3. Set the Name as **HF_TOKEN**.\n",
    "\n",
    "4. Paste your token into the Value field.\n",
    "\n",
    "5. Ensure the Notebook access toggle is turned On."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import userdata\n",
    "  print(\"Running the notebook on Google Colab\")\n",
    "  IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"Running the notebook on Visual Studio or JupyterLab\")\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9ms-jTSZUQL"
   },
   "source": [
    "## Installation: MaxText & Other Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠️ Note:** The installation process in following cell may take a few minutes to complete. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"Running the notebook in Google Colab\")\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"Running the notebook on JupyterLab\")\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSPRVbi7n6tB"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
    "    %cd /content/maxtext\n",
    "\n",
    "    # Install uv, a fast Python package installer\n",
    "    !pip install uv\n",
    "\n",
    "    # Install MaxText and its dependencies\n",
    "    !uv pip install -e .[tpu] --resolution=lowest\n",
    "    !python3 -m MaxText.install_maxtext_extra_deps\n",
    "\n",
    "    # Install Tunix for post-training notebooks\n",
    "    !uv pip install git+https://github.com/google/tunix\n",
    "    \n",
    "    # Install vllm for post-training notebooks\n",
    "    !git clone https://github.com/vllm-project/vllm.git\n",
    "    !VLLM_TARGET_DEVICE=\"tpu\" uv pip install ./vllm\n",
    "\n",
    "    # Install tpu-inference for post-training notebooks\n",
    "    !git clone https://github.com/vllm-project/tpu-inference.git\n",
    "    !uv pip install ./tpu-inference\n",
    "    !uv pip install --no-deps qwix==0.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywtealAxZUQM"
   },
   "source": [
    "### Restart Session (only if running on Google Colab)\n",
    "To apply certain changes, you need to restart the session.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Navigate to the menu at the top of the screen.\n",
    "2.  Click on **Runtime**.\n",
    "3.  Select **Restart session** from the dropdown menu.\n",
    "\n",
    "You will be asked to confirm the action in a pop-up dialog. Click on **Yes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clexf-j7ZUQM"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkBI9A3JZUQM"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import os\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "import MaxText\n",
    "from maxtext.configs import pyconfig\n",
    "from maxtext.examples.sft_train_and_evaluate import evaluate_model, get_test_dataset\n",
    "from maxtext.integration.tunix.tunix_adapter import TunixMaxTextAdapter\n",
    "from maxtext.utils.globals import MAXTEXT_REPO_ROOT, MAXTEXT_PKG_DIR\n",
    "from maxtext.trainers.post_train.sft import train_sft\n",
    "\n",
    "# Suppress vLLM logging with a severity level below ERROR\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\"\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.rl.rollout.vllm_rollout import VllmRollout\n",
    "\n",
    "from datetime import datetime\n",
    "from flax import nnx\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"MaxText installation path: {MAXTEXT_PKG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not jax.distributed.is_initialized():\n",
    "  jax.distributed.initialize()\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBbPN-uVZUQM"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "except ImportError:\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "\n",
    "# If not found in the environment, prompt the user for input securely\n",
    "# getpass function ensures the token is hidden while you type\n",
    "if not HF_TOKEN:\n",
    "    from getpass import getpass\n",
    "    HF_TOKEN = getpass(\"Hugging Face token not found in environment. Please enter it here: \")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Authenticated with Hugging Face successfully!\")\n",
    "else:\n",
    "    print(\"Authentication failed: Hugging Face token is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aENuzm9iZUQM"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjPYYl3zZUQM"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"qwen3-0.6b\"\n",
    "TOKENIZER_PATH = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    TOKENIZER_PATH,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# set the path to the model checkpoint or leave empty to download from HuggingFace\n",
    "MODEL_CHECKPOINT_PATH = \"\"\n",
    "if not MODEL_CHECKPOINT_PATH:\n",
    "   MODEL_CHECKPOINT_PATH = f\"{MAXTEXT_PKG_DIR}/qwen_checkpoint\"\n",
    "   print(\"Model checkpoint will be downloaded from HuggingFace at: \",  MODEL_CHECKPOINT_PATH)\n",
    "   print(\"Set MODEL_CHECKPOINT_PATH if you do not wish to download the checkpoint.\")\n",
    "\n",
    "\n",
    "RUN_NAME = datetime.now().strftime(\"%Y-%m-%d-%H-%m-%S\")\n",
    "\n",
    "# This is the directory where the fine-tuned model checkpoint will be saved\n",
    "BASE_OUTPUT_DIRECTORY = f\"{MAXTEXT_PKG_DIR}/maxtext_qwen06_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L37Ij4NZUQM"
   },
   "source": [
    "## Download Qwen3-0.6B Model Checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJanDAc0ZUQM"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
    "    # install torch for the conversion script\n",
    "    !python3 -m pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "    !JAX_PLATFORMS=cpu PYTHONPATH={MAXTEXT_PKG_DIR} {sys.executable} -m maxtext.checkpoint_conversion.to_maxtext \\\n",
    "      {MAXTEXT_PKG_DIR}/configs/base.yml \\\n",
    "      model_name={MODEL_NAME} \\\n",
    "      base_output_directory={MODEL_CHECKPOINT_PATH} \\\n",
    "      hf_access_token={HF_TOKEN} \\\n",
    "      use_multimodal=false \\\n",
    "      scan_layers=true \\\n",
    "      skip_jax_distributed_system=True\n",
    "\n",
    "if not os.path.exists(MODEL_CHECKPOINT_PATH):\n",
    "    raise ValueError(\"Model checkpoint conversion failed. Check the logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC-hILG0ZUQM"
   },
   "source": [
    "## Dataset Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3MLdr9kZUQM"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"openai/gsm8k\"\n",
    "TRAIN_DATA_SPLIT = \"train\"\n",
    "TEST_DATA_SPLIT = \"test\"\n",
    "HF_DATA_DIR = \"main\"\n",
    "TRAIN_DATA_COLUMNS = [\"question\", \"answer\"]\n",
    "CHAT_TEMPLATE_PATH = f\"{MAXTEXT_REPO_ROOT}/src/maxtext/examples/chat_templates/math_qa.json\"\n",
    "if not os.path.exists(CHAT_TEMPLATE_PATH):\n",
    "    raise FileNotFoundError(f\"Chat template not found: {CHAT_TEMPLATE_PATH}\")\n",
    "NUM_TEST_SAMPLES = 20  # Total number of samples to test\n",
    "BATCH_SIZE = 1  # Number of test samples to process in a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeAHmxSYZUQM"
   },
   "source": [
    "## MaxText Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "In-jdp1AAwrL"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "config = pyconfig.initialize(\n",
    "    [\n",
    "        \"\",\n",
    "        f\"{MAXTEXT_PKG_DIR}/configs/post_train/sft.yml\",\n",
    "        f\"load_parameters_path={MODEL_CHECKPOINT_PATH}/0/items\",\n",
    "        f\"model_name={MODEL_NAME}\",\n",
    "        f\"hf_access_token={HF_TOKEN}\",\n",
    "        f\"base_output_directory={BASE_OUTPUT_DIRECTORY}\",\n",
    "        f\"run_name={RUN_NAME}\",\n",
    "        f\"tokenizer_path={TOKENIZER_PATH}\",\n",
    "        f\"hf_path={DATASET_NAME}\",\n",
    "        f\"train_split={TRAIN_DATA_SPLIT}\",\n",
    "        f\"hf_data_dir={HF_DATA_DIR}\",\n",
    "        f\"train_data_columns={TRAIN_DATA_COLUMNS}\",\n",
    "        \"steps=500\",\n",
    "        \"per_device_batch_size=1\",\n",
    "        \"max_target_length=1024\",\n",
    "        \"learning_rate=3e-6\",\n",
    "        \"weight_dtype=bfloat16\",\n",
    "        \"dtype=bfloat16\",\n",
    "        f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9b0GWo-ZUQM"
   },
   "source": [
    "## Initial Setup & Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDqFmvUCZUQM"
   },
   "source": [
    "### Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wscWYxrtZUQM"
   },
   "outputs": [],
   "source": [
    "test_dataset = get_test_dataset(config, tokenizer)\n",
    "test_dataset = test_dataset[:NUM_TEST_SAMPLES]\n",
    "test_dataset = test_dataset.to_iter_dataset().batch(BATCH_SIZE, drop_remainder=True)\n",
    "TOTAL_BATCHES = NUM_TEST_SAMPLES // BATCH_SIZE\n",
    "print(\n",
    "    f\"Processing {NUM_TEST_SAMPLES} examples with a batch size of {BATCH_SIZE}. This will result in {TOTAL_BATCHES} total batches for the test run.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLSvOOEUZUQM"
   },
   "source": [
    "### Create SFT Trainer State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IHsC0m6ZUQM"
   },
   "outputs": [],
   "source": [
    "trainer, mesh = train_sft.setup_trainer_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpKtEqzFZUQM"
   },
   "source": [
    "### Create vLLM Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-pf_rbqZUQM"
   },
   "outputs": [],
   "source": [
    "tunix_model = TunixMaxTextAdapter(trainer.model)\n",
    "vllm_rollout = VllmRollout(\n",
    "    model=tunix_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config_or_size=1280,\n",
    "    mesh=mesh,\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        rollout_vllm_model_version=TOKENIZER_PATH,\n",
    "        rollout_vllm_hbm_utilization=0.8,\n",
    "        rollout_vllm_init_with_random_weights=True,\n",
    "        rollout_vllm_tpu_backend_type=\"jax\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "567gTxsEZUQM"
   },
   "source": [
    "## Evaluation before SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnACa3zCZUQM"
   },
   "outputs": [],
   "source": [
    "print(\"Running Pre-SFT Evaluation...\")\n",
    "score = evaluate_model(test_dataset, vllm_rollout, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5-M4iYkZUQN"
   },
   "outputs": [],
   "source": [
    "print(\"========================= Score for PRE-SFT Evaluation =========================\")\n",
    "print(f\"Percentage of test samples where the model produced the correct numerical answer: {score['correct']}%\")\n",
    "print(\n",
    "    f\"Percentage of test samples where the model produced the numerical answer within 10%: {score['partially_correct']}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of test samples where the model's output adheres to the expected structure: {score['correct_format']}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJE1ookSAzz-"
   },
   "source": [
    "## SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "mgwpNgQYCJEd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Starting SFT Training...\")\n",
    "trainer = train_sft.train_model(config, trainer, mesh)\n",
    "print(\"SFT Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEdNYRhwZUQN"
   },
   "source": [
    "## Evaluation after SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcsZacZdZUQN"
   },
   "outputs": [],
   "source": [
    "print(\"Running Post-SFT Evaluation...\")\n",
    "model = TunixMaxTextAdapter(trainer.model)\n",
    "state = nnx.state(model)\n",
    "vllm_rollout.update_params(state)\n",
    "score = evaluate_model(test_dataset, vllm_rollout, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "-JtYTPvJZUQN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"========================= Score for POST-SFT Evaluation =========================\")\n",
    "print(f\"Percentage of test samples where the model produced the correct numerical answer: {score['correct']}%\")\n",
    "print(\n",
    "    f\"Percentage of test samples where the model produced the numerical answer within 10%: {score['partially_correct']}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of test samples where the model's output adheres to the expected structure: {score['correct_format']}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
