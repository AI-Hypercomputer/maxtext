{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3.1-8B-Instruct Reinforcement Learning Demo\n",
    "\n",
    "This notebook demonstrates training on Llama3.1-8B-Instruct model with either GRPO (Group Relative Policy Optimization) or GSPO (Group Sequence Policy Optimization).\n",
    "\n",
    "This notebook can run on **TPU v6e-8** or **v5p-8**.\n",
    "\n",
    "## What is GRPO/GSPO?\n",
    "\n",
    "GRPO/GSPO is an RL algorithm that enhances reasoning abilities of LLMs by:\n",
    "1. Generating multiple responses for each prompt\n",
    "2. Evaluating responses using reward models  \n",
    "3. Calculating relative advantages to update the policy\n",
    "\n",
    "The difference is in the loss function - either it's optimizing each token (GRPO) or the whole sequence(GSPO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Change Runtime Type (only if running on Google Colab)\n",
    "\n",
    "**Instructions:**\n",
    "1.  Navigate to the menu at the top of the screen.\n",
    "2.  Click on **Runtime**.\n",
    "3.  Select **Change runtime type** from the dropdown menu.\n",
    "4.  Select **v6e-8** or **v5p-8 TPU** as the **Hardware accelerator**.\n",
    "5. Click on **Save**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Your Hugging Face Token\n",
    "\n",
    "To access model checkpoint from the Hugging Face Hub, you need to authenticate with a personal access token.\n",
    "\n",
    "**Follow these steps to get your token:**\n",
    "\n",
    "1.  **Navigate to the Access Tokens page** in your Hugging Face account settings. You can go there directly by visiting this URL:\n",
    "    *   [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "2.  **Create a new token** by clicking the **\"+ Create new token\"** button.\n",
    "\n",
    "3.  **Give your token a name** and assign it a **`read` role**. The `read` role is sufficient for downloading models.\n",
    "\n",
    "4.  **Copy the generated token**. You will need this in the later steps.\n",
    "\n",
    "**Follow these steps to store your token (only if running on Google Colab):**\n",
    "\n",
    "1. On the left sidebar of your Colab window, click the key icon (the Secrets tab).\n",
    "\n",
    "2. Click **\"+ Add new secret\"**.\n",
    "\n",
    "3. Set the Name as **HF_TOKEN**.\n",
    "\n",
    "4. Paste your token into the Value field.\n",
    "\n",
    "5. Ensure the Notebook access toggle is turned On."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import userdata\n",
    "  print(\"Running the notebook on Google Colab\")\n",
    "  IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"Running the notebook on Visual Studio or JupyterLab\")\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation: MaxText and Dependencies\n",
    "\n",
    "**‚ö†Ô∏è Note:** The installation process in following cell may take a few minutes to complete. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/AI-Hypercomputer/maxtext.git\n",
    "    %cd /content/maxtext\n",
    "\n",
    "    # Install uv, a fast Python package installer\n",
    "    !pip install uv\n",
    "\n",
    "    # Install MaxText and its dependencies\n",
    "    !uv pip install -e .[tpu] --resolution=lowest\n",
    "    !python3 -m MaxText.install_maxtext_extra_deps\n",
    "\n",
    "    # Install vLLM for Jax and TPUs\n",
    "    !uv pip install vllm-tpu\n",
    "    !uv pip install --no-deps qwix==0.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart Session (only if running on Google Colab)\n",
    "To apply certain changes, you need to restart the session.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Navigate to the menu at the top of the screen.\n",
    "2.  Click on **Runtime**.\n",
    "3.  Select **Restart session** from the dropdown menu.\n",
    "\n",
    "You will be asked to confirm the action in a pop-up dialog. Click on **Yes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import MaxText\n",
    "from huggingface_hub import login\n",
    "from etils import epath\n",
    "import jax\n",
    "\n",
    "from maxtext.trainers.post_train.rl.train_rl import rl_train, setup_configs_and_devices\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"SKIP_JAX_PRECOMPILE\"] = \"1\"  # Faster startup for vLLM\n",
    "# Suppress vLLM logging with a severity level below ERROR\n",
    "os.environ[\"VLLM_LOGGING_LEVEL\"] = \"ERROR\"\n",
    "\n",
    "MAXTEXT_PKG_DIR = os.path.dirname(MaxText.__file__)\n",
    "MAXTEXT_REPO_ROOT = os.sep.join([\"maxtext\" if p == \"MaxText\" else p for p in MAXTEXT_PKG_DIR.split(os.sep)])\n",
    "print(f\"MaxText installation path: {MAXTEXT_PKG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not jax.distributed.is_initialized():\n",
    "  jax.distributed.initialize()\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "else:\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "\n",
    "# If not found in the environment, prompt the user for input securely\n",
    "# getpass function ensures the token is hidden while you type\n",
    "if not HF_TOKEN:\n",
    "    from getpass import getpass\n",
    "    HF_TOKEN = getpass(\"Hugging Face token not found in environment. Please enter it here: \")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Authenticated with Hugging Face successfully!\")\n",
    "else:\n",
    "    print(\"Authentication failed: Hugging Face token is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.1-8b\"\n",
    "TOKENIZER_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "RUN_NAME = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "LOSS_ALGO=\"grpo\" #  or \"gspo-token\" if you want to use GSPO\n",
    "\n",
    "CHAT_TEMPLATE_PATH = f\"{MAXTEXT_REPO_ROOT}/examples/chat_templates/gsm8k_rl.json\"\n",
    "if not os.path.exists(CHAT_TEMPLATE_PATH):\n",
    "    raise FileNotFoundError(f\"Chat template not found: {CHAT_TEMPLATE_PATH}\")\n",
    "\n",
    "# set the path to the model checkpoint or leave empty to download from HuggingFace\n",
    "MODEL_CHECKPOINT_PATH = \"\"\n",
    "if not MODEL_CHECKPOINT_PATH:\n",
    "   MODEL_CHECKPOINT_PATH = f\"{MAXTEXT_PKG_DIR}/llama_checkpoint\"\n",
    "   print(\"Model checkpoint will be downloaded from HuggingFace at: \",  MODEL_CHECKPOINT_PATH)\n",
    "   print(\"Set MODEL_CHECKPOINT_PATH if you do not wish to download the checkpoint.\")\n",
    "    \n",
    "OUTPUT_DIRECTORY = f\"{MAXTEXT_PKG_DIR}/rl_llama3_output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Llama3.1-8B Model Checkpoint from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not epath.Path(MODEL_CHECKPOINT_PATH).exists():\n",
    "    # install torch for the conversion script\n",
    "    !python3 -m pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "    !JAX_PLATFORMS=cpu PYTHONPATH={MAXTEXT_PKG_DIR} {sys.executable} -m maxtext.checkpoint_conversion.to_maxtext \\\n",
    "      {MAXTEXT_PKG_DIR}/configs/base.yml \\\n",
    "      model_name={MODEL_NAME} \\\n",
    "      base_output_directory={MODEL_CHECKPOINT_PATH} \\\n",
    "      hf_access_token={HF_TOKEN} \\\n",
    "      use_multimodal=false \\\n",
    "      scan_layers=true \\\n",
    "      skip_jax_distributed_system=True\n",
    "\n",
    "    if not epath.Path(MODEL_CHECKPOINT_PATH).exists():\n",
    "        raise ValueError(\"Model checkpoint conversion failed. Check the logs above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxText Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration for RL training\n",
    "config_argv = [\n",
    "    \"\",\n",
    "    f\"{MAXTEXT_PKG_DIR}/configs/post_train/rl.yml\",\n",
    "    f\"model_name={MODEL_NAME}\",\n",
    "    f\"tokenizer_path={TOKENIZER_PATH}\",\n",
    "    f\"run_name={RUN_NAME}\",\n",
    "    f\"chat_template_path={CHAT_TEMPLATE_PATH}\",\n",
    "    f\"load_parameters_path={MODEL_CHECKPOINT_PATH}/0/items\",\n",
    "    f\"base_output_directory={OUTPUT_DIRECTORY}\",\n",
    "    f\"hf_access_token={HF_TOKEN}\",\n",
    "    \"debug.rl=False\",\n",
    "    f\"rl.loss_algo={LOSS_ALGO}\",\n",
    "    \"use_pathways=False\"\n",
    "]\n",
    "\n",
    "trainer_config, sampler_config, trainer_devices, sampler_devices = setup_configs_and_devices(config_argv)\n",
    "\n",
    "rl_train_steps = int(\n",
    "    trainer_config.num_batches\n",
    "    * trainer_config.rl.num_iterations\n",
    "    * trainer_config.train_fraction\n",
    "    * trainer_config.num_epoch\n",
    ")\n",
    "\n",
    "print(\"‚úì Configuration initialized successfully\")\n",
    "print(f\"üìÅ Output directory: {trainer_config.base_output_directory}\")\n",
    "print(f\"ü§ñ Model: {trainer_config.model_name}\")\n",
    "print(f\"üìä RL Train Steps: {rl_train_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"üöÄ Starting {LOSS_ALGO} Training...\")\n",
    "print(\"=\" * 80)\n",
    "try:\n",
    "    rl_train(trainer_config, sampler_config, trainer_devices, sampler_devices)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ Training Completed Successfully!\")\n",
    "    print(f\"‚úçÔ∏è Note the improved evaluation accuracy metrics with just {rl_train_steps} RL training steps!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìÅ Checkpoints saved to: {trainer_config.checkpoint_dir}\")\n",
    "    print(f\"üìä TensorBoard logs: {trainer_config.tensorboard_dir}\")\n",
    "    print(f\"üéØ Model ready for inference!\")\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚ùåTraining Failed!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Learn More\n",
    "\n",
    "- **CLI Usage**: https://maxtext.readthedocs.io/en/latest/tutorials/rl.html\n",
    "- **Configuration**: See `src/maxtext/configs/rl.yml` for all available options\n",
    "- **Documentation**: Check `src/maxtext/trainers/post_train/rl/train_rl.py` for the `rl_train` function implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maxtext_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
