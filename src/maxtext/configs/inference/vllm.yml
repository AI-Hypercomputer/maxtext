# Copyright 2023â€“2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

base_config: "base.yml"
attention: "vllm_rpa"
# NNX required for vLLM integration
enable_nnx: True
# Avoid re-initializing JAX distributed system when using vLLM
skip_jax_distributed_system: True
# Scanned layers are not supported with vLLM integration
scan_layers: False
# Set weight dtype to bfloat16 as is done in vLLM
weight_dtype: bfloat16


# -------------- Logical Axis Rules --------------
mesh_axes: ['data', 'attn_dp', 'model', 'expert']
logical_axis_rules: [
                      ['activation_batch', ['expert']],
                      ['activation_batch_no_exp', []],
                      ['activation_embed_and_logits_batch', ['expert']],
                      ['activation_embed_and_logits_batch_sequence', ['expert']],
                      ['activation_heads', ['model']],
                      ['activation_kv_heads', ['model']],
                      ['activation_attn_length', ['expert']],
                      ['activation_attn_length_no_exp', []],
                      ['activation_length', ['data', 'expert']],
                      ['activation_length_no_exp', 'data'],
                      ['activation_q_length', ['expert']],
                      ['activation_attn_embed', 'model'],
                      ['activation_embed', ['model', 'attn_dp']],
                      ['activation_mlp', ['model', 'attn_dp']],
                      ['activation_kv', ['model']],
                      ['activation_prefill_kv_batch', ['expert']],
                      ['activation_kv_batch', ['expert']],
                      ['activation_kv_batch_no_exp', []],
                      ['activation_kv_head_dim', ['model']],
                      ['activation_vocab', ['model', 'attn_dp']],
                      ['activation_norm_length', []],
                      ['activation_exp', ['expert']],
                      ['decode_batch', ['expert']],
                      ['decode_length', []],
                      ['mlp', ['model', 'attn_dp']],
                      ['mlp_no_fsdp', ['model', 'attn_dp']],
                      ['vocab', ['model', 'attn_dp']],
                      ['heads', ['model']],
                      ['q_heads', ['model']],
                      ['kv_heads', ['model']],
                      ['kv_head_dim', []],
                      ['kv', []],
                      ['embed', ['expert']],
                      ['embed_tensor_transpose', ['attn_dp', 'model']],
                      ['embed_no_exp', []],
                      ['q_lora', ['expert']],
                      ['kv_lora', ['expert']],
                      ['norm', []],
                      ['cache_heads', ['model']],
                      ['exp', ['expert']],
                      ['paged_kv_heads', ['model']],
                    ]
data_sharding: [['data', 'attn_dp', 'model', 'expert']]
input_data_sharding_logical_axes: ['activation_embed_and_logits_batch']
