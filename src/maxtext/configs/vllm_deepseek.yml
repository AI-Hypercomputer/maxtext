# Copyright 2023â€“2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


base_config: "vllm.yml"

logical_axis_rules: [
                      ['activation_batch', ['']],
                      ['activation_batch_no_exp', []],
                      ['activation_embed_and_logits_batch', ['expert']],
                      ['activation_embed_and_logits_batch_sequence', ['expert']],
                      ['activation_heads', ['model']],
                      ['activation_kv_heads', ['model']],
                      ['activation_attn_length', ['expert']],
                      ['activation_attn_length_no_exp', []],
                      ['activation_length', ['data', 'expert']],
                      ['activation_length_no_exp', 'data'],
                      ['activation_q_length', ['expert']],
                      ['activation_attn_embed', 'model'],
                      ['activation_embed', ['model', 'attn_dp']],
                      ['activation_mlp', ['model', 'attn_dp', 'expert']],
                      ['activation_kv', ['model']],
                      ['activation_prefill_kv_batch', ['expert']],
                      ['activation_kv_batch', ['']],
                      ['activation_kv_batch_no_exp', []],
                      ['activation_kv_head_dim', ['model', 'attn_dp', 'expert']],
                      ['activation_vocab', ['model', 'attn_dp']],
                      ['activation_norm_length', []],
                      ['activation_exp', ['expert']],
                      ['decode_batch', ['expert']],
                      ['decode_length', []],
                      ['mlp_no_fsdp', ['model', 'attn_dp', 'expert']],
                      ['vocab', ['model', 'attn_dp', 'expert']],
                      ['heads', ['expert', 'attn_dp', 'model']],
                      ['q_heads', []],
                      ['kv_heads', []],
                      ['kv_head_dim', ['model', 'attn_dp', 'expert']],
                      ['kv', ['model', 'attn_dp', 'expert']],
                      ['kv', []],
                      ['embed', []],
                      ['mlp', ['model', 'attn_dp', 'expert']],
                      ['moe_mlp', []],
                      ['embed_tensor_transpose', ['attn_dp', 'model']],
                      ['embed_no_exp', []],
                      ['q_lora', []],
                      ['kv_lora', []],
                      ['norm', []],
                      ['cache_heads', ['model']],
                      ['exp', ['expert', 'attn_dp', 'model']],
                      ['paged_kv_heads', ['model']],
                      ['cache_batch_prefill', []],
                      ['cache_batch', []],
                      ['cache_sequence', []],
                      ['cache_heads_none', []],
                      ['cache_kv', []],
                      ['kv_lora_up_proj',['expert', 'attn_dp', 'model']],
                      ['q_lora_up_proj',['expert', 'attn_dp', 'model']],
                    ]