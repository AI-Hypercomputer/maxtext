{
    "cc4d3d1cf05c88afb62a108e558ccb20177b5c5e224dab3ec96c88faf96a125d": [
        "/github.com/huggingface/transformers/blob/main/src/transformers/cache_utils.py#Cache",
        "/github.com/huggingface/transformers/blob/main/src/transformers/generation.py#GenerationMixin",
        "/github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
        "/github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#MoeModelOutputWithPast",
        "/github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel",
        "/github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
        "/github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func",
        "/github.com/huggingface/transformers/blob/main/src/transformers/processing_utils.py#Unpack",
        "/github.com/huggingface/transformers/blob/main/src/transformers/utils.py#TransformersKwargs",
        "/github.com/huggingface/transformers/blob/main/src/transformers/utils.py#auto_docstring",
        "/github.com/huggingface/transformers/blob/main/src/transformers/utils.py#can_return_tuple",
        "torch.py#nn.Linear",
        "typing.py#Optional",
        "typing.py#Union"
    ]
}