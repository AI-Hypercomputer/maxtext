[
    {
        "block_name": "src/MaxText/layers/attention_mla.py#mla_as_linen",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "def mla_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    q_lora_rank: int = 0,\n    kv_lora_rank: int = 512,\n    qk_nope_head_dim: int = 128,\n    qk_rope_head_dim: int = 64,\n    v_head_dim: int = 128,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    mscale: float = 1.0,  # scaling factor for softmax\n    rope_factor: float = 40.0,  # rotary embedding factor\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an MLA as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `MLA` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MLA,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      q_lora_rank=q_lora_rank,\n      kv_lora_rank=kv_lora_rank,\n      qk_nope_head_dim=qk_nope_head_dim,\n      qk_rope_head_dim=qk_rope_head_dim,\n      v_head_dim=v_head_dim,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      mscale=mscale,\n      rope_factor=rope_factor,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "functionality": "This is a factory function that wraps the NNX-based `MLA` (Multi-Head Latent Attention) class to make it compatible with the Flax Linen framework. It takes numerous configuration parameters and passes them to the `MLA` class constructor via the `nnx_wrappers.to_linen` utility.",
            "usage": "Call this function with the desired configuration parameters (e.g., `config`, `num_query_heads`, `mesh`) to instantiate an MLA attention layer as a standard Flax Linen module. The returned module can then be used within a Linen model's `setup` or `__call__` method."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_mla.py#MLA",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "class MLA(Attention):\n  \"\"\"Multi-Head Latent Attention (MLA) layer.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      q_lora_rank: int = 0,\n      kv_lora_rank: int = 512,\n      qk_nope_head_dim: int = 128,\n      qk_rope_head_dim: int = 64,\n      v_head_dim: int = 128,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      mscale: float = 1.0,  # scaling factor for softmax\n      rope_factor: float = 40.0,  # rotary embedding factor\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the MLA module.\n\n    Args:\n      config: The model configuration.\n      ... and other configuration parameters for MLA attention.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    base_kv_cache = config.attention != \"paged\" and config.mla_naive_kvcache\n\n    # Setting these before call to super because a field is used in super\n    self.q_lora_rank = q_lora_rank\n    self.kv_lora_rank = kv_lora_rank\n    self.qk_nope_head_dim = qk_nope_head_dim\n    self.qk_rope_head_dim = qk_rope_head_dim\n    self.v_head_dim = v_head_dim\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.mscale = mscale\n    self.rope_factor = rope_factor\n\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n\n    super().__init__(\n        config=config,\n        num_query_heads=num_query_heads,\n        num_kv_heads=num_kv_heads,\n        head_dim=head_dim,\n        max_target_length=max_target_length,\n        mesh=mesh,\n        attention_kernel=attention_kernel,\n        inputs_q_shape=inputs_q_shape,\n        inputs_kv_shape=inputs_kv_shape,\n        dtype=dtype,\n        weight_dtype=weight_dtype,\n        max_prefill_predict_length=max_prefill_predict_length,\n        dropout_rate=dropout_rate,\n        kernel_init=kernel_init,\n        float32_qk_product=float32_qk_product,\n        float32_logits=float32_logits,\n        quant=quant,\n        kv_quant=kv_quant,\n        attention_type=attention_type,\n        attn_logits_soft_cap=attn_logits_soft_cap,\n        sliding_window_size=sliding_window_size,\n        use_ragged_attention=use_ragged_attention,\n        ragged_block_size=ragged_block_size,\n        use_qk_norm=use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        use_bias_in_projections=use_bias_in_projections,\n        temperature_tuning=temperature_tuning,\n        temperature_tuning_scale=temperature_tuning_scale,\n        temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n        prefill_query_axis_names=prefill_query_axis_names,\n        prefill_key_axis_names=prefill_key_axis_names,\n        prefill_value_axis_names=prefill_value_axis_names,\n        query_axis_names=query_axis_names,\n        key_axis_names=key_axis_names,\n        value_axis_names=value_axis_names,\n        ep_query_axis_names=ep_query_axis_names,\n        ep_key_axis_names=ep_key_axis_names,\n        ep_value_axis_names=ep_value_axis_names,\n        input_axis_names=input_axis_names,\n        ep_input_axis_names=ep_input_axis_names,\n        out_axis_names=out_axis_names,\n        ep_out_axis_names=ep_out_axis_names,\n        prefill_input_axis_names=prefill_input_axis_names,\n        decode_input_axis_names=decode_input_axis_names,\n        prefill_out_axis_names=prefill_out_axis_names,\n        decode_out_axis_names=decode_out_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        compute_axis_order=compute_axis_order,\n        reshape_q=reshape_q,\n        is_nope_layer=is_nope_layer,\n        is_vision=is_vision,\n        model_mode=model_mode,\n        base_kv_cache=base_kv_cache,\n        rngs=rngs,\n    )\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.MlaKVCache_0 = self.init_mla_kv_caches(inputs_kv_shape) if model_mode != MODEL_MODE_TRAIN else None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the MLA-specific projections.\"\"\"\n    # Assert required configuration parameters for MLA attention.\n    assert (\n        self.config.attention_type == AttentionType.MLA.value\n    ), f\"MLA requires MLA attention type {AttentionType.MLA.value}\"\n    assert self.kv_lora_rank > 0, \"KV LoRA rank must be > 0\"\n    assert self.qk_nope_head_dim > 0, \"QK NoPe head dim must be > 0\"\n    assert self.qk_rope_head_dim > 0, \"QK RoPE head dim must be > 0\"\n    assert self.v_head_dim > 0, \"V head dim must be > 0\"\n    assert self.num_query_heads == self.num_kv_heads, \"MLA requires equal number of query and kv heads\"\n    assert not self.config.fused_qkv, \"Fused QKV is not supported for MLA\"\n\n    if self.q_lora_rank == 0:\n      # Standard Q projection (without LoRA).\n      self.query = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n    else:\n      # LoRA path for Q.\n      self.wq_a = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=self.q_lora_rank,\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_lora_up_proj\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n      self.q_norm = RMSNorm(\n          num_features=self.q_lora_rank,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.wq_b = DenseGeneral(\n          in_features_shape=self.q_lora_rank,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"q_lora\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n\n    # KV LoRA path.\n    self.wkv_a = DenseGeneral(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.kv_lora_rank + self.qk_rope_head_dim,\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"kv_lora_up_proj\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.kv_norm = RMSNorm(\n        num_features=self.kv_lora_rank,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.wkv_b = DenseGeneral(\n        in_features_shape=self.kv_lora_rank,\n        out_features_shape=(\n            self.num_query_heads,\n            (self.qk_nope_head_dim + self.v_head_dim),\n        ),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"kv_lora\", \"kv_heads\", \"kv_head_dim\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # Set softmax scaling.\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n    # Setup paged attention op\n    if self.config.attention == \"paged\":\n      # Set head_dim to the max of qk_head_dim and v_head_dim. The current paged\n      # attention kernel requires the head_dim to be the same for q, k, v.\n      head_dim = max(self.qk_head_dim, self.v_head_dim)\n      # Align head_dim to the pagedattn_head_dim_alignment if specified.\n      if self.config.pagedattn_head_dim_alignment > 0:\n        alignment = self.config.pagedattn_head_dim_alignment\n        head_dim = (head_dim + alignment - 1) // alignment * alignment\n      self.ds_paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n  def mla_query_projection(self, inputs_q: Array, inputs_positions: Array, model_mode) -> Array:\n    \"\"\"Query projection for MLA, e.g. includes LoRA if q_lora_rank > 0.\"\"\"\n    # Set softmax scaling.\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    if self.q_lora_rank == 0:\n      q = self.query(inputs_q)\n    else:\n      # LoRA path\n      low_rank_q = self.wq_a(inputs_q)  # [B, L, q_lora_rank]\n      low_rank_q = self.q_norm(low_rank_q)  # RMSNorm on low rank\n      q = self.wq_b(low_rank_q)  # [B, L, n_heads * qk_head_dim]\n\n    # Split into non-positional and rotary parts.\n    q_nope, q_pe = jnp.split(q, [self.qk_nope_head_dim], axis=-1)\n    q_pe = self.apply_rotary_embedding(q_pe, inputs_positions=inputs_positions)\n    # Query projection is scaled by self.softmax_scale to be consistent MaxText implementation.\n    # DeepSeek v3 was doing it in attention score computation.\n    query = jnp.concatenate([q_nope, q_pe], axis=-1) * self.softmax_scale\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n    return query\n\n  def mla_get_key_value(self, low_rank_main, key_rope, model_mode):\n    \"\"\"get (key,value) pair from mla\"\"\"\n    kv_out = self.wkv_b(low_rank_main)\n\n    # Split kv_out into key_nope and value parts.\n    key_nope, value = jnp.split(kv_out, [self.qk_nope_head_dim], axis=-1)\n    key_rope = jnp.broadcast_to(key_rope, (key_nope.shape[0], key_nope.shape[1], self.num_query_heads, key_rope.shape[3]))\n\n    key = jnp.concatenate([key_nope, key_rope], axis=-1)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n    return key, value\n\n  def init_mla_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes MlaKVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      An MlaKVCache module instance.\n\n    Raises:\n      ValueError: If the configuration is invalid.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in MlaKVCache.\n    # However, MlaKVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # MlaKVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.MlaKVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_head_size=self.kv_lora_rank,\n        value_head_size=self.qk_rope_head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        model_mode=self.model_mode,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        rngs=self.rngs,\n    )\n\n  def update_mla_kv_caches(self, low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk=None):\n    \"\"\"Updates the MLA (Multi-Head Latent Attention) KV caches.\n\n    This method is specific to the MLA attention mechanism. It calls the\n    `mla_kv_cache_as_linen` module to update and retrieve the caches, which\n    store latent representations (`low_rank_main`) and RoPE-applied keys\n    (`key_rope`). It then reconstructs the full key and value tensors from\n    the cached components.\n\n    Args:\n      low_rank_main: The main latent component of the key.\n      key_rope: The RoPE-applied component of the key.\n      decoder_segment_ids: Segment IDs for decoder masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, reconstructed from the MLA cache, or None.\n      - The autoregressive key-value cache, reconstructed from the MLA cache, or None.\n    \"\"\"\n\n    prefill_mla_cache, ar_mla_cache = self.MlaKVCache_0(\n        key_latent=low_rank_main,\n        key_rope=key_rope,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n\n    if prefill_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids = prefill_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      prefill_kv_cache = key, value, decoder_segment_ids\n    else:\n      prefill_kv_cache = None\n\n    if ar_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids, lengths = ar_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      ar_kv_cache = key, value, decoder_segment_ids, lengths\n    else:\n      ar_kv_cache = None\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def mla_kv_projection(self, inputs: Array, inputs_positions: Array, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"MLA key/value projection with integrated rotary embedding.\"\"\"\n    low_rank = self.wkv_a(inputs)\n    low_rank_main, low_rank_rope = jnp.split(low_rank, [self.kv_lora_rank], axis=-1)\n    low_rank_main = self.kv_norm(low_rank_main)\n\n    # Apply rotary embedding to key_rope.\n    key_rope = jnp.expand_dims(low_rank_rope, axis=2)\n    key_rope = self.apply_rotary_embedding(key_rope, inputs_positions=inputs_positions)\n\n    key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n    cached_values = [None, None]\n    if self.config.attention != \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      if self.config.mla_naive_kvcache:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      else:\n        cached_values = self.update_mla_kv_caches(\n            low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk\n        )\n\n    return key, value, cached_values\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Optional[Any] = None,\n  ) -> Array:\n    \"\"\"Forward pass for MLA, reusing `AttentionOp` for the actual attention.\n\n    Args:\n      inputs_q: Query input [batch, q_length, embed_dim].\n      inputs_kv: KV input   [batch, kv_length, embed_dim].\n      inputs_positions: Positions for rotary embeddings or similar.\n      decoder_segment_ids: Segment IDs for masking, if any.\n      model_mode: \"train\", \"prefill\", or \"autoregressive\".\n      deterministic: Disables dropout if set to True.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      A tensor of shape [batch, length, embed_dim] containing the\n      MLA-attended outputs.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n\n    query = self.mla_query_projection(inputs_q, inputs_positions, model_mode)\n    key, value, cached_values = self.mla_kv_projection(\n        inputs_kv, inputs_positions, decoder_segment_ids, model_mode, previous_chunk\n    )\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.ds_paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      unnormalized_out = unnormalized_out[..., : self.v_head_dim]\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      out = self.attention_op(query, key, value, decoder_segment_ids, model_mode, cached_values)\n\n    if model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    out = self.out_projection(out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "multi_head_latent_attention",
            "purpose": "Implements the Multi-Head Latent Attention (MLA) mechanism, which uses LoRA-like projections for keys and values and separates positional (RoPE) and non-positional (NoPe) components.",
            "input": {
                "shape": "[batch, sequence_length, embed_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Apply logical constraints to input tensors based on the model's operational mode (e.g., train, prefill).",
                "Project the query input `inputs_q` into a query tensor using `mla_query_projection`.",
                "Project the key/value input `inputs_kv` into key and value tensors using `mla_kv_projection`, which also handles KV caching during inference.",
                "Apply gradient checkpointing names to the projected query, key, and value.",
                "Compute the attention output using either a paged attention kernel (`ds_paged_attention_op`) for inference or a standard attention operation (`attention_op`).",
                "Apply a final linear projection (`out_projection`) to the attention output.",
                "Apply a gradient checkpointing name to the final output."
            ],
            "output": {
                "shape": "[batch, sequence_length, embed_dim]"
            },
            "dependencies": [
                "maxtext.layers.attentions.Attention",
                "maxtext.layers.linears.DenseGeneral",
                "maxtext.layers.normalizations.RMSNorm",
                "maxtext.inference.kvcache.MlaKVCache",
                "maxtext.inference.paged_attention.PagedAttentionOp"
            ],
            "parameters": {
                "q_lora_rank": "The rank for the LoRA-like projection for queries. If 0, a standard dense projection is used.",
                "kv_lora_rank": "The rank for the LoRA-like projection for keys and values.",
                "qk_nope_head_dim": "The dimension of the non-positional (NoPe) part of the query and key heads.",
                "qk_rope_head_dim": "The dimension of the rotary positional embedding (RoPE) part of the query and key heads.",
                "v_head_dim": "The dimension of the value heads."
            },
            "notes": [
                "This class inherits from a base `Attention` class.",
                "It uses a specific projection scheme where keys and queries are split into a non-positional (NoPe) part and a rotary positional (RoPE) part.",
                "It has a specialized KV caching mechanism (`MlaKVCache`) for inference that stores latent representations, unless `mla_naive_kvcache` is enabled.",
                "Supports paged attention for efficient inference."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLA module, setting up configuration parameters, calling the superclass constructor, and initializing the MLA-specific KV cache for inference modes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set MLA-specific attributes like LoRA ranks and head dimensions.",
                        "Calculate the combined query/key head dimension (`qk_head_dim`).",
                        "Call the `super().__init__` method to initialize the base `Attention` class.",
                        "Initialize `MlaKVCache_0` by calling `init_mla_kv_caches` if not in training mode."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Attention",
                        "self.init_mla_kv_caches"
                    ],
                    "notes": [
                        "Several MLA-specific attributes are set before calling `super().__init__` because they are used within the parent's initialization process (specifically in `_init_projections`)."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the MLA-specific linear projection layers (DenseGeneral) and normalization layers (RMSNorm).",
                    "input": {
                        "shape": "Takes `inputs_q_shape` and `inputs_kv_shape` tuples.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert that the configuration is valid for MLA.",
                        "Initialize query projection layers, either a standard `DenseGeneral` or a LoRA path (`wq_a`, `q_norm`, `wq_b`) based on `q_lora_rank`.",
                        "Initialize the key/value LoRA projection path (`wkv_a`, `kv_norm`, `wkv_b`).",
                        "Calculate and set the `softmax_scale` for attention scores.",
                        "Initialize the output projection layer (`out`).",
                        "Initialize the paged attention operator if configured."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "RMSNorm",
                        "paged_attention.PagedAttentionOp"
                    ],
                    "notes": [
                        "This method is called by the `__init__` of the parent `Attention` class and defines the core trainable weights for the MLA layer."
                    ]
                },
                "mla_query_projection": {
                    "purpose": "Projects the input query tensor, optionally applies LoRA, splits it into NoPe and RoPE parts, applies rotary embeddings, and scales the result.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, embed_dim], inputs_positions: [batch, q_length]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Apply standard or LoRA-based projection to get an intermediate query tensor `q`.",
                        "Split `q` into non-positional (`q_nope`) and positional (`q_pe`) parts.",
                        "Apply rotary embeddings to `q_pe`.",
                        "Concatenate `q_nope` and the modified `q_pe`.",
                        "Scale the result by `softmax_scale`.",
                        "Apply logical constraints for sharding."
                    ],
                    "output": {
                        "shape": "[batch, q_length, num_query_heads, qk_head_dim]"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "RMSNorm",
                        "self.apply_rotary_embedding"
                    ],
                    "notes": [
                        "The scaling by `softmax_scale` is applied directly to the projected query, which may differ from other attention implementations."
                    ]
                },
                "mla_get_key_value": {
                    "purpose": "Generates the final key and value tensors from the main latent component and the RoPE component of the key.",
                    "input": {
                        "shape": "low_rank_main: [batch, kv_length, kv_lora_rank], key_rope: [batch, kv_length, 1, qk_rope_head_dim]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Project `low_rank_main` using `wkv_b` to get a combined tensor.",
                        "Split the result into `key_nope` and `value`.",
                        "Broadcast `key_rope` to match the number of heads.",
                        "Concatenate `key_nope` and `key_rope` to form the final `key`.",
                        "Apply logical constraints for sharding."
                    ],
                    "output": {
                        "shape": "Returns a tuple (key, value) with shapes [batch, kv_length, num_kv_heads, qk_head_dim] and [batch, kv_length, num_kv_heads, v_head_dim] respectively."
                    },
                    "dependencies": [
                        "self.wkv_b"
                    ],
                    "notes": [
                        "This is a helper method used to reconstruct K and V from their latent representations, both in the initial projection and after retrieving from the cache."
                    ]
                },
                "init_mla_kv_caches": {
                    "purpose": "Initializes the specialized `MlaKVCache` module used for inference.",
                    "input": {
                        "shape": "inputs_kv_shape: A tuple representing the KV input shape.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract `batch_size` from the input shape.",
                        "Instantiate `kvcache.MlaKVCache` with configured parameters."
                    ],
                    "output": {
                        "shape": "An instance of `kvcache.MlaKVCache`."
                    },
                    "dependencies": [
                        "kvcache.MlaKVCache"
                    ],
                    "notes": [
                        "Uses a placeholder sequence length during initialization, as the cache's internal shapes are based on max lengths, not the initial input length."
                    ]
                },
                "update_mla_kv_caches": {
                    "purpose": "Updates and retrieves the MLA KV caches during inference, then reconstructs the full key and value tensors from the cached latent components.",
                    "input": {
                        "shape": "low_rank_main: [batch, kv_length, kv_lora_rank], key_rope: [batch, kv_length, 1, qk_rope_head_dim], decoder_segment_ids: [batch, kv_length]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Call the `MlaKVCache_0` module with the latent key components to get cached prefill and autoregressive data.",
                        "If prefill data is returned, reconstruct the full prefill key and value using `mla_get_key_value`.",
                        "If autoregressive data is returned, reconstruct the full AR key and value using `mla_get_key_value`.",
                        "Return the reconstructed caches."
                    ],
                    "output": {
                        "shape": "A list containing two elements: the reconstructed prefill KV cache and the reconstructed autoregressive KV cache."
                    },
                    "dependencies": [
                        "self.MlaKVCache_0",
                        "self.mla_get_key_value"
                    ],
                    "notes": []
                },
                "mla_kv_projection": {
                    "purpose": "Projects the input KV tensor into key and value, applies rotary embeddings, and updates the KV cache for inference.",
                    "input": {
                        "shape": "inputs: [batch, kv_length, embed_dim], inputs_positions: [batch, kv_length]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Apply the first linear projection (`wkv_a`) to get a low-rank representation.",
                        "Split the result into `low_rank_main` and `low_rank_rope`.",
                        "Normalize `low_rank_main` with `kv_norm`.",
                        "Apply rotary embeddings to `low_rank_rope` to get `key_rope`.",
                        "Generate the full `key` and `value` for the current step using `mla_get_key_value`.",
                        "If in inference mode, update the KV cache using either `update_kv_caches` (naive) or `update_mla_kv_caches` (specialized)."
                    ],
                    "output": {
                        "shape": "A tuple containing the current key, value, and the cached values: `(key, value, cached_values)`."
                    },
                    "dependencies": [
                        "self.wkv_a",
                        "self.kv_norm",
                        "self.apply_rotary_embedding",
                        "self.mla_get_key_value",
                        "self.update_mla_kv_caches"
                    ],
                    "notes": [
                        "This method encapsulates the entire key/value generation and caching pipeline for MLA."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the main forward pass for the MLA layer, orchestrating projection, attention computation, and output projection.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, embed_dim], inputs_kv: [batch, kv_length, embed_dim]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Apply logical constraints to inputs for sharding.",
                        "Call `mla_query_projection` to get the query tensor.",
                        "Call `mla_kv_projection` to get key, value, and cached values.",
                        "Apply gradient checkpointing names.",
                        "Compute attention using either `ds_paged_attention_op` or `attention_op`.",
                        "Apply the final output projection.",
                        "Apply a final gradient checkpointing name."
                    ],
                    "output": {
                        "shape": "[batch, q_length, embed_dim]"
                    },
                    "dependencies": [
                        "self.mla_query_projection",
                        "self.mla_kv_projection",
                        "self.ds_paged_attention_op",
                        "self.attention_op",
                        "self.out_projection"
                    ],
                    "notes": [
                        "This is the main entry point for the layer's forward computation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_compute_axis_order",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_compute_axis_order(s: AxisIdxes) -> None:\n  valid_compute_axis_order = ((0, 1, 2, 3), (0, 2, 1, 3))\n  if s not in valid_compute_axis_order:  # currently supported compute_axis_order\n    raise ValueError(\"Invalid compute_axis_order was passed. Valid options \", valid_compute_axis_order)",
        "analysis": {
            "functionality": "Validates that a given tuple representing a compute axis order is one of the two supported configurations, raising a ValueError if it is not.",
            "usage": "Call this function with a tuple of four integers (type `AxisIdxes`) to check if it's a valid compute axis order. The function returns `None` on success or raises a `ValueError` on failure. The valid orders are `(0, 1, 2, 3)` and `(0, 2, 1, 3)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#apply_mask_to_logits",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def apply_mask_to_logits(logits: Array, mask: Array):\n  \"\"\"Applies a floating-point mask to a set of logits.\n\n  The mask is represented as a tensor with some dtype where 0 represents true and values\n  below a large negative number (here set to\n  get_large_negative_number(logits.dtype) / 2) represent false. Applying the mask\n  leaves the logits alone in the true case and replaces them by\n  get_large_negative_number(logits.dtype) in the false case. Previously, this was\n  done by adding the logits to the mask; however, this leads to a bad fusion\n  decision in the compiler that saves the values in memory rather than\n  just the predicate. This implementation avoids that problem.\n\n  from https://github.com/google/praxis/blob/4712a6b9ee13e224b86e235ff55f7c6bab9fbab3/praxis/py_utils.py#L706\n\n  Args:\n    logits: A JTensor of logit values.\n    mask: A JTensor of mask values with the encoding described in the\n      function documentation.\n\n  Returns:\n    Masked logits.\n  \"\"\"\n  return jnp.where((mask >= DEFAULT_MASK_VALUE * 0.5), logits, DEFAULT_MASK_VALUE)",
        "analysis": {
            "module_type": "logit_masking",
            "purpose": "Applies a floating-point mask to a logit tensor, replacing values with a large negative number where the mask indicates 'false'.",
            "input": {
                "shape": "logits: A JTensor of any shape. mask: A JTensor with a shape broadcastable to the logits tensor.",
                "dtype": "float32"
            },
            "processing_steps": [
                "Create a boolean condition by checking where the input `mask` tensor's values are greater than or equal to `DEFAULT_MASK_VALUE * 0.5`.",
                "Use `jnp.where` to return the original `logits` value if the condition is true, or `DEFAULT_MASK_VALUE` if the condition is false."
            ],
            "output": {
                "shape": "Same as the input `logits` tensor."
            },
            "dependencies": [
                "jax.numpy.where",
                "MaxText.common_types.DEFAULT_MASK_VALUE"
            ],
            "parameters": {},
            "notes": [
                "The mask is encoded such that 0.0 represents positions to keep (true) and a large negative value represents positions to mask out (false).",
                "This implementation uses `jnp.where` instead of addition to avoid specific compiler fusion issues that could negatively impact performance, as detailed in the docstring."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_flash_attention_with_sinks_on_gpu",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_flash_attention_with_sinks_on_gpu(sinks: Array | None) -> None:\n  \"\"\"Helper function to check for sinks with flash attention on GPU.\"\"\"\n  if sinks is not None:\n    raise ValueError(\"The flash attention with sinks is not supported on GPU yet.\")",
        "analysis": {
            "module_type": "gpu_compatibility_check",
            "purpose": "Validates that attention sinks are not used with flash attention on GPU, as this feature is currently unsupported, and raises an error if they are.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the input `sinks` is not None.",
                "If `sinks` is not None, raise a ValueError indicating the feature is unsupported on GPU."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This is a helper function designed to enforce a hardware-specific implementation constraint.",
                "The function's primary purpose is its side effect: raising a `ValueError` to halt execution for an invalid configuration.",
                "It is called within the `apply_attention` method before dispatching to GPU-specific flash attention implementations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#ChunkedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class ChunkedCausalMask(splash_attention_mask._ComputableMask):  # pylint: disable=protected-access\n  \"\"\"Lazy chunked causal mask.\n\n  Attention is causal within each chunk (0, K), (K, 2K), (2K, 3K), ... tokens attend to each other but not across chunks.\n  Llama4 models use interleaved chunk attention along with global attention.\n\n  This mask class inherits from splash_attention_mask._ComputableMask and is designed to be used with Splash Attention.\n  It allows the mask logic to be computed on-the-fly or fused into the attention kernel, avoiding the memory cost of\n  materializing the full (sequence_length, sequence_length) boolean mask array, which can be prohibitive for long sequences.\n\n  Attributes:\n    chunk_size: The size of each attention chunk.\n  \"\"\"\n\n  chunk_size: int\n\n  def __init__(\n      self,\n      shape: tuple[int, int],\n      chunk_size: int,\n      shard_count: int = 1,\n  ):\n    if chunk_size <= 0:\n      raise ValueError(\"chunk_size must be positive\")\n    self.chunk_size = chunk_size\n\n    # Define the mask function for chunk attention\n    def chunked_causal_mask_function(q_ids, kv_ids):\n      \"\"\"Computes the mask logic for the given slice indices.\"\"\"\n      if q_ids.size == 0 or kv_ids.size == 0:\n        return np.empty((q_ids.shape[0], kv_ids.shape[1]), dtype=np.bool_)\n\n      # Condition 1: Same chunk\n      q_chunk = q_ids // self.chunk_size\n      kv_chunk = kv_ids // self.chunk_size\n      same_chunk = q_chunk == kv_chunk\n\n      # Condition 2: Causal\n      causal = q_ids >= kv_ids\n\n      return same_chunk & causal\n\n    # Initialize the parent ComputableMask with this function\n    super().__init__(\n        shape=shape,\n        mask_function=chunked_causal_mask_function,\n        shard_count=shard_count,\n    )\n\n  # Implement equality and hashing based on relevant attributes\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n    # Compare shape, chunk_size, and the underlying q_sequence array\n    return (\n        self.shape == other.shape\n        and self.chunk_size == other.chunk_size\n        and np.array_equal(self.q_sequence, other.q_sequence)\n    )\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.chunk_size,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "chunked_causal_mask",
            "purpose": "Implements a lazy, on-the-fly computable mask for chunked causal attention, where attention is causal within fixed-size chunks but not across them, designed for use with Splash Attention kernels.",
            "input": {
                "shape": "N/A (Initialized with configuration parameters, not a tensor).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "In the constructor, defines an inner `chunked_causal_mask_function` that takes query and key-value indices (`q_ids`, `kv_ids`).",
                "This function calculates if query and key-value indices belong to the same chunk using integer division by `chunk_size`.",
                "It then calculates if the query index is greater than or equal to the key-value index to enforce causality.",
                "The same-chunk and causal conditions are combined using a logical AND.",
                "The parent `_ComputableMask` class is initialized with the overall mask shape and the defined `chunked_causal_mask_function`."
            ],
            "output": {
                "shape": "The class instance itself. The underlying mask function produces a boolean array of shape `[num_q_indices, num_kv_indices]` when called by the attention kernel."
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "numpy"
            ],
            "parameters": {
                "chunk_size": "The size of each attention chunk. Attention is causal within a chunk but disallowed between different chunks.",
                "shape": "A tuple `(q_seq_len, kv_seq_len)` defining the full dimensions of the attention mask.",
                "shard_count": "The number of shards for the mask computation."
            },
            "notes": [
                "This is a 'lazy' mask, meaning the full boolean mask is not materialized in memory. Instead, the logic is computed on-the-fly by the Splash Attention kernel, which is memory-efficient for long sequences.",
                "The mask enforces that tokens can only attend to other tokens within the same chunk (e.g., tokens 0 to K-1 form a chunk, K to 2K-1 form another) and only to previous or current tokens within that chunk."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the ChunkedCausalMask object with its shape, chunk size, and sharding configuration.",
                    "input": {
                        "shape": "shape: tuple[int, int], chunk_size: int, shard_count: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates that `chunk_size` is positive.",
                        "Defines the `chunked_causal_mask_function` which encapsulates the core masking logic.",
                        "Calls the parent `_ComputableMask` constructor, passing the shape, the custom mask function, and shard count."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "splash_attention_mask._ComputableMask"
                    ],
                    "notes": [
                        "The actual mask computation logic is defined in a nested function and passed to the superclass."
                    ]
                },
                "__eq__": {
                    "purpose": "Defines equality comparison based on shape, chunk size, and the underlying query sequence array.",
                    "input": {
                        "shape": "other: object",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the other object is an instance of `ChunkedCausalMask`.",
                        "Compare `self.shape` with `other.shape`.",
                        "Compare `self.chunk_size` with `other.chunk_size`.",
                        "Compare `self.q_sequence` with `other.q_sequence` using `np.array_equal`."
                    ],
                    "output": {
                        "shape": "boolean"
                    },
                    "dependencies": [
                        "numpy.array_equal"
                    ],
                    "notes": [
                        "Ensures two mask objects are considered equal if they will produce the same mask."
                    ]
                },
                "__hash__": {
                    "purpose": "Computes a hash for the object, making it usable in hash-based data structures like dictionaries or sets.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Creates a tuple of the object's type, shape, chunk_size, and the byte representation of the `q_sequence` array.",
                        "Computes the hash of this tuple."
                    ],
                    "output": {
                        "shape": "int"
                    },
                    "dependencies": [],
                    "notes": [
                        "Hashing is based on the same attributes used for the equality check to maintain consistency."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_generate_chunk_attention_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _generate_chunk_attention_mask(mask_shape: tuple[int, int], chunk_size: int, q_offset: int = 0) -> jax.Array:\n  \"\"\"Generates an explicit boolean mask for chunked causal attention.\n\n  This function computes the full boolean mask array where True indicates\n  attention is allowed based on chunked causal rules (tokens attend only\n  within the same chunk, and causally within that chunk).\n\n  Args:\n    mask_shape: The desired shape of the mask (q_seq_len, kv_seq_len).\n    chunk_size: The size of the attention chunks.\n\n  Returns:\n    A boolean mask of shape `mask_shape` where True indicates attention is\n    allowed according to chunked causal rules, and False otherwise.\n\n  Raises:\n    ValueError: If chunk_window_size is None or not positive.\n  \"\"\"\n\n  row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0) + q_offset\n  col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n  if chunk_size <= 0:\n    raise ValueError(\"chunk_size must be positive\")\n\n  # chunk mask calculation\n  same_chunk = (row_ids // chunk_size) == (col_ids // chunk_size)\n  chunk_mask = same_chunk & (row_ids >= col_ids)\n  return chunk_mask",
        "analysis": {
            "module_type": "chunked_causal_attention_mask_generator",
            "purpose": "Generates an explicit boolean mask for chunked causal attention, where tokens attend only within the same chunk and causally within that chunk.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Generate row indices (`row_ids`) for the given `mask_shape` using `jax.lax.broadcasted_iota` and add the `q_offset`.",
                "Generate column indices (`col_ids`) for the given `mask_shape` using `jax.lax.broadcasted_iota`.",
                "Validate that `chunk_size` is positive, raising a ValueError if it is not.",
                "Calculate a boolean mask `same_chunk` where `(row_ids // chunk_size) == (col_ids // chunk_size)`.",
                "Combine the `same_chunk` mask with a causal condition (`row_ids >= col_ids`) using a logical AND to create the final `chunk_mask`.",
                "Return the `chunk_mask`."
            ],
            "output": {
                "shape": "Matches the input `mask_shape` (e.g., [q_seq_len, kv_seq_len])."
            },
            "dependencies": [
                "jax.lax.broadcasted_iota",
                "jax.numpy"
            ],
            "parameters": {
                "mask_shape": "A tuple `(q_seq_len, kv_seq_len)` specifying the desired shape of the output mask.",
                "chunk_size": "The integer size of the attention chunks.",
                "q_offset": "An optional integer offset added to the query/row indices, used for chunked prefill scenarios."
            },
            "notes": [
                "The function returns a materialized boolean JAX array, where True indicates attention is allowed.",
                "This is a helper function used to create masks for attention mechanisms that are not using lazy evaluation (like Splash Attention's `ChunkedCausalMask`)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_block_mask_indices",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_block_mask_indices(bidirectional_mask):\n  \"\"\"Creates block mask identifying segments based on a bidirectional mask.\n\n  Args:\n    bidirectional_mask: boolean mask, e.g. [011110011010].\n\n  Returns:\n    block mask for segments, e.g. [011110022030].\n  \"\"\"\n  # Left pad 0.\n  padded_mask = jnp.pad(bidirectional_mask, [(0, 0), (1, 0)], constant_values=0)\n  boundary = padded_mask[..., 1:] > padded_mask[..., :-1]\n  numbered_boundary = jnp.cumsum(boundary, axis=-1)\n  return bidirectional_mask * numbered_boundary",
        "analysis": {
            "module_type": "segment_identifier",
            "purpose": "Identifies contiguous segments of True/1 values in a boolean mask and assigns a unique, incrementing integer ID to each segment, while preserving zeroed-out positions.",
            "input": {
                "shape": "[batch_size, sequence_length]",
                "dtype": "boolean or integer"
            },
            "processing_steps": [
                "Pad the input mask with a zero at the beginning of the sequence dimension.",
                "Identify the starting boundary of each segment by comparing each element to its predecessor.",
                "Generate segment IDs by performing a cumulative sum along the sequence axis on the boundary mask.",
                "Multiply the original mask with the generated segment IDs to zero out the positions that were not part of any segment."
            ],
            "output": {
                "shape": "[batch_size, sequence_length]"
            },
            "dependencies": [
                "jax.numpy.pad",
                "jax.numpy.cumsum"
            ],
            "parameters": {},
            "notes": [
                "This is a helper function used in the creation of block-wise attention masks.",
                "The segment IDs start from 1.",
                "For an input like [0, 1, 1, 1, 0, 1, 1], the output will be [0, 1, 1, 1, 0, 2, 2]."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_bidirectional_block_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_bidirectional_block_mask(bidirectional_mask):\n  \"\"\"Creates bidirectional block mask from bidirectional_mask, where True corresponds to image tokens.\n  bidirectional_mask shape: [B, L]\n  bidirectional_block_mask shape: [B, L, L]\n  Examples:\n  bidirectional_mask = [[0, 1, 1, 1, 0, 0]]\n  bidirectional_block_mask = [[\n      [False, False, False, False, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False, False, False, False, False, False],\n      [False, False, False, False, False, False],\n  ]]\n  \"\"\"\n  q_block_indices = _make_block_mask_indices(bidirectional_mask)\n  kv_block_indices = q_block_indices\n  bidirectional_block_mask = (kv_block_indices[:, None, :] == q_block_indices[..., None]) & (\n      q_block_indices[..., None] > 0\n  )\n  return bidirectional_block_mask",
        "analysis": {
            "module_type": "bidirectional_block_mask_generator",
            "purpose": "Creates a 2D square attention mask from a 1D sequence mask, allowing bidirectional attention only within contiguous segments marked as 'True' or non-zero.",
            "input": {
                "shape": "[batch_size, sequence_length]",
                "dtype": "int or bool"
            },
            "processing_steps": [
                "Call `_make_block_mask_indices` to convert the input mask into a tensor of block indices, where each contiguous non-zero segment gets a unique positive integer ID.",
                "Assign the generated block indices to both query and key indices.",
                "Broadcast the query and key indices to create a square matrix of shape [B, L, L].",
                "Create a boolean mask where `True` indicates that the query and key tokens belong to the same block index.",
                "Create a second boolean mask to filter out positions that are not part of any block (i.e., where the index is 0).",
                "Combine the two masks with a logical AND to produce the final mask.",
                "Return the resulting bidirectional block mask."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, sequence_length]"
            },
            "dependencies": [
                "_make_block_mask_indices",
                "jax.numpy"
            ],
            "parameters": {},
            "notes": [
                "The function is designed to enable full attention within specific segments of a sequence (e.g., image tokens in a multimodal model) while isolating them from other segments.",
                "The example in the docstring clearly illustrates how a segment of `1`s in the input `[0, 1, 1, 1, 0, 0]` results in a 3x3 block of `True` values in the output attention mask."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#attention_op_as_linen",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def attention_op_as_linen(\n    *,\n    config: Config,\n    mesh: Mesh,\n    attention_kernel: str,\n    max_target_length: int,\n    num_query_heads: int,\n    num_kv_heads: int,\n    float32_qk_product: bool = False,\n    max_prefill_predict_length: int = -1,\n    float32_logits: bool = False,\n    flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n    flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n    flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n    flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n    ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    reshape_q: bool = False,\n    dropout_rate: float = 0.0,\n    dtype: DType = jnp.float32,\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    chunk_attn_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n):\n  \"\"\"A factory function to create an AttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `AttentionOp` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      AttentionOp,\n      config=config,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      max_target_length=max_target_length,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      float32_qk_product=float32_qk_product,\n      max_prefill_predict_length=max_prefill_predict_length,\n      float32_logits=float32_logits,\n      flash_axis_names_q=flash_axis_names_q,\n      flash_axis_names_q_ep=flash_axis_names_q_ep,\n      flash_axis_names_kv=flash_axis_names_kv,\n      flash_axis_names_kv_ep=flash_axis_names_kv_ep,\n      flash_axis_names_splash_kernel=flash_axis_names_splash_kernel,\n      flash_axis_names_splash_kernel_ep=flash_axis_names_splash_kernel_ep,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      ragged_qkv_axis_names=ragged_qkv_axis_names,\n      ragged_lengths_names=ragged_lengths_names,\n      compute_axis_order=compute_axis_order,\n      key_axis_order=key_axis_order,\n      reshape_q=reshape_q,\n      dropout_rate=dropout_rate,\n      dtype=dtype,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      chunk_attn_window_size=chunk_attn_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "linen_attention_op_wrapper",
            "purpose": "A factory function that wraps the NNX-based `AttentionOp` class, making it usable as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `AttentionOp` class into a Linen-compatible module.",
                "Forwards all its arguments (config, mesh, attention_kernel, etc.) to the `AttentionOp` constructor via the wrapper.",
                "Adds `metadata_fn=variable_to_logically_partitioned` to the wrapped module's configuration for parameter partitioning."
            ],
            "output": {
                "shape": "Returns a Flax Linen module class, not a tensor. The shape of the module's output tensor depends on its `__call__` method."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "AttentionOp",
                "variable_to_logically_partitioned",
                "Config",
                "Mesh",
                "Quant",
                "KVQuant",
                "AttentionType"
            ],
            "parameters": {
                "config": "The main configuration object for the model.",
                "mesh": "The JAX device mesh for distributed computation.",
                "attention_kernel": "Specifies the attention implementation to use (e.g., 'dot_product', 'flash', 'autoselected').",
                "num_query_heads": "The number of attention heads for the query.",
                "num_kv_heads": "The number of attention heads for the key and value, enabling Grouped-Query Attention (GQA).",
                "attention_type": "The type of attention mechanism to use, such as GLOBAL, LOCAL_SLIDING, or CHUNK.",
                "use_ragged_attention": "A boolean flag to enable ragged attention for autoregressive decoding."
            },
            "notes": [
                "This function serves as a bridge to use an NNX-defined module within a Linen-based model architecture.",
                "The returned object is a Linen module that can be instantiated and used like any other native Linen layer."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#AttentionOp",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class AttentionOp(nnx.Module):\n  \"\"\"Attention operation\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      attention_kernel: str,\n      max_target_length: int,\n      num_query_heads: int,\n      num_kv_heads: int,\n      float32_qk_product: bool = False,\n      max_prefill_predict_length: int = -1,\n      float32_logits: bool = False,\n      flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n      flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n      flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n      flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n      ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      reshape_q: bool = False,\n      dropout_rate: float = 0.0,\n      dtype: DType = jnp.float32,\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      chunk_attn_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      rngs: nnx.Rngs | None = None,\n  ):\n    \"\"\"Initializes the AttentionOp module.\n\n    Args:\n      config: The configuration for the model.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use.\n      max_target_length: The maximum target length.\n      num_query_heads: The number of query heads.\n      num_kv_heads: The number of key/value heads.\n      float32_qk_product: Whether to compute qk_product in float32.\n      max_prefill_predict_length: The maximum prefill predict length.\n      float32_logits: Whether to compute logits in float32.\n      flash_axis_names_kv: The logical axis names for the KV cache in flash attention.\n      flash_axis_names_q: The logical axis names for the query in flash attention.\n      flash_axis_names_splash_kernel: The logical axis names for the splash attention kernel.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      ragged_qkv_axis_names: The logical axis names for ragged QKV tensors.\n      ragged_lengths_names: The logical axis names for ragged lengths.\n      compute_axis_order: The order of axes for computation.\n      key_axis_order: The order of axes for the key.\n      ... and other configuration parameters.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.max_target_length = max_target_length\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.float32_qk_product = float32_qk_product\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.float32_logits = float32_logits\n    self.flash_axis_names_q = flash_axis_names_q\n    self.flash_axis_names_q_ep = flash_axis_names_q_ep\n    self.flash_axis_names_kv = flash_axis_names_kv\n    self.flash_axis_names_kv_ep = flash_axis_names_kv_ep\n    self.flash_axis_names_splash_kernel = flash_axis_names_splash_kernel\n    self.flash_axis_names_splash_kernel_ep = flash_axis_names_splash_kernel_ep\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.ragged_qkv_axis_names = ragged_qkv_axis_names\n    self.ragged_lengths_names = ragged_lengths_names\n    self.compute_axis_order = compute_axis_order\n    self.key_axis_order = key_axis_order\n    self.reshape_q = reshape_q\n    self.dropout_rate = dropout_rate\n    self.dtype = dtype\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.chunk_attn_window_size = chunk_attn_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n\n    def maybe_create_nnx(einsum, *args):\n      if isinstance(einsum, nn.Module):\n        return nnx_wrappers.ToNNX(einsum, rngs=rngs).lazy_init(*args)\n      return einsum\n\n    # qk_product\n    if self.kv_quant:\n      # Dummy inputs for lazy initialization\n      b = 1\n      t_prefill = self.max_prefill_predict_length\n      t_ar = 1  # Autoregressive mode has a query length of 1\n      n = self.num_query_heads\n      n_kv = self.num_kv_heads\n      d = self.config.head_dim\n      g = n // n_kv\n      s_prefill = self.max_prefill_predict_length\n      s_ar = self.max_target_length\n\n      # Dummy query/key/value shapes as before...\n      dummy_query_prefill = jnp.zeros((b, t_prefill, n_kv, g, d), dtype=self.dtype)\n      dummy_key_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_query_ar = jnp.zeros((b, t_ar, n_kv, g, d), dtype=self.dtype)\n      dummy_key_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      dummy_attn_weights_prefill = jnp.zeros((b, n_kv, g, t_prefill, s_prefill), dtype=jnp.float32)\n      dummy_value_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_attn_weights_ar = jnp.zeros((b, n_kv, g, t_ar, s_ar), dtype=jnp.float32)\n      dummy_value_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      # Prefill AqtEinsum instances\n      self.AqtEinsum_0 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_prefill, dummy_key_prefill\n      )\n      self.AqtEinsum_1 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_prefill,\n          dummy_value_prefill,\n      )\n      # Autoregressive AqtEinsum instances\n      self.AqtEinsum_2 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_ar, dummy_key_ar\n      )\n      self.AqtEinsum_3 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_ar,\n          dummy_value_ar,\n      )\n    else:\n      self.AqtEinsum_0 = jnp.einsum\n      self.AqtEinsum_1 = jnp.einsum\n      self.AqtEinsum_2 = jnp.einsum\n      self.AqtEinsum_3 = jnp.einsum\n\n  def check_attention_inputs(self, query: Array, key: Array | KVTensor, value: Array | KVTensor) -> None:\n    \"\"\"Check attention inputs.\"\"\"\n\n    assert key.ndim == value.ndim, f\"k (dim {key.ndim}), v (dim {value.ndim}) must have same rank.\"\n    assert (\n        query.shape[:-3] == key.shape[:-3] == value.shape[:-3]\n    ), f\"{query.shape[:-3]=}, {key.shape[:-3]=}, {value.shape[:-3]=} batch dims must match.\"\n    assert key.shape[-2] == value.shape[-2], \"k, v num_kv_heads must match.\"\n    assert key.shape[-3] == value.shape[-3], \"k, v lengths must match.\"\n    assert query.shape[-1] == key.shape[-1], \"q, k depths must match.\"\n\n  def generate_attention_mask(\n      self,\n      query,\n      key,\n      decoder_segment_ids: Array | None,\n      model_mode: str,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n  ) -> Array | None:\n    \"\"\"Generates a combined attention mask for Transformer models.\n\n    This function constructs an attention mask by potentially combining\n    several types of masks based on the input parameters and model\n    configuration. The generated mask dictates which query-key pairs are\n    allowed to attend to each other.\n\n    The masking logic can enforce:\n    1.  **Sequence Separation:** Using `decoder_segment_ids`, attention is\n      confined within distinct sequences in a batch. This is crucial when\n      multiple unrelated sequences are packed together.\n    2.  **Causality:** Preventing attention to future positions. This is\n      standard for autoregressive decoding. For chunked prefill, as\n      described in the SARATHI paper [2], causality is adjusted based\n      on `previous_chunk` information.\n    3.  **Specialized Attention Patterns:** Depending on `self.attention_type`,\n      it can apply:\n      * Local Sliding Window Attention: Restricts attention to a\n          fixed-size window around each query position.\n      * Chunk Attention: Divides sequences into chunks and applies\n          masking at the chunk level.\n    4.  **Bidirectional Attention for Sub-sequences:** If `bidirectional_mask`\n      is provided (e.g., for image tokens in a multimodal model),\n      those parts of the sequence can attend bidirectionally, and this\n      mask is OR-ed with other generated masks.\n\n    The overall approach and specific masking techniques are influenced by\n    efficient attention mechanisms like those found in the Pallas MHA\n    Flash Attention reference [1].\n\n    Args:\n      query: The query tensor, typically of shape\n          `[batch_size, q_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      key: The key tensor, typically of shape\n          `[batch_size, kv_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      decoder_segment_ids: Optional `Array` of shape `[batch_size, q_sequence_length]`.\n          Identifies distinct sequences within the batch. Attention is\n          restricted to elements within the same segment ID. In autoregressive\n          mode, specific values (e.g., `common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR`)\n          can mark the currently active sequence for decoding.\n      model_mode: A string (e.g., `common_types.MODEL_MODE_AUTOREGRESSIVE`,\n          `MODEL_MODE_PREFILL`) indicating the operational\n          mode. This significantly influences mask generation, particularly\n          how causality and segment separation are handled.\n      previous_chunk: Optional. Information about previously processed\n          key/value chunks, often a tensor representing the previous keys/values.\n          Used to correctly offset causal masks in chunked attention or\n          streaming scenarios. Its shape might be\n          `[batch_size, prev_kv_sequence_length, ...]`.\n      bidirectional_mask: Optional `Array` of shape `[batch_size, kv_sequence_length]`.\n          If provided, this boolean mask indicates tokens (e.g., image tokens)\n          that are allowed to attend bidirectionally. The resulting\n          block-wise bidirectional mask is combined with other masks using a\n          logical OR.\n\n    Returns:\n      An `Array` representing the attention mask, broadcastable to the shape\n      `[batch_size, num_heads, q_sequence_length, kv_sequence_length]`.\n      Positions with `0.0` allow attention, while positions with\n      `DEFAULT_MASK_VALUE` (a large negative number) prevent it.\n      Returns `None` if no masking is determined to be necessary based on\n      the inputs and configuration.\n\n    References:\n      [1] JAX Pallas MHA Flash Attention:\n          https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py\n      [2] SARATHI: Efficient LLM Inference by Piggybacking Decodes with\n          Chunked Prefills - ArXiv:2308.16369 (https://arxiv.org/abs/2308.16369)\n    \"\"\"\n    mask = None\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      mask = decoder_segment_ids[:, None, None, None, :] == DECODING_ACTIVE_SEQUENCE_INDICATOR\n    elif decoder_segment_ids is not None:\n      mask = decoder_segment_ids[:, :, None] == decoder_segment_ids[:, None, :]\n      mask = mask[:, None, None, :, :]\n\n    _, q_seq_len, _, _ = query.shape\n    _, kv_seq_len, _, _ = key.shape\n    next_pos = 0\n    if previous_chunk is not None:\n      next_pos = previous_chunk.shape[1]\n      if mask is not None:\n        mask = mask[:, :, :, next_pos : next_pos + q_seq_len, :]\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and q_seq_len == 1:\n      # In autoregression, the query position is the last position in the KV sequence.\n      next_pos = kv_seq_len - 1\n\n    causal_mask = None\n    # We enforce causality except for AUTOREGRESSION\n    if model_mode != MODEL_MODE_AUTOREGRESSIVE and self.attention_type != AttentionType.FULL:\n      mask_shape = (q_seq_len, kv_seq_len)\n      # row_ids indicates the position of query\n      # col_ids indicates the position of kv\n      row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)\n      col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n      # Attention mask for chunked prefill is generated in the same way\n      # as mentioned in SARATHI - https://arxiv.org/abs/2308.16369\n      causal_mask = (col_ids <= row_ids + next_pos)[None, None, None, :, :]\n\n    output_mask = None\n    if (mask is not None) and (causal_mask is not None):\n      output_mask = jnp.logical_and(mask, causal_mask)\n    elif mask is not None:\n      output_mask = mask\n    elif causal_mask is not None:\n      output_mask = causal_mask\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and output_mask is not None:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n\n      row_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (q_seq_len, 1), 0) + next_pos\n      col_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (1, kv_seq_len), 1)\n      sliding_mask = (col_ids_sliding > (row_ids_sliding - self.sliding_window_size)) & (\n          col_ids_sliding <= row_ids_sliding\n      )\n      output_mask = sliding_mask * output_mask\n    elif self.attention_type == AttentionType.CHUNK and output_mask is not None:\n      mask_shape = (q_seq_len, kv_seq_len)\n      chunk_mask = _generate_chunk_attention_mask(\n          mask_shape=(q_seq_len, kv_seq_len), chunk_size=self.chunk_attn_window_size, q_offset=next_pos\n      )\n      output_mask = chunk_mask * output_mask\n\n    if bidirectional_mask is not None:\n      image_mask = _make_bidirectional_block_mask(bidirectional_mask)\n      output_mask = output_mask | image_mask[:, None, None, ...]\n\n    return jnp.where(output_mask, 0.0, DEFAULT_MASK_VALUE) if output_mask is not None else None\n\n  def calculate_moba_gate_logic(self, q_item, k_item, q_pos_item):\n    \"\"\"Computes the block-level MoBA gating intermediates for one batch item.\n\n    Args:\n      q_item: Query tensor shaped `[q_len, n_q_heads, head_dim]`.\n      k_item: Key tensor shaped `[kv_len, n_kv_heads, head_dim]`.\n      q_pos_item: Absolute query positions shaped `[q_len]`, used to derive the\n        chunk index for each query.\n        For example, during prefill after 128 tokens\n        have been processed `q_pos_item` is `jnp.arange(128, 128 + q_len)`,\n        while in autoregressive decode with a single query token it is\n        `jnp.array([kv_len - 1])`.\n\n    Returns:\n      `need_attend`, a boolean mask of shape `[n_kv_heads, g, q_len, num_block]`\n      indicating which key blocks each query should attend to. The additional\n      values in the returned tuple are debug intermediates used for logging and\n      diagnostics when inspecting the gating behaviour.\n    \"\"\"\n    q_len, n_q_heads, head_dim = q_item.shape\n    kv_len, n_kv_heads, _ = k_item.shape\n    g = n_q_heads // n_kv_heads\n\n    q_item_f32 = q_item.astype(jnp.float32).reshape(q_len, n_kv_heads, g, head_dim)  # grouped-query attention (GQA)\n\n    moba_chunk_size = self.config.moba_chunk_size\n    moba_topk = self.config.moba_topk\n\n    num_block = math.ceil(kv_len / moba_chunk_size)\n\n    block_ids = jnp.arange(kv_len, dtype=jnp.int32) // moba_chunk_size  # chunk index for each key position\n    # Sum key vectors per chunk so we can later average within each block.\n    key_gate_weight_sum = jax.ops.segment_sum(\n        k_item.astype(jnp.float32), block_ids, num_segments=num_block\n    )  # [num_block, n_kv_heads, head_dim]\n    # Count how many tokens end up in each chunk so we can take the mean.\n    block_counts = jax.ops.segment_sum(\n        jnp.ones((kv_len,), dtype=jnp.float32), block_ids, num_segments=num_block\n    )  # [num_block]\n    # Mean Pooling, Avoid division by zero for empty blocks.\n    key_gate_weight = key_gate_weight_sum / jnp.maximum(\n        block_counts[:, None, None], 1\n    )  # [num_block, n_kv_heads, head_dim]\n\n    # Take the dot product between each query and every key chunk to get a score.\n    gate = jnp.einsum(\"skgd,Nkd->kgsN\", q_item_f32, key_gate_weight)  # [n_kv_heads, g, q_len, num_block]\n    gate_before_masking = gate\n\n    q_block_idx = q_pos_item // moba_chunk_size  # chunk id for each query\n    block_indices = jnp.arange(num_block)  # list every key chunk index\n\n    q_block_idx_b = jnp.expand_dims(q_block_idx, axis=-1)  # [q_len, 1]\n    block_indices_b = jnp.expand_dims(block_indices, axis=0)  # [1, num_block]\n\n    # Block-causal masking: a query can't attend to future key blocks,\n    # and must attend to its own key block.\n    mask_future = q_block_idx_b > block_indices_b\n    gate = jnp.where(mask_future, gate, -float(\"inf\"))\n    mask_diag = q_block_idx_b == block_indices_b\n    gate = jnp.where(mask_diag, float(\"inf\"), gate)\n    gate_after_masking = gate\n\n    k_for_topk = min(moba_topk, num_block)\n    gate_top_k_val, gate_top_k_idx = jax.lax.top_k(gate, k=k_for_topk)  # [n_kv_heads, g, q_len, k_for_topk]\n    gate_top_k_val_min = jnp.min(gate_top_k_val, axis=-1, keepdims=True)  # [n_kv_heads, g, q_len, 1]\n    need_attend_threshold_mask = gate >= gate_top_k_val_min  # [n_kv_heads, g, q_len, num_block]\n\n    # Tie-breaking: if multiple blocks have the same gate value as the k-th\n    # block, we only select the ones that appear in the top-k indices.\n    gate_idx_mask = jnp.sum(\n        jax.nn.one_hot(gate_top_k_idx, num_block, dtype=jnp.bool_), axis=-2\n    )  # [n_kv_heads, g, q_len, num_block]\n    need_attend = jnp.logical_and(need_attend_threshold_mask, gate_idx_mask)  # [n_kv_heads, g, q_len, num_block]\n\n    return (\n        key_gate_weight,\n        gate_before_masking,\n        gate_after_masking,\n        gate_top_k_val,\n        gate_top_k_idx,\n        gate_top_k_val_min,\n        need_attend_threshold_mask,\n        gate_idx_mask,\n        need_attend,  # [n_kv_heads, g, q_len, num_block]\n    )\n\n  def generate_moba_mask_single_item(self, q_item, k_item, q_positions):\n    \"\"\"Generates the token-level MoBA additive mask for a single batch item.\"\"\"\n    q_len, _, _ = q_item.shape\n    kv_len, _, _ = k_item.shape\n    moba_chunk_size = self.config.moba_chunk_size\n\n    # Run the gating logic to find which key blocks this query cares about.\n    *_, need_attend = self.calculate_moba_gate_logic(q_item, k_item, q_positions)\n\n    # Expand the block-level `need_attend` mask to a token-level mask.\n    k_block_indices = jnp.arange(kv_len, dtype=jnp.int32) // moba_chunk_size\n    token_level_need_attend = need_attend[..., k_block_indices]\n\n    # Convert the boolean mask to float mask values.\n    gate = jnp.where(token_level_need_attend, 0.0, -float(\"inf\"))\n\n    # Apply a final per-token causal mask to ensure causality within chunks.\n    k_indices = jax.lax.broadcasted_iota(jnp.int32, (q_len, kv_len), 1)\n    q_indices = q_positions[:, None]\n    causal_mask = q_indices >= k_indices\n    gate = jnp.where(causal_mask, gate, -float(\"inf\"))\n\n    # Return the additive mask for this batch item.\n    return gate\n\n  def _generate_moba_mask(self, query: Array, key: Array, q_positions: Array) -> Array:\n    \"\"\"Builds the token-level MoBA additive mask for the whole batch.\n\n    Args:\n      query: Query tensor shaped `[batch, q_len, n_q_heads, head_dim]`.\n      key: Key tensor shaped `[batch, kv_len, n_kv_heads, head_dim]`.\n      q_positions: Absolute query positions shaped `[q_len]`, shared across the\n        batch, identifying the starting offset of each query token.\n        For example, in prefill after 128 tokens we pass\n        `jnp.arange(128, 128 + q_len)`, while in autoregressive decode with a\n        single new token the vector is `[kv_len - 1]` for each batch element.\n\n    Returns:\n      Additive attention mask with shape\n      `[batch, n_kv_heads, n_q_heads // n_kv_heads, q_len, kv_len]` containing\n      `0.` for permitted positions and `-inf` for masked ones.\n    \"\"\"\n    # vmap over the batch dimension of query and key. q_positions is constant across the batch.\n    moba_mask = jax.vmap(self.generate_moba_mask_single_item, in_axes=(0, 0, None))(query, key, q_positions)\n    return moba_mask\n\n  def apply_attention(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      lengths: Array | None,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply attention\"\"\"\n    self.check_attention_inputs(query, key, value)\n    length = query.shape[-3]\n    target_hardware = self.mesh.devices[(0,) * self.mesh.devices.ndim].platform\n\n    if use_ragged_attention and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      if lengths is None:\n        lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      if target_hardware == \"tpu\":\n        impl = self.tpu_ragged_attention\n      elif target_hardware == \"gpu\":\n        impl = self.gpu_ragged_attention\n      else:\n        raise NotImplementedError(target_hardware)\n      return impl(query, key, value, lengths, self.ragged_block_size)\n\n    elif (\n        self.attention_kernel == \"dot_product\"\n        or (self.attention_kernel == \"autoselected\" and model_mode == MODEL_MODE_AUTOREGRESSIVE)\n        or (self.attention_kernel == \"autoselected\" and length < 128)\n        or (self.attention_kernel == \"paged\")\n    ):\n      return self.apply_attention_dot(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          previous_chunk,\n          bidirectional_mask=bidirectional_mask,\n          sinks=sinks,\n          qk_product_einsum=qk_product_einsum,\n          wv_product_einsum=wv_product_einsum,\n      )\n    elif self.attention_kernel in (\"flash\", \"autoselected\"):\n      if target_hardware == \"tpu\":\n        if isinstance(key, KVTensor):\n          key = key.dequant()\n        if isinstance(value, KVTensor):\n          value = value.dequant()\n\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          raise ValueError(\n              \"\"\"Decode not supported with flash attention.\n                              Use `dot_product` instead.\"\"\"\n          )\n        return (\n            self.tpu_flash_attention(query, key, value, decoder_segment_ids, self.attn_logits_soft_cap, sinks),\n            None,\n            None,\n        )\n      else:\n        validate_flash_attention_with_sinks_on_gpu(sinks)\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          # fallback to dot_product as pallas gpu flash attention doesn't support decode stage\n          return self.apply_attention_dot(\n              query,\n              key,\n              value,\n              decoder_segment_ids,\n              model_mode,\n              bidirectional_mask=bidirectional_mask,\n              qk_product_einsum=qk_product_einsum,\n              wv_product_einsum=wv_product_einsum,\n          )\n        else:\n          head_axis = -2\n          num_query_heads = query.shape[head_axis]\n          num_kv_heads = key.shape[head_axis]\n          if num_query_heads != num_kv_heads:\n            # Handle cases where the number of query heads is different from the number of key/value heads.\n            if num_query_heads % num_kv_heads != 0:\n              raise ValueError(\n                  f\"Number of query heads ({num_query_heads}) must be divisible by number of key/value heads ({num_kv_heads}).\"\n              )\n            # TODO Investigate if the KV copy can be eliminated. It's likely redundant.\n            q_heads_per_kv_head = num_query_heads // num_kv_heads\n\n            key = jnp.repeat(\n                key, q_heads_per_kv_head, axis=head_axis\n            )  # key shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n            value = jnp.repeat(\n                value, q_heads_per_kv_head, axis=head_axis\n            )  # value shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n          out = gpu_pallas_attention.mha(query, key, value, decoder_segment_ids, sm_scale=1.0, causal=True)\n          return out, None, None\n    elif self.attention_kernel == \"cudnn_flash_te\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n        raise ValueError(\n            \"\"\"Decode not supported with flash attention.\n                           Use `dot_product` instead.\"\"\"\n        )\n      return self.cudnn_flash_attention(query, key, value, decoder_segment_ids, model_mode), None, None\n    elif self.attention_kernel == \"cudnn_flash_jax\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      return *self.cudnn_jax_flash_attention(query, key, value, decoder_segment_ids, model_mode), None\n    else:\n      raise ValueError(f\"Unexpected attention kernel {self.attention_kernel=}.\")\n\n  def gpu_ragged_attention(self, q: Array, k: Array | KVTensor, v: Array | KVTensor, lengths: Array, block_size: int):\n    \"\"\"gpu ragged attention\"\"\"\n    batch_size, q_length, q_heads, head_dim = q.shape\n\n    # Reshape q to match gqa's expected shape\n    q_for_gqa = q.squeeze(axis=1)\n\n    # Define logical axis names - clearer and avoids repeated calls.\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n    bnd = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS, CACHE_KV))\n    bn = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS))\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(bnd, bsnd, bsnd, b, None),\n        out_specs=(bnd, bn, bn),\n        check_vma=False,\n    )\n    def wrap_ragged_attention(\n        q: Array, k: Array, v: Array, lengths: Array, block_size: int\n    ) -> Tuple[Array, Array, Array]:\n      # Use the original gqa function to get the attention output\n      \"\"\"\n      Wraps the GQA function with appropriate sharding.\n\n      Args:\n          q: Query tensor.\n          k: Key tensor.\n          v: Value tensor.\n          lengths: Sequence lengths.\n          block_size: Block size for attention.\n\n      Returns:\n          A tuple containing the output, max, and sum tensors.\n      \"\"\"\n      # Use the original gqa function to get the attention output\n      local_out, (local_sum, local_max) = gpu_pallas_decode_attention.gqa(\n          q=q,\n          k=k,\n          v=v,\n          kv_seq_len=lengths,\n          block_k=block_size,\n          sm_scale=1.0,\n          return_residuals=True,\n          normalize_output=False,\n      )\n      return local_out, local_max, local_sum\n\n    local_out, local_max, local_sum = wrap_ragged_attention(q_for_gqa, k, v, lengths, block_size)\n\n    # Reshape local_out, local_max and local_sum to match Maxtext requirements\n    local_out = local_out.reshape(batch_size, q_length, q_heads, head_dim)\n    local_max = local_max.reshape(batch_size, q_length, q_heads, 1)\n    local_sum = local_sum.reshape(batch_size, q_length, q_heads, 1)\n    return local_out, local_max, local_sum\n\n  def tpu_ragged_attention(\n      self, query: Array, key: Array | KVTensor, value: Array | KVTensor, lengths: Array, block_size: int\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Ragged Attention.\"\"\"\n    if isinstance(query, KVTensor):\n      raise TypeError(\"Ragged attention does not currently support quantized tensors.\")\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            bsnd,\n            bsnd,\n            bsnd,\n            b,\n            None,\n        ),\n        out_specs=bsnd,\n        check_vma=False,\n    )\n    def wrap_ragged_attention(query, key, value, lengths, block_size):\n      if query.shape[-2] == key.shape[-2]:\n        return ragged_mha(query, key, value, lengths, block_size=block_size)\n      else:\n        return ragged_gqa(query, key, value, lengths, block_size=block_size)\n\n    return wrap_ragged_attention(query, key, value, lengths, block_size)\n\n  def tpu_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      attn_logits_soft_cap: float | None = None,\n      sinks: Array | None = None,\n  ) -> Array:\n    \"\"\"TPU Flash Attention.\"\"\"\n\n    cp_size = self.config.context_parallel_size\n    load_balanced_context_parallel = self.config.context_parallel_load_balance\n\n    # Transpose to ('batch', 'heads', 'length', 'kv')\n    query = jnp.transpose(query, axes=(0, 2, 1, 3))\n    key = jnp.transpose(key, axes=(0, 2, 1, 3))\n    value = jnp.transpose(value, axes=(0, 2, 1, 3))\n    segment_axis_names_q = None\n    segment_axis_names_kv = None\n    sink_axis_names = nn.logical_to_mesh_axes((HEAD,))\n    if decoder_segment_ids is not None:\n      if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH_NO_EXP, Q_LENGTH))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH_NO_EXP, KV_LENGTH))\n      else:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH, Q_LENGTH_NO_EXP))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH, KV_LENGTH))\n\n    if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel_ep)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q_ep)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv_ep)\n    else:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv)\n\n    global global_block_q, global_block_kv, global_block_kv_compute, global_block_q_dkv, global_block_kv_dkv\n    global global_block_kv_dkv_compute, global_block_q_dq, global_block_kv_dq, global_use_fused_bwd_kernel\n    global global_q_layout, global_k_layout, global_v_layout\n    global_block_q = self.config.sa_block_q\n    global_block_kv = self.config.sa_block_kv\n    global_block_kv_compute = self.config.sa_block_kv_compute\n    global_block_q_dkv = self.config.sa_block_q_dkv\n    global_block_kv_dkv = self.config.sa_block_kv_dkv\n    global_block_kv_dkv_compute = self.config.sa_block_kv_dkv_compute\n    global_block_q_dq = self.config.sa_block_q_dq\n    global_block_kv_dq = self.config.sa_block_kv_dq\n    global_use_fused_bwd_kernel = self.config.sa_use_fused_bwd_kernel\n    global_q_layout = self.config.sa_q_layout\n    global_k_layout = self.config.sa_k_layout\n    global_v_layout = self.config.sa_v_layout\n\n    devices_in_data_fsdp = self.mesh.shape[\"data\"] * self.mesh.shape[\"fsdp\"]\n    assert (query.shape[0] / devices_in_data_fsdp).is_integer(), (\n        \"Batch dimension should be shardable among the devices in data and fsdp\"\n        \" axis\"\n        f\" got {query.shape[0]=}/{devices_in_data_fsdp=}\"\n    )\n\n    # create_splash_attention kernel\n    block_sizes = splash_attention_kernel.BlockSizes(\n        block_q=min(global_block_q, query.shape[2]),\n        block_kv=min(global_block_kv, key.shape[2]),\n        block_kv_compute=min(global_block_kv_compute, key.shape[2]),\n        block_q_dkv=min(global_block_q_dkv, query.shape[2]),\n        block_kv_dkv=min(global_block_kv_dkv, key.shape[2]),\n        block_kv_dkv_compute=min(global_block_kv_dkv_compute, query.shape[2]),\n        block_q_dq=None if global_use_fused_bwd_kernel else min(global_block_q_dq, query.shape[2]),\n        block_kv_dq=None if global_use_fused_bwd_kernel else min(global_block_kv_dq, query.shape[2]),\n        use_fused_bwd_kernel=global_use_fused_bwd_kernel,\n        q_layout=splash_attention_kernel.QKVLayout[global_q_layout],\n        k_layout=splash_attention_kernel.QKVLayout[global_k_layout],\n        v_layout=splash_attention_kernel.QKVLayout[global_v_layout],\n    )\n\n    mask_shape = (query.shape[2], key.shape[2])  # (q_seq_len, kv_seq_len)\n    if self.attention_type == AttentionType.FULL:\n      mask = splash_attention_mask.FullMask(mask_shape)\n    else:\n      mask = splash_attention_mask.CausalMask(shape=mask_shape)\n\n    # Create LoadBalancedCausalMask if cp and load_balancing\n    if cp_size > 1 and load_balanced_context_parallel:\n      mask = LoadBalancedCausalMask(shape=mask_shape, cp_size=cp_size)\n\n    # TODO: figure out local_sliding attention + load_balancing, default is global\n    # Apply local masking if local sliding attention is enabled.\n    if self.attention_type == AttentionType.LOCAL_SLIDING:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n      mask &= splash_attention_mask.LocalMask(\n          shape=(query.shape[2], key.shape[2]),\n          window_size=(self.sliding_window_size, self.sliding_window_size),\n          offset=0,\n      )\n    elif self.attention_type == AttentionType.CHUNK:\n      if self.chunk_attn_window_size is None:\n        raise ValueError(\"chunk_attn_window_size must be set for chunk attention type\")\n\n      mask &= ChunkedCausalMask(shape=(query.shape[2], key.shape[2]), chunk_size=self.chunk_attn_window_size)\n\n    # Create multi-head mask\n    multi_head_mask = splash_attention_mask.MultiHeadMask(masks=(mask,) * query.shape[1])\n\n    # Create the splash attention kernel object separately, jit it for performance\n    @partial(\n        jax.jit,\n        static_argnames=[\n            \"multi_head_mask\",\n            \"shard_head_size\",\n        ],\n    )\n    def wrap_splash_kernel(multi_head_mask, shard_head_size=1):\n      splash_kernel = splash_attention_kernel.make_splash_mha(\n          mask=multi_head_mask,\n          head_shards=shard_head_size,  # the size of the axis if sharding over heads\n          q_seq_shards=cp_size,  # axis for sequence sharding\n          block_sizes=block_sizes,\n          attn_logits_soft_cap=attn_logits_soft_cap,\n          residual_checkpoint_name=\"context\",\n      )\n      return splash_kernel\n\n    logical_axis_rules_head = np.array(\n        [self.mesh.shape[physical_axes] for physical_axes in dict(self.config.logical_axis_rules)[HEAD]]\n    )\n    shard_head_size = np.prod(logical_axis_rules_head)\n    splash_kernel = wrap_splash_kernel(multi_head_mask, int(shard_head_size))\n    named_sharding = jax.sharding.NamedSharding(self.mesh, axis_names_splash_kernel)\n    segment_axis_names_splash_kernel = splash_kernel.manual_sharding_spec(named_sharding)\n\n    # Now call the function wrap_flash_attention which does the actual computation.\n    # The splash kernel is passed as a parameter to the function. Since we have the shard map\n    # decorating the wrap_flash_attention function, the data will be correctly sharded\n    # meaning q will be sharded over sequence aka context length but K and V will be duplicated\n    # The shardings are specified in the in_specs and out_specs of the shard_map decorator:\n    # 'segment_axis_names_q' maps to ['activation_q_length', ['context']] meaning that q is sharded over the context axis\n    #  'segment_axis_names_kv' maps to ['activation_kv_length', []] meaning that K and V are not sharded\n    # splash_kernel is sharded over (HEAD, LENGTH)\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            axis_names_q,\n            axis_names_kv,\n            axis_names_kv,\n            segment_axis_names_q,\n            segment_axis_names_kv,\n            segment_axis_names_splash_kernel,\n            None,  # no sharding for cp_size\n            None,  # no sharding for load_balanced_context_parallel\n            sink_axis_names,  # sharding align with query heads\n        ),\n        out_specs=axis_names_q,\n        check_vma=False,\n    )\n    def wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids_q,\n        decoder_segment_ids_kv,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    ):\n      # If load_balanced_context_parallel is enabled, reorder the key and value tensors\n      # to ensure that they are contiguous in memory.\n      # This is necessary for the splash attention kernel to work correctly because it expects\n      # the K and V to be contiguous. Note that K and V are not sharded over the sequence aka context axis\n      # This was we get the unsharded unpermuted key and value tensors\n      if cp_size > 1 and load_balanced_context_parallel:\n        key = max_utils.reorder_sequence(tensor=key, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        value = max_utils.reorder_sequence(tensor=value, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        decoder_segment_ids_unpermuted = max_utils.reorder_sequence(\n            tensor=decoder_segment_ids_kv, cp_size=cp_size, seq_dim=1, to_contiguous=True\n        )\n\n      if decoder_segment_ids_q is not None:\n        if cp_size > 1 and load_balanced_context_parallel:\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(\n              decoder_segment_ids_q, decoder_segment_ids_unpermuted\n          )\n        else:\n          # if cp=1, decoder_segment_ids_q is the same as decoder_segment_ids_kv\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(decoder_segment_ids_q, decoder_segment_ids_kv)\n      else:\n        decoder_segment_ids_tuple = None\n      # TODO(ranran): remove if/else branch once b/441336842 is fixed\n      if version.parse(jax.__version__) < version.parse(\"0.7.2.dev20250824\"):\n        attention_output = jax.vmap(splash_kernel)(query, key, value, decoder_segment_ids_tuple)\n      else:\n        attention_output = jax.vmap(splash_kernel, in_axes=(0, 0, 0, 0, None))(\n            query, key, value, decoder_segment_ids_tuple, sinks\n        )\n      return attention_output\n\n    x = wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids,\n        decoder_segment_ids,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    )\n\n    x = jnp.transpose(x, axes=(0, 2, 1, 3))\n\n    return x\n\n  def cudnn_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> Array:\n    \"\"\"CUDNN Flash Attention with Transformer Engine.\n    1. Stable API, supports GQA, SWA (only with causal masking)\n    2. Head_dim = 256 is also supported from TE-1.12 stable release with CUDNN 12.6\n    \"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from transformer_engine.jax.flax.transformer import DotProductAttention  # pytype: disable=import-error\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    using_context_parallelism = self.mesh.shape[\"context\"] > 1\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and using_context_parallelism:\n      raise AssertionError(\"Sliding window attention is not supported when context parallelism is enabled\")\n\n    sliding_window_size = None\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or not self.config.enable_padding_causal_mask:\n      sliding_window_size = [self.sliding_window_size, 0]\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or using_context_parallelism:\n      mask_type = \"causal\"  # SWA and Context Parallelism only work with causal masking\n      attn_mask = None\n    else:\n      # generate attn_mask\n      mask_type = \"padding_causal\"  # only padding_causal mask type can take a created mask\n      attn_mask = self.generate_attention_mask(query, key, decoder_segment_ids, model_mode)\n\n    dpa_layer = DotProductAttention(\n        head_dim=head_dim,\n        num_attention_heads=self.num_query_heads,\n        num_gqa_groups=self.num_kv_heads,\n        attn_mask_type=mask_type,  # 'no_mask', 'padding', 'causal', or 'padding_causal'\n        attn_bias_type=\"no_bias\",  # 'no_bias', 'pre_scale_bias' or 'post_scale_bias'\n        attention_dropout=self.dropout_rate,\n        dropout_rng_name=\"aqt\",\n        dtype=self.dtype,\n        float32_logits=self.float32_logits,\n        qkv_layout=\"BSHD_BSHD_BSHD\",  # 'BS3HD', 'BSHD_BS2HD' or 'BSHD_BSHD_BSHD'\n        scale_factor=1.0,\n        transpose_batch_sequence=False,\n        window_size=sliding_window_size,\n        context_parallel_causal_load_balanced=self.config.context_parallel_load_balance,\n        context_parallel_axis=\"context\",\n    )\n    return dpa_layer(query, key, value, mask=attn_mask)\n\n  def cudnn_jax_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> tuple[Array, Array]:\n    \"\"\"CUDNN Flash Attention with JAX SDPA API.\"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from jax._src.cudnn.fused_attention_stablehlo import (\n        dot_product_attention,\n        MaskType,\n    )\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          q_seqlen=lengths,\n          kv_seqlen=lengths,\n          mask_type=MaskType.PADDING,\n          scale=1.0,\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    else:\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          mask_type=MaskType.CAUSAL,\n          scale=1.0 / math.sqrt(head_dim),\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    output = checkpoint_name(output, \"context\")\n    lse = checkpoint_name(lse, \"context\")\n    return output, lse\n\n  def compute_local_attention(\n      self,\n      attn_weights: Array,\n      value: Array | KVTensor,\n      q_seq_len: int,\n      model_mode: str,\n      wv_product_einsum: Callable[..., Array],\n      sinks: Array | None = None,\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Computes the attention of a local subset of the kv cache.\n    Local attention results will need to be combined with any other local attentions and normalized\n    Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n\n    Args:\n        attn_weights (Array): Product of query and key\n        value (Array): Current value\n        aqt_rng (PRNGKey | None): Optional rng\n\n    Returns:\n        (local_out, local_max,): where\n          local_out is local unnormalized output\n          local_max is the local max of exponentials\n          local_sum is the sum of exponentials for this chunk, divided by exp(local_max).\n    \"\"\"\n    b, n_kv, g, t, s = attn_weights.shape\n    n_q = n_kv * g\n    logits = jnp.reshape(attn_weights, (b, n_q, t, s))\n    if sinks is not None:\n      # broadcast sinks to match the attn weights dimension and combine\n      sinks_param = sinks.astype(attn_weights.dtype)  # (n_q,)\n      sinks_logits = sinks_param[jnp.newaxis, :, jnp.newaxis, jnp.newaxis]  # (1, n_q, 1, 1)\n      sinks_logits = jnp.broadcast_to(sinks_logits, (b, n_q, t, 1))\n      logits = jnp.concatenate([logits, sinks_logits], axis=-1)\n\n    # softmax\n    local_max = jnp.max(logits, axis=-1, keepdims=True)\n    local_exps_combined = jnp.exp(logits - local_max)\n    local_sum = jnp.sum(local_exps_combined, axis=-1, keepdims=True)\n\n    # reshape and transpose\n    local_exps = local_exps_combined[..., :s]\n    local_exps = jnp.reshape(local_exps, (b, n_kv, g, t, s))\n    local_max = jnp.transpose(local_max, (0, 2, 1, 3))  # (b, t, n_q, 1)\n    local_sum = jnp.transpose(local_sum, (0, 2, 1, 3))  # (b, t, n_q, 1)\n\n    local_out = self.wv_product(local_exps, value, model_mode, wv_product_einsum)\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n    elif model_mode == MODEL_MODE_PREFILL:\n      local_out = partitioning.with_sharding_constraint(local_out, (BATCH, KV_LENGTH, HEAD, D_KV))\n\n    if self.reshape_q and q_seq_len == 1:\n      local_max = local_max[:, 0:1, :, :]\n      local_sum = local_sum[:, 0:1, :, :]\n      local_out = local_out[:, 0:1, :, :]\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_max = partitioning.with_sharding_constraint(local_max, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_sum = partitioning.with_sharding_constraint(local_sum, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n\n    return local_out, local_max, local_sum\n\n  def is_partition_in_decode(self, seq_len):\n    return self.config.ici_context_autoregressive_parallelism > 0 and seq_len == 1\n\n  def apply_attention_dot(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply Attention.\"\"\"\n    validate_compute_axis_order(self.compute_axis_order)\n    # Casting qk_product and softmaxt computation for float32 for model stability.\n    if self.float32_qk_product:\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      query = query.astype(jnp.float32)\n      key = key.astype(jnp.float32)\n\n    # special sharding for decode\n    q_seq_len = query.shape[1]\n    prefill_qkv_sharding = (BATCH, PREFILL_LENGTH, HEAD, D_KV)\n    decode_qkv_sharding = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV)\n    if self.is_partition_in_decode(q_seq_len):\n      query = partitioning.with_sharding_constraint(query, decode_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, decode_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, decode_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, decode_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, decode_qkv_sharding)\n    elif model_mode == MODEL_MODE_PREFILL:\n      query = partitioning.with_sharding_constraint(query, prefill_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, prefill_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, prefill_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, prefill_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, prefill_qkv_sharding)\n\n    attn_weights = self.qk_product(query, key, q_seq_len, model_mode, qk_product_einsum)\n    if self.is_partition_in_decode(q_seq_len):\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n\n    if self.attn_logits_soft_cap:\n      attn_weights = jnp.tanh(attn_weights / self.attn_logits_soft_cap)\n      attn_weights = attn_weights * self.attn_logits_soft_cap\n\n    # Casting softmaxt computation for float32 for model stability.\n    if self.float32_logits:\n      attn_weights = attn_weights.astype(jnp.float32)\n\n    attn_mask = self.generate_attention_mask(\n        query, key, decoder_segment_ids, model_mode, previous_chunk, bidirectional_mask\n    )\n    if self.config.moba:\n      kv_seq_len = key.shape[1]\n      # This logic for `next_pos` is duplicated from `generate_attention_mask`.\n      # It determines the starting position of the query sequence.\n      next_pos = 0\n      if previous_chunk is not None:\n        next_pos = previous_chunk.shape[1]\n      elif model_mode == MODEL_MODE_AUTOREGRESSIVE and q_seq_len == 1:\n        next_pos = kv_seq_len - 1\n      q_positions = jnp.arange(next_pos, next_pos + q_seq_len)\n\n      # The gate calculation in MoBA uses the unscaled query.\n      # With scaled query, the gate values are scaled, but since the top-k selection\n      # is scale-invariant, we can use the scaled query directly.\n      moba_mask = self._generate_moba_mask(query, key, q_positions)\n      attn_weights += moba_mask\n\n    if self.is_partition_in_decode(q_seq_len):\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n    if attn_mask is not None:\n      attn_weights = apply_mask_to_logits(attn_weights, attn_mask)\n    return self.compute_local_attention(attn_weights, value, q_seq_len, model_mode, wv_product_einsum, sinks)\n\n  def qk_product(\n      self, query: Array, key: Array | KVTensor, q_seq_len: int, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"Query-Key product.\n\n    Args:\n      query: Query projection, in shape of [b, t, n, d]\n      key: Key projection in shape of [b, s, n_kv, d]\n\n    Returns:\n      results in shape [b, n_kv, n // n_kv, t, s].\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n    b, t, n, d = query.shape\n    n_kv = key.shape[-2]\n    assert n_kv == self.num_kv_heads\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, 2, n_kv, n // n_kv, d))\n      result = einsum(\"btkgd,bskd->bkgts\", query, key, **precision_kwargs)\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      query = jnp.transpose(query, axes=self.compute_axis_order)\n      key = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), key)\n      query = jnp.reshape(query, (b, n_kv, n // n_kv, t, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, n_kv, n // n_kv, 2, d))\n      result = einsum(\"bkgtd,bksd->bkgts\", query, key, **precision_kwargs)\n    else:\n      raise NotImplementedError(self.compute_axis_order)\n    return result\n\n  def wv_product(\n      self, attn_weights: Array, value: Array | KVTensor, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"weighted value product.\n\n    Args:\n      attn_weights: Computed results of qk_einsum, in shape [b, n_kv, n // n_kv, t, s]\n      value: Value projection, in shape of [b, s, n_kv, d]\n\n    Returns:\n      result in shape [b, t, n, d]\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if self.kv_quant:\n      # manually cast to bf16 to avoid the fp32 XLA ops for speedup\n      if isinstance(value, KVTensor) and self.kv_quant.dtype == jnp.float8_e4m3fn:\n        value.qvalue = value.qvalue.astype(jnp.bfloat16)\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      out = einsum(\"bkgts,bskd->btkgd\", attn_weights, value, **precision_kwargs)\n      b, t, n_kv, g, d = out.shape\n      result = jnp.reshape(out, (b, t, n_kv * g, d))\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      value = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), value)\n      out = einsum(\"bkgts,bksd->bkgtd\", attn_weights, value, **precision_kwargs)\n      b, n_kv, g, t, d = out.shape\n      result = jnp.reshape(out, (b, n_kv * g, t, d))\n      result = self.reverse_transepose(result, self.compute_axis_order)\n    return result\n\n  def reverse_transepose(self, transposed_array, transpose_axis_order):\n    return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)\n\n  def normalize_cudnn_attention(self, local_outs, local_stats):\n    \"\"\"Normalize across two cuDNN attentions\n\n    Args:\n        local_outs (list): List of outputs entries for each cudnn attention\n          in shape [b, t, n, d].\n        local_stats (list): List of logsumexp entries for each cudnn attention\n          in shape [b, n, t].\n\n    Returns:\n        Array: Combined attention that has been normalized in shape [b, t, n, d].\n    \"\"\"\n    # reshape stat to have shape [b, n, t, 1]\n    stat0 = local_stats[0].reshape((*local_stats[0].shape, 1))\n    stat1 = local_stats[1].reshape((*local_stats[1].shape, 1))\n    global_stat = jnp.log(jnp.exp(stat0) + jnp.exp(stat1))\n    # # transpose stat to have shape [b, t, n, 1] for elemenwise multiplication\n    attn_out = local_outs[0].astype(jnp.float32) * jnp.exp(stat0 - global_stat).transpose((0, 2, 1, 3)) + local_outs[\n        1\n    ].astype(jnp.float32) * jnp.exp(stat1 - global_stat).transpose((0, 2, 1, 3))\n    return attn_out.astype(local_stats[0].dtype)\n\n  def normalize_attention(self, local_outs, local_maxes, local_sums):\n    \"\"\"Normalize across multiple localized attentions\n\n    Args:\n        local_outs (list): List of unnormalized outputs entries for each local attention\n        local_maxes (list): List of max exponentials entries for each local attention\n        local_sums (list): List of exponential sum entries for each local attention\n\n    Returns:\n        Array: Combined attention that has been normalized\n    \"\"\"\n    # Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n    global_max = functools.reduce(jnp.maximum, local_maxes)\n    global_sum = sum(\n        (jnp.exp(local_max - global_max) * local_sum for (local_sum, local_max) in zip(local_sums, local_maxes))\n    )\n\n    attn_out = 0\n    for local_max, local_out in zip(local_maxes, local_outs):\n      local_normalizer = jnp.exp(local_max - global_max) / global_sum\n      attn_out += local_normalizer * local_out\n    return attn_out\n\n  def __call__(\n      self,\n      query,\n      key,\n      value,\n      decoder_segment_ids,\n      model_mode,\n      cached_values=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n      sinks=None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n  ):\n    if cached_values is None:\n      prefill_kv_cache, ar_kv_cache = None, None\n    else:\n      prefill_kv_cache, ar_kv_cache = cached_values[0], cached_values[1]\n    if model_mode != MODEL_MODE_TRAIN:\n      assert prefill_kv_cache\n      key, value, decoder_segment_ids = prefill_kv_cache\n\n    prefill_unnormalized_output, prefill_exponentials_max, prefill_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=None,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n        bidirectional_mask=bidirectional_mask,\n        sinks=sinks,\n        qk_product_einsum=self.AqtEinsum_0,\n        wv_product_einsum=self.AqtEinsum_1,\n    )\n\n    # Return the \"prefill\" cache if it actually the combined prefill+ar kv cache\n    if ar_kv_cache is None:\n      if prefill_exponentials_sum is not None:\n        return prefill_unnormalized_output / prefill_exponentials_sum\n      return prefill_unnormalized_output\n\n    key, value, decoder_segment_ids, lengths = ar_kv_cache\n    ar_unnormalized_output, ar_exponentials_max, ar_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=lengths,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        bidirectional_mask=bidirectional_mask,\n        qk_product_einsum=self.AqtEinsum_2,\n        wv_product_einsum=self.AqtEinsum_3,\n    )\n\n    if ar_unnormalized_output is not None:\n      unnormalized_outputs = [prefill_unnormalized_output, ar_unnormalized_output]\n      exponentials_maxes = [prefill_exponentials_max, ar_exponentials_max]\n      exponentials_sums = [prefill_exponentials_sum, ar_exponentials_sum]\n      if prefill_exponentials_max is not None and prefill_exponentials_sum is None:\n        prefill_stat = prefill_exponentials_max\n        ar_stat = ar_exponentials_max\n        stats = [prefill_stat, ar_stat]\n        return self.normalize_cudnn_attention(unnormalized_outputs, stats)\n      else:\n        return self.normalize_attention(unnormalized_outputs, exponentials_maxes, exponentials_sums)\n    else:\n      return prefill_unnormalized_output / prefill_exponentials_sum",
        "analysis": {
            "module_type": "attention_operation",
            "purpose": "A highly configurable module that encapsulates various attention mechanisms (dot-product, flash, ragged) for different hardware backends (TPU, GPU), handling masking, quantization, and KV caching for both training and inference.",
            "input": {
                "shape": "The primary input is through the `__call__` method, which takes `query` ([batch, q_len, num_query_heads, head_dim]), `key` ([batch, kv_len, num_kv_heads, head_dim]), `value` ([batch, kv_len, num_kv_heads, head_dim]), and other arguments.",
                "dtype": "Configurable, typically jnp.float32 or jnp.bfloat16."
            },
            "processing_steps": [
                "The `__call__` method orchestrates the attention computation.",
                "It unpacks KV cache values if provided for inference.",
                "It calls `apply_attention` to select and execute the appropriate attention kernel based on configuration (e.g., 'dot_product', 'flash', 'ragged') and hardware.",
                "If attention is computed over multiple cache chunks (e.g., prefill and autoregressive), it normalizes and combines the results using `normalize_attention`."
            ],
            "output": {
                "shape": "The output shape is the same as the query tensor: [batch, q_len, num_query_heads, head_dim]."
            },
            "dependencies": [
                "nnx.Module",
                "jax.sharding.Mesh",
                "max_utils",
                "splash_attention_kernel",
                "gpu_pallas_attention",
                "ragged_mha",
                "ragged_gqa",
                "KVQuant",
                "KVTensor",
                "Quant"
            ],
            "parameters": {
                "attention_kernel": "Specifies the attention implementation to use, e.g., 'dot_product', 'flash', 'autoselected'.",
                "num_query_heads": "The number of attention heads for the query.",
                "num_kv_heads": "The number of attention heads for key and value, enabling Grouped-Query Attention (GQA).",
                "attention_type": "The type of attention pattern, e.g., 'GLOBAL', 'LOCAL_SLIDING', 'CHUNK'.",
                "use_ragged_attention": "A boolean flag to enable ragged attention for efficient processing of sequences with varying lengths.",
                "kv_quant": "An optional configuration for quantizing the key-value cache.",
                "config": "A global configuration object containing various model parameters like head dimension, precision, etc."
            },
            "notes": [
                "This is a polymorphic module that dispatches to different, highly optimized attention implementations based on configuration, hardware, and operational mode (prefill vs. autoregressive).",
                "It integrates complex logic for various masking strategies, including causal, sliding window, chunked, and bidirectional masking.",
                "The module is designed to work within a sharded data-parallel environment, using `jax.shard_map` for distributed computation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the AttentionOp module with extensive configuration options for attention type, hardware, quantization, and sharding.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters such as `attention_kernel`, `num_query_heads`, `dtype`, etc.",
                        "Conditionally initialize `AqtEinsum` instances for quantized matrix multiplication if `kv_quant` is enabled, using dummy tensors for lazy initialization.",
                        "If quantization is not used, set einsum functions to `jnp.einsum`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Rngs",
                        "KVQuant",
                        "nnx_wrappers.ToNNX",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "This method sets up the module based on a large number of configuration flags, determining which attention logic will be used at runtime."
                    ]
                },
                "check_attention_inputs": {
                    "purpose": "Asserts that the shapes of query, key, and value tensors are compatible for attention computation.",
                    "input": {
                        "shape": "query, key, and value tensors.",
                        "dtype": "Array or KVTensor"
                    },
                    "processing_steps": [
                        "Assert that key and value have the same number of dimensions.",
                        "Assert that batch dimensions match across query, key, and value.",
                        "Assert that key and value have the same number of KV heads and sequence lengths.",
                        "Assert that query and key have the same head dimension."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This is a validation utility to catch shape mismatches early."
                    ]
                },
                "generate_attention_mask": {
                    "purpose": "Generates a combined attention mask based on sequence separation, causality, and specialized patterns like sliding window or chunked attention.",
                    "input": {
                        "shape": "query, key, decoder_segment_ids, and other optional masks.",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Create a base mask from `decoder_segment_ids` for sequence separation.",
                        "Create a causal mask unless in autoregressive mode or using full attention.",
                        "Combine the base and causal masks.",
                        "If `attention_type` is 'LOCAL_SLIDING' or 'CHUNK', apply the corresponding specialized mask.",
                        "If `bidirectional_mask` is provided, combine it with the existing mask.",
                        "Convert the final boolean mask to float values (0.0 for allowed, large negative for masked)."
                    ],
                    "output": {
                        "shape": "An attention mask broadcastable to [batch, num_heads, q_len, kv_len], or None if no masking is needed."
                    },
                    "dependencies": [
                        "_generate_chunk_attention_mask",
                        "_make_bidirectional_block_mask"
                    ],
                    "notes": [
                        "The logic handles complex scenarios for packed inputs and different attention patterns."
                    ]
                },
                "apply_attention": {
                    "purpose": "Dispatches the attention computation to the appropriate backend implementation based on configuration, hardware, and model mode.",
                    "input": {
                        "shape": "query, key, value, decoder_segment_ids, lengths, model_mode, etc.",
                        "dtype": "Array or KVTensor"
                    },
                    "processing_steps": [
                        "Check input tensor validity.",
                        "If `use_ragged_attention` is enabled in autoregressive mode, select TPU or GPU ragged attention implementation.",
                        "If `attention_kernel` is 'dot_product' or 'autoselected' (for decode or short sequences), call `apply_attention_dot`.",
                        "If `attention_kernel` is 'flash' or 'autoselected', select TPU or GPU flash attention implementation.",
                        "If `attention_kernel` is 'cudnn_flash_te' or 'cudnn_flash_jax', call the corresponding cuDNN implementation.",
                        "Raise an error for an unsupported kernel."
                    ],
                    "output": {
                        "shape": "A tuple containing (attention_output, local_max, local_sum). The last two elements may be None for flash implementations."
                    },
                    "dependencies": [
                        "self.apply_attention_dot",
                        "self.tpu_flash_attention",
                        "self.gpu_ragged_attention",
                        "self.tpu_ragged_attention",
                        "self.cudnn_flash_attention",
                        "self.cudnn_jax_flash_attention",
                        "gpu_pallas_attention.mha"
                    ],
                    "notes": [
                        "This method is the central router for selecting the most efficient attention backend."
                    ]
                },
                "apply_attention_dot": {
                    "purpose": "Implements the standard dot-product attention mechanism, including masking and optional MoBA logic.",
                    "input": {
                        "shape": "query, key, value, decoder_segment_ids, model_mode, etc.",
                        "dtype": "Array or KVTensor"
                    },
                    "processing_steps": [
                        "Optionally cast inputs to float32 for stability.",
                        "Apply sharding constraints to tensors.",
                        "Compute query-key dot product using `self.qk_product`.",
                        "Apply `attn_logits_soft_cap` if configured.",
                        "Generate and apply the appropriate attention mask.",
                        "If MoBA is enabled, generate and apply the MoBA mask.",
                        "Compute the final weighted value product and normalization statistics using `self.compute_local_attention`."
                    ],
                    "output": {
                        "shape": "A tuple (local_out, local_max, local_sum) representing the unnormalized output and softmax statistics."
                    },
                    "dependencies": [
                        "self.qk_product",
                        "self.generate_attention_mask",
                        "self._generate_moba_mask",
                        "self.compute_local_attention",
                        "apply_mask_to_logits"
                    ],
                    "notes": [
                        "This serves as the fundamental, non-fused attention implementation."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the complete attention operation, handling KV caching and normalization of outputs from different cache sources.",
                    "input": {
                        "shape": "query, key, value, decoder_segment_ids, model_mode, cached_values, etc.",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Unpack `cached_values` into prefill and autoregressive KV caches.",
                        "Call `self.apply_attention` for the prefill cache.",
                        "If an autoregressive cache is present, call `self.apply_attention` for it as well.",
                        "If results from both caches are available, combine and normalize them using `self.normalize_attention` or `self.normalize_cudnn_attention`.",
                        "If only one result is available, normalize and return it."
                    ],
                    "output": {
                        "shape": "[batch, q_len, num_query_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.apply_attention",
                        "self.normalize_attention",
                        "self.normalize_cudnn_attention"
                    ],
                    "notes": [
                        "This is the main public method of the class. It orchestrates computations over multiple KV cache sources, which is common in scenarios like chunked prefill."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#LoadBalancedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class LoadBalancedCausalMask(splash_attention_mask._ComputableMask):\n  \"\"\"Lazy causal mask, prevents the model from attending to future tokens.\n  Attributes:\n    offset: Offset of q start wrt kv. A positive offset shifts the bottom\n      triangle upward, a negative one shifts it downward. A negative offset\n      makes the first 'offset' rows of the attention matrix all 0s which leads\n      to undefined softmax.\n  \"\"\"\n\n  offset: int\n  shape: tuple[int, int]\n  cp_size: int\n\n  def __init__(self, shape: tuple[int, int], offset: int = 0, shard_count: int = 1, cp_size: int = 4):\n    self.offset = offset\n\n    def causal_mask_function(q_ids, kv_ids):\n      if self.offset == 0:\n        return q_ids >= kv_ids\n      else:\n        return q_ids + self.offset >= kv_ids\n\n    arr = np.arange(shape[0])\n    # we reorder the mask to be load balanced following the same approach as\n    # used to reorder the input tokens\n    out = max_utils.reorder_mask_load_balancing(arr[None, :, None, None], cp_size, 1)\n    q_sequence = out[0, :, 0, 0]\n\n    mask_function = causal_mask_function\n\n    super().__init__(\n        shape=shape,\n        mask_function=mask_function,\n        shard_count=shard_count,\n    )\n    self.q_sequence = q_sequence\n\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n\n    return self.shape == other.shape and self.offset == other.offset and np.array_equal(self.q_sequence, other.q_sequence)\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.offset,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "load_balanced_causal_mask",
            "purpose": "Creates a lazy, load-balanced causal attention mask for Splash Attention, preventing attention to future tokens while distributing computation evenly in a context-parallel setting.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The class is initialized with shape, offset, and context parallelism size.",
                "It generates a reordered query sequence for load balancing.",
                "The instance is then used by an attention kernel which calls its internal mask function with query and key indices to compute the mask on-the-fly."
            ],
            "output": {
                "shape": "An object representing a boolean mask of shape specified during initialization (e.g., [q_length, kv_length]). The full mask is not materialized."
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "max_utils.reorder_mask_load_balancing",
                "numpy"
            ],
            "parameters": {
                "shape": "A tuple (q_length, kv_length) defining the dimensions of the conceptual attention mask.",
                "offset": "An integer offset for the causal relationship, shifting the causal boundary.",
                "cp_size": "The context parallelism size, used to determine the reordering for load balancing.",
                "shard_count": "The number of shards, passed to the superclass constructor."
            },
            "notes": [
                "This is a 'lazy' mask, meaning the full boolean matrix is not stored in memory. Instead, a function is provided to the attention kernel to compute mask values as needed.",
                "It inherits from a protected class `_ComputableMask` from the Splash Attention library.",
                "The load balancing is achieved by reordering the query sequence indices based on the `cp_size`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the lazy, load-balanced causal mask object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define a nested function `causal_mask_function` that implements the causal logic `q_ids + offset >= kv_ids`.",
                        "Create an array of query indices using `np.arange`.",
                        "Call `max_utils.reorder_mask_load_balancing` on the query indices to get a load-balanced sequence.",
                        "Store the reordered sequence as `self.q_sequence`.",
                        "Call the superclass constructor with the shape, the causal function, and shard count."
                    ],
                    "output": {
                        "shape": "An initialized instance of LoadBalancedCausalMask."
                    },
                    "dependencies": [
                        "numpy.arange",
                        "max_utils.reorder_mask_load_balancing",
                        "splash_attention_mask._ComputableMask.__init__"
                    ],
                    "notes": [
                        "The constructor sets up the object but does not materialize the full mask."
                    ]
                },
                "__eq__": {
                    "purpose": "Checks if two LoadBalancedCausalMask objects are equal.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the other object is of the same type.",
                        "Compare the `shape`, `offset`, and `q_sequence` attributes of both objects."
                    ],
                    "output": {
                        "shape": "A single boolean value."
                    },
                    "dependencies": [
                        "numpy.array_equal"
                    ],
                    "notes": [
                        "Equality is determined by the mask's configuration (shape, offset) and the specific load-balanced query sequence."
                    ]
                },
                "__hash__": {
                    "purpose": "Computes a hash for the mask object, allowing it to be used in hash-based collections.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a tuple containing the object's type, shape, offset, and the byte representation of the `q_sequence` array.",
                        "Call the built-in `hash()` function on this tuple."
                    ],
                    "output": {
                        "shape": "A single integer hash value."
                    },
                    "dependencies": [],
                    "notes": [
                        "The hash is based on the same attributes that define equality, ensuring consistency."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#L2Norm",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class L2Norm(nnx.Module):\n  \"\"\"\n  Implementation of L2Norm in JAX.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n\n  eps: float = 1e-6\n  rngs: nnx.Rngs = None  # Not used in L2Norm but passed in by nnx.bridge.to_linen\n\n  def __call__(self, x):\n    return x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)",
        "analysis": {
            "module_type": "l2_normalization",
            "purpose": "A Flax NNX module that performs L2 normalization on an input tensor along its last dimension.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the module with an epsilon value for numerical stability."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "eps": "A small float value added to the denominator for numerical stability."
            },
            "notes": [
                "The class is defined as a dataclass.",
                "It includes an `rngs` attribute to be compatible with `nnx.bridge.to_linen`, but it is not used in the normalization logic."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the L2 normalization to the input tensor.",
                    "input": {
                        "shape": "[..., feature_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Square the input tensor `x` element-wise.",
                        "Compute the mean of the squared tensor along the last axis, keeping the dimension.",
                        "Add the epsilon value `self.eps` to the mean for numerical stability.",
                        "Calculate the reciprocal square root of the result using `jax.lax.rsqrt`.",
                        "Multiply the original input tensor `x` by the reciprocal square root to normalize it."
                    ],
                    "output": {
                        "shape": "[..., feature_dim]"
                    },
                    "dependencies": [
                        "jax.lax.rsqrt",
                        "jnp.mean"
                    ],
                    "notes": [
                        "The normalization is performed along the last axis (`axis=-1`)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#l2_norm_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def l2_norm_as_linen(self, eps: float = 1e-6):\n  \"\"\"\n  Initializes the L2Norm module and returns it as a Linen module.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n  return nnx_wrappers.to_linen(L2Norm, eps=eps, metadata_fn=variable_to_logically_partitioned)",
        "analysis": {
            "module_type": "l2_norm_factory",
            "purpose": "Initializes the L2Norm NNX module and wraps it to be used as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "float"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `L2Norm` NNX module into a Flax Linen module.",
                "Passes the `eps` parameter to the `L2Norm` constructor during wrapping.",
                "Provides `variable_to_logically_partitioned` as the `metadata_fn` for sharding."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "L2Norm",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "eps": "A small float value (epsilon) used for numerical stability in the L2 normalization calculation."
            },
            "notes": [
                "This function serves as a factory to make the `L2Norm` NNX module compatible with the Flax Linen API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#attention_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def attention_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an Attention as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Attention` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Attention,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "attention_factory",
            "purpose": "A factory function that creates a Flax Linen attention module by wrapping an NNX-based `Attention` class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the NNX `Attention` class into a Flax Linen compatible module.",
                "Forwards all its configuration arguments (e.g., `config`, `num_query_heads`, `mesh`) to the `Attention` class constructor during the wrapping process."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Attention",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main configuration object containing model-wide settings.",
                "num_query_heads": "Number of attention heads for queries.",
                "num_kv_heads": "Number of attention heads for keys and values, used for Grouped-Query Attention (GQA).",
                "head_dim": "The dimension of each attention head.",
                "mesh": "The JAX device mesh for distributed computation.",
                "attention_kernel": "Specifies the attention implementation to use (e.g., 'dot_product', 'flash').",
                "inputs_q_shape": "The shape of the query input tensor, required for NNX initialization.",
                "inputs_kv_shape": "The shape of the key/value input tensor, required for NNX initialization.",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill', 'autoregressive')."
            },
            "notes": [
                "This function acts as a compatibility bridge to use an NNX-defined module within a Linen-based model architecture.",
                "It does not perform any tensor computations itself but rather instantiates and returns a module that does."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#Attention",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class Attention(nnx.Module):\n  \"\"\"Attention Module.\n\n  This module implements multi-headed attention as described in the\n  original Transformer paper. It projects the inputs into query, key, and\n  value vectors, applies the attention mechanism, and projects the results to\n  an output vector.\n\n  Attributes:\n    config: The model configuration.\n    num_query_heads: Number of query attention heads.\n    num_kv_heads: Number of key-value attention heads.\n    head_dim: The dimension of each attention head.\n    max_target_length: Maximum sequence length.\n    mesh: The device mesh.\n    attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n    inputs_q_shape: Query inputs shape for initialization, required by NNX.\n    inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n    dtype: The data type for computation.\n    weight_dtype: The data type for weights.\n    max_prefill_predict_length: Maximum length for prefill.\n    dropout_rate: The dropout rate.\n    kernel_init: Initializer for the kernel of the dense layers.\n    float32_qk_product: If True, compute query-key product in float32.\n    float32_logits: If True, cast logits to float32 before softmax.\n    quant: Quantization configuration.\n    kv_quant: KV cache quantization configuration.\n    attention_type: The type of attention (e.g., 'global', 'local_sliding').\n    attn_logits_soft_cap: Soft cap for attention logits.\n    ... and other configuration parameters.\n  \"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      base_kv_cache: bool = True,\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the Attention module.\n\n    Attributes:\n      config: The model configuration.\n      num_query_heads: Number of query attention heads.\n      num_kv_heads: Number of key-value attention heads.\n      head_dim: The dimension of each attention head.\n      max_target_length: Maximum sequence length.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n      inputs_q_shape: Query inputs shape for initialization, required by NNX.\n      inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n      dtype: The data type for computation.\n      weight_dtype: The data type for weights.\n      max_prefill_predict_length: Maximum length for prefill.\n      dropout_rate: The dropout rate.\n      kernel_init: Initializer for the kernel of the dense layers.\n      float32_qk_product: If True, compute query-key product in float32.\n      float32_logits: If True, cast logits to float32 before softmax.\n      quant: Quantization configuration.\n      kv_quant: KV cache quantization configuration.\n      attention_type: The type of attention (e.g., 'global', 'local_sliding').\n      attn_logits_soft_cap: Soft cap for attention logits.\n      sliding_window_size: The size of the sliding window for local attention.\n      use_ragged_attention: Whether to use ragged attention for decoding.\n      ragged_block_size: The block size for ragged attention.\n      use_qk_norm: Whether to apply normalization to query and key.\n      query_pre_attn_scalar: Scalar to apply to query before attention.\n      use_bias_in_projections: Whether to use bias in Q, K, V, and output projections.\n      temperature_tuning: Whether to use temperature tuning for attention.\n      temperature_tuning_scale: The scale for temperature tuning.\n      temperature_tuning_floor_scale: The floor scale for temperature tuning.\n      ... other configuration parameters.\n      is_nope_layer: Whether this is a \"NoPE\" (No Position-Embedding) layer.\n      is_vision: Whether this is a vision attention layer.\n      model_mode: The model's operational mode (e.g., 'train', 'prefill').\n      base_kv_cache: Whether to use base (non-MLA) kv cache, if KVCache is used\n      rngs: RNG state for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n\n    self.config = config\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.head_dim = head_dim\n    self.max_target_length = max_target_length\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.dropout_rate = dropout_rate\n    self.kernel_init = kernel_init\n    self.float32_qk_product = float32_qk_product\n    self.float32_logits = float32_logits\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n    self.use_qk_norm = use_qk_norm\n    self.query_pre_attn_scalar = query_pre_attn_scalar\n    self.use_bias_in_projections = use_bias_in_projections\n    self.temperature_tuning = temperature_tuning\n    self.temperature_tuning_scale = temperature_tuning_scale\n    self.temperature_tuning_floor_scale = temperature_tuning_floor_scale\n    self.prefill_query_axis_names = prefill_query_axis_names\n    self.prefill_key_axis_names = prefill_key_axis_names\n    self.prefill_value_axis_names = prefill_value_axis_names\n    self.query_axis_names = query_axis_names\n    self.key_axis_names = key_axis_names\n    self.value_axis_names = value_axis_names\n    self.ep_query_axis_names = ep_query_axis_names\n    self.ep_key_axis_names = ep_key_axis_names\n    self.ep_value_axis_names = ep_value_axis_names\n    self.input_axis_names = input_axis_names\n    self.ep_input_axis_names = ep_input_axis_names\n    self.out_axis_names = out_axis_names\n    self.ep_out_axis_names = ep_out_axis_names\n    self.prefill_input_axis_names = prefill_input_axis_names\n    self.decode_input_axis_names = decode_input_axis_names\n    self.prefill_out_axis_names = prefill_out_axis_names\n    self.decode_out_axis_names = decode_out_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.compute_axis_order = compute_axis_order\n    self.reshape_q = reshape_q\n    self.is_nope_layer = is_nope_layer\n    self.is_vision = is_vision\n    self.model_mode = model_mode\n    self.rngs = rngs\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.KVCache_0 = (\n        self.init_kv_caches(inputs_kv_shape=inputs_kv_shape)\n        if self.model_mode != MODEL_MODE_TRAIN and base_kv_cache\n        else None\n    )\n\n    self.rotary_embedding = self.init_rotary_embedding()\n\n    self.attention_op = AttentionOp(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        max_prefill_predict_length=self.max_prefill_predict_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_query_heads,\n        num_kv_heads=self.num_kv_heads,\n        dropout_rate=self.dropout_rate,\n        dtype=self.dtype,\n        compute_axis_order=self.compute_axis_order,\n        reshape_q=self.reshape_q,\n        attention_type=self.attention_type,\n        attn_logits_soft_cap=self.attn_logits_soft_cap,\n        sliding_window_size=self.sliding_window_size,\n        chunk_attn_window_size=self.config.chunk_attn_window_size,\n        use_ragged_attention=self.use_ragged_attention,\n        ragged_block_size=self.ragged_block_size,\n        rngs=self.rngs,\n    )\n    # When paged attention is enabled, paged attention op is used for all model modes except TRAIN,\n    # which uses default attention op.\n    if self.config.attention == \"paged\":\n      self.paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=self.head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n    self._init_projections(inputs_q_shape, inputs_kv_shape)\n\n    if self.config.attention_sink:\n      self.sinks = nnx.Param(\n          default_bias_init(self.rngs.params(), (self.config.num_query_heads,), self.weight_dtype),\n          sharding=(None,),\n      )\n    else:\n      self.sinks = None\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      self.query_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.key_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.query_norm = None\n      self.key_norm = None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the query, key, value, and output projections.\"\"\"\n    if self.config.fused_qkv:\n      self.qkv_proj = self.init_qkv_w(inputs_shape=inputs_q_shape)\n    else:\n      self.query = self.init_query_w(inputs_q_shape=inputs_q_shape)\n      self.key = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n      self.value = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n  def init_query_w(self, inputs_q_shape: Tuple) -> nnx.Module:\n    \"\"\"Query projection initialization.\"\"\"\n\n    # NOTE: T5 does not explicitly rescale the attention logits by\n    #       1/sqrt(depth_kq)!  This is folded into the initializers of the\n    #       linear transformations, which is equivalent under Adafactor.\n    # We disable depth_scaling when using qk_norm or a query_pre_attn_scalar\n    # to avoid applying scaling twice.\n    if self.config.use_qk_norm or (self.query_pre_attn_scalar is not None and self.query_pre_attn_scalar != 1.0):\n      depth_scaling = 1.0\n    else:\n      depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n\n    def query_init(*args):\n      # pylint: disable=no-value-for-parameter\n      return self.kernel_init(*args) / depth_scaling\n\n    kernel_axes = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"embed\", \"q_heads\", \"kv\")\n    )\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_q_shape),\n        out_features_shape=(self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=query_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def query_projection(self, inputs_q: Array) -> Array:\n    \"\"\"Query projection.\"\"\"\n\n    return self.query(inputs_q)\n\n  def init_kv_w(self, inputs_kv_shape: Tuple) -> nnx.Module:\n    \"\"\"Initializes the key or value projection.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A DenseGeneral module that performs the key or value projection.\n    \"\"\"\n    if self.num_kv_heads == -1:\n      raise ValueError(\"num_kv_heads is not defined.\")\n\n    if self.num_query_heads % self.num_kv_heads != 0:\n      raise ValueError(\"Invalid num_kv_heads for GQA.\")\n\n    kernel_axes = (\n        (None, None, None)\n        if self.config.ici_context_autoregressive_parallelism > 1\n        else (\"embed\", \"kv_heads\", \"kv_head_dim\")\n    )\n\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_kv_shape),\n        out_features_shape=(self.num_kv_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def kv_projection(self, inputs_kv: Array, proj_name: str) -> nnx.Module:\n    \"\"\"Applies the key or value projection.\n\n    Args:\n      inputs_kv: The input tensor to project.\n      proj_name: The name of the projection (\"key\" or \"value\").\n\n    Returns:\n      The projected key or value tensor.\n\n    Raises:\n      ValueError: If `proj_name` is not one of the supported values\n        (\"key\", \"value\").\n\n    \"\"\"\n    if proj_name == \"key\":\n      return self.key(inputs_kv)\n    elif proj_name == \"value\":\n      return self.value(inputs_kv)\n    else:\n      raise ValueError(f\"proj_name must be 'key' or 'value', but got {proj_name}\")\n\n  def init_qkv_w(self, inputs_shape: Tuple) -> nnx.Module:\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_shape),\n        out_features_shape=(3, self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = self.qkv_proj(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def init_out_w(self, output_dim: int) -> nnx.Module:\n    \"\"\"out projection\"\"\"\n    out_kernel_axis = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"heads\", \"kv\", \"embed\")\n    )\n    return DenseGeneral(\n        in_features_shape=(self.num_query_heads, self.head_dim),\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=out_kernel_axis,  # trade speed with memory\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def out_projection(self, out: Array) -> Array:\n    \"\"\"out projection\"\"\"\n\n    return self.out(out)\n\n  def convert_dense_general_inputs_shape(\n      self,\n      inputs_shape: tuple[int, ...] | None = None,\n      axis: Union[Iterable[int], int] = -1,\n  ) -> Union[Iterable[int], int]:\n    axis = canonicalize_tuple(axis)\n    return tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n\n  def init_rotary_embedding(self):\n    \"\"\"Initializes the rotary embeddings, handling different model types.\n\n    Returns:\n      The rotary embedding module that will be used in the model.\n    \"\"\"\n    if self.config.attention_type == AttentionType.MLA.value:\n      # For MLA attention RoPE is applied to only `self.qk_rope_head_dim` portion the heads.\n      rope_embedding_dims = self.qk_rope_head_dim\n    else:\n      rope_embedding_dims = self.head_dim\n\n    rope_type = self.config.rope_type.lower()\n    rope_use_scale = self.config.rope_use_scale\n    if self.is_vision:\n      rotary_embedding = LlamaVisionRotaryEmbedding(\n          image_size=self.config.image_size_for_vit,\n          patch_size=self.config.patch_size_for_vit,\n          hidden_size=self.config.hidden_size_for_vit,\n          num_attention_heads=self.config.num_attention_heads_for_vit,\n          rope_theta=self.config.rope_theta_for_vit,\n          fprop_dtype=self.dtype,\n          rngs=self.rngs,\n      )\n    elif self.config.model_name.startswith(\"llama3.1\") or rope_type.startswith(\"llama3.1\"):\n      rotary_embedding = LLaMARotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=self.config.rope_max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          use_scale=rope_use_scale,\n          rngs=self.rngs,\n      )\n    elif rope_type.startswith(\"yarn\"):\n      rotary_embedding = YarnRotaryEmbedding(\n          max_position_embeddings=self.config.max_position_embeddings,\n          original_max_position_embeddings=self.config.original_max_position_embeddings,\n          beta_fast=self.config.beta_fast,\n          beta_slow=self.config.beta_slow,\n          rope_theta=self.config.rope_max_timescale,\n          rope_factor=self.config.rope_factor,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          interleave=self.config.rope_interleave,\n          truncate=self.config.rope_truncate,\n          attention_scaling=self.config.rope_attention_scaling,\n          rngs=self.rngs,\n      )\n    else:\n      max_timescale = self.config.rope_max_timescale\n      # For local attention use local_rope_max_timescale if it's is positive\n      if self.attention_type == AttentionType.LOCAL_SLIDING and self.config.local_rope_max_timescale > 0:\n        max_timescale = self.config.local_rope_max_timescale\n\n      rope_linear_scaling_factor = self.config.rope_linear_scaling_factor\n      # In gemma3, linear scaling factor does not apply to local sliding layers.\n      if self.config.model_name.startswith(\"gemma3\") and self.attention_type == AttentionType.LOCAL_SLIDING:\n        rope_linear_scaling_factor = 1.0\n\n      rotary_embedding = RotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          rope_linear_scaling_factor=rope_linear_scaling_factor,\n          rngs=self.rngs,\n      )\n    return rotary_embedding\n\n  def apply_rotary_embedding(self, inputs: Array, inputs_positions: Optional[Array | None] = None):\n    \"\"\"Applies rotary embeddings, handling different model types.\n\n    Args:\n      inputs: The input tensor to apply rotary embeddings to.\n      inputs_positions: The positions of the inputs.\n      name: A name for the embedding layer.\n\n    Returns:\n      The input tensor with rotary embeddings applied.\n    \"\"\"\n    return self.rotary_embedding(inputs, inputs_positions)\n\n  def init_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes KVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A KVCache module instance.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in KVCache.\n    # However, KVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # KVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.KVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_heads=self.num_kv_heads,\n        value_heads=self.num_kv_heads,\n        key_head_size=self.head_dim,\n        value_head_size=self.head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n  def update_kv_caches(self, key, value, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"Updates the KV caches for prefill and autoregressive modes.\n\n    This method uses a kvcache module to update and retrieve the key-value\n    caches based on the current operational mode.\n\n    Args:\n      key: The key tensor for the current attention computation.\n      value: The value tensor for the current attention computation.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, or None.\n      - The autoregressive key-value cache, or None.\n    \"\"\"\n    prefill_kv_cache, ar_kv_cache = self.KVCache_0(\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Any = None,\n  ):\n    \"\"\"Applies Attention on the input data.\n\n    Projects the inputs into multi-headed query, key, and value vectors,\n    applies dot-product attention, and project the results to an output vector.\n\n    This method handles three modes:\n    1.  **Training**: The KV cache is ignored.\n    2.  **Prefill**: The KV cache is filled with the key-value pairs from the input sequence.\n    3.  **Autoregressive Decoding**: The KV cache is used to provide context from previous steps.\n\n    In the cache initialization call, `inputs_q` has a shape [batch, length,\n    q_features] and `inputs_kv`: [batch, length, kv_features]. During the\n    incremental decoding stage, query, key and value all have the shape [batch,\n    1, qkv_features] corresponding to a single step.\n\n    Args:\n      inputs_q: Input queries of shape `[batch, q_length, q_features]`.\n      inputs_kv: Key/values of shape `[batch, kv_length, kv_features]`.\n      inputs_positions: Input positions for rotary embeddings.\n      decoder_segment_ids: Segment IDs for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      deterministic: If True, disables dropout.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      output of shape `[batch, length, q_features]`.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.decode_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.decode_input_axis_names)\n\n    # apply projection.\n    if self.config.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.query_projection(inputs_q)\n      key = self.kv_projection(inputs_kv, proj_name=\"key\")\n      value = self.kv_projection(inputs_kv, proj_name=\"value\")\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    # NOTE: llama 4 does L2 normalization after RoPE\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      query = self.query_norm(query)\n      key = self.key_norm(key)\n\n    # NOTE: is_nope_layer should be used in attention mask and also used in attention tuning\n    use_rope = not self.is_nope_layer\n    use_qk_norm = self.use_qk_norm and use_rope\n\n    if use_rope:\n      query = self.apply_rotary_embedding(query, inputs_positions=inputs_positions)\n      key = self.apply_rotary_embedding(key, inputs_positions=inputs_positions)\n\n    if use_qk_norm and is_llama4_decoder_block:\n      l2_norm = L2Norm(eps=self.config.normalization_layer_epsilon)\n      query = l2_norm(query)\n      key = l2_norm(key)\n\n    # apply query_pre_attn_scalar if it's present.\n    if self.query_pre_attn_scalar and self.query_pre_attn_scalar != 1.0:\n      query = query * self.query_pre_attn_scalar\n\n    if self.temperature_tuning and not use_rope:\n      attn_scales = (\n          jnp.log(jnp.floor((inputs_positions.astype(self.dtype) + 1.0) / self.temperature_tuning_floor_scale) + 1.0)\n          * self.temperature_tuning_scale\n          + 1.0\n      )\n      query = (query * attn_scales[:, :, jnp.newaxis, jnp.newaxis]).astype(self.dtype)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      query = nn.with_logical_constraint(query, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      key = nn.with_logical_constraint(key, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n      value = nn.with_logical_constraint(value, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    assert not self.config.quantize_kvcache or self.kv_quant\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      cached_values = [None, None]\n      if model_mode != MODEL_MODE_TRAIN:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      out = self.attention_op(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          cached_values,\n          previous_chunk,\n          bidirectional_mask,\n          self.sinks,\n      )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      out = nn.with_logical_constraint(out, self.prefill_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.decode_out_axis_names)\n    out = self.out_projection(out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "multi_head_attention",
            "purpose": "Implements a multi-headed attention mechanism, projecting inputs into query, key, and value vectors, applying attention, and projecting the results to an output vector.",
            "input": {
                "shape": "inputs_q: [batch, q_length, q_features], inputs_kv: [batch, kv_length, kv_features]",
                "dtype": "Configurable via the 'dtype' parameter, e.g., jnp.float32."
            },
            "processing_steps": [
                "Apply logical constraints to input tensors based on `model_mode`.",
                "Project inputs into query, key, and value tensors using either fused or separate projections.",
                "Optionally apply RMSNorm to query and key tensors.",
                "Optionally apply rotary position embeddings (RoPE) to query and key tensors.",
                "Optionally apply L2 normalization to query and key tensors (for Llama4 style models).",
                "Optionally scale the query tensor with a pre-attention scalar.",
                "Optionally apply temperature tuning scaling to the query tensor.",
                "Apply logical constraints to projected Q, K, V tensors for sharding.",
                "Perform the attention computation using either `paged_attention_op` (for paged inference) or `attention_op` (for training or standard inference).",
                "Update KV cache if in an inference mode (`prefill` or `autoregressive`).",
                "Apply logical constraints to the output of the attention operation.",
                "Project the attention output to the final dimension using an output projection layer."
            ],
            "output": {
                "shape": "[batch, length, q_features]"
            },
            "dependencies": [
                "nnx.Module",
                "Config",
                "AttentionOp",
                "paged_attention.PagedAttentionOp",
                "kvcache.KVCache",
                "DenseGeneral",
                "RMSNorm",
                "L2Norm",
                "RotaryEmbedding",
                "LLaMARotaryEmbedding",
                "YarnRotaryEmbedding",
                "LlamaVisionRotaryEmbedding"
            ],
            "parameters": {
                "config": "The model configuration object.",
                "num_query_heads": "Number of query attention heads.",
                "num_kv_heads": "Number of key-value attention heads (for Grouped-Query Attention).",
                "head_dim": "The dimension of each attention head.",
                "attention_kernel": "The attention kernel to use (e.g., 'dot_product', 'flash').",
                "model_mode": "The operational mode of the model, e.g., 'train', 'prefill', 'autoregressive'.",
                "use_qk_norm": "If True, applies normalization to query and key tensors.",
                "attention_type": "The type of attention (e.g., 'global', 'local_sliding').",
                "fused_qkv": "A boolean from the config indicating whether to use a single fused projection for Q, K, and V."
            },
            "notes": [
                "The class is an `nnx.Module`, designed to be compatible with Flax Linen via the `nnx_wrappers.to_linen` bridge.",
                "It supports different operational modes: training, prefill, and autoregressive decoding, with specific logic for KV caching in inference modes.",
                "Includes extensive configuration options for quantization, precision, sharding, and different attention variants like paged attention, sliding window attention, and fused QKV projections.",
                "Handles various types of rotary position embeddings (RoPE) based on the model configuration."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Attention module, setting up configuration and sub-modules like projections, rotary embeddings, KV cache, and attention operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters as instance attributes.",
                        "Initialize KV cache (`KVCache_0`) if not in training mode and `base_kv_cache` is true.",
                        "Initialize the appropriate rotary embedding module by calling `init_rotary_embedding`.",
                        "Initialize the main `AttentionOp` module.",
                        "Initialize `PagedAttentionOp` if paged attention is configured.",
                        "Initialize Q, K, V, and output projection layers by calling `_init_projections`.",
                        "Initialize optional attention sinks if configured.",
                        "Initialize optional query and key normalization layers (`RMSNorm`)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "kvcache.KVCache",
                        "AttentionOp",
                        "paged_attention.PagedAttentionOp",
                        "RotaryEmbedding",
                        "DenseGeneral",
                        "RMSNorm"
                    ],
                    "notes": [
                        "The initialization logic is highly configurable, adapting to different model modes, attention types, and hardware setups (via `mesh`)."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the query, key, value, and output projection layers.",
                    "input": {
                        "shape": "inputs_q_shape: Tuple, inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If `config.fused_qkv` is True, initialize a single `qkv_proj` layer using `init_qkv_w`.",
                        "Otherwise, initialize separate `query`, `key`, and `value` layers using `init_query_w` and `init_kv_w`.",
                        "Initialize the `out` projection layer using `init_out_w`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.init_qkv_w",
                        "self.init_query_w",
                        "self.init_kv_w",
                        "self.init_out_w"
                    ],
                    "notes": [
                        "This method modifies the instance by creating the projection sub-modules. It supports both separate and fused QKV projection layers."
                    ]
                },
                "init_rotary_embedding": {
                    "purpose": "Selects and initializes the appropriate rotary position embedding module based on the model configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine the embedding dimension based on attention type.",
                        "Check `config.rope_type`, `config.model_name`, and `is_vision` flags.",
                        "Instantiate one of `LlamaVisionRotaryEmbedding`, `LLaMARotaryEmbedding`, `YarnRotaryEmbedding`, or the base `RotaryEmbedding` class with corresponding configuration parameters."
                    ],
                    "output": {
                        "shape": "An initialized rotary embedding nnx.Module."
                    },
                    "dependencies": [
                        "LlamaVisionRotaryEmbedding",
                        "LLaMARotaryEmbedding",
                        "YarnRotaryEmbedding",
                        "RotaryEmbedding"
                    ],
                    "notes": [
                        "Dynamically chooses the RoPE implementation, allowing for different scaling strategies like YaRN and linear scaling."
                    ]
                },
                "init_kv_caches": {
                    "purpose": "Initializes the KVCache module for inference.",
                    "input": {
                        "shape": "inputs_kv_shape: [batch_size, sequence_length, features]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract batch size from the input shape.",
                        "Instantiate the `kvcache.KVCache` module with parameters derived from the main Attention module's configuration."
                    ],
                    "output": {
                        "shape": "An initialized `kvcache.KVCache` module instance."
                    },
                    "dependencies": [
                        "kvcache.KVCache"
                    ],
                    "notes": [
                        "Uses a placeholder sequence length of 1 during initialization, as the internal cache shapes depend on `max_prefill_length` and `max_target_length`."
                    ]
                },
                "update_kv_caches": {
                    "purpose": "Updates and retrieves the key-value caches for prefill and autoregressive decoding modes.",
                    "input": {
                        "shape": "key: [batch, seq, num_kv_heads, head_dim], value: [batch, seq, num_kv_heads, head_dim], decoder_segment_ids: [batch, seq]",
                        "dtype": "Matches the module's dtype."
                    },
                    "processing_steps": [
                        "Calls the initialized `KVCache_0` module with the current key, value, and other state information."
                    ],
                    "output": {
                        "shape": "A list containing two elements: the prefill key-value cache and the autoregressive key-value cache."
                    },
                    "dependencies": [
                        "self.KVCache_0"
                    ],
                    "notes": [
                        "This method abstracts the interaction with the KV cache module during inference."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the multi-head attention mechanism.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, q_features], inputs_kv: [batch, kv_length, kv_features], inputs_positions: [batch, length]",
                        "dtype": "Configurable, typically float32 or bfloat16."
                    },
                    "processing_steps": [
                        "Apply logical constraints to input tensors for sharding.",
                        "Project inputs to query, key, and value tensors.",
                        "Optionally apply normalization (RMSNorm/L2Norm) to query and key.",
                        "Optionally apply rotary position embeddings to query and key.",
                        "Optionally apply pre-attention scaling or temperature tuning to query.",
                        "Apply logical constraints to Q, K, V tensors.",
                        "If using paged attention in inference, call `paged_attention_op`.",
                        "Otherwise, update KV caches (if in inference) and call `attention_op`.",
                        "Apply logical constraints to the attention output.",
                        "Project the attention output to its final shape."
                    ],
                    "output": {
                        "shape": "[batch, length, q_features]"
                    },
                    "dependencies": [
                        "self.qkv_projection",
                        "self.query_projection",
                        "self.kv_projection",
                        "self.query_norm",
                        "self.key_norm",
                        "self.apply_rotary_embedding",
                        "L2Norm",
                        "self.paged_attention_op",
                        "self.update_kv_caches",
                        "self.attention_op",
                        "self.out_projection"
                    ],
                    "notes": [
                        "Handles different execution paths for training, prefill, and autoregressive decoding. Uses `nn.with_logical_constraint` for tensor sharding across devices."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#DecoderLayer",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class DecoderLayer(nn.Module):\n  \"\"\"\n  Transformer decoder layer that attends to the encoder.\n  This is the core, reusable building block for both the main model's\n  decoder stack and the auxiliary MTP layers.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    if self.model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_length\", \"activation_mlp\")\n    elif cfg.expert_shard_attention_option == EP_AS_CONTEXT and self.model_mode == MODEL_MODE_TRAIN:\n      logical_axis_names = (\"activation_batch_no_exp\", \"activation_length\", \"activation_mlp\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_length_no_exp\", \"activation_mlp\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    if model_mode == MODEL_MODE_PREFILL:\n      lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n    else:\n      lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=self.config,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n    else:\n      attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n\n    # MLP block.\n    mlp_lnx = linears.mlp_block(\n        in_features=lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        model_mode=model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(lnx, deterministic=deterministic)\n    if model_mode == MODEL_MODE_PREFILL:\n      mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n    else:\n      mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    next_layer_addition = mlp_lnx + attention_lnx\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out + inputs\n    if model_mode == MODEL_MODE_PREFILL:\n      layer_output = nn.with_logical_constraint(\n          layer_output,\n          logical_axis_names,\n      )\n    else:\n      layer_output = nn.with_logical_constraint(\n          layer_output,\n          logical_axis_names,\n      )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    return layer_output, None if cfg.scan_layers else layer_output",
        "analysis": {
            "module_type": "transformer_decoder_layer",
            "purpose": "A single, reusable transformer decoder layer that performs self-attention and a feed-forward MLP block, forming a core building block for a decoder stack.",
            "input": {
                "shape": "See the '__call__' method.",
                "dtype": "See the '__call__' method."
            },
            "processing_steps": [
                "The primary logic is within the '__call__' method, which processes input tensors through the decoder layer's components."
            ],
            "output": {
                "shape": "See the '__call__' method."
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.linears.mlp_block",
                "flax.linen.Dropout",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like dimensions, dropout rates, and data types.",
                "mesh": "The JAX device mesh used for distributed computation and sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This layer uses a pre-normalization architecture, where layer normalization is applied before the self-attention and MLP blocks.",
                "The self-attention and MLP blocks are applied in parallel to the same normalized input, and their outputs are summed before the residual connection. This differs from the standard sequential application.",
                "The layer's return signature is designed to be compatible with `flax.linen.scan` for efficient stacking of layers."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through one transformer decoder layer, applying self-attention and an MLP block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "Inferred from `config.dtype` (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply logical sharding constraints to the input tensor using `nn.with_logical_constraint`.",
                        "Apply `checkpoint_name` for gradient checkpointing.",
                        "Apply pre-attention RMS normalization (`rms_norm`) to the input.",
                        "Compute self-attention on the normalized input using an `attention_as_linen` layer.",
                        "Compute the MLP transformation on the same normalized input using `linears.mlp_block`.",
                        "Sum the outputs of the parallel attention and MLP blocks.",
                        "Apply dropout to the summed result.",
                        "Add the original input tensor to create a residual connection, resulting in the final layer output.",
                        "Optionally record intermediate activation metrics if `config.record_internal_nn_metrics` is true."
                    ],
                    "output": {
                        "shape": "Returns a tuple: `(layer_output, carry)`. `layer_output` has shape `[batch_size, sequence_length, embedding_dim]`. `carry` is either `None` or a tensor of the same shape, depending on `config.scan_layers`."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "linears.mlp_block",
                        "nn.Dropout",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "The `deterministic` argument controls whether dropout is applied.",
                        "The `model_mode` argument influences sharding annotations and potentially the behavior of sub-layers like attention.",
                        "The return value is a tuple `(output, carry)` to support `flax.linen.scan`. The `carry` is `None` if `config.scan_layers` is true, otherwise it is the same as the output."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#SequentialBlockDecoderLayers",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class SequentialBlockDecoderLayers(nn.Module):\n  \"\"\"Sequential unscanned series of decoder layers.\"\"\"\n\n  decoder_layer: Any\n  num_decoder_layers: int\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  model_mode: str\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic: bool,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ) -> jnp.ndarray:\n    for lyr in range(self.num_decoder_layers):\n      inputs = self.decoder_layer(\n          config=self.config, mesh=self.mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=model_mode\n      )(\n          inputs,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          slot=slot,\n          page_state=page_state,\n      )\n      if self.config.scan_layers:\n        inputs = inputs[0]  #  When scan_layers is True the decoder layers return (outputs, None).\n    if self.config.scan_layers:\n      return inputs, None  # pytype: disable=bad-return-type\n    else:\n      return inputs",
        "analysis": {
            "module_type": "sequential_block_decoder_layers",
            "purpose": "Applies a specified decoder layer sequentially for a given number of times, serving as a non-scanned alternative to a Flax scan operation over layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a decoder layer, number of layers, config, mesh, quantization, and model mode.",
                "The `__call__` method iterates `num_decoder_layers` times, applying the decoder layer to the input tensor in each iteration."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "page_manager.PageState"
            ],
            "parameters": {
                "decoder_layer": "The nn.Module class for the decoder layer to be applied.",
                "num_decoder_layers": "The number of times the decoder layer is sequentially applied.",
                "config": "The model configuration object.",
                "config.scan_layers": "A boolean flag that adjusts the processing and return signature to be compatible with Flax's `nn.scan`."
            },
            "notes": [
                "This module is typically used when `config.scan_layers` is false but pipeline parallelism with layer scanning per stage (`scan_layers_per_stage`) is enabled.",
                "It provides a simple sequential loop over layers while maintaining API compatibility with scanned layers if needed."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Sequentially applies the configured decoder layer `num_decoder_layers` times to the input tensor.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Loop `self.num_decoder_layers` times.",
                        "Call the `self.decoder_layer` instance with the current `inputs` and other forwarded arguments.",
                        "If `self.config.scan_layers` is true, unpack the tuple `(outputs, None)` returned by the layer and update `inputs` with `outputs`.",
                        "After the loop, check `self.config.scan_layers`.",
                        "If true, return `(inputs, None)`.",
                        "If false, return `inputs`."
                    ],
                    "output": {
                        "shape": "Returns `[batch_size, sequence_length, hidden_dim]` if `config.scan_layers` is false, otherwise returns a tuple `([batch_size, sequence_length, hidden_dim], None)`."
                    },
                    "dependencies": [
                        "self.decoder_layer"
                    ],
                    "notes": [
                        "The return signature is conditional on `self.config.scan_layers` to maintain API compatibility with scanned layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#Decoder",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class Decoder(nn.Module):\n  \"\"\"A stack of decoder layers as a part of an encoder-decoder architecture.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: None | Quant = None\n  model_mode: str = MODEL_MODE_TRAIN\n\n  def setup(self):\n    \"\"\"Initialize decoder layer.\"\"\"\n    self.decoder_layer = self.get_decoder_layers()\n    self.norm_layer = self.get_norm_layer(num_features=self.config.emb_dim)\n    if self.config.using_pipeline_parallelism:\n      pipeline_stage_module = self.get_pipeline_stage_module(self.decoder_layer)\n      remat_policy = self.get_remat_policy()\n      self.pipeline_module = pipeline.Pipeline(\n          config=self.config, mesh=self.mesh, layers=pipeline_stage_module, remat_policy=remat_policy\n      )\n\n  def minimal_policy(self, with_context=False):\n    \"\"\"Helper for creating minimal checkpoint policies.\"\"\"\n    names = [\n        \"query_proj\",\n        \"value_proj\",\n        \"key_proj\",\n        \"qkv_proj\",\n        \"out_proj\",\n        \"mlpwi_0\",\n        \"mlpwi_1\",\n        \"mlpwi\",\n        \"mlpwo\",\n    ]\n    if with_context:\n      names.append(\"context\")\n    return jax.checkpoint_policies.save_only_these_names(*names)\n\n  def get_remat_policy(self):\n    \"\"\"Get remat policy\"\"\"\n    policy = None\n    cfg = self.config\n    if cfg.remat_policy != \"none\":\n      if cfg.remat_policy in (\"minimal_with_context\", \"minimal_flash\"):\n        # save all\n        if cfg.remat_policy == \"minimal_flash\":\n          max_logging.log(\"WARNING: 'minimal_flash' will be deprecated soon, please use 'minimal_with_context' instead.\")\n          max_logging.log(\"WARNING: 'minimal_flash' will be deprecated soon, please use 'minimal_with_context' instead.\")\n        policy = self.minimal_policy(with_context=True)\n      elif cfg.remat_policy == \"minimal\":\n        # save all except context\n        policy = self.minimal_policy()\n      elif cfg.remat_policy == \"save_dot_with_context_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"context\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlpwi\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n            \"mlpwo\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_qkv_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n        )\n      elif cfg.remat_policy == \"qkv_proj_offloaded\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\"query_proj\", \"value_proj\", \"key_proj\"],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"minimal_offloaded\":\n        # offload all except context\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\n                \"query_proj\",\n                \"value_proj\",\n                \"key_proj\",\n                \"qkv_proj\",\n                \"out_proj\",\n                \"mlpwi_0\",\n                \"mlpwi_1\",\n                \"mlpwi\",\n                \"mlpwo\",\n            ],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"custom\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=cfg.tensors_on_device,\n            names_which_can_be_offloaded=cfg.tensors_to_offload,\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"save_out_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"out_proj\",\n        )\n      else:\n        assert cfg.remat_policy == \"full\", \"Remat policy needs to be on list of remat policies\"\n        policy = None\n    return policy\n\n  def get_decoder_layers(self):\n    \"\"\"Retrieves a list of decoder layer classes based on the `decoder_block` config.\n\n    Returns:\n        A list containing one or more `nn.Module` classes for the decoder.\n    \"\"\"\n    match self.config.decoder_block:\n      case DecoderBlockType.DEFAULT:\n        return [DecoderLayer]\n      case DecoderBlockType.LLAMA2:\n        return [llama2.LlamaDecoderLayerToLinen]\n      case DecoderBlockType.MISTRAL:\n        # TODO(ranran): update to Mistral with sliding window attention\n        return [mistral.MistralDecoderLayerToLinen]\n      case DecoderBlockType.MIXTRAL:\n        return [mixtral.MixtralDecoderLayerToLinen]\n      case DecoderBlockType.DEEPSEEK:\n        if self.config.use_batch_split_schedule:\n          return [deepseek_batchsplit.DeepSeekDenseLayer, deepseek_batchsplit.DeepSeekMoELayer]\n        else:\n          return [deepseek.DeepSeekDenseLayer, deepseek.DeepSeekMoELayer]\n      case DecoderBlockType.GEMMA:\n        return [gemma.GemmaDecoderLayerToLinen]\n      case DecoderBlockType.GEMMA2:\n        return [gemma2.Gemma2DecoderLayerToLinen]\n      case DecoderBlockType.GEMMA3:\n        return [gemma3.Gemma3DecoderLayerToLinen]\n      case DecoderBlockType.GPT3:\n        return [gpt3.Gpt3DecoderLayer]\n      case DecoderBlockType.GPT_OSS:\n        return [gpt_oss.GptOssScannableBlock] if self.config.scan_layers else [gpt_oss.GptOssDecoderLayer]\n      case DecoderBlockType.QWEN3:\n        return [qwen3.Qwen3DecoderLayerToLinen]\n      case DecoderBlockType.QWEN3_MOE:\n        return [qwen3.Qwen3MoeDecoderLayerToLinen]\n      case DecoderBlockType.QWEN3_NEXT:\n        return [qwen3.Qwen3NextScannableBlockToLinen] if self.config.scan_layers else [qwen3.Qwen3NextDecoderLayerToLinen]\n      case DecoderBlockType.SIMPLE:\n        return [simple_layer.SimpleDecoderLayerToLinen]\n      case DecoderBlockType.SIMPLE_MLP:\n        return [simple_layer.SimpleMlpDecoderLayerToLinen]\n      case DecoderBlockType.LLAMA4:\n        return [llama4.Llama4ScannableBlockToLinen] if self.config.scan_layers else [llama4.Llama4DecoderLayerToLinen]\n      case _:\n        # Default case to handle any unknown decoder block types.\n        raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def set_remat_policy(self, block_layers, policy):\n    \"\"\"Set remat policy\"\"\"\n    RemattedBlockLayers = []\n    for block_layer in block_layers:\n      if self.config.parameter_memory_host_offload:\n        # Define parameter movement with mesh-based sharding\n        def move_to_device(variables):\n          \"\"\"Move parameters to device with proper sharding.\"\"\"\n\n          def map_fn(path, value):\n            max_logging.log(f\"models.py: Moving parameter {path} to device\")\n            return jax.device_put(value, max_utils.device_space())\n\n          return jax.tree_util.tree_map_with_path(map_fn, variables)\n\n        # Transform layer class before remat\n        block_layer = nn.map_variables(block_layer, [\"params\"], move_to_device, mutable=True)\n\n      # Apply remat policy to layer\n      layer = nn.remat(\n          block_layer,\n          prevent_cse=maxtext_utils.should_prevent_cse_in_remat(self.config),\n          policy=policy,\n          static_argnums=(4, 5),  # Deterministic and model mode are static arguments.\n      )\n      RemattedBlockLayers.append(layer)\n    return RemattedBlockLayers\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer (return type inherits from nn.Module)\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.QWEN3_MOE,\n        DecoderBlockType.QWEN3_NEXT,\n        DecoderBlockType.GPT_OSS,\n        DecoderBlockType.SIMPLE,\n        DecoderBlockType.SIMPLE_MLP,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(rms_norm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      return functools.partial(gpt3.gpt3_layer_norm, num_features=num_features, reductions_in_fp32=False, use_bias=True)\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def scan_decoder_layers(self, cfg, decoder_layer, length, metadata_axis_name, mesh, in_axes_tuple, **kwargs):\n    \"\"\"scan decoder layers, calls `flax.linen.transforms.scan`\"\"\"\n    initializing = self.is_mutable_collection(\"params\")\n    params_spec = cfg.param_scan_axis if initializing else ScanIn(cfg.param_scan_axis)\n    cache_spec = 0\n    scan_fn = nn.scan(\n        decoder_layer,\n        variable_axes={\n            \"params\": params_spec,\n            \"cache\": cache_spec,\n            \"intermediates\": 0,\n            \"aqt\": 0,\n            \"_overwrite_with_gradient\": 0,\n        },\n        split_rngs={\n            \"params\": True,\n            \"dropout\": cfg.enable_dropout,\n        },\n        in_axes=in_axes_tuple,\n        length=length,\n        metadata_params={nn.PARTITION_NAME: metadata_axis_name},\n    )\n    return scan_fn(\n        config=cfg, mesh=mesh, name=metadata_axis_name, quant=self.quant, **kwargs  # pytype: disable=wrong-keyword-args\n    )\n\n  def get_pipeline_stage_module(self, decoder_blocks):\n    \"\"\"get pipeline stage module\"\"\"\n\n    def get_layer_to_pipeline(blocks, cfg):\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        return blocks[1]  # return the sparse block\n      else:\n        return blocks[0]\n\n    cfg = self.config\n    base_stage = get_layer_to_pipeline(decoder_blocks, cfg)\n    if cfg.set_remat_policy_on_layers_per_stage:\n      policy = self.get_remat_policy()\n      base_stage = self.set_remat_policy([base_stage], policy)[0]\n    if cfg.num_layers_per_pipeline_stage == 1:\n      stage_module = base_stage(config=cfg, mesh=self.mesh, quant=self.quant, model_mode=self.model_mode)\n    elif cfg.scan_layers_per_stage:\n      stage_module = self.scan_decoder_layers(\n          cfg,\n          base_stage,\n          cfg.num_layers_per_pipeline_stage,\n          \"layers_per_stage\",\n          self.mesh,\n          in_axes_tuple=(nn.broadcast,) * 4,\n      )\n    else:\n      stage_module = SequentialBlockDecoderLayers(\n          decoder_layer=base_stage,\n          num_decoder_layers=cfg.num_layers_per_pipeline_stage,\n          config=cfg,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=self.model_mode,\n      )\n    return stage_module\n\n  @nn.compact\n  def _apply_embedding(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      image_embeddings=None,\n      bidirectional_mask=None,\n      image_masks=None,\n  ):\n    \"\"\"Applies token and positional embeddings to the input tokens.\"\"\"\n    cfg = self.config\n\n    y = shared_embedding(decoder_input_tokens.astype(\"int32\"), model_mode=model_mode)\n\n    # Merge the image embeddings with the text embeddings for multimodal models\n    if image_embeddings is not None and cfg.use_multimodal:\n      if cfg.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\", \"llama4-17b-16e\", \"llama4-17b-128e\"]:\n        y = multimodal_utils.merge_mm_embeddings(\n            text_embeddings=y,\n            vision_embeddings=image_embeddings,\n            mask=bidirectional_mask,\n            image_masks=image_masks,\n        )\n      # TODO(hengtaoguo): Add support for other multimodal models such as Llama4, refactor if needed\n      else:\n        raise ValueError(f\"Unsupported model_name for multimodal: {cfg.model_name}\")\n\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n    y = y.astype(cfg.dtype)\n\n    if cfg.use_untrainable_positional_embedding:\n      y = positional_embedding_as_linen(embedding_dims=cfg.base_emb_dim)(y, decoder_positions)\n\n    if cfg.trainable_position_size > 0:\n      y += embed_as_linen(\n          num_embeddings=cfg.trainable_position_size,\n          num_features=cfg.emb_dim,\n          dtype=cfg.dtype,\n          embedding_init=nn.initializers.normal(stddev=1.0),\n          name=\"position_embedder\",\n          config=cfg,\n      )(decoder_positions, model_mode=model_mode)\n    return y\n\n  @nn.compact\n  def _apply_output_head(self, shared_embedding: nn.Module | nnx.Module, y, deterministic, model_mode):\n    \"\"\"Applies final normalization and projects hidden states to logits.\"\"\"\n\n    cfg = self.config\n    y = self.get_norm_layer(num_features=y.shape[-1])(\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"decoder_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n    )(y)\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n\n    # [batch, length, emb_dim] -> [batch, length, vocab_size]\n    if cfg.logits_via_embedding:\n      # Use the transpose of embedding matrix for logit transform.\n      if isinstance(shared_embedding, nnx.Module):\n        embedding_table = shared_embedding.embedding.value\n      else:\n        embedding_table = shared_embedding.variables[\"params\"][\"embedding\"]\n      if isinstance(embedding_table, nn.spmd.LogicallyPartitioned):\n        embedding_table = embedding_table.unbox()\n      attend_dtype = jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype\n      logits = attend_on_embedding(y, embedding_table, attend_dtype, self.config)\n\n      if self.config.normalize_embedding_logits:\n        # Correctly normalize pre-softmax logits for this shared case.\n        logits = logits / jnp.sqrt(y.shape[-1])\n      if cfg.final_logits_soft_cap:\n        logits = logits / cfg.final_logits_soft_cap\n        logits = jnp.tanh(logits) * cfg.final_logits_soft_cap\n    else:\n      logits = linears.dense_general(\n          inputs_shape=y.shape,\n          out_features_shape=cfg.vocab_size,\n          weight_dtype=cfg.weight_dtype,\n          dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n          kernel_axes=(\"embed\", \"vocab\"),\n          name=\"logits_dense\",\n          matmul_precision=self.config.matmul_precision,\n          parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n      )(\n          y\n      )  # We do not quantize the logits matmul.\n    if model_mode in (MODEL_MODE_PREFILL, MODEL_MODE_AUTOREGRESSIVE):\n      logits = nn.with_logical_constraint(logits, (None, None, \"activation_vocab\"))\n    elif cfg.num_vocab_tiling == 1:\n      logits = nn.with_logical_constraint(\n          logits, (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_vocab\")\n      )\n\n    if self.config.cast_logits_to_fp32:\n      logits = logits.astype(jnp.float32)\n\n    return logits\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      decoder_segment_ids=None,\n      deterministic=False,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      bidirectional_mask: None | Any = None,\n      image_embeddings: None | jnp.ndarray = None,\n      image_masks: None | jnp.ndarray = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    assert decoder_input_tokens.ndim == 2  # [batch, len]\n\n    # [batch, length] -> [batch, length, emb_dim]\n    y = self._apply_embedding(\n        shared_embedding,\n        decoder_input_tokens,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        image_embeddings,\n        bidirectional_mask,\n        image_masks,\n    )\n\n    policy = self.get_remat_policy()\n    RemattedBlockLayers = self.set_remat_policy(self.decoder_layer, policy)\n    # scan does not support kwargs in layer call, passing broadcast_args as positional arg\n    broadcast_args = (\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n    )\n    if cfg.using_pipeline_parallelism:\n      if cfg.pipeline_fsdp_ag_once:\n        partition_spec = self.pipeline_module.get_weight_sharding(\n            y, decoder_segment_ids, decoder_positions, deterministic, model_mode\n        )\n      else:\n        partition_spec = None  # This partition spec is only used for the fsdp_ag_once feature.\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n        dense_layer = RemattedBlockLayers[0]\n        moe_layer = RemattedBlockLayers[1]\n        num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n        num_moe_layers_outside_pp = num_moe_layers - self.config.pipeline_parallel_layers\n        logical_axis_rules_pp_as_dp = maxtext_utils.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n        # We chose not to pipeline the dense layers, only sparse for SPMD.\n        with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n          )(y, *broadcast_args)\n          if num_moe_layers_outside_pp > 0:\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                moe_layer,\n                num_moe_layers_outside_pp,\n                \"moe_layers\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n                model_mode=model_mode,\n            )(y, *broadcast_args)\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n      else:  # Not DeepSeek\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n        remaining_layers = self.config.num_decoder_layers - self.config.pipeline_parallel_layers\n        if remaining_layers > 0:\n          logical_axis_rules_pp_as_dp = maxtext_utils.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n          with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                RemattedBlockLayers[0],\n                remaining_layers,\n                \"layers_outside_pipeline\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n                model_mode=model_mode,\n            )(y, *broadcast_args)\n    else:\n      if cfg.scan_layers:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n          layer_call_kwargs = {\n              \"page_state\": page_state,\n              \"previous_chunk\": previous_chunk,\n              \"slot\": slot,\n          }\n          dense_layer = RemattedBlockLayers[0]\n          dense_layer.__call__ = functools.partial(dense_layer.__call__, **layer_call_kwargs)\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n          )(y, *broadcast_args)\n          moe_layer = RemattedBlockLayers[1]\n          moe_layer.__call__ = functools.partial(moe_layer.__call__, **layer_call_kwargs)\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              moe_layer,\n              num_moe_layers,\n              \"moe_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n          )(y, *broadcast_args)\n        elif cfg.decoder_block == DecoderBlockType.GEMMA3:\n          y = self._apply_gemma3_scanned_blocks(\n              y,\n              decoder_segment_ids,\n              decoder_positions,\n              deterministic,\n              model_mode,\n              bidirectional_mask,\n              previous_chunk,\n              page_state,\n              slot,\n          )\n        else:\n          RemattedBlockLayer = RemattedBlockLayers[0]\n          scan_length = int(cfg.num_decoder_layers / cfg.inhomogeneous_layer_cycle_interval)\n          layer_kwargs = {}\n          if cfg.decoder_block == DecoderBlockType.LLAMA4:\n            layer_kwargs = {\n                \"nope_layer_interval\": self.config.nope_layer_interval,\n                \"interleave_moe_layer_step\": self.config.interleave_moe_layer_step,\n            }\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              RemattedBlockLayer,\n              scan_length,\n              \"layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n              **layer_kwargs,\n          )(y, *broadcast_args)\n      else:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Unscanned layers must have a length of 2 using deepseek.\"\n          dense_layer = RemattedBlockLayers[0]\n          moe_layer = RemattedBlockLayers[1]\n\n          layers = [dense_layer, moe_layer]\n          layer_prefixes = [\"dense_layers\", \"moe_layers\"]\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          num_layers_list = [cfg.first_num_dense_layers, num_moe_layers]\n          # Iterate over the two layer groups (dense and MoE) and apply layer transformation\n          for layer, num_layers, layer_prefix in zip(layers, num_layers_list, layer_prefixes):\n            for index in range(num_layers):\n              y = layer(\n                  config=cfg, mesh=mesh, name=f\"{layer_prefix}_{index}\", quant=self.quant, model_mode=self.model_mode\n              )(\n                  y,\n                  decoder_segment_ids,\n                  decoder_positions,\n                  deterministic,\n                  model_mode,\n                  previous_chunk=previous_chunk,\n                  page_state=page_state,\n                  slot=slot,\n              )\n        else:\n          for lyr in range(cfg.num_decoder_layers):\n            RemattedBlockLayer = RemattedBlockLayers[0]\n            layer_kwargs = {}\n            layer_call_kwargs = {}\n            if cfg.decoder_block == DecoderBlockType.GEMMA3:\n              # Gemma3 uses both global and sliding window attention depending on the layer index.\n              layer_kwargs = {\"attention_type\": gemma3.get_attention_type(layer_id=lyr)}\n              layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n            if cfg.decoder_block == DecoderBlockType.LLAMA4:\n              layer_kwargs = {\n                  \"is_nope_layer\": llama4.determine_is_nope_layer(lyr, self.config.nope_layer_interval),\n                  \"is_moe_layer\": llama4.determine_is_moe_layer(lyr, self.config.interleave_moe_layer_step),\n              }\n            if cfg.decoder_block == DecoderBlockType.QWEN3_NEXT:\n              layer_kwargs = {\"layer_idx\": lyr}\n            if cfg.decoder_block == DecoderBlockType.GPT_OSS:\n              layer_kwargs = {\"attention_type\": gpt_oss.get_attention_type(layer_id=lyr)}\n            layer = RemattedBlockLayer(\n                config=cfg, mesh=mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=self.model_mode, **layer_kwargs\n            )\n            y = layer(\n                y,\n                decoder_segment_ids,\n                decoder_positions,\n                deterministic,\n                model_mode,\n                previous_chunk=previous_chunk,\n                page_state=page_state,\n                slot=slot,\n                **layer_call_kwargs,\n            )\n\n    assert isinstance(y, jax.Array)\n\n    # After the final transformer layer, `y` holds the raw, un-normalized hidden state.\n    hidden_state = y\n\n    # When vocab tiling is enabled in training mode, full logits won't generate to reduce memory\n    # Instead, we keep track on the hidden states, which has smaller size compared to full logits\n    if cfg.num_vocab_tiling > 1 and self.model_mode == MODEL_MODE_TRAIN:\n      logits = None\n      self.sow(\"intermediates\", \"hidden_states\", hidden_state)\n    else:\n      logits = self._apply_output_head(shared_embedding, hidden_state, deterministic, model_mode)\n\n    # The API of the Decoder is now a tuple, providing both the main output\n    # and the raw hidden state needed for auxiliary tasks.\n    return logits, hidden_state\n\n  def _apply_gemma3_scanned_blocks(\n      self,\n      y,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      bidirectional_mask,\n      previous_chunk,\n      page_state,\n      slot,\n  ):\n    \"\"\"Applies Gemma3 scanned decoder blocks, handling main scan and remainders.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n\n    # Define the repeating pattern length and calculate how many full blocks to scan\n    attention_pattern_length = len(gemma3.GEMMA3_ATTENTION_PATTERN)\n    scan_length = cfg.num_decoder_layers // attention_pattern_length\n\n    policy = self.get_remat_policy()\n    RemattedGemma3Block = self.set_remat_policy([gemma3.Gemma3ScannableBlockToLinen], policy)[0]\n\n    layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n    layer_kwargs = {\"num_of_layers\": attention_pattern_length}\n\n    # Apply the main scan over the full blocks\n    if scan_length > 0:\n      broadcast_args = (\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n      y, _ = self.scan_decoder_layers(\n          cfg,\n          RemattedGemma3Block,\n          scan_length,\n          \"layers\",\n          mesh,\n          in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          model_mode=self.model_mode,\n          **layer_kwargs,\n      )(y, *broadcast_args, **layer_call_kwargs)\n\n    # Apply any remaining layers that did not fit into a full scanned block\n    num_remaining_layers = cfg.num_decoder_layers % attention_pattern_length\n    if num_remaining_layers > 0:\n      # We name the remainder block with a 'remainder' suffix to avoid parameter name collisions\n      rem_layer_kwargs = {\"num_of_layers\": num_remaining_layers}\n      # pytype: disable=wrong-keyword-args\n      layer = RemattedGemma3Block(\n          config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode, name=\"layers_remainder\", **rem_layer_kwargs\n      )\n      y, _ = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          **layer_call_kwargs,\n      )\n    return y",
        "analysis": {
            "module_type": "transformer_decoder",
            "purpose": "A stack of decoder layers that processes input tokens, applies self-attention and MLP blocks, and generates logits for next token prediction.",
            "input": {
                "shape": "[batch_size, sequence_length]",
                "dtype": "int32"
            },
            "processing_steps": [
                "Calls `_apply_embedding` to convert input tokens to embeddings and add positional information.",
                "Determines and applies a rematerialization (gradient checkpointing) policy to the decoder layers.",
                "Processes the embeddings through a stack of decoder layers, with complex conditional logic for handling pipeline parallelism, scanned layers, and different model architectures (e.g., DeepSeek, Gemma3).",
                "The final hidden state from the transformer layers is captured.",
                "If not using vocabulary tiling in training mode, calls `_apply_output_head` to apply final normalization and project the hidden state to logits.",
                "If using vocabulary tiling in training mode, the hidden state is saved to an intermediate collection and logits are not computed."
            ],
            "output": {
                "shape": "A tuple of (logits, hidden_state). Logits shape: [batch_size, sequence_length, vocab_size] (or None). Hidden state shape: [batch_size, sequence_length, emb_dim]."
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "Config",
                "Quant",
                "pipeline.Pipeline",
                "DecoderLayer",
                "rms_norm",
                "attend_on_embedding",
                "linears.dense_general",
                "multimodal_utils"
            ],
            "parameters": {
                "config": "The main configuration object containing model hyperparameters like `emb_dim`, `num_decoder_layers`, `decoder_block`, `remat_policy`, `scan_layers`, `using_pipeline_parallelism`, etc.",
                "mesh": "The JAX device mesh for distributed computation.",
                "quant": "Optional quantization configuration object.",
                "model_mode": "Specifies the operational mode (e.g., 'train', 'prefill', 'autoregressive')."
            },
            "notes": [
                "The class handles various execution strategies, including standard layer-by-layer processing, scanned layers (`scan_layers`), and pipeline parallelism (`using_pipeline_parallelism`).",
                "It dynamically selects the decoder layer implementation based on `config.decoder_block`.",
                "It supports gradient checkpointing (rematerialization) with various policies defined in `config.remat_policy`.",
                "For multimodal models, it merges text and image embeddings.",
                "During training with vocabulary tiling, it may return `None` for logits and instead save the hidden states to an intermediate collection to save memory."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the decoder layer type, normalization layer, and pipeline module if pipeline parallelism is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `get_decoder_layers` to determine the decoder block type.",
                        "Calls `get_norm_layer` to get the final normalization layer.",
                        "If `config.using_pipeline_parallelism` is true, it calls `get_pipeline_stage_module` and `get_remat_policy` to initialize a `pipeline.Pipeline` module."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.get_decoder_layers",
                        "self.get_norm_layer",
                        "self.get_pipeline_stage_module",
                        "self.get_remat_policy",
                        "pipeline.Pipeline"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during module initialization."
                    ]
                },
                "minimal_policy": {
                    "purpose": "Creates a JAX checkpoint policy to save only a minimal set of specified activation names.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines a base list of tensor names to save (e.g., 'query_proj', 'out_proj').",
                        "Optionally adds 'context' to the list if `with_context` is True.",
                        "Calls `jax.checkpoint_policies.save_only_these_names` with the list."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jax.checkpoint_policies.save_only_these_names"
                    ],
                    "notes": [
                        "This is a helper method for `get_remat_policy`."
                    ]
                },
                "get_remat_policy": {
                    "purpose": "Determines and returns the appropriate JAX checkpointing (rematerialization) policy based on the `config.remat_policy` string.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reads `config.remat_policy`.",
                        "Uses a series of if/elif statements to select a policy based on the string value.",
                        "Constructs the policy using helpers like `self.minimal_policy` or functions from `jax.checkpoint_policies`."
                    ],
                    "output": {
                        "shape": "A JAX checkpoint policy object or None."
                    },
                    "dependencies": [
                        "jax.checkpoint_policies",
                        "self.minimal_policy"
                    ],
                    "notes": [
                        "Supports various pre-defined string keys for different checkpointing strategies, including offloading to host memory."
                    ]
                },
                "get_decoder_layers": {
                    "purpose": "Selects and returns the appropriate decoder layer class(es) based on `config.decoder_block`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Uses a `match-case` statement on `self.config.decoder_block`.",
                        "Returns a list of `nn.Module` classes corresponding to the specified block type (e.g., `DecoderLayer`, `llama2.LlamaDecoderLayerToLinen`)."
                    ],
                    "output": {
                        "shape": "A list containing one or more `nn.Module` classes."
                    },
                    "dependencies": [
                        "DecoderBlockType",
                        "DecoderLayer",
                        "llama2.LlamaDecoderLayerToLinen",
                        "mistral.MistralDecoderLayerToLinen",
                        "mixtral.MixtralDecoderLayerToLinen",
                        "deepseek.DeepSeekDenseLayer",
                        "gemma3.Gemma3DecoderLayerToLinen",
                        "etc."
                    ],
                    "notes": [
                        "For some block types like `DEEPSEEK`, it may return multiple layer classes (e.g., for dense and MoE layers)."
                    ]
                },
                "set_remat_policy": {
                    "purpose": "Applies a rematerialization policy to a list of decoder layer classes by wrapping them with `nn.remat`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through the input `block_layers`.",
                        "If `config.parameter_memory_host_offload` is enabled, wraps the layer with `nn.map_variables` to handle parameter movement.",
                        "Wraps each layer class with `nn.remat`, applying the provided `policy`."
                    ],
                    "output": {
                        "shape": "A list of rematerialized layer classes."
                    },
                    "dependencies": [
                        "flax.linen.remat",
                        "flax.linen.map_variables",
                        "maxtext_utils.should_prevent_cse_in_remat"
                    ],
                    "notes": [
                        "The `static_argnums` for `nn.remat` are hardcoded to `(4, 5)` corresponding to `deterministic` and `model_mode` arguments in the layer's `__call__`."
                    ]
                },
                "get_norm_layer": {
                    "purpose": "Selects and returns the appropriate normalization layer function based on `config.decoder_block`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks `self.config.decoder_block`.",
                        "Returns a `functools.partial` of either `rms_norm` or `gpt3.gpt3_layer_norm` with `num_features` bound."
                    ],
                    "output": {
                        "shape": "A partially applied function that acts as a normalization layer."
                    },
                    "dependencies": [
                        "functools.partial",
                        "rms_norm",
                        "gpt3.gpt3_layer_norm",
                        "DecoderBlockType"
                    ],
                    "notes": [
                        "Most decoder types default to `rms_norm`."
                    ]
                },
                "scan_decoder_layers": {
                    "purpose": "Wraps a decoder layer with `flax.linen.scan` to efficiently apply it multiple times over the layer dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Configures `variable_axes` for parameters, cache, etc., to be scanned over.",
                        "Configures `split_rngs` for params and dropout.",
                        "Calls `nn.scan` with the provided layer and configuration.",
                        "Calls the resulting scanned function with the provided arguments."
                    ],
                    "output": {
                        "shape": "The output of the scanned function."
                    },
                    "dependencies": [
                        "flax.linen.scan",
                        "flax.linen.partitioning.ScanIn"
                    ],
                    "notes": [
                        "This is a key component for enabling the `scan_layers=True` optimization."
                    ]
                },
                "get_pipeline_stage_module": {
                    "purpose": "Constructs the module for a single stage of a pipeline-parallel execution.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Selects the base layer for the pipeline (with special handling for `DEEPSEEK`).",
                        "Optionally applies a remat policy to the base layer.",
                        "If `num_layers_per_pipeline_stage` is 1, instantiates the base layer.",
                        "If `scan_layers_per_stage` is true, uses `scan_decoder_layers` to create a scanned module.",
                        "Otherwise, wraps the layer in `SequentialBlockDecoderLayers`."
                    ],
                    "output": {
                        "shape": "An `nn.Module` instance representing a single pipeline stage."
                    },
                    "dependencies": [
                        "self.get_remat_policy",
                        "self.set_remat_policy",
                        "self.scan_decoder_layers",
                        "SequentialBlockDecoderLayers"
                    ],
                    "notes": [
                        "This method encapsulates the logic for how layers are grouped within a single pipeline stage."
                    ]
                },
                "_apply_embedding": {
                    "purpose": "Applies token and positional embeddings to the input token IDs.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length]",
                        "dtype": "int32"
                    },
                    "processing_steps": [
                        "Looks up token embeddings using `shared_embedding`.",
                        "If multimodal, merges image embeddings using `multimodal_utils.merge_mm_embeddings`.",
                        "Applies dropout.",
                        "Optionally adds untrainable sinusoidal positional embeddings.",
                        "Optionally adds trainable positional embeddings."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "nn.Dropout",
                        "multimodal_utils.merge_mm_embeddings",
                        "positional_embedding_as_linen",
                        "embed_as_linen"
                    ],
                    "notes": [
                        "This is a helper method for `__call__`."
                    ]
                },
                "_apply_output_head": {
                    "purpose": "Applies final normalization and projects hidden states to vocabulary logits.",
                    "input": {
                        "shape": "y: [batch_size, sequence_length, emb_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the final normalization layer (`self.get_norm_layer`).",
                        "Applies dropout.",
                        "If `config.logits_via_embedding` is true, computes logits by dot product with the embedding table.",
                        "Otherwise, uses a final dense layer to project to `vocab_size`.",
                        "Optionally applies soft capping and normalization to logits.",
                        "Casts logits to float32 if configured."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "self.get_norm_layer",
                        "nn.Dropout",
                        "attend_on_embedding",
                        "linears.dense_general"
                    ],
                    "notes": [
                        "This is a helper method for `__call__`."
                    ]
                },
                "__call__": {
                    "purpose": "The main forward pass of the decoder stack, processing embeddings through layers to produce logits.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length]",
                        "dtype": "int32"
                    },
                    "processing_steps": [
                        "Calls `_apply_embedding` to get initial embeddings.",
                        "Gets and applies the rematerialization policy to the decoder layers.",
                        "Processes the embeddings through the stack of decoder layers, handling logic for pipeline parallelism, scanned layers, and different model architectures.",
                        "If not using vocab tiling in training, calls `_apply_output_head` to compute logits.",
                        "If using vocab tiling in training, saves the final hidden state and returns `None` for logits."
                    ],
                    "output": {
                        "shape": "A tuple of (logits, hidden_state). Logits shape: [batch_size, sequence_length, vocab_size] (or None). Hidden state shape: [batch_size, sequence_length, emb_dim]."
                    },
                    "dependencies": [
                        "self._apply_embedding",
                        "self.get_remat_policy",
                        "self.set_remat_policy",
                        "self.scan_decoder_layers",
                        "self.pipeline_module",
                        "self._apply_output_head",
                        "self._apply_gemma3_scanned_blocks"
                    ],
                    "notes": [
                        "The core logic is complex, with many conditional paths based on the configuration for parallelism (`using_pipeline_parallelism`), layer stacking (`scan_layers`), and model architecture (`decoder_block`)."
                    ]
                },
                "_apply_gemma3_scanned_blocks": {
                    "purpose": "Applies Gemma3 scanned decoder blocks, handling both the main repeating pattern and any remaining layers.",
                    "input": {
                        "shape": "y: [batch_size, sequence_length, emb_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculates the number of full repeating blocks (`scan_length`) based on `gemma3.GEMMA3_ATTENTION_PATTERN`.",
                        "Applies remat policy to `gemma3.Gemma3ScannableBlockToLinen`.",
                        "If `scan_length > 0`, calls `self.scan_decoder_layers` to process the main blocks.",
                        "If there are remaining layers, instantiates and calls a `Gemma3ScannableBlockToLinen` for the remainder."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "gemma3.Gemma3ScannableBlockToLinen",
                        "self.get_remat_policy",
                        "self.set_remat_policy",
                        "self.scan_decoder_layers"
                    ],
                    "notes": [
                        "This is a specialized helper method for the `__call__` method when `config.decoder_block` is `GEMMA3` and `scan_layers` is true."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#self_attention_with_norm",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def self_attention_with_norm(\n    inputs,\n    cfg,\n    mesh,\n    quant,\n    decoder_segment_ids,\n    decoder_positions,\n    deterministic,\n    model_mode,\n    previous_chunk=None,\n    page_state: None | page_manager.PageState = None,\n    slot: None | int = None,\n):\n  \"\"\"self-attention with normalization\"\"\"\n  # Normalization\n  lnx_rms = rms_norm(\n      num_features=inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"pre_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )\n  lnx = lnx_rms(inputs)\n  if model_mode == MODEL_MODE_PREFILL:\n    logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n  else:\n    logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n  attention_layer = attention_mla.mla_as_linen(\n      config=cfg,\n      num_query_heads=cfg.num_query_heads,\n      num_kv_heads=cfg.num_kv_heads,\n      head_dim=cfg.head_dim,\n      max_target_length=cfg.max_target_length,\n      max_prefill_predict_length=cfg.max_prefill_predict_length,\n      attention_kernel=cfg.attention,\n      attention_type=cfg.attention_type,\n      inputs_q_shape=lnx.shape,\n      inputs_kv_shape=lnx.shape,\n      mesh=mesh,\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      dropout_rate=cfg.dropout_rate,\n      name=\"self_attention\",\n      quant=quant,\n      kv_quant=quantizations.configure_kv_quant(cfg),\n      q_lora_rank=cfg.q_lora_rank,\n      kv_lora_rank=cfg.kv_lora_rank,\n      qk_nope_head_dim=cfg.qk_nope_head_dim,\n      qk_rope_head_dim=cfg.qk_rope_head_dim,\n      v_head_dim=cfg.v_head_dim,\n      max_position_embeddings=cfg.max_position_embeddings,\n      original_max_position_embeddings=cfg.original_max_position_embeddings,\n      mscale=cfg.mscale,\n      rope_factor=cfg.rope_factor,\n      model_mode=model_mode,\n  )\n\n  attention_lnx = attention_layer(\n      lnx,\n      lnx,\n      decoder_positions,\n      decoder_segment_ids=decoder_segment_ids,\n      deterministic=deterministic,\n      model_mode=model_mode,\n      previous_chunk=previous_chunk,\n      page_state=page_state,\n      slot=slot,\n  )\n\n  attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n  intermediate_inputs = inputs + attention_lnx\n\n  # Normalization\n  hidden_states = rms_norm(\n      num_features=intermediate_inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"post_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )(intermediate_inputs)\n  hidden_states = nn.with_logical_constraint(hidden_states, logical_axis_names)\n  return hidden_states, intermediate_inputs",
        "analysis": {
            "module_type": "self_attention_with_normalization",
            "purpose": "Performs a self-attention operation preceded by a normalization layer, followed by a residual connection and another normalization layer.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Inferred from cfg.dtype"
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input tensor.",
                "Apply a logical axis constraint to the normalized tensor based on the model_mode.",
                "Initialize and apply a multi-head latent attention (MLA) layer using the normalized tensor as query, key, and value.",
                "Add the attention output to the original input tensor in a residual connection, creating `intermediate_inputs`.",
                "Apply post-attention RMS normalization to `intermediate_inputs` to produce `hidden_states`.",
                "Apply a logical axis constraint to the final `hidden_states`."
            ],
            "output": {
                "shape": "A tuple of two tensors: (hidden_states, intermediate_inputs), both with shape [batch_size, sequence_length, hidden_dim]."
            },
            "dependencies": [
                "rms_norm",
                "attention_mla.mla_as_linen",
                "nn.with_logical_constraint",
                "page_manager.PageState"
            ],
            "parameters": {
                "cfg": "A configuration object containing model hyperparameters like dtype, head dimensions, dropout rate, and normalization epsilon.",
                "model_mode": "A string indicating the operational mode (e.g., 'prefill'), which affects logical axis naming for sharding.",
                "deterministic": "A boolean flag to control dropout behavior.",
                "previous_chunk": "Optional tensor for stateful decoding in chunked attention.",
                "page_state": "Optional PageState object for managing paged attention state.",
                "slot": "Optional integer slot for paged attention."
            },
            "notes": [
                "The function returns two tensors: the final normalized output (`hidden_states`) and the intermediate result after the residual connection but before the final normalization (`intermediate_inputs`).",
                "This module implements a Pre-LN (Layer Normalization) transformer block architecture.",
                "The specific attention implementation used is Multi-Head Latent Attention (MLA)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#post_process",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def post_process(cfg, layer_output, sow):\n  \"\"\"postprocessing.\"\"\"\n  if cfg.record_internal_nn_metrics:\n    sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n    sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n    sow(\n        \"intermediates\",\n        \"activation_fraction_zero\",\n        jnp.sum(layer_output == 0) / jnp.size(layer_output),\n    )\n\n  if cfg.scan_layers:\n    return layer_output, None\n  else:\n    return layer_output",
        "analysis": {
            "functionality": "This function performs post-processing on a layer's output. It conditionally records activation statistics (mean, standard deviation, fraction of zeros) if configured to do so. It also formats the return value to be compatible with Flax's `scan` operation by returning a tuple if `cfg.scan_layers` is true.",
            "usage": "Call this function at the end of a transformer layer. Pass the configuration object `cfg`, the layer's output tensor `layer_output`, and a `sow` callable for recording metrics. The function returns either the `layer_output` tensor directly or a tuple `(layer_output, None)` depending on the `cfg.scan_layers` setting."
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekDenseLayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekDenseLayer(nn.Module):\n  \"\"\"DeepSeek-style dense layer with Multi-Head Latent Attention.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n    mlp_lnx = linears.mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "module_type": "deepseek_dense_layer",
            "purpose": "Implements a single decoder layer for a DeepSeek-style transformer model, consisting of a self-attention block followed by a dense MLP block with residual connections.",
            "input": {
                "shape": "N/A (Handled by the '__call__' method)",
                "dtype": "N/A (Handled by the '__call__' method)"
            },
            "processing_steps": [
                "The '__call__' method executes the forward pass of the layer."
            ],
            "output": {
                "shape": "N/A (Handled by the '__call__' method)"
            },
            "dependencies": [
                "flax.linen.Module",
                "self_attention_with_norm",
                "MaxText.layers.linears.mlp_block",
                "post_process",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like mlp_dim, dropout_rate, dtype, etc.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill' or 'decode'.",
                "quant": "An optional configuration object for quantization."
            },
            "notes": [
                "This class represents a standard transformer decoder layer with a dense feed-forward network, as opposed to a Mixture-of-Experts (MoE) layer.",
                "It uses a pre-normalization architecture (LayerNorm before attention and MLP).",
                "The implementation includes logical constraints for tensor sharding and gradient checkpointing for memory efficiency."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the DeepSeek dense decoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "Depends on config.dtype (e.g., float32, bfloat16)"
                    },
                    "processing_steps": [
                        "Apply logical constraints and a checkpoint name to the input tensor.",
                        "Call `self_attention_with_norm` which performs layer normalization, self-attention, and the first residual connection.",
                        "Pass the result through an `mlp_block` (the feed-forward network).",
                        "Add the MLP output to the intermediate input from the attention block (the second residual connection).",
                        "Apply dropout to the result.",
                        "Apply a final logical constraint.",
                        "Return the output after passing it through the `post_process` function."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "linears.mlp_block",
                        "post_process",
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint",
                        "flax.linen.Dropout"
                    ],
                    "notes": [
                        "The method's behavior, specifically tensor axis naming for sharding, differs based on the `model_mode` parameter.",
                        "Accepts optional arguments `previous_chunk`, `page_state`, and `slot` to support inference techniques like chunked processing and paged attention."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekMoELayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekMoELayer(nn.Module):\n  \"\"\"DeepSeek-style MoE layer with Multi-Head Latent Attention.\n  Supports dropless and dropping base on configs.\n  Uses a bias in routing instead of load balancing loss.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        self.config,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx = moe.get_routed_and_shared_moe(\n        name=\"DeepSeekMoeBlock_0\",\n        config=cfg,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "module_type": "deepseek_moe_layer",
            "purpose": "Implements a DeepSeek-style transformer layer that combines Multi-Head Latent Attention with a Mixture-of-Experts (MoE) block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nn.Module",
                "self_attention_with_norm",
                "moe.get_routed_and_shared_moe",
                "post_process",
                "initializers.nd_dense_init"
            ],
            "parameters": {
                "config": "Configuration object (Config) containing model hyperparameters like dropout_rate, dtype, etc.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill'.",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This layer supports both dropless and dropping MoE routing based on the configuration.",
                "It uses a bias in the MoE router instead of a load balancing loss.",
                "The MoE block is explicitly named 'DeepSeekMoeBlock_0' to ensure reverse compatibility with existing checkpoints."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the layer, applying self-attention and then a Mixture-of-Experts block with residual connections.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "cfg.dtype"
                    },
                    "processing_steps": [
                        "Apply logical constraints and a checkpoint name to the input tensor.",
                        "Call `self_attention_with_norm` to perform self-attention and normalization, producing `hidden_states` and an intermediate residual `intermediate_inputs`.",
                        "Pass `hidden_states` through the `moe.get_routed_and_shared_moe` block to get the MLP output.",
                        "Add the MoE output to the intermediate residual connection (`intermediate_inputs`).",
                        "Apply dropout to the result.",
                        "Apply final logical constraints.",
                        "Call `post_process` on the final output tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "self_attention_with_norm",
                        "moe.get_routed_and_shared_moe",
                        "nn.Dropout",
                        "post_process"
                    ],
                    "notes": [
                        "The logical axis names for sharding constraints are determined by the `model_mode` (prefill vs. decode)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek_batchsplit.py#DeepSeekGenericLayer",
        "file_path": "src/MaxText/layers/deepseek_batchsplit.py",
        "code_block": "class DeepSeekGenericLayer(nn.Module):\n  \"\"\"Generic DeepSeek layer with Multi-Head Latent Attention.\n\n  This is to be used as a base class for DeepSeek layers with dense/sparse MLPs.\n\n  This class follows a pattern of separating module creation from execution.\n  `*_layer()` methods (e.g., `attention_layer`) are factories for `nn.Module`s,\n  called in `setup()` to initialize sub-layers. The module instances are stored\n  in `*_op` attributes (e.g., `self.attention_op`). The corresponding methods\n  (e.g., `attention`) are called during execution in `__call__` and wrap the\n  `*_op` modules with logic like logical constraints. This keeps `__call__`\n  clean and readable.\n  \"\"\"\n\n  config: common_types.Config\n  mesh: jax.sharding.Mesh\n  model_mode: str\n  quant: None | quantizations.AqtQuantization = None\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    x = self.with_logical_constraint(inputs)\n    x = jax.ad_checkpoint.checkpoint_name(x, \"decoder_layer_input\")\n\n    x += self.attention(\n        self.pre_attention_norm(x),\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n\n    x += self.mlp(self.post_attention_norm(x), deterministic)\n    x = self.dropout(x, deterministic)\n    return self.post_process(x)\n\n  def setup(self):\n    self.pre_attention_norm_op = self.rms_norm_layer(\"pre_attention_layer_norm\")\n    self.post_attention_norm_op = self.rms_norm_layer(\n        \"post_attention_layer_norm\"\n    )\n    self.attention_op = self.attention_layer()\n    self.mlp_op = self.mlp_layer()\n    self.dropout_op = self.dropout_layer()\n\n  @property\n  def logical_axis_names(self):\n    if self.model_mode == common_types.MODEL_MODE_PREFILL:\n      return (\n          \"activation_batch\",\n          \"prefill_activation_norm_length\",\n          \"activation_embed\",\n      )\n    else:\n      return (\n          \"activation_batch\",\n          \"activation_norm_length\",\n          \"activation_embed\",\n      )\n\n  def with_logical_constraint(self, x):\n    return nn.with_logical_constraint(x, self.logical_axis_names)\n\n  def rms_norm_layer(self, name):\n    return normalizations.rms_norm(\n        num_features=self.config.base_emb_dim,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        name=name,\n        kernel_axes=(\"norm\",),\n        epsilon=self.config.normalization_layer_epsilon,\n    )\n\n  def pre_attention_norm(self, x):\n    return self.with_logical_constraint(self.pre_attention_norm_op(x))\n\n  def post_attention_norm(self, x):\n    return self.with_logical_constraint(self.post_attention_norm_op(x))\n\n  def attention_layer(self):\n    inputs_shape = (\n        self.config.per_device_batch_size,\n        self.config.max_target_length,\n        self.config.base_emb_dim,\n    )\n    return attention_mla.mla_as_linen(\n        config=self.config,\n        num_query_heads=self.config.num_query_heads,\n        num_kv_heads=self.config.num_kv_heads,\n        head_dim=self.config.head_dim,\n        max_target_length=self.config.max_target_length,\n        max_prefill_predict_length=self.config.max_prefill_predict_length,\n        attention_kernel=self.config.attention,\n        attention_type=self.config.attention_type,\n        inputs_q_shape=inputs_shape,\n        inputs_kv_shape=inputs_shape,\n        mesh=self.mesh,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        dropout_rate=self.config.dropout_rate,\n        name=\"self_attention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(self.config),\n        q_lora_rank=self.config.q_lora_rank,\n        kv_lora_rank=self.config.kv_lora_rank,\n        qk_nope_head_dim=self.config.qk_nope_head_dim,\n        qk_rope_head_dim=self.config.qk_rope_head_dim,\n        v_head_dim=self.config.v_head_dim,\n        max_position_embeddings=self.config.max_position_embeddings,\n        original_max_position_embeddings=self.config.original_max_position_embeddings,\n        mscale=self.config.mscale,\n        rope_factor=self.config.rope_factor,\n        model_mode=self.model_mode,\n    )\n\n  def attention(\n      self,\n      x,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    \"\"\"Executes the attention layer.\"\"\"\n    return self.with_logical_constraint(\n        self.attention_op(\n            x,\n            x,\n            decoder_positions,\n            decoder_segment_ids=decoder_segment_ids,\n            deterministic=deterministic,\n            model_mode=self.model_mode,\n            previous_chunk=previous_chunk,\n            page_state=page_state,\n            slot=slot,\n        )\n    )\n\n  def mlp_layer(self):\n    raise NotImplementedError()\n\n  def mlp(self, x, deterministic):\n    raise NotImplementedError()\n\n  def dropout_layer(self):\n    return nn.Dropout(rate=self.config.dropout_rate, broadcast_dims=(-2,))\n\n  def dropout(self, x, deterministic):\n    return self.with_logical_constraint(\n        self.dropout_op(x, deterministic=deterministic)\n    )\n\n  def post_process(self, x):\n    \"\"\"Collect statistics about the output of the layer.\"\"\"\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(x))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(x))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(x == 0) / jnp.size(x),\n      )\n\n    if self.config.scan_layers:\n      return x, None\n    else:\n      return x",
        "analysis": {
            "functionality": "The `DeepSeekGenericLayer` class is a base class for a single layer in a DeepSeek transformer model. It implements the standard transformer block architecture: a multi-head attention mechanism followed by a feed-forward network (MLP), with residual connections, layer normalization, and dropout. The specific MLP implementation is left to be defined by subclasses.",
            "usage": "This class is not meant to be instantiated directly. It should be subclassed to provide a concrete implementation for the `mlp_layer` and `mlp` methods, as seen in `DeepSeekDenseLayer` and `DeepSeekMoELayer`. The subclass would then be used as a building block within a larger transformer model. The `__call__` method processes an input tensor through one layer of transformer computation.\n\nInputs to `__call__`:\n- `inputs`: The primary input tensor of shape `[batch_size, sequence_length, embedding_dim]`.\n- `decoder_segment_ids`: Tensor for identifying different segments in the input sequence.\n- `decoder_positions`: Tensor indicating the position of each token.\n- `deterministic`: A boolean flag to control dropout.\n- `model_mode`: A string indicating the operational mode (e.g., 'prefill', 'decode').\n\nOutput of `__call__`:\n- A tensor of the same shape as the input, `[batch_size, sequence_length, embedding_dim]`, representing the output of the transformer layer."
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek_batchsplit.py#DeepSeekDenseLayer",
        "file_path": "src/MaxText/layers/deepseek_batchsplit.py",
        "code_block": "class DeepSeekDenseLayer(DeepSeekGenericLayer):\n  \"\"\"DeepSeek layer with dense MLP.\"\"\"\n\n  def mlp_layer(self):\n    return linears.mlp_block(\n        in_features=self.config.base_emb_dim,\n        intermediate_dim=self.config.mlp_dim,\n        activations=self.config.mlp_activations,\n        intermediate_dropout_rate=self.config.dropout_rate,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        name=\"mlp\",\n        config=self.config,\n        quant=self.quant,\n    )\n\n  def mlp(self, x, deterministic):\n    return self.with_logical_constraint(self.mlp_op(x, deterministic))",
        "analysis": {
            "module_type": "deepseek_dense_layer",
            "purpose": "Implements a DeepSeek transformer layer that uses a standard dense Multi-Layer Perceptron (MLP) block for the feed-forward network.",
            "input": {
                "shape": "[batch_size, sequence_length, base_emb_dim]",
                "dtype": "Depends on config.dtype (e.g., float32, bfloat16)."
            },
            "processing_steps": [
                "Applies pre-attention RMS normalization to the input.",
                "Computes self-attention using the Multi-Head Latent Attention mechanism.",
                "Adds the attention output to the input via a residual connection.",
                "Applies post-attention RMS normalization.",
                "Processes the result through a dense MLP block.",
                "Adds the MLP output to the result via a second residual connection.",
                "Applies dropout."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, base_emb_dim]"
            },
            "dependencies": [
                "DeepSeekGenericLayer",
                "linears.mlp_block"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters like base_emb_dim, mlp_dim, mlp_activations, dropout_rate, and dtypes.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "A string indicating the model's operational mode (e.g., 'prefill', 'autoregress').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This class inherits its main structure and attention mechanism from `DeepSeekGenericLayer` and specializes it by providing a dense MLP implementation.",
                "The main forward pass logic is defined in the `__call__` method of the parent `DeepSeekGenericLayer` class."
            ],
            "methods": {
                "mlp_layer": {
                    "purpose": "Creates and returns a dense MLP block module during layer initialization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `linears.mlp_block` to instantiate the MLP module using parameters from the configuration object."
                    ],
                    "output": {
                        "shape": "Returns a Flax `nn.Module` instance for the MLP block."
                    },
                    "dependencies": [
                        "linears.mlp_block"
                    ],
                    "notes": [
                        "This method is called by the `setup()` method of the parent class to initialize the `self.mlp_op` attribute."
                    ]
                },
                "mlp": {
                    "purpose": "Applies the initialized dense MLP block to an input tensor during the forward pass.",
                    "input": {
                        "shape": "[batch_size, sequence_length, base_emb_dim]",
                        "dtype": "Depends on config.dtype."
                    },
                    "processing_steps": [
                        "Calls the initialized MLP module (`self.mlp_op`) with the input tensor `x`.",
                        "Applies a logical constraint for tensor sharding via `self.with_logical_constraint`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, base_emb_dim]"
                    },
                    "dependencies": [
                        "self.mlp_op",
                        "self.with_logical_constraint"
                    ],
                    "notes": [
                        "This method overrides the abstract `mlp` method in the parent `DeepSeekGenericLayer`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek_batchsplit.py#DeepSeekMoELayer",
        "file_path": "src/MaxText/layers/deepseek_batchsplit.py",
        "code_block": "class DeepSeekMoELayer(DeepSeekGenericLayer):\n  \"\"\"DeepSeek MoE layer that uses a batch-split schedule.\"\"\"\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n      split_factor: int = 2,\n  ):\n    x = self.with_logical_constraint(inputs)\n    x = jax.ad_checkpoint.checkpoint_name(x, \"decoder_layer_input\")\n\n    # Helper functions.\n    def _split(x):\n      if x is None:\n        return [None] * split_factor\n      else:\n        return jnp.split(x, split_factor, axis=0)\n\n    def _merge(x):\n      return jnp.concatenate(x, axis=0)\n\n    def _attn(x, decoder_segment_ids, decoder_positions):\n      return self.attention(\n          self.pre_attention_norm(x),\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          previous_chunk,\n          page_state,\n          slot,\n      )\n\n    def _moe(x):\n      return self.mlp(self.post_attention_norm(x), deterministic)\n\n    # Split the inputs into micro-batches.\n    x = _split(x)\n    dpos = _split(decoder_positions)\n    dseg = _split(decoder_segment_ids)\n\n    # Attention.\n    x = [xi + _attn(xi, yi, zi) for xi, yi, zi in zip(x, dseg, dpos)]\n\n    # Mixture-of-experts.\n    x = [xi + _moe(xi) for xi in x]\n\n    # Merge the micro-batches back into a single batch.\n    x = _merge(x)\n\n    x = self.dropout(x, deterministic)\n    return self.post_process(x)\n\n  def init(self, *args, **kwargs):\n    # Calls the parent init method for testing parity.\n    return super().init(*args, **kwargs, method=super().__call__)\n\n  def mlp_layer(self):\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with\n    # existing checkpoints. The `name` represents the weight name in\n    # JAX/checkpoints and so the class name is just for readability.\n    return moe.get_routed_and_shared_moe(\n        name=\"DeepSeekMoeBlock_0\",\n        config=self.config,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(\n            1.0, \"fan_in\", \"truncated_normal\"\n        ),\n        kernel_axes=(\"embed\", None),\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        quant=self.quant,\n    )\n\n  def mlp(self, x, _):\n    return self.with_logical_constraint(self.mlp_op(x))",
        "analysis": {
            "module_type": "deepseek_moe_layer",
            "purpose": "Implements a DeepSeek Mixture-of-Experts (MoE) transformer layer that processes inputs by splitting them into micro-batches.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "float32"
            },
            "processing_steps": [
                "The `__call__` method splits the input tensors (inputs, segment_ids, positions) into micro-batches along the batch axis.",
                "For each micro-batch, it applies self-attention with a residual connection.",
                "For each resulting micro-batch, it applies the Mixture-of-Experts (MoE) MLP with a residual connection.",
                "The processed micro-batches are concatenated back into a single tensor.",
                "A final dropout and post-processing step are applied."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "DeepSeekGenericLayer",
                "moe.get_routed_and_shared_moe",
                "jax.numpy",
                "jax.ad_checkpoint"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dimensions, dtypes, and MoE settings.",
                "mesh": "The JAX mesh for device placement and sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'prefill', 'autoregress').",
                "quant": "Optional quantization configuration.",
                "split_factor": "An integer parameter to the `__call__` method that determines the number of micro-batches to split the input into."
            },
            "notes": [
                "This class inherits from `DeepSeekGenericLayer` and overrides the `__call__`, `mlp_layer`, and `mlp` methods.",
                "The batch-split schedule is the key feature, designed to manage memory or computation by processing data in smaller chunks.",
                "The `init` method is overridden specifically for testing parity with the parent class."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes the input tensor through the MoE layer using a batch-split execution schedule.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "Inputs are typically float32, while ids and positions are integers."
                    },
                    "processing_steps": [
                        "Apply logical constraints and name the input for gradient checkpointing.",
                        "Define and use local helper functions `_split`, `_merge`, `_attn`, and `_moe`.",
                        "Split `inputs`, `decoder_positions`, and `decoder_segment_ids` into `split_factor` micro-batches.",
                        "Iterate through micro-batches, applying attention (`_attn`) with a residual connection.",
                        "Iterate through micro-batches again, applying the MoE MLP (`_moe`) with a residual connection.",
                        "Merge the processed micro-batches back into a single tensor.",
                        "Apply dropout and post-processing."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self.attention",
                        "self.mlp",
                        "self.dropout",
                        "self.post_process",
                        "jnp.split",
                        "jnp.concatenate"
                    ],
                    "notes": [
                        "This method overrides the parent's `__call__` to implement the batch-splitting logic."
                    ]
                },
                "init": {
                    "purpose": "Initializes the layer's parameters for testing parity by calling the parent class's `init` method with the parent's `__call__`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `super().init` with `method=super().__call__`."
                    ],
                    "output": {
                        "shape": "Initialized parameters for the module."
                    },
                    "dependencies": [
                        "DeepSeekGenericLayer.init",
                        "DeepSeekGenericLayer.__call__"
                    ],
                    "notes": [
                        "This method is specifically for ensuring testing parity with the parent class implementation."
                    ]
                },
                "mlp_layer": {
                    "purpose": "Creates and returns the Mixture-of-Experts (MoE) MLP module for this layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `moe.get_routed_and_shared_moe` with parameters from the config to instantiate the MoE layer."
                    ],
                    "output": {
                        "shape": "An nn.Module instance representing the MoE block."
                    },
                    "dependencies": [
                        "moe.get_routed_and_shared_moe",
                        "initializers.nd_dense_init"
                    ],
                    "notes": [
                        "The name 'DeepSeekMoeBlock_0' is hardcoded for reverse compatibility with existing checkpoints."
                    ]
                },
                "mlp": {
                    "purpose": "Applies the instantiated MoE MLP block (`self.mlp_op`) to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Calls `self.mlp_op` on the input `x`.",
                        "Wraps the result with `self.with_logical_constraint`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self.mlp_op",
                        "self.with_logical_constraint"
                    ],
                    "notes": [
                        "The second argument `_` (deterministic) is ignored, as MoE routing logic handles determinism internally."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#_maybe_move_embedding_to_device",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def _maybe_move_embedding_to_device(embedding_table: Array, config: Config) -> Array:\n  \"\"\"Moves embedding table to device if parameter offloading is enabled.\"\"\"\n  if config.parameter_memory_host_offload:\n    max_logging.log(\"embeddings.py: Moving embedding parameter to device\")\n    return jax.device_put(embedding_table, max_utils.device_space())\n  return embedding_table",
        "analysis": {
            "functionality": "Conditionally moves an embedding table (JAX Array) to the accelerator device based on the `config.parameter_memory_host_offload` setting.",
            "usage": "Call this function with an `embedding_table` (JAX Array) and a `config` object. It returns the `embedding_table`, which will be located on the accelerator device if parameter offloading is enabled, otherwise it returns the original array. This is typically used just before the embedding table is needed for computation to manage memory."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#embed_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def embed_as_linen(\n    *,\n    num_embeddings: int,\n    num_features: int,\n    config: Config,\n    cast_input_dtype: None | DType = None,\n    dtype: DType = jnp.float32,\n    attend_dtype: None | DType = None,\n    embedding_init: Initializer = default_embed_init,\n    name: str | None = None,\n):\n  \"\"\"Initializes the Embed NNX module and returns it as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Embed` module within\n  a Linen model. It wraps the `Embed` module using `nnx.bridge.to_linen`,\n  making it compatible with the Linen API.\n\n  Args:\n    num_embeddings: The number of embeddings.\n    num_features: The number of feature dimensions for each embedding.\n    config: The model configuration.\n    cast_input_dtype: The dtype to cast the input to, if any.\n    dtype: The dtype of the embedding vectors.\n    attend_dtype: The dtype for the `attend` method.\n    embedding_init: The initializer for the embedding matrix.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `Embed` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Embed,\n      num_embeddings=num_embeddings,\n      num_features=num_features,\n      config=config,\n      cast_input_dtype=cast_input_dtype,\n      dtype=dtype,\n      attend_dtype=attend_dtype,\n      embedding_init=embedding_init,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "functionality": "This function serves as a factory and wrapper. It initializes an NNX `Embed` module with the provided configuration and then wraps it using `nnx_wrappers.to_linen` to make it compatible with the Flax Linen API. This allows an NNX-defined embedding layer to be used seamlessly within a Linen model.",
            "usage": "Call this function with embedding parameters like `num_embeddings` and `num_features` to get a Linen-compatible module. The returned module can then be used in a Linen model's `setup` or `__call__` method. When the returned module is called with an input tensor of integer token IDs, it will produce a tensor of corresponding embedding vectors."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#Embed",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class Embed(nnx.Module):\n  \"\"\"A parameterized function from integers [0, n) to d-dimensional vectors.\"\"\"\n\n  def __init__(\n      self,\n      num_embeddings: int,\n      num_features: int,\n      config: Config,\n      cast_input_dtype: None | DType = None,\n      dtype: DType = jnp.float32,\n      attend_dtype: None | DType = None,\n      embedding_init: Initializer = default_embed_init,\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the Embed module.\n\n    Args:\n      num_embeddings: The number of embeddings.\n      num_features: The number of feature dimensions for each embedding.\n      config: The model configuration.\n      cast_input_dtype: The dtype to cast the input to, if any.\n      dtype: The dtype of the embedding vectors.\n      attend_dtype: The dtype for the `attend` method.\n      embedding_init: The initializer for the embedding matrix.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.num_embeddings = num_embeddings\n    self.num_features = num_features\n    self.config = config\n    self.cast_input_dtype = cast_input_dtype\n    self.dtype = dtype\n    self.attend_dtype = attend_dtype\n\n    self.embedding = nnx.Param(\n        embedding_init(rngs.params(), (self.num_embeddings, self.num_features), self.config.weight_dtype),\n        sharding=(\"vocab\", \"embed\"),\n    )\n\n  def __call__(self, inputs: Array, model_mode: str = MODEL_MODE_TRAIN) -> Array:\n    \"\"\"Embeds the inputs along the last dimension.\n\n    Args:\n      inputs: input data, all dimensions are considered batch dimensions.\n\n    Returns:\n      Output which is embedded input data.  The output shape follows the input,\n      with an additional `num_features` dimension appended.\n    \"\"\"\n    cfg = self.config\n    if self.cast_input_dtype:\n      inputs = inputs.astype(self.cast_input_dtype)\n    if not jnp.issubdtype(inputs.dtype, jnp.integer):\n      raise ValueError(\"Input type must be an integer or unsigned integer.\")\n\n    embedding = _maybe_move_embedding_to_device(self.embedding.value, self.config)\n\n    if cfg.use_iota_embed:\n      iota = lax.iota(jnp.int32, self.num_embeddings)\n      one_hot = jnp.array(inputs[..., jnp.newaxis] == iota, dtype=self.dtype)\n      output = jnp.dot(one_hot, jnp.asarray(embedding, self.dtype))\n    else:\n      output = jnp.asarray(embedding, self.dtype)[inputs]\n\n    output_prefill_axis_names = (\"activation_embed_and_logits_batch\", \"prefill_activation_length\", \"activation_embed\")\n    output_default_axis_names = (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_embed\")\n\n    if model_mode == MODEL_MODE_PREFILL:\n      output = nn.with_logical_constraint(output, output_prefill_axis_names)\n    else:\n      output = nn.with_logical_constraint(output, output_default_axis_names)\n    return output\n\n  def attend(self, query: Array) -> Array:\n    \"\"\"Attend over the embedding using a query array.\n\n    Args:\n      query: array with last dimension equal the feature depth `num_features` of the\n        embedding.\n\n    Returns:\n      An array with final dim `num_embeddings` corresponding to the batched\n      inner-product of the array of query vectors against each embedding.\n      Commonly used for weight-sharing between embeddings and logit transform\n      in NLP models.\n    \"\"\"\n    embedding = self.embedding.value\n    attend_dtype = self.attend_dtype if self.attend_dtype is not None else self.dtype\n    return attend_on_embedding(query, embedding, attend_dtype, self.config)",
        "analysis": {
            "module_type": "embedding_layer",
            "purpose": "A parameterized module that maps integer indices from a vocabulary to dense, d-dimensional vector representations.",
            "input": {
                "shape": "N/A (The module is a class, see the '__call__' method for its primary input.)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes an embedding matrix of shape [num_embeddings, num_features] as an `nnx.Param` with specified sharding."
            ],
            "output": {
                "shape": "N/A (The module is a class, see the '__call__' method for its primary output.)"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.Rngs",
                "Config",
                "Initializer",
                "_maybe_move_embedding_to_device",
                "attend_on_embedding"
            ],
            "parameters": {
                "num_embeddings": "The size of the vocabulary or the number of unique items to embed.",
                "num_features": "The dimensionality of the output embedding vectors.",
                "config": "The model configuration object, containing settings like `weight_dtype` and `use_iota_embed`.",
                "dtype": "The data type of the embedding vectors.",
                "attend_dtype": "The data type used for the `attend` method's computation.",
                "embedding_init": "The initializer function for the embedding matrix."
            },
            "notes": [
                "The embedding matrix is sharded with `('vocab', 'embed')`.",
                "The `rngs` parameter in `__init__` is a temporary requirement for compatibility with `nnx.bridge.to_linen` and is not used directly by the `Embed` module's logic."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Embed module, setting up the embedding matrix and configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration attributes like `num_embeddings`, `num_features`, and `dtype`.",
                        "Initialize the embedding matrix as an `nnx.Param` using the provided `embedding_init` function and `rngs`.",
                        "Assign sharding annotations to the embedding parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.Rngs",
                        "Initializer"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the embedding lookup, converting integer input tensors into dense vector representations.",
                    "input": {
                        "shape": "[..., sequence_length]",
                        "dtype": "integer"
                    },
                    "processing_steps": [
                        "Optionally cast the input tensor to `cast_input_dtype`.",
                        "Validate that the input tensor has an integer data type.",
                        "Optionally move the embedding table to the active device via `_maybe_move_embedding_to_device`.",
                        "Perform embedding lookup either by direct indexing or via a one-hot dot-product if `config.use_iota_embed` is true.",
                        "Apply logical constraints for sharding using `nn.with_logical_constraint` based on the `model_mode`."
                    ],
                    "output": {
                        "shape": "[..., sequence_length, num_features]"
                    },
                    "dependencies": [
                        "_maybe_move_embedding_to_device",
                        "jax.lax.iota",
                        "jnp.dot",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The method of embedding lookup (direct indexing vs. one-hot dot product) is determined by the `config.use_iota_embed` flag."
                    ]
                },
                "attend": {
                    "purpose": "Computes the dot product of a query tensor with the embedding matrix, typically used for generating logits from final hidden states in models with tied weights.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Retrieve the embedding table.",
                        "Call the `attend_on_embedding` function to compute the dot product between the query and the transposed embedding table."
                    ],
                    "output": {
                        "shape": "[..., num_embeddings]"
                    },
                    "dependencies": [
                        "attend_on_embedding"
                    ],
                    "notes": [
                        "This method facilitates weight sharing between the input embedding layer and the final output projection layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#attend_on_embedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def attend_on_embedding(query: Array, embedding_table: Array, attend_dtype: DType, config: Config) -> Array:\n  \"\"\"Attend over an embedding table using a query array.\n\n  TODO: Remove this method when Embed bridge to Linen is no longer needed\n\n  Args:\n    query: An array with a last dimension equal to the feature depth of the embedding.\n    embedding_table: The embedding table to attend over.\n    attend_dtype: The data type for the attention computation.\n    config: The model configuration, used to check for parameter offloading.\n\n  Returns:\n    An array with a final dimension equal to `num_embeddings`, corresponding to the\n    batched inner-product of the query vectors against each embedding.\n  \"\"\"\n  embedding_table = _maybe_move_embedding_to_device(embedding_table, config)\n  return jnp.dot(query, jnp.asarray(embedding_table, jnp.bfloat16).T, preferred_element_type=attend_dtype)",
        "analysis": {
            "functionality": "Calculates the batched inner-product (dot product) between a query tensor and an embedding table. This is commonly used for computing output logits in models with shared input/output embeddings.",
            "usage": "Call this function with a query tensor, an embedding table, a desired computation data type, and a configuration object. The query's last dimension must match the embedding table's feature dimension. The function returns a tensor where the last dimension is the size of the embedding vocabulary, representing the scores for each embedding vector."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the RotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      RotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "rotary_embedding_factory",
            "purpose": "Initializes the NNX `RotaryEmbedding` module and wraps it to be used as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RotaryEmbedding` NNX module into a Linen-compatible module.",
                "Forwards all specified parameters (`min_timescale`, `max_timescale`, `embedding_dims`, etc.) to the `RotaryEmbedding` constructor."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, which determines the periodicity of the added signal.",
                "max_timescale": "End of the geometric index, which determines the frequency of the added signal.",
                "embedding_dims": "Dimension of the embedding to be generated.",
                "cast_as_fprop_dtype": "Whether to cast the output of the created module to the `fprop_dtype`.",
                "fprop_dtype": "The data type for the forward pass of the created module."
            },
            "notes": [
                "This function acts as a bridge, allowing an NNX-defined module (`RotaryEmbedding`) to be seamlessly integrated into a Flax Linen model.",
                "The returned object is a Linen module, not a tensor."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#RotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class RotaryEmbedding(nnx.Module):\n  \"\"\"Rotary Position Embedding.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      # Not used in RotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rope_linear_scaling_factor: float = 1.0,\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the RotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    self.min_timescale = min_timescale\n    self.max_timescale = max_timescale\n    self.embedding_dims = embedding_dims\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.rope_linear_scaling_factor = rope_linear_scaling_factor\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def timescale(self):\n    \"\"\"Returns the timescale for the rotary embedding.\"\"\"\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n    if self.rope_linear_scaling_factor != 1.0:\n      timescale = timescale * self.rope_linear_scaling_factor\n    return timescale\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      inputs: jax.Array,\n      position: None | jax.Array = None,\n  ) -> jax.Array:\n    \"\"\"Generates a jax.Array of sinusoids with different frequencies.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. Since rotary position embeddings are applied to query and\n        keys after projection, it is assumed of shape [B, S, N, H].\n      position: Optional position jax.Array which denotes the position of each\n        token in the sequence. This only needs to be supplied when the sequence\n        is packed. It is of shape [B, S].\n\n    Returns:\n      a jax.Array of shape [B, S, N, H] which includes the inputs together with\n      the rotary position embedding incorporated in it.\n    \"\"\"\n    assert position is not None\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape\" \"[batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding\" \"must match the hidden dimension of the inputs.\"\n      )\n\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n    sin = jnp.sin(sinusoid_inp).astype(inputs.dtype)\n    cos = jnp.cos(sinusoid_inp).astype(inputs.dtype)\n    first_half, second_half = jnp.split(inputs, 2, axis=-1)\n    first_part = first_half * cos - second_half * sin\n    second_part = second_half * cos + first_half * sin\n    if self.cast_as_fprop_dtype:\n      first_part = first_part.astype(self.fprop_dtype)\n      second_part = second_part.astype(self.fprop_dtype)\n    x_out = jnp.concatenate((first_part, second_part), axis=-1)\n    return x_out",
        "analysis": {
            "module_type": "rotary_position_embedding",
            "purpose": "Applies rotary position embeddings (RoPE) to an input tensor to encode positional information, typically for query and key tensors in attention mechanisms.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "min_timescale": "The starting value for the geometric sequence of timescales.",
                "max_timescale": "The ending value for the geometric sequence of timescales.",
                "embedding_dims": "The dimensionality of the embeddings, which must match the last dimension of the input tensor.",
                "cast_as_fprop_dtype": "A boolean indicating whether to cast the output to `fprop_dtype`.",
                "fprop_dtype": "The target data type for the output tensor if casting is enabled.",
                "rope_linear_scaling_factor": "A factor to linearly scale the computed timescale."
            },
            "notes": [
                "The embedding dimension must be a multiple of 2.",
                "The `rngs` parameter in the constructor is for compatibility with `nnx.bridge.to_linen` and is not used by this module."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RotaryEmbedding module with timescale parameters, embedding dimensions, and data types.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store initialization parameters as instance attributes.",
                        "Validate that `embedding_dims` is a multiple of 2."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Initializes hyperparameters required for calculating the rotary embeddings."
                    ]
                },
                "timescale": {
                    "purpose": "Computes and returns the timescale vector used for generating sinusoidal embeddings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate a geometric progression of values from `min_timescale` to `max_timescale` over half the embedding dimension.",
                        "Optionally scale the result by `rope_linear_scaling_factor`."
                    ],
                    "output": {
                        "shape": "[embedding_dims // 2]"
                    },
                    "dependencies": [
                        "jnp.arange"
                    ],
                    "notes": [
                        "This is a `@property`, so it is computed on access."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the rotary position embedding to the input tensor using provided token positions.",
                    "input": {
                        "shape": "[B, S, N, H] for `inputs`, and [B, S] for `position`.",
                        "dtype": "A JAX array type, typically float (e.g., jnp.bfloat16)."
                    },
                    "processing_steps": [
                        "Validate input tensor shape and dimensions.",
                        "Expand dimensions of the `position` tensor for broadcasting.",
                        "Calculate sinusoidal inputs by dividing `position` by `self.timescale`.",
                        "Compute sine and cosine of the sinusoidal inputs.",
                        "Split the input tensor into two halves along the last dimension.",
                        "Apply the rotary transformation using element-wise multiplication and addition with the sine and cosine values.",
                        "Optionally cast the transformed parts to `self.fprop_dtype`.",
                        "Concatenate the two transformed parts back into a single tensor."
                    ],
                    "output": {
                        "shape": "[B, S, N, H], same as the input `inputs` tensor."
                    },
                    "dependencies": [
                        "jnp.sin",
                        "jnp.cos",
                        "jnp.split",
                        "jnp.concatenate"
                    ],
                    "notes": [
                        "The `position` argument is mandatory and must not be None.",
                        "The input tensor is assumed to have a shape of [batch, sequence, heads, dims]."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    use_scale: bool = True,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LLaMARotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    use_scale: Whether to apply LLaMA3.1 scaling factor.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LLaMARotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      use_scale=use_scale,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "functionality": "This function initializes a `LLaMARotaryEmbedding` NNX module with the provided configuration and wraps it to be compatible with the Flax Linen API.",
            "usage": "Call this function with rotary embedding parameters (min/max timescale, embedding dimensions, etc.) to get a Linen-compatible module that can be used within a larger Flax model to apply LLaMA-style rotary position embeddings."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LLaMARotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LLaMARotaryEmbedding(RotaryEmbedding):\n  \"\"\"LLaMA variant of ROPE.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      use_scale: bool = True,\n      # Not used in LLaMARotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the LLaMARotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      use_scale: Whether to apply LLaMA3.1 scaling factor.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    super().__init__(\n        min_timescale=min_timescale,\n        max_timescale=max_timescale,\n        embedding_dims=embedding_dims,\n        cast_as_fprop_dtype=cast_as_fprop_dtype,\n        fprop_dtype=fprop_dtype,\n        rngs=rngs,\n    )\n\n    # LLaMA3.1 ROPE scaling, see the original pytorch implementation:\n    # https://github.com/meta-llama/llama-models/blob/301ca3a2b3b10e94ddcd1fdd2c57e52f812e1cac/models/llama3/reference_impl/model.py#L45C5-L45C18\n    self.use_scale = use_scale\n\n  @property\n  def timescale(self):\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    fraction = jnp.repeat(fraction, 2)\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n\n    # Apply scaling factor if enabled\n    if self.use_scale:\n      timescale = 1.0 / jax.vmap(self._apply_scaling_factor)(1.0 / timescale)\n\n    # Expand timescale dimensions for broadcasting\n    return timescale[jnp.newaxis, jnp.newaxis, jnp.newaxis, :]\n\n  def _apply_scaling_factor(self, freq):\n    \"\"\"apply scaling factor to rotary position embedding.\"\"\"\n    scale_factor = 8\n    low_freq_factor = 1\n    high_freq_factor = 4\n    old_context_len = 8192  # original llama3 length\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n    wavelen = 2 * jnp.pi / freq\n\n    def lower_wavelen(freq):\n      return freq\n\n    def bigger_or_equal_wavelen(freq):\n      def bigger_wavelen(freq):\n        return freq / scale_factor\n\n      def equal_wavelen(freq):\n        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n        return (1 - smooth) * freq / scale_factor + smooth * freq\n\n      bigger_wavelen_cond = wavelen > low_freq_wavelen\n      return jax.lax.cond(bigger_wavelen_cond, bigger_wavelen, equal_wavelen, freq)\n\n    lower_wavelen_cond = wavelen < high_freq_wavelen\n    return jax.lax.cond(lower_wavelen_cond, lower_wavelen, bigger_or_equal_wavelen, freq)\n\n  def __call__(self, inputs: jax.Array, position: None | jax.Array = None) -> jax.Array:\n    \"\"\"Applies LLaMA variant of rotary position embedding.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. It is assumed of shape [B, S, N, H].\n      position: Optional position array [B, S]. Only needed when the sequence\n        is packed.\n\n    Returns:\n      A jax.Array of shape [B, S, N, H] with rotary position embeddings applied.\n    \"\"\"\n    # Ensure input is 4D\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [B, S, N, H].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\"\n      )\n\n    # Shift the inputs left and right as per LLaMA's specific behavior\n    inputs_shifted_left = jnp.concatenate([inputs[..., 1:], inputs[..., :1]], axis=-1)\n    inputs_shifted_right = jnp.concatenate([inputs[..., -1:], inputs[..., :-1]], axis=-1)\n    inputs_shifted = jax.lax.select(\n        jnp.tile(\n            jnp.mod(jnp.arange(self.embedding_dims, dtype=jnp.int32), 2),\n            inputs.shape[:-1] + (1,),\n        ),\n        inputs_shifted_right,\n        inputs_shifted_left,\n    )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.float32)[jnp.newaxis, :]\n\n    # Calculate sinusoidal input\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n\n    sin = jnp.sin(sinusoid_inp)\n    cos = jnp.cos(sinusoid_inp)\n\n    # Apply alternating sign\n    sign = jnp.tile(jnp.array([-1, 1]), self.embedding_dims // 2)\n\n    # Combine original inputs with sinusoidal information\n    outputs = inputs * cos + inputs_shifted * sin * sign\n\n    if self.cast_as_fprop_dtype:\n      outputs = outputs.astype(self.fprop_dtype)\n\n    return outputs",
        "analysis": {
            "module_type": "llama_rotary_embedding",
            "purpose": "Implements the LLaMA-specific variant of Rotary Position Embedding (ROPE), including optional LLaMA3.1 frequency scaling.",
            "input": {
                "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                "dtype": "jax.Array (float)"
            },
            "processing_steps": [
                "The `__call__` method is invoked to apply the embedding."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, num_heads, head_dim]"
            },
            "dependencies": [
                "RotaryEmbedding",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, determining the periodicity of the signal.",
                "max_timescale": "End of the geometric index, determining the frequency of the signal.",
                "embedding_dims": "Dimension of the embedding to be generated, must match the input's last dimension.",
                "use_scale": "A boolean indicating whether to apply the LLaMA3.1 frequency scaling factor."
            },
            "notes": [
                "This class inherits from `RotaryEmbedding` but overrides the `timescale` property and the `__call__` method to implement LLaMA-specific logic.",
                "The core rotational logic involves shifting the input tensor and applying an alternating sign, which differs from the standard RoPE implementation of splitting the tensor into two halves."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the LLaMARotaryEmbedding module, setting up parameters for timescale and optional LLaMA3.1 scaling.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the parent `RotaryEmbedding` constructor.",
                        "Stores the `use_scale` boolean flag."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RotaryEmbedding"
                    ],
                    "notes": [
                        "The `rngs` parameter is a placeholder for compatibility with `nnx.bridge.to_linen` and is not used."
                    ]
                },
                "timescale": {
                    "purpose": "Calculates the timescale for the rotary embeddings, applying LLaMA3.1 scaling if enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculates a base fraction based on `embedding_dims`.",
                        "Computes the initial timescale using `min_timescale`, `max_timescale`, and the fraction.",
                        "If `use_scale` is true, applies the scaling factor by calling `_apply_scaling_factor` via `jax.vmap`.",
                        "Expands the timescale dimensions for broadcasting."
                    ],
                    "output": {
                        "shape": "[1, 1, 1, embedding_dims]"
                    },
                    "dependencies": [
                        "jax.vmap",
                        "self._apply_scaling_factor"
                    ],
                    "notes": [
                        "This is a property that computes the timescale on access."
                    ]
                },
                "_apply_scaling_factor": {
                    "purpose": "Applies the LLaMA3.1 scaling factor to a given frequency based on its corresponding wavelength.",
                    "input": {
                        "shape": "Scalar",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Defines scaling parameters like `scale_factor`, `low_freq_factor`, `high_freq_factor`, and `old_context_len`.",
                        "Calculates wavelength from the input frequency.",
                        "Uses `jax.lax.cond` to apply different scaling logic based on the wavelength relative to predefined thresholds."
                    ],
                    "output": {
                        "shape": "Scalar"
                    },
                    "dependencies": [
                        "jax.lax.cond"
                    ],
                    "notes": [
                        "This method implements the specific frequency scaling logic from the LLaMA3.1 reference implementation."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the LLaMA-style rotary position embedding to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                        "dtype": "jax.Array (float)"
                    },
                    "processing_steps": [
                        "Validates the input tensor shape.",
                        "Creates a shifted version of the input tensor by concatenating slices.",
                        "Determines token positions, creating a range if not provided.",
                        "Calculates sinusoidal input by dividing positions by the `timescale` property.",
                        "Computes sine and cosine of the sinusoidal input.",
                        "Creates an alternating sign array.",
                        "Combines the original and shifted inputs using the computed sine, cosine, and sign values.",
                        "Optionally casts the output to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.timescale"
                    ],
                    "notes": [
                        "If the `position` argument is not provided, it is inferred from the input's sequence length."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#yarn_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def yarn_rotary_embedding_as_linen(\n    *,\n    embedding_dims: int,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    beta_fast: float = 32,\n    beta_slow: float = 1,\n    rope_theta: float = 10000.0,\n    rope_factor: float = 40,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n    interleave: bool = True,\n    truncate: bool = True,\n    attention_scaling: bool = False,\n):\n  \"\"\"Initializes the YarnRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_position_embeddings: The maximum number of positions.\n    original_max_position_embeddings: The original maximum number of positions.\n    beta_fast: The fast beta parameter for YaRN.\n    beta_slow: The slow beta parameter for YaRN.\n    rope_theta: The base for the rotary frequencies.\n    rope_factor: The scaling factor for RoPE.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    name: The name of the module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      YarnRotaryEmbedding,\n      embedding_dims=embedding_dims,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      beta_fast=beta_fast,\n      beta_slow=beta_slow,\n      rope_theta=rope_theta,\n      rope_factor=rope_factor,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      interleave=interleave,\n      truncate=truncate,\n      attention_scaling=attention_scaling,\n  )",
        "analysis": {
            "functionality": "This function initializes and configures a `YarnRotaryEmbedding` module and then wraps it as a Flax Linen module. This allows the NNX-based `YarnRotaryEmbedding` to be used seamlessly within a Linen-based neural network architecture.",
            "usage": "Call this function with the desired YaRN configuration parameters (e.g., `embedding_dims`, `max_position_embeddings`, `rope_factor`) to get a Linen-compatible module instance. This instance can then be used within a Linen model's `setup` or `__call__` method to apply YaRN positional embeddings to query and key tensors."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#YarnRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class YarnRotaryEmbedding(nnx.Module):\n  \"\"\"Yarn rotary embedding.\n\n  Based on https://arxiv.org/abs/2309.00071\n  This implementation uses DeepSeek-v3 PyTorch as reference\n  https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L294\n\n  Attributes:\n    embedding_dims: Dimension of the embedding to be generated.\n    max_position_embeddings: The maximum sequence length that will be encountered.\n    original_max_position_embeddings: The sequence length for which the base frequencies were defined.\n    beta_fast: Lower bound parameter for correction.\n    beta_slow: Upper bound parameter for correction.\n    rope_theta: The base theta value for the frequency computation.\n    rope_factor: Factor applied to adjust the frequencies.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    rope_interleave: Whether complex representation is interleaved or concatenated.\n    rope_truncate: Whether or not to floor lower bound and ceil upper bound for correction range.\n    rope_attention_scaling: Whether or not to scale the rotary embedding output.\n    rngs: rng keys passed in by nnx.bridge.to_linen.\n  \"\"\"\n\n  def __init__(\n      self,\n      embedding_dims: int,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      beta_fast: float = 32,\n      beta_slow: float = 1,\n      rope_theta: float = 10000.0,\n      rope_factor: float = 40,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      interleave=True,\n      truncate=True,\n      attention_scaling=False,\n      # Not used in YarnRotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the YarnRotaryEmbedding module.\"\"\"\n    self.embedding_dims = embedding_dims\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.beta_fast = beta_fast\n    self.beta_slow = beta_slow\n    self.rope_theta = rope_theta\n    self.rope_factor = rope_factor\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.interleave = interleave\n    self.truncate = truncate\n    self.attention_scaling = attention_scaling\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    half_dim = self.embedding_dims // 2\n    # Compute base frequencies for each (even-indexed) dimension.\n    # (Note: We use jnp.arange with float32 for precision.)\n    freqs = 1.0 / (self.rope_theta ** (2.0 * jnp.arange(0, half_dim, dtype=jnp.float32) / self.embedding_dims))\n\n    low, high = self._find_correction_range(\n        self.beta_fast,\n        self.beta_slow,\n        self.embedding_dims,\n        self.rope_theta,\n        self.original_max_position_embeddings,\n        self.truncate,\n    )\n    smooth = 1 - self._linear_ramp_factor(low, high, half_dim)\n    # The corrected frequency is a weighted mix of the scaled and base values.\n    freqs = freqs / self.rope_factor * (1 - smooth) + freqs * smooth\n\n    # Precompute frequencies for all positions by taking the outer product.\n    t = jnp.arange(self.max_position_embeddings, dtype=jnp.float32)  # shape [max_position_embeddings]\n    # This gives a [max_position_embeddings, half_dim] tensor with rows as time steps.\n    freqs = jnp.outer(t, freqs)\n\n    # Compute the complex \u201ccis\u201d values: exp(i * theta).\n    return jnp.exp(1j * freqs)  # shape [max_position_embeddings, half_dim]\n\n  def _find_correction_dim(self, num_rotations: float, dim: int, base: float, max_position_embeddings: int) -> float:\n    \"\"\"Compute the correction dimension for a given number of rotations.\"\"\"\n    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n\n  def _find_correction_range(\n      self, low_rot: float, high_rot: float, dim: int, base: float, max_position_embeddings: int, truncate: bool\n  ):\n    \"\"\"Computes the range of correction dimensions for rotary positional embeddings.\n\n    Args:\n        low_rot (float): Lower bound for the number of rotations.\n        high_rot (float): Upper bound for the number of rotations.\n        dim (int): Dimensionality of the embedding space.\n        base (float): Base value for the exponential computation.\n        max_position_embeddings (int): Maximum sequence length.\n        truncate (bool): Whether to floor lower bound and ceil upper bound.\n\n    Returns:\n        tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n    \"\"\"\n    low = self._find_correction_dim(low_rot, dim, base, max_position_embeddings)\n    high = self._find_correction_dim(high_rot, dim, base, max_position_embeddings)\n    if truncate:\n      low = math.floor(low)\n      high = math.ceil(high)\n    low = max(low, 0)\n    high = min(high, dim - 1)\n    return low, high\n\n  def _linear_ramp_factor(self, min_val: float, max_val: float, dim: int) -> Array:\n    \"\"\"Computes a linear ramp over the dimension.\n\n    Returns a jax.Array of shape (dim,) with values between 0 and 1.\n    \"\"\"\n    if min_val == max_val:\n      max_val += 0.001  # Avoid division by zero.\n    linear_func = (jnp.arange(dim, dtype=jnp.float32) - min_val) / (max_val - min_val)\n    return jnp.clip(linear_func, 0, 1)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies the rotary positional embedding using the precomputed complex frequencies.\n\n    Args:\n      inputs: jax.Array of shape [B, S, N, H]. (H must equal self.embedding_dims.)\n      position: jax.Array of shape [B, S] with integer positions (indexes into precomputed freqs).\n\n    Returns:\n      jax.Array of shape [B, S, N, H] with the rotary embedding applied.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\"\n      )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.int32)[jnp.newaxis, :]\n    else:\n      position = position.astype(jnp.int32)\n\n    # Lookup the precomputed frequencies using the position indices.\n    # self.freqs_cis has shape [max_position_embeddings, half_dim] so we use jnp.take along axis 0.\n    # After indexing, shape becomes [B, S, half_dim]; we then add an axis for the heads.\n    freqs = jnp.take(self.freqs_cis, position, axis=0)  # shape: [B, S, half_dim]\n    freqs = freqs[:, :, jnp.newaxis, :]  # shape: [B, S, 1, half_dim]\n\n    if self.interleave:\n      # Inputs with interleaved format [real1, img1, real2, img2, ...] at last dimension\n      # Convert the last dimension into a complex representation.\n      # First reshape so that each pair of numbers represents the real and imaginary parts.\n      B, S, N, H = inputs.shape\n      half_dim = H // 2\n      inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n      first_half, second_half = inputs_reshaped[..., 0], inputs_reshaped[..., 1]\n    else:\n      # Inputs with concatenated format [real1, real2, ..., img1, img2, ...] at last dimension\n      first_half, second_half = jnp.split(inputs, 2, axis=-1)\n\n    inputs_complex = first_half + 1j * second_half  # shape: [B, S, N, half_dim]\n    # Apply the rotary transformation via complex multiplication.\n    rotated = inputs_complex * freqs  # shape: [B, S, N, half_dim]\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    # [real1, real2, ..., img1, img2, ...]\n    output = jnp.concatenate([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n\n    if self.attention_scaling:\n      attention_scaling = 1.0 if self.rope_factor <= 1 else (0.1 * math.log(self.rope_factor) + 1.0)\n      output = output * attention_scaling\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n    return output",
        "analysis": {
            "module_type": "yarn_rotary_embedding",
            "purpose": "Implements the YaRN (Yet another RoPE extensioN) method to apply rotary positional embeddings, designed to efficiently extend the context window of language models.",
            "input": {
                "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                "dtype": "float32 or bfloat16"
            },
            "processing_steps": [
                "Determine token positions, defaulting to `arange(sequence_length)` if not provided.",
                "Look up precomputed complex frequencies (`freqs_cis`) based on token positions.",
                "Reshape the input tensor into a complex representation, handling both interleaved and concatenated formats.",
                "Apply the rotary transformation by performing complex multiplication between the input and the looked-up frequencies.",
                "Convert the complex result back into a real tensor.",
                "Optionally apply attention scaling.",
                "Optionally cast the output to the forward pass data type (`fprop_dtype`)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, num_heads, head_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "jax.numpy"
            ],
            "parameters": {
                "rope_factor": "Scaling factor applied to adjust the RoPE frequencies for context window extension.",
                "beta_fast": "Lower bound parameter for the YaRN correction, controlling the interpolation for high-frequency dimensions.",
                "beta_slow": "Upper bound parameter for the YaRN correction, controlling the interpolation for low-frequency dimensions.",
                "original_max_position_embeddings": "The original maximum sequence length the model was trained on, used as a basis for frequency correction.",
                "interleave": "A boolean indicating whether the input tensor's last dimension has an interleaved ([real1, imag1, real2, imag2, ...]) or concatenated ([real..., imag...]) format."
            },
            "notes": [
                "The core logic involves precomputing complex sinusoidal frequencies (`freqs_cis`) that are corrected using the YaRN interpolation technique.",
                "This implementation is based on the DeepSeek-v3 PyTorch reference implementation.",
                "The `position` argument to the `__call__` method is necessary for packed sequences where a token's position is not its index in the sequence dimension."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the YarnRotaryEmbedding module with configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration parameters like `embedding_dims`, `rope_factor`, `beta_fast`, etc., to instance attributes.",
                        "Validates that `embedding_dims` is a multiple of 2."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "freqs_cis": {
                    "purpose": "Precomputes and returns the complex sinusoidal frequencies (`cis`) for all positions up to `max_position_embeddings`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Compute base frequencies based on `rope_theta` and `embedding_dims`.",
                        "Calculate the correction range using `_find_correction_range`.",
                        "Compute a linear ramp factor (`smooth`) for interpolation.",
                        "Apply the YaRN correction to the frequencies, mixing scaled and base values.",
                        "Generate frequencies for all positions using an outer product with a time vector `t`.",
                        "Convert the final frequencies to complex numbers using `jnp.exp(1j * freqs)`."
                    ],
                    "output": {
                        "shape": "[max_position_embeddings, embedding_dims // 2]"
                    },
                    "dependencies": [
                        "_find_correction_range",
                        "_linear_ramp_factor",
                        "jnp.outer",
                        "jnp.exp"
                    ],
                    "notes": [
                        "This is a property, so it's computed on the first access."
                    ]
                },
                "_find_correction_dim": {
                    "purpose": "Helper method to compute the correction dimension for a given number of rotations, a key part of the YaRN interpolation.",
                    "input": {
                        "shape": "Scalar values for `num_rotations`, `dim`, `base`, `max_position_embeddings`.",
                        "dtype": "float, int, float, int"
                    },
                    "processing_steps": [
                        "Applies a logarithmic formula to find the dimension index corresponding to a specific rotational frequency."
                    ],
                    "output": {
                        "shape": "Scalar"
                    },
                    "dependencies": [
                        "math.log"
                    ],
                    "notes": []
                },
                "_find_correction_range": {
                    "purpose": "Computes the start and end dimension indices for applying the YaRN frequency correction.",
                    "input": {
                        "shape": "Scalar values for `low_rot`, `high_rot`, `dim`, `base`, `max_position_embeddings`, `truncate`.",
                        "dtype": "float, float, int, float, int, bool"
                    },
                    "processing_steps": [
                        "Calls `_find_correction_dim` for both the low and high rotation bounds.",
                        "Optionally floors the low bound and ceils the high bound.",
                        "Clamps the resulting range to be within valid dimension indices `[0, dim - 1]`."
                    ],
                    "output": {
                        "shape": "A tuple of two integers `(low, high)`."
                    },
                    "dependencies": [
                        "_find_correction_dim"
                    ],
                    "notes": []
                },
                "_linear_ramp_factor": {
                    "purpose": "Computes a linear ramp of values between 0 and 1 over a specified dimension range, used for smooth interpolation in YaRN.",
                    "input": {
                        "shape": "Scalar values for `min_val`, `max_val`, `dim`.",
                        "dtype": "float, float, int"
                    },
                    "processing_steps": [
                        "Creates a linear function over the range `[min_val, max_val]` for `dim` steps.",
                        "Clips the output values to be between 0 and 1."
                    ],
                    "output": {
                        "shape": "[dim]"
                    },
                    "dependencies": [
                        "jnp.arange",
                        "jnp.clip"
                    ],
                    "notes": [
                        "Adds a small epsilon to `max_val` if `min_val == max_val` to avoid division by zero."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the YaRN rotary positional embedding to an input tensor.",
                    "input": {
                        "shape": "inputs: `[batch_size, sequence_length, num_heads, head_dim]`, position (optional): `[batch_size, sequence_length]`",
                        "dtype": "inputs: float, position: int32"
                    },
                    "processing_steps": [
                        "Validate input tensor shape.",
                        "Generate position indices if not provided.",
                        "Look up the precomputed `freqs_cis` using the position indices.",
                        "Convert the input tensor's last dimension to a complex number representation, handling both `interleave` and concatenated formats.",
                        "Apply rotation by multiplying the complex input with the complex frequencies.",
                        "Convert the result back to a real tensor by concatenating its real and imaginary parts.",
                        "Optionally apply `attention_scaling`.",
                        "Optionally cast the final output to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.freqs_cis",
                        "jnp.take",
                        "jnp.split",
                        "jnp.concatenate"
                    ],
                    "notes": [
                        "The `position` argument is used for packed sequences where each token's position is not its index in the sequence dimension."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#positional_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def positional_embedding_as_linen(*, embedding_dims: int, max_wavelength: int = _MAX_WAVELENGTH):\n  \"\"\"Initializes the PositionalEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      PositionalEmbedding,\n      embedding_dims=embedding_dims,\n      max_wavelength=max_wavelength,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "positional_embedding_factory",
            "purpose": "Initializes a `PositionalEmbedding` NNX module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `PositionalEmbedding` NNX module into a Linen module.",
                "Passes `embedding_dims` and `max_wavelength` as arguments for the `PositionalEmbedding` constructor.",
                "Provides `variable_to_logically_partitioned` as the `metadata_fn` for handling parameter partitioning."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "PositionalEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "This function is a factory that returns a Flax Linen module instance, not a tensor.",
                "It facilitates the use of an NNX module within a Linen-based model architecture."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#PositionalEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class PositionalEmbedding(nnx.Module):\n  \"\"\"A layer that adds sinusoidal positional embeddings to the input.\n\n  Attributes:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  \"\"\"\n\n  embedding_dims: int\n  max_wavelength: int = _MAX_WAVELENGTH\n\n  rngs: nnx.Rngs = None  # Not used in PositionalEmbedding but passed in by nnx.bridge.to_linen\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      input_embedding: jax.Array,\n      position: jax.Array,\n  ) -> jax.Array:\n    num_timescales = self.embedding_dims // 2\n    log_timescale_increment = jnp.log(float(self.max_wavelength)) / jnp.maximum(\n        jnp.asarray(num_timescales, dtype=jnp.float32) - 1, 1\n    )\n    inv_timescales = jnp.exp(jnp.arange(num_timescales, dtype=jnp.float32) * -log_timescale_increment)\n    position = position[:, :, jnp.newaxis]\n    inv_timescales = inv_timescales[jnp.newaxis, jnp.newaxis, :]\n    scaled_time = position * inv_timescales\n    signal = jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)], axis=-1)\n    # signal = jnp.pad(signal, [[0, jnp.mod(self.embedding_dims, 2)]])\n    position_embedding = signal.astype(jnp.float32)\n    return input_embedding + position_embedding",
        "analysis": {
            "module_type": "positional_embedding",
            "purpose": "A layer that computes and adds sinusoidal positional embeddings to input token embeddings.",
            "input": {
                "shape": "input_embedding: [batch_size, sequence_length, embedding_dims], position: [batch_size, sequence_length]",
                "dtype": "jax.Array (float for input_embedding, integer/float for position)"
            },
            "processing_steps": [
                "The `__call__` method is invoked to compute and apply the positional embeddings."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dims]"
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "This module implements the sinusoidal positional encoding from the 'Attention Is All You Need' paper.",
                "It is defined as a `dataclasses.dataclass`.",
                "The `rngs` attribute is accepted for compatibility with `nnx.bridge.to_linen` but is not used."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Calculates sinusoidal positional embeddings based on input positions and adds them to the input token embeddings.",
                    "input": {
                        "shape": "input_embedding: [batch_size, sequence_length, embedding_dims], position: [batch_size, sequence_length]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Calculate the number of timescales as half of the embedding dimension.",
                        "Compute inverse timescales using a logarithmic scale from 1 to `max_wavelength`.",
                        "Expand dimensions of `position` and `inv_timescales` for broadcasting.",
                        "Calculate `scaled_time` by multiplying the expanded `position` and `inv_timescales`.",
                        "Generate a signal by concatenating the sine and cosine of `scaled_time` along the last axis.",
                        "Cast the resulting signal to float32 to create the `position_embedding`.",
                        "Return the sum of the original `input_embedding` and the computed `position_embedding`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dims]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "The output shape is identical to the `input_embedding` shape."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_vision_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_vision_rotary_embedding_as_linen(\n    *,\n    image_size: int,\n    patch_size: int,\n    hidden_size: int,\n    num_attention_heads: int,\n    rope_theta: float = 10000.0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LlamaVisionRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    image_size: The size of the input image.\n    patch_size: The size of the image patches.\n    hidden_size: The size of the hidden dimension.\n    num_attention_heads: The number of attention heads.\n    rope_theta: The base theta value for the frequency computation.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: The name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LlamaVisionRotaryEmbedding,\n      image_size=image_size,\n      patch_size=patch_size,\n      hidden_size=hidden_size,\n      num_attention_heads=num_attention_heads,\n      rope_theta=rope_theta,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "llama_vision_rotary_embedding_factory",
            "purpose": "A factory function that initializes the `LlamaVisionRotaryEmbedding` NNX module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `LlamaVisionRotaryEmbedding` class.",
                "Passes configuration parameters such as `image_size`, `patch_size`, `hidden_size`, etc., to the `LlamaVisionRotaryEmbedding` constructor through the wrapper.",
                "Specifies `variable_to_logically_partitioned` as the metadata function for handling variable partitioning."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance that encapsulates the `LlamaVisionRotaryEmbedding` logic."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "LlamaVisionRotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "image_size": "The size of the input image.",
                "patch_size": "The size of the image patches.",
                "hidden_size": "The size of the hidden dimension.",
                "num_attention_heads": "The number of attention heads.",
                "rope_theta": "The base theta value for the frequency computation.",
                "cast_as_fprop_dtype": "Whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The data type for the forward pass."
            },
            "notes": [
                "This function serves as a bridge to allow an NNX-defined module to be used within a larger model built with Flax Linen.",
                "The actual rotary embedding computation is implemented in the `LlamaVisionRotaryEmbedding` class."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LlamaVisionRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LlamaVisionRotaryEmbedding(nnx.Module):\n  \"\"\"Rotary position embedding for Llama4 vision encoder.\n\n  Based on Pytorch Reference\n  https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n  This implementation follows the Llama4 vision encoder's rotary embedding approach,\n  which uses 2D coordinates (x, y) to generate rotary position embeddings.\n\n  Attributes:\n    image_size: int size of the input image\n    patch_size: int size of the image patches\n    hidden_size: int size of the hidden dimension\n    num_attention_heads: int number of attention heads\n    rope_theta: float = 10000.0 base theta value for the frequency computation\n    cast_as_fprop_dtype: bool = True whether to cast the output to the fprop dtype\n    fprop_dtype: DType = jnp.bfloat16 the dtype of the output\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  Returns:\n    jax.Array of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n    where vision rotary position embeddings are applied.\n  \"\"\"\n\n  image_size: int\n  patch_size: int\n  hidden_size: int\n  num_attention_heads: int\n  rope_theta: float = 10000.0\n  cast_as_fprop_dtype: bool = True\n  fprop_dtype: DType = jnp.bfloat16\n  # Not used in LlamaVisionRotaryEmbedding but passed in by nnx.bridge.to_linen.\n  # TODO: Remove when bridge no longer needed\n  rngs: nnx.Rngs = None\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    idx = self.image_size // self.patch_size\n    img_idx = jnp.arange(idx**2, dtype=jnp.int32).reshape(idx**2, 1)\n    img_idx = jnp.concatenate([img_idx, img_idx[:1]], axis=0)\n    img_idx = img_idx.at[-1, -1].set(-2)  # ID_CLS_TOKEN\n\n    # Get 2D coordinates\n    frequencies_x = img_idx % idx  # x coordinates\n    frequencies_y = img_idx // idx  # y coordinates\n\n    # Compute frequency dimensions\n    freq_dim = self.hidden_size // self.num_attention_heads // 2\n    rope_freq = 1.0 / (self.rope_theta ** (jnp.arange(0, freq_dim, 2)[: (freq_dim // 2)].astype(jnp.float32) / freq_dim))\n\n    # Compute frequencies for x and y coordinates\n    freqs_x = (frequencies_x + 1)[..., None] * rope_freq[None, None, :]\n    freqs_y = (frequencies_y + 1)[..., None] * rope_freq[None, None, :]\n\n    # Interleave x and y frequencies\n    freqs_x = jnp.repeat(freqs_x, 2, axis=-1)\n    freqs_y = jnp.repeat(freqs_y, 2, axis=-1)\n\n    # Combine frequencies\n    freqs = jnp.concatenate([freqs_x, freqs_y], axis=-1).astype(jnp.float32)\n    freqs = freqs[..., ::2]\n\n    # Mask out invalid positions\n    freqs = jnp.where(img_idx.reshape(-1, 1, 1) < 0, 0, freqs)\n    # Convert to complex representation\n    return jnp.exp(1j * freqs)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies rotary embeddings to the input tensor for Llama4 vision encoder.\n\n    Args:\n      inputs: Input tensor of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n\n    Returns:\n      Tensor with rotary embeddings applied, maintaining the same shape as input.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\n          \"\"\"Input is assumed to be a rank 4 tensor of shape [batch_size_times_tiles, num_patches_incl_cls, \n          num_heads, head_dim].\"\"\"\n      )\n\n    # Reshape inputs to complex representation\n    B, S, N, H = inputs.shape\n    half_dim = H // 2\n\n    # Convert the last dimension into a complex representation.\n    # First reshape so that each pair of numbers represents the real and imaginary parts.\n    inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n    inputs_complex = inputs_reshaped[..., 0] + 1j * inputs_reshaped[..., 1]\n\n    # Reshape freqs_ci for broadcasting\n    freqs_ci = self.freqs_cis[jnp.newaxis, :, :, :]\n\n    # Apply rotary transformation\n    rotated = inputs_complex * freqs_ci\n\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    rotated_real = jnp.stack([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n    output = rotated_real.reshape(B, S, N, H)\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n\n    return output",
        "analysis": {
            "module_type": "llama_vision_rotary_embedding",
            "purpose": "Implements rotary position embeddings for a vision encoder, generating embeddings based on 2D patch coordinates.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes parameters such as image size, patch size, and hidden dimension.",
                "Pre-computes complex frequency embeddings (`freqs_cis`) based on 2D patch coordinates in a property.",
                "When called, it converts the input tensor to a complex representation.",
                "Applies the rotary transformation by multiplying the complex input with the pre-computed frequencies.",
                "Converts the result back to a real tensor.",
                "Optionally casts the output to a specified floating-point data type."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.numpy"
            ],
            "parameters": {
                "image_size": "The size (height and width) of the input image.",
                "patch_size": "The size (height and width) of the image patches.",
                "hidden_size": "The size of the hidden dimension of the model.",
                "num_attention_heads": "The number of attention heads in the attention mechanism.",
                "rope_theta": "The base theta value for the frequency computation.",
                "cast_as_fprop_dtype": "A boolean indicating whether to cast the output to `fprop_dtype`.",
                "fprop_dtype": "The target data type for the output if casting is enabled."
            },
            "notes": [
                "This module is specifically designed for the Llama4 vision encoder.",
                "It generates position embeddings from the 2D coordinates (x, y) of image patches.",
                "A special CLS token is handled during frequency generation by setting its ID to -2 and masking its frequency contribution to zero."
            ],
            "methods": {
                "freqs_cis": {
                    "purpose": "Pre-computes the complex sinusoidal frequencies (cis) for the rotary embeddings based on 2D image patch coordinates.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the number of patches per side (`idx`).",
                        "Generate a 1D index for each patch and append a special index for the CLS token.",
                        "Convert the 1D index into 2D coordinates (x, y).",
                        "Compute the base rotary frequencies (`rope_freq`) based on `rope_theta` and `hidden_size`.",
                        "Calculate separate frequencies for x and y coordinates.",
                        "Interleave and concatenate x and y frequencies.",
                        "Mask out invalid positions (like the CLS token) by setting their frequencies to 0.",
                        "Convert the final frequencies into a complex representation using `jnp.exp(1j * freqs)`."
                    ],
                    "output": {
                        "shape": "[num_patches_incl_cls, 1, head_dim]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a `@property`, so it's computed on first access and cached per instance.",
                        "It handles a special CLS token by setting its index to -2, which is then used to mask its frequency to 0."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the pre-computed rotary embeddings to an input tensor.",
                    "input": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]",
                        "dtype": "float (e.g., jnp.bfloat16)"
                    },
                    "processing_steps": [
                        "Validate that the input tensor has a rank of 4.",
                        "Reshape the last dimension of the input into a complex representation.",
                        "Broadcast and multiply the complex input with the pre-computed `freqs_cis`.",
                        "Convert the resulting complex tensor back into a real tensor by stacking its real and imaginary parts.",
                        "Reshape the output to match the original input shape.",
                        "Optionally cast the output to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.freqs_cis"
                    ],
                    "notes": [
                        "The `position` argument is defined in the method signature but is not used in the implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/encoders.py#VisionEncoder",
        "file_path": "src/MaxText/layers/encoders.py",
        "code_block": "class VisionEncoder(nn.Module):\n  \"\"\"Vision encoder to encode images into soft tokens.\"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    self.vision_encoder_layer = self.get_vision_encoder_layers()\n\n  def get_vision_encoder_layers(self):\n    \"\"\"Get vision encoder layers specific to the model, classes of nn.Module type.\"\"\"\n    if self.config.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\"]:\n      from MaxText.layers import gemma3  # pylint: disable=import-outside-toplevel\n\n      return [gemma3.gemma3visionencoder_as_linen, gemma3.visionembedder_as_linen]\n    elif self.config.model_name in [\"llama4-17b-16e\", \"llama4-17b-128e\"]:\n      from MaxText.layers import llama4  # pylint: disable=import-outside-toplevel\n\n      return [llama4.llama4visionmodel_as_linen, llama4.llama4multimodalprojector_as_linen]\n    else:\n      raise ValueError(f\"No VisionEncoder implemented for {self.config.model_name} yet\")\n\n  @nn.compact\n  def __call__(self, input_images, deterministic=False):\n    cfg = self.config\n    mesh = self.mesh\n    # vision encoder output, frozen params in many cases\n    embeddings = self.vision_encoder_layer[0](config=cfg, mesh=mesh)(input_images, deterministic=deterministic)\n    if cfg.freeze_vision_encoder_params:\n      embeddings = jax.lax.stop_gradient(embeddings)\n\n    if len(self.vision_encoder_layer) > 1:\n      # vision embedder / projection layer, not frozen in most cases, trained / finetuned together with main model\n      embeddings = self.vision_encoder_layer[1](config=cfg, mesh=mesh)(embeddings)\n    return embeddings",
        "analysis": {
            "module_type": "vision_encoder",
            "purpose": "A factory-like module that selects and applies a model-specific vision encoder to transform input images into a sequence of embeddings (soft tokens).",
            "input": {
                "shape": "See the __call__ method.",
                "dtype": "See the __call__ method."
            },
            "processing_steps": [
                "In the `setup` method, `get_vision_encoder_layers` is called to select the appropriate vision encoder and projector layers based on `config.model_name`.",
                "The `__call__` method processes the input images through the selected layers to produce embeddings."
            ],
            "output": {
                "shape": "See the __call__ method."
            },
            "dependencies": [
                "flax.linen.Module",
                "jax",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.gemma3",
                "MaxText.layers.llama4"
            ],
            "parameters": {
                "config.model_name": "A string that determines which specific vision encoder implementation to use (e.g., 'gemma3-4b', 'llama4-17b-16e').",
                "config.freeze_vision_encoder_params": "A boolean flag that, if true, prevents gradients from flowing back into the main vision encoder, effectively freezing its weights during training."
            },
            "notes": [
                "This class acts as a factory, dynamically importing and selecting the correct vision encoder layers at runtime based on the configuration.",
                "The architecture supports a two-stage process: a main vision encoder followed by an optional projection layer (e.g., `visionembedder` or `multimodalprojector`)."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the module by selecting the appropriate vision encoder layers and assigning them to an instance variable.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.get_vision_encoder_layers()`.",
                        "Assigns the returned list of layer classes to `self.vision_encoder_layer`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.get_vision_encoder_layers"
                    ],
                    "notes": [
                        "This is a standard Flax setup method that is called once during module initialization."
                    ]
                },
                "get_vision_encoder_layers": {
                    "purpose": "Selects and returns the appropriate vision encoder and projector layer classes based on the `model_name` in the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks `self.config.model_name` to determine the model family (e.g., Gemma3, Llama4).",
                        "Performs a late import of the corresponding model-specific layers.",
                        "Returns a list containing the vision encoder class and an optional projector class.",
                        "Raises a ValueError if the `model_name` is not supported."
                    ],
                    "output": {
                        "shape": "Returns a list of one or two `nn.Module` classes."
                    },
                    "dependencies": [
                        "MaxText.layers.gemma3",
                        "MaxText.layers.llama4"
                    ],
                    "notes": [
                        "This method implements a factory pattern to dynamically select components."
                    ]
                },
                "__call__": {
                    "purpose": "Processes a batch of input images through the selected vision encoder and optional projector to produce embeddings.",
                    "input": {
                        "shape": "[batch_size, height, width, channels]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Passes `input_images` through the first layer (`self.vision_encoder_layer[0]`).",
                        "If `config.freeze_vision_encoder_params` is true, applies `jax.lax.stop_gradient` to the resulting embeddings.",
                        "If a second layer exists, passes the embeddings through it.",
                        "Returns the final embeddings."
                    ],
                    "output": {
                        "shape": "[batch_size, num_tokens, embedding_dim]"
                    },
                    "dependencies": [
                        "jax.lax.stop_gradient"
                    ],
                    "notes": [
                        "The `@nn.compact` decorator allows for inline initialization of the submodules within this method.",
                        "The `deterministic` flag is passed to the underlying layers to control behaviors like dropout."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma.py#GemmaDecoderLayer",
        "file_path": "src/MaxText/layers/gemma.py",
        "code_block": "class GemmaDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: Optional[Quant] = None,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=self.mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    self.pre_ffw_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp = MlpBlock(\n        config=config,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_manager=None,\n      page_state=None,\n      slot=None,\n  ):\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = self.pre_self_attention_norm(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm(attention_lnx)\n\n    mlp_lnx = self.mlp(attn_output, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        self.activation_axis_names,\n    )\n\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for the Gemma model, consisting of a self-attention block and a feed-forward MLP block with pre-normalization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes sub-modules: RMSNorm for pre-attention and pre-MLP normalization, a self-attention mechanism, an MLP block, and a dropout layer.",
                "Processes input tensors through the layer's `__call__` method."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "max_utils"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like embedding dimension, number of heads, MLP dimension, and dropout rate.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This class implements a standard pre-normalization transformer decoder layer architecture.",
                "It is designed to be used within a larger transformer model, potentially stacked multiple times.",
                "The class uses `nnx`, a new neural network library in Flax."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the sub-modules of the decoder layer: self-attention, MLP, normalization layers, and dropout.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize `pre_self_attention_norm` (RMSNorm).",
                        "Initialize `self_attention` (Attention).",
                        "Initialize `pre_ffw_norm` (RMSNorm).",
                        "Initialize `mlp` (MlpBlock).",
                        "Initialize `dropout` (Dropout)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "Sub-modules are configured based on the provided `config`, `mesh`, `model_mode`, and optional `quant` objects."
                    ]
                },
                "__call__": {
                    "purpose": "Processes an input tensor through one decoder layer, applying self-attention, MLP, and residual connections.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "config.dtype"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input.",
                        "Pass the normalized input through the self-attention mechanism.",
                        "Add the original input to the attention output (first residual connection).",
                        "Apply pre-MLP RMS normalization to the result of the first residual connection.",
                        "Pass the normalized result through the MLP block.",
                        "Add the result of the first residual connection to the MLP output (second residual connection).",
                        "Apply dropout to the final result.",
                        "Optionally record internal activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "flax.linen.with_logical_constraint",
                        "jax.ad_checkpoint.checkpoint_name"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is active.",
                        "If `config.scan_layers` is True, it returns a tuple `(output_tensor, None)` for compatibility with `flax.linen.scan`.",
                        "It takes `decoder_segment_ids` and `decoder_positions` as inputs for the attention mechanism."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma2.py#Gemma2DecoderLayer",
        "file_path": "src/MaxText/layers/gemma2.py",
        "code_block": "class Gemma2DecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: Optional[Quant] = None,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_norm_local = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.self_attention_local = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=self.mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        attention_type=attentions.AttentionType.LOCAL_SLIDING,\n        sliding_window_size=config.sliding_window_size,\n        attn_logits_soft_cap=config.attn_logits_soft_cap,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_attn_norm:\n      self.post_self_attention_norm_local = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    self.pre_ffw_norm_local = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp_local = MlpBlock(\n        config=config,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_ffw_norm:\n      self.post_ffw_norm_local = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n\n    self.pre_self_attention_norm_global = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.self_attention_global = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=self.mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=True,\n        float32_logits=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        attention_type=attentions.AttentionType.GLOBAL,\n        attn_logits_soft_cap=config.attn_logits_soft_cap,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_attn_norm:\n      self.post_self_attention_norm_global = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    self.pre_ffw_norm_global = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp_global = MlpBlock(\n        config=config,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_ffw_norm:\n      self.post_ffw_norm_global = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = self.pre_self_attention_norm_local(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention_local(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if self.config.use_post_attn_norm:\n      attention_lnx = self.post_self_attention_norm_local(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm_local(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = self.mlp_local(attn_output, deterministic=deterministic)\n\n    if self.config.use_post_ffw_norm:\n      mlp_lnx = self.post_ffw_norm_local(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    ### global part\n    inputs = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = self.pre_self_attention_norm_global(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention_global(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if self.config.use_post_attn_norm:\n      attention_lnx = self.post_self_attention_norm_global(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm_global(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = self.mlp_global(attn_output, deterministic=deterministic)\n    if self.config.use_post_ffw_norm:\n      mlp_lnx = self.post_ffw_norm_global(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma2_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for the Gemma2 model, which sequentially applies a local (sliding window) attention block and a global attention block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes local and global self-attention modules.",
                "Initializes local and global MLP blocks.",
                "Initializes RMS normalization layers for pre-attention, post-attention (optional), pre-MLP, and post-MLP (optional) stages for both local and global blocks.",
                "Initializes a dropout layer.",
                "The forward pass (`__call__`) processes input through the local attention/MLP block and then passes that output through the global attention/MLP block."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "max_utils",
                "quantizations"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dimensions, dropout rates, attention types, and layer settings.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill' or 'autoregressive'.",
                "quant": "Optional quantization configuration for weights and activations."
            },
            "notes": [
                "This layer is specific to the Gemma2 architecture.",
                "It uniquely combines a local sliding window attention mechanism with a global attention mechanism within a single layer.",
                "The processing pipeline is: local attention -> local MLP -> global attention -> global MLP, with residual connections and normalizations at each stage.",
                "The use of post-attention and post-MLP normalization is conditional, controlled by `config.use_post_attn_norm` and `config.use_post_ffw_norm` respectively."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes all sub-modules for the decoder layer, including separate sets of attention, MLP, and normalization layers for both the local and global processing paths.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, model_mode, quant, and rngs.",
                        "Initialize `pre_self_attention_norm_local` and `self_attention_local` (with LOCAL_SLIDING type).",
                        "Initialize `pre_ffw_norm_local` and `mlp_local`.",
                        "Initialize `dropout` layer.",
                        "Initialize `pre_self_attention_norm_global` and `self_attention_global` (with GLOBAL type).",
                        "Initialize `pre_ffw_norm_global` and `mlp_global`.",
                        "Conditionally initialize post-attention and post-MLP normalization layers based on config.",
                        "Set `activation_axis_names` for logical partitioning based on `model_mode`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "Two distinct sets of attention and MLP blocks are created: one for local processing and one for global processing."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer by first processing inputs through the local block (attention and MLP) and then passing the result through the global block (attention and MLP).",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, emb_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "inputs: config.dtype, others: typically integer"
                    },
                    "processing_steps": [
                        "Process inputs through the local block: pre-norm -> self-attention -> optional post-norm -> residual connection.",
                        "Process the result through the local MLP block: pre-norm -> MLP -> optional post-norm -> residual connection -> dropout.",
                        "Take the output of the local block as input to the global block.",
                        "Process inputs through the global block: pre-norm -> self-attention -> optional post-norm -> residual connection.",
                        "Process the result through the global MLP block: pre-norm -> MLP -> optional post-norm -> residual connection -> dropout.",
                        "Optionally record internal activation metrics.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The `deterministic` argument controls whether dropout is applied.",
                        "If `config.scan_layers` is true, returns a tuple `(layer_output, None)` for compatibility with `flax.linen.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_attention_type",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_attention_type(layer_id):\n  layer_id %= len(GEMMA3_ATTENTION_PATTERN)\n  return GEMMA3_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Determines the attention type for a given layer ID based on a repeating pattern.",
            "input": {
                "shape": "Scalar",
                "dtype": "int"
            },
            "processing_steps": [
                "Calculate the layer ID modulo the length of the `GEMMA3_ATTENTION_PATTERN` tuple.",
                "Return the attention type from `GEMMA3_ATTENTION_PATTERN` at the calculated index."
            ],
            "output": {
                "shape": "Scalar (AttentionType enum member)"
            },
            "dependencies": [
                "GEMMA3_ATTENTION_PATTERN",
                "AttentionType"
            ],
            "parameters": {},
            "notes": [
                "This function implements a cyclic attention pattern where the type of attention (e.g., LOCAL_SLIDING, GLOBAL) repeats every 6 layers as defined by `GEMMA3_ATTENTION_PATTERN`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_query_pre_attn_scalar",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_query_pre_attn_scalar(config) -> float:\n  \"\"\"Returns the scalar to multiply the query by before attention.\"\"\"\n  if config.model_name in [\"gemma3-4b\", \"gemma3-12b\"]:\n    return config.head_dim**-0.5\n  elif config.model_name == \"gemma3-27b\":\n    return (config.base_emb_dim // config.base_num_query_heads) ** -0.5\n  else:\n    raise ValueError(f\"Unsupported model name: {config.model_name}\")",
        "analysis": {
            "module_type": "attention_scalar_calculator",
            "purpose": "Calculates and returns a model-specific scalar value used to scale the query tensor before the attention mechanism.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the `model_name` attribute from the input `config` object.",
                "If the model is 'gemma3-4b' or 'gemma3-12b', calculate the scalar as the inverse square root of `config.head_dim`.",
                "If the model is 'gemma3-27b', calculate the scalar as the inverse square root of `config.base_emb_dim` divided by `config.base_num_query_heads`.",
                "If the model name is not supported, raise a ValueError.",
                "Return the calculated float scalar."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [
                "Config"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters like `model_name`, `head_dim`, `base_emb_dim`, and `base_num_query_heads`."
            },
            "notes": [
                "The calculation of the scalar is conditional on the `config.model_name`.",
                "This function is a helper used to configure the `Attention` layer."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3DecoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3DecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer for Gemma3.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      attention_type: AttentionType = AttentionType.LOCAL_SLIDING,\n  ):\n    \"\"\"Initializes the Gemma3DecoderLayer.\n\n    Args:\n      config: The Config object with model hyperparameters.\n      mesh: The device mesh for distributed training.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: The random number generators for initialization.\n      quant: The quantization configuration.\n      attention_type: The type of attention to use.\n    \"\"\"\n\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.rngs = rngs\n    self.attention_type = attention_type\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    query_pre_attn_scalar = get_query_pre_attn_scalar(config)\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        attention_type=self.attention_type,\n        sliding_window_size=config.sliding_window_size,\n        attn_logits_soft_cap=config.attn_logits_soft_cap,\n        use_qk_norm=True,  # Gemma 3 models use query, key normalizations\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    if self.config.use_post_attn_norm:\n      self.post_self_attention_norm = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.post_self_attention_norm = None\n\n    self.pre_ffw_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        quant=self.quant,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    if self.config.use_post_ffw_norm:\n      self.post_ffw_norm = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.post_ffw_norm = None\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n      bidirectional_mask=None,\n  ):\n    cfg = self.config\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    lnx = self.pre_self_attention_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    # Self-attention block\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        bidirectional_mask=bidirectional_mask,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = self.post_self_attention_norm(attention_lnx)\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = self.mlp(attn_output, deterministic=deterministic)\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = self.post_ffw_norm(mlp_lnx)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma3_decoder_layer",
            "purpose": "Represents a single transformer decoder layer for the Gemma3 model, encapsulating self-attention and a feed-forward network with normalization and residual connections.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes sub-modules: RMSNorm for normalization, Attention for self-attention, MlpBlock for the feed-forward network, and Dropout for regularization.",
                "Conditionally creates post-attention and post-feed-forward normalization layers based on the configuration.",
                "The `__call__` method defines the forward pass, applying the sub-modules in a pre-normalization transformer block architecture."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "max_utils.get_batch_seq_len_for_mode",
                "get_query_pre_attn_scalar",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "quantizations"
            ],
            "parameters": {
                "config": "The Config object containing model hyperparameters like embedding dimension, number of heads, MLP dimension, dropout rate, etc.",
                "mesh": "The device mesh for distributed training and model sharding.",
                "model_mode": "A string specifying the operational mode, which can be 'train', 'prefill', or 'autoregressive'.",
                "attention_type": "The type of attention mechanism to use, such as LOCAL_SLIDING or GLOBAL."
            },
            "notes": [
                "This class is an nnx.Module, part of the Flax NNX API.",
                "The layer structure follows a pre-normalization pattern (Norm -> Attention -> Add -> Norm -> MLP -> Add).",
                "Post-normalization for attention and MLP blocks is optional and controlled by `config.use_post_attn_norm` and `config.use_post_ffw_norm` respectively."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the sub-modules of the Gemma3 decoder layer, including normalization, attention, and MLP blocks.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration objects (config, mesh, quant, rngs, attention_type).",
                        "Initialize `pre_self_attention_norm` as an RMSNorm layer.",
                        "Calculate `query_pre_attn_scalar` using a helper function.",
                        "Initialize the `self_attention` block with detailed configuration.",
                        "Conditionally initialize `post_self_attention_norm` if `config.use_post_attn_norm` is true.",
                        "Initialize `pre_ffw_norm` as an RMSNorm layer.",
                        "Initialize the `mlp` block.",
                        "Conditionally initialize `post_ffw_norm` if `config.use_post_ffw_norm` is true.",
                        "Initialize the `dropout` layer.",
                        "Set `activation_axis_names` for logical partitioning based on `model_mode`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_utils.get_batch_seq_len_for_mode",
                        "get_query_pre_attn_scalar",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "The specific configuration of sub-modules is determined by the `config` and `model_mode` arguments."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying self-attention and a feed-forward network with residual connections.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, emb_dim]",
                        "dtype": "config.dtype"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Compute self-attention on the normalized input.",
                        "Optionally apply post-attention RMS normalization.",
                        "Add the original input to the attention output (first residual connection).",
                        "Apply pre-feed-forward RMS normalization to the result of the first residual connection.",
                        "Pass the result through the MLP block.",
                        "Optionally apply post-feed-forward RMS normalization.",
                        "Add the result from the first residual connection to the MLP output (second residual connection).",
                        "Apply dropout to the final result.",
                        "Optionally record internal activation metrics via `self.sow`."
                    ],
                    "output": {
                        "shape": "If config.scan_layers is False: [batch_size, sequence_length, emb_dim]. If True: a tuple containing the tensor and None."
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "jax.ad_checkpoint.checkpoint_name",
                        "self.pre_self_attention_norm",
                        "self.self_attention",
                        "self.post_self_attention_norm",
                        "self.pre_ffw_norm",
                        "self.mlp",
                        "self.post_ffw_norm",
                        "self.dropout"
                    ],
                    "notes": [
                        "The method uses a pre-normalization architecture.",
                        "It takes additional arguments like `decoder_segment_ids` and `decoder_positions` for the attention mechanism.",
                        "The `deterministic` flag controls whether dropout is applied."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3ScannableBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3ScannableBlock(nnx.Module):\n  \"\"\"A repeatable block of Gemma3 decoder layers.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      num_of_layers: int = 1,\n  ):\n    \"\"\"Initializes the Gemma3ScannableBlock.\n\n    Args:\n      config: The Config object with model hyperparameters.\n      mesh: The device mesh for distributed training.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: The random number generators for initialization.\n      quant: The quantization configuration.\n      num_of_layers: The number of layers in the model.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n    self.num_of_layers = num_of_layers\n\n    for layer_id in range(self.num_of_layers):\n      attention_type = get_attention_type(layer_id)\n      layer_name = f\"layers_{layer_id}\"\n      layer = Gemma3DecoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          model_mode=self.model_mode,\n          rngs=self.rngs,\n          quant=self.quant,\n          attention_type=attention_type,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot=None,\n      page_state=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n  ):\n\n    cfg = self.config\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n\n    for layer_id in range(self.num_of_layers):\n      y = getattr(self, f\"layers_{layer_id}\")(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          bidirectional_mask=bidirectional_mask,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gemma3_scannable_block",
            "purpose": "A container module that sequentially applies a specified number of Gemma3 decoder layers to an input tensor.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "Gemma3DecoderLayer",
                "get_attention_type",
                "jax.ad_checkpoint.checkpoint_name",
                "flax.linen.with_logical_constraint"
            ],
            "parameters": {
                "num_of_layers": "The number of `Gemma3DecoderLayer` instances to create and apply.",
                "config.scan_layers": "A boolean flag that controls the execution flow and return format, enabling layer scanning optimizations."
            },
            "notes": [
                "This class dynamically creates `num_of_layers` instances of `Gemma3DecoderLayer` during initialization.",
                "The attention type for each internal `Gemma3DecoderLayer` is determined by its layer index using the `get_attention_type` function, which cycles through the `GEMMA3_ATTENTION_PATTERN`.",
                "The module's behavior, particularly its return value in the `__call__` method, changes based on the `config.scan_layers` setting."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the block by creating a specified number of `Gemma3DecoderLayer` instances.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (`config`, `mesh`, `model_mode`, etc.).",
                        "Iterate from `layer_id` 0 to `num_of_layers - 1`.",
                        "Determine the `attention_type` for the current layer using `get_attention_type(layer_id)`.",
                        "Instantiate a `Gemma3DecoderLayer` with the determined `attention_type` and other configurations.",
                        "Assign the created layer as an attribute to the class instance with a name like `layers_{layer_id}`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Gemma3DecoderLayer",
                        "get_attention_type"
                    ],
                    "notes": [
                        "The method dynamically adds attributes named `layers_{layer_id}` to the instance."
                    ]
                },
                "__call__": {
                    "purpose": "Processes the input tensor by passing it sequentially through each of the contained `Gemma3DecoderLayer`s.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "Depends on `config.dtype`"
                    },
                    "processing_steps": [
                        "Apply a logical constraint and a checkpoint name to the input tensor.",
                        "Initialize an intermediate tensor `y` with the input tensor.",
                        "Iterate from `layer_id` 0 to `num_of_layers - 1`.",
                        "Retrieve the corresponding `Gemma3DecoderLayer` instance using `getattr`.",
                        "Pass the intermediate tensor `y` and other arguments through the layer.",
                        "If `config.scan_layers` is true, unpack the layer's output tuple to update `y`.",
                        "After the loop, if `config.scan_layers` is true, return `(y, None)`.",
                        "Otherwise, return `y`."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is false: [batch_size, sequence_length, embedding_dim]. If true, it's the first element of a tuple with this shape."
                    },
                    "dependencies": [
                        "Gemma3DecoderLayer"
                    ],
                    "notes": [
                        "The return signature changes based on the `config.scan_layers` flag. This is to support JAX's `scan` transformation for memory efficiency.",
                        "It forwards all relevant arguments like `decoder_segment_ids`, `decoder_positions`, etc., to each internal layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#_posemb_sincos_2d",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def _posemb_sincos_2d(\n    h: int,\n    w: int,\n    *,\n    width: int,\n    temperature: float = 10_000.0,\n    precision: str = \"default\",\n    dtype: jnp.dtype = jnp.float32,\n):\n  \"\"\"Follows the MoCo v3 logic.\"\"\"\n  y, x = jnp.mgrid[:h, :w]  # pylint: disable=unpacking-non-sequence\n\n  assert width % 4 == 0, \"Width must be mult of 4 for sincos posemb\"\n  omega = jnp.arange(width // 4) / (width // 4 - 1)\n  omega = 1.0 / (temperature**omega)\n  y = jnp.einsum(\"m,d->md\", y.flatten(), omega, precision=precision)\n  x = jnp.einsum(\"m,d->md\", x.flatten(), omega, precision=precision)\n  pe = jnp.concatenate([jnp.sin(x), jnp.cos(x), jnp.sin(y), jnp.cos(y)], axis=1)\n  return jnp.asarray(pe, dtype)[None, :, :]",
        "analysis": {
            "module_type": "sinusoidal_positional_embedding_2d",
            "purpose": "Generates a 2D sinusoidal positional embedding for a grid of specified height and width, following the MoCo v3 logic.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a 2D grid of coordinates (y, x) of shape [h, w] using jnp.mgrid.",
                "Calculate frequency values 'omega' based on the embedding 'width' and 'temperature'.",
                "Apply frequencies to flattened x and y coordinates using jnp.einsum.",
                "Compute sine and cosine of the transformed x and y coordinates.",
                "Concatenate sin(x), cos(x), sin(y), and cos(y) along the feature dimension.",
                "Add a leading batch dimension to the final embedding tensor."
            ],
            "output": {
                "shape": "[1, h * w, width]"
            },
            "dependencies": [
                "jax.numpy"
            ],
            "parameters": {
                "h": "Height of the 2D grid.",
                "w": "Width of the 2D grid.",
                "width": "The total dimension of the positional embedding vector.",
                "temperature": "A scaling factor for the frequencies, defaults to 10,000.0.",
                "precision": "Precision for the einsum operation.",
                "dtype": "The data type of the output tensor."
            },
            "notes": [
                "This is a private helper function, indicated by the leading underscore.",
                "The implementation asserts that the 'width' parameter must be a multiple of 4.",
                "The output tensor is explicitly cast to the specified dtype."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#MlpBlockViT",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class MlpBlockViT(nnx.Module):\n  \"\"\"NNX version of Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.block_id = block_id\n    self.rngs = rngs\n\n    self.Dense_0 = DenseGeneral(\n        in_features_shape=self.config.hidden_size_for_vit,\n        out_features_shape=self.config.intermediate_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(rate=self.config.dropout_rate)\n    self.Dense_1 = DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    \"\"\"Applies the Transformer MlpBlock module.\"\"\"\n    x = self.Dense_0(x)\n    x = nnx.gelu(x)\n    x = self.Dropout_0(x, deterministic=deterministic)\n    x = self.Dense_1(x)\n    return x",
        "analysis": {
            "module_type": "vision_transformer_mlp_block",
            "purpose": "Implements a standard feed-forward network (MLP) block for a Vision Transformer, consisting of two dense layers with a GELU activation and dropout in between.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "DenseGeneral",
                "nnx.Dropout",
                "Config",
                "nnx.Rngs"
            ],
            "parameters": {
                "config.hidden_size_for_vit": "The input and output feature dimension of the MLP block.",
                "config.intermediate_size_for_vit": "The feature dimension of the intermediate layer.",
                "config.dropout_rate": "The dropout rate applied after the GELU activation.",
                "config.dtype_mm": "The data type for matrix multiplications within the dense layers.",
                "config.matmul_precision": "The precision for matrix multiplications."
            },
            "notes": [
                "This is an implementation using the Flax NNX API.",
                "The `block_id` parameter is stored but not used in the forward pass."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLP block, setting up two dense layers and a dropout layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes self.Dense_0 (first dense layer).",
                        "Initializes self.Dropout_0.",
                        "Initializes self.Dense_1 (second dense layer)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "nnx.Dropout",
                        "Config",
                        "nnx.Rngs"
                    ],
                    "notes": [
                        "The layers are configured based on parameters from the `config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the MLP transformation to an input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Apply the first dense layer (self.Dense_0).",
                        "Apply GELU activation (nnx.gelu).",
                        "Apply dropout (self.Dropout_0), conditional on the 'deterministic' flag.",
                        "Apply the second dense layer (self.Dense_1)."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "jax.Array",
                        "nnx.gelu"
                    ],
                    "notes": [
                        "The output shape is the same as the input shape.",
                        "Dropout is disabled when 'deterministic' is True."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder1DBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder1DBlock(nnx.Module):\n  \"\"\"Single transformer encoder block (MHSA + MLP).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.block_id = block_id\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.seq_len = (self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2\n\n    self.LayerNorm_0 = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.MultiHeadDotProductAttention_0 = Attention(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=self.seq_len,\n        float32_qk_product=self.config.float32_qk_product,\n        float32_logits=self.config.float32_logits,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        mesh=self.mesh,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        inputs_kv_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        dropout_rate=0,\n        is_nope_layer=True,\n        use_bias_in_projections=True,\n        attention_type=AttentionType.FULL,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / (self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit) ** 0.5,\n        model_mode=\"train\",\n        is_vision=True,\n        rngs=self.rngs,\n    )\n    self.LayerNorm_1 = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.MlpBlockViT_0 = MlpBlockViT(\n        block_id=self.block_id,\n        config=self.config,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, deterministic: bool = False) -> jax.Array:\n    y = self.LayerNorm_0(x)\n\n    y = self.MultiHeadDotProductAttention_0(inputs_q=y, inputs_kv=y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n\n    y = self.LayerNorm_1(x)\n    y = self.MlpBlockViT_0(y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n    return x",
        "analysis": {
            "module_type": "vision_transformer_encoder_block",
            "purpose": "Implements a single transformer encoder block for a Vision Transformer (ViT), consisting of a multi-head self-attention layer followed by a feed-forward MLP block, with residual connections and layer normalization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the sequence length based on image and patch sizes from the config.",
                "Initializes a LayerNorm layer (self.LayerNorm_0).",
                "Initializes a MultiHeadDotProductAttention layer (self.MultiHeadDotProductAttention_0).",
                "Initializes a second LayerNorm layer (self.LayerNorm_1).",
                "Initializes an MlpBlockViT layer (self.MlpBlockViT_0).",
                "Initializes a Dropout layer (self.Dropout_0)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.LayerNorm",
                "Attention",
                "MlpBlockViT",
                "nnx.Dropout",
                "jax"
            ],
            "parameters": {
                "config": "Configuration object containing hyperparameters for the ViT encoder block, such as hidden size, number of attention heads, and dropout rate.",
                "mesh": "The JAX device mesh for distributed computation.",
                "block_id": "An integer identifier for the specific encoder block.",
                "rngs": "An nnx.Rngs object for managing random number generation for initialization and dropout."
            },
            "notes": [
                "This class implements a standard pre-normalization (Pre-LN) transformer block architecture.",
                "The attention mechanism is configured specifically for a vision model, as indicated by parameters like `is_vision=True` and `is_nope_layer=True`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the transformer encoder block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Apply pre-attention layer normalization to the input tensor `x`.",
                        "Pass the normalized tensor through the multi-head self-attention layer.",
                        "Apply dropout to the attention output.",
                        "Add the result to the original input tensor via a residual connection.",
                        "Apply pre-MLP layer normalization to the result of the first residual connection.",
                        "Pass the normalized tensor through the MLP block.",
                        "Apply dropout to the MLP output.",
                        "Add the result to the tensor from the first residual connection via a second residual connection.",
                        "Return the final output tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "self.LayerNorm_0",
                        "self.MultiHeadDotProductAttention_0",
                        "self.Dropout_0",
                        "self.LayerNorm_1",
                        "self.MlpBlockViT_0"
                    ],
                    "notes": [
                        "The `deterministic` boolean argument controls whether the dropout layers are active (False) or bypassed (True)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder(nnx.Module):\n  \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"encoderblock_{lyr}\"\n      layer = Encoder1DBlock(\n          block_id=lyr,\n          config=self.config,\n          mesh=self.mesh,\n          rngs=self.rngs,\n      )\n      setattr(self, layer_name, layer)\n    self.encoder_norm = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    # TODO(aireenmei, hengtaoguo): add if-scan branch to enable scan support for vision encoder\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      x = getattr(self, f\"encoderblock_{lyr}\")(x, deterministic=deterministic)\n    x = self.encoder_norm(x)\n    return x",
        "analysis": {
            "functionality": "The `Encoder` class implements a standard Transformer Encoder architecture. It consists of a stack of `Encoder1DBlock` layers followed by a final Layer Normalization. This module processes a sequence of embeddings, applying self-attention and feed-forward networks repeatedly to produce a refined sequence of embeddings of the same shape.",
            "usage": "To use this class, instantiate it with a configuration object (`Config`), a JAX mesh, and random number generators (`nnx.Rngs`). Then, call the instance with an input tensor of shape `[batch_size, sequence_length, hidden_dim]` and a boolean `deterministic` flag. The module will return an output tensor of the same shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Einsum",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Einsum(nnx.Module):\n  \"\"\"Einsum is a convenience module for parameterized tensor multiplication.\"\"\"\n\n  def __init__(\n      self,\n      shape: tuple[int, ...],\n      initializer: nnx.initializers.Initializer = nnx.initializers.normal(),\n      dtype: jnp.dtype | None = None,\n      precision: str = \"default\",\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.precision = precision\n    self.w = nnx.Param(initializer(rngs.params(), shape, dtype))\n\n  def __call__(self, eqn: str, x: jax.Array) -> jax.Array:\n    return jnp.einsum(eqn, x, self.w, precision=self.precision)",
        "analysis": {
            "module_type": "parameterized_einsum",
            "purpose": "A convenience module for performing a tensor multiplication (einsum) with a learnable weight parameter.",
            "input": {
                "shape": "N/A (Handled by the __call__ method)",
                "dtype": "N/A (Handled by the __call__ method)"
            },
            "processing_steps": [
                "Initializes a learnable weight parameter 'w' with a specified shape."
            ],
            "output": {
                "shape": "N/A (Handled by the __call__ method)"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.initializers.Initializer",
                "jnp.einsum"
            ],
            "parameters": {
                "shape": "The shape of the learnable weight tensor 'w'.",
                "initializer": "The initializer function for the weight tensor.",
                "dtype": "The data type of the weight tensor.",
                "precision": "The precision for the einsum computation (e.g., 'default')."
            },
            "notes": [
                "This module encapsulates a learnable weight tensor and applies it to an input tensor using the flexible Einstein summation convention, defined by an equation string provided during the forward pass."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module's precision and the learnable weight parameter 'w'.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the `precision` value.",
                        "Initializes a learnable parameter `self.w` of the given `shape` and `dtype` using the provided `initializer` and random number generator."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.Initializer"
                    ],
                    "notes": [
                        "The `rngs` argument is required to get a 'params' key for the initializer."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the Einstein summation operation between an input tensor `x` and the internal weight `self.w`.",
                    "input": {
                        "shape": "[...]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Calls `jnp.einsum` with the provided equation string `eqn`, the input tensor `x`, the internal weight `self.w`, and the configured `precision`."
                    ],
                    "output": {
                        "shape": "Determined by the einsum equation string `eqn` and the shapes of the input `x` and the weight `w`."
                    },
                    "dependencies": [
                        "jnp.einsum"
                    ],
                    "notes": [
                        "The first argument `eqn` is a string that defines the tensor contraction, e.g., '...tm,md->...td'."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionEmbedder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionEmbedder(nnx.Module):\n  \"\"\"Projects image embeddings to the embedding space of the text encoder.\"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.mm_soft_embedding_norm = RMSNorm(\n        num_features=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.mm_input_projection = Einsum(\n        shape=(self.config.hidden_size_for_vit, self.config.emb_dim),\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, eqn: str = \"...tm,md->...td\") -> jax.Array:\n    x = self.mm_soft_embedding_norm(x)\n    x = self.mm_input_projection(eqn, x)\n    return x",
        "analysis": {
            "module_type": "vision_embedder",
            "purpose": "Projects image embeddings from the vision transformer's hidden space to the text encoder's embedding space.",
            "input": {
                "shape": "[..., num_tokens, hidden_size_for_vit]",
                "dtype": "Matches config.dtype_mm"
            },
            "processing_steps": [
                "Applies RMS normalization to the input image embeddings.",
                "Projects the normalized embeddings to the text encoder's embedding dimension using a parameterized einsum operation."
            ],
            "output": {
                "shape": "[..., num_tokens, emb_dim]"
            },
            "dependencies": [
                "RMSNorm",
                "Einsum",
                "nnx.Module"
            ],
            "parameters": {
                "config.hidden_size_for_vit": "The hidden size of the vision transformer.",
                "config.emb_dim": "The embedding dimension of the text encoder.",
                "config.matmul_precision": "The precision for the einsum matrix multiplication.",
                "config.dtype_mm": "The data type for multi-modal operations."
            },
            "notes": [
                "This module acts as a bridge between a vision encoder and a text model."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the VisionEmbedder module, setting up the normalization and projection layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes an `RMSNorm` layer for input normalization.",
                        "Initializes an `Einsum` layer with a learnable weight matrix for projection."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "RMSNorm",
                        "Einsum"
                    ],
                    "notes": [
                        "The projection weight shape is `(config.hidden_size_for_vit, config.emb_dim)`."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass, normalizing and projecting the input image embeddings.",
                    "input": {
                        "shape": "[..., num_tokens, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Apply `self.mm_soft_embedding_norm` to the input tensor `x`.",
                        "Apply `self.mm_input_projection` using the provided einsum equation (`eqn`) to project the normalized tensor."
                    ],
                    "output": {
                        "shape": "[..., num_tokens, emb_dim]"
                    },
                    "dependencies": [
                        "self.mm_soft_embedding_norm",
                        "self.mm_input_projection"
                    ],
                    "notes": [
                        "The default einsum equation `...tm,md->...td` performs a matrix multiplication on the last two dimensions of the input."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#visionembedder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def visionembedder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a VisionEmbedder module.\"\"\"\n  return nnx_wrappers.to_linen(\n      VisionEmbedder,\n      config,\n      mesh=mesh,\n      name=\"VisionEmbedder_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "vision_embedder_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible version of the NNX `VisionEmbedder` module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Wraps the `VisionEmbedder` NNX module using `nnx_wrappers.to_linen`.",
                "Passes the `config` and `mesh` arguments to the `VisionEmbedder` constructor during wrapping.",
                "Sets the name of the resulting Linen module to 'VisionEmbedder_0'.",
                "Specifies `variable_to_logically_partitioned` as the metadata function for parameter partitioning."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "VisionEmbedder",
                "initializers.variable_to_logically_partitioned",
                "Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "The Config object containing model hyperparameters, which is passed to the VisionEmbedder.",
                "mesh": "The JAX device mesh for distributed computation, passed to the VisionEmbedder."
            },
            "notes": [
                "This function serves as a bridge to allow an NNX-defined module (`VisionEmbedder`) to be used within a larger Flax Linen-based model architecture.",
                "The `abstract_init=False` argument indicates a specific initialization strategy for the NNX to Linen conversion.",
                "The `metadata_fn` is crucial for setting up correct parameter sharding in a distributed environment."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionExit",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionExit(nnx.Module):\n  \"\"\"The vision exit layer.\n\n  Possibly downsample the soft tokens to a required output length.\n\n  Attributes:\n    output_length: The embed will be spatially avg-pooled to this output length.\n  \"\"\"\n\n  def __init__(self, output_length: int = 256, *, rngs: nnx.Rngs):\n    self.output_length = output_length\n    self.rngs = rngs\n\n  def __call__(self, x):\n    cur_length = x.shape[1]\n    if cur_length == self.output_length:\n      return x\n    cur_width = int(cur_length**0.5)\n    assert cur_width**2 == cur_length\n    output_width = int(self.output_length**0.5)\n    assert output_width**2 == self.output_length, f\"Cannot pool {x.shape=} to {self.output_length}=!\"\n    batch_size = x.shape[0]\n    embed_dim = x.shape[-1]\n    x = jnp.reshape(x, (batch_size, cur_width, cur_width, embed_dim))\n    assert not cur_width % output_width, f\"{cur_width=} {output_width=}\"\n    window = cur_width // output_width\n    window_shape = (window, window)\n    x = nnx.avg_pool(x, window_shape=window_shape, strides=window_shape)\n    batch_size, height, width, embed_dim = x.shape\n    return jnp.reshape(x, (batch_size, height * width, embed_dim))",
        "analysis": {
            "module_type": "vision_exit_layer",
            "purpose": "A module that downsamples a sequence of vision tokens to a specified output length by reshaping them into a 2D grid and applying average pooling.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the module with a target `output_length`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.avg_pool",
                "jax.numpy"
            ],
            "parameters": {
                "output_length": "The target sequence length for the output tokens after downsampling. Defaults to 256."
            },
            "notes": [
                "This layer assumes that both the input sequence length and the target `output_length` are perfect squares, as it treats the sequence as a flattened 2D grid.",
                "The side length of the input grid must be perfectly divisible by the side length of the output grid."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies 2D average pooling to downsample the input tensor if its sequence length does not match the target `output_length`.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embed_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Check if the input sequence length equals `output_length`; if so, return the input directly.",
                        "Calculate the side lengths of the input and output grids by taking the square root of their respective lengths.",
                        "Reshape the input tensor from `[batch_size, sequence_length, embed_dim]` to `[batch_size, cur_width, cur_width, embed_dim]`.",
                        "Calculate the pooling window size based on the ratio of input to output width.",
                        "Apply `nnx.avg_pool` with the calculated window shape and strides to downsample the tensor.",
                        "Reshape the pooled tensor back to `[batch_size, output_length, embed_dim]` and return it."
                    ],
                    "output": {
                        "shape": "[batch_size, output_length, embed_dim]"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "nnx.avg_pool"
                    ],
                    "notes": [
                        "The method includes assertions to validate that input/output lengths are perfect squares and that pooling is possible (i.e., input width is divisible by output width)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#vision_exit_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def vision_exit_as_linen(x: jax.Array, output_length: int) -> jax.Array:\n  \"\"\"A wrapper to use VisionExit as a function.\"\"\"\n  return nnx.bridge.to_linen(VisionExit, output_length=output_length)(x)",
        "analysis": {
            "module_type": "vision_exit_wrapper",
            "purpose": "A functional wrapper that instantiates and applies the VisionExit module to an input tensor, effectively downsampling its sequence length.",
            "input": {
                "shape": "[batch_size, sequence_length, embed_dim]",
                "dtype": "jax.Array (e.g., float32)"
            },
            "processing_steps": [
                "Instantiates the `VisionExit` NNX module with the specified `output_length`.",
                "Converts the instantiated `VisionExit` module into a Linen-compatible function using `nnx.bridge.to_linen`.",
                "Calls the resulting function with the input tensor `x`."
            ],
            "output": {
                "shape": "[batch_size, output_length, embed_dim]"
            },
            "dependencies": [
                "jax.Array",
                "flax.nnx.bridge.to_linen",
                "VisionExit"
            ],
            "parameters": {
                "output_length": "The target sequence length to which the input tensor will be downsampled via average pooling."
            },
            "notes": [
                "This function provides a stateless, functional interface for the `VisionExit` NNX module, which is useful in contexts expecting pure functions like Linen's `nn.apply`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3VisionEncoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3VisionEncoderLayer(nnx.Module):\n  \"\"\"gemma 3 vision encoder layer\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.embedding = nnx.Conv(\n        in_features=self.config.num_channels_for_vit,\n        out_features=self.config.hidden_size_for_vit,\n        kernel_size=(self.config.patch_size_for_vit, self.config.patch_size_for_vit),\n        strides=self.config.conv_stride_for_vit,\n        padding=\"VALID\",\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.pos_embedding = self._get_posemb(\n        self.config.posemb_type_for_vit,\n        seqshape=(\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n        ),\n        width=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n    self.Transformer = Encoder(\n        config=self.config,\n        mesh=self.mesh,\n        rngs=self.rngs,\n    )\n    self.VisionExit = VisionExit(output_length=256, rngs=self.rngs)\n\n  def _get_posemb(\n      self,\n      typ: str,\n      *,\n      seqshape: tuple[int, int],\n      width: int,\n      dtype: jnp.dtype = jnp.float32,\n  ):\n    \"\"\"Returns the position embedding.\"\"\"\n    if typ == \"learn\":\n      shape = (1, seqshape[0] * seqshape[1], width)\n      initializer = nnx.initializers.normal(stddev=1 / (width**0.5))\n      return nnx.Param(initializer(self.rngs.params(), shape, dtype))\n    elif typ == \"sincos2d\":\n      return _posemb_sincos_2d(*seqshape, width=width, dtype=dtype, precision=self.config.matmul_precision)\n    else:\n      raise ValueError(f\"Unknown posemb type: {typ}\")\n\n  def __call__(self, inputs, deterministic, train=False):\n    \"\"\"ViT model that transforms image inputs to image embeddings.\n    Args:\n      inputs: jnp.array shaped [B, N, H, W, C], e.g. [4, 1, 896, 896, 3]\n    Returns:\n      jnp.array for image embeddings, shaped [B, N, P, D], e.g. [4, 1, 256, 1152]\n    \"\"\"\n    # currently only supports N=1, the inputs shape is [B, H, W, C]\n    if len(inputs.shape) == 4:\n      inputs = inputs[:, None, :]\n    b, n, h, w, c = inputs.shape\n    x = jnp.reshape(inputs, [b * n, h, w, c])\n    # Gemma3 uses conv2d with stride 14 and kernel size 14 to extract patches.\n    x = self.embedding(x)\n    bn, h, w, c = x.shape\n    x = jnp.reshape(x, [bn, h * w, c])\n\n    x = self.pos_embedding + x\n    x = self.Dropout_0(x)\n\n    # Transformer encoder to extract image features.\n    x = self.Transformer(x, deterministic=deterministic)\n\n    # Gemma3 use a vision exit layer to downsample the soft tokens to a required output length.\n    x = self.VisionExit(x)\n    bn, l, c = x.shape\n    x = jnp.reshape(x, [b, n, l, c])\n    return x",
        "analysis": {
            "module_type": "gemma3_vision_encoder_layer",
            "purpose": "A Vision Transformer (ViT) model that transforms image inputs into a sequence of image embeddings.",
            "input": {
                "shape": "[B, N, H, W, C] or [B, H, W, C]",
                "dtype": "jnp.float32 (inferred)"
            },
            "processing_steps": [
                "Reshapes the input image tensor to merge the batch (B) and sequence (N) dimensions.",
                "Applies a convolutional layer (`self.embedding`) to extract image patches.",
                "Reshapes the patches into a sequence.",
                "Adds positional embeddings (`self.pos_embedding`) to the patch sequence.",
                "Applies dropout.",
                "Processes the sequence through a Transformer Encoder (`self.Transformer`).",
                "Downsamples the output sequence to a fixed length using a VisionExit layer (`self.VisionExit`).",
                "Reshapes the final tensor to separate the batch (B) and sequence (N) dimensions."
            ],
            "output": {
                "shape": "[B, N, P, D] (e.g., [4, 1, 256, 1152])"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Conv",
                "nnx.Dropout",
                "Encoder",
                "VisionExit",
                "_posemb_sincos_2d"
            ],
            "parameters": {
                "config": "The Config object with model hyperparameters, such as image size, patch size, hidden dimensions, and positional embedding type.",
                "mesh": "The JAX device mesh for distributed training.",
                "rngs": "The random number generators for initialization and dropout."
            },
            "notes": [
                "This class encapsulates the entire vision encoding process, from raw images to a sequence of feature vectors.",
                "It supports both learnable and sinusoidal 2D positional embeddings, configured via `config.posemb_type_for_vit`.",
                "The output sequence length is downsampled to a fixed size (256) as defined in the `VisionExit` layer initialization."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the vision encoder layer, setting up submodules like convolutional embedding, positional embedding, Transformer encoder, and vision exit layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes `nnx.Conv` for patch embedding based on config parameters.",
                        "Calls `_get_posemb` to create positional embeddings (either 'learn' or 'sincos2d').",
                        "Initializes `nnx.Dropout` with the rate from the config.",
                        "Initializes the `Encoder` module.",
                        "Initializes the `VisionExit` module with a fixed output length of 256."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Conv",
                        "nnx.Dropout",
                        "Encoder",
                        "VisionExit",
                        "self._get_posemb"
                    ],
                    "notes": [
                        "All submodules are configured using parameters from the `config` object."
                    ]
                },
                "_get_posemb": {
                    "purpose": "Creates and returns the positional embedding based on the specified type ('learn' or 'sincos2d').",
                    "input": {
                        "shape": "N/A",
                        "dtype": "jnp.dtype"
                    },
                    "processing_steps": [
                        "Checks the `typ` argument.",
                        "If `typ` is 'learn', it creates a learnable `nnx.Param` with a normal initializer.",
                        "If `typ` is 'sincos2d', it calls the `_posemb_sincos_2d` function to generate fixed embeddings.",
                        "Raises a ValueError for unknown types."
                    ],
                    "output": {
                        "shape": "[1, seqshape[0] * seqshape[1], width]"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.normal",
                        "_posemb_sincos_2d"
                    ],
                    "notes": [
                        "This is a private helper method called during the initialization of the main class."
                    ]
                },
                "__call__": {
                    "purpose": "Transforms a batch of images into a sequence of embeddings.",
                    "input": {
                        "shape": "[B, N, H, W, C] or [B, H, W, C]",
                        "dtype": "jnp.float32 (inferred)"
                    },
                    "processing_steps": [
                        "Optionally adds a dimension `N` if the input shape is 4D.",
                        "Reshapes the input from [B, N, H, W, C] to [B*N, H, W, C].",
                        "Applies the convolutional embedding (`self.embedding`) to create patches.",
                        "Reshapes the patched output into a sequence of shape [B*N, H*W, C].",
                        "Adds the positional embedding (`self.pos_embedding`) to the sequence.",
                        "Applies dropout (`self.Dropout_0`).",
                        "Passes the sequence through the Transformer encoder (`self.Transformer`).",
                        "Downsamples the sequence length using the vision exit layer (`self.VisionExit`).",
                        "Reshapes the final output back to [B, N, L, C]."
                    ],
                    "output": {
                        "shape": "[B, N, P, D]"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "self.embedding",
                        "self.pos_embedding",
                        "self.Dropout_0",
                        "self.Transformer",
                        "self.VisionExit"
                    ],
                    "notes": [
                        "The method handles both 4D and 5D input tensors, assuming N=1 for 4D inputs.",
                        "The `deterministic` flag is passed to the Transformer encoder and dropout layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#gemma3visionencoder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def gemma3visionencoder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a Gemma3VisionEncoder module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      Gemma3VisionEncoderLayer,\n      config=config,\n      mesh=mesh,\n      name=\"Gemma3VisionEncoderLayer_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "vision_encoder_factory",
            "purpose": "Wraps an NNX `Gemma3VisionEncoderLayer` into a Flax Linen module for use in a Linen-based model architecture.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `Gemma3VisionEncoderLayer` NNX module into a Flax Linen module.",
                "Passes the `config` and `mesh` arguments to the `Gemma3VisionEncoderLayer` constructor during the conversion.",
                "Specifies the module name as 'Gemma3VisionEncoderLayer_0', sets `abstract_init=False`, and provides `variable_to_logically_partitioned` as the metadata function for parameter sharding.",
                "Returns the created Linen module instance."
            ],
            "output": {
                "shape": "A Flax Linen module instance that wraps the `Gemma3VisionEncoderLayer`."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Gemma3VisionEncoderLayer",
                "initializers.variable_to_logically_partitioned",
                "Config",
                "Mesh"
            ],
            "parameters": {
                "config": "A `Config` object containing model hyperparameters for the vision encoder.",
                "mesh": "A JAX `Mesh` object for distributed computation and model parallelism."
            },
            "notes": [
                "This function acts as a factory or bridge, enabling the use of an NNX-defined module within a Flax Linen ecosystem.",
                "The `metadata_fn` (`variable_to_logically_partitioned`) is crucial for setting up the correct sharding for the module's parameters.",
                "Setting `abstract_init=False` means the module's parameters are initialized immediately upon creation, rather than waiting for the first call."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3LayerNorm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3LayerNorm(nnx.Module):\n  \"\"\"GPT3 Layer normalization operating on the last axis of the input data.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.zeros,\n      use_bias: bool = True,\n      reductions_in_fp32: bool = False,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.use_bias = use_bias\n    self.reductions_in_fp32 = reductions_in_fp32\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    self.scale = nnx.Param(\n        self.scale_init(rngs.params(), (num_features,), self.weight_dtype),\n        sharding=self.kernel_axes,\n    )\n    if self.use_bias:\n      self.bias = nnx.Param(\n          initializers.default_bias_init(rngs.params(), (num_features,), self.weight_dtype), sharding=self.kernel_axes\n      )\n    else:\n      self.bias = None\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    if self.reductions_in_fp32:\n      x = jnp.asarray(x, jnp.float32)\n    mean = jnp.mean(x, axis=[-1], keepdims=True)\n    var = jnp.mean(jnp.square(x - mean), axis=[-1], keepdims=True)\n    normed_inputs = (x - mean) * lax.rsqrt(var + self.epsilon)\n    if self.reductions_in_fp32:\n      normed_inputs = normed_inputs.astype(self.dtype)\n\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"gpt3.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    output = normed_inputs * (scale + 1)\n\n    if self.bias is not None:\n      bias = self.bias.value\n      bias = jnp.asarray(bias, self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "module_type": "gpt3_layer_norm",
            "purpose": "Implements a GPT-3 style layer normalization, which normalizes the input over the last dimension and applies a learned affine transformation.",
            "input": {
                "shape": "[..., num_features]",
                "dtype": "The dtype specified during initialization (e.g., jnp.float32)."
            },
            "processing_steps": [
                "Optionally cast input to float32 for reduction operations.",
                "Calculate mean and variance along the last axis of the input tensor.",
                "Normalize the input using the calculated mean, variance, and epsilon.",
                "Optionally cast the normalized input back to the original dtype.",
                "Apply a learned affine transformation using `output = normalized_input * (scale + 1) + bias`."
            ],
            "output": {
                "shape": "Same as input shape: [..., num_features]."
            },
            "dependencies": [
                "flax.nnx.Module",
                "flax.nnx.Param",
                "jax.numpy",
                "jax.lax",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, i.e., the size of the last dimension.",
                "epsilon": "A small float added to variance to avoid dividing by zero.",
                "dtype": "The data type of the computation.",
                "weight_dtype": "The data type of the learnable parameters (scale and bias).",
                "use_bias": "A boolean indicating whether to add a learnable bias term.",
                "reductions_in_fp32": "A boolean indicating whether to perform mean and variance calculations in float32 for improved precision.",
                "parameter_memory_host_offload": "A boolean indicating whether to offload parameters to host memory and move them to the device during the forward pass."
            },
            "notes": [
                "This implementation is specific to GPT-3, notably using `(scale + 1)` for scaling instead of just `scale`. The `scale` parameter is typically initialized to zeros.",
                "The normalization is always performed over the last axis of the input tensor."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the layer normalization module, setting up configuration and creating the learnable `scale` and optional `bias` parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like epsilon, dtype, etc.",
                        "Initialize the `scale` parameter as an `nnx.Param` using the provided `scale_init` function.",
                        "If `use_bias` is true, initialize the `bias` parameter as an `nnx.Param`.",
                        "If `use_bias` is false, set `self.bias` to None."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.Rngs",
                        "initializers.default_bias_init"
                    ],
                    "notes": [
                        "The `scale` parameter is initialized using `nn.initializers.zeros` by default."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the layer normalization to an input tensor.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Optionally cast input `x` to `jnp.float32` if `reductions_in_fp32` is true.",
                        "Calculate the mean of `x` along the last axis.",
                        "Calculate the variance of `x` along the last axis.",
                        "Normalize the input: `(x - mean) * lax.rsqrt(var + self.epsilon)`.",
                        "Optionally cast the normalized inputs back to `self.dtype`.",
                        "Retrieve the `scale` parameter value, potentially moving it from host to device.",
                        "Apply scaling: `output = normed_inputs * (scale + 1)`.",
                        "If `bias` exists, retrieve its value and add it to the output.",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "Same as input shape: [..., num_features]."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.lax",
                        "max_logging",
                        "max_utils"
                    ],
                    "notes": [
                        "The core logic of the layer normalization is executed in this method."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#gpt3_layer_norm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "def gpt3_layer_norm(\n    *,\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.zeros,\n    use_bias: bool = True,\n    reductions_in_fp32: bool = False,\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Initializes the gpt3_layer_norm module.\n\n  Args:\n    num_features: the number of features.\n    epsilon: the epsilon for the layer norm.\n    dtype: the dtype of the computation (default: float32).\n    weight_dtype: the dtype of the weights (default: float32).\n    kernel_axes: logical axes for partitioning the kernel.\n    scale_init: initializer for the scale.\n    use_bias: whether to add bias in linear transformation.\n    reductions_in_fp32: whether to do reductions in fp32.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n\n  module = nnx_wrappers.to_linen(\n      Gpt3LayerNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      use_bias=use_bias,\n      reductions_in_fp32=reductions_in_fp32,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "gpt3_layer_norm_factory",
            "purpose": "A factory function that initializes and returns a GPT-3 style Layer Normalization module compatible with Flax Linen by wrapping the `Gpt3LayerNorm` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `Gpt3LayerNorm` NNX module into a Flax Linen module.",
                "Passes all configuration parameters to the `Gpt3LayerNorm` constructor via the wrapper.",
                "Sets `initializers.variable_to_logically_partitioned` as the metadata function for parameter partitioning."
            ],
            "output": {
                "shape": "Returns a callable Flax Linen module."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Gpt3LayerNorm",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor to be normalized.",
                "epsilon": "A small float added to variance to avoid dividing by zero.",
                "dtype": "The data type for the computation.",
                "weight_dtype": "The data type for the scale and bias parameters.",
                "kernel_axes": "Logical axes for partitioning the kernel (scale and bias parameters).",
                "scale_init": "Initializer for the scale parameter.",
                "use_bias": "A boolean indicating whether to use a bias parameter.",
                "reductions_in_fp32": "A boolean indicating whether to perform the mean and variance calculations in float32 for stability.",
                "parameter_memory_host_offload": "A boolean to determine if parameters should be offloaded to host memory."
            },
            "notes": [
                "This function acts as a bridge between the NNX-defined `Gpt3LayerNorm` class and the broader Flax Linen-based model architecture.",
                "The returned object is a callable Linen module which, when called, will apply layer normalization to an input tensor."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3MultiHeadAttention",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3MultiHeadAttention(nn.Module):\n  \"\"\"Multi-head attention in gpt3.\n\n  Attributes:\n    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n      should be divisible by the number of heads.\n    head_dim: dimension of each head.\n    max_target_length: maximum length of output\n    max_prefill_predict_length: size of the maximum prefill\n    mesh: device mesh\n    dtype: the dtype of the computation.\n    dropout_rate: dropout rate\n    kernel_init: initializer for the kernel of the Dense layers.\n    float32_qk_product: bool, if True then compute logits via float32 qk_product to avoid\n      numerical issues with bfloat16.\n    float32_logits: bool, if True then cast logits to float32 before softmax to avoid\n      numerical issues with bfloat16.\n    fused_qkv: whether to fuse query, key and value into one projection.\n    quant: Quant, stores quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n  \"\"\"\n\n  config: Config\n  num_heads: int\n  head_dim: int\n  max_target_length: int\n  max_prefill_predict_length: int\n  mesh: Mesh\n  attention_kernel: str\n  dtype: DType = jnp.float32\n  weight_dtype: DType = jnp.float32\n  dropout_rate: float = 0.0\n  kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\")\n  float32_qk_product: bool = False  # computes logits in float32 for stability.\n  float32_logits: bool = True  # cast logits in float32 for stability.\n  fused_qkv: bool = True\n  quant: None | Quant = None\n  kv_quant: None | KVQuant = None\n  use_bias: bool = True\n\n  input_axis_names: AxisNames = (BATCH, LENGTH, EMBED)\n  query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  key_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  value_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  out_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(3, self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def projection(self, inputs: Array, proj_name: str) -> Array:\n    \"\"\"individual projection for one of q, k and v.\"\"\"\n    proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    return proj\n\n  def out_projection(self, output_dim: int, out: Array) -> Array:\n    \"\"\"output projection\"\"\"\n    out_proj = dense_general(\n        inputs_shape=out.shape,\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"heads\", \"kv\", \"embed\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=\"out\",\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(out)\n    return out_proj\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs_q: Array,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n  ):\n    inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n    if self.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.projection(inputs_q, proj_name=\"query\")\n      key = self.projection(inputs_q, proj_name=\"key\")\n      value = self.projection(inputs_q, proj_name=\"value\")\n\n    depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n    query /= depth_scaling\n\n    # annotate with sharding constraint.\n    query = nn.with_logical_constraint(query, self.query_axis_names)\n    query = checkpoint_name(query, \"query_proj\")\n    key = nn.with_logical_constraint(key, self.key_axis_names)\n    key = checkpoint_name(key, \"key_proj\")\n    value = nn.with_logical_constraint(value, self.value_axis_names)\n    value = checkpoint_name(value, \"value_proj\")\n\n    attention_op = attention_op_as_linen(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_heads,\n        num_kv_heads=self.num_heads,\n        dtype=self.dtype,\n    )\n\n    out = attention_op(query, key, value, decoder_segment_ids, model_mode)\n\n    out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    # apply output projection,  output dim is set to the input dim.\n    out = self.out_projection(inputs_q.shape[-1], out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "gpt3_multi_head_attention",
            "purpose": "Implements the multi-head self-attention mechanism as used in GPT-3 style models, including input projections, attention computation, and output projection.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Projects the input tensor `inputs_q` into query, key, and value tensors, either through a single fused projection or three separate projections.",
                "Scales the query tensor by the inverse square root of the head dimension.",
                "Computes the attention output by calling an external attention operation (`attention_op_as_linen`).",
                "Projects the attention output back to the original input embedding dimension."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.linears.dense_general",
                "maxtext.layers.attention_op.attention_op_as_linen",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "num_heads": "The number of attention heads.",
                "head_dim": "The dimension of each attention head.",
                "fused_qkv": "A boolean flag that determines whether to use a single fused projection for query, key, and value, or separate projections.",
                "attention_kernel": "A string specifying the backend attention implementation to use (e.g., 'dot_product').",
                "float32_qk_product": "If True, computes the query-key product in float32 for numerical stability.",
                "float32_logits": "If True, casts the attention logits to float32 before the softmax operation for numerical stability.",
                "quant": "Configuration for activation and weight quantization.",
                "use_bias": "A boolean flag indicating whether to include bias terms in the linear projections."
            },
            "notes": [
                "The core attention logic is delegated to `attention_op_as_linen`, making the implementation modular.",
                "The class uses `nn.with_logical_constraint` to apply sharding annotations for distributed training.",
                "Gradient checkpointing is applied to intermediate tensors (`qkv_proj`, `query_proj`, `key_proj`, `value_proj`, `out_proj`) to save memory."
            ],
            "methods": {
                "qkv_projection": {
                    "purpose": "Performs a fused projection of the input tensor to generate Query, Key, and Value tensors in a single operation.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calls `dense_general` to project the input into a shape of [batch_size, sequence_length, 3, num_heads, head_dim].",
                        "Applies `checkpoint_name` to the projected tensor.",
                        "Splits the projected tensor along the third dimension to get separate query, key, and value tensors."
                    ],
                    "output": {
                        "shape": "Tuple of three tensors, each of shape [batch_size, sequence_length, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "dense_general",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "This method is only called when the `fused_qkv` attribute is True."
                    ]
                },
                "projection": {
                    "purpose": "Performs an individual linear projection for one of the Query, Key, or Value tensors.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calls `dense_general` to project the input into a shape of [batch_size, sequence_length, num_heads, head_dim]."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "dense_general"
                    ],
                    "notes": [
                        "This method is called three times (for Q, K, and V) when the `fused_qkv` attribute is False."
                    ]
                },
                "out_projection": {
                    "purpose": "Projects the concatenated attention head outputs back to the input embedding dimension.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calls `dense_general` to project the input tensor, combining the `heads` and `kv` dimensions, to the specified output dimension."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, output_dim]"
                    },
                    "dependencies": [
                        "dense_general"
                    ],
                    "notes": [
                        "The `output_dim` is typically the original embedding dimension of the input to the attention layer."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the forward pass of the multi-head attention layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Conditionally calls `qkv_projection` or `projection` to get query, key, and value tensors.",
                        "Scales the query tensor.",
                        "Calls `attention_op_as_linen` to perform the core attention computation.",
                        "Calls `out_projection` to project the result back to the input embedding dimension."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "self.qkv_projection",
                        "self.projection",
                        "self.out_projection",
                        "attention_op_as_linen",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "This method orchestrates the entire attention process, from input projection to final output projection.",
                        "It accepts an optional `decoder_segment_ids` for packing sequences."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3DecoderLayer",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = gpt3_layer_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n        reductions_in_fp32=False,\n        use_bias=True,\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    assert (\n        cfg.num_query_heads == cfg.num_kv_heads\n    ), f\"{cfg.num_query_heads=} should be the same as {cfg.num_kv_heads=} in gpt3\"\n    attention_layer = Gpt3MultiHeadAttention(\n        config=cfg,\n        num_heads=cfg.num_query_heads,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        mesh=mesh,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        fused_qkv=cfg.fused_qkv,\n        use_bias=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n    )\n\n    attention_lnx = attention_layer(\n        lnx, decoder_segment_ids=decoder_segment_ids, model_mode=model_mode, deterministic=deterministic\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attention_lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        use_bias=True,\n        use_pre_norm=True,\n        config=cfg,\n        quant=self.quant,\n    )(attention_lnx, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = attention_lnx + mlp_lnx\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gpt3_decoder_layer",
            "purpose": "Implements a single layer of a GPT-3 style transformer decoder, consisting of a self-attention block and a feed-forward MLP block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a Gpt3MultiHeadAttention layer.",
                "Initializes an MLP block.",
                "Applies pre-attention layer normalization.",
                "Processes the normalized input through the self-attention block.",
                "Adds a residual connection from the original input to the attention output.",
                "Processes the result through the MLP block.",
                "Adds a second residual connection from the attention output to the MLP output.",
                "Applies dropout to the final layer output.",
                "Optionally records internal activation metrics."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "Gpt3MultiHeadAttention",
                "mlp_block",
                "gpt3_layer_norm",
                "models.Config",
                "jax.sharding.Mesh",
                "quantizations.Quant"
            ],
            "parameters": {
                "config": "A configuration object (`models.Config`) containing model hyperparameters like dimensions, dropout rates, and data types.",
                "mesh": "The JAX device mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This layer uses a pre-normalization architecture (LayerNorm before attention and MLP).",
                "It includes two residual connections, one after the self-attention block and another after the MLP block.",
                "An assertion ensures that the number of query heads equals the number of key/value heads, which is characteristic of the GPT-3 architecture.",
                "If `config.scan_layers` is true, the `__call__` method returns a tuple `(layer_output, None)` to be compatible with `nn.scan`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through the decoder layer, applying self-attention and an MLP transformation with residual connections.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim], decoder_segment_ids: [batch_size, sequence_length]",
                        "dtype": "Based on config.dtype (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply logical constraints and checkpointing to the input tensor.",
                        "Apply `gpt3_layer_norm` to the inputs.",
                        "Pass the normalized tensor through the `Gpt3MultiHeadAttention` layer.",
                        "Add the original input to the attention output (first residual connection).",
                        "Pass the result through the `mlp_block`.",
                        "Add the output of the attention stage to the MLP output (second residual connection).",
                        "Apply `nn.Dropout`.",
                        "Apply final logical constraints.",
                        "Optionally record internal metrics via `self.sow`.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "gpt3_layer_norm",
                        "Gpt3MultiHeadAttention",
                        "mlp_block",
                        "nn.Dropout",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The `deterministic` flag is passed to the attention, MLP, and dropout layers to control their behavior during training vs. inference.",
                        "The arguments `decoder_positions`, `previous_chunk`, `page_state`, and `slot` are accepted but not used within this method's body."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#get_attention_type",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "def get_attention_type(layer_id):\n  \"\"\"Get attention type based on layer ID.\"\"\"\n  layer_id %= len(GPT_OSS_ATTENTION_PATTERN)\n  return GPT_OSS_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "module_type": "attention_type_selector",
            "purpose": "Determines the attention type for a given layer by cyclically selecting from a predefined pattern.",
            "input": {
                "shape": "Scalar integer representing the layer ID.",
                "dtype": "int"
            },
            "processing_steps": [
                "Calculate an index by taking the input `layer_id` modulo the length of the `GPT_OSS_ATTENTION_PATTERN` tuple.",
                "Return the attention type from `GPT_OSS_ATTENTION_PATTERN` at the calculated index."
            ],
            "output": {
                "shape": "Scalar value of type `attentions.AttentionType`."
            },
            "dependencies": [
                "GPT_OSS_ATTENTION_PATTERN"
            ],
            "parameters": {},
            "notes": [
                "The function implements a cyclic pattern for attention types, alternating between `LOCAL_SLIDING` and `GLOBAL` as defined in the global `GPT_OSS_ATTENTION_PATTERN` constant."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssDecoderLayer",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  attention_type: AttentionType\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"GptOssAttention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_bias_in_projections=cfg.attention_bias,\n        attention_type=self.attention_type,\n        sliding_window_size=cfg.sliding_window_size,\n        query_pre_attn_scalar=(cfg.head_dim**-0.5),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    mlp_lnx, load_balance_loss = moe.get_routed_moe(\n        name=\"GptOssMlp\",\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gpt_oss_decoder_layer",
            "purpose": "A single transformer decoder layer for a GPT OSS model, containing a self-attention block and a Mixture-of-Experts (MoE) MLP block.",
            "input": {
                "shape": "[activation_batch, activation_norm_length, activation_embed]",
                "dtype": "Configurable via `config.dtype` (e.g., float32, bfloat16)."
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input.",
                "Apply a self-attention mechanism.",
                "Add the attention output to the original input via a residual connection.",
                "Apply post-attention RMS normalization.",
                "Process the result through a Mixture-of-Experts (MoE) MLP layer.",
                "Add the MLP output to the intermediate input via a second residual connection.",
                "Apply dropout to the final output."
            ],
            "output": {
                "shape": "If `config.scan_layers` is false: [activation_batch, activation_norm_length, activation_embed]. If true: ([activation_batch, activation_norm_length, activation_embed], None)."
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.normalizations.rms_norm",
                "maxtext.layers.attentions.attention_as_linen",
                "maxtext.layers.moe.get_routed_moe",
                "flax.linen.Dropout"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like `dtype`, `num_query_heads`, `num_kv_heads`, `head_dim`, `mlp_dim`, `num_experts`, `num_experts_per_tok`, `dropout_rate`, and `scan_layers`.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "attention_type": "The type of attention mechanism to use (e.g., LOCAL_SLIDING, GLOBAL).",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This layer uses two residual connections: one after the attention block and one after the MLP block.",
                "It utilizes RMSNorm for layer normalization before both the attention and MLP blocks.",
                "The MLP is a Mixture-of-Experts (MoE) layer, which may produce a load balancing loss that is stored as an intermediate value.",
                "The return signature depends on the `config.scan_layers` flag, making it compatible with `flax.linen.scan`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the decoder layer, applying self-attention and a Mixture-of-Experts MLP with residual connections and normalization.",
                    "input": {
                        "shape": "inputs: [activation_batch, activation_norm_length, activation_embed], decoder_segment_ids: [activation_batch, activation_norm_length], decoder_positions: [activation_batch, activation_norm_length]",
                        "dtype": "Configurable via `config.dtype`."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization.",
                        "Instantiate and call the self-attention layer (`attention_as_linen`).",
                        "Add a residual connection: `intermediate_inputs = inputs + attention_lnx`.",
                        "Apply post-attention RMS normalization.",
                        "Call the Mixture-of-Experts MLP layer (`moe.get_routed_moe`), capturing the `load_balance_loss`.",
                        "Add a second residual connection: `layer_output = mlp_lnx + intermediate_inputs`.",
                        "Apply dropout.",
                        "Optionally store the `load_balance_loss` and other activation metrics using `self.sow`.",
                        "Return the final layer output, potentially in a tuple with `None` if `config.scan_layers` is true."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is false: [activation_batch, activation_norm_length, activation_embed]. If true: ([activation_batch, activation_norm_length, activation_embed], None)."
                    },
                    "dependencies": [
                        "maxtext.layers.normalizations.rms_norm",
                        "maxtext.layers.attentions.attention_as_linen",
                        "maxtext.layers.moe.get_routed_moe",
                        "flax.linen.Dropout",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The `deterministic` argument controls whether dropout is applied.",
                        "The `model_mode` argument is passed down to the attention layer to control its behavior (e.g., caching for decoding)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssScannableBlock",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssScannableBlock(nn.Module):\n  \"\"\"A repeatable block of GPT OSS decoder layers.\n\n    This block applies multiple decoder layers sequentially, using the attention\n    pattern defined by GPT_OSS_ATTENTION_PATTERN. It's designed to be\n    used with `nn.scan` for efficient compilation.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    num_of_layers: int, number of decoder layers in the block\n    quant: Optional[Quant], quantization config\n  \"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      attention_type = get_attention_type(layer_id)\n      layer = GptOssDecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          model_mode=model_mode,\n          name=f\"layers_{layer_id}\",\n          attention_type=attention_type,\n          quant=self.quant,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gpt_oss_scannable_block",
            "purpose": "Applies a sequence of GPT OSS decoder layers, designed to be efficiently compiled using `nn.scan`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "GptOssDecoderLayer",
                "get_attention_type",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "config": "MaxText model configuration object, containing parameters like `inhomogeneous_layer_cycle_interval` and `scan_layers`.",
                "mesh": "JAX device mesh for model sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This block is a fundamental building block for creating a deep GPT-style decoder by stacking multiple layers.",
                "The number of layers within this block is determined by `config.inhomogeneous_layer_cycle_interval`.",
                "The attention pattern (e.g., local or global) for each layer is determined cyclically based on the layer's index within the block."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass by sequentially applying multiple `GptOssDecoderLayer` instances to the input tensor.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Apply logical sharding constraints and a gradient checkpoint name to the input tensor.",
                        "Initialize an output tensor `y` with the input tensor.",
                        "Loop `config.inhomogeneous_layer_cycle_interval` times:",
                        "  a. Determine the attention type for the current layer using `get_attention_type`.",
                        "  b. Instantiate and apply a `GptOssDecoderLayer`.",
                        "  c. If `config.scan_layers` is true, unpack the layer's tuple output to update `y`.",
                        "Return the final processed tensor."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is True, returns a tuple `(tensor, None)` where the tensor shape is [batch_size, sequence_length, hidden_dim]. Otherwise, returns the tensor directly with the same shape."
                    },
                    "dependencies": [
                        "GptOssDecoderLayer",
                        "get_attention_type"
                    ],
                    "notes": [
                        "The output format is conditional on the `config.scan_layers` flag. This is to ensure compatibility with `nn.scan`, which expects a specific tuple output format `(carry, output)`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#nd_dense_init",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def nd_dense_init(scale, mode, distribution):\n  \"\"\"Creates a variance-scaling initializer with dynamic in/out axes.\n\n  This function is a factory that returns an initializer function. The returned\n  function is a wrapper around `jax.nn.initializers.variance_scaling` that\n  allows the `in_axis` and `out_axis` to be specified at call time, rather\n  than at creation time.\n\n  Args:\n    scale: The scaling factor for the variance.\n    mode: The mode for variance scaling ('fan_in', 'fan_out', 'fan_avg').\n    distribution: The distribution to sample from ('normal', 'uniform', etc.).\n\n  Returns:\n    A function that takes a PRNG key, shape, dtype, in_axis, and out_axis,\n    and returns an initialized array.\n  \"\"\"\n\n  def init_fn(key, shape, dtype, in_axis, out_axis):\n    \"\"\"Initializes an array using variance scaling with specified axes.\"\"\"\n    fn = jax.nn.initializers.variance_scaling(scale, mode, distribution, in_axis, out_axis)\n    return fn(key, shape, dtype)\n\n  return init_fn",
        "analysis": {
            "functionality": "A factory function that creates a JAX variance-scaling initializer where the input and output axes for scaling can be specified dynamically at call time.",
            "usage": "Call `nd_dense_init` with `scale`, `mode`, and `distribution` to obtain an initializer function. This returned function can then be called with a JAX PRNG key, a shape, a dtype, and the specific `in_axis` and `out_axis` to generate an initialized array."
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#variable_to_logically_partitioned",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def variable_to_logically_partitioned(variable: nnx.VariableState):\n  \"\"\"Wraps an NNX variable's value in `nn.LogicallyPartitioned`.\n\n  This function inspects the metadata of an `nnx.VariableState` object. If\n  sharding information ('sharding' or 'sharding_names') is present, it wraps\n  the variable's value in `nn.LogicallyPartitioned` to apply the specified\n  sharding constraints.\n\n  It handles special cases for `aqt_tensor.QTensor` and variables of type\n  `_overwrite_with_gradient` by returning their values directly without\n  wrapping.\n\n  Args:\n    variable: The `nnx.VariableState` object to process.\n\n  Returns:\n    The variable's value, potentially wrapped in `nn.LogicallyPartitioned`.\n  \"\"\"\n  if isinstance(variable.value, aqt_tensor.QTensor):\n    return variable.value\n\n  if variable.type.__name__ == \"_overwrite_with_gradient\":\n    return variable.value\n\n  metadata = variable.get_metadata()\n  if \"sharding\" in metadata or \"sharding_names\" in metadata:\n    if \"sharding_names\" in metadata:\n      sharding_names = metadata[\"sharding_names\"]\n    else:\n      sharding_names = metadata[\"sharding\"]\n    return nn.LogicallyPartitioned(  # type: ignore[wrong-keyword-args]\n        variable.value,\n        sharding_names,  # type: ignore[arg-type]\n        mesh=metadata.get(\"mesh\"),\n        rules=metadata.get(\"rules\"),\n    )\n  else:\n    return variable.value",
        "analysis": {
            "functionality": "Wraps an NNX variable's value in `nn.LogicallyPartitioned` if sharding metadata is present, applying sharding constraints for distributed computation. It bypasses this wrapping for `aqt_tensor.QTensor` and `_overwrite_with_gradient` variable types.",
            "usage": "Call this function with an `nnx.VariableState` object. It inspects the variable's metadata for 'sharding' or 'sharding_names' keys. If found, it returns the variable's value wrapped in `nn.LogicallyPartitioned`. Otherwise, or for specific variable types, it returns the raw value."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_convert_to_activation_function",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _convert_to_activation_function(fn_or_string: str | Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Convert a string to an activation function.\"\"\"\n  if fn_or_string == \"linear\":\n    return lambda x: x\n  elif isinstance(fn_or_string, str):\n    return getattr(nn, fn_or_string)\n  elif callable(fn_or_string):\n    return fn_or_string\n  else:\n    raise ValueError(\n        f\"\"\"Don't know how to convert {fn_or_string}\n                         to an activation function\"\"\"\n    )",
        "analysis": {
            "functionality": "This function converts a given string or callable into a corresponding activation function. It handles 'linear' as a special case, retrieves standard activation functions from `flax.linen.nn` by name, and passes through callables directly.",
            "usage": "Call this function with a string representing an activation function (e.g., 'relu', 'gelu', 'linear') or an existing callable function. It will return a callable activation function. For example, `activation_fn = _convert_to_activation_function('relu')`."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#normalize_axes",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def normalize_axes(axes: Iterable[int], ndim: int) -> tuple[int, ...]:\n  # A tuple by convention. len(axes_tuple) then also gives the rank efficiently.\n  return tuple(ax if ax >= 0 else ndim + ax for ax in axes)",
        "analysis": {
            "module_type": "axis_normalization_utility",
            "purpose": "Converts an iterable of axis indices, including negative ones, into a tuple of their positive equivalents based on the total number of dimensions.",
            "input": {
                "shape": "axes: An iterable of integers (e.g., a list or tuple). ndim: An integer representing the total number of dimensions.",
                "dtype": "int"
            },
            "processing_steps": [
                "Iterate through each axis `ax` in the input `axes`.",
                "If `ax` is negative, convert it to its positive equivalent by adding `ndim`.",
                "Collect all normalized axes into a new tuple."
            ],
            "output": {
                "shape": "A tuple of integers with the same length as the input `axes` iterable."
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "The function handles both positive and negative indices.",
                "The output is a tuple by convention, which allows for efficient rank calculation via `len()`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#canonicalize_tuple",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def canonicalize_tuple(x):\n  if isinstance(x, Iterable):\n    return tuple(x)\n  else:\n    return (x,)",
        "analysis": {
            "functionality": "Converts an input into a tuple. If the input is an iterable, it's converted to a tuple. If it's a non-iterable, it's wrapped in a single-element tuple.",
            "usage": "Call the function with a single argument `x`, which can be any Python object. It returns a tuple. For example, `canonicalize_tuple([1, 2])` returns `(1, 2)` and `canonicalize_tuple(5)` returns `(5,)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general(inputs, kernel, kernel_axes, axis, contract_ind, matmul_precision, quant):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant:\n    dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n    dot_general = dot_general_cls()\n    return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "module_type": "generalized_matrix_multiplication_helper",
            "purpose": "Computes a `lax.dot_general` operation, optionally using a quantized implementation if a quantization configuration is provided.",
            "input": {
                "shape": "inputs: Array, kernel: Array. Specific shapes depend on the contraction axes.",
                "dtype": "Typically float (e.g., float32, bfloat16)."
            },
            "processing_steps": [
                "Convert the `matmul_precision` string to a `lax.Precision` object.",
                "Check if the `quant` object is provided.",
                "If `quant` is provided, instantiate and call its quantized `dot_general` class.",
                "Otherwise, call the standard `jax.lax.dot_general` with the specified precision.",
                "Return the result of the dot product."
            ],
            "output": {
                "shape": "The shape resulting from the `dot_general` operation on the input tensors."
            },
            "dependencies": [
                "jax.lax.dot_general",
                "jax.lax.Precision",
                "quantizations.AqtQuantization"
            ],
            "parameters": {
                "inputs": "The first input array for the dot product.",
                "kernel": "The second input array (e.g., weight matrix) for the dot product.",
                "kernel_axes": "Logical axes for partitioning the kernel, used by the quantization logic.",
                "axis": "The axis/axes in the `inputs` tensor to be contracted.",
                "contract_ind": "The corresponding axis/axes in the `kernel` tensor to be contracted.",
                "matmul_precision": "A string specifying the precision for the matrix multiplication (e.g., 'default').",
                "quant": "An optional quantization configuration object. If provided, a quantized `dot_general` is used."
            },
            "notes": [
                "This function acts as a dispatcher, selecting between a standard or a quantized `dot_general` implementation.",
                "The contraction dimensions for `dot_general` are hardcoded to `((axis, contract_ind), ((), ()))`.",
                "When quantization is enabled, the `precision` argument to the underlying `dot_general` call is set to `None`, as precision is handled by the quantization implementation itself."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general_nnx",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general_nnx(\n    inputs, kernel, axis, contract_ind, matmul_precision, quant_dot_general: nnx_wrappers.ToNNX | None, initializing: bool\n):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant_dot_general is not None:\n    if initializing:\n      quant_dot_general.lazy_init(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n    return quant_dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None, mutable=[\"aqt\"])\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "module_type": "quantized_dot_general",
            "purpose": "Computes a `lax.dot_general` operation, optionally using a provided quantization module if one is supplied.",
            "input": {
                "shape": "inputs: Array, kernel: Array. The shapes must be compatible for a dot product based on `axis` and `contract_ind`.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `quant_dot_general` module is provided.",
                "If `quant_dot_general` is provided and `initializing` is true, call `lazy_init` on the quantization module.",
                "If `quant_dot_general` is provided, execute the dot product using the quantization module.",
                "If `quant_dot_general` is not provided, execute the standard `lax.dot_general` with the specified `matmul_precision`.",
                "Return the result of the dot product."
            ],
            "output": {
                "shape": "The resulting tensor from the dot product operation."
            },
            "dependencies": [
                "jax.lax.dot_general",
                "jax.lax.Precision",
                "nnx_wrappers.ToNNX"
            ],
            "parameters": {
                "matmul_precision": "The precision level for the standard `lax.dot_general` operation (e.g., 'default').",
                "quant_dot_general": "An optional `nnx_wrappers.ToNNX` module for performing quantized matrix multiplication.",
                "initializing": "A boolean flag that, if true, triggers the lazy initialization of the quantization module on the first call."
            },
            "notes": [
                "This function serves as a conditional dispatcher, choosing between a standard `lax.dot_general` and a custom, potentially quantized, version.",
                "When quantization is used, `mutable=['aqt']` is passed to the module, indicating that quantization-related state may be updated during the call."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#DenseGeneral",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class DenseGeneral(nnx.Module):\n  \"\"\"A linear transformation with flexible axes.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Iterable[int] | int,\n      out_features_shape: Iterable[int] | int,\n      axis: Iterable[int] | int = -1,\n      weight_dtype: DType = jnp.float32,\n      dtype: DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: tuple[None | str, ...] = (),\n      quant: None | Quant = None,\n      use_bias: bool = False,\n      matmul_precision: str = \"default\",\n      parameter_memory_host_offload: bool = False,\n      *,  # Following arguments are keyword-only\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the DenseGeneral module.\n\n    Args:\n      in_features_shape: tuple with numbers of input features for axes specified in\n        'axis'.\n      out_features_shape: tuple with numbers of output features.\n      axis: tuple with axes to apply the transformation on.\n      weight_dtype: the dtype of the weights (default: float32).\n      dtype: the dtype of the computation (default: float32).\n      kernel_init: initializer function for the weight matrix.\n      kernel_axes: logical axes for partitioning the kernel.\n      quant: quantization config, defaults to None implying no quantization.\n      use_bias: whether to add bias in linear transformation.\n      matmul_precision: Precision for matrix multiplication.\n      parameter_memory_host_offload: Determines whether to offload params to host\n      rngs: RNG state for initialization in nnx.\n    \"\"\"\n    self.in_features_shape = canonicalize_tuple(in_features_shape)\n    self.out_features_shape = canonicalize_tuple(out_features_shape)\n    self.axis = canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.quant = quant\n    self.use_bias = use_bias\n    self.matmul_precision = matmul_precision\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: Array, _initializing: bool = False) -> Array:\n    \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n\n    Args:\n      inputs: The nd-array to be transformed.\n\n    Returns:\n      The transformed input.\n    \"\"\"\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = normalize_axes(self.axis, inputs.ndim)\n\n    for i, ax in enumerate(norm_axis):\n      if inputs.shape[ax] != self.in_features_shape[i]:\n        raise ValueError(\n            f\"Input dimension {inputs.shape[ax]} at axis {ax} \"\n            f\"does not match expected input feature size {self.in_features_shape[i]}\"\n        )\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n      # Move logit_dense kernel to device if parameter offloading is enabled\n      if self.parameter_memory_host_offload:\n        max_logging.log(\"linear.py: Moving parameter logits_dense kernel to device\")\n        kernel = jax.device_put(kernel, max_utils.device_space())\n      kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(self.axis)))\n    output = _compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n\n    if self.bias is not None:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "module_type": "dense_general",
            "purpose": "A linear transformation with flexible axes, allowing for matrix multiplication over specified dimensions of the input tensor.",
            "input": {
                "shape": "N/A (Handled by the __call__ method)",
                "dtype": "N/A (Handled by the __call__ method)"
            },
            "processing_steps": [
                "Initializes kernel and optional bias parameters based on input/output feature shapes and axes.",
                "Optionally sets up a quantized matrix multiplication operator if a quantization config is provided.",
                "Applies the linear transformation to an input tensor via the __call__ method."
            ],
            "output": {
                "shape": "N/A (Handled by the __call__ method)"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.numpy",
                "numpy",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.quantizations",
                "MaxText.layers.initializers",
                "_compute_dot_general_nnx",
                "canonicalize_tuple",
                "normalize_axes"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features to be contracted.",
                "out_features_shape": "The shape of the output features.",
                "axis": "The axis or axes of the input tensor to apply the linear transformation on.",
                "quant": "Quantization configuration object, if any.",
                "use_bias": "Boolean indicating whether to include a bias term.",
                "matmul_precision": "Precision for the matrix multiplication operation (e.g., 'default').",
                "parameter_memory_host_offload": "Determines whether to offload parameters to host memory."
            },
            "notes": [
                "This module is a generalization of a standard dense (fully connected) layer.",
                "It supports optional quantization via the `quant` parameter.",
                "The implementation uses `jax.lax.dot_general` for the core computation, wrapped in `_compute_dot_general_nnx` to handle quantization."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the DenseGeneral module, setting up parameters like kernel and optional bias.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Canonicalize `in_features_shape`, `out_features_shape`, and `axis` to tuples.",
                        "Store configuration parameters like `dtype`, `quant`, `use_bias`, etc.",
                        "Calculate the kernel shape based on input and output feature shapes.",
                        "Initialize the kernel parameter `self.kernel` using `kernel_init` unless in quantization serving mode.",
                        "If `use_bias` is true, initialize the bias parameter `self.bias`.",
                        "If quantization is enabled, set up the quantized dot-general operator and perform a lazy initialization with a dummy input."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "canonicalize_tuple",
                        "nd_dense_init",
                        "default_bias_init",
                        "nnx.Param",
                        "quantizations",
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "The kernel and bias are initialized as `nnx.Param` objects, which are trainable parameters.",
                        "The `rngs` argument is required for parameter initialization."
                    ]
                },
                "quant_dot_general": {
                    "purpose": "A property to retrieve the quantized dot-general operator if it exists.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `self._quant_dot_general_name` is set.",
                        "If set, return the corresponding attribute which is the quantization module.",
                        "Otherwise, return None."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This provides a clean way to access the potentially dynamically named quantization module."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the linear transformation to the input tensor.",
                    "input": {
                        "shape": "[..., in_feature_dim_1, ..., in_feature_dim_N, ...], where the dimensions corresponding to `axis` must match `in_features_shape`.",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Cast input tensor to `self.dtype`.",
                        "Normalize the `axis` parameter relative to the input's dimensions.",
                        "Validate that the input shape along the specified axes matches `in_features_shape`.",
                        "Retrieve the kernel (weight matrix), potentially moving it from host to device if offloading is enabled.",
                        "Call `_compute_dot_general_nnx` to perform the matrix multiplication, which may be quantized.",
                        "If `self.bias` is not None, add the bias term to the output.",
                        "Return the transformed tensor."
                    ],
                    "output": {
                        "shape": "The input shape with the dimensions specified by `axis` replaced by `out_features_shape`."
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "normalize_axes",
                        "quantizations.in_serve_mode",
                        "jax.device_put",
                        "_compute_dot_general_nnx"
                    ],
                    "notes": [
                        "The `_initializing` flag is used during setup with a dummy input to handle lazy initialization for quantization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#dense_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def dense_general(\n    *,\n    inputs_shape: tuple[int, ...] | None = None,\n    in_features_shape: tuple[int, ...] | int | None = None,\n    out_features_shape: Iterable[int] | int,\n    axis: Iterable[int] | int = -1,\n    weight_dtype: DType = jnp.float32,\n    dtype: DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: tuple[None | str, ...] = (),\n    quant: None | Quant = None,\n    use_bias: bool = False,\n    matmul_precision: str = \"default\",\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Creates a DenseGeneral Linen module using nnx.bridge.to_linen.\n\n  Args:\n    inputs_shape: tuple with the shape of the inputs\n    in_features_shape: tuple with numbers of input features for axes specified in\n      'axis'.\n    out_features_shape: tuple with numbers of output features.\n    axis: tuple with axes to apply the transformation on.\n    weight_dtype: the dtype of the weights (default: float32).\n    dtype: the dtype of the computation (default: float32).\n    kernel_init: initializer function for the weight matrix.\n    kernel_axes: logical axes for partitioning the kernel.\n    quant: quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n    matmul_precision: Precision for matrix multiplication.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n  if not (inputs_shape is not None) ^ (in_features_shape is not None):\n    raise ValueError(\"Exactly one of inputs_shape or in_features must be specified.\")\n\n  if inputs_shape is not None:\n    axis = canonicalize_tuple(axis)\n    in_features_shape = tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n  else:\n    assert in_features_shape is not None\n  module = nnx_wrappers.to_linen(\n      DenseGeneral,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      quant=quant,\n      use_bias=use_bias,\n      matmul_precision=matmul_precision,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "dense_general_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible `DenseGeneral` module by wrapping the corresponding NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Validate that exactly one of `inputs_shape` or `in_features_shape` is provided.",
                "If `inputs_shape` is provided, derive `in_features_shape` from it based on the specified `axis`.",
                "Call `nnx_wrappers.to_linen` to create a Linen module from the `DenseGeneral` NNX class.",
                "Return the newly created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance. The shape transformation performed by this module depends on the configuration parameters provided to this factory."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "DenseGeneral",
                "canonicalize_tuple",
                "normalize_axes",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "inputs_shape": "Optional tuple with the shape of the inputs, used to infer `in_features_shape`.",
                "in_features_shape": "Optional tuple with numbers of input features for axes specified in 'axis'.",
                "out_features_shape": "Tuple with numbers of output features.",
                "axis": "Tuple with axes to apply the linear transformation on.",
                "quant": "Quantization configuration object, defaults to None (no quantization).",
                "kernel_axes": "Logical axes for partitioning the kernel weight for model parallelism."
            },
            "notes": [
                "This function acts as a bridge, allowing an NNX-defined module (`DenseGeneral`) to be used within a Flax Linen-based model architecture.",
                "The user must specify either the full input shape (`inputs_shape`) or the feature dimensions (`in_features_shape`), but not both."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#Dropout",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class Dropout(nnx.Dropout):\n  \"\"\"Forked nnx.Dropout that is easier to use with bridge\"\"\"\n\n  def __init__(  # pylint: disable=super-init-not-called\n      self,\n      rate: float,\n      *,\n      broadcast_dims: Sequence[int] = (),\n      deterministic: bool = False,\n      rng_collection: str = \"dropout\",\n      rngs: nnx.Rngs | None = None,\n  ):\n    self.rate = rate\n    self.broadcast_dims = broadcast_dims\n    self.deterministic = deterministic\n    self.rng_collection = rng_collection\n\n    if isinstance(rngs, nnx.Rngs):\n      self.rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else rngs\n    else:\n      raise TypeError(f\"rngs must be a Rngs, RngStream or None, but got {type(rngs)}.\")",
        "analysis": {
            "module_type": "dropout_layer",
            "purpose": "A custom Dropout layer, forked from `flax.nnx.Dropout`, with a modified initialization process for easier integration with a bridging mechanism.",
            "input": {
                "shape": "Any tensor shape, e.g., [batch_size, sequence_length, hidden_dim].",
                "dtype": "float32 or other float types."
            },
            "processing_steps": [
                "Initializes dropout parameters and forks the RNG stream during construction.",
                "Applies dropout to the input tensor during the forward pass (inherited from nnx.Dropout)."
            ],
            "output": {
                "shape": "Same as the input shape."
            },
            "dependencies": [
                "flax.nnx.Dropout",
                "flax.nnx.Rngs"
            ],
            "parameters": {
                "rate": "The probability of an element to be zeroed out.",
                "broadcast_dims": "Sequence of dimensions to broadcast the dropout mask across.",
                "deterministic": "If True, dropout is disabled and the input is returned unchanged.",
                "rng_collection": "The name of the RNG stream to use for generating the dropout mask.",
                "rngs": "The RNG state object from which a new, forked stream is created for this layer."
            },
            "notes": [
                "This class intentionally does not call the `__init__` method of its parent class, `nnx.Dropout`, to provide custom handling of the `rngs` object.",
                "The actual dropout operation is performed by the `__call__` method inherited from `nnx.Dropout`, which is not shown in this code block."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Dropout layer's configuration and forks the provided RNG stream.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assign `rate`, `broadcast_dims`, `deterministic`, and `rng_collection` to instance attributes.",
                        "Verify that the `rngs` argument is an `nnx.Rngs` instance.",
                        "Create a unique RNG stream for this layer by calling `rngs.fork()`.",
                        "Raise a TypeError if `rngs` is not a valid type."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Rngs"
                    ],
                    "notes": [
                        "This constructor explicitly avoids calling `super().__init__()`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#MlpBlock",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class MlpBlock(nnx.Module):\n  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      in_features: int,\n      intermediate_dim: int = 2048,\n      activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      intermediate_dropout_rate: float = 0.1,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      use_bias: bool = False,\n      use_pre_norm: bool = False,\n      quant: None | Quant = None,\n      model_mode: None | str = None,\n      *,\n      rngs: nnx.Rngs,\n  ) -> None:\n    \"\"\"A MlpBlock module.\n\n    Args:\n      config: Config object containing model parameters.\n      in_features: Number of input features.\n      intermediate_dim: Shared dimension of hidden layers.\n      activations: Type of activations for each layer.  Each element is either\n        'linear', a string function name in flax.linen, or a function.\n      kernel_init: Kernel function, passed to the dense layers.\n      deterministic: Whether the dropout layers should be deterministic.\n      intermediate_dropout_rate: Dropout rate used after the intermediate layers.\n      dtype: computation data type for the dense layer.\n      weight_dtype: weight data type for the dense layer.\n      use_bias: whether to add bias in all feedforward layers.\n      use_pre_norm: whether to add pre layer norm in mlp layers.\n      quant: Optional quantization config, no quantization if None.\n    \"\"\"\n    self.config = config\n    self.in_features = in_features\n    self.intermediate_dim = intermediate_dim\n    self.activations = activations\n    self.kernel_init = kernel_init\n    self.intermediate_dropout_rate = intermediate_dropout_rate\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.use_bias = use_bias\n    self.use_pre_norm = use_pre_norm\n    self.quant = quant\n    self.model_mode = model_mode\n\n    if self.use_pre_norm:\n      self.mlp_layer_norm = self.get_norm_layer(num_features=in_features)(\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          epsilon=config.normalization_layer_epsilon,\n          rngs=rngs,\n      )\n    else:\n      self.mlp_layer_norm = None\n\n    if config.fused_mlp:\n      self.wi = DenseGeneral(\n          in_features_shape=in_features,\n          out_features_shape=(len(self.activations), self.intermediate_dim),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"num_activations\", \"mlp\"),\n          quant=self.quant,\n          use_bias=self.use_bias,\n          matmul_precision=self.config.matmul_precision,\n          rngs=rngs,\n      )\n    else:\n      for idx in range(len(self.activations)):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = DenseGeneral(\n            in_features_shape=in_features,\n            out_features_shape=self.intermediate_dim,\n            dtype=self.dtype,\n            weight_dtype=self.weight_dtype,\n            kernel_init=self.kernel_init,\n            kernel_axes=(\"embed\", \"mlp\"),\n            quant=self.quant,\n            use_bias=self.use_bias,\n            matmul_precision=self.config.matmul_precision,\n            rngs=rngs,\n        )\n        setattr(self, dense_name, module)\n    self.dropout = Dropout(rate=self.intermediate_dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n    self.wo = DenseGeneral(\n        in_features_shape=self.intermediate_dim,\n        out_features_shape=in_features,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"mlp\", \"embed\"),\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n        rngs=rngs,\n    )\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer.\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(normalizations.RMSNorm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      from MaxText.layers import gpt3  # pylint: disable=import-outside-toplevel\n\n      return functools.partial(\n          gpt3.Gpt3LayerNorm, num_features=num_features, reductions_in_fp32=False, use_bias=self.use_bias\n      )\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def __call__(self, inputs, decode: bool = False, deterministic: bool = False):\n    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n    cfg = self.config\n\n    if self.mlp_layer_norm is not None:\n      inputs = self.mlp_layer_norm(inputs)\n\n    # Iterate over specified MLP input activation functions.\n    # e.g. ('relu',) or ('gelu', 'linear') for gated-gelu.\n    activations = []\n    if cfg.fused_mlp:\n      x = self.wi(inputs)\n      x = checkpoint_name(x, \"mlpwi\")\n      for idx, act_fn in enumerate(self.activations):\n        y = _convert_to_activation_function(act_fn)(x[:, :, idx, ...])\n        activations.append(y)\n    else:\n      for idx, act_fn in enumerate(self.activations):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = getattr(self, dense_name)\n        x = module(inputs)\n        x = checkpoint_name(x, \"mlp\" + dense_name)\n        if cfg.activations_in_float32:\n          x = x.astype(jnp.float32)\n        x = _convert_to_activation_function(act_fn)(x)\n        activations.append(x)\n\n    # Take elementwise product of above intermediate activations.\n    x = functools.reduce(operator.mul, activations).astype(self.dtype)\n    # Apply dropout and final dense output projection.\n    x = self.dropout(x, deterministic=deterministic)  # Broadcast along length.\n    if self.model_mode == MODEL_MODE_PREFILL:\n      x = nn.with_logical_constraint(x, (\"activation_batch\", \"prefill_activation_length\", \"activation_mlp\"))\n    elif cfg.expert_shard_attention_option == EP_AS_CONTEXT and self.model_mode == MODEL_MODE_TRAIN:\n      x = nn.with_logical_constraint(x, (\"activation_batch_no_exp\", \"activation_length\", \"activation_mlp\"))\n    else:\n      x = nn.with_logical_constraint(x, (\"activation_batch\", \"activation_length_no_exp\", \"activation_mlp\"))\n\n    output = self.wo(x)\n\n    output = checkpoint_name(output, \"mlpwo\")\n    return output",
        "analysis": {
            "module_type": "transformer_mlp_block",
            "purpose": "Implements a standard feed-forward network (MLP) block, a core component of a Transformer layer.",
            "input": {
                "shape": "[batch_size, sequence_length, in_features]",
                "dtype": "jnp.float32 or as configured by the 'dtype' parameter."
            },
            "processing_steps": [
                "Optionally applies pre-layer normalization to the input tensor if `use_pre_norm` is True.",
                "Projects the input into an intermediate dimension using one or more dense layers (`wi`).",
                "Applies one or more activation functions to the intermediate representations.",
                "Combines the activated representations, typically through element-wise multiplication for gated MLPs.",
                "Applies dropout to the combined representation.",
                "Projects the result back to the original input dimension using an output dense layer (`wo`)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, in_features]"
            },
            "dependencies": [
                "nnx.Module",
                "DenseGeneral",
                "Dropout",
                "normalizations.RMSNorm",
                "gpt3.Gpt3LayerNorm",
                "_convert_to_activation_function"
            ],
            "parameters": {
                "config": "A Config object containing global model parameters like `fused_mlp`, `decoder_block`, and `matmul_precision`.",
                "in_features": "The number of input and output features for the block.",
                "intermediate_dim": "The dimensionality of the hidden layer(s) within the MLP.",
                "activations": "A sequence of activation functions to apply. Multiple activations are used for gated architectures (e.g., SwiGLU).",
                "intermediate_dropout_rate": "The dropout rate applied after the activation functions.",
                "use_pre_norm": "A boolean indicating whether to apply layer normalization before the MLP transformation.",
                "fused_mlp": "A boolean from the config that determines whether to use a single, larger dense layer for all activation paths for potential optimization."
            },
            "notes": [
                "The architecture of the input projection (`wi` layers) changes based on the `config.fused_mlp` flag.",
                "Supports gated MLPs (like SwiGLU) by accepting multiple activation functions in the `activations` sequence, which are then element-wise multiplied.",
                "The type of normalization layer used (if `use_pre_norm` is True) is determined by `config.decoder_block`.",
                "Uses `nn.with_logical_constraint` to provide sharding hints to the JAX compiler."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLP block by creating the necessary dense layers, dropout, and an optional normalization layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters from the constructor arguments.",
                        "Conditionally instantiate a normalization layer via `get_norm_layer` if `use_pre_norm` is true.",
                        "If `config.fused_mlp` is true, create a single `DenseGeneral` layer (`wi`) for all activation paths.",
                        "If `config.fused_mlp` is false, create a separate `DenseGeneral` layer (`wi_idx`) for each activation function.",
                        "Instantiate a `Dropout` layer.",
                        "Instantiate the output `DenseGeneral` layer (`wo`)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "Dropout",
                        "get_norm_layer"
                    ],
                    "notes": [
                        "The number and shape of the `wi` dense layers depend on `config.fused_mlp` and the length of the `activations` sequence."
                    ]
                },
                "get_norm_layer": {
                    "purpose": "Selects and returns the appropriate normalization layer class based on the model's decoder type specified in the config.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check the value of `self.config.decoder_block`.",
                        "Return a partially initialized `normalizations.RMSNorm` for Llama, Mistral, Gemma, etc.",
                        "Return a partially initialized `gpt3.Gpt3LayerNorm` for GPT3.",
                        "Raise a ValueError for unsupported decoder types."
                    ],
                    "output": {
                        "shape": "A partially applied class constructor for a normalization layer."
                    },
                    "dependencies": [
                        "functools.partial",
                        "normalizations.RMSNorm",
                        "gpt3.Gpt3LayerNorm",
                        "DecoderBlockType"
                    ],
                    "notes": [
                        "This method centralizes the logic for selecting different normalization implementations required by various model architectures."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the MLP block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, in_features]",
                        "dtype": "jnp.float32 or as configured"
                    },
                    "processing_steps": [
                        "Apply pre-layer normalization if it exists.",
                        "Apply the input dense layer(s) (`wi` or `wi_idx`).",
                        "Apply activation functions via `_convert_to_activation_function`.",
                        "Element-wise multiply the activated tensors using `functools.reduce`.",
                        "Cast the result to the configured `dtype`.",
                        "Apply dropout, controlled by the `deterministic` flag.",
                        "Apply logical constraints for tensor sharding.",
                        "Apply the output dense layer (`wo`).",
                        "Return the final output tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, in_features]"
                    },
                    "dependencies": [
                        "_convert_to_activation_function",
                        "checkpoint_name",
                        "functools.reduce",
                        "operator.mul",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The logic for the initial projection and activation differs based on the `config.fused_mlp` setting.",
                        "The `deterministic` flag controls the behavior of the dropout layer, typically set to True during inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#mlp_block",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def mlp_block(\n    *,\n    config: Config,\n    in_features: int,\n    intermediate_dim: int = 2048,\n    activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    intermediate_dropout_rate: float = 0.1,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    use_bias: bool = False,\n    use_pre_norm: bool = False,\n    quant: None | Quant = None,\n    model_mode: None | str = None,\n    name: None | str = None,\n):\n  \"\"\"Creates a MlpBlock Linen module using nnx.bridge.to_linen.\"\"\"\n  module = nnx_wrappers.to_linen(\n      MlpBlock,\n      config=config,\n      in_features=in_features,\n      intermediate_dim=intermediate_dim,\n      activations=activations,\n      kernel_init=kernel_init,\n      intermediate_dropout_rate=intermediate_dropout_rate,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      use_bias=use_bias,\n      use_pre_norm=use_pre_norm,\n      quant=quant,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "mlp_block_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible version of the `MlpBlock` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `MlpBlock` NNX module.",
                "Passes configuration parameters such as `in_features`, `intermediate_dim`, and `activations` to the `MlpBlock` constructor.",
                "Returns the instantiated and wrapped Flax Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module. The module itself, when called, will output a tensor of shape [batch_size, sequence_length, in_features]."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlpBlock",
                "variable_to_logically_partitioned",
                "max_utils.Config",
                "Quant"
            ],
            "parameters": {
                "config": "The main configuration object containing model parameters like `dtype`, `matmul_precision`, etc.",
                "in_features": "The number of input features to the MLP block.",
                "intermediate_dim": "The shared dimension of the MLP's hidden layers.",
                "activations": "A sequence of activation functions to be applied in the MLP.",
                "quant": "Optional quantization configuration for the dense layers within the MLP block.",
                "use_pre_norm": "If True, a layer normalization is applied to the input before the MLP transformations."
            },
            "notes": [
                "This function serves as a bridge to make the NNX-defined `MlpBlock` usable within a Flax Linen model structure.",
                "The `abstract_init=False` argument ensures the module's parameters are initialized upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama2.py#LlamaDecoderLayer",
        "file_path": "src/MaxText/layers/llama2.py",
        "code_block": "class LlamaDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      model_mode: str,\n      mesh: Mesh,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n  ):\n\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        quant=self.quant,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = self.pre_self_attention_layer_norm(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    # Self-attention block\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # MLP block.\n    mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "llama_decoder_layer",
            "purpose": "Implements a single decoder layer for a Llama-style transformer model, including self-attention and a feed-forward MLP block.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes sub-modules: `RMSNorm` for pre and post-attention normalization, an `Attention` block, an `MlpBlock`, and `Dropout`.",
                "In the forward pass, applies pre-normalization, self-attention, a residual connection, post-normalization, an MLP block, a second residual connection, and dropout."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "nnx.Module",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "max_utils",
                "page_manager",
                "quantizations"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like `emb_dim`, `num_query_heads`, `num_kv_heads`, `head_dim`, `mlp_dim`, `dropout_rate`, etc.",
                "model_mode": "A string indicating the model's operational mode (e.g., 'prefill'), which affects initialization.",
                "mesh": "The JAX device mesh for distributed computation.",
                "rngs": "An `nnx.Rngs` object for managing random number generation.",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This class implements a standard transformer decoder layer with a Pre-LN (Layer Normalization) architecture.",
                "It is designed to be compatible with JAX's `scan` transformation, as indicated by the conditional return value in `__call__` based on `config.scan_layers`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the sub-modules of the Llama decoder layer, including normalization, self-attention, and MLP blocks.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine a dummy input shape based on `model_mode` using `max_utils.get_batch_seq_len_for_mode`.",
                        "Initialize `pre_self_attention_layer_norm` as an `RMSNorm` layer.",
                        "Initialize `self_attention` as an `Attention` layer.",
                        "Initialize `post_self_attention_layer_norm` as an `RMSNorm` layer.",
                        "Initialize `mlp` as an `MlpBlock`.",
                        "Initialize `dropout` as a `Dropout` layer.",
                        "Set `activation_axis_names` based on the `model_mode`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_utils.get_batch_seq_len_for_mode",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "The initialization is heavily configured by the `Config` object.",
                        "It sets up different logical axis names for activations depending on whether the `model_mode` is 'prefill' or not."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying self-attention and an MLP block with residual connections and normalization.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, emb_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "float32 or bfloat16 (inferred from config.dtype)"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Pass the normalized tensor through the self-attention block (`self.self_attention`).",
                        "Add the output of the attention block to the original input (first residual connection).",
                        "Apply post-attention RMS normalization to the result of the first residual connection.",
                        "Pass the normalized tensor through the MLP block (`self.mlp`).",
                        "Add the output of the MLP block to the result of the first residual connection (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally record internal activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "The method follows a Pre-LN (Layer Normalization) architecture.",
                        "It uses `nn.with_logical_constraint` to enforce sharding annotations.",
                        "The return signature changes based on the `config.scan_layers` flag, returning `(layer_output, None)` if true, which is a common pattern for use with `jax.lax.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4UnfoldConvolution",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4UnfoldConvolution(nnx.Module):\n  \"\"\"implementation of Llama4UnfoldConvolution for Llama4 Multi modal model.\n\n  This module extracts patches from input images and projects them to hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.vit_unfold_linear = linears.DenseGeneral(\n        in_features_shape=(\n            self.config.num_channels_for_vit * self.config.patch_size_for_vit * self.config.patch_size_for_vit\n        ),\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=rngs,\n    )\n\n  def __call__(self, inputs: Array) -> Array:\n    batch_size, num_channels, img, _ = inputs.shape\n    num_patches = (img // self.config.patch_size_for_vit) ** 2\n\n    patches = lax.conv_general_dilated_patches(\n        inputs,\n        filter_shape=[self.config.patch_size_for_vit, self.config.patch_size_for_vit],\n        window_strides=[self.config.patch_size_for_vit, self.config.patch_size_for_vit],\n        padding=\"VALID\",\n        dimension_numbers=(\"NCHW\", \"HWIO\", \"NCHW\"),\n        precision=lax.Precision(self.config.matmul_precision),\n        preferred_element_type=self.config.dtype_mm,\n    )\n\n    patches = patches.reshape(\n        batch_size, num_channels * self.config.patch_size_for_vit * self.config.patch_size_for_vit, num_patches\n    )\n    patches = patches.transpose(0, 2, 1)\n\n    hidden_states = self.vit_unfold_linear(patches)\n\n    return hidden_states",
        "analysis": {
            "module_type": "unfold_convolution_projection",
            "purpose": "Extracts non-overlapping patches from an input image tensor and projects them into a lower-dimensional embedding space.",
            "input": {
                "shape": "[batch_size, num_channels, height, width]",
                "dtype": "The data type specified by `config.dtype_mm`."
            },
            "processing_steps": [
                "Extracts non-overlapping patches from the input image using `lax.conv_general_dilated_patches`.",
                "Reshapes and transposes the patches to `[batch_size, num_patches, flattened_patch_dim]`.",
                "Applies a dense linear layer to project each patch to `hidden_size_for_vit`."
            ],
            "output": {
                "shape": "[batch_size, num_patches, hidden_size_for_vit]"
            },
            "dependencies": [
                "nnx.Module",
                "linears.DenseGeneral",
                "jax.lax"
            ],
            "parameters": {
                "patch_size_for_vit": "The height and width of each square patch to be extracted from the image.",
                "hidden_size_for_vit": "The dimension of the output embedding for each projected patch.",
                "num_channels_for_vit": "The number of channels in the input image tensor.",
                "dtype_mm": "The data type for multi-modal computations.",
                "matmul_precision": "The precision for the matrix multiplication operations."
            },
            "notes": [
                "This module effectively performs a convolution with a stride equal to the patch size to create a sequence of flattened patch embeddings, which is a common first step in Vision Transformer (ViT) architectures."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module and sets up the linear projection layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the configuration and RNGs.",
                        "Initializes a `linears.DenseGeneral` layer for patch projection based on parameters from the `config` object."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.DenseGeneral",
                        "Config",
                        "nnx.Rngs"
                    ],
                    "notes": [
                        "The input feature shape for the dense layer is calculated as `num_channels_for_vit * patch_size_for_vit * patch_size_for_vit`."
                    ]
                },
                "__call__": {
                    "purpose": "Extracts patches from the input image tensor and applies the linear projection.",
                    "input": {
                        "shape": "[batch_size, num_channels, height, width]",
                        "dtype": "The data type specified by `config.dtype_mm`."
                    },
                    "processing_steps": [
                        "Calculates the number of patches based on image and patch size.",
                        "Extracts patches using `lax.conv_general_dilated_patches`.",
                        "Reshapes the patches into a sequence of shape `[batch_size, num_patches, flattened_patch_dim]`.",
                        "Applies the `self.vit_unfold_linear` layer to project the patches."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "jax.lax.conv_general_dilated_patches",
                        "self.vit_unfold_linear"
                    ],
                    "notes": [
                        "The number of patches is calculated as `(height // patch_size_for_vit) ** 2`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#pixel_shuffle",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def pixel_shuffle(input_tensor: Array, shuffle_ratio: float) -> Array:\n  \"\"\"Apply pixel shuffle operation to the input tensor.\"\"\"\n  batch_size, num_patches, channels = input_tensor.shape\n  patch_size = int(math.sqrt(num_patches))\n\n  # Reshape to [batch_size, patch_size, patch_size, channels]\n  input_tensor = input_tensor.reshape(batch_size, patch_size, patch_size, -1)\n  batch_size, height, width, channels = input_tensor.shape\n\n  # Reshape to [batch_size, height, width * shuffle_ratio, channels / shuffle_ratio]\n  reshaped_tensor = input_tensor.reshape(batch_size, height, int(width * shuffle_ratio), int(channels / shuffle_ratio))\n\n  # Transpose to [batch_size, width * shuffle_ratio, height, channels / shuffle_ratio]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape to [batch_size, height * shuffle_ratio, width * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.reshape(\n      batch_size, int(height * shuffle_ratio), int(width * shuffle_ratio), int(channels / (shuffle_ratio**2))\n  )\n\n  # Transpose to [batch_size, width * shuffle_ratio, height * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape back to [batch_size, num_patches, channels]\n  output_tensor = reshaped_tensor.reshape(batch_size, -1, reshaped_tensor.shape[-1])\n  return output_tensor",
        "analysis": {
            "module_type": "pixel_shuffle_operation",
            "purpose": "Rearranges elements of a tensor from the channel dimension to the spatial dimensions, effectively increasing spatial resolution while decreasing channel depth.",
            "input": {
                "shape": "[batch_size, num_patches, channels]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Infer spatial dimensions (`patch_size`) from the number of patches, assuming a square arrangement.",
                "Reshape the input from `[B, N, C]` to a 4D spatial tensor `[B, H, W, C]`.",
                "Perform a series of reshape and transpose operations to move data from the channel dimension to the height and width dimensions, controlled by `shuffle_ratio`.",
                "Reshape the resulting 4D tensor back to a 3D tensor `[B, new_num_patches, new_channels]`."
            ],
            "output": {
                "shape": "[batch_size, num_patches * (shuffle_ratio**2), channels / (shuffle_ratio**2)]"
            },
            "dependencies": [
                "math.sqrt"
            ],
            "parameters": {
                "shuffle_ratio": "The factor by which to upscale the spatial dimensions and downscale the channel dimension."
            },
            "notes": [
                "The input `num_patches` must be a perfect square.",
                "The input `channels` must be divisible by `shuffle_ratio` squared.",
                "This operation is a form of depth-to-space transformation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP(nnx.Module):\n  \"\"\"MLP block for Llama4EncoderLayer.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.vit_encoder_layer_mlp_fc1 = linears.DenseGeneral(\n        in_features_shape=self.config.hidden_size_for_vit,\n        out_features_shape=self.config.intermediate_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.vit_encoder_layer_mlp_fc2 = linears.DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, hidden_states: Array) -> Array:\n    hidden_states = self.vit_encoder_layer_mlp_fc1(hidden_states)\n    hidden_states = nnx.gelu(hidden_states, approximate=False)\n    hidden_states = self.vit_encoder_layer_mlp_fc2(hidden_states)\n    return hidden_states",
        "analysis": {
            "module_type": "vision_mlp",
            "purpose": "Implements a standard two-layer feed-forward network (MLP) with a GELU activation for a vision transformer encoder layer.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                "dtype": "config.dtype_mm"
            },
            "processing_steps": [
                "The `__call__` method processes the input tensor through a sequence of a dense layer, a GELU activation, and another dense layer."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
            },
            "dependencies": [
                "nnx.Module",
                "linears.DenseGeneral",
                "nnx.gelu",
                "Config"
            ],
            "parameters": {
                "hidden_size_for_vit": "The input and output feature dimension of the MLP.",
                "intermediate_size_for_vit": "The size of the hidden layer within the MLP.",
                "dtype_mm": "The data type for the dense layers' weights and computations.",
                "matmul_precision": "The precision for matrix multiplication operations in the dense layers."
            },
            "notes": [
                "This module is a core component of the `Llama4VisionEncoderLayer`, serving as its feed-forward block."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLP block by creating two dense layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes `vit_encoder_layer_mlp_fc1` as a `DenseGeneral` layer mapping from `hidden_size_for_vit` to `intermediate_size_for_vit`.",
                        "Initializes `vit_encoder_layer_mlp_fc2` as a `DenseGeneral` layer mapping from `intermediate_size_for_vit` back to `hidden_size_for_vit`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.DenseGeneral",
                        "Config",
                        "nnx.Rngs"
                    ],
                    "notes": [
                        "Both dense layers are configured to use a bias."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the MLP.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Pass the input `hidden_states` through the first dense layer (`vit_encoder_layer_mlp_fc1`).",
                        "Apply a non-approximate GELU activation function.",
                        "Pass the result through the second dense layer (`vit_encoder_layer_mlp_fc2`).",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "nnx.gelu"
                    ],
                    "notes": [
                        "The output shape is identical to the input shape."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP2",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP2(nnx.Module):\n  \"\"\"MLP block for Llama4VisionPixelShuffleMLP.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.vit_pixel_shuffle_mlp_fc1 = linears.DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.projector_input_dim_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.vit_pixel_shuffle_mlp_fc2 = linears.DenseGeneral(\n        in_features_shape=self.config.projector_input_dim_for_vit,\n        out_features_shape=self.config.projector_output_dim_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.dropout = linears.Dropout(rate=self.config.projector_dropout_for_vit, rngs=self.rngs)\n\n  def __call__(self, hidden_states: Array, deterministic: bool = False) -> Array:\n    hidden_states = self.vit_pixel_shuffle_mlp_fc1(hidden_states)\n    hidden_states = nnx.gelu(hidden_states, approximate=False)\n    hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n    hidden_states = self.vit_pixel_shuffle_mlp_fc2(hidden_states)\n    hidden_states = nnx.gelu(hidden_states, approximate=False)\n    return hidden_states",
        "analysis": {
            "module_type": "multi_layer_perceptron",
            "purpose": "A two-layer Multi-Layer Perceptron (MLP) with GELU activations and dropout, designed for use within the Llama4VisionPixelShuffleMLP module.",
            "input": {
                "shape": "[batch_size, sequence_length, intermediate_size_for_vit]",
                "dtype": "The data type specified by config.dtype_mm."
            },
            "processing_steps": [
                "Initializes a first dense layer (vit_pixel_shuffle_mlp_fc1) mapping from `intermediate_size_for_vit` to `projector_input_dim_for_vit`.",
                "Initializes a second dense layer (vit_pixel_shuffle_mlp_fc2) mapping from `projector_input_dim_for_vit` to `projector_output_dim_for_vit`.",
                "Initializes a dropout layer with a rate specified by `config.projector_dropout_for_vit`."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, projector_output_dim_for_vit]"
            },
            "dependencies": [
                "linears.DenseGeneral",
                "linears.Dropout",
                "nnx.gelu"
            ],
            "parameters": {
                "intermediate_size_for_vit": "The input feature dimension for the first linear layer.",
                "projector_input_dim_for_vit": "The intermediate feature dimension, serving as the output of the first linear layer and input to the second.",
                "projector_output_dim_for_vit": "The final output feature dimension of the MLP.",
                "projector_dropout_for_vit": "The dropout rate applied between the two linear layers.",
                "dtype_mm": "The data type used for matrix multiplications in the multi-modal components.",
                "matmul_precision": "The precision for matrix multiplication operations."
            },
            "notes": [
                "This MLP is specifically used in the context of the Llama4VisionPixelShuffleMLP.",
                "Both dense layers are configured without a bias term.",
                "A GELU activation function is applied after each dense layer."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the two-layer MLP transformation to the input hidden states.",
                    "input": {
                        "shape": "[batch_size, sequence_length, intermediate_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Pass the input through the first dense layer (vit_pixel_shuffle_mlp_fc1).",
                        "Apply the GELU activation function.",
                        "Apply the dropout layer.",
                        "Pass the result through the second dense layer (vit_pixel_shuffle_mlp_fc2).",
                        "Apply the GELU activation function again."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "self.vit_pixel_shuffle_mlp_fc1",
                        "nnx.gelu",
                        "self.dropout",
                        "self.vit_pixel_shuffle_mlp_fc2"
                    ],
                    "notes": [
                        "The `deterministic` parameter controls the behavior of the dropout layer; if True, dropout is disabled."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionPixelShuffleMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionPixelShuffleMLP(nnx.Module):\n  \"\"\"Implementation of Llama4VisionPixelShuffleMLP for Llama4 Multi modal model.\n\n  This module applies pixel shuffle operation and MLP to encoded patches.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.pixel_shuffle_ratio = self.config.pixel_shuffle_ratio_for_vit\n    self.pixel_shuffle_mlp = Llama4VisionMLP2(config=config, rngs=self.rngs)\n\n  def __call__(self, encoded_patches: Array, deterministic: bool = False) -> Array:\n    # Apply pixel shuffle operation\n    encoded_patches = pixel_shuffle(encoded_patches, self.pixel_shuffle_ratio)\n\n    # Apply MLP transformation\n    result = self.pixel_shuffle_mlp(encoded_patches, deterministic=deterministic)\n\n    return result",
        "analysis": {
            "module_type": "pixel_shuffle_mlp",
            "purpose": "Applies a pixel shuffle operation followed by a multi-layer perceptron (MLP) to encoded image patches.",
            "input": {
                "shape": "[batch_size, num_patches, channels]",
                "dtype": "Array"
            },
            "processing_steps": [
                "Applies a pixel shuffle operation to the input `encoded_patches` using the `pixel_shuffle` function.",
                "Passes the shuffled patches through the `pixel_shuffle_mlp` (an instance of `Llama4VisionMLP2`)."
            ],
            "output": {
                "shape": "[batch_size, new_num_patches, projector_output_dim_for_vit]"
            },
            "dependencies": [
                "nnx.Module",
                "pixel_shuffle",
                "Llama4VisionMLP2"
            ],
            "parameters": {
                "pixel_shuffle_ratio_for_vit": "The ratio used for the pixel shuffle operation, which rearranges patch data to change spatial and channel dimensions."
            },
            "notes": [
                "This module is part of the vision processing pipeline in the Llama4 multi-modal model."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by setting the configuration and instantiating the `Llama4VisionMLP2` submodule.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets the `pixel_shuffle_ratio` from the config.",
                        "Initializes the `Llama4VisionMLP2` module as `self.pixel_shuffle_mlp`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Llama4VisionMLP2"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass by applying the pixel shuffle and MLP transformation.",
                    "input": {
                        "shape": "[batch_size, num_patches, channels]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calls the `pixel_shuffle` function on the input tensor `encoded_patches`.",
                        "Passes the result through the `self.pixel_shuffle_mlp` module."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches * (pixel_shuffle_ratio**2), projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "pixel_shuffle"
                    ],
                    "notes": [
                        "The `deterministic` flag is passed to the MLP to control dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4MultiModalProjector",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4MultiModalProjector(nnx.Module):\n  \"\"\"Implementation of Llama4MultiModalProjector for Llama4 Multi modal model.\n\n  This module projects vision features to text hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.vit_multi_modal_projector = linears.DenseGeneral(\n        in_features_shape=self.config.vision_output_dim_for_vit,\n        out_features_shape=self.config.base_emb_dim,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, image_features: Array) -> Array:\n    \"\"\"Project image features to text hidden dimension.\n\n    Args:\n      image_features: Input tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]\n\n    Returns:\n      Tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]\n    \"\"\"\n    b, t, c, d = image_features.shape\n\n    # Reshape image_features to [b * t, c, d] and project to text hidden dimension\n    image_features = image_features.reshape(b * t, c, d)\n    hidden_states = self.vit_multi_modal_projector(image_features)\n    _, c, d = hidden_states.shape\n    hidden_states = hidden_states.reshape(b, t, c, d)\n    return hidden_states",
        "analysis": {
            "module_type": "multi_modal_projector",
            "purpose": "Projects vision features from the vision model's output dimension to the text model's hidden dimension using a linear transformation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a `linears.DenseGeneral` layer to map from `config.vision_output_dim_for_vit` to `config.base_emb_dim`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "linears.DenseGeneral"
            ],
            "parameters": {
                "vision_output_dim_for_vit": "The feature dimension of the input vision features.",
                "base_emb_dim": "The target feature dimension, corresponding to the text model's embedding dimension.",
                "dtype_mm": "The data type for the multi-modal projection layer.",
                "matmul_precision": "The precision for the matrix multiplication in the dense layer."
            },
            "notes": [
                "This class is designed to bridge the vision and text modalities in a multi-modal model.",
                "The linear projection is performed without a bias term."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the projector module by creating a `DenseGeneral` linear layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes a `linears.DenseGeneral` layer with input dimension `config.vision_output_dim_for_vit` and output dimension `config.base_emb_dim`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.DenseGeneral",
                        "Config",
                        "Mesh",
                        "nnx.Rngs"
                    ],
                    "notes": [
                        "The method takes configuration objects (`Config`, `Mesh`) and an RNGs object, not tensors."
                    ]
                },
                "__call__": {
                    "purpose": "Projects a batch of image features to the text hidden dimension.",
                    "input": {
                        "shape": "[batch_size, num_patches, channels, vision_output_dim]",
                        "dtype": "The data type specified by `config.dtype_mm`."
                    },
                    "processing_steps": [
                        "Get the shape of the input `image_features` as `[b, t, c, d]`.",
                        "Reshape `image_features` to `[b * t, c, d]` by merging the first two dimensions.",
                        "Apply the `vit_multi_modal_projector` (a `DenseGeneral` layer) to the reshaped tensor.",
                        "Reshape the resulting `hidden_states` back to `[b, t, c, new_d]`."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, channels, base_emb_dim]"
                    },
                    "dependencies": [
                        "self.vit_multi_modal_projector"
                    ],
                    "notes": [
                        "The reshaping operations allow the linear projection to be applied efficiently across the `channels` and `vision_output_dim` dimensions for each patch."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#llama4multimodalprojector_as_linen",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def llama4multimodalprojector_as_linen(config: Config, mesh: Mesh):\n  return nnx_wrappers.to_linen(\n      Llama4MultiModalProjector,\n      config=config,\n      mesh=mesh,\n      name=\"Llama4MultiModalProjector_0\",\n      abstract_init=False,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "Wraps the `Llama4MultiModalProjector` NNX module into a Flax Linen module.",
            "usage": "This function is used as a factory to create a Linen-compatible version of the `Llama4MultiModalProjector`. It takes a `Config` object and a `Mesh` object as input and returns an `nn.Module` instance."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_nope_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_nope_layer(layer_id: int, nope_layer_interval: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` should use RoPE or not (NoPE).\n\n  Args:\n    layer_id: The index of the layer.\n    nope_layer_interval: The interval at which layers should use NoPE.\n\n  Returns:\n    True if the layer should use NoPE, False otherwise.\n  \"\"\"\n  return nope_layer_interval is not None and nope_layer_interval > 0 and (layer_id + 1) % nope_layer_interval == 0",
        "analysis": {
            "module_type": "layer_type_determiner",
            "purpose": "Determines if a given transformer layer should be a 'NoPE' (No Position Embeddings) layer based on its index and a specified interval.",
            "input": {
                "shape": "N/A",
                "dtype": "int"
            },
            "processing_steps": [
                "Check if `nope_layer_interval` is not None and is greater than 0.",
                "Check if the 1-based layer index (`layer_id` + 1) is perfectly divisible by `nope_layer_interval` using the modulo operator.",
                "Return True if all conditions are met, otherwise return False."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "layer_id": "The 0-based index of the layer.",
                "nope_layer_interval": "The interval at which layers should use NoPE. If None or non-positive, NoPE is disabled."
            },
            "notes": [
                "The function uses a 1-based index (`layer_id + 1`) for the modulo calculation. For an interval `N`, layers at indices `N-1`, `2N-1`, `3N-1`, etc., will be designated as NoPE layers."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_moe_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_moe_layer(layer_id: int, interleave_moe_layer_step: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` is MoE layer.\n\n  This function implements a striding pattern. For example:\n  - If moe_layer_stride is 1, all layers are MoE layers.\n  - If moe_layer_stride is 2, layers with index 1, 3, 5, ... are MoE layers.\n\n  Args:\n    layer_id: The 0-based index of the layer being checked.\n    interleave_moe_layer_step: The interval or stride for placing MoE layers.\n\n  Returns:\n    True if the layer is MoE layer, False otherwise.\n  \"\"\"\n  return (\n      interleave_moe_layer_step is not None\n      and interleave_moe_layer_step > 0\n      and (layer_id + 1) % interleave_moe_layer_step == 0\n  )",
        "analysis": {
            "functionality": "This function determines if a specific layer in a neural network should be a Mixture-of-Experts (MoE) layer based on a striding pattern. It checks if the layer's 1-based index is a multiple of a given step value.",
            "usage": "To use this function, provide the 0-based layer index (`layer_id`) and the stride for MoE layers (`interleave_moe_layer_step`). It returns `True` if the layer is an MoE layer according to the stride, and `False` otherwise. For example, with `interleave_moe_layer_step=2`, it returns `True` for `layer_id` 1, 3, 5, etc."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4DecoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4DecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer for Llama4.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      is_nope_layer: bool = False,\n      is_moe_layer: bool = False,\n  ):\n    \"\"\"Initializes the Llama4 decoder layer.\n\n    Args:\n      config: The main model configuration object.\n      mesh: The device mesh used for sharding parameters and activations.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: An `nnx.Rngs` object to provide random numbers.\n      quant: An optional configuration for quantization. Defaults to None.\n      is_nope_layer: If True, this layer will be configured as No Position Embeddings layer. Defaults to False.\n      is_moe_layer: If True, this layer will use a MoE block. Defaults to False as Dense.\n    \"\"\"\n\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.rngs = rngs\n    self.is_nope_layer = is_nope_layer\n    self.is_moe_layer = is_moe_layer\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    # Instead of scaling the query values in the checkpoint conversion (`llama_or_mistral_ckpt`)\n    # we'll do it dynamically in the forward pass of Attention\n    query_pre_attn_scalar = config.head_dim**-0.5\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        is_nope_layer=self.is_nope_layer,\n        use_qk_norm=config.use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        temperature_tuning=config.temperature_tuning,\n        temperature_tuning_scale=0.1,\n        temperature_tuning_floor_scale=8192.0,\n        # note: chunk_attn_window_size is set in the config\n        attention_type=AttentionType.GLOBAL if self.is_nope_layer else AttentionType.CHUNK,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    if self.is_moe_layer:\n      # NOTE: the name Llama4MoEBlock_0 is to ensure reverse compatibility with\n      # existing checkpoints for MoE block.\n      self.Llama4MoEBlock_0 = RoutedAndSharedMoE(\n          config=config,\n          mesh=self.mesh,\n          kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n          kernel_axes=(\"embed\", None),\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          quant=self.quant,\n          rngs=self.rngs,\n      )\n    else:\n      self.mlp = MlpBlock(\n          in_features=config.emb_dim,\n          intermediate_dim=config.mlp_dim,\n          activations=config.mlp_activations,\n          intermediate_dropout_rate=config.dropout_rate,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          config=config,\n          quant=self.quant,\n          model_mode=model_mode,\n          rngs=self.rngs,\n      )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  @property\n  def moe_block(self):\n    return self.Llama4MoEBlock_0\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n    assert cfg.num_experts >= 1, \"Expected the Llama4 config to have `num_experts > 1`.\"\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    # Self-attention block\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    if self.is_moe_layer:\n      mlp_lnx = self.moe_block(hidden_states)\n    else:\n      mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "llama4_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for the Llama4 model, including self-attention, an optional Mixture-of-Experts (MoE) or standard MLP block, and residual connections.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "config.dtype (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Applies pre-attention RMS normalization to the input.",
                "Performs self-attention on the normalized input.",
                "Adds the attention output to the original input via a residual connection.",
                "Applies post-attention RMS normalization.",
                "Processes the result through either a Mixture-of-Experts (MoE) block or a standard MLP block, based on the `is_moe_layer` flag.",
                "Adds the MLP/MoE output to the result of the first residual connection via a second residual connection.",
                "Applies dropout for regularization."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "RoutedAndSharedMoE",
                "Dropout",
                "max_utils.get_batch_seq_len_for_mode"
            ],
            "parameters": {
                "is_moe_layer": "A boolean flag that determines whether to use a Mixture-of-Experts block (`RoutedAndSharedMoE`) or a standard feed-forward network (`MlpBlock`).",
                "is_nope_layer": "A boolean flag that, if True, configures the attention mechanism to not use positional embeddings (NoPE) and use global attention.",
                "config.emb_dim": "The embedding dimension of the model.",
                "config.mlp_dim": "The intermediate dimension of the MLP block.",
                "config.num_query_heads": "The number of query heads in the self-attention mechanism.",
                "config.num_kv_heads": "The number of key/value heads in the self-attention mechanism.",
                "config.dropout_rate": "The dropout rate used for regularization.",
                "model_mode": "Specifies the operational mode (e.g., 'train', 'prefill', 'autoregressive'), which influences internal configurations."
            },
            "notes": [
                "The layer's architecture is conditional; it instantiates and uses either an MoE block or an MLP block but not both.",
                "The query scaling factor (1/sqrt(head_dim)) is applied dynamically within the `Attention` module.",
                "The name `Llama4MoEBlock_0` is used for the MoE block to ensure backward compatibility with existing checkpoints.",
                "If `config.scan_layers` is True, the `__call__` method returns a tuple `(output, None)` for compatibility with JAX's `scan` transformation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes all sub-modules of the decoder layer, including normalization layers, the attention mechanism, and either an MLP or MoE block.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine dummy input shapes based on `model_mode`.",
                        "Initialize `pre_self_attention_layer_norm` (RMSNorm).",
                        "Initialize the `self_attention` module with detailed configuration.",
                        "Initialize `post_self_attention_layer_norm` (RMSNorm).",
                        "Conditionally initialize either `Llama4MoEBlock_0` (RoutedAndSharedMoE) or `mlp` (MlpBlock) based on `is_moe_layer`.",
                        "Initialize the `dropout` layer.",
                        "Set logical axis names for activations based on `model_mode`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RMSNorm",
                        "Attention",
                        "RoutedAndSharedMoE",
                        "MlpBlock",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode"
                    ],
                    "notes": [
                        "The initialization is highly dependent on the provided `Config` object and `model_mode` string."
                    ]
                },
                "moe_block": {
                    "purpose": "Provides property-based access to the Mixture-of-Experts block.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the `self.Llama4MoEBlock_0` attribute."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RoutedAndSharedMoE"
                    ],
                    "notes": [
                        "This is a convenience property to get the MoE module, which is named `Llama4MoEBlock_0` for checkpoint compatibility."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying attention and feed-forward transformations.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "config.dtype"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization.",
                        "Pass the result through the self-attention block.",
                        "Add the attention output to the original input (first residual connection).",
                        "Apply post-attention RMS normalization.",
                        "Conditionally pass the result through the MoE block or the MLP block.",
                        "Add the MLP/MoE output to the result of the first residual connection (second residual connection).",
                        "Apply dropout.",
                        "Optionally record internal activation metrics.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "self.pre_self_attention_layer_norm",
                        "self.self_attention",
                        "self.post_self_attention_layer_norm",
                        "self.moe_block",
                        "self.mlp",
                        "self.dropout",
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The method takes additional arguments like `decoder_segment_ids` and `decoder_positions` which are passed to the attention mechanism.",
                        "The control flow depends on `self.is_moe_layer` to select between the MoE and MLP paths.",
                        "If `config.scan_layers` is True, the output is a tuple `(layer_output, None)`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4ScannableBlock",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4ScannableBlock(nnx.Module):\n  \"\"\"A repeatable block given nope_layer_interval and interleave_moe_layer_step.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      nope_layer_interval: int = 1,\n      interleave_moe_layer_step: int = 1,\n  ):\n    \"\"\"Initializes the scannable block.\n\n    Args:\n      config: The main model configuration object.\n      mesh: The device mesh used for sharding parameters and activations.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: An `nnx.Rngs` object to provide random numbers for initialization.\n      quant: An optional configuration for quantization. Defaults to None.\n      nope_layer_interval: Specifies the interval for inserting a NoPE layer.\n      interleave_moe_layer_step: Specifies the interval for inserting a MoE layer.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n    self.nope_layer_interval = nope_layer_interval\n    self.interleave_moe_layer_step = interleave_moe_layer_step\n\n    for layer_id in range(self.config.inhomogeneous_layer_cycle_interval):\n      nope_layer = determine_is_nope_layer(layer_id, self.nope_layer_interval)\n      moe_layer = determine_is_moe_layer(layer_id, self.interleave_moe_layer_step)\n      layer_name = f\"layers_{layer_id}\"\n      layer = Llama4DecoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          model_mode=self.model_mode,\n          rngs=self.rngs,\n          quant=self.quant,\n          is_nope_layer=nope_layer,\n          is_moe_layer=moe_layer,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n\n    cfg = self.config\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      y = getattr(self, f\"layers_{layer_id}\")(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "llama4_scannable_block",
            "purpose": "A repeatable block of Llama4 decoder layers that can be configured with interleaved NoPE (No Position Embeddings) and MoE (Mixture of Experts) layers, designed for use with JAX's scan operations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "Llama4DecoderLayer",
                "determine_is_nope_layer",
                "determine_is_moe_layer",
                "Config",
                "Mesh",
                "nnx.Rngs",
                "Quant"
            ],
            "parameters": {
                "inhomogeneous_layer_cycle_interval": "The number of unique decoder layers to create within this block, forming a cycle.",
                "nope_layer_interval": "Specifies the interval for inserting a NoPE (No Position Embeddings) layer.",
                "interleave_moe_layer_step": "Specifies the interval for inserting a MoE (Mixture of Experts) layer.",
                "scan_layers": "A boolean from the config that, if true, adjusts the output format to be compatible with JAX's `lax.scan`."
            },
            "notes": [
                "This block is designed to be a repeatable unit, often used within a `lax.scan` operation over the main decoder stack.",
                "The specific configuration (standard, NoPE, or MoE) of each internal `Llama4DecoderLayer` is determined during initialization based on its index."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the scannable block by creating a sequence of `Llama4DecoderLayer` instances based on the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (`config`, `mesh`, `model_mode`, etc.).",
                        "Iterate from `layer_id` 0 to `config.inhomogeneous_layer_cycle_interval - 1`.",
                        "For each `layer_id`, call `determine_is_nope_layer` to check if it's a NoPE layer.",
                        "For each `layer_id`, call `determine_is_moe_layer` to check if it's a MoE layer.",
                        "Instantiate a `Llama4DecoderLayer` with the determined NoPE and MoE flags.",
                        "Assign the created layer to an attribute named `layers_{layer_id}`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Llama4DecoderLayer",
                        "determine_is_nope_layer",
                        "determine_is_moe_layer"
                    ],
                    "notes": [
                        "This method dynamically builds the block's internal layers based on intervals for NoPE and MoE layers."
                    ]
                },
                "__call__": {
                    "purpose": "Processes the input tensor through the sequence of internal decoder layers.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Apply a logical constraint to the input tensor for sharding.",
                        "Apply a checkpoint name for gradient checkpointing.",
                        "Iterate through the pre-initialized `Llama4DecoderLayer` instances.",
                        "Pass the tensor and other arguments through each layer sequentially.",
                        "If `config.scan_layers` is true, unpack the layer's output tuple to get the hidden state.",
                        "Return the final hidden state."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is false: [batch_size, sequence_length, embedding_dim]. If true: a tuple `([batch_size, sequence_length, embedding_dim], None)`."
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "Llama4DecoderLayer"
                    ],
                    "notes": [
                        "The return signature changes based on the `config.scan_layers` flag to support `lax.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoderLayer(nnx.Module):\n  \"\"\"Transformer encoder layer for Llama4 vision model.\"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.hidden_states_shape = (\n        self.config.per_device_batch_size,\n        (self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1,\n        self.config.hidden_size_for_vit,\n    )\n\n    self.input_layer_norm = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.self_attention_vision = Attention(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=(self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=self.hidden_states_shape,\n        inputs_kv_shape=self.hidden_states_shape,\n        float32_qk_product=self.config.float32_qk_product,\n        float32_logits=self.config.float32_logits,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        mesh=self.mesh,\n        dropout_rate=0,\n        name=\"self_attention_vision\",\n        attention_type=AttentionType.FULL,\n        is_nope_layer=False,\n        use_bias_in_projections=True,\n        is_vision=True,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / math.sqrt(self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit),\n        # The vision encoder processes an image in a single forward pass to produce\n        # embeddings. It doesn't have the concept of \"prefill\" and \"autoregressive\"\n        # steps that a text decoder has. Therefore, it doesn't need a KV cache for\n        # its self-attention mechanism.\n        model_mode=MODEL_MODE_TRAIN,\n        rngs=self.rngs,\n    )\n    self.post_attention_layer_norm = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.Llama4VisionMLP_0 = Llama4VisionMLP(config=self.config, rngs=self.rngs)\n\n  def __call__(\n      self,\n      hidden_states: Array,\n      deterministic: bool = False,\n  ):\n    residual = hidden_states\n    hidden_states = self.input_layer_norm(hidden_states)\n    hidden_states = self.self_attention_vision(\n        inputs_q=hidden_states,\n        inputs_kv=hidden_states,\n        deterministic=deterministic,\n    )\n    hidden_states = residual + hidden_states\n    residual = hidden_states\n    hidden_states = self.post_attention_layer_norm(hidden_states)\n    hidden_states = self.Llama4VisionMLP_0(hidden_states)\n    hidden_states = residual + hidden_states\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_encoder_layer",
            "purpose": "A single transformer encoder layer for the Llama4 vision model, applying self-attention and a feed-forward network with pre-normalization and residual connections.",
            "input": {
                "shape": "[per_device_batch_size, (image_size_for_vit // patch_size_for_vit) ** 2 + 1, hidden_size_for_vit]",
                "dtype": "config.dtype_mm"
            },
            "processing_steps": [
                "Store the input `hidden_states` as `residual`.",
                "Apply `input_layer_norm` to `hidden_states`.",
                "Pass the normalized `hidden_states` through the `self_attention_vision` module.",
                "Add the attention output to the original `residual` (first residual connection).",
                "Store the result of the first residual connection as the new `residual`.",
                "Apply `post_attention_layer_norm` to the new `hidden_states`.",
                "Pass the normalized states through the `Llama4VisionMLP_0` module.",
                "Add the MLP output to the second `residual` (second residual connection).",
                "Return the final `hidden_states`."
            ],
            "output": {
                "shape": "[per_device_batch_size, (image_size_for_vit // patch_size_for_vit) ** 2 + 1, hidden_size_for_vit]"
            },
            "dependencies": [
                "nnx.LayerNorm",
                "Attention",
                "Llama4VisionMLP"
            ],
            "parameters": {
                "hidden_size_for_vit": "The dimensionality of the token embeddings and hidden states for the vision transformer.",
                "num_attention_heads_for_vit": "The number of attention heads in the self-attention mechanism.",
                "normalization_layer_epsilon": "The epsilon value used in the LayerNorm layers for numerical stability.",
                "image_size_for_vit": "The size of the input image for the vision transformer.",
                "patch_size_for_vit": "The size of the patches the image is divided into."
            },
            "notes": [
                "This layer implements a pre-normalization transformer architecture, where LayerNorm is applied before both the self-attention and MLP sub-layers.",
                "Two residual connections are used: one after the self-attention block and another after the MLP block.",
                "The self-attention module is specifically configured for vision processing, operating in a non-causal, full-attention mode without a KV cache, as indicated by `model_mode=MODEL_MODE_TRAIN`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the submodules of the encoder layer, including two LayerNorms, a self-attention block, and an MLP.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the expected shape of the hidden states.",
                        "Initialize `input_layer_norm` (nnx.LayerNorm).",
                        "Initialize `self_attention_vision` (Attention) with vision-specific parameters.",
                        "Initialize `post_attention_layer_norm` (nnx.LayerNorm).",
                        "Initialize `Llama4VisionMLP_0` (Llama4VisionMLP)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "nnx.LayerNorm",
                        "Attention",
                        "Llama4VisionMLP"
                    ],
                    "notes": [
                        "The `Attention` module is configured with `model_mode=MODEL_MODE_TRAIN` because the vision encoder processes an image in a single forward pass and does not require prefill or autoregressive modes."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the transformer encoder layer.",
                    "input": {
                        "shape": "[per_device_batch_size, (image_size_for_vit // patch_size_for_vit) ** 2 + 1, hidden_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Apply pre-attention LayerNorm.",
                        "Apply self-attention.",
                        "Add the first residual connection.",
                        "Apply post-attention LayerNorm.",
                        "Apply the MLP block.",
                        "Add the second residual connection."
                    ],
                    "output": {
                        "shape": "[per_device_batch_size, (image_size_for_vit // patch_size_for_vit) ** 2 + 1, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "self.input_layer_norm",
                        "self.self_attention_vision",
                        "self.post_attention_layer_norm",
                        "self.Llama4VisionMLP_0"
                    ],
                    "notes": [
                        "The `deterministic` flag is passed to the self-attention module to control its behavior (e.g., dropout), although dropout is configured to be 0 for this layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoder",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoder(nnx.Module):\n  \"\"\"Transformer encoder consisting of multiple Llama4VisionEncoderLayer layers.\n\n  This encoder is based on the PyTorch reference implementation and uses multiple\n  encoder layers to process vision input.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"layers_{lyr}\"\n      layer = Llama4VisionEncoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          rngs=self.rngs,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(self, hidden_states: Array, deterministic: bool = False):\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"layers_{lyr}\"\n      layer = getattr(self, layer_name)\n      hidden_states = layer(hidden_states, deterministic=deterministic)\n    return hidden_states",
        "analysis": {
            "module_type": "vision_transformer_encoder",
            "purpose": "A Transformer encoder that processes vision embeddings by sequentially applying multiple Llama4VisionEncoderLayer instances.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Array (defined by config.dtype_mm)"
            },
            "processing_steps": [
                "Initializes `config.num_hidden_layers_for_vit` instances of `Llama4VisionEncoderLayer`.",
                "In the forward pass, sequentially applies each `Llama4VisionEncoderLayer` to the input `hidden_states` in a loop."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "Llama4VisionEncoderLayer",
                "flax.nnx.Module"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters, notably `num_hidden_layers_for_vit`.",
                "mesh": "JAX device mesh used for sharding model parameters and activations."
            },
            "notes": [
                "This module acts as a container and orchestrator for a stack of `Llama4VisionEncoderLayer`s.",
                "The number of layers is determined by `config.num_hidden_layers_for_vit`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the encoder by creating and storing a specified number of `Llama4VisionEncoderLayer` instances.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates from 0 to `config.num_hidden_layers_for_vit`.",
                        "Instantiates a `Llama4VisionEncoderLayer` for each iteration.",
                        "Uses `setattr` to assign the layer to the instance with a name like `layers_{lyr}`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Llama4VisionEncoderLayer"
                    ],
                    "notes": [
                        "The layers are dynamically named and attached to the class instance."
                    ]
                },
                "__call__": {
                    "purpose": "Processes the input tensor by passing it sequentially through all the encoder layers.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Iterate from 0 to `config.num_hidden_layers_for_vit`.",
                        "Retrieve the corresponding layer using `getattr`.",
                        "Call the layer with the current `hidden_states`.",
                        "Update `hidden_states` with the output of the layer.",
                        "Return the final `hidden_states`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `deterministic` flag is passed down to each sub-layer.",
                        "The output shape is identical to the input shape."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionModel",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionModel(nnx.Module):\n  \"\"\"Llama4 vision model for processing image inputs.\n\n  This model extracts patches from input image tiles and processes them\n  through Llama4VisionEncoder and other vision-specific layers.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.scale = self.config.hidden_size_for_vit**-0.5\n    self.num_patches = (self.config.tile_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1\n    self.initializer = nnx.initializers.normal(self.scale)\n\n    self.class_embedding = nnx.Param(\n        self.initializer(self.rngs.params(), (self.config.hidden_size_for_vit,), self.config.dtype_mm)\n    )\n    self.positional_embedding_vlm = nnx.Param(\n        self.initializer(self.rngs.params(), (self.num_patches, self.config.hidden_size_for_vit), self.config.dtype_mm)\n    )\n    self.layernorm_pre = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit,\n        epsilon=self.config.normalization_layer_epsilon,\n        dtype=self.config.dtype_mm,\n        rngs=self.rngs,\n    )\n    self.layernorm_post = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit,\n        epsilon=self.config.normalization_layer_epsilon,\n        dtype=self.config.dtype_mm,\n        rngs=self.rngs,\n    )\n\n    self.Llama4UnfoldConvolution_0 = Llama4UnfoldConvolution(config=self.config, rngs=self.rngs)\n    self.Llama4VisionEncoder_0 = Llama4VisionEncoder(config=self.config, mesh=self.mesh, rngs=self.rngs)\n    self.Llama4VisionPixelShuffleMLP_0 = Llama4VisionPixelShuffleMLP(config=self.config, rngs=self.rngs)\n\n  def __call__(\n      self,\n      pixel_values: Array,\n      output_attentions: None | bool = None,\n      output_hidden_states: None | bool = None,\n      return_dict: None | bool = None,\n      deterministic: None | bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the Llama4 vision model.\n\n    Args:\n      inputs: Input tensor of shape:\n              [batch_size * num_images, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]\n      deterministic: Whether to use deterministic mode (disables dropout)\n\n    Returns:\n      Final hidden states from the vision encoder of shape:\n      [batch_size * num_images, num_tiles, num_patches, vision_output_dim_for_vit]\n    \"\"\"\n    # Reshape pixel values to combine batch and num_tiles dimensions\n    b, t, c, h, w = pixel_values.shape\n    pixel_values = jnp.reshape(pixel_values, [b * t, c, h, w])\n\n    hidden_states = self.Llama4UnfoldConvolution_0(pixel_values)\n\n    # Add class embedding to the beginning of the sequence\n    class_embedding_expanded = jnp.expand_dims(jnp.expand_dims(self.class_embedding, axis=0), axis=0)\n    class_embedding = jnp.broadcast_to(\n        class_embedding_expanded, (hidden_states.shape[0], 1, self.config.hidden_size_for_vit)\n    )\n    hidden_states = jnp.concatenate([hidden_states, class_embedding], axis=1)\n\n    # Add positional embedding\n    hidden_states += self.positional_embedding_vlm\n\n    # Transformation layers\n    hidden_states = self.layernorm_pre(hidden_states)\n    hidden_states = self.Llama4VisionEncoder_0(hidden_states)\n    hidden_states = self.layernorm_post(hidden_states)\n    hidden_states = hidden_states[:, :-1, :]\n\n    hidden_states = self.Llama4VisionPixelShuffleMLP_0(hidden_states)\n\n    # Reshape hidden states\n    _, patch_num, patch_dim = hidden_states.shape\n    hidden_states = jnp.reshape(hidden_states, [b, t, patch_num, patch_dim])\n\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_model",
            "purpose": "A vision model that processes image tiles by extracting patches, encoding them through a vision transformer, and applying further transformations to produce image features.",
            "input": {
                "shape": "[batch_size * num_images, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                "dtype": "config.dtype_mm"
            },
            "processing_steps": [
                "Reshapes the input `pixel_values` to combine the batch and tile dimensions.",
                "Extracts and projects image patches using `Llama4UnfoldConvolution_0`.",
                "Prepends a learnable class embedding to the sequence of patch embeddings.",
                "Adds positional embeddings to the sequence.",
                "Applies pre-encoder layer normalization (`layernorm_pre`).",
                "Processes the sequence through the `Llama4VisionEncoder_0`.",
                "Applies post-encoder layer normalization (`layernorm_post`).",
                "Removes the class embedding token from the sequence.",
                "Applies the `Llama4VisionPixelShuffleMLP_0` transformation.",
                "Reshapes the final hidden states to separate the batch and tile dimensions."
            ],
            "output": {
                "shape": "[batch_size * num_images, num_tiles, num_patches, vision_output_dim_for_vit]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.LayerNorm",
                "Llama4UnfoldConvolution",
                "Llama4VisionEncoder",
                "Llama4VisionPixelShuffleMLP",
                "jax.numpy"
            ],
            "parameters": {
                "config": "A Config object containing model parameters like hidden sizes, patch sizes, and data types.",
                "mesh": "A JAX device mesh used for sharding parameters and activations."
            },
            "notes": [
                "This model follows a standard Vision Transformer (ViT) architecture, including patch extraction, a class token, positional embeddings, and a transformer encoder.",
                "The class token is used during the encoder stage but is removed before the final pixel shuffle and MLP projection.",
                "The model is designed to process images as a collection of tiles, which are processed in a batch-like manner."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the vision model's layers, embeddings, and parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculates scale and num_patches based on the config.",
                        "Initializes `class_embedding` and `positional_embedding_vlm` as learnable parameters (`nnx.Param`).",
                        "Initializes pre and post `nnx.LayerNorm` layers.",
                        "Instantiates `Llama4UnfoldConvolution_0`, `Llama4VisionEncoder_0`, and `Llama4VisionPixelShuffleMLP_0` sub-modules."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "nnx.initializers.normal",
                        "nnx.Param",
                        "nnx.LayerNorm",
                        "Llama4UnfoldConvolution",
                        "Llama4VisionEncoder",
                        "Llama4VisionPixelShuffleMLP"
                    ],
                    "notes": [
                        "The model parameters and sub-modules are configured using the provided `Config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the vision model, transforming image pixel values into feature embeddings.",
                    "input": {
                        "shape": "[batch_size * num_images, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                        "dtype": "Array (typically float)"
                    },
                    "processing_steps": [
                        "Reshapes pixel_values to combine batch and tile dimensions.",
                        "Calls `Llama4UnfoldConvolution_0` to get initial hidden states from patches.",
                        "Prepends a broadcasted class embedding to the hidden states.",
                        "Adds `positional_embedding_vlm` to the hidden states.",
                        "Applies `layernorm_pre`.",
                        "Passes hidden states through `Llama4VisionEncoder_0`.",
                        "Applies `layernorm_post`.",
                        "Removes the class token from the sequence.",
                        "Passes the result through `Llama4VisionPixelShuffleMLP_0`.",
                        "Reshapes the final tensor to reintroduce the tile dimension."
                    ],
                    "output": {
                        "shape": "[batch_size * num_images, num_tiles, num_patches, vision_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "Llama4UnfoldConvolution",
                        "Llama4VisionEncoder",
                        "Llama4VisionPixelShuffleMLP"
                    ],
                    "notes": [
                        "The `deterministic` flag is passed to sub-modules to control behaviors like dropout.",
                        "The arguments `output_attentions`, `output_hidden_states`, and `return_dict` are included in the method signature for API consistency but are not used in the implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#llama4visionmodel_as_linen",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def llama4visionmodel_as_linen(config: Config, mesh: Mesh) -> nn.Module:\n  return nnx_wrappers.to_linen(\n      Llama4VisionModel,\n      config=config,\n      mesh=mesh,\n      name=\"Llama4VisionModel_0\",\n      abstract_init=False,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "nnx_to_linen_wrapper_factory",
            "purpose": "Wraps the NNX `Llama4VisionModel` class into a Flax Linen module, making it compatible with Linen APIs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `Llama4VisionModel` NNX module into a Flax Linen module."
            ],
            "output": {
                "shape": "Returns an instance of a Flax Linen module (`nn.Module`)."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Llama4VisionModel",
                "initializers.variable_to_logically_partitioned",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "The main model configuration object.",
                "mesh": "The JAX device mesh used for sharding parameters and activations."
            },
            "notes": [
                "This function acts as a factory to create a Linen-compatible version of the `Llama4VisionModel`.",
                "The resulting Linen module is explicitly named 'Llama4VisionModel_0'.",
                "The `metadata_fn` is used to apply logical partitioning rules to the model's variables during conversion."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/mistral.py#MistralDecoderLayer",
        "file_path": "src/MaxText/layers/mistral.py",
        "code_block": "class MistralDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      model_mode: str,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        quant=self.quant,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      page_state: None | int = None,\n      slot: None | int = None,\n      previous_chunk=None,\n  ):\n\n    cfg = self.config\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # MLP block.\n    mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mistral_decoder_layer",
            "purpose": "Implements a single layer of a Mistral-style transformer decoder, including self-attention, an MLP block, and residual connections with RMS normalization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes sub-modules: RMSNorm for pre and post self-attention normalization, an Attention module, an MlpBlock, and a Dropout layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "Config",
                "Mesh",
                "Quant",
                "max_utils"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like embedding dimension, MLP dimension, number of heads, and dropout rate.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "mesh": "A JAX device mesh for parallel computation.",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This class is designed to be a building block within a larger transformer model.",
                "It uses a pre-normalization architecture (LayerNorm -> Attention -> Residual -> LayerNorm -> MLP -> Residual).",
                "The class `MistralDecoderLayerToLinen` is a wrapper that converts this nnx.Module to a Flax Linen compatible module."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the sub-modules of the decoder layer, including attention, MLP, and normalization layers, based on the provided configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, quant, and rngs.",
                        "Initialize `pre_self_attention_layer_norm` (RMSNorm).",
                        "Initialize `self_attention` (Attention).",
                        "Initialize `post_self_attention_layer_norm` (RMSNorm).",
                        "Initialize `mlp` (MlpBlock).",
                        "Initialize `dropout` (Dropout)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_utils.get_batch_seq_len_for_mode",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "Config",
                        "Mesh",
                        "Quant"
                    ],
                    "notes": [
                        "The initialization of sub-modules like `Attention` and `MlpBlock` depends heavily on the `model_mode` and `config` parameters."
                    ]
                },
                "__call__": {
                    "purpose": "Processes an input tensor through one complete decoder layer, applying self-attention, MLP, and residual connections.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "Depends on config.dtype (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Pass the normalized tensor through the self-attention module.",
                        "Add the output of the attention module to the original input (first residual connection).",
                        "Apply post-attention RMS normalization to the result of the first residual connection.",
                        "Pass the result through the MLP block.",
                        "Add the output of the MLP block to the result of the first residual connection (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally record internal activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final processed tensor."
                    ],
                    "output": {
                        "shape": "If config.scan_layers is False: [batch_size, sequence_length, emb_dim]. If True: a tuple of (tensor with shape [batch_size, sequence_length, emb_dim], None)."
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is active.",
                        "The return signature changes based on the `config.scan_layers` flag, a pattern used for layers inside a `flax.linen.scan`.",
                        "It accepts several arguments for managing attention caches in different decoding modes (e.g., `page_state`, `slot`, `previous_chunk`)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/mixtral.py#MixtralDecoderLayer",
        "file_path": "src/MaxText/layers/mixtral.py",
        "code_block": "class MixtralDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  @nn.compact\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant = None,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.MoeBlock_0 = moe.RoutedMoE(\n        config=config,\n        num_experts=config.num_experts,\n        num_experts_per_tok=config.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=config.mlp_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    load_balance_loss = None\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx, load_balance_loss = self.MoeBlock_0(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mixtral_decoder_layer",
            "purpose": "Implements a single transformer decoder layer for a Mixtral model, featuring a self-attention mechanism and a Mixture-of-Experts (MoE) block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes sub-modules: RMSNorm for pre and post-attention normalization, an Attention module, a RoutedMoE block, and a Dropout layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "max_utils",
                "RMSNorm",
                "Attention",
                "moe.RoutedMoE",
                "Dropout",
                "initializers"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like embedding dimension, number of heads, MLP dimension, etc.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This class encapsulates the core building block of a Mixtral decoder, combining self-attention with a sparse MoE layer instead of a dense MLP."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the sub-modules of the Mixtral decoder layer, including normalization, self-attention, and the Mixture-of-Experts block.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine dummy input shapes based on config and model_mode.",
                        "Initialize `pre_self_attention_layer_norm` (RMSNorm).",
                        "Initialize `self_attention` (Attention).",
                        "Initialize `post_self_attention_layer_norm` (RMSNorm).",
                        "Initialize `MoeBlock_0` (moe.RoutedMoE).",
                        "Initialize `dropout` (Dropout)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_utils.get_batch_seq_len_for_mode",
                        "RMSNorm",
                        "Attention",
                        "moe.RoutedMoE",
                        "Dropout",
                        "initializers.nd_dense_init"
                    ],
                    "notes": [
                        "The initialization of sub-modules is heavily dependent on the provided `Config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying self-attention and the MoE block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "Determined by config.dtype (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Perform self-attention on the normalized input.",
                        "Add the attention output to the original input (first residual connection).",
                        "Apply post-attention RMS normalization.",
                        "Pass the result through the Mixture-of-Experts (MoE) block, which returns the MLP output and a load balancing loss.",
                        "Add the MoE output to the result of the first residual connection (second residual connection).",
                        "Apply dropout.",
                        "Optionally record intermediate metrics like MoE load balancing loss and activation statistics.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]. If config.scan_layers is true, returns a tuple `(output, None)`."
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "jnp"
                    ],
                    "notes": [
                        "The method includes two residual connections: one after the attention block and another after the MoE block.",
                        "It uses `sow` to collect intermediate values like the MoE load balancing loss for later use, e.g., in the total loss calculation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinenPure",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinenPure(nn.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n  # pylint: enable=attribute-defined-outside-init\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    kwargs[\"model_mode\"] = model_mode\n    return nn.Module.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    kwargs[\"model_mode\"] = model_mode\n    return nn.Module.apply(module, *args, **kwargs)\n\n  def setup(self):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n    self.shared_embedding = embed_as_linen(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        name=\"token_embedder\",\n        config=cfg,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n    self.decoder = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      self.mtp_block = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n\n  def logits_from_hidden_states(self, hidden_states, deterministic, model_mode):\n    \"\"\"\n    Compute logits from hidden states (wrapping decoder._apply_output_head).\n    This function is only used for vocabulary tiling.\n    \"\"\"\n    # pylint: disable=protected-access\n    logits = self.decoder._apply_output_head(\n        shared_embedding=self.shared_embedding,\n        y=hidden_states,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    return logits\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      encoder_image_masks: None | jnp.ndarray = None,\n      enable_dropout=True,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method=None,\n  ):\n    \"\"\"Applies Transformer decoder-branch on encoded-input and target.\n\n    Args:\n      true_length: (Optional) Prompt length before padding\n      slot: (Optional) An integer representing the decode batch index selected\n        for this request.\n    \"\"\"\n\n    if decoder_segment_ids is not None and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.shared_embedding,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n        image_masks=encoder_image_masks,\n    )\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    if self.is_initializing() and self.config.mtp_num_layers > 0:\n      if decoder_target_tokens is None:\n        dummy_shape = decoder_input_tokens.shape\n        decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.shared_embedding,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n          model_mode=model_mode,\n      )\n\n    return logits",
        "analysis": {
            "module_type": "autoregressive_transformer",
            "purpose": "Defines an autoregressive transformer model using Flax Linen, handling token embeddings, optional vision encoding, and decoding to produce logits.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes shared embedding, an optional vision encoder, a decoder, and an optional Multi-Token Prediction (MTP) block in the `setup` method.",
                "In the `__call__` method, it processes optional image inputs through the vision encoder.",
                "Passes token inputs and optional image embeddings to the decoder to compute logits and hidden states.",
                "If MTP is enabled, it uses the hidden states and target tokens to compute and sow an auxiliary loss.",
                "Returns the final logits."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization",
                "MaxText.layers.embeddings.embed_as_linen",
                "MaxText.layers.encoders.VisionEncoder",
                "MaxText.layers.decoders.Decoder",
                "MaxText.layers.multi_token_prediction.MultiTokenPredictionBlock"
            ],
            "parameters": {
                "config": "Configuration object (Config) containing model hyperparameters like vocab_size, emb_dim, dtype, etc.",
                "mesh": "JAX sharding mesh for distributed computation.",
                "quant": "Quantization configuration object (Quant).",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill', 'autoregressive')."
            },
            "notes": [
                "This is a pure Flax Linen implementation of the transformer.",
                "It supports multimodal inputs via an optional VisionEncoder.",
                "It can include a Multi-Token Prediction (MTP) block for auxiliary loss calculation during training.",
                "The `init` and `apply` methods are overridden to handle the `model_mode` parameter correctly, cloning the module for the specific mode."
            ],
            "methods": {
                "init": {
                    "purpose": "Initializes the model for a specific `model_mode`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Clones the module with the specified `model_mode`.",
                        "Adds `model_mode` to kwargs.",
                        "Calls the parent `nn.Module.init`."
                    ],
                    "output": {
                        "shape": "Initialized model parameters (Flax FrozenDict)."
                    },
                    "dependencies": [
                        "nn.Module.init"
                    ],
                    "notes": [
                        "This method ensures that the model is initialized with the correct configuration for the given operational mode."
                    ]
                },
                "apply": {
                    "purpose": "Applies the model for a specific `model_mode`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Clones the module with the specified `model_mode`.",
                        "Adds `model_mode` to kwargs.",
                        "Calls the parent `nn.Module.apply`."
                    ],
                    "output": {
                        "shape": "The result of the model's forward pass."
                    },
                    "dependencies": [
                        "nn.Module.apply"
                    ],
                    "notes": [
                        "This method ensures that the model is applied with the correct configuration for the given operational mode."
                    ]
                },
                "setup": {
                    "purpose": "Initializes the sub-modules of the transformer like embeddings, decoder, and optional vision encoder.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes the shared token embedding layer (`shared_embedding`) using `embed_as_linen`.",
                        "Initializes the `VisionEncoder` if `config.use_multimodal` is true.",
                        "Initializes the `Decoder` module.",
                        "Initializes the `MultiTokenPredictionBlock` if `config.mtp_num_layers > 0`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "embed_as_linen",
                        "VisionEncoder",
                        "Decoder",
                        "MultiTokenPredictionBlock"
                    ],
                    "notes": [
                        "This method is called by Flax during the model's initialization."
                    ]
                },
                "logits_from_hidden_states": {
                    "purpose": "Computes logits from final hidden states, primarily for vocabulary tiling.",
                    "input": {
                        "shape": "hidden_states: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "jnp.ndarray (e.g., float32)"
                    },
                    "processing_steps": [
                        "Calls the decoder's internal `_apply_output_head` method with the shared embedding and hidden states."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "Decoder._apply_output_head"
                    ],
                    "notes": [
                        "This is a helper function specifically used for vocabulary tiling scenarios."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the main forward pass of the transformer model.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], encoder_images (optional): [batch_size, ...]",
                        "dtype": "decoder_input_tokens: int32, decoder_positions: int32, encoder_images: float"
                    },
                    "processing_steps": [
                        "Checks for invalid argument combinations (e.g., `decoder_segment_ids` in autoregressive mode).",
                        "If multimodal, passes `encoder_images` through `vision_encoder` to get `image_embeddings`.",
                        "If multimodal, creates a `bidirectional_mask` based on special token placeholders.",
                        "Calls the `decoder` module with embeddings and other inputs to get `logits` and `hidden_state`.",
                        "Handles dummy tensor creation for MTP block tracing during initialization.",
                        "If MTP is enabled, calls the `mtp_block` with the hidden state and target tokens to compute auxiliary losses.",
                        "Returns the primary `logits`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "VisionEncoder",
                        "Decoder",
                        "MultiTokenPredictionBlock"
                    ],
                    "notes": [
                        "This is the main entry point for the model's forward pass. It orchestrates the vision encoder (if any), the decoder, and the MTP block (if any)."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#transformer_as_linen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "def transformer_as_linen(\n    config: Config,\n    mesh: Mesh,\n    quant: Quant,\n    model_mode: str = MODEL_MODE_TRAIN,\n    *,\n    name: str | None = None,\n) -> nnx_wrappers.ToLinen | TransformerLinenPure:\n  \"\"\"Constructs a Transformer model as a Linen or NNX module.\n\n  This function returns an autoregressive Transformer model as either a Linen module\n  or an NNX-wrapped module, depending on the `config.enable_nnx` flag. The returned module\n  is suitable for training, evaluation, or decoding.\n\n  If `config.enable_nnx` is True, returns a `TransformerLinen` that wraps the NNX-style\n  Transformer for integration with NNX-specific APIs and workflows.\n  Otherwise, returns a pure Flax Linen implementation (`TransformerLinenPure`).\n\n  Args:\n    config (Config): The configuration object specifying model hyperparameters and options.\n    mesh (Mesh): The JAX sharding mesh for device partitioning.\n    quant (Quant): The quantization module or configuration to use.\n    model_mode (str, optional): The operational mode for the model, e.g.\n      training, prefill, or autoregressive. Defaults to `MODEL_MODE_TRAIN`.\n    name (str, optional): Optional module name for Linen/NNX construction.\n\n  Returns:\n    nnx_wrappers.ToLinen | TransformerLinenPure:\n      A constructed Transformer model compatible with the specified framework (Linen or NNX).\n  \"\"\"\n  if config.enable_nnx:\n    return TransformerLinen(\n        Transformer,\n        args=(),\n        kwargs=nn.FrozenDict(\n            {\n                \"mesh\": mesh,\n                \"config\": config,\n                \"quant\": quant,\n                \"model_mode\": model_mode,\n            }\n        ),\n        metadata_fn=initializers.variable_to_logically_partitioned,\n        name=name,\n    )\n  else:\n    return TransformerLinenPure(config, mesh, quant, model_mode=model_mode, name=name)",
        "analysis": {
            "functionality": "This function acts as a factory to construct a Transformer model. It conditionally returns either a pure Flax Linen implementation (`TransformerLinenPure`) or an NNX-based model wrapped in a Linen-compatible interface (`TransformerLinen`), based on a configuration flag.",
            "usage": "Call this function with a configuration object (`Config`), a JAX mesh (`Mesh`), a quantization configuration (`Quant`), and an optional model mode (`str`). It returns an uninitialized Transformer model instance. The specific implementation (Linen or NNX) is determined by `config.enable_nnx`."
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinen(nnx_wrappers.ToLinen):\n  \"\"\"Transformer model as a linen module.\"\"\"\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    kwargs[\"model_mode\"] = model_mode\n    return nnx_wrappers.ToLinen.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    kwargs[\"model_mode\"] = model_mode\n    return nnx_wrappers.ToLinen.apply(module, *args, **kwargs)",
        "analysis": {
            "module_type": "nnx_to_linen_transformer_wrapper",
            "purpose": "A wrapper class that makes an NNX-based Transformer model compatible with the Flax Linen API, specifically handling the dynamic setting of `model_mode` for `init` and `apply` calls.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Inherits from nnx_wrappers.ToLinen to bridge NNX and Linen.",
                "Overrides `init` and `apply` to inject a dynamic `model_mode` into the underlying NNX module's configuration before execution."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.ToLinen",
                "Transformer"
            ],
            "parameters": {
                "model_mode": "The operational mode for the model (e.g., 'train', 'prefill', 'autoregressive'), which is dynamically passed during `init` and `apply` calls."
            },
            "notes": [
                "This class is instantiated by the `transformer_as_linen` factory function when `config.enable_nnx` is True.",
                "It ensures that the `model_mode` can be changed at runtime for both initialization and forward passes without modifying the stored model state."
            ],
            "methods": {
                "init": {
                    "purpose": "Initializes the wrapped NNX model, dynamically setting its `model_mode` based on the provided argument.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Copies the module's stored keyword arguments and updates 'model_mode'.",
                        "Clones the module instance with the updated arguments.",
                        "Calls the parent `ToLinen.init` method with the cloned module and new arguments."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToLinen.init"
                    ],
                    "notes": [
                        "Allows for initializing the model in a specific mode (e.g., 'train') which can affect parameter shapes or initialization logic in the underlying NNX module."
                    ]
                },
                "apply": {
                    "purpose": "Applies the wrapped NNX model (forward pass), dynamically setting its `model_mode` based on the provided argument.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Copies the module's stored keyword arguments and updates 'model_mode'.",
                        "Clones the module instance with the updated arguments.",
                        "Calls the parent `ToLinen.apply` method with the cloned module and new arguments."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToLinen.apply"
                    ],
                    "notes": [
                        "Enables switching the model's behavior at runtime (e.g., from training with dropout to inference without it) by changing the `model_mode` argument."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#Transformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class Transformer(nnx.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  def __init__(self, config: Config, mesh: Mesh, quant: Quant, *, model_mode: str = MODEL_MODE_TRAIN, rngs: nnx.Rngs):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.model_mode = model_mode\n\n    cfg = self.config\n    mesh = self.mesh\n    self.token_embedder = Embed(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        config=cfg,\n        rngs=rngs,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n\n    decoder_linen = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    self.decoder = nnx_wrappers.ToNNX(decoder_linen, rngs=rngs)\n    self.hidden_states = None\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config=cfg, model_mode=model_mode)\n    dummy_decoder_input_tokens = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n    dummy_decoder_positions = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n\n    self.decoder.lazy_init(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=dummy_decoder_input_tokens,\n        decoder_positions=dummy_decoder_positions,\n    )\n\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      mtp_block_linen = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n      self.mtp_block = nnx_wrappers.ToNNX(mtp_block_linen, rngs=rngs)\n\n      self.mtp_block.lazy_init(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=jnp.ones((1, 1, self.config.emb_dim), dtype=self.config.dtype),\n          input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          target_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          target_mask=jnp.ones((1, 1), dtype=jnp.int32),\n          position_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          decoder_segment_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          deterministic=True,\n      )\n\n  def no_op(self, *args, **kwargs):\n    \"\"\"A no-op method to allow the model to be used in a lazy context.\"\"\"\n    return\n\n  def init_cache(self, cache_size: int, batch_size: int, dtype=jnp.float32):\n    \"\"\"Initializes the KV cache for the Transformer.\n\n    Args:\n      cache_size: The maximum size of the KV cache.\n      batch_size: The batch size for which the cache is initialized.\n      dtype: Data type for the cache. Defaults to `jnp.float32`.\n\n    Returns:\n      True if the cache is successfully initialized.\n    \"\"\"\n    return True\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      cache=None,\n      encoder_images: jax.Array | None = None,\n      encoder_image_masks: jax.Array | None = None,\n      enable_dropout=True,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      true_length: int | None = None,\n      slot: int | None = None,\n      page_state: page_manager.PageState | None = None,\n      decoder_target_tokens: jax.Array | None = None,\n      decoder_target_mask: jax.Array | None = None,\n  ):\n    \"\"\"Applies the Zero-1 FSDP wrapped Transformer model.\n\n    This method handles the all-gather operation for model weights before\n    applying the underlying Transformer model, and then releases them.\n\n    Args:\n      decoder_input_tokens: Input tokens for the decoder.\n      decoder_positions: Positional encodings for the decoder inputs.\n      decoder_segment_ids: Segment IDs for the decoder inputs (optional).\n      encoder_images: Encoder images for multimodal models (optional).\n      enable_dropout: Whether to enable dropout. Defaults to True.\n      previous_chunk: Previous chunk for incremental decoding (optional).\n      true_length: True length of the prompt before padding (optional).\n      slot: An integer representing the decode batch index selected for this request (optional).\n      page_state: Page state for paged attention (optional).\n      partition_spec: Partition specification for FSDP all-gather.\n      decoder_target_tokens: Target tokens for the decoder (optional, used in MTP).\n      decoder_target_mask: Target mask for the decoder (optional, used in MTP).\n      nnx_method: Method to call on the NNX module (optional).\n\n    Returns:\n      Logits from the Transformer model.\n    \"\"\"\n    if decoder_segment_ids is not None and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n        image_masks=encoder_image_masks,\n    )\n\n    # Materialize hidden state when vocab tiling is enabled\n    if self.config.num_vocab_tiling > 1:\n      self.hidden_states = hidden_state\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    # if self.is_initializing() and self.config.mtp_num_layers > 0:\n    #   if decoder_target_tokens is None:\n    #     dummy_shape = decoder_input_tokens.shape\n    #     decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n          model_mode=model_mode,\n      )\n\n    return logits",
        "analysis": {
            "module_type": "autoregressive_transformer",
            "purpose": "Defines an autoregressive transformer model using Flax NNX, including token embeddings, an optional vision encoder, a decoder, and an optional multi-token prediction block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes `token_embedder` (Embed).",
                "Initializes `vision_encoder` (VisionEncoder) if `use_multimodal` is true.",
                "Initializes `decoder` by wrapping a Linen `Decoder` with `ToNNX`.",
                "Performs lazy initialization of the `decoder` with dummy inputs.",
                "Initializes `mtp_block` (MultiTokenPredictionBlock) if `mtp_num_layers > 0`.",
                "Performs lazy initialization of the `mtp_block` with dummy inputs."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "Config",
                "Mesh",
                "Quant",
                "Embed",
                "VisionEncoder",
                "Decoder",
                "nnx_wrappers.ToNNX",
                "MultiTokenPredictionBlock",
                "max_utils"
            ],
            "parameters": {
                "config": "Configuration object specifying model hyperparameters and options.",
                "mesh": "The JAX sharding mesh for device partitioning.",
                "quant": "The quantization configuration object.",
                "model_mode": "The operational mode for the model (e.g., training, prefill, autoregressive)."
            },
            "notes": [
                "This is the NNX-based implementation of the Transformer model.",
                "It uses lazy initialization for its decoder and optional MTP block to construct parameters based on dummy inputs."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Transformer model's components, including embeddings, vision encoder (if applicable), decoder, and multi-token prediction block (if applicable).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes `self.token_embedder` using the `Embed` class.",
                        "Conditionally initializes `self.vision_encoder` using `VisionEncoder` if `config.use_multimodal` is true.",
                        "Initializes a Linen `Decoder` and wraps it into an NNX module `self.decoder` using `nnx_wrappers.ToNNX`.",
                        "Creates dummy input tokens and positions based on the model mode.",
                        "Calls `self.decoder.lazy_init` with the dummy inputs to initialize its parameters.",
                        "Conditionally initializes and lazy-initializes `self.mtp_block` if `config.mtp_num_layers > 0`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Embed",
                        "VisionEncoder",
                        "Decoder",
                        "nnx_wrappers.ToNNX",
                        "MultiTokenPredictionBlock",
                        "max_utils.get_batch_seq_len_for_mode"
                    ],
                    "notes": [
                        "The method uses lazy initialization with dummy tensors to build the computational graph and parameters for the decoder and MTP block."
                    ]
                },
                "no_op": {
                    "purpose": "A no-operation method to allow the model to be used in a lazy context without performing any computation.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns immediately without performing any operations."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "init_cache": {
                    "purpose": "A placeholder method for initializing the Key-Value cache for autoregressive decoding.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the boolean value True."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The current implementation is a stub and always returns True, suggesting the actual cache initialization might be handled elsewhere."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the Transformer model to generate logits from input tokens.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], encoder_images: [batch_size, height, width, channels] (optional)",
                        "dtype": "decoder_input_tokens: int32, decoder_positions: int32, encoder_images: float-like (e.g., float32, bfloat16)"
                    },
                    "processing_steps": [
                        "Optionally processes `encoder_images` through `self.vision_encoder` to get `image_embeddings` if the model is multimodal.",
                        "Optionally creates a `bidirectional_mask` for multimodal inputs.",
                        "Calls the `self.decoder` module with token embeddings, positions, and other inputs to get logits and the final hidden state.",
                        "Stores the final hidden state in `self.hidden_states` if vocabulary tiling is enabled.",
                        "If `mtp_num_layers > 0`, calls the `self.mtp_block` with the final hidden state and target tokens to compute auxiliary losses.",
                        "Returns the final logits."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "self.vision_encoder",
                        "self.decoder",
                        "self.mtp_block"
                    ],
                    "notes": [
                        "Handles both standard language modeling and multimodal inputs.",
                        "If the multi-token prediction (MTP) block is enabled, it is called as a 'side-car' to sow auxiliary losses but does not affect the returned logits."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#ZeroOneTransformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class ZeroOneTransformer(nn.Module):\n  \"\"\"\n  A wrapper for the base Transformer model designed to implement the Zero-1\n  FSDP optimization.\n\n  The goal of this optimization is to reduce communication overhead. In the standard\n  FSDP implementation, an all-gather operation on the model weights is performed twice\n  for each gradient accumulation microbatch (once for the forward pass, once for the backward pass).\n  This class changes that behavior. When enabled, it performs the all-gather operation\n  only *once* per full gradient accumulation step. It gathers the full weights into\n  memory, runs all the microbatch forward and backward passes, and then releases the\n  full weights. This trades higher peak memory usage for significantly reduced\n  network communication, which can improve training speed if sufficient memory is\n  available.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n\n  def setup(self):\n    \"\"\"Sets up the underlying Transformer model.\n\n    This method initializes the `self.model` attribute by calling the\n    `transformer_as_linen` factory function.\n    \"\"\"\n    self.model = transformer_as_linen(self.config, self.mesh, self.quant, self.model_mode)\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      encoder_image_masks: None | jnp.ndarray = None,\n      enable_dropout=True,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      partition_spec=None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method: str | None = None,\n  ):\n    \"\"\"Applies the Zero-1 FSDP wrapped Transformer model.\n\n    This method handles the all-gather operation for model weights before\n    applying the underlying Transformer model, and then releases them.\n\n    Args:\n      decoder_input_tokens: Input tokens for the decoder.\n      decoder_positions: Positional encodings for the decoder inputs.\n      decoder_segment_ids: Segment IDs for the decoder inputs (optional).\n      encoder_images: Encoder images for multimodal models (optional).\n      enable_dropout: Whether to enable dropout. Defaults to True.\n      previous_chunk: Previous chunk for incremental decoding (optional).\n      true_length: True length of the prompt before padding (optional).\n      slot: An integer representing the decode batch index selected for this\n        request (optional).\n      page_state: Page state for paged attention (optional).\n      partition_spec: Partition specification for FSDP all-gather.\n      decoder_target_tokens: Target tokens for the decoder (optional, used in\n        MTP).\n      decoder_target_mask: Target mask for the decoder (optional, used in MTP).\n      nnx_method: Method to call on the NNX module (optional).\n\n    Returns:\n      Logits from the Transformer model.\n    \"\"\"\n    if self.is_initializing():\n      return self.model(\n          decoder_input_tokens=decoder_input_tokens,\n          decoder_positions=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          encoder_images=encoder_images,\n          encoder_image_masks=encoder_image_masks,\n          enable_dropout=enable_dropout,\n          model_mode=model_mode,\n          previous_chunk=previous_chunk,\n          true_length=true_length,\n          slot=slot,\n          page_state=page_state,\n      )\n    all_model_weights = all_gather_over_fsdp(\n        self.model.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n    )\n\n    return self.model.apply(\n        all_model_weights,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        encoder_images=encoder_images,\n        encoder_image_masks=encoder_image_masks,\n        enable_dropout=enable_dropout,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n        true_length=true_length,\n        slot=slot,\n        page_state=page_state,\n        mutable=False,\n        decoder_target_tokens=decoder_target_tokens,\n        decoder_target_mask=decoder_target_mask,\n        nnx_method=nnx_method,\n    )",
        "analysis": {
            "module_type": "zero_one_transformer",
            "purpose": "A wrapper for a base Transformer model that implements the Zero-1 FSDP optimization to reduce communication overhead by performing a single all-gather operation per gradient accumulation step.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes an underlying Transformer model using `transformer_as_linen` in the `setup` method.",
                "During the forward pass (`__call__`), it checks if the model is being initialized.",
                "If not initializing, it performs an `all_gather_over_fsdp` operation to collect the full model weights from all devices.",
                "It then calls the underlying model's `apply` method with the gathered weights and input tensors."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext_utils.all_gather_over_fsdp",
                "models.transformer_as_linen",
                "common_types.Config",
                "jax.sharding.Mesh",
                "layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "The configuration object specifying model hyperparameters and options.",
                "mesh": "The JAX sharding mesh for device partitioning.",
                "quant": "The quantization configuration object.",
                "model_mode": "The operational mode for the model, e.g., training or prefill."
            },
            "notes": [
                "This class trades higher peak memory usage for significantly reduced network communication, which can improve training speed.",
                "The core optimization is gathering weights only once per full gradient accumulation step, rather than once per microbatch.",
                "During initialization, the all-gather operation is skipped to allow for proper parameter creation."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the underlying Transformer model by calling the `transformer_as_linen` factory function.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `transformer_as_linen` with the class's config, mesh, quant, and model_mode.",
                        "Assigns the returned model instance to `self.model`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "models.transformer_as_linen"
                    ],
                    "notes": [
                        "This is a standard Flax `setup` method, called automatically during module initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the Transformer model, performing the Zero-1 FSDP optimization by gathering all model weights before the forward pass.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Checks if the model is in its initialization phase via `self.is_initializing()`.",
                        "If initializing, directly calls the underlying `self.model` with the provided arguments.",
                        "If not initializing, gathers the full model weights from all devices using `all_gather_over_fsdp`.",
                        "Invokes the underlying model's `apply` method, passing the gathered weights and the original input arguments."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "maxtext_utils.all_gather_over_fsdp"
                    ],
                    "notes": [
                        "The `partition_spec` argument is used by `all_gather_over_fsdp` to correctly gather the sharded weights."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations(\n    inputs: jax.Array,\n    sort_indices: jax.Array,\n    use_custom_vjp: bool,\n) -> jax.Array:\n  \"\"\"Sort activations by `sort_indices`.\n\n  If `use_custom_vjp=True`, then we use a custom backward pass that\n  reverses the sort order. Specifically, this unsort operation is simply a sort\n  with `jnp.argsort(sort_indices)` as the sort indices. This is only needed in\n  the case where the compiler generates a less efficient backward pass op.\n\n  Note that `use_custom_vjp=True` assumes that `sort_indices` is a permutation\n  of `jnp.arange(inputs.shape[0])`.\n\n  Args:\n    inputs: `(tokens, ...)`-shaped array of input activations to sort.\n    sort_indices: `(tokens,)`-shaped array containing the sort order.\n    use_custom_vjp: Whether to use the explicit backward pass.\n\n  Returns:\n    `(tokens, ...)`-shaped array of input activations sorted by `sort_indices`.\n  \"\"\"\n  assert inputs.shape[0] == sort_indices.shape[0]\n\n  with jax.named_scope(\"sort_activations\"):\n    if use_custom_vjp:\n      return _sort_activations_custom(inputs, sort_indices)\n    return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "This function sorts an input JAX array along its first dimension using a provided array of indices. It includes an option to use a custom Vector-Jacobian Product (VJP) for the backward pass, which can optimize performance by explicitly defining the unsorting operation.",
            "usage": "Call this function with an input array, an array of sort indices, and a boolean flag. For example: `sorted_array = _sort_activations(inputs=my_array, sort_indices=my_indices, use_custom_vjp=True)`. The `inputs` array should have a shape of `(tokens, ...)` and `sort_indices` should have a shape of `(tokens,)`. The function returns a new array with the same shape and dtype as the input, but with its rows reordered according to the indices."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom(inputs: jax.Array, sort_indices: jax.Array) -> jax.Array:\n  \"\"\"Sort functions with custom vjp.\"\"\"\n  return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "This function sorts the first dimension of an input JAX array according to a given array of indices. It is decorated with `@jax.custom_vjp`, indicating that it has a custom-defined gradient for backpropagation, with this function representing the forward pass logic.",
            "usage": "Call this function with an input tensor and a 1D tensor of sort indices. For example, `_sort_activations_custom(inputs=my_array, sort_indices=my_indices)`. The function returns a new array where the rows (first dimension) of `my_array` are reordered according to `my_indices`. Its primary use is within a custom JAX gradient computation."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_fwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_fwd(inputs: jax.Array, sort_indices: jax.Array) -> tuple[jax.Array, jax.Array]:\n  \"\"\"Forward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  return _sort_activations_custom(inputs, sort_indices), sort_indices",
        "analysis": {
            "module_type": "custom_vjp_forward_pass",
            "purpose": "Implements the forward pass for a custom JAX VJP, sorting an input array and saving the sort indices for the backward pass.",
            "input": {
                "shape": "inputs: [N, ...], sort_indices: [N]",
                "dtype": "jax.Array"
            },
            "processing_steps": [
                "Calls `_sort_activations_custom` to sort the `inputs` array according to `sort_indices`.",
                "Returns a tuple containing the sorted array and the original `sort_indices`."
            ],
            "output": {
                "shape": "A tuple: (sorted_array: [N, ...], sort_indices: [N])"
            },
            "dependencies": [
                "_sort_activations_custom"
            ],
            "parameters": {},
            "notes": [
                "This function is the `fwd` component of a `jax.custom_vjp` definition for `_sort_activations_custom`.",
                "The returned `sort_indices` are used as residuals by the corresponding backward pass function, `_sort_activations_custom_bwd`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_bwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_bwd(residuals: jax.Array, grads: jax.Array) -> tuple[jax.Array, None]:\n  \"\"\"Backward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  sort_indices = residuals\n  return _sort_activations_custom(grads, jnp.argsort(sort_indices)), None",
        "analysis": {
            "module_type": "custom_vjp_backward_pass",
            "purpose": "Implements the backward pass for the custom VJP of `_sort_activations_custom`, unsorting the incoming gradients to their original positions before the forward pass sort.",
            "input": {
                "shape": "residuals (sort_indices): [tokens,], grads: [tokens, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assign the input `residuals` to `sort_indices`.",
                "Compute the inverse permutation indices by calling `jnp.argsort(sort_indices)`.",
                "Apply the inverse permutation to the `grads` tensor using the `_sort_activations_custom` function.",
                "Return a tuple containing the permuted gradients and None."
            ],
            "output": {
                "shape": "A tuple: (unsorted_grads: [tokens, ...], None)"
            },
            "dependencies": [
                "_sort_activations_custom",
                "jax.numpy.argsort"
            ],
            "parameters": {},
            "notes": [
                "This function is registered as the backward pass for `_sort_activations_custom` via `defvjp`.",
                "It assumes the `residuals` input from the VJP mechanism is the `sort_indices` array from the forward pass.",
                "The returned `None` corresponds to the gradient of the `sort_indices` input of the original function, treating it as non-differentiable."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#random_routing",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def random_routing(rng_key, gate_logits, num_experts_per_tok):\n  \"\"\"Performs random routing of tokens to experts.\n\n  Args:\n    rng_key: A JAX PRNGKey for randomness.\n    gate_logits: A JAX array of shape (batch_size, sequence_length, num_experts)\n      representing the logits for each expert.\n    num_experts_per_tok: The number of experts to select for each token.\n\n  Returns:\n    A tuple containing:\n      - top_k_indices: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the indices of the selected experts for each\n                       token.\n      - top_k_weights: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the weights for the selected experts.\n  \"\"\"\n  bs, seq_len, num_experts = gate_logits.shape\n  indices = jnp.arange(num_experts).repeat(bs * seq_len)\n  selected_num = bs * seq_len * num_experts_per_tok\n  top_k_indices = jax.random.choice(rng_key, indices, shape=(selected_num,)).reshape(bs, seq_len, num_experts_per_tok)\n  top_k_weights = jnp.take_along_axis(gate_logits, top_k_indices, axis=-1)\n  return top_k_weights, top_k_indices",
        "analysis": {
            "module_type": "random_routing_function",
            "purpose": "Performs random routing of tokens to experts by selecting a specified number of experts for each token randomly, rather than based on the highest scores.",
            "input": {
                "shape": "gate_logits: [batch_size, sequence_length, num_experts]",
                "dtype": "JAX array (e.g., float32)"
            },
            "processing_steps": [
                "Extract shape (batch_size, sequence_length, num_experts) from the input `gate_logits`.",
                "Create a pool of expert indices for each token position by repeating `jnp.arange(num_experts)`.",
                "Use `jax.random.choice` to randomly select `num_experts_per_tok` expert indices for each token.",
                "Reshape the selected indices into `top_k_indices` with shape [batch_size, sequence_length, num_experts_per_tok].",
                "Use `jnp.take_along_axis` to gather the gate logits corresponding to the `top_k_indices`, creating `top_k_weights`.",
                "Return `top_k_weights` and `top_k_indices`."
            ],
            "output": {
                "shape": "A tuple of two JAX arrays, both with shape [batch_size, sequence_length, num_experts_per_tok]."
            },
            "dependencies": [
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "num_experts_per_tok": "The number of experts to randomly select for each token."
            },
            "notes": [
                "This function provides an alternative to top-k routing by choosing experts randomly, which can be used for exploration or specific training strategies.",
                "The returned `top_k_weights` are the original logit values of the randomly selected experts, not normalized probabilities."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#GateLogit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class GateLogit(nnx.Module):\n  \"\"\"A layer used to compute gate logits, allowing to return the pre bias values for DeepSeek routing.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Union[Iterable[int], int],\n      out_features_shape: Union[Iterable[int], int],\n      model_name: str,\n      rngs: nnx.Rngs,\n      axis: Union[Iterable[int], int] = -1,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: Tuple[Optional[str], ...] = (),\n      use_bias: bool = False,\n      score_func: str = \"\",\n      quant: Optional[quantizations.AqtQuantization] = None,\n      matmul_precision: str = \"default\",\n  ):\n    \"\"\"Initializes the GateLogit module.\n\n    Attributes:\n      in_features_shape: The shape of the input features.\n      out_features_shape: The shape of the output features, typically the number of experts.\n      model_name: The name of the model.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      axis: The axis or axes over transformation is applied.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      use_bias: Whether to add learnable bias in gate logit scores. When enabled,\n        this bias aids expert load balancing (like in DeepSeek V3), and is not\n        part of the loss calculation.\n      score_func: Scoring function for output normalization before applying bias.\n      quant: The quantization configuration. If None, no quantization is applied.\n      matmul_precision: The precision level for the matrix multiplication.\n    \"\"\"\n    self.in_features_shape = linears.canonicalize_tuple(in_features_shape)\n    self.out_features_shape = linears.canonicalize_tuple(out_features_shape)\n    self.model_name = model_name\n    self.axis = linears.canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.use_bias = use_bias\n    self.score_func = score_func\n    self.quant = quant\n    self.matmul_precision = matmul_precision\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: jax.Array, _initializing: bool = False) -> Tuple[jax.Array, Optional[jax.Array]]:\n\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = linears.normalize_axes(self.axis, inputs.ndim)\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n    kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(norm_axis)))\n    output = linears._compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n    pre_bias_logits = None\n\n    if self.score_func:\n      output = linears._convert_to_activation_function(self.score_func)(output)\n      if self.model_name.startswith(\"deepseek3\"):\n        pre_bias_logits = output\n\n    if self.use_bias:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output, pre_bias_logits",
        "analysis": {
            "module_type": "gate_logit",
            "purpose": "Computes gate logits for routing tokens to experts in a Mixture-of-Experts (MoE) model, with an option to return pre-bias values for specific routing strategies like DeepSeek's.",
            "input": {
                "shape": "[..., in_features_shape]",
                "dtype": "The `dtype` specified during initialization (e.g., jnp.float32)."
            },
            "processing_steps": [
                "Casts input tensor to the specified dtype.",
                "Computes a dot product between the input and the kernel weight.",
                "Optionally applies a scoring function to the result.",
                "Optionally adds a bias term.",
                "Returns the final logits and, for specific models, the pre-bias logits."
            ],
            "output": {
                "shape": "A tuple of `(jax.Array, Optional[jax.Array])`. The first array has a shape of `[..., out_features_shape]`. The second, if not None, has the same shape."
            },
            "dependencies": [
                "flax.nnx",
                "jax.numpy",
                "MaxText.layers.linears",
                "MaxText.layers.quantizations",
                "MaxText.layers.nnx_wrappers"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features.",
                "out_features_shape": "The shape of the output features, typically the number of experts.",
                "model_name": "The name of the model, used for special logic (e.g., 'deepseek3').",
                "use_bias": "If True, a learnable bias is added to the output logits.",
                "score_func": "An optional scoring function (e.g., 'sigmoid') applied to the output before the bias.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This layer is a key component of a Mixture-of-Experts router.",
                "It has special handling for 'deepseek3' models to return pre-bias logits, which are used in its specific routing algorithm."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the GateLogit layer, setting up parameters like kernel and optional bias, and handling quantization setup.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Canonicalize input/output feature shapes and axis.",
                        "Initialize kernel parameter `self.kernel` using `kernel_init` if not in quantization serve mode.",
                        "Initialize optional bias parameter `self.bias` if `use_bias` is True.",
                        "If quantization is enabled, set up the quantized dot-general operator.",
                        "Perform a dummy forward pass if quantization is enabled to initialize quantization parameters."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.canonicalize_tuple",
                        "quantizations.in_serve_mode",
                        "nnx.Param",
                        "initializers.default_bias_init",
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "The kernel parameter is not initialized if `quantizations.in_serve_mode` is true, as it's expected to be part of the quantization state."
                    ]
                },
                "quant_dot_general": {
                    "purpose": "Provides access to the quantized dot-general operator if quantization is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `_quant_dot_general_name` is set.",
                        "If set, return the corresponding attribute.",
                        "Otherwise, return None."
                    ],
                    "output": {
                        "shape": "Returns an `nnx_wrappers.ToNNX` object or `None`."
                    },
                    "dependencies": [
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "This is a property, not a method that performs computation on tensors."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass to compute the gate logits.",
                    "input": {
                        "shape": "[..., in_features_shape]",
                        "dtype": "The `dtype` specified during initialization."
                    },
                    "processing_steps": [
                        "Cast input tensor to the specified `dtype`.",
                        "Get the kernel tensor, either from `self.kernel` or a zero tensor if in serve mode.",
                        "Compute the dot product between inputs and the kernel using `linears._compute_dot_general_nnx`.",
                        "Optionally apply a scoring function to the output.",
                        "If the model name starts with 'deepseek3', store the pre-bias logits.",
                        "If `use_bias` is True, add the bias term to the output.",
                        "Return the final logits and the optional pre-bias logits."
                    ],
                    "output": {
                        "shape": "A tuple of `(jax.Array, Optional[jax.Array])`, where the first element has shape `[..., out_features_shape]`."
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "linears.normalize_axes",
                        "quantizations.in_serve_mode",
                        "linears._compute_dot_general_nnx",
                        "linears._convert_to_activation_function"
                    ],
                    "notes": [
                        "Returns a tuple where the second element, `pre_bias_logits`, is only non-None for 'deepseek3' models when a `score_func` is applied."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedMoE(nnx.Module):\n  \"\"\"Implements a routed MoE block.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      num_experts: int,\n      num_experts_per_tok: int,\n      mesh: jax.sharding.Mesh,\n      kernel_init: attentions.NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      intermediate_dim: int = 2048,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"Initializes the RoutedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      num_experts: Number of experts.\n      num_experts_per_tok: Number of experts for each token.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      intermediate_dim: Intermediate dimension of MoE.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.num_experts = num_experts\n    self.num_experts_per_tok = num_experts_per_tok\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.intermediate_dim = intermediate_dim\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n\n    if self.config.fsdp_shard_on_exp:\n      # special sharding for dsv3\n      self.wi_kernel_axes = (\"embed_no_exp\", None, \"mlp\")\n      self.wo_kernel_axes = (\"embed_no_exp\", \"mlp\", None)\n    else:\n      self.wi_kernel_axes = (\"exp\", \"embed_no_exp\", \"mlp\")\n      self.wo_kernel_axes = (\"exp\", \"mlp\", \"embed_no_exp\")\n\n    self.gate = GateLogit(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.num_experts,\n        model_name=self.config.model_name,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        kernel_init=self.kernel_init,\n        kernel_axes=self.kernel_axes,\n        use_bias=self.config.routed_bias,\n        score_func=self.config.routed_score_func,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # pylint: disable=protected-access\n    self.activation_fn = linears._convert_to_activation_function(self.config.mlp_activations[0])\n\n    kernel_in_axis = np.arange(1)\n    kernel_out_axis = np.arange(1, 2)\n\n    if quantizations.in_serve_mode(self.quant):\n      # During aqt convert state we delete kernel weight from params to save\n      # memory. Instead they are retrieved from the tensors stored in the 'aqt'\n      # collection.\n      self.wi_0 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wi_1 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wo = jnp.zeros((num_experts, intermediate_dim, self.config.emb_dim))\n    else:\n      self.wi_0 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wi_1 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wo = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (self.num_experts, self.intermediate_dim, self.config.emb_dim),\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wo_kernel_axes,\n      )\n\n    if self.config.mlp_bias:\n      wi_bias_axes = (\"exp\", \"activation_mlp\")\n      wo_bias_axes = (\"exp\", \"activation_embed\")\n      wi_bias_shape = (self.num_experts, self.intermediate_dim)\n      wo_bias_shape = (self.num_experts, self.config.emb_dim)\n      self.wi_0_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wi_1_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wo_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wo_bias_shape, self.weight_dtype),\n          sharding=wo_bias_axes,\n      )\n    else:\n      self.wi_0_bias = None\n      self.wi_1_bias = None\n      self.wo_bias = None\n\n  def get_expert_parallelism_size(self):\n    return self.mesh.shape[\"expert\"]\n\n  def get_tensor_parallelism_size(self):\n    return self.mesh.shape[\"tensor\"]\n\n  def get_tensor_transpose_parallelism_size(self):\n    return self.mesh.shape[\"tensor_transpose\"]\n\n  def get_context_autoregressive_parallelism_size(self):\n    return self.mesh.shape[\"context_autoregressive\"]\n\n  def get_topk(self, gate_logits, pre_bias_logits, rngs=None):\n    \"\"\"get topk.\"\"\"\n    # shape of top_k_weights & top_k_indices:\n    # (batch, sequence, num_experts_per_tok).\n    if self.config.use_random_routing:\n      if rngs is None:\n        raise ValueError(\"The random key cannot be None for random routing.\")\n      # Reuse the 'dropout' RNG stream to ensure random routing\n      rng = rngs.dropout()\n      top_k_weights, top_k_indices = random_routing(rng, gate_logits, self.num_experts_per_tok)\n      return top_k_weights, top_k_indices\n\n    if self.config.model_name.startswith(\"deepseek3\"):\n      top_k_weights, top_k_indices = self.deepseek_routing(gate_logits, pre_bias_logits)\n    else:\n      top_k_weights, top_k_indices = jax.lax.top_k(gate_logits, self.num_experts_per_tok)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.DEEPSEEK:\n      top_k_weights = self.deepseek_scale_weights(top_k_weights)\n    elif self.config.decoder_block != ctypes.DecoderBlockType.LLAMA4:\n      top_k_weights = jax.nn.softmax(top_k_weights.astype(jnp.float32), axis=-1).astype(self.dtype)\n\n    # This is the Qwen3-specific normalization of router weights.\n    if self.config.norm_topk_prob:\n      top_k_weights /= top_k_weights.sum(axis=-1, keepdims=True)\n\n    return top_k_weights, top_k_indices\n\n  def deepseek_scale_weights(self, weights):\n    \"\"\"Scales weights according to DeepSeek's v3 reference implementation.\"\"\"\n    # https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L592-L594.\n    if self.config.routed_score_func == \"sigmoid\":\n      weights /= weights.sum(-1, keepdims=True)\n    weights *= self.config.routed_scaling_factor\n    return weights\n\n  def expert_group_mask(self, gate_logits: jax.Array) -> jax.Array:\n    \"\"\"Returns a mask that selects only the top-k groups of experts.\n\n    Groups of experts are selected based on the sum of the top-2 expert scores\n    for each group.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n\n    Returns:\n      Array of shape `(batch, seq, num_experts)` that is 1 for experts in the\n      top-k groups and 0 elsewhere.\n    \"\"\"\n    # Find top groups based on each group's top-2 expert scores, where\n    # `scores_grouped.shape =\n    # (batch * seq, n_routing_groups, experts_per_group)`.\n    scores_grouped = jnp.reshape(\n        gate_logits,\n        gate_logits.shape[:-1] + (self.config.n_routing_groups, -1),\n    )\n    top2_in_group_vals, _ = jax.lax.top_k(scores_grouped, k=2)\n    group_scores = jnp.sum(jnp.astype(top2_in_group_vals, jnp.float32), axis=-1)\n    _, group_idx = jax.lax.top_k(group_scores, k=self.config.topk_routing_group)\n\n    # Mask selected groups so that only those experts are considered.\n    group_mask = jax.nn.one_hot(group_idx, num_classes=self.config.n_routing_groups, dtype=jnp.float32)\n    group_mask = jnp.sum(group_mask, axis=-2)\n\n    # Apply masks and get top-k indices.\n    score_mask_expanded = jnp.broadcast_to(\n        group_mask[..., None],\n        group_mask.shape + (self.num_experts // self.config.n_routing_groups,),\n    )\n    return jnp.reshape(\n        score_mask_expanded,\n        score_mask_expanded.shape[:-2] + (self.num_experts,),\n    )\n\n  def deepseek_routing(self, gate_logits: jax.Array, pre_bias_logits: jax.Array) -> tuple[jax.Array, jax.Array]:\n    \"\"\"DeepSeek routing logit.\n\n    If the configuration does not specify routing groups (`n_routing_groups` is\n    -1), we use a standard top-k routing mechanism. Otherwise, we force all\n    selected experts to be from the a subset of the highest rated expert groups.\n\n    The selection process uses post_bias logits, while the return weights use\n    pre_bias logits.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n      pre_bias_logits: Array of shape `(batch, seq,num_experts)`.\n\n    Returns:\n      - top_k_weights: `(batch, seq, num_experts_per_tok)` array of weight values for\n        each selected expert.\n      - top_k_indices: `(batch, seq, num_experts_per_tok)` array of indices\n        identifying the selected experts for each token.\n    \"\"\"\n    expert_mask = 1 if self.config.n_routing_groups == -1 else self.expert_group_mask(gate_logits)\n    _, top_k_indices = jax.lax.top_k(\n        jnp.where(expert_mask > 0, gate_logits, -jnp.inf),\n        k=self.num_experts_per_tok,\n    )\n    top_k_weights = jnp.take_along_axis(pre_bias_logits, top_k_indices, axis=-1)\n    return top_k_weights, top_k_indices\n\n  def apply_ffn_activation(self, layer_w0, layer_w1):\n    \"\"\"Applies FFN activation function.\"\"\"\n    with jax.named_scope(\"ffn_act\"):\n      if self.config.decoder_block == ctypes.DecoderBlockType.GPT_OSS:\n        layer_w0 = jnp.clip(layer_w0, a_min=None, a_max=self.config.mlp_activations_limit)\n        layer_w1 = jnp.clip(layer_w1, a_min=-self.config.mlp_activations_limit, a_max=self.config.mlp_activations_limit)\n        layer_act = self.activation_fn(layer_w0 * 1.702)\n        glu = jnp.multiply(layer_w0, layer_act)\n        intermediate_layer = jnp.multiply(glu, (layer_w1 + 1))\n      else:\n        layer_act = self.activation_fn(layer_w0)\n        intermediate_layer = jnp.multiply(layer_act, layer_w1)\n      return intermediate_layer.astype(self.dtype)\n\n  def permute(self, inputs, gate_logits, pre_bias_logits, use_custom_sort_vjp=True, rngs=None, roll_to_expert_id=None):\n    \"\"\"Permute tokens to group by expert to fit gmm call.\"\"\"\n    # reshape inputs (batch, sequence, emb) to (batch * sequence, emb)\n    inputs_shape = inputs.shape\n    bsz_times_seq_len = inputs_shape[0] * inputs_shape[1]\n    inputs_2d = jnp.reshape(inputs, (bsz_times_seq_len, inputs_shape[2]))\n    weights, selected_experts = self.get_topk(gate_logits, pre_bias_logits, rngs)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n      # weights will be of shape (batch_size, seq_len, num_experts_per_tok)\n      router_scores = jax.nn.sigmoid(weights.astype(jnp.float32))  # weights are top_k_weights here\n      # Squeeze router_scores to (batch_size * seq_len, num_experts_per_tok)\n      inputs_2d = inputs_2d * router_scores.reshape(bsz_times_seq_len, -1)\n\n    flatten_selected_experts = jnp.ravel(selected_experts)\n    if roll_to_expert_id is not None:\n      flatten_selected_experts = (flatten_selected_experts - roll_to_expert_id) % self.num_experts\n    sorted_selected_experts = jnp.argsort(flatten_selected_experts)\n    # sort inputs for number of selected experts\n    replicated_inputs_2d = jnp.repeat(inputs_2d, self.num_experts_per_tok, axis=0)\n    sorted_inputs = _sort_activations(replicated_inputs_2d, sorted_selected_experts, use_custom_sort_vjp).astype(\n        self.dtype\n    )\n    group_size = jnp.bincount(flatten_selected_experts, length=self.num_experts)\n    # Return the experts for each sorted input.\n    expert_indices = jnp.arange(self.num_experts)\n    sorted_experts = jnp.repeat(\n        expert_indices,\n        repeats=group_size,\n        total_repeat_length=flatten_selected_experts.shape[0],\n    )\n    return (\n        sorted_inputs,\n        sorted_selected_experts,\n        weights,\n        group_size,\n        sorted_experts,\n    )\n\n  def unpermute(\n      self,\n      intermediate,\n      sorted_selected_experts,\n      weights,\n      batch_size,\n      sequence_length,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Unpermute tokens to original order and combine weights.\"\"\"\n\n    unsort_intermediate = _sort_activations(\n        intermediate,\n        jnp.argsort(sorted_selected_experts),\n        use_custom_sort_vjp,\n    )\n    reshaped_weights = jnp.reshape(weights, (-1, self.num_experts_per_tok))\n    reshaped_intermediate = jnp.reshape(\n        unsort_intermediate,\n        (reshaped_weights.shape[0], self.num_experts_per_tok, -1),\n    )\n    with jax.named_scope(\"weight_sum\"):\n      matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n      if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n        # For Llama4, combine using weights of 1 for selected experts\n        reshaped_weights = jnp.ones_like(reshaped_weights)\n      output = jnp.einsum(\n          \"BKE,BK -> BE\",\n          reshaped_intermediate.astype(jnp.float32),\n          reshaped_weights.astype(jnp.float32),\n          precision=matmul_precision,\n      )\n    return output.reshape(batch_size, sequence_length, -1).astype(self.dtype)\n\n  @staticmethod\n  def local_permute(\n      inputs,\n      global_group_sizes,\n      local_expert_size,\n      shard_index,\n      is_offset=False,\n      global_sorted_experts=None,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Permutes tokens locally within an expert shard.\n\n    This function prepares the input tokens for processing by the experts\n    located\n    on the current shard. It groups the tokens by their assigned local expert\n    index (0 to local_expert_size - 1).\n\n    Args:\n      inputs: The input data (tokens) assigned to the experts on this shard.\n        Shape `[tokens, emb_dim]`.\n      global_group_sizes: The count of tokens assignments for each global expert\n        across all the batch shards. Shape `[num_batch_shards, num_experts].\n      local_expert_size: The number of experts handled by the current shard.\n      shard_index: The index of the current expert shard (0 to\n        num_expert_parallelism - 1).\n      is_offset: If True, assumes `inputs` are pre-sorted by global expert ID\n        and selects the slice relevant to this shard's assigned experts. If\n        False, assumes that `inputs` corresponding to the shard's experts start\n        from the beginning of the tensor but need to be permuted by expert ID.\n      global_sorted_experts: Global expert IDs for the `inputs` used when\n        `is_offset` is True. Shape `[total_tokens_for_this_shard]`.\n\n    Returns:\n      A tuple containing:\n        sorted_inputs: Input data permuted local expert ID.\n        sorted_indices: Indices used to permute the inputs.\n        local_group_size: Number of tokens assigned to each local expert on this\n          shard.\n        sorted_experts_ids: expert ID corresponding to each token of the permuted\n        inputs.\n    \"\"\"\n\n    # Slice the count of local expert IDs in each batch shard.\n    # all_shard_local_sizes.shape: [expert_shard, local_expert_size]\n    all_shard_local_sizes = jax.lax.dynamic_slice_in_dim(\n        global_group_sizes,\n        shard_index * local_expert_size,\n        local_expert_size,\n        axis=1,\n    )\n    local_sizes = all_shard_local_sizes.reshape(-1)\n\n    # Total count of the local expert IDs is the sum of the counts across all\n    # batch shards, since all batch shards will send their contributions to the\n    # current expert shard.\n    local_group_size = jnp.sum(all_shard_local_sizes, axis=0)\n\n    # In this case, the data that needs to be processed by the local shard\n    # does not start from row 0 but actually starts at\n    # (jnp.concatenate((jnp.array([0]),\n    #  jnp.cumsum(local_group_sizes[:-1]))[shard_id]).\n    # This happens if batches (`inputs`) are replicated across expert shards and\n    # pre-sorted by global Expert ID (via permute()).\n    if is_offset:\n      divided_assignments = jnp.floor_divide(global_sorted_experts, local_expert_size)\n      expert_indices = jnp.where(\n          divided_assignments == shard_index,\n          jnp.mod(global_sorted_experts, local_expert_size),\n          local_expert_size,\n      )\n\n    # In this case the `input` data has been received from the batch shards and\n    # needs to be reorganized in order of local Expert IDs.\n    else:\n      base_indices = jnp.mod(jnp.arange(local_sizes.shape[0]), local_expert_size)\n      expert_indices = jnp.repeat(base_indices, local_sizes, total_repeat_length=inputs.shape[0])\n\n    sorted_indices = jnp.argsort(expert_indices)\n    sorted_inputs = _sort_activations(inputs, sorted_indices, use_custom_sort_vjp)\n    sorted_experts_ids = expert_indices[sorted_indices]\n    return (\n        sorted_inputs,\n        sorted_indices,\n        local_group_size,\n        sorted_experts_ids,\n    )\n\n  @staticmethod\n  def get_all_to_all_params(\n      all_shards_group_sizes,\n      shard_id,\n      num_expert_parallelism,\n      is_batch_sharded=True,\n  ):\n    \"\"\"Generates input offsets, send sizes, output offsets, and receive sizes used for ragged_all_to_all.\"\"\"\n\n    class TransformStrategy(enum.Enum):\n      INPUT_OFFSET = enum.auto()\n      SEND_SIZE = enum.auto()\n      OUTPUT_OFFSET = enum.auto()\n      RECV_SIZE = enum.auto()\n\n    def transform_array(input_array, shard_id, strategy, is_batch_sharded):\n      \"\"\"Transforms the input array based on the specified strategy.\"\"\"\n      # Prepares it for the usage with `ragged_all_to_all` API. The\n      # transformation determines how data is sent and received between shards.\n      if is_batch_sharded:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # Index of input array for the send\n          local_array = input_array[shard_id]\n          return jnp.concatenate((jnp.array([0]), jnp.cumsum(local_array)[:-1]))\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # Size of input array for the send\n          return input_array[shard_id]\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # Received index in the target output\n          zero_row = jnp.zeros((1,) + input_array.shape[1:], dtype=input_array.dtype)\n          array_with_zeros = jnp.concatenate((zero_row, input_array), axis=0)\n          cumulated_array = jnp.cumsum(array_with_zeros, axis=0, dtype=input_array.dtype)\n          return cumulated_array[shard_id]\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array[:, shard_id]\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n      # If the batch is unsharded then we send the same data slice to all other\n      # shards. We also assume each shard will have the local processed inputs\n      # sorted to start from index 0. Finally, len(input_array.shape) == 1 since\n      # there is only one batch shard.\n      else:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # The data on each shard always starts at 0.\n          return jnp.zeros(num_expert_parallelism, dtype=input_array.dtype)\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # The send amount is always the amount of data the current expert\n          # shard needs to process.\n          return jnp.repeat(input_array[shard_id], num_expert_parallelism)\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # The offset in each shard will just be the start of the group which\n          # that shard is responsible for.\n          output_offset = jnp.concatenate((jnp.array([0]), jnp.cumsum(input_array[:-1])))[shard_id]\n          return jnp.repeat(output_offset, num_expert_parallelism)\n        # The amount that each shard receives from all other shards is\n        # equivalent to the group sizes (aka input_array).\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n    input_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.INPUT_OFFSET,\n        is_batch_sharded,\n    )\n    send_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.SEND_SIZE,\n        is_batch_sharded,\n    )\n    output_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.OUTPUT_OFFSET,\n        is_batch_sharded,\n    )\n    recv_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.RECV_SIZE,\n        is_batch_sharded,\n    )\n    return input_offsets, send_sizes, output_offsets, recv_sizes\n\n  def transform_bias(self, experts_index, *biases):\n    \"\"\"Selects bias values for a variable number of bias tensors based on chosen experts.\"\"\"\n    return tuple(bias[experts_index] for bias in biases)\n\n  def sparse_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ):\n    \"\"\"Perform sparse matrix multiplication of inputs and Experts.\"\"\"\n\n    def gmm(inputs, kernel, tiling, group_sizes, expert_assignments):\n      pad_length = self.config.tile_batch_seq\n      hs_shape = inputs.shape\n      # pad length is the 1st dimension of tiling size in gmm call\n      if inputs.shape[0] != expert_assignments.shape[0]:\n        raise ValueError(\"The number of input tokens must match the number of expert\" \" assignments!\")\n      padding_amount = 0\n      if hs_shape[0] % pad_length:\n        padding_amount = pad_length - hs_shape[0] % pad_length\n        inputs = jax.lax.pad(inputs, jnp.array(0.0, dtype=inputs.dtype), [(0, padding_amount, 0), (0, 0, 0)])\n\n      inputs = inputs.astype(self.dtype)\n      kernel = kernel.astype(self.dtype)\n\n      lhs_quantize_dtype, rhs_quantize_dtype = None, None\n      if self.quant is not None:\n        quant_dg = self.quant.quant_dg\n        lhs_quantize_dtype = quant_dg.fwd.dg_quantizer.lhs.numerics.get_dtype()\n        rhs_quantize_dtype = quant_dg.fwd.dg_quantizer.rhs.numerics.get_dtype()\n      m, k, n = inputs.shape[0], inputs.shape[1], kernel.shape[2]\n      tiling = (\n          min(tiling[0], m),\n          min(tiling[1], k),\n          min(tiling[2], n),\n      )\n      if self.config.megablox:\n        if self.config.use_tokamax_gmm:\n          output = tokamax_api.ragged_dot(  #  pylint: disable=possibly-used-before-assignment\n              lhs=inputs,\n              rhs=kernel,\n              group_sizes=group_sizes,\n              precision=jax.lax.Precision.DEFAULT,\n              preferred_element_type=self.dtype,\n              implementation=\"mosaic\",\n          )\n        else:\n          output = mblx.gmm(\n              lhs=inputs,\n              rhs=kernel,\n              group_sizes=group_sizes,\n              preferred_element_type=self.dtype,\n              tiling=tiling,\n              lhs_quantize_dtype=lhs_quantize_dtype,\n              rhs_quantize_dtype=rhs_quantize_dtype,\n              use_qwix_quantization=self.config.use_qwix_quantization,\n          )\n      else:\n        rhs_inputs = kernel\n        if isinstance(kernel, aqt.QTensor):\n          if kernel.bias or kernel.sparsity_mask or len(kernel.scale) > 1:\n            raise ValueError(\"Unsupported usecase for ragged_dot with quantized kernel.\")\n          rhs_inputs = kernel.qvalue\n        with set_xla_metadata(ragged_dot_tiling=\",\".join([str(t) for t in tiling])):\n          output = jax.lax.ragged_dot(\n              lhs=inputs,\n              rhs=rhs_inputs,\n              group_sizes=group_sizes,\n              preferred_element_type=self.dtype,\n          )\n        if isinstance(kernel, aqt.QTensor):\n          # Multiply outputs by the kernely scale\n          scales = jnp.take(kernel.scale[0].squeeze(), indices=expert_assignments, axis=0)\n          if padding_amount > 0:\n            scales = jax.lax.pad(\n                scales,\n                jnp.array(0.0, dtype=scales.dtype),\n                [(0, padding_amount, 0), (0, 0, 0)],\n            )\n          output *= scales\n      if padding_amount > 0:\n        output = output[: hs_shape[0]]\n      return output\n\n    # Currently, we support data, tensor, and expert parallelism with Megablox.\n    # We all gather the input activations over tensor parallelism to follow\n    # https://parsa.epfl.ch/course-info/cs723/papers/Megatron.pdf.\n\n    # Check if the batch should be sharded by expert and whether the batch_size\n    # supports this. For example, for interleaved inference, prefill always has\n    # batch_size=1 while decode can have batch_size > 1.\n    try:\n      is_batch_sharded_by_expert = (\n          \"expert\"\n          in tuple(\n              filter(\n                  lambda tup: tup[0] == \"activation_batch\",\n                  self.config.logical_axis_rules,\n              )\n          )[\n              0\n          ][1]\n      )\n    except:  # pylint: disable=bare-except\n      is_batch_sharded_by_expert = False\n    if is_batch_sharded_by_expert and inputs.shape[0] > 1:\n      batch_logical_axis = \"activation_batch\"\n    else:\n      batch_logical_axis = \"activation_batch_no_exp\"\n\n    if self.get_tensor_transpose_parallelism_size() > 1:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n    else:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n\n    gate_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      pre_bias_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    else:\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits_pspec = None\n\n    # w0, w1, wo needs to be un sharded on fsdp / fsdp_transpose axis, so use\n    # mlp_no_fsdp axis\n    if self.config.fsdp_shard_on_exp:\n      # special sharding for dsv3 to remove overhead between gmm/AG\n      w0_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", \"mlp_no_fsdp\", None))\n    else:\n      w0_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n    if isinstance(w0_kernel, aqt.QTensor):\n      w0_pspec = aqt.partition_spec(w0_pspec, (1,), w0_kernel.dtype, use_bias=False)\n    if isinstance(w1_kernel, aqt.QTensor):\n      w1_pspec = aqt.partition_spec(w1_pspec, (1,), w1_kernel.dtype, use_bias=False)\n    if isinstance(wo_kernel, aqt.QTensor):\n      wo_pspec = aqt.partition_spec(wo_pspec, (1,), wo_kernel.dtype, use_bias=False)\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            input_partition_pspec,\n            gate_logits_pspec,\n            pre_bias_logits_pspec,\n            w0_pspec,\n            w1_pspec,\n            wo_pspec,\n            w0_bias_pspec,\n            w1_bias_pspec,\n            wo_bias_pspec,\n            None,\n        ),\n        out_specs=(nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))),\n        check_vma=False,\n    )\n    def wrapper(x, logits, pre_bias_logits, w0, w1, wo, w0_bias, w1_bias, wo_bias, rngs):\n      batch_size, sequence_length, _ = x.shape\n      expert_axis_name = \"expert\"\n      expert_shard_id = jax.lax.axis_index(expert_axis_name)\n      num_expert_parallelism = self.get_expert_parallelism_size()\n      if self.config.use_ring_of_experts:\n        # The ring-of-experts strategy first duplicates the inputs to all\n        # expert shards, and then routes within each shard.\n\n        # Duplicate inputs to all expert shards.\n        x, logits, pre_bias_logits = tuple(\n            jax.lax.all_gather(z, axis_name=expert_axis_name, tiled=True) for z in (x, logits, pre_bias_logits)\n        )\n\n        # \"Route\" tokens within each shard.\n        num_experts_per_shard = self.config.num_experts // num_expert_parallelism\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x,\n            logits,\n            pre_bias_logits,\n            self.config.use_custom_sort_vjp,\n            roll_to_expert_id=num_experts_per_shard * expert_shard_id,\n        )\n\n        # Filter down to the group sizes that apply to only the experts in the\n        # current shard.\n        group_sizes = group_sizes[:num_experts_per_shard]\n        mask = jnp.arange(x.shape[0]) < jnp.sum(group_sizes)\n        x = jnp.where(mask[:, None], x, 0)\n      else:\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x, logits, pre_bias_logits, self.config.use_custom_sort_vjp, rngs\n        )\n\n        if num_expert_parallelism > 1:\n          batch_axis = \"expert\" if is_batch_sharded_by_expert else \"data\"\n          # get group sizes for all shards\n          local_expert_size = self.config.num_experts // num_expert_parallelism\n          reshaped_group_sizes = jnp.sum(group_sizes.reshape(-1, local_expert_size), axis=1)\n          global_group_sizes = group_sizes\n          if is_batch_sharded_by_expert:\n            all_shards_group_sizes = jax.lax.all_gather(reshaped_group_sizes, axis_name=batch_axis)\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                all_shards_group_sizes,\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n\n            # TODO(ranran): For better performance, we could update output buffer to a smaller\n            # size to replace self.get_expert_parallelism_size() for efficiency,\n            # Or we could apply capacity_factor for excessive experts.\n            # Note: Reducing buffer increase the risk of token dropping under unbalanced distribution.\n\n            # In the worst case, all of the global input data is assigned to each expert in the current shard.\n            # This would result in num_expert_shards * input_size * experts_per_shard assignments. However, if\n            # experts_per_shard > num_experts_per_tok we cannot assign more than num_experts_per_tok to all of the inputs.\n            max_local_experts_per_tok = min(local_expert_size, self.config.num_experts_per_tok)\n            buffer_size = int(\n                num_expert_parallelism\n                * self.config.per_device_batch_size\n                * self.config.max_target_length\n                * max_local_experts_per_tok\n            )\n            output_shape = jnp.zeros((buffer_size, self.config.emb_dim), dtype=x.dtype)\n\n            x = jax.lax.ragged_all_to_all(\n                x,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n            global_group_sizes = jax.lax.all_gather(group_sizes, axis_name=expert_axis_name)\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes,\n                local_expert_size,\n                shard_index=expert_shard_id,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n          else:\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes[None, :],\n                local_expert_size,\n                shard_index=expert_shard_id,\n                is_offset=True,\n                global_sorted_experts=selected_experts,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n\n      if self.config.mlp_bias:\n        w0_bias, w1_bias, wo_bias = self.transform_bias(selected_experts, w0_bias, w1_bias, wo_bias)\n\n      gmm_fn = functools.partial(\n          gmm,\n          group_sizes=group_sizes,\n          expert_assignments=selected_experts,\n      )\n      wi_tile_size = (\n          self.config.tile_batch_seq,\n          self.config.tile_embed_dim,\n          self.config.tile_mlp_dim,\n      )\n      wo_tile_size = (\n          self.config.tile_batch_seq,\n          self.config.tile_mlp_dim,\n          self.config.tile_embed_dim,\n      )\n      layer_w0 = gmm_fn(x, w0, tiling=wi_tile_size)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w0 = jax.lax.psum(layer_w0, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w0 = layer_w0 + w0_bias\n      layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n\n      layer_w1 = gmm_fn(x, w1, tiling=wi_tile_size)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w1 = jax.lax.psum(layer_w1, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w1 = layer_w1 + w1_bias\n      layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      intermediate_layer = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      intermediate_output = gmm_fn(intermediate_layer, wo, tiling=wo_tile_size)\n      if self.get_tensor_parallelism_size() > 1:\n        intermediate_output = jax.lax.psum_scatter(intermediate_output, \"tensor\", scatter_dimension=1, tiled=True)\n      if self.config.mlp_bias:\n        intermediate_output = intermediate_output + wo_bias\n      intermediate_output = adc.checkpoint_name(intermediate_output, \"mlpwo\")\n\n      if self.config.use_ring_of_experts:\n        # Set the outputs of tokens which were not processed to 0.\n        mask = jnp.arange(intermediate_output.shape[0]) < jnp.sum(group_sizes)\n        intermediate_output = jnp.where(mask[:, None], intermediate_output, 0)\n\n        # Unsort and deduplicate the outputs locally.\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n        # Sum up the partial outputs across the expert shards.\n        output = jnp.reshape(output, (-1, sequence_length, self.config.emb_dim))\n        output = jax.lax.psum_scatter(output, expert_axis_name, scatter_dimension=0, tiled=True)\n\n      else:\n        if num_expert_parallelism > 1:\n          original_inputs_first_dim = batch_size * sequence_length * self.config.num_experts_per_tok\n          if sorted_selected_experts.shape[0] != original_inputs_first_dim:\n            raise ValueError(\"original_inputs_first_dim does not match the original tensor\" \" shape!\")\n          output_shape = jnp.zeros(\n              (\n                  original_inputs_first_dim,\n                  self.config.emb_dim // self.get_tensor_parallelism_size(),\n              ),\n              dtype=intermediate_output.dtype,\n          )\n          if is_batch_sharded_by_expert:\n            # locally unpermute back to the original order\n            local_output = _sort_activations(\n                intermediate_output,\n                jnp.argsort(local_sorted_indices),  # pylint: disable=undefined-variable\n                self.config.use_custom_sort_vjp,\n            )\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                jnp.transpose(all_shards_group_sizes),  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                local_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n          else:\n            # If bach is replicated across EP shards then each shard should send\n            # 0..local_shard_size data to the other shards and receive the\n            # local_shard data from all of the other shards using\n            # ragged_all_to_all.\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                reshaped_group_sizes,  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n                is_batch_sharded=False,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                intermediate_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n      return output, None\n\n    if self.config.moe_fsdp_use_two_stage_all_gather:\n      # Unshard on fsdp axis\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n\n      # Unshard on fsdp_transpose axis\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp\", \"embed_tensor_transpose\"))\n\n      # Make sure XLA does not optimize by combining above All-Gather to unshard\n      # on FSDP axis and the subsequent unshard on fsdp_transpose axis\n      w0_kernel = jax.lax.optimization_barrier(w0_kernel)\n      w1_kernel = jax.lax.optimization_barrier(w1_kernel)\n      wo_kernel = jax.lax.optimization_barrier(wo_kernel)\n\n      # Unshard on both fsdp and fsdp_transpose transpose\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n\n    return wrapper(\n        inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias, self.rngs\n    )\n\n  def reshape_and_update_weights(self, weights, indices):\n    \"\"\"reshape and update weights.\"\"\"\n    # input of weights and indices: (batch_size, seq_len, num_experts_per_tok)\n    # output of updated weights: (batch_size, seq_len, num_experts)\n    update_weights = jnp.zeros((weights.shape[0], weights.shape[1], self.num_experts), dtype=self.dtype)\n    index_update = (\n        jnp.arange(weights.shape[0])[:, None, None],\n        jnp.arange(weights.shape[1])[:, None],\n        indices,\n    )\n    update_weights = update_weights.at[index_update].set(weights)\n    return update_weights\n\n  def get_context_partition_and_sub_seq(self, seq_len):\n    cp = self.get_context_autoregressive_parallelism_size()\n    if seq_len % cp != 0:\n      cp = 1\n    sub_seq = seq_len // cp\n    return cp, sub_seq\n\n  def generate_masks_subgroup(self, top_k_indices, softmax_probs):\n    \"\"\"Subgroup mask generation for inference only.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    # Break sequence into subsequences (groups) of tokens, and route only within\n    # each group.\n    top_k_indices = jnp.reshape(top_k_indices, (batch_size, cp, sub_seq, top_k_indices.shape[2]))\n\n    tokens_per_batch = sub_seq * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, cp, sub_seq * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=2)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=3)\n\n    # reshape & update weights\n    softmax_probs = jnp.reshape(\n        softmax_probs,\n        ((batch_size, cp, sub_seq, self.num_experts)),\n    )\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=3) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    # ici_context_parallelism\n    dispatch_mask = jnp.reshape(\n        dispatch_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n    combine_mask = jnp.reshape(\n        combine_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n\n    return dispatch_mask, combine_mask\n\n  def generate_masks(self, top_k_indices, softmax_probs):\n    \"\"\"Generate masks.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n\n    tokens_per_batch = seq_len * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, seq_len * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=1)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, seq_len, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=2)\n\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, seq_len, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=2) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    return dispatch_mask, combine_mask\n\n  # See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details.\n  def load_balance_loss(self, top_k_indices, logits) -> jax.Array:\n    \"\"\"Compute the load balance loss.\"\"\"\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    summed_expert_mask = jnp.sum(expert_mask, axis=2)\n    # Get fraction of tokens dispatched to each expert\n    density = jnp.mean(summed_expert_mask, axis=1)\n    # get fraction of probability allocated to each expert\n    density_prob = jnp.mean(logits, axis=1)\n    loss = jnp.mean(density * density_prob) * (self.num_experts**2) * self.config.load_balance_loss_weight\n    return loss\n\n  def get_einsum(\n      self,\n      rhs_mesh_axes: Tuple[Optional[str], ...] = (),\n      einsum_name: str | None = None,\n  ):\n    \"\"\"Get the Einstein summation.\"\"\"\n\n    # the check is to prevent aqteinsum as einsum op for dispatch and combine\n    # einsums in ase when capacity_factor > 0\n    # this is necessary to load pre-quantized weights in case of inference\n    if self.config.model_call_mode == \"inference\" and einsum_name in (\n        DISPATCH,\n        COMBINE,\n    ):\n      return jnp.einsum\n\n    if self.quant:\n\n      def aqt_einsum(*args, **kwargs):  # pylint: disable=unused-argument\n        # simply skip kwargs, since aqt einsum doesn't support any kwargs\n        # like precision\n        is_aqt = not isinstance(self.quant, quantizations.Fp8Quantization)\n        kw = {\"mesh_axes\": rhs_mesh_axes} if is_aqt else {\"dtype\": self.dtype}\n        return self.quant.einsum(**kw)(*args)  # pytype: disable=attribute-error\n\n      einsum_op = aqt_einsum\n    else:\n      einsum_op = jnp.einsum\n    return einsum_op\n\n  def maybe_all_gather_kernel_weight_in_expert_parallelism(\n      self, kernel: jax.Array, kernel_axes: Tuple[Optional[str], ...]\n  ):\n    \"\"\"All-gather kernel weight in expert parallelism if needed.\"\"\"\n    if self.get_expert_parallelism_size() > 1:\n      # This will trigger all-gather using weight_dtype\n      # relax it unless really necessary in expert parallelism only\n      # Otherwise compiler will handle communication automatically\n      # esp. with int8 quantization, kernel will be all-gathered in int8 instead\n      # of weight_dtype\n      kernel = nn.with_logical_constraint(kernel, kernel_axes)\n    return kernel\n\n  def dense_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[jax.Array, Optional[jax.Array]]:\n    \"\"\"Dense matrix multiplication.\"\"\"\n    # gate_logits: batch, length, expert\n    gate_logits = nn.with_logical_constraint(gate_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits = nn.with_logical_constraint(pre_bias_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    top_k_weights, top_k_indices = self.get_topk(gate_logits, pre_bias_logits, self.rngs)\n    is_llama4_decoder_layer = self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4\n    if is_llama4_decoder_layer:\n      router_scores = jax.nn.sigmoid(top_k_weights.astype(jnp.float32)).astype(self.dtype)\n      inputs = inputs * router_scores\n    else:\n      weights = self.reshape_and_update_weights(top_k_weights, top_k_indices)\n    matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n\n    if self.config.model_call_mode != \"inference\":\n      softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n      loss = self.load_balance_loss(top_k_indices, softmax_probs)\n    else:\n      loss = None\n    batch_size = inputs.shape[0]\n    seq_len = inputs.shape[1]\n\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    if self.config.capacity_factor > 0:\n      # token dropping if needed\n      if self.config.model_call_mode != \"inference\":\n        # TODO(b/425930949): remove this pylint by refactoring the logic here.\n        dispatch_mask, combine_mask = self.generate_masks(\n            top_k_indices, weights  # pylint: disable=undefined-variable,possibly-used-before-assignment\n        )\n        mask_axes = (\"activation_batch\", \"activation_norm_length\", None, None)\n        dispatch_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_embed\",\n        )\n        mlp_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_mlp\",\n        )\n        dispatch_eimsum = \"BSM,BSEC -> EBCM\"\n        mlp_up_einsum = \"EBCM,EMH -> EBCH\"\n        mlp_down_einsum = \"EBCH,EHM -> EBCM\"\n        output_einsum = \"EBCM,BSEC -> BSM\"\n      else:\n        # TODO(b/425930507): Try replacing `softmax_probs` with padded weights\n        # and verify with decode acc tests.\n        softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n        dispatch_mask, combine_mask = self.generate_masks_subgroup(top_k_indices, softmax_probs)\n        if self.get_context_autoregressive_parallelism_size() > 0 and cp == 1:\n          mask_axes = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        else:\n          mask_axes = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        dispatch_eimsum = \"BNSM,BNSEC -> EBNCM\"\n        mlp_up_einsum = \"EBNCM,EMH -> EBNCH\"\n        mlp_down_einsum = \"EBNCH,EHM -> EBNCM\"\n        output_einsum = \"EBNCM,BNSEC -> BNSM\"\n\n        inputs = jnp.reshape(inputs, (batch_size, cp, sub_seq, inputs.shape[2]))\n        inputs = nn.with_logical_constraint(inputs, input_axis)\n\n      dispatch_mask = nn.with_logical_constraint(dispatch_mask, mask_axes)\n      combine_mask = nn.with_logical_constraint(combine_mask, mask_axes)\n\n      with jax.named_scope(\"dispatch\"):\n        # only cp during prefill\n        dispatch = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=DISPATCH)(\n            dispatch_eimsum, inputs, dispatch_mask, precision=matmul_precision\n        )\n        if cp > 1:\n          dispatch = nn.with_logical_constraint(\n              dispatch,\n              (\n                  None,\n                  \"activation_batch_no_exp\",\n                  \"activation_norm_length\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        dispatch = nn.with_logical_constraint(\n            dispatch,\n            dispatch_axis,\n        )\n      with jax.named_scope(\"wi_0\"):\n        w0_kernel_axes = (\"exp\", None, \"mlp\")\n        w0_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w0_kernel, w0_kernel_axes)\n        layer_w0 = self.get_einsum(rhs_mesh_axes=w0_kernel_axes)(\n            mlp_up_einsum, dispatch, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w0_bias = w0_bias[:, None, None, :]\n          layer_w0 = layer_w0 + w0_bias\n\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = nn.with_logical_constraint(\n            layer_w0,\n            mlp_axis,\n        )\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        w1_kernel_axes = (\"exp\", None, \"mlp\")\n        w1_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w1_kernel, w1_kernel_axes)\n        layer_w1 = self.get_einsum(rhs_mesh_axes=w1_kernel_axes)(\n            mlp_up_einsum, dispatch, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w1_bias = w1_bias[:, None, None, :]\n          layer_w1 = layer_w1 + w1_bias\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = nn.with_logical_constraint(\n            layer_w1,\n            mlp_axis,\n        )\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n      with jax.named_scope(\"wo\"):\n        wo_kernel_axes = (\"exp\", \"mlp\", None)\n        wo_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(wo_kernel, wo_kernel_axes)\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=wo_kernel_axes)(\n            mlp_down_einsum,\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          wo_bias = wo_bias[:, None, None, :]\n          intermediate_layer = intermediate_layer + wo_bias\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        if self.config.model_call_mode != \"inference\":\n          intermediate_layer = nn.with_logical_constraint(\n              intermediate_layer,\n              (\n                  \"activation_exp\",\n                  \"activation_batch_no_exp\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"combine\"):\n        # Matmul & element wise operation\n        output = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=COMBINE)(\n            output_einsum,\n            intermediate_layer,\n            combine_mask,\n            precision=matmul_precision,\n        )\n        if output.ndim == 4:\n          output = jnp.reshape(\n              output,\n              (\n                  output.shape[0],\n                  output.shape[1] * output.shape[2],\n                  output.shape[3],\n              ),\n          )\n      return output, loss\n    else:\n      inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n      with jax.named_scope(\"wi_0\"):\n        layer_w0 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w0 = layer_w0 + w0_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        layer_w1 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w1 = layer_w1 + w1_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      with jax.named_scope(\"wo\"):\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=self.wo_kernel_axes)(\n            \"BSEH,EHM -> BSEM\",\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          intermediate_layer = intermediate_layer + wo_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"w_sum\"):\n        if is_llama4_decoder_layer:\n          weights = self.reshape_and_update_weights(jnp.ones_like(top_k_weights), top_k_indices)\n        # cast to f32 for sum up in einsum op\n        output = jnp.einsum(\n            \"BSEM,BSE -> BSM\",\n            intermediate_layer.astype(jnp.float32),\n            weights.astype(jnp.float32),  # pylint: disable=undefined-variable,possibly-used-before-assignment\n            precision=matmul_precision,\n        ).astype(self.dtype)\n      return output, None\n\n  def retrieve_quantized_weight(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[aqt.QTensor, aqt.QTensor, aqt.QTensor]:\n    \"\"\"Retrieve quantized weights.\"\"\"\n    # This is called only during tracing. This is to invoke creation of\n    # quantized tensor inside AqtEinsum.  After jit, this will become no-op and\n    # will not affect performance.\n    _ = self.dense_matmul(\n        inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n    )\n\n    w0_kernel = self.variables[\"aqt\"][\"AqtEinsum_0\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    w1_kernel = self.variables[\"aqt\"][\"AqtEinsum_1\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    wo_kernel = self.variables[\"aqt\"][\"AqtEinsum_2\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n\n    w0_kernel = max_utils.unbox_logicallypartioned(w0_kernel)\n    w1_kernel = max_utils.unbox_logicallypartioned(w1_kernel)\n    wo_kernel = max_utils.unbox_logicallypartioned(wo_kernel)\n    return w0_kernel, w1_kernel, wo_kernel\n\n  def __call__(self, inputs: jax.Array) -> tuple[jax.Array, Optional[jax.Array]]:\n    cfg = self.config\n    inputs = inputs.astype(cfg.dtype)\n    gate_logits, pre_bias_logits = self.gate(inputs)\n\n    w0_kernel = jnp.asarray(self.wi_0[...], self.dtype)\n    w1_kernel = jnp.asarray(self.wi_1[...], self.dtype)\n    wo_kernel = jnp.asarray(self.wo[...], self.dtype)\n\n    if cfg.mlp_bias:\n      w0_bias = jnp.asarray(self.wi_0_bias[...], self.dtype)\n      w1_bias = jnp.asarray(self.wi_1_bias[...], self.dtype)\n      wo_bias = jnp.asarray(self.wo_bias[...], self.dtype)\n    else:\n      w0_bias, w1_bias, wo_bias = None, None, None\n\n    if cfg.sparse_matmul:\n      if quantizations.in_serve_mode(self.quant):\n        w0_kernel, w1_kernel, wo_kernel = self.retrieve_quantized_weight(\n            inputs,\n            gate_logits,\n            pre_bias_logits,\n            w0_kernel,\n            w1_kernel,\n            wo_kernel,\n            w0_bias,\n            w1_bias,\n            wo_bias,\n        )\n      return self.sparse_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )\n    else:\n      return self.dense_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )",
        "analysis": {
            "module_type": "routed_mixture_of_experts",
            "purpose": "Implements a Mixture of Experts (MoE) layer where each input token is routed to a subset of experts for processing, supporting both sparse and dense computation modes.",
            "input": {
                "shape": "[batch_size, sequence_length, emb_dim]",
                "dtype": "float32 or bfloat16 (determined by config.dtype)"
            },
            "processing_steps": [
                "Computes routing scores (gate logits) for each token over all experts using a GateLogit layer.",
                "Retrieves the kernel and bias weights for the expert MLPs.",
                "Based on the `config.sparse_matmul` flag, it dispatches to either the sparse or dense computation path.",
                "Calls `sparse_matmul` for token-to-expert routing and computation, typically using custom kernels like `gmm` or `ragged_dot`.",
                "Calls `dense_matmul` for an einsum-based implementation that can handle token dropping via capacity factor.",
                "Returns the processed tensor and an optional load-balancing loss (during training)."
            ],
            "output": {
                "shape": "A tuple of (output_tensor, optional_loss). The output_tensor has shape [batch_size, sequence_length, emb_dim]. The optional_loss is a scalar."
            },
            "dependencies": [
                "flax.nnx.Module",
                "GateLogit",
                "quantizations.AqtQuantization",
                "_sort_activations",
                "jax.sharding.Mesh",
                "ctypes.Config"
            ],
            "parameters": {
                "config": "The main configuration object containing settings like `emb_dim`, `mlp_activations`, `sparse_matmul`, `capacity_factor`, etc.",
                "num_experts": "The total number of experts in the MoE layer.",
                "num_experts_per_tok": "The number of experts to which each token is routed.",
                "intermediate_dim": "The hidden dimension of each expert's MLP.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This is a core component of a sparse transformer model.",
                "It initializes several `nnx.Param` for weights (wi_0, wi_1, wo) and biases.",
                "It handles different sharding strategies for weights based on `config.fsdp_shard_on_exp`.",
                "The main entry point is the `__call__` method, which acts as a dispatcher."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RoutedMoE module, setting up the gating mechanism, expert weights, and biases.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Determine kernel sharding axes based on `config.fsdp_shard_on_exp`.",
                        "Initialize the `GateLogit` module for routing.",
                        "Initialize the MLP activation function.",
                        "Initialize expert weights (`wi_0`, `wi_1`, `wo`) as `nnx.Param` or zeros if in serving mode with quantization.",
                        "Initialize expert biases if `config.mlp_bias` is true."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "GateLogit",
                        "nnx.Param",
                        "quantizations.in_serve_mode",
                        "linears._convert_to_activation_function",
                        "default_bias_init"
                    ],
                    "notes": [
                        "Handles special logic for quantization serving mode where weights are initialized as zeros and retrieved later."
                    ]
                },
                "get_topk": {
                    "purpose": "Selects the top-k experts for each token based on gate logits and applies model-specific routing logic.",
                    "input": {
                        "shape": "gate_logits: [batch, sequence, num_experts], pre_bias_logits: [batch, sequence, num_experts]",
                        "dtype": "float32 or bfloat16"
                    },
                    "processing_steps": [
                        "If `config.use_random_routing` is enabled, call `random_routing`.",
                        "If the model is `deepseek3`, call `self.deepseek_routing`.",
                        "Otherwise, use `jax.lax.top_k` to find the top experts.",
                        "Apply model-specific weight scaling or normalization (e.g., for DeepSeek, Llama4, Qwen3)."
                    ],
                    "output": {
                        "shape": "A tuple of (top_k_weights, top_k_indices), both with shape [batch, sequence, num_experts_per_tok]."
                    },
                    "dependencies": [
                        "random_routing",
                        "self.deepseek_routing",
                        "jax.lax.top_k",
                        "jax.nn.softmax"
                    ],
                    "notes": [
                        "Contains conditional logic to handle different routing strategies for various model architectures."
                    ]
                },
                "permute": {
                    "purpose": "Prepares tokens for sparse expert computation by sorting them based on their assigned expert.",
                    "input": {
                        "shape": "inputs: [batch, seq, emb], gate_logits: [batch, seq, num_experts], pre_bias_logits: [batch, seq, num_experts]",
                        "dtype": "float32 or bfloat16"
                    },
                    "processing_steps": [
                        "Reshape inputs to [batch*seq, emb].",
                        "Call `self.get_topk` to get expert assignments.",
                        "Flatten expert assignments and sort them.",
                        "Repeat and sort the input tokens according to the sorted expert assignments using `_sort_activations`.",
                        "Calculate `group_size` (number of tokens per expert) using `jnp.bincount`."
                    ],
                    "output": {
                        "shape": "A tuple containing (sorted_inputs, sorted_selected_experts, weights, group_size, sorted_experts)."
                    },
                    "dependencies": [
                        "self.get_topk",
                        "_sort_activations",
                        "jnp.bincount"
                    ],
                    "notes": [
                        "This is a key preprocessing step for efficient sparse computation, grouping tokens destined for the same expert together."
                    ]
                },
                "unpermute": {
                    "purpose": "Reverses the permutation, combining the outputs from different experts and restoring the original token order.",
                    "input": {
                        "shape": "intermediate: [total_tokens, emb_dim], sorted_selected_experts: [total_tokens], weights: [batch, seq, num_experts_per_tok]",
                        "dtype": "float32 or bfloat16"
                    },
                    "processing_steps": [
                        "Unsort the intermediate expert outputs using `_sort_activations` with an inverse permutation.",
                        "Reshape the unsorted outputs and weights.",
                        "Combine the expert outputs for each token by taking a weighted sum using `jnp.einsum`.",
                        "Reshape the final output to [batch_size, sequence_length, emb_dim]."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "_sort_activations",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "This is the final step in the sparse computation path to produce the layer's output."
                    ]
                },
                "sparse_matmul": {
                    "purpose": "Performs the MoE computation using a sparse approach, where tokens are explicitly routed and sent to experts, often leveraging specialized kernels.",
                    "input": {
                        "shape": "inputs: [batch, seq, emb], gate_logits: [batch, seq, num_experts], pre_bias_logits: [batch, seq, num_experts], plus kernel and bias weights.",
                        "dtype": "float32 or bfloat16"
                    },
                    "processing_steps": [
                        "Define a `wrapper` function decorated with `jax.shard_map` to handle parallelism.",
                        "Inside the wrapper, call `self.permute` to sort tokens by expert.",
                        "Handle inter-device communication (e.g., `ragged_all_to_all`) if using expert parallelism.",
                        "Use a `gmm` (grouped matrix multiplication) function, which calls `mblx.gmm` or `jax.lax.ragged_dot`, to apply expert MLPs.",
                        "Call `self.unpermute` to combine results and restore the original token order."
                    ],
                    "output": {
                        "shape": "A tuple of (output_tensor, None), where output_tensor is [batch, seq, emb]."
                    },
                    "dependencies": [
                        "jax.shard_map",
                        "self.permute",
                        "self.unpermute",
                        "jax.lax.ragged_all_to_all",
                        "mblx.gmm",
                        "jax.lax.ragged_dot"
                    ],
                    "notes": [
                        "This is a complex method that orchestrates distributed sparse computation and is central to the efficiency of the MoE layer."
                    ]
                },
                "dense_matmul": {
                    "purpose": "Performs the MoE computation using a dense approach with einsums, which is more general and can handle token dropping via a capacity factor.",
                    "input": {
                        "shape": "inputs: [batch, seq, emb], gate_logits: [batch, seq, num_experts], pre_bias_logits: [batch, seq, num_experts], plus kernel and bias weights.",
                        "dtype": "float32 or bfloat16"
                    },
                    "processing_steps": [
                        "Call `self.get_topk` to get expert assignments and weights.",
                        "Optionally calculate `load_balance_loss` during training.",
                        "If `capacity_factor > 0`, generate dispatch/combine masks to enforce a limit on tokens per expert.",
                        "Use `jnp.einsum` to dispatch inputs to experts, apply the MLP transformations, and combine the outputs based on the routing weights or masks."
                    ],
                    "output": {
                        "shape": "A tuple of (output_tensor, optional_loss_tensor), where output_tensor is [batch, seq, emb]."
                    },
                    "dependencies": [
                        "self.get_topk",
                        "self.load_balance_loss",
                        "self.generate_masks",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "This method simulates routing with dense operations, making it compatible with hardware that lacks specialized sparse kernels."
                    ]
                },
                "__call__": {
                    "purpose": "The main forward pass of the MoE layer, dispatching to either sparse or dense computation.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "float32 or bfloat16"
                    },
                    "processing_steps": [
                        "Call `self.gate` to get routing logits.",
                        "Retrieve kernel and bias weights.",
                        "If `config.sparse_matmul` is true, call `self.sparse_matmul`.",
                        "Otherwise, call `self.dense_matmul`."
                    ],
                    "output": {
                        "shape": "A tuple of (output_tensor, optional_loss), where output_tensor has shape [batch_size, sequence_length, emb_dim]."
                    },
                    "dependencies": [
                        "self.gate",
                        "self.sparse_matmul",
                        "self.dense_matmul"
                    ],
                    "notes": [
                        "This method acts as the primary entry point and controller for the MoE layer's logic."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedAndSharedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedAndSharedMoE(nnx.Module):\n  \"\"\"Implements a block which combines shared and routed experts.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      mesh: jax.sharding.Mesh,\n      kernel_init: NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"nitializes the RoutedAndSharedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n    # NOTE: the name MoeBlock_0 is to ensure reverse compatibility with\n    # existing checkpoints for routed experts.\n    self.MoeBlock_0 = RoutedMoE(\n        config=self.config,\n        num_experts=self.config.num_experts,\n        num_experts_per_tok=self.config.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=self.config.moe_mlp_dim,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n    self.shared_experts = linears.MlpBlock(\n        in_features=self.config.emb_dim,\n        intermediate_dim=self.config.shared_experts * self.config.moe_mlp_dim,\n        activations=self.config.mlp_activations,\n        intermediate_dropout_rate=self.config.dropout_rate,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        config=self.config,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n\n  @property\n  def routed_moe(self):\n    return self.MoeBlock_0\n\n  def __call__(self, inputs: jax.Array) -> jax.Array:\n    routed_experts, _ = self.routed_moe(inputs)\n    shared_experts = self.shared_experts(inputs)\n    return routed_experts + shared_experts",
        "analysis": {
            "module_type": "routed_and_shared_mixture_of_experts",
            "purpose": "Implements a block that combines the outputs of a routed Mixture-of-Experts (MoE) layer and a standard shared MLP layer.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "The input is processed in parallel by a `RoutedMoE` block and a `linears.MlpBlock` (shared experts).",
                "The outputs from both blocks are added together element-wise."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "RoutedMoE",
                "linears.MlpBlock",
                "ctypes.Config",
                "jax.sharding.Mesh",
                "initializers.NdInitializer",
                "quantizations.AqtQuantization"
            ],
            "parameters": {
                "config.num_experts": "The total number of experts in the routed MoE layer.",
                "config.num_experts_per_tok": "The number of experts to route each token to.",
                "config.shared_experts": "A multiplier for the intermediate dimension of the shared MLP block.",
                "config.moe_mlp_dim": "The intermediate dimension for each expert in both the routed and shared blocks.",
                "config.emb_dim": "The embedding dimension of the input and output."
            },
            "notes": [
                "The routed expert module is named `MoeBlock_0` to ensure reverse compatibility with existing checkpoints.",
                "This architecture allows the model to combine specialized, sparsely activated experts with a dense, shared transformation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by creating a `RoutedMoE` instance and a shared `MlpBlock` instance.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration parameters like `config`, `mesh`, `dtype`, etc.",
                        "Instantiates a `RoutedMoE` module as `self.MoeBlock_0` for the routed experts.",
                        "Instantiates a `linears.MlpBlock` module as `self.shared_experts`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RoutedMoE",
                        "linears.MlpBlock",
                        "initializers.nd_dense_init"
                    ],
                    "notes": [
                        "The name `MoeBlock_0` is intentionally used for the `RoutedMoE` instance to maintain backward compatibility with checkpoints."
                    ]
                },
                "routed_moe": {
                    "purpose": "A property that returns the internal `RoutedMoE` instance (`self.MoeBlock_0`).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns `self.MoeBlock_0`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This is a read-only property for accessing the routed expert component."
                    ]
                },
                "__call__": {
                    "purpose": "Processes the input tensor through both the routed and shared experts and returns their sum.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Pass the input through the `routed_moe` module to get the output from the routed experts.",
                        "Pass the input through the `shared_experts` MLP block.",
                        "Perform element-wise addition of the outputs from the routed and shared experts."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "self.routed_moe",
                        "self.shared_experts"
                    ],
                    "notes": [
                        "The `routed_moe` call returns a tuple `(output, loss)`, but only the output tensor is used in this method."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_gate_logit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_gate_logit(\n    inputs_shape: tuple[int, ...],\n    out_features_shape: Union[Iterable[int], int],\n    model_name: str,\n    axis: Union[Iterable[int], int] = -1,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: Tuple[Optional[str], ...] = (),\n    use_bias: bool = False,\n    score_func: str = \"\",\n    quant: Optional[quantizations.AqtQuantization] = None,\n    matmul_precision: str = \"default\",\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a GateLogit Linen module.\"\"\"\n\n  axis = linears.canonicalize_tuple(axis)\n  in_features_shape = tuple(inputs_shape[ax] for ax in linears.normalize_axes(axis, len(inputs_shape)))\n\n  module = nnx_wrappers.to_linen(\n      GateLogit,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      model_name=model_name,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      use_bias=use_bias,\n      score_func=score_func,\n      quant=quant,\n      matmul_precision=matmul_precision,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "gate_logit_factory",
            "purpose": "A factory function that creates and returns a Flax Linen module instance of the `GateLogit` NNX class.",
            "input": {
                "shape": "N/A (takes configuration parameters, not tensors). The `inputs_shape` parameter specifies the shape of the tensor the returned module will expect.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Canonicalizes the `axis` parameter using `linears.canonicalize_tuple`.",
                "Calculates the `in_features_shape` for the `GateLogit` module by selecting dimensions from `inputs_shape` based on the normalized `axis`.",
                "Instantiates a `GateLogit` NNX module and wraps it as a Flax Linen module using `nnx_wrappers.to_linen`.",
                "Returns the created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance. This module, when called, will produce a tensor where the dimensions specified by `axis` in the input are replaced by `out_features_shape`."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "GateLogit",
                "linears.canonicalize_tuple",
                "linears.normalize_axes",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "inputs_shape": "The shape of the expected input tensor for the module being created.",
                "out_features_shape": "The shape of the output features from the created module, typically the number of experts.",
                "model_name": "The name of the model, which can affect routing logic within the GateLogit module.",
                "axis": "The axis or axes over which the linear transformation is applied.",
                "quant": "Optional AQT quantization configuration for the module.",
                "use_bias": "Boolean indicating whether to add a learnable bias to the gate logits.",
                "score_func": "Optional scoring function for output normalization before applying bias."
            },
            "notes": [
                "This function acts as a bridge, allowing an NNX-defined module (`GateLogit`) to be used within a standard Flax Linen model architecture.",
                "The `in_features_shape` for the underlying `GateLogit` module is dynamically computed based on the full `inputs_shape` and the specified `axis`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_moe(\n    config: ctypes.Config,\n    num_experts: int,\n    num_experts_per_tok: int,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    intermediate_dim: int = 2048,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedMoE,\n      config=config,\n      num_experts=num_experts,\n      num_experts_per_tok=num_experts_per_tok,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      intermediate_dim=intermediate_dim,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "routed_mixture_of_experts_factory",
            "purpose": "A factory function that creates and configures a `RoutedMoE` Flax Linen module by wrapping the corresponding `nnx.Module`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RoutedMoE` nnx.Module into a Flax Linen module.",
                "Passes all configuration parameters (config, num_experts, mesh, etc.) to the `RoutedMoE` constructor during conversion.",
                "Returns the newly created Flax Linen module instance."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RoutedMoE",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "A `ctypes.Config` object containing model hyperparameters.",
                "num_experts": "The total number of expert networks in the MoE layer.",
                "num_experts_per_tok": "The number of experts to which each token is routed (top-k).",
                "mesh": "The JAX device mesh used for model parallelism and sharding.",
                "intermediate_dim": "The intermediate dimension of the feed-forward network within each expert.",
                "quant": "Optional AQT quantization configuration for the expert weights."
            },
            "notes": [
                "This function returns a Flax Linen module instance, not a tensor.",
                "The returned module, when called, takes an input tensor of shape `[batch_size, sequence_length, embedding_dim]` and produces an output tensor of the same shape, along with an optional load balancing loss.",
                "The `abstract_init=False` argument ensures that the module's parameters are initialized immediately upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_and_shared_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_and_shared_moe(\n    config: ctypes.Config,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedAndSharedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedAndSharedMoE,\n      config=config,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "routed_and_shared_moe_factory",
            "purpose": "A factory function that creates a Flax Linen module from a `RoutedAndSharedMoE` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RoutedAndSharedMoE` NNX module into a Flax Linen module.",
                "Passes all configuration parameters to the `RoutedAndSharedMoE` constructor through the wrapper.",
                "Returns the created Linen module."
            ],
            "output": {
                "shape": "A Flax Linen module instance of `RoutedAndSharedMoE`."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RoutedAndSharedMoE",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "A `ctypes.Config` object containing model and training configurations.",
                "mesh": "A `jax.sharding.Mesh` object for device parallelism.",
                "kernel_init": "An `NdInitializer` for initializing kernel weights.",
                "kernel_axes": "A tuple of logical axis names for partitioning the kernel.",
                "weight_dtype": "The data type for the kernel weights.",
                "dtype": "The data type for computation.",
                "quant": "An optional `AqtQuantization` configuration for quantization."
            },
            "notes": [
                "This function acts as a bridge between the NNX-defined `RoutedAndSharedMoE` class and the Flax Linen API.",
                "The `abstract_init=False` argument ensures that the module's parameters are initialized immediately upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#roll_and_mask",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def roll_and_mask(x: jnp.ndarray, shift: int = -1) -> jnp.ndarray:\n  \"\"\"\n  Performs a leftward roll on the sequence axis (axis=1) and masks the\n  newly created invalid positions at the end of the sequence.\n  Assumes input `x` has a batch dimension at axis 0 and sequence at axis 1.\n\n  Args:\n    x: The input array of shape [batch, seq_len, ...].\n    shift: The number of positions to shift left.\n\n  Returns:\n    The rolled array of the same shape as x.\n  \"\"\"\n  # If shift is 0, it's a no-op. Return the original array.\n  if shift == 0:\n    return x\n\n  # to set the last `abs(shift)` elements of the sequence to zero.\n  return jnp.roll(x, shift, axis=1).at[:, shift:, ...].set(0)",
        "analysis": {
            "module_type": "sequence_roll_and_mask",
            "purpose": "Performs a circular left shift on the sequence axis of a tensor and masks the wrapped-around elements at the end by setting them to zero.",
            "input": {
                "shape": "[batch_size, sequence_length, ...]",
                "dtype": "numeric (e.g., float32, int32)"
            },
            "processing_steps": [
                "Check if `shift` is 0; if so, return the input tensor `x` unchanged.",
                "Perform a circular shift on the input tensor `x` along the sequence axis (axis=1) by `shift` positions using `jnp.roll`.",
                "Set the last `abs(shift)` elements along the sequence axis to 0 to mask the wrapped-around values."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, ...]"
            },
            "dependencies": [
                "jax.numpy.roll"
            ],
            "parameters": {
                "x": "The input array of shape [batch, seq_len, ...].",
                "shift": "The number of positions to shift left. Defaults to -1."
            },
            "notes": [
                "The function assumes the input tensor `x` has a batch dimension at axis 0 and a sequence dimension at axis 1.",
                "This operation is commonly used in sequence models to align inputs and targets for predicting future tokens."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionLayer",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionLayer(nn.Module):\n  \"\"\"\n  Implements Multi-Token Prediction (MTP) step:\n      1. Normalization of previous hidden state and target token embedding.\n      2. Concatenation and Projection of normalized features.\n      3. Processing through a Transformer Decoder Layer.\n\n      Equation Representation (Conceptual):\n          norm_h = RMSNorm(h_prev)\n          norm_e = RMSNorm(e_target)\n          h_proj = W_p(concat(norm_h, norm_e))\n          h_next = TransformerLayer(h_proj, pos_ids, segment_ids, ...)\n\n      It takes the previous hidden state and target embedding as input and outputs the\n      processed hidden state from its internal transformer block.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  layer_number: int\n  transformer_layer_module: Type[DecoderLayer] = DecoderLayer\n\n  @nn.compact\n  def __call__(\n      self,\n      prev_hidden_state: jnp.ndarray,\n      target_token_embedding: jnp.ndarray,\n      position_ids: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> jnp.ndarray:\n    \"\"\"\n    Applies the MTP combination, projection, and internal transformer processing.\n\n    Args:\n        prev_hidden_state: Hidden state from the previous step/layer.\n                           Shape: [batch, seq_len, hidden_size]\n        target_token_embedding: Embedding of the target token. In the context of MTP,\n                                this often refers to a token at a position relative\n                                to the current step, where the offset is determined\n                                by the layer number `k` (i.e., token t+k).\n                                Shape: [batch, seq_len, embed_dim]\n        position_ids: Original position IDs for the sequence.\n                      Shape: [batch, seq_len]\n        decoder_segment_ids: Original segment IDs for the sequence (for attention mask).\n                             Shape: [batch, seq_len]\n        deterministic: If true, disable dropout.\n        model_mode: The current operational mode (train, eval, decode).\n\n    Returns:\n        next_hidden_state: The hidden state produced by this MTP step's internal transformer.\n                           Shape: [batch, seq_len, hidden_size]\n    \"\"\"\n    cfg = self.config\n    mesh = self.mesh\n    k = self.layer_number\n\n    # --- 1. Normalize Hidden State and Embedding ---\n    embedding_norm_layer = rms_norm(\n        num_features=target_token_embedding.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_embedding_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n    embedding_norm = embedding_norm_layer(target_token_embedding)\n\n    hidden_state_norm_layer = rms_norm(\n        num_features=prev_hidden_state.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_hidden_state_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n\n    hidden_state_norm = hidden_state_norm_layer(prev_hidden_state)\n\n    # --- 2. Concatenate Normalized Representations ---\n    # Shape: [B, S, 2*H]\n    concatenated_features = jnp.concatenate([embedding_norm, hidden_state_norm], axis=-1)\n\n    # --- 3. Project Concatenated Features ---\n    # Projects from 2*H back down to H\n    projection_layer = dense_general(\n        inputs_shape=concatenated_features.shape,\n        out_features_shape=cfg.base_emb_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        use_bias=False,\n        kernel_axes=(\"concat_embed\", \"embed\"),\n        name=f\"mtp_{k}_projection\",\n    )\n    # Shape: [B, S, H]\n    projected_features = projection_layer(concatenated_features)\n\n    # --- 4. Pass through MTP Transformer Block ---\n    output = self.transformer_layer_module(\n        config=cfg, mesh=mesh, model_mode=model_mode, name=f\"mtp_{k}_transformer_layer\"\n    )(\n        inputs=projected_features,\n        decoder_segment_ids=decoder_segment_ids,\n        decoder_positions=position_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    if isinstance(output, tuple):\n      # Handles the scan=True case, where the output is a tuple.\n      next_hidden_state = output[0]\n    else:\n      # Handles the scan=False case, where the output is a single tensor.\n      next_hidden_state = output\n\n    # Shape: [B, S, H]\n    # --- Return Processed Hidden State ---\n    return next_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_layer",
            "purpose": "Implements a single Multi-Token Prediction (MTP) step by normalizing, combining, and projecting a previous hidden state and a target token embedding, then processing the result through a transformer decoder layer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Normalizes the previous hidden state and the target token embedding using RMSNorm.",
                "Concatenates the two normalized tensors.",
                "Projects the concatenated tensor back to the model's hidden dimension.",
                "Processes the projected tensor through an internal transformer decoder layer.",
                "Returns the output hidden state from the transformer layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.normalizations.rms_norm",
                "maxtext.layers.linears.dense_general",
                "maxtext.layers.decoders.DecoderLayer",
                "maxtext.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like dtype, embedding dimensions, and normalization epsilon.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "layer_number": "An integer `k` that identifies this MTP layer's position in the sequence, used for naming sub-modules.",
                "transformer_layer_module": "The class type to use for the internal transformer block, defaults to `DecoderLayer`."
            },
            "notes": [
                "This layer represents a single step in a chain of MTP layers.",
                "The internal layers (normalization, projection, transformer) are named uniquely using the `layer_number`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the MTP combination, projection, and internal transformer processing to generate the next hidden state.",
                    "input": {
                        "shape": "{'prev_hidden_state': [batch_size, sequence_length, hidden_dim], 'target_token_embedding': [batch_size, sequence_length, embed_dim], 'position_ids': [batch_size, sequence_length], 'decoder_segment_ids': [batch_size, sequence_length]}",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Apply RMS normalization to `target_token_embedding`.",
                        "Apply RMS normalization to `prev_hidden_state`.",
                        "Concatenate the normalized embedding and hidden state along the feature dimension.",
                        "Apply a dense projection layer to reduce the concatenated feature dimension back to `hidden_dim`.",
                        "Pass the projected features through the internal transformer layer (`self.transformer_layer_module`).",
                        "Extract the hidden state from the transformer layer's output, handling both tuple and single tensor returns."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "jnp.concatenate",
                        "dense_general",
                        "self.transformer_layer_module"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is enabled in the internal transformer layer.",
                        "The method handles different output formats from the transformer layer, which can vary depending on whether it's used within a `scan` operation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionBlock",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionBlock(nn.Module):\n  \"\"\"Orchestrates the MTP process by running a sequence of MTP layers.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  transformer_layer_module: Type[DecoderLayer]\n  decoder: Decoder\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding,\n      main_hidden_state,\n      input_ids,\n      target_ids,\n      target_mask,\n      position_ids,\n      decoder_segment_ids,\n      model_mode,\n      deterministic,\n  ):\n    cfg = self.config\n    # The initial hidden state for the MTP chain is the raw output from the main model.\n    mtp_hidden_state = main_hidden_state\n\n    # These variables are updated sequentially in each loop iteration,\n    # moving the prediction window one token to the right each time.\n    rolled_input_ids = input_ids\n    rolled_target_ids = target_ids\n    rolled_target_mask = target_mask\n    rolled_position_id = position_ids\n\n    # Range chosen to align with the naming convention of the paper\n    for k in range(1, cfg.mtp_num_layers + 1):\n      # Sequentially roll all tensors to prepare data for predicting the k-th future token.\n      rolled_input_ids = roll_and_mask(rolled_input_ids)\n      rolled_target_ids = roll_and_mask(rolled_target_ids)\n      rolled_target_mask = roll_and_mask(rolled_target_mask)\n      rolled_position_id = roll_and_mask(rolled_position_id)\n\n      # Embed the k-th future input tokens using the shared embedding module\n      target_token_embedding = self.decoder._apply_embedding(\n          shared_embedding, rolled_input_ids, rolled_position_id, deterministic, self.decoder.model_mode\n      )\n\n      # Instantiate and apply the MTP layer for this step\n      mtp_layer = MultiTokenPredictionLayer(\n          config=cfg,\n          mesh=self.mesh,\n          layer_number=k,\n          name=f\"mtp_layer_{k}\",\n          transformer_layer_module=self.transformer_layer_module,\n      )\n\n      next_mtp_hidden_state = mtp_layer(\n          mtp_hidden_state,\n          target_token_embedding,\n          position_ids,\n          decoder_segment_ids,\n          deterministic,\n          self.decoder.model_mode,\n      )\n\n      # Project to logits using the shared embedding transpose\n      mtp_logits = self.decoder._apply_output_head(shared_embedding, next_mtp_hidden_state, deterministic, model_mode)\n\n      # Calculate cross-entropy loss for this specific layer's prediction\n      mtp_xent, _ = max_utils.cross_entropy_with_logits(\n          mtp_logits, jax.nn.one_hot(rolled_target_ids, cfg.vocab_size), 0.0\n      )\n      mtp_xent_masked = mtp_xent * rolled_target_mask\n\n      # This logic doesn't run during model initialization to avoid unwated population of the mutable collections.\n      if not self.is_initializing():\n        # For evaluation, save the top prediction and a valid token mask.\n        # This is only active for the target layer during an eval run.\n        if cfg.mtp_eval_target_module == k and self.is_mutable_collection(\"mtp_acceptance\"):\n          mtp_top_1_pred = jnp.argmax(mtp_logits, axis=-1)\n          self.sow(\"mtp_acceptance\", \"mtp_preds\", mtp_top_1_pred)\n          self.sow(\"mtp_acceptance\", \"mtp_mask\", rolled_target_mask)\n\n        # For training, save the loss components for this MTP head.\n        # This is only active during a training run.\n        if self.is_mutable_collection(\"mtp_losses\"):\n          self.sow(\"mtp_losses\", \"losses\", jnp.sum(mtp_xent_masked))\n          self.sow(\"mtp_losses\", \"weights\", jnp.sum(rolled_target_mask))\n\n      # The output of this layer is the input for the next, maintaining the causal chain.\n      mtp_hidden_state = next_mtp_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_block",
            "purpose": "Orchestrates the Multi-Token Prediction (MTP) process by iteratively applying a sequence of MTP layers to predict multiple future tokens and calculate their respective losses.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the MTP hidden state with the main model's final hidden state.",
                "Iterates from 1 to `config.mtp_num_layers` to predict the k-th future token in each step.",
                "In each iteration, it rolls input tensors, creates target token embeddings, and processes them through a `MultiTokenPredictionLayer`.",
                "Calculates logits and cross-entropy loss for each prediction step.",
                "Uses `sow` to store losses for training and top-1 predictions for evaluation in mutable collections."
            ],
            "output": {
                "shape": "N/A (The module does not return a tensor; it uses `sow` to store results in mutable collections)."
            },
            "dependencies": [
                "MultiTokenPredictionLayer",
                "Decoder",
                "roll_and_mask",
                "max_utils.cross_entropy_with_logits",
                "DecoderLayer"
            ],
            "parameters": {
                "config.mtp_num_layers": "The number of future tokens to predict, determining the number of MTP layers to run.",
                "config.mtp_eval_target_module": "Specifies which MTP layer's predictions to use for calculating the acceptance rate during evaluation."
            },
            "notes": [
                "The module does not return a tensor directly. Instead, it uses Flax's `sow` mechanism to record losses and predictions in mutable collections named 'mtp_losses' and 'mtp_acceptance'.",
                "The hidden state is passed sequentially from one MTP layer to the next, forming a causal chain."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass, executing a chain of MTP layers to predict multiple future tokens.",
                    "input": {
                        "shape": "shared_embedding: [vocab_size, hidden_dim], main_hidden_state: [batch, seq, hidden], input_ids/target_ids/target_mask/position_ids/decoder_segment_ids: [batch, seq]",
                        "dtype": "float32 for embeddings/states, int32 for ids/masks"
                    },
                    "processing_steps": [
                        "Initialize MTP hidden state from `main_hidden_state`.",
                        "Loop `k` from 1 to `config.mtp_num_layers`.",
                        "Roll input, target, mask, and position tensors using `roll_and_mask`.",
                        "Embed the rolled input IDs via `decoder._apply_embedding`.",
                        "Instantiate and apply a `MultiTokenPredictionLayer` to get the next hidden state.",
                        "Project the hidden state to logits via `decoder._apply_output_head`.",
                        "Calculate cross-entropy loss using `max_utils.cross_entropy_with_logits`.",
                        "Conditionally `sow` losses (for training) or predictions (for evaluation) into mutable collections.",
                        "Update the MTP hidden state for the next iteration."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "roll_and_mask",
                        "MultiTokenPredictionLayer",
                        "Decoder._apply_embedding",
                        "Decoder._apply_output_head",
                        "max_utils.cross_entropy_with_logits",
                        "self.sow"
                    ],
                    "notes": [
                        "The primary output is through side effects via `self.sow`, not a return value.",
                        "The loop iteratively shifts the prediction window one token to the right."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_loss",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_loss(intermediate_outputs, config):\n  \"\"\"Calculates the Multi Token Prediction loss from intermediate outputs.\"\"\"\n  losses_path = (\"mtp_losses\", \"mtp_block\", \"losses\")\n  weights_path = (\"mtp_losses\", \"mtp_block\", \"weights\")\n\n  mtp_losses = maxtext_utils.get_nested_value(intermediate_outputs, losses_path, default=())\n  mtp_weights = maxtext_utils.get_nested_value(intermediate_outputs, weights_path, default=())\n\n  if not mtp_losses:  # MTP heads did not run\n    return 0.0\n\n  sum_of_all_mtp_losses = jnp.sum(jnp.array(mtp_losses))\n  sum_of_all_mtp_weights = jnp.sum(jnp.array(mtp_weights))\n\n  avg_mtp_loss = sum_of_all_mtp_losses / (sum_of_all_mtp_weights + EPS)\n  scaled_mtp_loss = avg_mtp_loss * config.mtp_loss_scaling_factor\n  return scaled_mtp_loss",
        "analysis": {
            "module_type": "mtp_loss_calculator",
            "purpose": "Calculates the scaled, average Multi-Token Prediction (MTP) loss from intermediate model outputs.",
            "input": {
                "shape": "intermediate_outputs: Nested dictionary, config: Object",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract lists of MTP losses and weights from the `intermediate_outputs` dictionary using predefined paths.",
                "Return 0.0 if no MTP losses are found, indicating MTP heads did not run.",
                "Sum all extracted MTP losses and weights.",
                "Calculate the average MTP loss by dividing the sum of losses by the sum of weights, adding a small epsilon for numerical stability.",
                "Scale the average loss using the `mtp_loss_scaling_factor` from the config.",
                "Return the final scaled MTP loss."
            ],
            "output": {
                "shape": "Scalar (a single float value)."
            },
            "dependencies": [
                "maxtext_utils.get_nested_value",
                "jax.numpy.sum",
                "jax.numpy.array",
                "MaxText.globals.EPS"
            ],
            "parameters": {
                "config.mtp_loss_scaling_factor": "A float value used to scale the final calculated average MTP loss."
            },
            "notes": [
                "The function gracefully handles cases where MTP is not active by checking for the presence of `mtp_losses` and returning zero.",
                "Losses and weights are expected to be located at specific nested paths within the `intermediate_outputs` dictionary: ('mtp_losses', 'mtp_block', 'losses') and ('mtp_losses', 'mtp_block', 'weights')."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_acceptance_rate",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_acceptance_rate(intermediate_outputs, config):\n  \"\"\"Calculates the MTP acceptance rate from intermediate outputs.\"\"\"\n\n  sown_data = maxtext_utils.get_nested_value(intermediate_outputs, (\"mtp_acceptance\", \"mtp_block\"), {})\n  mtp_preds = maxtext_utils.get_nested_value(sown_data, (\"mtp_preds\",), [None])[0]\n  valid_mask = maxtext_utils.get_nested_value(sown_data, (\"mtp_mask\",), [None])[0]\n\n  # These values are only \"sown\" (saved) during an evaluation run and only for the specific\n  # MTP layer specified by `config.mtp_eval_target_module`. This check handles cases\n  # where the required data is absent (e.g., during a training step) and prevents errors.\n  if mtp_preds is None or valid_mask is None:\n    return 0.0\n\n  # Get the main model's greedy predictions from the logits.\n  main_model_preds = jnp.argmax(intermediate_outputs[\"logits\"], axis=-1)\n\n  # Roll the main model's predictions to align them in time with the MTP head's target.\n  rolled_main_preds = main_model_preds\n  for _ in range(config.mtp_eval_target_module):\n    rolled_main_preds = roll_and_mask(rolled_main_preds)\n\n  # Compare the aligned predictions. The `valid_mask` ensures that the comparison\n  # only happens on valid tokens, ignoring the placeholder values introduced at the\n  # end of the sequence by the `roll_and_mask` operation.\n  correct_predictions = jnp.sum((mtp_preds == rolled_main_preds) * valid_mask)\n  total_valid_tokens = jnp.sum(valid_mask)\n\n  # Return acceptance rate as a percentage\n  return (correct_predictions / (total_valid_tokens + EPS)) * 100",
        "analysis": {
            "module_type": "metric_calculation",
            "purpose": "Calculates the Multi-Token Prediction (MTP) acceptance rate by comparing the predictions of a specific MTP head with the time-aligned greedy predictions of the main model.",
            "input": {
                "shape": "intermediate_outputs: A dictionary containing model outputs, including 'logits' of shape [batch_size, seq_len, vocab_size] and sown MTP data. config: A configuration object.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract MTP predictions (`mtp_preds`) and a validity mask (`valid_mask`) from the `intermediate_outputs` dictionary using `maxtext_utils.get_nested_value`.",
                "Return 0.0 if `mtp_preds` or `valid_mask` are not found, which is expected during training steps.",
                "Calculate the main model's greedy predictions by applying `jnp.argmax` to the `logits` from `intermediate_outputs`.",
                "Time-align the main model's predictions by repeatedly applying the `roll_and_mask` function, controlled by `config.mtp_eval_target_module`.",
                "Compare the aligned main model predictions with the MTP predictions, using the `valid_mask` to exclude invalid tokens.",
                "Calculate the sum of correct predictions and the sum of total valid tokens.",
                "Compute the acceptance rate as `(correct_predictions / (total_valid_tokens + EPS)) * 100`."
            ],
            "output": {
                "shape": "Scalar (float)."
            },
            "dependencies": [
                "maxtext_utils.get_nested_value",
                "jnp.argmax",
                "jnp.sum",
                "roll_and_mask"
            ],
            "parameters": {
                "config.mtp_eval_target_module": "An integer specifying which MTP head's predictions to use for the comparison. This value determines how many times the main model's predictions are rolled for time-alignment."
            },
            "notes": [
                "This function is intended for use during evaluation, as the necessary MTP predictions and masks are only 'sown' (saved) in that mode for the specific layer targeted for evaluation.",
                "The acceptance rate measures how often the MTP head's prediction for a future token matches what the main model would have greedily predicted for that same token.",
                "A small epsilon (`EPS`) is added to the denominator to prevent division by zero."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#is_vanilla_variable",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def is_vanilla_variable(vs: variablelib.VariableState) -> bool:\n  \"\"\"A variables state is vanilla if its metadata is essentially blank.\n\n  Returns False only if it has non-empty hooks or any non-built-in attribute.\n  \"\"\"\n  for key, value in vs.get_metadata().items():\n    if key.endswith(\"_hooks\"):\n      if value != ():\n        return False\n    else:\n      return False\n  return True",
        "analysis": {
            "module_type": "variable_state_checker",
            "purpose": "Checks if a `VariableState` object is 'vanilla', meaning its metadata is essentially blank, containing only empty hook attributes.",
            "input": {
                "shape": "N/A",
                "dtype": "flax.nnx.variablelib.VariableState"
            },
            "processing_steps": [
                "Retrieve the metadata dictionary from the input `VariableState` object.",
                "Iterate through each key-value pair in the metadata.",
                "If a key ends with '_hooks', check if its value is a non-empty tuple. If it is, return False.",
                "If a key does not end with '_hooks', it is considered a non-built-in attribute, so return False immediately.",
                "If the loop completes without returning False, return True."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.variablelib.VariableState"
            ],
            "parameters": {
                "N/A": "This function does not take parameters from a configuration object."
            },
            "notes": [
                "A 'vanilla' variable state is defined as one where all metadata keys end in '_hooks' and their corresponding values are empty tuples.",
                "The function returns False as soon as it finds a non-empty hook or any metadata key that does not end in '_hooks'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_var(vs: variablelib.VariableState) -> meta.AxisMetadata:\n  metadata = vs.get_metadata()\n  if \"linen_meta_type\" in metadata:\n    linen_type = metadata[\"linen_meta_type\"]\n    if hasattr(linen_type, \"from_nnx_metadata\"):\n      return linen_type.from_nnx_metadata({\"value\": vs.value, **metadata})\n    return linen_type(vs.value, **metadata)\n  if is_vanilla_variable(vs):\n    return vs.value\n  return nnx.bridge.NNXMeta(vs.type, vs.value, metadata)",
        "analysis": {
            "functionality": "Converts an NNX `VariableState` object into a Linen-compatible variable representation. The output format depends on the metadata of the input variable state.",
            "usage": "Call this function with an `nnx.variablelib.VariableState` instance. It returns either the raw value (for simple variables), a reconstructed Linen `meta.AxisMetadata` object (if `linen_meta_type` is in metadata), or an `nnx.bridge.NNXMeta` object for other cases."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#get_col_name",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def get_col_name(keypath: tp.Sequence[Any]) -> str:\n  \"\"\"Given the keypath of a Flax variable type, return its Linen collection name.\"\"\"\n  # Infer variable type from the leaf's path, which contains its Linen collection name\n  assert isinstance(keypath[0], jax.tree_util.DictKey)\n  return str(keypath[0].key)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Extracts the Linen collection name from a JAX tree utility keypath.",
            "input": {
                "shape": "A sequence of path elements, where the first element is a `jax.tree_util.DictKey`.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that the first element of the input `keypath` is an instance of `jax.tree_util.DictKey`.",
                "Access the `key` attribute of the first element of the `keypath`.",
                "Convert the key to a string and return it."
            ],
            "output": {
                "shape": "A string representing the Linen collection name."
            },
            "dependencies": [
                "jax.tree_util.DictKey"
            ],
            "parameters": {
                "keypath": "A sequence representing the path to a leaf in a pytree, typically generated by `jax.tree_util.tree_map_with_path`."
            },
            "notes": [
                "This function infers the variable's collection name from the first element of its path in a nested dictionary structure, which is assumed to be the collection name itself."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_nnx_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_nnx_var(col: str, x: meta.AxisMetadata | Any) -> variablelib.Variable:\n  \"\"\"Convert a Linen variable to an NNX variable.\"\"\"\n  vtype = variablelib.variable_type_from_name(col, allow_register=True)\n  if isinstance(x, nnx.bridge.NNXMeta):\n    assert vtype == x.var_type, f\"Type stored in NNXMeta {x.var_type} != type inferred from collection name {vtype}\"\n    return x.to_nnx_variable()\n  if isinstance(x, meta.AxisMetadata):\n    x_metadata = vars(x)\n    if hasattr(x, \"to_nnx_metadata\"):\n      x_metadata = x.to_nnx_metadata()\n    assert hasattr(x, \"value\")\n    return vtype(**x_metadata, linen_meta_type=type(x))\n  return vtype(x)",
        "analysis": {
            "functionality": "Converts a Flax Linen variable, which may include metadata, into its corresponding Flax NNX Variable type based on a given collection name.",
            "usage": "Call this function with a collection name string (e.g., 'params') and a Linen variable value. The value can be a raw JAX array, a `flax.core.meta.AxisMetadata` instance, or an `nnx.bridge.NNXMeta` instance. The function returns a `flax.nnx.variablelib.Variable` object."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_recursive_merge",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _recursive_merge(dict1, dict2):\n  \"\"\"Recursively merge two dicts.\"\"\"\n  flat_map = nnx.traversals.flatten_mapping(dict1)\n  flat_map |= nnx.traversals.flatten_mapping(dict2)\n  return nnx.traversals.unflatten_mapping(flat_map)",
        "analysis": {
            "module_type": "dictionary_utility",
            "purpose": "Recursively merges two dictionaries by flattening them, combining them, and then unflattening the result.",
            "input": {
                "shape": "N/A",
                "dtype": "dict"
            },
            "processing_steps": [
                "Flatten the first input dictionary `dict1` using `nnx.traversals.flatten_mapping`.",
                "Flatten the second input dictionary `dict2` and merge it into the flattened representation of `dict1`.",
                "Unflatten the merged map back into a nested dictionary structure using `nnx.traversals.unflatten_mapping`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.traversals.flatten_mapping",
                "nnx.traversals.unflatten_mapping"
            ],
            "parameters": {
                "dict1": "The base dictionary.",
                "dict2": "The dictionary to merge into `dict1`. Its values will overwrite `dict1`'s values on key collision."
            },
            "notes": [
                "The function handles nested dictionaries by operating on their flattened representations.",
                "In case of key conflicts at any level of nesting, the value from `dict2` takes precedence."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_vars_to_nnx_attrs",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_vars_to_nnx_attrs(variables: tp.Mapping[str, Any]) -> dict[str, Any]:\n  \"\"\"Convert a dict of Linen-style variables to NNX variables.\"\"\"\n  nnx_vars = jax.tree_util.tree_map_with_path(\n      lambda kp, x: to_nnx_var(get_col_name(kp), x),\n      variables,\n      is_leaf=lambda x: not isinstance(x, dict),\n  )\n\n  flat_paths: dict[tuple, tp.Any] = {}\n\n  for col_name, col_variables in nnx_vars.items():  # pylint: disable=unused-variable\n    for path, variable in nnx.traversals.flatten_mapping(col_variables).items():\n      if path in flat_paths:\n        raise ValueError(\n            f\"Found duplicate variable path {path} with variables \"\n            f\"{flat_paths[path]} and {variable}. \"\n            \"This is not allowed in NNX.\"\n        )\n      flat_paths[path] = variable\n\n  nnx_vars = nnx.traversals.unflatten_mapping(flat_paths)\n  return nnx_vars",
        "analysis": {
            "module_type": "linen_to_nnx_variable_converter",
            "purpose": "Converts a dictionary of Flax Linen-style variables into a dictionary of Flax NNX-style attributes, ensuring no duplicate paths exist.",
            "input": {
                "shape": "A nested dictionary mapping collection names to variable trees, e.g., {'params': {'dense': {'kernel': ...}}}",
                "dtype": "Any"
            },
            "processing_steps": [
                "Apply `to_nnx_var` to each leaf of the input `variables` dictionary using `jax.tree_util.tree_map_with_path` to convert Linen variables to NNX variables.",
                "Initialize an empty dictionary `flat_paths` to store flattened variable paths.",
                "Iterate through each collection in the converted `nnx_vars`.",
                "For each collection, flatten its variable tree into path-variable pairs using `nnx.traversals.flatten_mapping`.",
                "Check for and raise a `ValueError` if any duplicate variable path is found across all collections.",
                "Store the unique path-variable pairs in `flat_paths`.",
                "Reconstruct a nested dictionary from the flattened paths using `nnx.traversals.unflatten_mapping`."
            ],
            "output": {
                "shape": "A nested dictionary representing NNX attributes, structured similarly to the input but with NNX Variable objects as leaves."
            },
            "dependencies": [
                "jax.tree_util.tree_map_with_path",
                "to_nnx_var",
                "get_col_name",
                "flax.nnx.traversals.flatten_mapping",
                "flax.nnx.traversals.unflatten_mapping"
            ],
            "parameters": {
                "variables": "A mapping (e.g., a dictionary) from collection names (like 'params') to the corresponding variable structures."
            },
            "notes": [
                "A key purpose of this function is to enforce the NNX constraint that all variable paths must be unique across all collections, raising a ValueError if a duplicate is found.",
                "The conversion logic relies on helper functions `to_nnx_var` and `get_col_name` to handle the per-variable transformation and collection name inference."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#nnx_attrs_to_linen_vars",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def nnx_attrs_to_linen_vars(nnx_attrs: dict) -> dict:\n  \"\"\"Convert a dict of NNX variables (or variable states) to Linen-style variables.\"\"\"\n  linen_structured = {}\n  for kp, v in nnx.traversals.flatten_mapping(nnx_attrs).items():\n    if isinstance(v, variablelib.Variable):\n      col_name = variablelib.variable_name_from_type(type(v))\n      v = to_linen_var(v.to_state())\n    elif isinstance(v, variablelib.VariableState):\n      col_name = variablelib.variable_name_from_type(v.type)\n      v = to_linen_var(v)\n    else:\n      raise ValueError(f\"Cannot infer collection name from value: {v}\")\n    linen_structured[(col_name, *kp)] = v\n  variables = nnx.traversals.unflatten_mapping(linen_structured)\n  return variables",
        "analysis": {
            "module_type": "nnx_to_linen_variable_converter",
            "purpose": "Converts a dictionary of Flax NNX variables or variable states into the nested dictionary format expected by Flax Linen.",
            "input": {
                "shape": "N/A",
                "dtype": "dict"
            },
            "processing_steps": [
                "Flatten the input dictionary of NNX attributes using `nnx.traversals.flatten_mapping`.",
                "Iterate through each key path and value in the flattened dictionary.",
                "For each value, determine its collection name based on its type (`nnx.Variable` or `nnx.VariableState`).",
                "Convert the NNX variable or state to a Linen-compatible value using the `to_linen_var` function.",
                "Construct a new key by prepending the collection name to the original key path.",
                "Store the new key and converted value in a temporary dictionary.",
                "Unflatten the temporary dictionary into a nested structure using `nnx.traversals.unflatten_mapping`.",
                "Return the final nested dictionary of Linen variables."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.traversals.flatten_mapping",
                "flax.nnx.traversals.unflatten_mapping",
                "flax.nnx.variablelib",
                "to_linen_var"
            ],
            "parameters": {
                "nnx_attrs": "A dictionary representing the attributes of an NNX module, where values are typically `nnx.Variable` or `nnx.VariableState` instances."
            },
            "notes": [
                "The function transforms the flat NNX attribute structure into a hierarchical Linen variable structure, where the top-level keys are collection names (e.g., 'params', 'batch_stats').",
                "It raises a ValueError if it encounters a value that is not an `nnx.Variable` or `nnx.VariableState`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_set_initializing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _set_initializing(module: Module, initializing: bool):\n  for _, value in graph.iter_graph(module):\n    if isinstance(value, Pytree):\n      value._object__state._initializing = initializing",
        "analysis": {
            "module_type": "graph_state_modifier",
            "purpose": "Recursively sets the `_initializing` flag on all `Pytree` objects within an NNX module's graph.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through all nodes in the graph of the input `module` using `graph.iter_graph`.",
                "For each node, check if it is an instance of `Pytree`.",
                "If it is a `Pytree`, set its internal `_object__state._initializing` attribute to the provided boolean value."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.graph.iter_graph",
                "flax.nnx.module.Module",
                "flax.nnx.Pytree"
            ],
            "parameters": {},
            "notes": [
                "This function modifies the module's state in-place.",
                "It directly accesses and modifies a protected attribute (`_object__state._initializing`), indicating it's an internal utility function.",
                "This function is used to switch the module between initialization mode and regular execution mode, as seen in the `lazy_init` function."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#lazy_init",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def lazy_init(fn: Module | tp.Callable[..., tp.Any], *args, **kwargs):\n  \"\"\"To run through an arbitrary nnx.Module method and initialize all its needed state.\n\n  Here used to trigger initialization of all `LinenToNNX` module variables.\"\"\"\n  if isinstance(fn, Module):\n    module = fn\n    assert callable(fn)\n  else:\n    if not (hasattr(fn, \"__self__\") and isinstance(fn.__self__, Module)):\n      raise ValueError(f\"{fn = } needs to be a method of an NNX Module.\")\n    module = fn.__self__\n  _set_initializing(module, True)\n  try:\n    _ = fn(*args, **kwargs)\n  finally:\n    _set_initializing(module, False)\n  return fn",
        "analysis": {
            "functionality": "A utility function that executes a given method of an `nnx.Module` to trigger the lazy initialization of its state. It sets an internal 'initializing' flag on the module, runs the method, and then resets the flag.",
            "usage": "Call `lazy_init` with an `nnx.Module` instance (or one of its methods) and the necessary sample input arguments required by that method. The function will execute the method to initialize the module's parameters and then return the original module/method. Example: `initialized_model = lazy_init(model, sample_input)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#current_linen_module",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def current_linen_module() -> linen.Module | None:\n  \"\"\"Get the current Linen module from the Linen context.\"\"\"\n  if linen.module._context.module_stack:  # pylint: disable=W0212\n    return linen.module._context.module_stack[-1]  # pylint: disable=W0212\n  return None",
        "analysis": {
            "module_type": "context_utility",
            "purpose": "Retrieves the currently active `linen.Module` instance from Flax Linen's internal context stack.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the internal `linen.module._context.module_stack` is non-empty.",
                "If the stack is non-empty, return the last element (the current module).",
                "If the stack is empty, return None."
            ],
            "output": {
                "shape": "Returns a `linen.Module` object or `None`."
            },
            "dependencies": [
                "flax.linen"
            ],
            "parameters": {},
            "notes": [
                "This function accesses a protected member (`_context.module_stack`) of the `flax.linen` library to get the current module context.",
                "It returns `None` if called outside the scope of a Linen module's execution (e.g., outside `setup` or `__call__`)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToNNX",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToNNX(Module):\n  \"\"\"A wrapper to turn any Linen module into an NNX module.\n\n  The result NNX module can be used standalone with all NNX APIs, or as a submodule of\n  another NNX module.\n\n  Since Linen module initialization requires a sample input, you need to call `lazy_init`\n  with an argument to initialize the variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> linen_module = nn.Dense(features=64)\n    >>> x = jax.numpy.ones((1, 32))\n    >>> # Like Linen init(), initialize with a sample input\n    >>> model = nnx.bridge.ToNNX(linen_module, rngs=nnx.Rngs(0)).lazy_init(x)\n    >>> # Like Linen apply(), but using NNX's direct call method\n    >>> y = model(x)\n    >>> model.kernel.shape\n    (32, 64)\n\n  Args:\n    module: The Linen Module instance.\n    rngs: The `nnx.Rngs` instance being passed to any NNX module.\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  def __init__(\n      self,\n      module: linen.Module,\n      rngs: Rngs | jax.Array | None = None,\n  ):\n    self.to_nnx__module = module\n\n    self.to_nnx__rngs: Rngs | None\n    if isinstance(rngs, jax.Array):\n      self.to_nnx__rngs = Rngs(params=rngs)\n    elif isinstance(rngs, nnx.Rngs):\n      self.to_nnx__rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else nnx.clone(rngs)  # type: ignore\n    else:\n      self.to_nnx__rngs = rngs\n\n  def lazy_init(self, *args, **kwargs):\n    \"\"\"A shortcut of calling `nnx.bridge.lazy_init()` upon this module.\"\"\"\n    return lazy_init(self, *args, **kwargs)\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    maybe_method = getattr(type(self.to_nnx__module), name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def __call__(\n      self,\n      *args: Any,\n      rngs: Rngs | jax.Array | None = None,\n      method: tp.Callable[..., Any] | str | None = None,\n      mutable: tp.Any = None,\n      **kwargs: Any,\n  ) -> Any:\n    # Shape-based lazy init of the flax variables\n    if rngs is None:\n      rngs = self.to_nnx__rngs\n    if isinstance(rngs, nnx.Rngs):\n      _rngs = {name: stream() for name, stream in rngs.items()}\n    elif isinstance(rngs, jax.Array):\n      _rngs = {\"params\": rngs}\n    else:\n      _rngs = {}\n    # rename default to params\n    if \"params\" not in _rngs and \"default\" in _rngs:\n      _rngs[\"params\"] = _rngs.pop(\"default\")\n    if self._object__state.initializing:\n      out, updates = self.to_nnx__module.init_with_output(_rngs, *args, method=method, **kwargs)\n    else:\n      nnx_attrs = {\n          k: v\n          for k, v in vars(self).items()\n          if not k.startswith(\"to_nnx__\") and not k.startswith(\"_pytree__\") and not k.startswith(\"_object__\")\n      }\n      variables = nnx_attrs_to_linen_vars(nnx_attrs)\n\n      # Get `mutable` from top level bridge.Module context if any\n      if mutable is not None:\n        pass\n      elif (m := bdg_module.current_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      elif (m := current_linen_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      else:\n        mutable = False\n\n      out = self.to_nnx__module.apply(variables, *args, rngs=_rngs, method=method, mutable=mutable, **kwargs)\n\n      # Split out the updates if `mutable` is passed into the Flax module\n      if mutable is not False:\n        out, updates = out\n      else:\n        updates = None\n\n    # Split out the updates if `mutable` is passed into the Flax module\n    if updates:\n      nnx_attrs = linen_vars_to_nnx_attrs(updates)\n      # nnx.update(self, nnx_attrs)\n      # TODO(cgarciae): ideally we just do an update but currently dictionaries don't allow\n      # insertion of new keys, we need to enable this in NNX to simplify the code below\n      # to the simple nnx.update(self, nnx_attrs) above.\n      for attr_name, value in nnx_attrs.items():\n        if hasattr(self, attr_name) and isinstance(value, dict):\n          original_value = getattr(self, attr_name)\n          new_values = _recursive_merge(original_value, value)\n          setattr(self, attr_name, nnx.data(new_values))\n        else:\n          setattr(self, attr_name, nnx.data(value))\n\n    return out",
        "analysis": {
            "functionality": "The `ToNNX` class is a wrapper that converts a Flax Linen module into an NNX module. This allows a traditional, stateless Linen module to be used as a stateful component within the NNX framework, making it compatible with NNX APIs and other NNX modules.",
            "usage": "Instantiate the class with a Flax Linen module instance and an optional `nnx.Rngs` object. Then, call the `lazy_init()` method with a sample input tensor to initialize the module's parameters. After initialization, the instance can be called like a regular function to perform a forward pass. Example: `model = nnx.bridge.ToNNX(linen_module, rngs=nnx.Rngs(0)).lazy_init(sample_input)` followed by `output = model(input_data)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_rngs_dict",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_rngs_dict(linen_module: linen.Module, add_default: bool = False):\n  \"\"\"Given a module, split out one of its every active RNG key collections.\"\"\"\n  assert linen_module.scope is not None, \"linen_rngs_dict() must be called inside a Linen module.\"\n  rngs: dict[str, tp.Any] = {name: linen_module.make_rng(name) for name in linen_module.scope.rngs.keys()}\n  if add_default and \"default\" not in rngs:\n    rngs[\"default\"] = 0\n  return rngs",
        "analysis": {
            "module_type": "rng_dictionary_generator",
            "purpose": "Extracts all active RNG streams from a Flax Linen module's scope into a dictionary, optionally adding a 'default' key.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that the function is called within a Linen module's scope.",
                "Iterate over the active RNG stream names in the module's scope.",
                "For each stream name, call `linen_module.make_rng()` to generate a new JAX PRNG key.",
                "Collect the stream names and their corresponding keys into a dictionary.",
                "If `add_default` is True and a 'default' key does not already exist, add it with a value of 0.",
                "Return the dictionary of RNGs."
            ],
            "output": {
                "shape": "A dictionary mapping RNG stream names (str) to JAX PRNG keys (jax.Array) or an integer."
            },
            "dependencies": [
                "flax.linen.Module"
            ],
            "parameters": {
                "linen_module": "The Flax Linen module instance from which to extract RNGs. Must be called within its scope.",
                "add_default": "A boolean flag. If True, ensures a 'default' key exists in the output dictionary, adding it with a value of 0 if not present."
            },
            "notes": [
                "This function must be called from within a method of a `linen.Module` (e.g., `__call__` or `setup`) where the module's scope is active.",
                "The 'default' RNG, if added, is assigned the integer value 0, not a JAX PRNG key."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_get_module_method",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _get_module_method(module, method: tp.Callable[..., Any] | str | None):\n  \"\"\"Get a callable method from the module, or raise TypeError.\"\"\"\n  if method is None:\n    method = \"__call__\"\n\n  if isinstance(method, str):\n    attribute_name = method\n    method = getattr(type(module), attribute_name)\n    if not callable(method):\n      class_name = type(module).__name__\n      raise TypeError(f\"'{class_name}.{attribute_name}' must be a callable, got {type(method)}.\")\n  if not callable(method):\n    class_name = type(module).__name__\n    raise TypeError(f\"'{method}' must be a callable, got {type(method)}.\")\n\n  return method",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Retrieves a callable method from a module object, handling cases where the method is specified by name (string) or is None (defaults to '__call__'), and raises a TypeError if the result is not callable.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "If the 'method' argument is None, default it to the string '__call__'.",
                "If 'method' is a string, retrieve the attribute with that name from the module's class.",
                "Check if the retrieved attribute is callable, raising a TypeError if not.",
                "If 'method' was not a string, check if it is callable, raising a TypeError if not.",
                "Return the validated callable method."
            ],
            "output": {
                "shape": "A Python callable object."
            },
            "dependencies": [],
            "parameters": {
                "module": "The module instance from which to retrieve the method.",
                "method": "The method to retrieve, which can be a callable, a string name, or None."
            },
            "notes": [
                "This function is a helper to ensure that a valid, callable method is obtained before it's used, providing clear error messages for invalid inputs.",
                "When a string is provided for 'method', the lookup is performed on the module's class (`type(module)`) rather than its instance."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_fix_for_qwix_quantization",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _fix_for_qwix_quantization(module: Module):\n  \"\"\"Process the nnx module to make it compatible with QWIX quantization.\n\n  Normally Qwix only works with pure Linen modules or pure NNX modules. When\n  NNX modules are called inside Linen modules, Qwix will have issues to\n    * detect the correct module path when a Jax op (e.g. einsum) is called.\n    * detect the input types (whether it's a weight) of the Jax op.\n\n  This function will fix those issues.\n\n  Args:\n    module: The NNX module to be processed.\n  \"\"\"\n\n  # Wrap the __call__ function of the nnx modules to make sure the linen module\n  # path is updated correctly.\n  def wrap(call_fn, name: str):\n    def wrapped(*args, **kwargs):\n      if not linen.module._context.module_stack:  # pylint: disable=W0212\n        return call_fn(*args, **kwargs)\n      nn_module = linen.module._context.module_stack[-1]  # pylint: disable=W0212\n      old_path = nn_module.path\n      # We modify the path of the current nn module in place. This is a little\n      # bit hacky but should be good as a temporary solution.\n      nn_module.scope.path += (name,)\n      try:\n        return call_fn(*args, **kwargs)\n      finally:\n        nn_module.scope.path = old_path\n\n    return wrapped\n\n  for path, node in nnx.iter_graph(module):\n    # Only enable it on non-root nnx modules.\n    if path and isinstance(node, nnx.Module):\n      node.__class__ = type(\n          node.__class__.__name__,\n          (node.__class__,),\n          {\n              \"__call__\": wrap(node.__class__.__call__, str(path[-1])),\n          },\n      )\n\n  # Set the correct weight names. We call QtProvider.process_model_inputs here\n  # to avoid using Qwix internal APIs.\n  qwix.QtProvider.process_model_inputs(None, module, None, None)",
        "analysis": {
            "module_type": "qwix_quantization_fixer",
            "purpose": "Processes an NNX module in-place to make it compatible with QWIX quantization, particularly when the NNX module is used within a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "flax.nnx.Module"
            },
            "processing_steps": [
                "Define a `wrap` function that temporarily modifies the current Linen module's scope path to include the NNX submodule's name during its `__call__` execution.",
                "Iterate through the graph of the input NNX module using `nnx.iter_graph`.",
                "For each non-root submodule, dynamically create a new class that inherits from the submodule's original class.",
                "Override the `__call__` method of this new class with the `wrap` function.",
                "Replace the submodule's class with the newly created, patched class.",
                "Call `qwix.QtProvider.process_model_inputs` on the modified module to set correct weight names for quantization."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "flax.linen",
                "qwix.QtProvider"
            ],
            "parameters": {
                "module": "The NNX module to be processed."
            },
            "notes": [
                "This function modifies the input `module` in-place and returns None.",
                "It is designed to fix issues where Qwix cannot detect the correct module path or input types for Jax ops when an NNX module is called from within a Linen module.",
                "The implementation uses monkey-patching to wrap the `__call__` method of each submodule.",
                "The docstring mentions that modifying the Linen module's path directly is a 'hacky but temporary solution'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToLinen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToLinen(linen.Module):\n  \"\"\"A wrapper to turn any NNX module into a Linen module.\n\n  The result Linen module can be used standalone with all Linen APIs, or as a\n  submodule of\n  another Linen module.\n\n  Since NNX modules are stateful and owns the state, we only create it once\n  during init\n  time, and will track its state and static data as separate variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> model = nnx.bridge.ToLinen(nnx.Linear, args=(32, 64))\n    >>> x = jax.numpy.ones((1, 32))\n    >>> y, variables = model.init_with_output(jax.random.key(0), x)\n    >>> y.shape\n    (1, 64)\n    >>> variables['params']['kernel'].shape\n    (32, 64)\n    >>> # The static GraphDef of the underlying NNX module\n    >>> variables.keys()\n    dict_keys(['params'])\n\n  Args:\n    nnx_class: The NNX Module class (not instance!).\n    args: The arguments that normally would be passed in to create the NNX\n      module.\n    kwargs: The keyword arguments that normally would be passed in to create the\n      NNX module.\n    skip_rng: True if this NNX module doesn't need `rngs` arg during\n      initialization (not common).\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  nnx_class: tp.Callable[..., Module]\n  args: tp.Sequence = ()\n  kwargs: tp.Mapping[str, tp.Any] = FrozenDict({})\n  skip_rng: bool = False\n  metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var\n\n  @linen.compact\n  def __call__(self, *args, nnx_method: tp.Callable[..., Any] | str | None = None, **kwargs):\n    def _module_kwargs():\n      maybe_add_default = not self.is_initializing()\n      module_kwargs = dict(self.kwargs)\n      if not self.skip_rng:\n        module_kwargs[\"rngs\"] = nnx.Rngs(**linen_rngs_dict(self, add_default=maybe_add_default))\n      return module_kwargs\n\n    # init codepath\n    if self.is_initializing():\n      module = self.nnx_class(*self.args, **_module_kwargs())\n      # TODO: add lazy_init here in case there's an `ToNNX` submodule under `module`.\n      # update linen variables before call module to save initial state\n      self._update_variables(module)\n      _fix_for_qwix_quantization(module)\n      method_fn = _get_module_method(module, nnx_method)\n      out = method_fn(module, *args, **kwargs)\n      return out\n\n    # create the nnx module\n    module = self.nnx_class(*self.args, **_module_kwargs())\n\n    # update nnx module from linen variables\n    def maybe_unbox(x):\n      if isinstance(x, meta.AxisMetadata):\n        return x.unbox()\n      return x\n\n    states = jtu.tree_map(\n        maybe_unbox,\n        list(core.unfreeze(self.variables).values()),  # type: ignore[wrong-arg-types]\n        is_leaf=lambda x: isinstance(x, meta.AxisMetadata),\n    )\n    if not states:\n      states = ({},)\n\n    new_state = nnx.merge_state(*states)\n    new_state_flat = nnx.traversals.flatten_mapping(new_state)\n    current_state_flat = nnx.traversals.flatten_mapping(nnx.state(module))\n    unknown_state_flat = {path: v for path, v in new_state_flat.items() if path not in current_state_flat}\n\n    if unknown_state_flat:\n      paths_str = \"\"\n      for path, _ in unknown_state_flat.items():\n        paths_str += f\"\\n  - {'/'.join(map(str, path))}\"\n\n      warnings.warn(f\"Found unknown module paths in incoming state:{paths_str}\")\n\n    nnx.update(module, new_state)\n\n    _fix_for_qwix_quantization(module)\n    method_fn = _get_module_method(module, nnx_method)\n    out = method_fn(module, *args, **kwargs)\n    self._update_variables(module)\n    return out\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    if name in self.kwargs:\n      return self.kwargs[name]\n    maybe_method = getattr(self.nnx_class, name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, nnx_method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def _update_variables(self, module):\n    \"\"\"Store the NNX module's graph def and state inside Linen module variables.\"\"\"\n    state = nnx.state(module, nnx.Not(nnx.RngState))\n\n    collection_flat_state: dict[str, list[tuple[tuple[str, ...], tp.Any]]] = {}\n\n    # group state by collection\n    for path, leaf in nnx.to_flat_state(state):\n      type_ = leaf.type if isinstance(leaf, nnx.VariableState) else type(leaf)\n      collection = variablelib.variable_name_from_type(type_, allow_register=True)\n      if collection not in collection_flat_state:\n        collection_flat_state[collection] = []\n      collection_flat_state[collection].append((path, leaf))\n\n    # update linen variables\n    for collection, flat_state in collection_flat_state.items():\n      if self.is_mutable_collection(collection):\n\n        def _to_linen_var(x):\n          if isinstance(x, nnx.VariableState):\n            if self.metadata_fn is not None:\n              return self.metadata_fn(x)  # pylint: disable=too-many-function-args\n            else:\n              return x.value\n          return x\n\n        collection_state = nnx.traversals.unflatten_mapping(flat_state)\n        collection_state = jax.tree.map(\n            _to_linen_var,\n            collection_state,\n            is_leaf=lambda x: isinstance(x, nnx.VariableState),\n        )\n        for k, v in collection_state.items():\n          self.put_variable(collection, k, v)",
        "analysis": {
            "module_type": "nnx_to_linen_wrapper",
            "purpose": "A wrapper to turn any NNX module into a Linen module, making it compatible with the Linen API.",
            "input": {
                "shape": "Depends on the wrapped NNX module's method.",
                "dtype": "Depends on the wrapped NNX module's method."
            },
            "processing_steps": [
                "Initializes an instance of the specified `nnx_class`.",
                "During the first call (initialization), it captures the state of the NNX module and stores it as Linen variables.",
                "On subsequent calls, it loads the state from Linen variables, updates the NNX module instance, executes the forward pass, and then updates the Linen variables with the new state.",
                "Uses `__getattr__` to delegate method calls to the wrapped NNX module."
            ],
            "output": {
                "shape": "Depends on the output of the wrapped NNX module's method."
            },
            "dependencies": [
                "flax.linen.Module",
                "flax.nnx.Module",
                "flax.nnx.Rngs",
                "flax.nnx.merge_state",
                "flax.nnx.update",
                "flax.nnx.state",
                "_get_module_method",
                "_fix_for_qwix_quantization",
                "linen_rngs_dict",
                "to_linen_var"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class to be wrapped.",
                "args": "Positional arguments to be passed to the nnx_class constructor.",
                "kwargs": "Keyword arguments to be passed to the nnx_class constructor.",
                "skip_rng": "If True, 'rngs' will not be passed to the nnx_class constructor during initialization.",
                "metadata_fn": "A function to convert an nnx.VariableState to a Linen-compatible variable."
            },
            "notes": [
                "The wrapper manages the state of the underlying NNX module by storing it in Linen variables.",
                "It handles two main execution paths: one for initialization (`is_initializing()`) and one for subsequent calls.",
                "The `__getattr__` method is overridden to allow calling methods of the wrapped NNX module directly on the wrapper instance."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the wrapped NNX module, handling both initialization and application logic.",
                    "input": {
                        "shape": "Depends on the wrapped NNX module's method.",
                        "dtype": "Depends on the wrapped NNX module's method."
                    },
                    "processing_steps": [
                        "Determine if the module is being initialized using `self.is_initializing()`.",
                        "If initializing: instantiate the NNX module, update Linen variables with its state via `_update_variables`, apply QWIX fix, get the target method, and execute it.",
                        "If not initializing: instantiate the NNX module, load state from Linen variables, update the module with this state via `nnx.update`, apply QWIX fix, get the target method, execute it, and update Linen variables with the new state via `_update_variables`.",
                        "Return the output of the method call."
                    ],
                    "output": {
                        "shape": "Depends on the output of the wrapped NNX module's method."
                    },
                    "dependencies": [
                        "linen_rngs_dict",
                        "_get_module_method",
                        "_fix_for_qwix_quantization",
                        "nnx.Rngs",
                        "nnx.merge_state",
                        "nnx.update",
                        "self._update_variables"
                    ],
                    "notes": [
                        "The method has distinct logic for the initialization phase versus subsequent calls.",
                        "It accepts an `nnx_method` argument to specify which method of the wrapped NNX module to call."
                    ]
                },
                "__getattr__": {
                    "purpose": "Provides access to the attributes and methods of the wrapped NNX module class.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the attribute exists on the parent class or in `self.kwargs`.",
                        "Attempt to get the attribute from the wrapped `self.nnx_class`.",
                        "If the attribute is a callable method, return a partial function that calls `self.__call__` with the method pre-filled.",
                        "Otherwise, fall back to the default `__getattribute__`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "This enables a more seamless API, allowing calls like `wrapper.some_nnx_method()` instead of `wrapper(nnx_method='some_nnx_method')`."
                    ]
                },
                "_update_variables": {
                    "purpose": "Extracts the state from the wrapped NNX module and stores it as variables within the Linen module's scope.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the state of the NNX module using `nnx.state`.",
                        "Flatten the state and group it by variable collection name.",
                        "Iterate through each mutable collection.",
                        "Convert the NNX variable states to Linen-compatible variables using `self.metadata_fn`.",
                        "Store the converted variables in the Linen module's state using `self.put_variable`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.state",
                        "nnx.to_flat_state",
                        "flax.nnx.variablelib.variable_name_from_type",
                        "nnx.traversals.unflatten_mapping",
                        "self.put_variable",
                        "self.is_mutable_collection"
                    ],
                    "notes": [
                        "This method is the core mechanism for synchronizing the state between the NNX object and the Linen wrapper."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_Missing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class _Missing:\n  ...",
        "analysis": {
            "functionality": "The `_Missing` class is a simple, empty class used to create a unique sentinel object. This pattern is employed to have a default value for function arguments that is distinct from `None`, allowing the function to differentiate between a user explicitly passing `None` and not providing the argument at all.",
            "usage": "Instantiate the class to create a sentinel value, e.g., `_MISSING = _Missing()`. This instance can then be used as a default value for an optional function parameter. Inside the function, you can check if the parameter's value `is _MISSING` to determine if the caller omitted it."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen(\n    nnx_class: tp.Callable[..., Module],\n    *args,\n    metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    name: str | None = None,\n    skip_rng: bool = False,\n    abstract_init: bool = True,\n    **kwargs,\n):\n  \"\"\"Shortcut of `nnx.bridge.ToLinen` if user is not changing any of its default fields.\"\"\"\n  return ToLinen(\n      nnx_class,\n      args=args,\n      kwargs=FrozenDict(kwargs),\n      metadata_fn=metadata_fn,\n      skip_rng=skip_rng,\n      name=name,\n  )",
        "analysis": {
            "module_type": "linen_wrapper_function",
            "purpose": "Provides a simplified way to create an instance of the `ToLinen` wrapper for an NNX module, acting as a shortcut for its constructor.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Accepts an NNX module class (`nnx_class`) and its constructor arguments (`*args`, `**kwargs`).",
                "Wraps the provided `kwargs` dictionary in a `flax.core.FrozenDict`.",
                "Instantiates the `ToLinen` class, passing along the `nnx_class`, its arguments, and other configuration options like `metadata_fn`, `skip_rng`, and `name`.",
                "Returns the newly created `ToLinen` module instance."
            ],
            "output": {
                "shape": "An instance of the `ToLinen` class, which is a `linen.Module`."
            },
            "dependencies": [
                "ToLinen",
                "flax.core.FrozenDict",
                "flax.nnx.module.Module",
                "flax.nnx.variablelib"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class to be wrapped into a Linen module.",
                "args": "Positional arguments to be passed to the `nnx_class` constructor.",
                "kwargs": "Keyword arguments to be passed to the `nnx_class` constructor.",
                "metadata_fn": "An optional function to handle the conversion of NNX VariableState metadata to a Linen-compatible format.",
                "skip_rng": "A boolean indicating whether to skip passing RNGs to the NNX module during its initialization.",
                "name": "An optional name for the resulting Linen module.",
                "abstract_init": "A boolean parameter that is accepted by the function but not passed to the `ToLinen` constructor, likely for API compatibility."
            },
            "notes": [
                "This function serves as a convenience shortcut for instantiating `nnx.bridge.ToLinen` when the user does not need to change its default fields."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_class",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_class(\n    base_nnx_class: type[M],\n    base_metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    base_skip_rng: bool = False,\n    **partial_kwargs: tp.Any,\n) -> type[ToLinen]:\n  \"\"\"A dynamically created Linen Module that wraps a specific NNX Module.\n\n  This class is not meant to be used directly. Instead, it is created and\n  returned by the `to_linen_class` function. It acts as a \"partially applied\"\n  version of the `ToLinen` wrapper, where the NNX module to be wrapped and\n  its default arguments are pre-configured.\n\n  When you instantiate this class, it behaves like a standard Linen module.\n  The arguments you provide during instantiation can override the defaults\n  that were set when this class was created by `to_linen_class`.\n\n  For example:\n    >>> from flax import linen as nn, nnx\n    >>> from MaxText.layers import linears\n    >>> # Create a specialized Linen wrapper for linears.DenseGeneral\n    >>> LinenDenseGeneral = to_linen_class(linears.DenseGeneral)\n    >>> # Now, LinenDenseGeneral can be used like a regular Linen module\n    >>> class MyModel(nn.Module):\n    ...   def setup(self):\n    ...     # Instantiate the wrapped linears.DenseGeneral with its arguments\n    ...     self.dense = LinenDenseGeneral(\n    ...         in_features_shape=10, out_features_shape=5\n    ...     )\n    ...   def __call__(self, x):\n    ...     return self.dense(x)\n\n  Attributes:\n    (The attributes are dynamically set by the `ToLinen` parent class based\n      on the arguments provided during instantiation.)\n  \"\"\"\n\n  def __init__(\n      self,\n      args=None,\n      kwargs=None,\n      nnx_class=None,\n      skip_rng=None,\n      metadata_fn=None,\n      name=_MISSING,\n      parent=_MISSING,\n      **other_kwargs,\n  ):\n    linen_kwargs = {}\n    if not isinstance(parent, _Missing):\n      linen_kwargs[\"parent\"] = parent\n    if not isinstance(name, _Missing):\n      linen_kwargs[\"name\"] = name\n    ToLinen.__init__(\n        self,\n        nnx_class=nnx_class or base_nnx_class,\n        args=args or (),\n        metadata_fn=metadata_fn or base_metadata_fn,\n        skip_rng=skip_rng or base_skip_rng,\n        kwargs=FrozenDict({**partial_kwargs, **(kwargs or {}), **other_kwargs}),\n        **linen_kwargs,\n    )\n\n  class ToLinenPartial(ToLinen):\n    \"\"\"A dynamically created Linen Module that wraps a specific NNX Module.\"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n      super().__init_subclass__(**kwargs)\n      cls.__init__ = __init__\n\n  ToLinenPartial.__init__ = __init__\n\n  return ToLinenPartial",
        "analysis": {
            "module_type": "nnx_to_linen_class_factory",
            "purpose": "A factory function that dynamically creates a Flax Linen Module class which is a specialized wrapper for a specific NNX Module class, effectively creating a 'partially applied' version of the `ToLinen` wrapper.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines a custom `__init__` method that merges pre-configured arguments (`base_nnx_class`, `base_metadata_fn`, `base_skip_rng`, `partial_kwargs`) with arguments provided at instantiation time.",
                "The custom `__init__` calls the parent `ToLinen.__init__` with the final combined configuration.",
                "Defines a new class `ToLinenPartial` that inherits from `ToLinen`.",
                "Replaces the `__init__` method of `ToLinenPartial` with the custom `__init__`.",
                "Returns the `ToLinenPartial` class object."
            ],
            "output": {
                "shape": "Returns a class object (type) that is a subclass of `ToLinen`."
            },
            "dependencies": [
                "ToLinen",
                "flax.core.FrozenDict",
                "flax.nnx.variablelib.VariableState",
                "to_linen_var"
            ],
            "parameters": {
                "base_nnx_class": "The NNX Module class that the returned Linen class will wrap.",
                "base_metadata_fn": "The default function for converting NNX variable state to a Linen variable format.",
                "base_skip_rng": "The default boolean value indicating if the wrapped NNX module's initialization needs an RNG stream.",
                "partial_kwargs": "Default keyword arguments to be passed to the `base_nnx_class` constructor when the returned Linen class is instantiated."
            },
            "notes": [
                "This function acts as a class factory, similar to `functools.partial` but for creating a class.",
                "The returned class is intended to be used like a standard Linen module, for example, by instantiating it inside another Linen module's `setup` method.",
                "Arguments provided during the instantiation of the returned class can override the default arguments set by this factory function."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#RMSNorm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "class RMSNorm(nnx.Module):\n  \"\"\"RMS normalization.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.ones,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.num_features = num_features\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n    self.scale = nnx.Param(\n        scale_init(rngs.params(), (num_features,), weight_dtype),\n        sharding=kernel_axes,\n    )\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    x = jnp.asarray(x, jnp.float32)\n    mean2 = jnp.mean(lax.square(x), axis=-1, keepdims=True)\n    y = jnp.asarray(x * lax.rsqrt(mean2 + self.epsilon), self.dtype)\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"normalizations.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    return y * scale",
        "analysis": {
            "module_type": "rms_normalization",
            "purpose": "Applies Root Mean Square (RMS) normalization to an input tensor, a variant of layer normalization.",
            "input": {
                "shape": "[..., num_features]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Calculate the mean of the squares of the input tensor along the last dimension.",
                "Normalize the input tensor using the calculated mean square and an epsilon value.",
                "Optionally move the learnable scale parameter from host to device.",
                "Scale the normalized tensor by a learnable weight parameter."
            ],
            "output": {
                "shape": "[..., num_features]"
            },
            "dependencies": [
                "flax.nnx",
                "jax.numpy",
                "jax.lax",
                "MaxText.max_utils",
                "MaxText.max_logging"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, typically the size of the last dimension.",
                "epsilon": "A small float added to the denominator for numerical stability.",
                "dtype": "The data type of the output tensor.",
                "weight_dtype": "The data type of the learnable scale parameter.",
                "parameter_memory_host_offload": "A boolean indicating whether to offload the scale parameter to host memory and move it to the device only during the forward pass."
            },
            "notes": [
                "This is a Flax NNX module.",
                "The normalization is performed over the last axis of the input tensor.",
                "The learnable scaling parameter `scale` is initialized to ones by default."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RMSNorm layer and its learnable scale parameter.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like num_features, epsilon, and dtype.",
                        "Initialize the 'scale' parameter as an nnx.Param of shape [num_features] using the provided initializer and sharding axes."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx",
                        "flax.linen.initializers"
                    ],
                    "notes": [
                        "Requires an `nnx.Rngs` object for parameter initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the RMS normalization transformation to the input tensor.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Cast the input tensor `x` to float32.",
                        "Compute the mean of the square of `x` along the last axis.",
                        "Normalize `x` by multiplying it with the reciprocal square root of (mean_square + epsilon).",
                        "Cast the normalized tensor to the module's configured `dtype`.",
                        "Retrieve the learnable `scale` parameter.",
                        "If `parameter_memory_host_offload` is true, explicitly move the `scale` parameter to the device.",
                        "Multiply the normalized tensor by the `scale` parameter."
                    ],
                    "output": {
                        "shape": "[..., num_features]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.lax",
                        "MaxText.max_utils",
                        "MaxText.max_logging"
                    ],
                    "notes": [
                        "The input is internally cast to float32 for the normalization calculation, while the final output is cast to the `dtype` specified during initialization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#rms_norm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def rms_norm(\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.ones,\n    name: None | str = None,\n    parameter_memory_host_offload: bool = False,\n):\n  \"\"\"Creates a RMSNorm module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      RMSNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "rms_norm_factory",
            "purpose": "A factory function that creates and returns a Flax Linen-compatible RMSNorm module by wrapping the `RMSNorm` nnx.Module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `RMSNorm` class.",
                "Passes configuration arguments (num_features, epsilon, etc.) to the `RMSNorm` constructor via the wrapper.",
                "Sets `variable_to_logically_partitioned` as the `metadata_fn` for parameter partitioning.",
                "Returns the created Linen-compatible module instance."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RMSNorm",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor, which determines the size of the learnable scale parameter.",
                "epsilon": "A small float added to the variance to avoid division by zero during normalization.",
                "dtype": "The data type for the computation and output of the created module.",
                "weight_dtype": "The data type for the learnable scale parameter.",
                "kernel_axes": "A tuple specifying the sharding for the learnable scale parameter.",
                "parameter_memory_host_offload": "A boolean indicating whether to offload the scale parameter to host memory."
            },
            "notes": [
                "This function does not perform any computation itself; it instantiates and configures a module.",
                "The returned object is a Flax Linen module, making the underlying `nnx.Module` compatible with the Linen API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#l2norm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def l2norm(x: Array, dim: int = -1, eps: float = 1e-6) -> Array:\n  \"\"\"L2 normalization function. Normalizes a vector to have a length of 1.\n\n  Args:\n    x: Input array.\n    dim: The axis or axes along which to normalize. Defaults to the last axis.\n    eps: Small epsilon to prevent division by zero.\n\n  Returns:\n    L2 normalized array with the same shape as x.\n  \"\"\"\n\n  inv_norm = jax.lax.rsqrt((x * x).sum(axis=dim, keepdims=True) + jnp.array(eps, dtype=x.dtype))\n  return x * inv_norm",
        "analysis": {
            "module_type": "l2_normalization",
            "purpose": "Performs L2 normalization on an input array along a specified dimension, scaling it to a unit length.",
            "input": {
                "shape": "[...]",
                "dtype": "Any numeric JAX type"
            },
            "processing_steps": [
                "Calculate the sum of squares of the input array `x` along the specified `dim`, keeping the dimensions.",
                "Add a small epsilon `eps` to the sum of squares for numerical stability.",
                "Compute the inverse square root of the result to get the inverse L2 norm.",
                "Multiply the original input array `x` by the inverse L2 norm."
            ],
            "output": {
                "shape": "Same shape as the input array `x`."
            },
            "dependencies": [
                "jax.lax.rsqrt",
                "jax.numpy.array"
            ],
            "parameters": {
                "dim": "The axis or axes along which to normalize. Defaults to the last axis (-1).",
                "eps": "A small float value added to the norm before taking the inverse square root to prevent division by zero."
            },
            "notes": [
                "The function effectively divides the input array by its L2 norm along the specified dimension.",
                "The `keepdims=True` argument in the sum operation is crucial for enabling correct broadcasting during the final multiplication."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/pipeline.py#Pipeline",
        "file_path": "src/MaxText/layers/pipeline.py",
        "code_block": "class Pipeline(nn.Module):\n  \"\"\"Module that implements pipelining across stages.\n\n  This module will loop over microbatches and execute the main body with a vmap for both the inputs and weights.\n  This will produce a pipeline pattern if the stage dimension is sharded.\n\n  Supports circular pipelines, and multiple layers per stage are used when a module that executes multiple layers\n  is passed as the layers input.\n\n  Attributes:\n    config: Importantly contains num_pipeline_microbatches, num_pipeline_repeats.\n    layers: A module instance that each stage can execute. It can either be a single layer such as a\n      LlamaDecoderLayer instance or scanned/looped set of decoder layers to execute multiple layers per stage.\n    mesh:  The device mesh of the system.\n    remat_policy: Remat policy to use for the loop iterations\n  \"\"\"\n\n  config: Config\n  layers: nn.Module  # The name of this property (layers) is reflected in the state pytree and thus also checkpoints.\n  mesh: Mesh\n  remat_policy: Any = None\n\n  def setup(self):  # pylint: disable=missing-function-docstring\n    self.num_stages = self.config.ici_pipeline_parallelism * self.config.dcn_pipeline_parallelism\n    self.forwarding_delay = 2 if self.config.pipeline_delay_activation_forwarding else 1\n    self.pipeline_microbatch_size = self.config.micro_batch_size_to_train_on // self.config.num_pipeline_microbatches\n    microbatches_per_stage = self.config.num_pipeline_microbatches // self.num_stages\n    self.microbatches_per_stage = microbatches_per_stage\n    self.use_circ_storage = self.need_circ_storage()\n\n    if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      self.batch_axis_name = \"activation_batch_no_exp\"\n      self.seq_len_axis_name = \"activation_length\"\n    else:\n      self.batch_axis_name = \"activation_batch\"\n      self.seq_len_axis_name = \"activation_length_no_exp\"\n\n  def need_circ_storage(self):\n    return (\n        self.config.num_pipeline_repeats > 1\n        and self.config.num_pipeline_microbatches > self.num_stages * self.forwarding_delay\n    )\n\n  def iterations_to_complete_first_microbatch_one_repeat(self):\n    # Return the number of iterations it takes for microbatch 0 to finish a repeat\n    return self.forwarding_delay * (self.num_stages - 1)\n\n  def iterations_to_complete_first_microbatch(self):\n    # Return the number of iterations it takes for microbatch 0 to finish the last stage of the last repeat\n    return (\n        self.config.num_pipeline_microbatches * (self.config.num_pipeline_repeats - 1)\n        + self.iterations_to_complete_first_microbatch_one_repeat()\n    )\n\n  def init_states(self, inputs):\n    \"\"\"Initialize components of state: state_io, shift, circular_storage and circular_storage_mover\n    Assumes input has already been reshaped into microbatches: [num_micro_batches, micro_batch_size, sequence, embed]\n\n    Returns a dictionary with properties\n      shift: zeros shape [num_stages, micro_size, sequence, embed]\n      prev_outputs: same shape as shift, only used when pipeline_delay_activation_forwarding is set to true, else None\n      state_io: reshaped inputs [num_stages, microbatches/stages, micro_size, sequence, embed]\n      circ_storage: zeros [num_stages, microbatches, micro_size, sequence, embed] when needed, else None\n      circ_storage_mover: zeros[num_stages, micro_size, sequence, embed] when needed, else None\n      loop_iteration: scalar set initially to 0.\n    \"\"\"\n\n    # Shift is used to rotate the output of each pipeline into the input of the next\n    # shift has shape [num_stages, micro_size, sequence, embed]\n    shift = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n\n    shift = nn.with_logical_constraint(\n        shift,\n        (\"activation_stage\", self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # Prev outputs has the same shape of the output (and shift)\n    if self.config.pipeline_delay_activation_forwarding:\n      prev_outputs = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n      prev_outputs = nn.with_logical_constraint(\n          prev_outputs,\n          (\"activation_stage\", self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n          rules=self.config.logical_axis_rules,\n          mesh=self.mesh,\n      )\n    else:\n      prev_outputs = None\n\n    # state_io (state input output) at first holds all of the input batches, but also will hold the outputs\n    #   as the pipeline runs/finishes\n    # state_io has shape [num_stages, microbatches/stages, micro_size, sequence, embed]\n    state_io = jnp.reshape(inputs, (self.num_stages, self.microbatches_per_stage) + inputs.shape[1:])\n    # We shard the pipeline_microbatch_size axis by data/fsdp, not num_microbatches since those are looped over.\n    state_io = nn.with_logical_constraint(\n        state_io,\n        (\"activation_stage\", None, self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # circ_storage is used to hold the final pipeline stage outputs before it is used for the next repeat. It is only\n    # needed when num_microbatches > num_stages, else instead the final stage will immediately pass to the first without\n    # additional storage.\n    # circ_storage has shape [num_stages, microbatches, micro_size, sequence, embed].\n    # Note that this shape is a factor of num_stages larger than necessary - each stage holds the global batch, but only\n    # stage 0 holds the real activations (since it will use them), the rest hold dummy ones. This amount of storage\n    # [global_batch, sequence, embed] is fine as long as there is some amount of additional sharding axes, e.g. FSDP,\n    # TP, DP (e.g. there are many devices that shard stage 0)\n    # We may look into alternatives using less storage if this becomes an issue (ideas in b/347603101).\n    if self.use_circ_storage:\n      circ_storage = jnp.zeros((self.num_stages,) + inputs.shape, dtype=inputs.dtype)\n    else:\n      circ_storage = None\n\n    # circ_storage_mover is used to push the microbatches from the pipeline into circ_storage with one buffer iteration\n    # of delay circ_storage_mover shape is same as shift: [num_stages, micro_size, sequence, embed]\n    if self.use_circ_storage:\n      circ_storage_mover = shift\n    else:\n      circ_storage_mover = None\n\n    init_loop_state = {\n        \"state_io\": state_io,\n        \"shift\": shift,\n        \"circ_storage\": circ_storage,\n        \"circ_storage_mover\": circ_storage_mover,\n        \"loop_iteration\": 0,\n        \"prev_outputs\": prev_outputs,\n    }\n    return init_loop_state\n\n  def get_iteration_inputs(self, loop_iteration, state_io, circ_storage, shift):\n    \"\"\"\n    Construct stages_in: the global array that is operated on for this iteration, shape same as\n    shift=[stages, micro_size, sequence, embed]\n    This is almost a rotated version of the last outputs, except for the first stage which must grab a new batch from\n    state_io or an old one from circ_storage\n    \"\"\"\n\n    # Setup potential input from state_io, which has a rotating microbatch index (size of microbatches_per_stage)\n    state_io_batch_idx = loop_iteration % self.microbatches_per_stage\n    state_io_slice = state_io[:, state_io_batch_idx]\n\n    if self.use_circ_storage:\n      # Setup potential input from circ_storage, which also has a rotating index for microbatch,\n      # size of num_microbatches\n      circ_storage_batch_idx = loop_iteration % self.config.num_pipeline_microbatches\n      circular_stage_in = circ_storage[:, circ_storage_batch_idx]\n    else:\n      # The last stage immediately flows into the first stage, use this rotated shift instead of circular storage\n      circular_stage_in = shift\n\n    # For early loop iterations we grab a new input for stage 0 from the state_io. Once each microbatch has left\n    # state_io we instead grab from the last stage's output (possibly buffered when num_microbatches > num_stages, e.g.\n    # from circ_storage).\n    first_stage_in = jnp.where(loop_iteration < self.config.num_pipeline_microbatches, state_io_slice, circular_stage_in)\n\n    # Note that first_stage_in may correspond to bubble computation during the last few iterations.\n    # However, these bubble computation results remain in the shift buffer (do not make it back to state_io) and are\n    # thus discarded / not returned.\n    # The final returned output is stored in the state_io, which has the appropriate total size of num_microbatches. The\n    # state_io will not contain bubble results at the end of the last iteration.\n\n    def select_state_or_input(first_stage_in, shift):\n      # Selects input for stage 0, shift for other stages\n      return jnp.where(jax.lax.broadcasted_iota(\"int32\", shift.shape, 0) == 0, first_stage_in, shift)\n\n    # Selects input (from stream_io) for stage 0, other stages get from shift (the rotated previous output)\n    stages_in = select_state_or_input(first_stage_in, shift)\n    stages_in = nn.with_logical_constraint(\n        stages_in,\n        (\"activation_stage\", self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n    return stages_in\n\n  def shard_dim_by_stages(self, x, dim: int):\n    # Shards a dimension by stages. Currently, the sharding of other dimensions are left up the compiler, alternatively\n    # we may want to copy over the sharding from the other input axes.\n    dims_mapping = [jax.sharding.PartitionSpec.UNCONSTRAINED] * x.ndim\n    dims_mapping[dim] = \"stage\"\n    dims_mapping = tuple(dims_mapping)\n    sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(*dims_mapping))\n    return jax.lax.with_sharding_constraint(x, sharding)\n\n  def get_microbatch_and_repeat_ids(self, loop_iteration):\n    \"\"\"Gets the microbatch_ids and repeat_ids for all stages on this loop_iteration. Works for both circular and\n    non-circular\"\"\"\n    # Stage 0 has processed one microbatch every loop_iter, but Stage 1 is 1 behind due to bubble, etc for other stages\n    microbatches_processed = jnp.maximum(loop_iteration - self.forwarding_delay * jnp.arange(self.num_stages), 0)\n    microbatch_ids = microbatches_processed % self.config.num_pipeline_microbatches\n    repeat_ids = microbatches_processed // self.config.num_pipeline_microbatches\n    return microbatch_ids, repeat_ids\n\n  def vmap_parallel_gather(self, weights, repeat_ids, repeat_dim_in_weights, stages_dim_in_weights):\n    \"\"\"Use vmap to implement a sharded parallel gather.\n    Parallel gather means each stage has its own weights, and gets one slice from it.\n    Args:\n      weights: Per-stage data to be gathered from.\n      repeat_ids: Integer tensor of shape [num_stages], the repeats of the stages.\n      repeat_dim_in_weights: The dimension in weights where repeat_ids are applied. The output will not\n        have this dimension.\n      stages_dim_in_weights: The dimension in weights that represents parallel stages.\n    Returns:\n      The per-stage gathered values. The shape is weights.shape but with repeat_dim_in_weights\n        removed.\n    \"\"\"\n\n    def _gather_one(x, repeat_id):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, repeat_id, 1, repeat_dim_in_weights), repeat_dim_in_weights)\n\n    gathered_weights_stage_dim = 0\n    repeat_ids = self.shard_dim_by_stages(repeat_ids, 0)\n    weights = self.shard_dim_by_stages(weights, stages_dim_in_weights)\n    stage_weights = jax.vmap(_gather_one, in_axes=(stages_dim_in_weights, 0), out_axes=gathered_weights_stage_dim)(\n        weights, repeat_ids\n    )\n    stage_weights = self.shard_dim_by_stages(stage_weights, gathered_weights_stage_dim)\n    return stage_weights\n\n  def vmap_gather(self, xs, ids, ids_dim):\n    \"\"\"Use vmap to implement a stage-wise sharded gather.\n\n    The stages share the same input, but they have different offsets.\n\n    Args:\n      xs: Data shared by all stages, to be gathered from.\n      ids: Integer tensor of shape [num_stages], the offsets of the stages.\n      ids_dim: The dimension in xs where ids are applied. In the output, this\n        dimension will be [num_stages], since each stage gets one slice.\n\n    Returns:\n      The per-stage gathered values. The shape is xs.shape but with ids_dim size\n        replaced with [num_stages].\n    \"\"\"\n\n    def _gather_one(x, i):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, i, 1, ids_dim), ids_dim)\n\n    ids = self.shard_dim_by_stages(ids, 0)\n    outs = jax.vmap(_gather_one, in_axes=(None, 0), out_axes=ids_dim)(xs, ids)\n    return self.shard_dim_by_stages(outs, 0)\n\n  def get_new_loop_state(self, output, loop_state):\n    \"\"\"\n    Update the various buffers given the output of the most recent iteration\n    * state_io: rotates left/up by 1 (the whole created in the last slot is filled with the most recent pipeline output)\n       * Pushing inputs up from top of state_io into first stage of shift\n       * Pulling outputs up from last stage of shift into bottom of state_io\n    * shift: rotate output (or prev_outputs if using delay) right/down by 1 - we imagine the pipeline moves to\n               right/down\n    * circ_storage: pushes circ_storage_mover (the output of the previous iteration) into rotating index of circ_storage\n    * circ_storage_mover: assigned to rotated output and pushed into circ_storage on the next iteration\n    * prev_outputs: is set to the current output\n    \"\"\"\n\n    old_state_io = loop_state[\"state_io\"]\n    old_circ_storage = loop_state[\"circ_storage\"]\n    old_circ_storage_mover = loop_state[\"circ_storage_mover\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n    old_prev_outputs = loop_state[\"prev_outputs\"]\n\n    def _rotate_right(arr):\n      # Use lax.slice to avoid generating a gather.\n      last = jax.lax.slice_in_dim(arr, self.num_stages - 1, self.num_stages, axis=0)\n      except_last = jax.lax.slice_in_dim(arr, 0, self.num_stages - 1, axis=0)\n      return jnp.concatenate([last, except_last], axis=0)\n\n    def _shift_right(arr):\n      padding = [[1, 0]] + [[0, 0]] * (arr.ndim - 1)\n      # Use lax.slice to guarantee the gradient is a pad.\n      return jax.lax.slice(jnp.pad(arr, padding), [0] * arr.ndim, arr.shape)\n\n    # Shift either rotates or shifts depending on if the last stage immediately must send to first or not\n    # For non-circular pipelines, the last stage does not need to send to first\n    # For circular pipelines with #micro = #stages, last stage immediately sends to first\n    # For circular pipelines with #micro > stages (circ_storage), last stage sends to circ storage\n    def _update_shift(output_in):\n      if self.config.num_pipeline_repeats == 1 or self.use_circ_storage:\n        return _shift_right(output_in)  # last stage does not have to send to first immediately\n      else:\n        return _rotate_right(output_in)  # last stage must immediately send to first\n\n    if self.config.pipeline_delay_activation_forwarding:\n      new_shift = _update_shift(old_prev_outputs)\n      new_prev_outputs = output\n    else:\n      new_shift = _update_shift(output)\n      new_prev_outputs = None\n\n    if self.use_circ_storage:\n      # Insert the circ_storage_mover into new_circ_storage at a microbatch-rotating index.\n      # circ_storage_mover still points to the output of PREVIOUS iteration, which should aid in allowing overlapped\n      # compute/async transfers\n      def _rotate_right_and_update(circ_storage_mover_in, circ_storage_in):\n        rotated = _rotate_right(circ_storage_mover_in)\n        rotated = jnp.expand_dims(rotated, 1)\n        # We rotate the pushing index into circ storage, and ensure that microbatch 0 lands in index 0\n        offset = (\n            loop_iteration - self.iterations_to_complete_first_microbatch_one_repeat() - 1\n        ) % self.config.num_pipeline_microbatches  # Note extra -1 b/c grabbing from the\n        # previous output - using circ_storage_mover before it is updated\n        return jax.lax.dynamic_update_slice_in_dim(circ_storage_in, rotated, offset, axis=1)\n\n      new_circ_storage = _rotate_right_and_update(old_circ_storage_mover, old_circ_storage)\n      new_circ_storage_mover = output\n    else:\n      new_circ_storage = None\n      new_circ_storage_mover = None\n\n    # Rotate stream_io left/up by 1 on rotating micro/stage index (stream_buf_idx), replacing the last/bottom with the\n    # last stage output\n    stream_buf_idx = loop_iteration % self.microbatches_per_stage\n    stream_slice = old_state_io[:, stream_buf_idx]\n\n    def _update_state_io(state_in, stream_slice, output):\n      # Shift the current slice to the left, then fill the last stage with the final output.\n      padding = [[0, 1]] + [[0, 0]] * (stream_slice.ndim - 1)\n      stream_slice = jax.lax.slice_in_dim(jnp.pad(stream_slice, padding), 1, stream_slice.shape[0] + 1, axis=0)\n      stream_slice = jnp.where(\n          jax.lax.broadcasted_iota(\"int32\", stream_slice.shape, 0) == self.num_stages - 1, output, stream_slice\n      )\n      stream_slice = jnp.expand_dims(stream_slice, 1)\n      return jax.lax.dynamic_update_slice_in_dim(state_in, stream_slice, stream_buf_idx, axis=1)\n\n    new_state = _update_state_io(old_state_io, stream_slice, output)\n\n    new_loop_state = {\n        \"state_io\": new_state,\n        \"shift\": new_shift,\n        \"circ_storage\": new_circ_storage,\n        \"circ_storage_mover\": new_circ_storage_mover,\n        \"loop_iteration\": loop_iteration + 1,\n        \"prev_outputs\": new_prev_outputs,\n    }\n    return new_loop_state\n\n  def permute_output_micro_per_stage_dim(self, output):\n    # The first real output (microbatch 0) takes a certain amount of loop iterations to finish and be pushed to\n    # state_io - it will land on a different index of state_io depending on the number of iterations.\n    microbatch_0_idx = self.iterations_to_complete_first_microbatch() % self.microbatches_per_stage\n    permutation = (\n        np.arange(self.microbatches_per_stage) + microbatch_0_idx\n    ) % self.microbatches_per_stage  # permute so the value in land_idx is moved into idx 0, and (land_idx + 1) appear\n    # in idx 1, etc\n    output = output[:, permutation]\n    return output\n\n  def get_current_stage_weights(self, pipeline_weights, loop_iteration):\n    \"\"\"\n    Gets the current weights used for one iteration. Outputs a pytree whose arrays have leading dimension of stages, e.g.\n    {'mlp': 'wo': [stages, mlp, embed]}. Stage 0 will use the 0th index of this pytree, Stage 1 the 1st index, etc.\n    For non-circular pipelines, this simply returns all weights - every weight is used in every iteraiton. However\n    for circular pipelines each stage grabs only the weights corresponding to the current repeat.\n    \"\"\"\n    if self.config.num_pipeline_repeats > 1:\n      return self.get_current_repeat_from_stages(pipeline_weights, loop_iteration)\n    else:\n      return pipeline_weights\n\n  def get_current_repeat_from_stages(self, weights, loop_iteration):\n    \"\"\"get current repeat from stages\"\"\"\n    _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    def gather_weights_for_stages_in(weights):\n      return jax.tree.map(\n          functools.partial(\n              self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n          ),\n          weights,\n      )\n\n    circular_metadata_params = {\n        nn.PARTITION_NAME: \"circular_repeats\",\n        \"sub_weight_split_dims_mapping\": (None,),\n        \"is_initializing\": self.is_initializing(),\n        \"x_times\": self.config.num_pipeline_repeats,\n        \"optimizer_dims_mapping\": None,\n    }\n    weights = meta.remove_axis(\n        weights, 0, circular_metadata_params\n    )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one circular\n    # entry per stage.\n    weights = gather_weights_for_stages_in(weights)\n    return weights\n\n  def get_vmap_func_for_init(self):\n    \"\"\"This vmap func is used to initialize the weights only on init.\"\"\"\n\n    def func_to_vmap(body_instance, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      return body_instance(stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0, \"_overwrite_with_gradient\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def get_main_vmap_func_for_iterations(self):\n    \"\"\"\n    Returns main stage function vmapped by number of stages.\n    This becomes a vmap over a single layer instance if body_instance is a single layer,\n    else a set of layers if body_instance is a set of layers.\n    \"\"\"\n\n    def func_to_vmap(\n        body_instance, weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode\n    ):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      weights = meta.remove_axis(\n          weights,\n          0,\n          {\n              nn.PARTITION_NAME: \"layers\",\n              \"sub_weight_split_dims_mapping\": (None,),\n              \"is_initializing\": self.is_initializing(),\n              \"x_times\": self.num_stages,\n          },\n      )\n      return body_instance.apply(weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def run_one_iteration(\n      self, loop_state, pipeline_weights, positions, segment_ids, deterministic, model_mode, decoder_layer_instance\n  ):\n    \"\"\"Run one loop iteration - gets weights and inputs for each stage, run the stages in parallel,\n    and update the loop state.\"\"\"\n    state_io = loop_state[\"state_io\"]\n    shift = loop_state[\"shift\"]\n    circ_storage = loop_state[\"circ_storage\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n\n    microbatch_ids, _ = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    stages_inputs = self.get_iteration_inputs(loop_iteration, state_io, circ_storage, shift)\n    # We checkpoint stages_inputs since we are grabbing only one slice of the state_io, don't need to save the entire\n    # buffer.\n    stages_inputs = jax.ad_checkpoint.checkpoint_name(stages_inputs, \"iteration_input\")\n    stages_positions = self.vmap_gather(positions, microbatch_ids, 0) if positions is not None else None\n    stages_segment_ids = self.vmap_gather(segment_ids, microbatch_ids, 0) if segment_ids is not None else None\n\n    vmap_func = self.get_main_vmap_func_for_iterations()\n\n    if self.config.num_pipeline_repeats > 1:\n      _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n      def prepare_vars_for_main_vmap(weights):\n        def gather_weights_for_stages_in(weights):\n          return jax.tree.map(\n              functools.partial(\n                  self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n              ),\n              weights,\n          )\n\n        circular_metadata_params = {\n            nn.PARTITION_NAME: \"circular_repeats\",\n            \"sub_weight_split_dims_mapping\": (None,),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.config.num_pipeline_repeats,\n            \"optimizer_dims_mapping\": None,\n        }\n        weights = meta.remove_axis(\n            weights, 0, circular_metadata_params\n        )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one\n        # circular entry per stage.\n        weights = gather_weights_for_stages_in(weights)\n        return weights\n\n      vmap_func = nn.map_variables(\n          vmap_func,\n          mapped_collections=[\"params\", \"_overwrite_with_gradient\", \"non_trainable\", \"summaries\", \"intermediates\"],\n          mutable=True,\n          trans_in_fn=prepare_vars_for_main_vmap,\n      )\n\n    stage_weights = self.get_current_stage_weights(pipeline_weights, loop_iteration)\n    stages_output = vmap_func(\n        decoder_layer_instance,\n        stage_weights,\n        stages_inputs,\n        stages_segment_ids,\n        stages_positions,\n        deterministic,\n        model_mode,\n    )\n    if self.config.scan_layers:\n      stages_output = stages_output[0]\n\n    new_state = self.get_new_loop_state(stages_output, loop_state)\n    return new_state\n\n  def get_pipeline_remat_policy(self):\n    \"\"\"Returns the pipeline remat policy for this pipeline.\"\"\"\n    # We ensure that the decoder layer inputs are saved, although we leave it to a custom\n    # policy if they should be saved to device or offloaded.\n    if self.config.remat_policy == \"custom\":\n      return self.remat_policy\n\n    save_input_policy = jax.checkpoint_policies.save_only_these_names(\"iteration_input\", \"decoder_layer_input\")\n    if self.remat_policy is not None:\n      remat_policy = jax.checkpoint_policies.save_from_both_policies(self.remat_policy, save_input_policy)\n    else:\n      remat_policy = save_input_policy\n    return remat_policy\n\n  def get_weight_sharding(self, *init_args):\n    \"\"\"get weight sharding function for this pipeline.\"\"\"\n    # Returns a partition spec of all weights. Requires passing in arguments to init.\n    key = jax.random.PRNGKey(0)\n    keys = {\"params\": key, \"dropout\": key, \"aqt\": key}\n    weights = self.init(keys, *init_args)\n\n    def get_partition_spec(pytree):\n      def _is_leaf(x):\n        return isinstance(x, nn.spmd.LogicallyPartitioned)\n\n      def get_partition_spec_leaf(leaf):\n        return leaf.get_partition_spec()\n\n      partition_spec_tree = jax.tree.map(get_partition_spec_leaf, pytree, is_leaf=_is_leaf)\n      return partition_spec_tree\n\n    partition_spec_with_extra_layer = get_partition_spec(weights)\n    partition_spec = {\"params\": partition_spec_with_extra_layer[\"params\"][\"layers\"]}\n    return partition_spec\n\n  def get_physical_spec_no_fsdp(self, full_logical):\n    \"\"\"\n    Get physical spec without fsdp.\n\n    TODO: Remove the expert sharding on attention weights as well, since those act like fsdp.\n\n    Args:\n      full_logical: original logical partition specs of all weights\n\n    Returns:\n      Modified physical spec with \"fsdp\" and \"fsdp_transpose\" removed\n    \"\"\"\n\n    def remove_fsdp_sharding(sharding_tree):\n      def _remove_fsdp_from_partition_spec(named_sharding):\n        if isinstance(named_sharding, jax.sharding.NamedSharding):\n          new_spec = []\n          for axis in named_sharding.spec:\n            if axis is None:\n              new_spec.append(None)\n            elif isinstance(axis, str):\n              if axis not in (\"fsdp\", \"fsdp_transpose\"):\n                new_spec.append(axis)\n              else:\n                new_spec.append(None)\n            elif isinstance(axis, (list, tuple)):\n              new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n              new_spec.append(tuple(new_axis))\n            else:\n              raise ValueError(f\"Unsupported axis type: {type(axis)}\")\n          return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n        return named_sharding\n\n      return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n    physical = nn.logical_to_mesh_sharding(full_logical, mesh=self.mesh, rules=self.config.logical_axis_rules)\n    physical_no_fsdp = remove_fsdp_sharding(physical)\n    return physical_no_fsdp\n\n  def all_gather_over_fsdp(self, sharding_info):\n    physical_constraint_no_fsdp = self.get_physical_spec_no_fsdp(sharding_info)\n    return jax.lax.with_sharding_constraint(self.layers.variables, physical_constraint_no_fsdp)\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      segment_ids: jnp.ndarray,\n      positions: jnp.ndarray,\n      deterministic: bool,\n      model_mode=MODEL_MODE_TRAIN,\n      partition_spec=None,  # Pytree of sharding specifications of the weights (aka self.layers.variables)\n  ) -> jnp.ndarray:\n    \"\"\"The main method that maps the series of decoder layer inputs to final layer outputs.\n    Has the same signature of a single decoder layer, and expects the same shapes, e.g. the inputs should have shape\n    [global_batch], and internally this will be reshapped into microbatches.\n    \"\"\"\n    # Reshape inputs of [global_batch, ...] to [microbatches, pipeline_microbatch_sizes, ...]\n    inputs = inputs.reshape(\n        (\n            self.config.num_pipeline_microbatches,\n            self.pipeline_microbatch_size,\n            self.config.max_target_length,\n            self.config.emb_dim,\n        )\n    )\n    example_inputs = jax.lax.broadcast(inputs[0], [self.num_stages])  # dummy inputs fed to initialize the module\n    # weights.\n    ag_sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(None, None))\n    if positions is not None:\n      # AG positions\n      positions = jax.lax.with_sharding_constraint(positions, ag_sharding)\n\n      positions = positions.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_position = jax.lax.broadcast(positions[0], [self.num_stages])\n      position_idx = 0\n    else:\n      example_position = None\n      position_idx = None\n    if segment_ids is not None:\n      segment_ids = jax.lax.with_sharding_constraint(segment_ids, ag_sharding)\n      segment_ids = segment_ids.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_segmentation = jax.lax.broadcast(segment_ids[0], [self.num_stages])\n      segment_idx = 0\n    else:\n      example_segmentation = None\n      segment_idx = None\n\n    loop_state = self.init_states(inputs)\n\n    # Each microbatch should go through each stage (with repeats) - so there is num_micro * (num_stages * repeats)\n    # compute to perform\n    # Each iteration is vmapped by num_stages, so the number of iterations should be\n    # num_micro * num_stages * repeats / num_stages = num_micro * repeats\n    # However due to the pipeline bubble some iterations process less than num_stages microbatches. It takes\n    # num_micro * repeat iterations for the last microbatch to start the final repeat, then an additional\n    # num_stages - 1 to finish the final repeat.\n    # Thus the total iterations is num_micro * repeat + num_stages - 1, & we may consider the num_stages - 1 as bubble.\n    # The bubble doubles when we use forwarding delay.\n    bubble_iterations = self.forwarding_delay * (self.num_stages - 1)\n    real_iterations = self.config.num_pipeline_microbatches * self.config.num_pipeline_repeats\n    total_iterations = real_iterations + bubble_iterations\n\n    if self.is_initializing():\n      vmap_func = self.get_vmap_func_for_init()\n\n      if self.config.num_pipeline_repeats > 1:\n        # To shard the weights on initialization for the circular pipeline we create weights of\n        # shape [num_repeat, num_stages, ...] (e.g. [num_repeat, num_stages, embed, mlp]) and shard the num_stages axis.\n        # We wrap the main stage vmap with a num_repeat vmap to generate this axis only for parameter initialization.\n        vmap_func = nn.vmap(\n            vmap_func,\n            in_axes=(0, segment_idx, position_idx, None, None),\n            variable_axes={\n                \"params\": 0,\n                \"_overwrite_with_gradient\": 0,\n                \"non_trainable\": 0,\n                \"hyper_params\": 0,\n            },\n            split_rngs={\"params\": True, \"dropout\": self.config.enable_dropout},\n            metadata_params={\n                nn.PARTITION_NAME: \"circular_repeats\",\n                \"sub_weight_split_dims_mapping\": (None,),\n                \"is_initializing\": True,\n                \"x_times\": self.config.num_pipeline_repeats,\n                \"optimizer_dims_mapping\": None,\n            },\n        )\n\n        example_inputs = jax.lax.broadcast(example_inputs, [self.config.num_pipeline_repeats])\n        example_segmentation = (\n            jax.lax.broadcast(example_segmentation, [self.config.num_pipeline_repeats])\n            if example_segmentation is not None\n            else None\n        )\n        example_position = (\n            jax.lax.broadcast(example_position, [self.config.num_pipeline_repeats])\n            if example_position is not None\n            else None\n        )\n      # We only need to run one set of stages to initialize the variables, instead of looping over all microbatches for\n      # the full total_iterations.\n      stage_outputs = vmap_func(\n          self.layers, example_inputs, example_segmentation, example_position, deterministic, model_mode\n      )\n      if self.config.scan_layers:\n        stage_outputs = stage_outputs[0]\n\n      # We return something of the correct shape (global_batch, sequence, embed) by reshaping a single stages output\n      # which has shape [pipeline_microbatch_size, sequence, embed]\n      if self.config.num_pipeline_repeats > 1:\n        stage_outputs = stage_outputs[0]  # Remove extra dimension created for the circular vmap\n      broadcasted_stage_outpus = jax.lax.broadcast(\n          stage_outputs[0], [self.config.micro_batch_size_to_train_on // self.pipeline_microbatch_size]\n      )\n      return jnp.reshape(\n          broadcasted_stage_outpus,\n          [self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim],\n      )\n\n    if self.config.pipeline_fsdp_ag_once:\n      all_pipeline_weights = all_gather_over_fsdp(\n          self.layers.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n      )\n    else:\n      all_pipeline_weights = self.layers.variables\n\n    def run_iteration_scannable(model, loop_state, xs):\n      # flax transforms like nn.scan and nn.remat can only be applied to nn.module classes or nn.module instances, so we\n      # explicitly wrap the run_one_iteration in this method - the 1st argument model (`self`) is a nn.module instance.\n      return (\n          model.run_one_iteration(\n              loop_state, all_pipeline_weights, positions, segment_ids, deterministic, model_mode, model.layers\n          ),\n          None,\n      )\n\n    if self.config.set_remat_policy_on_pipeline_iterations:\n      run_iteration_scannable = nn.remat(\n          run_iteration_scannable,\n          prevent_cse=not self.config.scan_pipeline_iterations,  # prevent_cse not used with scan\n          policy=self.get_pipeline_remat_policy(),\n      )\n\n    # The scan cannot be used on init since it broadcasts the weights, which aren't yet initialized.\n    if self.config.scan_pipeline_iterations:\n      variable_carry = []\n      variable_broadcast = [\n          \"params\",\n          \"_overwrite_with_gradient\",\n      ]  # All loop iterations need the weights for the full pipeline.\n      if self.is_mutable_collection(\"non_trainable\"):\n        variable_carry.append(\"non_trainable\")\n      else:\n        variable_broadcast.append(\"non_trainable\")\n      run_all_iterations_scanned = nn.scan(\n          run_iteration_scannable,\n          variable_axes={\n              \"summaries\": 0,\n              \"aux_loss\": 0,\n              \"intermediates\": 0,\n              \"hyper_params\": 0,\n          },\n          variable_broadcast=variable_broadcast,\n          variable_carry=variable_carry,\n          # Dropout/aqt keys will be split for each iteration.\n          split_rngs={\"random\": True},\n          length=total_iterations,\n      )\n      loop_state, _ = run_all_iterations_scanned(self, loop_state, None)\n    else:\n      for _ in range(total_iterations):\n        loop_state, _ = run_iteration_scannable(self, loop_state, None)\n\n    # The final output is located in the input/output array, however the output microbatches may be permuted relative to\n    # the input\n    final_output = self.permute_output_micro_per_stage_dim(loop_state[\"state_io\"])\n\n    # reshape outputs to match input shape of total batch instead of microbatches [batch, sequence, embed]\n    final_output = jnp.reshape(\n        final_output, (self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim)\n    )\n\n    return final_output",
        "analysis": {
            "module_type": "pipeline_parallel_module",
            "purpose": "Implements GPipe-style pipeline parallelism over a set of layers, supporting microbatching, stage-based execution, and circular pipeline schedules.",
            "input": {
                "shape": "inputs: [micro_batch_size_to_train_on, max_target_length, emb_dim], segment_ids: [micro_batch_size_to_train_on, max_target_length], positions: [micro_batch_size_to_train_on, max_target_length]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Reshape inputs from a global batch into microbatches.",
                "Initialize pipeline state buffers (e.g., for activations, inputs/outputs) using `init_states`.",
                "Calculate the total number of loop iterations required, including pipeline fill/drain bubbles.",
                "If in initialization mode, execute a vmapped version of the layers once to initialize parameters and return a correctly shaped dummy output.",
                "If in training/inference mode, loop for the total number of iterations.",
                "Within each loop iteration, execute `run_one_iteration`, which processes one microbatch per active stage in parallel.",
                "The loop can be implemented with either a Python for-loop or `flax.linen.scan`.",
                "After the loop, permute the microbatches in the output buffer to match the original input order using `permute_output_micro_per_stage_dim`.",
                "Reshape the final output from microbatches back to a single global batch tensor."
            ],
            "output": {
                "shape": "[micro_batch_size_to_train_on, max_target_length, emb_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "Config",
                "all_gather_over_fsdp",
                "flax.linen.vmap",
                "flax.linen.scan",
                "flax.linen.remat"
            ],
            "parameters": {
                "config.num_pipeline_microbatches": "The number of microbatches the global batch is split into.",
                "config.num_pipeline_repeats": "The number of times the layers are repeated in a circular pipeline schedule.",
                "config.ici_pipeline_parallelism": "Inter-chip interconnect pipeline parallelism degree.",
                "config.dcn_pipeline_parallelism": "Data center network pipeline parallelism degree.",
                "config.pipeline_delay_activation_forwarding": "If True, introduces an additional delay cycle for forwarding activations between stages.",
                "config.scan_pipeline_iterations": "If True, uses `nn.scan` to loop over pipeline iterations; otherwise, uses a Python for-loop."
            },
            "notes": [
                "The module uses `nn.vmap` to execute all pipeline stages in parallel within a single iteration.",
                "It has a distinct execution path for parameter initialization versus forward/backward passes, controlled by `self.is_initializing()`.",
                "Supports both standard GPipe and circular pipeline schedules by managing weights and activations accordingly."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes configuration-dependent attributes like number of stages and microbatch sizes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate `num_stages` from parallelism config.",
                        "Calculate `pipeline_microbatch_size`.",
                        "Determine if circular storage is needed via `need_circ_storage`.",
                        "Set logical axis names for activations."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.need_circ_storage"
                    ],
                    "notes": [
                        "This is a standard Flax setup method called once during module initialization."
                    ]
                },
                "init_states": {
                    "purpose": "Creates and initializes the buffers required for the pipeline loop, such as activation carriers and I/O storage.",
                    "input": {
                        "shape": "[num_micro_batches, micro_batch_size, sequence, embed]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Create zero-initialized `shift` buffer to pass activations between stages.",
                        "Create `prev_outputs` buffer if `pipeline_delay_activation_forwarding` is enabled.",
                        "Reshape input tensor into `state_io` buffer, organized by stage.",
                        "Create `circ_storage` and `circ_storage_mover` buffers if circular pipeline requires them.",
                        "Return all buffers in a dictionary."
                    ],
                    "output": {
                        "shape": "A dictionary of tensors with various shapes for managing pipeline state."
                    },
                    "dependencies": [
                        "jnp.zeros",
                        "jnp.reshape",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The shapes of the created buffers are critical for the pipeline's state management."
                    ]
                },
                "get_iteration_inputs": {
                    "purpose": "Constructs the input tensor for all stages for the current iteration by selecting from new microbatches, prior stage outputs, or circular storage.",
                    "input": {
                        "shape": "loop_iteration: scalar, state_io: [num_stages, microbatches/stage, ...], circ_storage: [num_stages, num_microbatches, ...], shift: [num_stages, ...]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Get the current microbatch slice from `state_io`.",
                        "Get the current microbatch slice from `circ_storage` if used.",
                        "Determine the input for the first stage: either a new microbatch from `state_io` or a completed one from `circ_storage` (or rotated `shift`).",
                        "Construct the full input tensor for all stages, where stage 0 gets the newly determined input and other stages get input from the `shift` buffer (output of the previous stage)."
                    ],
                    "output": {
                        "shape": "[num_stages, micro_size, sequence, embed]"
                    },
                    "dependencies": [
                        "jnp.where"
                    ],
                    "notes": [
                        "This method implements the core logic for data flow between stages and across loop iterations."
                    ]
                },
                "run_one_iteration": {
                    "purpose": "Executes a single parallel step across all pipeline stages.",
                    "input": {
                        "shape": "loop_state: Dictionary of state tensors, pipeline_weights: Pytree of model weights, positions: [num_microbatches, ...], segment_ids: [num_microbatches, ...]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get microbatch and repeat IDs for the current iteration using `get_microbatch_and_repeat_ids`.",
                        "Construct inputs for each stage using `get_iteration_inputs`.",
                        "Gather corresponding `positions` and `segment_ids` for each stage using `vmap_gather`.",
                        "Select the correct weights for each stage using `get_current_stage_weights`.",
                        "Execute the layer function across all stages in parallel using `nn.vmap`.",
                        "Update the pipeline state buffers with the new outputs using `get_new_loop_state`.",
                        "Return the new loop state."
                    ],
                    "output": {
                        "shape": "The updated loop state dictionary."
                    },
                    "dependencies": [
                        "self.get_microbatch_and_repeat_ids",
                        "self.get_iteration_inputs",
                        "self.vmap_gather",
                        "self.get_current_stage_weights",
                        "self.get_new_loop_state",
                        "nn.vmap"
                    ],
                    "notes": [
                        "This function represents the core computation within the pipeline loop."
                    ]
                },
                "get_new_loop_state": {
                    "purpose": "Updates all pipeline state buffers after one iteration based on the stage outputs.",
                    "input": {
                        "shape": "output: [num_stages, micro_size, sequence, embed], loop_state: Dictionary of state tensors",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Update the `shift` buffer by rotating or shifting the stage outputs.",
                        "Update `prev_outputs` if delayed forwarding is used.",
                        "If using circular storage, update `circ_storage` and `circ_storage_mover`.",
                        "Update the `state_io` buffer by storing the output of the final stage.",
                        "Increment the `loop_iteration` counter."
                    ],
                    "output": {
                        "shape": "The updated loop state dictionary."
                    },
                    "dependencies": [
                        "jax.lax.slice_in_dim",
                        "jnp.concatenate",
                        "jax.lax.dynamic_update_slice_in_dim"
                    ],
                    "notes": [
                        "This method manages the movement of data through the pipeline's various buffers."
                    ]
                },
                "__call__": {
                    "purpose": "Main entry point that executes the full pipeline schedule over the input data.",
                    "input": {
                        "shape": "inputs: [micro_batch_size_to_train_on, max_target_length, emb_dim], segment_ids: [micro_batch_size_to_train_on, max_target_length], positions: [micro_batch_size_to_train_on, max_target_length]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Reshape inputs into microbatches.",
                        "Initialize loop state buffers using `init_states`.",
                        "If initializing, run a single vmapped layer execution to create weights and return.",
                        "If not initializing, loop for a calculated number of iterations (including bubbles).",
                        "In each iteration, call `run_one_iteration` (potentially wrapped in `nn.remat` or `nn.scan`).",
                        "After the loop, permute the output microbatches to their correct order.",
                        "Reshape the final output from microbatches back to a single global batch tensor."
                    ],
                    "output": {
                        "shape": "[micro_batch_size_to_train_on, max_target_length, emb_dim]"
                    },
                    "dependencies": [
                        "self.init_states",
                        "self.run_one_iteration",
                        "self.permute_output_micro_per_stage_dim",
                        "all_gather_over_fsdp",
                        "flax.linen.scan",
                        "flax.linen.remat"
                    ],
                    "notes": [
                        "Handles both initialization and execution logic based on `self.is_initializing()`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Quantization:\n  \"\"\"Base class for quantization configurations\"\"\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Placeholder for dot_general implementation in subclasses.\"\"\"\n\n  def einsum(self, dtype: DType = jnp.float32):\n    \"\"\"Placeholder for einsum implementation in subclasses.\"\"\"",
        "analysis": {
            "module_type": "quantization_base_class",
            "purpose": "Serves as a base class or interface for different quantization configurations, defining placeholder methods for quantization-aware operations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes as a dataclass."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "dataclasses.dataclass",
                "typing.Tuple",
                "MaxText.common_types.DType",
                "jax.numpy"
            ],
            "parameters": {},
            "notes": [
                "This is an abstract base class intended to be subclassed by concrete quantization implementations like AqtQuantization or Fp8Quantization.",
                "The methods `dot_general_cls` and `einsum` are placeholders meant to be overridden."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "A placeholder method intended to be overridden by subclasses to return a specific `dot_general` implementation class or callable.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "This base implementation does nothing and returns None implicitly."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `mesh_axes` argument is intended for configuring tensor sharding in the subclass implementations."
                    ]
                },
                "einsum": {
                    "purpose": "A placeholder method intended to be overridden by subclasses to return a specific `einsum` implementation callable.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "This base implementation does nothing and returns None implicitly."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxText.common_types.DType",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The `dtype` argument is intended for specifying the computation precision in the subclass implementations."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_tiling_fn",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _tiling_fn(lhs, rhs, dimension_numbers, tile_size):\n  \"\"\"apply tiling function\"\"\"\n  del lhs, rhs\n\n  (lhs_ca, rhs_ca), _ = dimension_numbers\n  ret = tiled_dot_general.Cfg(\n      lhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n      rhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n  )\n\n  for lhs_idx, rhs_idx in zip(lhs_ca, rhs_ca):\n    ret.lhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=lhs_idx, tile_size=tile_size, tile_count=None))\n    ret.rhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=rhs_idx, tile_size=tile_size, tile_count=None))\n\n  return ret",
        "analysis": {
            "module_type": "tiling_configuration_generator",
            "purpose": "Generates a tiling configuration for a tiled dot-general operation by specifying how contracting dimensions should be tiled.",
            "input": {
                "shape": "Inputs are `lhs` and `rhs` tensors (unused), `dimension_numbers` (a tuple of lists of integers), and `tile_size` (an integer).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Unpack the contracting axes for the left-hand side (lhs) and right-hand side (rhs) from `dimension_numbers`.",
                "Initialize a `tiled_dot_general.Cfg` configuration object.",
                "Iterate through the pairs of contracting axes from `lhs` and `rhs`.",
                "For each contracting axis, create and append a `tiled_dot_general.AxisTiling` configuration with the specified `tile_size` to the `lhs` and `rhs` contraction axes lists in the Cfg object.",
                "Return the populated configuration object."
            ],
            "output": {
                "shape": "Returns a `tiled_dot_general.Cfg` object, which is not a tensor."
            },
            "dependencies": [
                "aqt.jax.v2.tiled_dot_general.Cfg",
                "aqt.jax.v2.tiled_dot_general.TensorTiling",
                "aqt.jax.v2.tiled_dot_general.AxisTiling"
            ],
            "parameters": {
                "tile_size": "The size of the tiles to be applied to the contracting dimensions."
            },
            "notes": [
                "The `lhs` and `rhs` input arguments are unused and immediately deleted; they likely exist to match a required function signature.",
                "This function is intended to be partially applied with a `tile_size` and used as the `tiling_fn` argument in AQT's dot-general operations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_rhs_axis_metadata_wrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _rhs_axis_metadata_wrapper(\n    x: jnp.ndarray,\n    tile_map,\n    no_sharding_axis: Sequence[int],\n    mesh_axes: Tuple[str, ...],\n    is_tiled: bool,\n    replicate_scale: bool = False,\n):\n  \"\"\"right-hand-side axis metadata wrapper\"\"\"\n  if replicate_scale:\n    # Temporarily using the shape to identify the scale.\n    # TODO: remove the replication once the 2d sharding quantization\n    # works as expected.\n    if len(x.shape) == 1:\n      return nn.with_logical_partitioning((lambda: x), tuple(None for _ in mesh_axes))()\n\n  mesh_axes = list(mesh_axes)\n  if is_tiled:\n    # tile_map is a mapping between original rank and a list of new, tiled rank.\n    if len(mesh_axes) < len(tile_map):\n      mesh_axes = [None] * (len(tile_map) - len(mesh_axes)) + mesh_axes\n    new_mesh_axes = [None] * len(x.shape)\n    for orig_rank, new_rank in tile_map.items():\n      assert new_rank\n      assert len(new_rank) <= 2\n      new_mesh_axes[new_rank[-1]] = mesh_axes[orig_rank]\n    mesh_axes = new_mesh_axes\n\n  if mesh_axes is not None and len(mesh_axes) > 0:\n    for no_shard_idx in no_sharding_axis:\n      if no_shard_idx < len(mesh_axes):\n        mesh_axes[no_shard_idx] = None\n\n  return nn.with_logical_partitioning((lambda: x), mesh_axes)()",
        "analysis": {
            "module_type": "tensor_sharding_wrapper",
            "purpose": "A helper function that applies sharding annotations (logical partitioning) to a right-hand-side (RHS) tensor, handling special cases for tiled operations and scale replication.",
            "input": {
                "shape": "The shape of the input tensor `x`, e.g., [dim_1, dim_2, ..., dim_n].",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Optionally, if `replicate_scale` is true and the input is a 1D tensor, replicate it across all mesh axes.",
                "If `is_tiled` is true, remap the `mesh_axes` according to the `tile_map` to match the new tiled tensor shape.",
                "Iterate through `no_sharding_axis` and set the corresponding `mesh_axes` entry to `None` to prevent sharding along those axes.",
                "Apply the final computed sharding specification to the input tensor `x` using `nn.with_logical_partitioning`."
            ],
            "output": {
                "shape": "The same shape as the input tensor `x`."
            },
            "dependencies": [
                "flax.linen.with_logical_partitioning"
            ],
            "parameters": {
                "x": "The input tensor to be annotated.",
                "tile_map": "A mapping from original tensor axis indices to new, tiled axis indices.",
                "no_sharding_axis": "A sequence of axis indices that should not be sharded.",
                "mesh_axes": "A tuple of mesh axis names defining the initial sharding plan.",
                "is_tiled": "A boolean indicating if the input tensor `x` has been tiled.",
                "replicate_scale": "A boolean flag to enable a special case for replicating 1D scale vectors across the mesh."
            },
            "notes": [
                "This function is specifically designed to prepare the right-hand-side (RHS) tensor of a dot-product-like operation for sharding.",
                "The logic for `replicate_scale` is noted as a temporary workaround for 2D sharding in quantization."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#AqtQuantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class AqtQuantization:\n  \"\"\"Configures AQT quantization github.com/google/aqt.\"\"\"\n\n  quant_dg: aqt_config.DotGeneral\n  quant_mode: aqt_flax.QuantMode = aqt_flax.QuantMode.TRAIN\n  replicate_scale: bool = False\n\n  def _get_mixed_precision_cfg(self):\n    \"\"\"get configuration for mixed precision\"\"\"\n    quant_dg = None\n    is_tiled = False\n    tiling_fn = None\n    # pylint: disable=protected-access\n    module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    tile_size = -1\n    for layer_name_re, layer_quant_dg in self.quant_dg.items():\n      if re.fullmatch(layer_name_re, module_path):\n        quant_dg, tile_size = layer_quant_dg\n    if quant_dg is None:\n      quant_dg, tile_size = self.quant_dg[DEFAULT]\n    if tile_size != -1:\n      is_tiled = True\n      tiling_fn = functools.partial(_tiling_fn, tile_size=tile_size)\n    return quant_dg, is_tiled, tiling_fn\n\n  def _get_rhs_axis_metadata_wrapper(\n      self, mesh_axes: Tuple[str, ...] = (), is_tiled: bool = False, replicate_scale: bool = False\n  ):\n    if self.quant_mode == aqt_flax.QuantMode.CONVERT:\n      return None\n    return functools.partial(\n        _rhs_axis_metadata_wrapper, mesh_axes=mesh_axes, is_tiled=is_tiled, replicate_scale=replicate_scale\n    )\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    # module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    # print(f\"quant_dg: {quant_dg}, is_tiled: {is_tiled}, module_path: {module_path}\")\n    aqt_dg_cls = functools.partial(\n        aqt_flax.AqtDotGeneral,\n        quant_dg,\n        rhs_quant_mode=self.quant_mode,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n        rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n        use_legacy_freezer=False,\n        tiling_fn=tiling_fn,\n    )\n    return aqt_dg_cls\n\n  def einsum(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns einsum configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    aqt_einsum = functools.partial(\n        aqt_flax.AqtEinsum(\n            cfg=quant_dg,\n            rhs_quant_mode=self.quant_mode,\n            lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n            rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n            rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n            use_legacy_freezer=False,\n            tiling_fn=tiling_fn,\n        )\n    )\n    return aqt_einsum",
        "analysis": {
            "module_type": "aqt_quantization_config",
            "purpose": "Configures and provides AQT (Algorithm Quantization Toolkit) implementations for dot_general and einsum operations, supporting mixed-precision quantization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "aqt.jax.v2.flax.aqt_flax",
                "functools",
                "re",
                "flax.linen.module"
            ],
            "parameters": {
                "quant_dg": "The AQT dot_general configuration. It can be a single `aqt_config.DotGeneral` object for uniform quantization or a dictionary mapping module path regexes to configurations for mixed-precision quantization.",
                "quant_mode": "The quantization mode, which can be `TRAIN`, `SERVE`, or `CONVERT` from `aqt_flax.QuantMode`.",
                "replicate_scale": "A boolean indicating whether to replicate the quantization scale across devices, typically used for 2D sharding."
            },
            "notes": [
                "This is a dataclass that acts as a factory for creating quantized linear algebra operations.",
                "It dynamically selects quantization configurations based on the module's path in the Flax model hierarchy when mixed precision is used (i.e., when `quant_dg` is a dictionary)."
            ],
            "methods": {
                "_get_mixed_precision_cfg": {
                    "purpose": "Retrieves the appropriate AQT configuration and tiling function for the current Flax module based on its path, enabling mixed-precision quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the current module's path from the Flax context (`nn.module._context`).",
                        "Iterate through the `self.quant_dg` dictionary to find a configuration whose regex key matches the module path.",
                        "If no match is found, use the default configuration specified by the `DEFAULT` key.",
                        "If a `tile_size` is specified in the matched config, create a partial tiling function `_tiling_fn`.",
                        "Return the selected `quant_dg` config, a boolean `is_tiled`, and the `tiling_fn`."
                    ],
                    "output": {
                        "shape": "A tuple containing (aqt_config.DotGeneral, bool, callable or None)."
                    },
                    "dependencies": [
                        "re.fullmatch",
                        "flax.linen.module",
                        "functools.partial",
                        "_tiling_fn"
                    ],
                    "notes": [
                        "This method relies on the Flax module context to determine which quantization rule to apply at runtime."
                    ]
                },
                "_get_rhs_axis_metadata_wrapper": {
                    "purpose": "Creates a partially applied function to handle sharding metadata for the right-hand-side tensor in a quantized operation.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `quant_mode` is `CONVERT`; if so, return None as metadata is not needed.",
                        "Return a `functools.partial` of the `_rhs_axis_metadata_wrapper` function, pre-filled with `mesh_axes`, `is_tiled`, and `replicate_scale`."
                    ],
                    "output": {
                        "shape": "A callable (the partially applied function) or None."
                    },
                    "dependencies": [
                        "functools.partial",
                        "_rhs_axis_metadata_wrapper",
                        "aqt_flax.QuantMode"
                    ],
                    "notes": [
                        "The returned wrapper is used by AQT to apply correct logical partitioning annotations for sharding."
                    ]
                },
                "dot_general_cls": {
                    "purpose": "Returns a partially configured `aqt_flax.AqtDotGeneral` class ready to be used as a replacement for `jax.lax.dot_general`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the appropriate quantization config, either by calling `_get_mixed_precision_cfg` or using `self.quant_dg` directly.",
                        "Get the sharding metadata handler by calling `_get_rhs_axis_metadata_wrapper`.",
                        "Create a `functools.partial` of `aqt_flax.AqtDotGeneral` with the retrieved quantization config, quant mode, freezer modes, and metadata wrapper.",
                        "Return the partially configured class."
                    ],
                    "output": {
                        "shape": "A callable which is the partially configured `AqtDotGeneral` class."
                    },
                    "dependencies": [
                        "self._get_mixed_precision_cfg",
                        "self._get_rhs_axis_metadata_wrapper",
                        "functools.partial",
                        "aqt_flax.AqtDotGeneral"
                    ],
                    "notes": [
                        "The returned callable can be passed to Flax layers (e.g., `nn.Dense`) via the `dot_general_cls` argument to enable quantization."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a partially configured `aqt_flax.AqtEinsum` instance ready to be used as a replacement for `jnp.einsum`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the appropriate quantization config, either by calling `_get_mixed_precision_cfg` or using `self.quant_dg` directly.",
                        "Get the sharding metadata handler by calling `_get_rhs_axis_metadata_wrapper`.",
                        "Create a `functools.partial` of an `aqt_flax.AqtEinsum` instance, configured with the retrieved quantization config, quant mode, freezer modes, and metadata wrapper.",
                        "Return the partially configured instance."
                    ],
                    "output": {
                        "shape": "A callable which is the partially configured `AqtEinsum` instance."
                    },
                    "dependencies": [
                        "self._get_mixed_precision_cfg",
                        "self._get_rhs_axis_metadata_wrapper",
                        "functools.partial",
                        "aqt_flax.AqtEinsum"
                    ],
                    "notes": [
                        "This provides a drop-in replacement for standard einsum operations to enable quantization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Quantization(Quantization):\n  \"\"\"Configures Fp8 quantization for NVIDIA GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.Fp8DirectDotGeneralOp\n\n  def einsum(self, dtype: DType = jnp.float32):\n    return _Fp8EinsumWrapper(dtype=dtype)",
        "analysis": {
            "module_type": "fp8_quantization_config",
            "purpose": "A configuration class that provides specific FP8-quantized linear algebra operations (dot_general and einsum) for NVIDIA GPUs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization",
                "flax.linen.nn",
                "_Fp8EinsumWrapper"
            ],
            "parameters": {},
            "notes": [
                "This class is a dataclass that inherits from the base `Quantization` class.",
                "It is specifically designed to provide FP8 implementations for NVIDIA hardware.",
                "The `quant_mode` class attribute is hardcoded to 'train'."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the Flax Linen module class for FP8 dot-general operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the nn.Fp8DirectDotGeneralOp class."
                    ],
                    "output": {
                        "shape": "Returns a class type, not a tensor. Shape is N/A."
                    },
                    "dependencies": [
                        "flax.linen.nn.Fp8DirectDotGeneralOp"
                    ],
                    "notes": [
                        "The `mesh_axes` argument is accepted to match the base class signature but is not used in this implementation."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a module that performs einsum operations using FP8 precision.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiates and returns an `_Fp8EinsumWrapper` with the specified computation `dtype`."
                    ],
                    "output": {
                        "shape": "Returns a module instance, not a tensor. Shape is N/A."
                    },
                    "dependencies": [
                        "_Fp8EinsumWrapper",
                        "jax.numpy.float32"
                    ],
                    "notes": [
                        "The returned wrapper handles casting the inputs to the correct computation data type before calling the underlying `nn.Fp8Einsum`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_Fp8EinsumWrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class _Fp8EinsumWrapper(nn.Module):\n  \"\"\"Wrapper for nn.Fp8Einsum to handle computation dtype.\"\"\"\n\n  dtype: DType\n\n  @nn.compact\n  def __call__(self, eqn, lhs, rhs, **kwargs):\n    # nn.Fp8Einsum determines compute dtype from rhs.\n    # We cast rhs to the desired computation dtype.\n    # nn.Fp8Einsum will then cast lhs to the same dtype.\n    rhs = rhs.astype(self.dtype)\n    return nn.Fp8Einsum(name=\"fp8_einsum\")(eqn, lhs, rhs, **kwargs)",
        "analysis": {
            "functionality": "The `_Fp8EinsumWrapper` class is a Flax module that wraps `nn.Fp8Einsum` to explicitly control the computation data type. It achieves this by casting the right-hand side (rhs) tensor to a specified dtype before passing it to the underlying `nn.Fp8Einsum` operation.",
            "usage": "Instantiate the class with a desired computation `dtype`. Then, call the instance with the same arguments as `jnp.einsum`: an equation string (`eqn`), a left-hand side tensor (`lhs`), and a right-hand side tensor (`rhs`). The module will perform an FP8 einsum operation and return the resulting tensor. This is used to enforce a specific precision for the matrix multiplication."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Einsum",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Einsum(nn.Module):\n  \"\"\"An fp8 einsum op.\n\n  Attributes:\n    amax_history_length: size of the amax history.\n    e4m3_dtype: e4m3 variants, e.g., e4m3fn, e4m3fnuz.\n    e5m2_dtype: e5m2 variants, e.g., e5m2, e5m2fnuz.\n    dtype: computation dtype.\n  \"\"\"\n\n  amax_history_length: int = 1024\n  e4m3_dtype: DType = jnp.float8_e4m3fn\n  e5m2_dtype: DType = jnp.float8_e5m2\n  dtype: DType = jnp.float32\n\n  def setup(self) -> None:\n    \"\"\"init with input_amax_history, kernel_amax_history, output_grad_amax_history,\n    input_scale, kernel_scale, output_grad_scale\"\"\"\n    scale_args = (\n        flax_initializers.ones_init(),\n        jax.random.PRNGKey(0),\n        (1,),\n        jnp.float32,\n    )\n    amax_history_args = (\n        flax_initializers.zeros_init(),\n        jax.random.PRNGKey(0),\n        (self.amax_history_length,),\n        jnp.float32,\n    )\n\n    OVERWRITE_WITH_GRADIENT = \"_overwrite_with_gradient\"\n    self.input_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"input_amax_history\", *amax_history_args)\n    self.kernel_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_amax_history\", *amax_history_args)\n    self.output_grad_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_amax_history\", *amax_history_args)\n\n    self.input_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"input_scale\", *scale_args)\n    self.kernel_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_scale\", *scale_args)\n    self.output_grad_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_scale\", *scale_args)\n\n  def __call__(self, eqn, *args, **kwargs):\n    assert len(args) == 2\n    x = args[0]\n    k = args[1]\n\n    comp_dtype = self.dtype\n    k = jnp.asarray(k, comp_dtype)\n    x = jnp.asarray(x, comp_dtype)\n\n    x_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, x, self.input_scale.value, self.input_amax_history.value)\n    k_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, k, self.kernel_scale.value, self.kernel_amax_history.value)\n\n    y_qdq = jnp.einsum(eqn, x_qdq, k_qdq, _dot_general=fp8_ops.dot_general_with_precision)\n\n    y = fp8_ops.out_qdq(\n        comp_dtype,\n        self.e5m2_dtype,\n        y_qdq,\n        self.output_grad_scale.value,\n        self.output_grad_amax_history.value,\n    )\n    return y",
        "analysis": {
            "functionality": "The `Fp8Einsum` class implements an Einstein summation (einsum) operation that internally uses 8-bit floating-point (FP8) precision for its computations. It manages the quantization and de-quantization of input tensors and the de-quantization of the output tensor, leveraging dynamic scaling factors based on amax history to maintain numerical stability.",
            "usage": "Instantiate the `Fp8Einsum` module and call it with an einsum equation string and two input tensors, similar to `jax.numpy.einsum`. For example: `fp8_einsum = Fp8Einsum(dtype=jnp.bfloat16); y = fp8_einsum('ab,bc->ac', x, k)`. The module requires state variables for scaling factors and amax history, which are initialized in the `setup` method and updated during training."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Quantization(Quantization):\n  \"\"\"Configures NANOO Fp8 quantization for AMD MI300/MI325 GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.NANOOFp8DotGeneralOp",
        "analysis": {
            "module_type": "nanoo_fp8_quantization_config",
            "purpose": "Configures and provides the necessary components for NANOO Fp8 quantization, specifically for AMD MI300/MI325 GPUs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization",
                "flax.linen.NANOOFp8DotGeneralOp"
            ],
            "parameters": {
                "quant_mode": "The quantization mode, hardcoded to 'train'."
            },
            "notes": [
                "This class is a specific implementation of the base 'Quantization' class for AMD GPUs.",
                "It provides a way to retrieve the correct dot-general operator for NANOO Fp8."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the Flax Linen operator class used for NANOO Fp8 dot-general operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the nn.NANOOFp8DotGeneralOp class."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.NANOOFp8DotGeneralOp"
                    ],
                    "notes": [
                        "The 'mesh_axes' input parameter is accepted but not used in this implementation.",
                        "The docstring incorrectly mentions 'aqt params', likely a copy-paste artifact from another class."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_int8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_int8_quant_config(config):\n  drhs_bits = None\n  drhs_accumulator_dtype = None\n  drhs_local_aqt = None\n  if config.quantization_local_shard_count != 0:\n    drhs_bits = 8\n    drhs_accumulator_dtype = jnp.int32\n    drhs_local_aqt = aqt_config.LocalAqt(contraction_axis_shard_count=config.quantization_local_shard_count)\n  return aqt_config.config_v3(\n      fwd_bits=8,\n      dlhs_bits=8,\n      drhs_bits=drhs_bits,\n      rng_type=\"jax.uniform\",\n      dlhs_local_aqt=None,\n      drhs_local_aqt=drhs_local_aqt,\n      fwd_accumulator_dtype=jnp.int32,\n      dlhs_accumulator_dtype=jnp.int32,\n      drhs_accumulator_dtype=drhs_accumulator_dtype,\n  )",
        "analysis": {
            "functionality": "This function creates and returns an AQT (Quantization Aware Training) configuration for 8-bit integer quantization. It conditionally configures local AQT for the backward pass based on the provided shard count.",
            "usage": "Call this function with a configuration object that has a `quantization_local_shard_count` attribute. It returns an `aqt_config.DotGeneral` object configured for int8 quantization, which can then be used to initialize an `AqtQuantization` class."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#ConstantBoundConfig",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class ConstantBoundConfig:\n  fwd_lhs_bound: float | None = None\n  fwd_rhs_bound: float | None = None\n  dlhs_lhs_bound: float | None = None\n  dlhs_rhs_bound: float | None = None\n  drhs_lhs_bound: float | None = None\n  drhs_rhs_bound: float | None = None",
        "analysis": {
            "functionality": "This code block defines an immutable dataclass, `ConstantBoundConfig`, used to store constant clipping bounds for quantization in AQT (AQuantized Training). It specifies optional float values for the forward pass (fwd), and the two backward passes (dlhs, drhs) for both the left-hand side (lhs) and right-hand side (rhs) tensors of a matrix multiplication.",
            "usage": "Instantiate this class with specific float values for the desired bounds to configure static quantization scaling. An instance of this class is passed to helper functions like `_build_const_scale_config` to modify an AQT configuration. If a bound is not provided (i.e., left as `None`), dynamic scaling is typically used for that tensor."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_const_scale_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_const_scale_config(\n    aqt_dg: aqt_config.DotGeneral,\n    cst_bound_config: ConstantBoundConfig,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a constant scale config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    cst_bound_config: The constant bound config.\n\n  Returns:\n    The AQT dot general config with constant scale config.\n  \"\"\"\n  if cst_bound_config.fwd_lhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_lhs_bound\n    )\n  if cst_bound_config.fwd_rhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_rhs_bound\n    )\n  if cst_bound_config.dlhs_lhs_bound:\n    aqt_dg.dlhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_lhs_bound\n    )\n\n  if cst_bound_config.dlhs_rhs_bound is not None:\n    aqt_dg.dlhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_rhs_bound\n    )\n\n  if cst_bound_config.drhs_lhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_lhs_bound\n    )\n\n  if cst_bound_config.drhs_rhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_rhs_bound\n    )\n\n  return aqt_dg",
        "analysis": {
            "functionality": "This function modifies an AQT dot general configuration (`aqt_dg`) by setting a constant calibration method for quantization. It uses bounds provided in a `ConstantBoundConfig` object to configure the calibration for the forward pass (fwd), derivative w.r.t. left-hand side (dlhs), and derivative w.r.t. right-hand side (drhs).",
            "usage": "Call this function with an `aqt_config.DotGeneral` object and a `ConstantBoundConfig` object. It will return the modified `aqt_config.DotGeneral` object with constant calibration functions set for any bounds specified in the `ConstantBoundConfig`."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#PerTensorScales",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class PerTensorScales:\n  fwd_lhs: bool = False\n  fwd_rhs: bool = False\n  dlhs_lhs: bool = False\n  dlhs_rhs: bool = False\n  drhs_lhs: bool = False\n  drhs_rhs: bool = False",
        "analysis": {
            "module_type": "configuration_dataclass",
            "purpose": "A dataclass to configure which parts of a quantized dot product operation (forward pass, and dlhs/drhs backward passes) should use per-tensor scaling.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "An instance of the PerTensorScales class."
            },
            "dependencies": [
                "dataclasses.dataclass",
                "_build_per_tensor_config"
            ],
            "parameters": {
                "fwd_lhs": "Boolean flag to enable per-tensor scaling for the left-hand side tensor in the forward pass. Defaults to False.",
                "fwd_rhs": "Boolean flag to enable per-tensor scaling for the right-hand side tensor in the forward pass. Defaults to False.",
                "dlhs_lhs": "Boolean flag to enable per-tensor scaling for the left-hand side tensor in the dlhs (gradient w.r.t. lhs) backward pass. Defaults to False.",
                "dlhs_rhs": "Boolean flag to enable per-tensor scaling for the right-hand side tensor in the dlhs backward pass. Defaults to False.",
                "drhs_lhs": "Boolean flag to enable per-tensor scaling for the left-hand side tensor in the drhs (gradient w.r.t. rhs) backward pass. Defaults to False.",
                "drhs_rhs": "Boolean flag to enable per-tensor scaling for the right-hand side tensor in the drhs backward pass. Defaults to False."
            },
            "notes": [
                "This class is used as a configuration object, primarily by the `_build_per_tensor_config` function to modify an `aqt_config.DotGeneral` object.",
                "When a flag is set to True, it results in setting the `calib_shared_axes` attribute to 'per_tensor' for the corresponding part of the AQT dot general configuration."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_per_tensor_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_per_tensor_config(\n    aqt_dg: aqt_config.DotGeneral,\n    per_tensor_scales: PerTensorScales,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a per tensor config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    per_tensor_scales: The per tensor scales config.\n\n  Returns:\n    The AQT dot general config with per tensor config.\n  \"\"\"\n  if per_tensor_scales.fwd_lhs:\n    aqt_dg.fwd.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.fwd_rhs:\n    aqt_dg.fwd.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_lhs:\n    aqt_dg.dlhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_rhs:\n    aqt_dg.dlhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_lhs:\n    aqt_dg.drhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_rhs:\n    aqt_dg.drhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  return aqt_dg",
        "analysis": {
            "functionality": "This function modifies an AQT DotGeneral configuration object to enable per-tensor quantization scaling for different parts of the dot product operation (forward pass, and backward pass gradients dlhs/drhs).",
            "usage": "Call this function with an `aqt_config.DotGeneral` object and a `PerTensorScales` dataclass instance. It will update the `calib_shared_axes` attribute on the `aqt_dg` object for each part where the corresponding flag in `per_tensor_scales` is True, and return the modified object. For example, `_build_per_tensor_config(my_aqt_config, PerTensorScales(fwd_rhs=True))` will configure the right-hand side of the forward pass to use per-tensor scaling."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_default_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_default_config(config):\n  \"\"\"Get aqt for 8-bit floating point quantization configuration.\"\"\"\n  aqt_dg = aqt_config.config_v4(\n      fwd_bits=\"e4m3\",\n      dlhs_bits=\"e5m2\",\n      drhs_bits=\"e5m2\",\n      use_dummy_static_bound=False,\n      fwd_accumulator_dtype=jnp.bfloat16,\n      dlhs_accumulator_dtype=jnp.bfloat16,\n      drhs_accumulator_dtype=jnp.bfloat16,\n      dlhs_use_fwd_quant=False,\n      drhs_use_fwd_quant=False,\n  )\n  constant_bound_config = None\n\n  if len(config.constant_bound_config) == 6:\n    fwd_lhs_bound, fwd_rhs_bound, dlhs_lhs_bound, dlhs_rhs_bound, drhs_lhs_bound, drhs_rhs_bound = (\n        config.constant_bound_config\n    )\n    constant_bound_config = ConstantBoundConfig(\n        fwd_lhs_bound=fwd_lhs_bound,\n        fwd_rhs_bound=fwd_rhs_bound,\n        dlhs_lhs_bound=dlhs_lhs_bound,\n        dlhs_rhs_bound=dlhs_rhs_bound,\n        drhs_lhs_bound=drhs_lhs_bound,\n        drhs_rhs_bound=drhs_rhs_bound,\n    )\n    aqt_dg = _build_const_scale_config(aqt_dg, constant_bound_config)\n\n  aqt_config.set_stochastic_rounding(\n      aqt_dg,\n      vjp_lhs_stochastic_rounding=False,\n      vjp_rhs_stochastic_rounding=False,\n      implementation=\"jax.uniform\",\n  )\n\n  per_tensor_scales = PerTensorScales(\n      fwd_lhs=True,\n      fwd_rhs=True,\n      dlhs_lhs=True,\n      dlhs_rhs=True,\n      drhs_lhs=True,\n      drhs_rhs=True,\n  )\n  return _build_per_tensor_config(aqt_dg, per_tensor_scales)",
        "analysis": {
            "module_type": "aqt_fp8_config_generator",
            "purpose": "Creates a default AQT DotGeneral configuration for 8-bit floating-point (FP8) quantization, supporting both dynamic and static scaling.",
            "input": {
                "shape": "N/A",
                "dtype": "MaxText.common_types.Config"
            },
            "processing_steps": [
                "Initialize a base FP8 AQT configuration using `aqt_config.config_v4` with 'e4m3' for forward and 'e5m2' for backward passes.",
                "Check if `config.constant_bound_config` is provided and has a length of 6.",
                "If constant bounds are provided, create a `ConstantBoundConfig` and apply it using `_build_const_scale_config` to enable static scaling.",
                "Configure stochastic rounding on the AQT configuration using `aqt_config.set_stochastic_rounding`.",
                "Create a `PerTensorScales` configuration to enable per-tensor scaling for all inputs.",
                "Apply the per-tensor scaling configuration using `_build_per_tensor_config`.",
                "Return the final AQT DotGeneral configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config",
                "jax.numpy",
                "ConstantBoundConfig",
                "_build_const_scale_config",
                "PerTensorScales",
                "_build_per_tensor_config",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.constant_bound_config": "An optional sequence of 6 float values for static quantization bounds (fwd_lhs, fwd_rhs, dlhs_lhs, dlhs_rhs, drhs_lhs, drhs_rhs). If not provided, dynamic scaling is used."
            },
            "notes": [
                "This function sets up a standard FP8 training recipe.",
                "By default, it configures dynamic scaling, but can be switched to static scaling by providing `constant_bound_config`.",
                "The accumulator data type for all passes is set to `jnp.bfloat16`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_quant_config(config):\n  \"\"\"get aqt for 8-bit floating point quantization configuration\"\"\"\n  cfg = aqt_config.config_v4(fwd_bits=\"e4m3\", dlhs_bits=None, drhs_bits=None, fwd_accumulator_dtype=jnp.bfloat16)\n  return cfg",
        "analysis": {
            "module_type": "quantization_config_generator",
            "purpose": "Creates and returns an AQT (Algorithm Quantization Toolkit) configuration for 8-bit floating-point quantization, specifically for the forward pass.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `aqt_config.config_v4` to create a quantization configuration.",
                "Sets forward pass quantization bits to 'e4m3' (8-bit float).",
                "Disables backward pass quantization by setting `dlhs_bits` and `drhs_bits` to None.",
                "Sets the forward pass accumulator data type to `jnp.bfloat16`.",
                "Returns the generated configuration object."
            ],
            "output": {
                "shape": "Returns an AQT configuration object, not a tensor."
            },
            "dependencies": [
                "aqt.jax.v2.config as aqt_config",
                "jax.numpy as jnp"
            ],
            "parameters": {
                "config": "A configuration object, which is passed as an argument but is not used within the function."
            },
            "notes": [
                "This configuration is tailored for inference or forward-pass-only scenarios, as gradient quantization for the backward pass is disabled."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_dot_general_make",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _dot_general_make(quant_cfg):\n  \"\"\"Create quantization configs for input matrices to a matmul\"\"\"\n  lhs_bits = quant_cfg[_A_BITS]\n  lhs_scale = quant_cfg[_A_SCALE]\n  rhs_bits = quant_cfg[_W_BITS]\n  rhs_scale = quant_cfg[_W_SCALE]\n  aqt_dg = aqt_config.dot_general_make(lhs_bits=lhs_bits, rhs_bits=rhs_bits)\n  if lhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=lhs_scale)\n  if rhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=rhs_scale)\n  return aqt_dg",
        "analysis": {
            "module_type": "aqt_dot_general_config_factory",
            "purpose": "Creates and configures an AQT DotGeneral configuration object for matrix multiplication based on specified bit-widths and clipping scales.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract activation ('a_bits', 'a_scale') and weight ('w_bits', 'w_scale') parameters from the input `quant_cfg` dictionary.",
                "Create a base AQT dot_general configuration using `aqt_config.dot_general_make` with the specified bit-widths.",
                "If the activation scale (`lhs_scale`) is less than 1.0, set the LHS quantizer's calibration to `AbsMaxCalibration` with the given scale.",
                "If the weight scale (`rhs_scale`) is less than 1.0, set the RHS quantizer's calibration to `AbsMaxCalibration` with the given scale.",
                "Return the configured AQT DotGeneral object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.dot_general_make",
                "aqt.jax.v2.calibration.AbsMaxCalibration",
                "functools"
            ],
            "parameters": {
                "quant_cfg": "A dictionary containing quantization settings, including 'a_bits', 'a_scale', 'w_bits', and 'w_scale' for activations (lhs) and weights (rhs)."
            },
            "notes": [
                "The function conditionally applies `AbsMaxCalibration` only if the provided scale is less than 1.0, which sets a custom clipping bound.",
                "This is a helper function used to construct configurations for mixed-precision quantization."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_default_mp_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_default_mp_config(default=None):\n  default_config = {_W_BITS: None, _A_BITS: None, _W_SCALE: 1.0, _A_SCALE: 1.0, _TILE_SIZE: -1}\n  if default:\n    default_config.update(default)\n  return default_config",
        "analysis": {
            "module_type": "configuration_helper",
            "purpose": "Creates a default configuration dictionary for mixed-precision quantization and optionally updates it with user-provided values.",
            "input": {
                "shape": "N/A",
                "dtype": "dict or None"
            },
            "processing_steps": [
                "Initialize a dictionary `default_config` with predefined keys and default values for quantization parameters.",
                "If a `default` dictionary is provided, update `default_config` with its key-value pairs.",
                "Return the final configuration dictionary."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_W_BITS",
                "_A_BITS",
                "_W_SCALE",
                "_A_SCALE",
                "_TILE_SIZE"
            ],
            "parameters": {
                "default": "An optional dictionary containing values to override the hardcoded defaults."
            },
            "notes": [
                "This function is used to establish a base configuration for mixed-precision quantization which can then be customized.",
                "The keys in the dictionary correspond to weight bits, activation bits, weight scale, activation scale, and tile size."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_mixed_precision_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_mixed_precision_quant_config(mixed_precision_config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  ret_config = {}\n  default_mp_config = _get_default_mp_config(default=mixed_precision_config.get(DEFAULT, None))\n  for layer_name_re, layer_quantization_config in mixed_precision_config.items():\n    # Make a copy of default_mp_config to avoid updating original dict\n    quant_config = default_mp_config.copy()\n    # print(f\"Mixed precision config: processing\n    # {layer_name_re} - {layer_quantization_config}, default config - {quant_config}\")\n    if layer_name_re != DEFAULT:\n      for k in quant_config:\n        quant_config[k] = layer_quantization_config.get(k, default_mp_config[k])\n    ret_config[layer_name_re] = [_dot_general_make(quant_config), quant_config[\"tile_size\"]]\n  return ret_config",
        "analysis": {
            "functionality": "This function processes a user-provided mixed-precision configuration dictionary and generates a structured AQT (AQuantized Training) configuration dictionary. It establishes a default quantization setting and then applies layer-specific overrides.",
            "usage": "Call this function with a dictionary `mixed_precision_config` where keys are regular expressions matching layer names and values are dictionaries specifying quantization parameters like 'w_bits', 'a_bits', 'w_scale', 'a_scale', and 'tile_size'. A special key '__default__' can be used to set the base configuration. The function returns a dictionary mapping each layer regex to a list containing an AQT DotGeneral configuration object and its corresponding tile size."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_quant_config(config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  if not config.quantization or config.quantization == \"\":\n    return None\n  if config.quantization == \"int8\":\n    return _get_int8_quant_config(config)\n  if config.quantization == \"intmp\":\n    assert config.quant_cfg_path, \"Must specify quant_cfg for mixed precision quantization\"\n    with open(config.quant_cfg_path, \"rt\", encoding=\"utf8\") as config_file:\n      mixed_precision_config = json.load(config_file)\n    return _get_mixed_precision_quant_config(mixed_precision_config)\n  if config.quantization == \"fp8\":\n    return \"fp8\"\n  if config.quantization == \"nanoo_fp8\":\n    return \"nanoo_fp8\"\n  if config.quantization == \"aqt_fp8\":\n    return _get_aqt_fp8_quant_config(config)\n  if config.quantization == \"aqt_fp8_full\":\n    return _get_aqt_fp8_default_config(config)\n\n  raise ValueError(f\"Invalid value configured for quantization {config.quantization}.\")",
        "analysis": {
            "functionality": "This function acts as a factory to retrieve the appropriate quantization configuration based on the 'quantization' setting in the input config object. It dispatches to specific helper functions for different quantization types like 'int8', 'intmp', 'aqt_fp8', etc.",
            "usage": "Call this function with a configuration object (e.g., `MaxText.common_types.Config`). It returns a quantization configuration object, a specific string ('fp8', 'nanoo_fp8'), or None if quantization is disabled. The returned value is then used to configure the model's quantization behavior."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_convert_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_convert_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.CONVERT)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Checks if a given quantization configuration object is active and in 'CONVERT' mode.",
            "input": {
                "shape": "N/A",
                "dtype": "Object (e.g., AqtQuantization instance)"
            },
            "processing_steps": [
                "Check if the input 'quant' object is truthy.",
                "If it is, check if its 'quant_mode' attribute is equal to aqt_flax.QuantMode.CONVERT.",
                "Return the boolean result of the logical AND operation."
            ],
            "output": {
                "shape": "Scalar (boolean)"
            },
            "dependencies": [
                "aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant": "The quantization configuration object to check. It is expected to have a 'quant_mode' attribute."
            },
            "notes": [
                "The function returns False if the 'quant' object is None or otherwise falsy, effectively handling cases where quantization is disabled."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_serve_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_serve_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.SERVE)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Checks if the provided quantization configuration object exists and is in serving mode.",
            "input": {
                "shape": "N/A",
                "dtype": "AqtQuantization object or None"
            },
            "processing_steps": [
                "Check if the input `quant` object is truthy (not None).",
                "If `quant` is truthy, check if its `quant_mode` attribute is equal to `aqt_flax.QuantMode.SERVE`.",
                "Return the boolean result of the combined check."
            ],
            "output": {
                "shape": "Scalar boolean"
            },
            "dependencies": [
                "aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant": "The quantization configuration object."
            },
            "notes": [
                "The function uses short-circuit evaluation. If `quant` is None, it returns False without accessing `quant.quant_mode`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quant_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quant_mode(quant_mode_str: str = \"train\"):\n  \"\"\"Set quant mode.\"\"\"\n  if quant_mode_str == \"train\":\n    return aqt_flax.QuantMode.TRAIN\n  elif quant_mode_str == \"serve\":\n    return aqt_flax.QuantMode.SERVE\n  elif quant_mode_str == \"convert\":\n    return aqt_flax.QuantMode.CONVERT\n  else:\n    raise ValueError(f\"Invalid quantization mode {quant_mode_str}.\")\n  return None",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Maps a string representation of a quantization mode ('train', 'serve', 'convert') to the corresponding `aqt_flax.QuantMode` enum value.",
            "input": {
                "shape": "N/A",
                "dtype": "string"
            },
            "processing_steps": [
                "Check if the input string `quant_mode_str` is 'train', 'serve', or 'convert'.",
                "Return the corresponding `aqt_flax.QuantMode` enum member.",
                "Raise a ValueError if the input string is not one of the valid options."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.flax.aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant_mode_str": "A string specifying the quantization mode. Must be one of 'train', 'serve', or 'convert'. Defaults to 'train'."
            },
            "notes": [
                "The function raises a `ValueError` for any input string other than the three accepted values.",
                "The final `return None` statement is unreachable."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_quantization(config: Config, quant_mode_str: str = \"train\"):\n  \"\"\"Configure quantization based on user config and quant mode.\"\"\"\n  if config.use_qwix_quantization:\n    return None\n  quant_cfg = _get_quant_config(config)\n  if quant_cfg:\n    if quant_cfg == \"fp8\":\n      return Fp8Quantization()\n    elif quant_cfg == \"nanoo_fp8\":\n      return NANOOFp8Quantization()\n    quant_mode = get_quant_mode(quant_mode_str)\n    replicate_scale = config.replicate_quant_scale if config.replicate_quant_scale else False\n    return AqtQuantization(quant_dg=quant_cfg, quant_mode=quant_mode, replicate_scale=replicate_scale)\n  return None",
        "analysis": {
            "functionality": "This function acts as a factory to create and configure a quantization object based on the provided configuration and mode string. It supports multiple quantization types including FP8, NANOO FP8, and AQT.",
            "usage": "Call this function with a `Config` object and an optional `quant_mode_str` ('train', 'serve', 'convert'). It will return an appropriate quantization configuration object (e.g., `Fp8Quantization`, `AqtQuantization`) or `None` if quantization is disabled or handled by another library (qwix)."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#match_aqt_and_unquantized_param",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def match_aqt_and_unquantized_param(aqt_params, params):\n  \"\"\"match aqt and unquantized params\"\"\"\n  aqt_param_flat, aqt_tree_def = jax.tree_util.tree_flatten_with_path(\n      aqt_params, is_leaf=lambda x: isinstance(x, aqt_tensor.QTensor)\n  )\n  param_tree_flat, _ = jax.tree_util.tree_flatten_with_path(params)\n  aqt_paths = []\n  # Original path of quantized AQT param path.\n  param_paths = []\n\n  for aqt_k, _ in aqt_param_flat:\n    index = None\n    for index, (k, _) in enumerate(param_tree_flat):\n      path_depth = len(k)\n      # every quantized parameter has AQT.. as the leaf node\n      # AqtDotGeneral and AqtEinsum replace leaf node.\n      # Therefore, leaf node should be ignored for path matching\n      # Note: Aqt only operates on kernels so don't pop bias parameters.\n      # Ref: https://github.com/AI-Hypercomputer/maxtext/compare/main...quantize_r1\n      if k[: path_depth - 1] == aqt_k[: path_depth - 1] and k[-1].key != \"bias\":\n        aqt_paths.append(aqt_k)\n        param_paths.append(k)\n        break\n    assert index is not None\n    # since the parameter is already added, we can delete it.\n    param_tree_flat.pop(index)\n  return jax.tree_util.tree_unflatten(aqt_tree_def, param_paths)",
        "analysis": {
            "module_type": "aqt_parameter_path_mapper",
            "purpose": "Matches quantized AQT parameters to their corresponding unquantized parameters in a model's state and returns a tree containing the paths of the unquantized parameters.",
            "input": {
                "shape": "Takes two PyTrees: `aqt_params` (containing aqt_tensor.QTensor objects) and `params` (the original unquantized parameters).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Flatten the `aqt_params` PyTree using `jax.tree_util.tree_flatten_with_path`, defining `aqt_tensor.QTensor` as a leaf.",
                "Flatten the unquantized `params` PyTree using `jax.tree_util.tree_flatten_with_path`.",
                "Iterate through each path in the flattened AQT parameters.",
                "For each AQT path, search for a matching path in the flattened unquantized parameters.",
                "A match is found if the paths are identical up to the final leaf node and the unquantized parameter is not a 'bias'.",
                "Store the full path of the matched unquantized parameter.",
                "Reconstruct a PyTree using the original AQT tree structure, with the matched unquantized parameter paths as the new leaves."
            ],
            "output": {
                "shape": "A PyTree with the same structure as `aqt_params`, where each leaf is a tuple representing the full path to the corresponding unquantized parameter."
            },
            "dependencies": [
                "jax.tree_util.tree_flatten_with_path",
                "jax.tree_util.tree_unflatten",
                "aqt.jax.v2.aqt_tensor.QTensor"
            ],
            "parameters": {},
            "notes": [
                "The matching logic assumes that AQT replaces the final leaf node of a parameter path (e.g., 'kernel') with its own structure, so it compares paths excluding this final node.",
                "The function explicitly skips matching any parameter whose final path key is 'bias'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_key_paths",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_key_paths(aqt_vars, params):\n  \"\"\"Generate a list of paths which have aqt state\"\"\"\n  aqt_to_unquantized_key_path = match_aqt_and_unquantized_param(aqt_vars, params)\n  aqt_key_paths, _ = jax.tree_util.tree_flatten(aqt_to_unquantized_key_path, is_leaf=lambda x: isinstance(x, tuple))\n  return list(aqt_key_paths)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Identifies and returns the PyTree key paths for parameters that have a corresponding AQT (quantization) state.",
            "input": {
                "shape": "Takes two PyTrees: `aqt_vars` (containing AQT state) and `params` (the original model parameters).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `match_aqt_and_unquantized_param` to create a PyTree mapping AQT variables to their corresponding unquantized parameter paths.",
                "Flattens the resulting PyTree using `jax.tree_util.tree_flatten` to extract all the key path tuples.",
                "Converts the flattened result into a list."
            ],
            "output": {
                "shape": "A list of tuples, where each tuple represents a key path to a parameter with AQT state."
            },
            "dependencies": [
                "match_aqt_and_unquantized_param",
                "jax.tree_util.tree_flatten"
            ],
            "parameters": {},
            "notes": [
                "This function is a helper used to find which parameters have been quantized, for example, to later remove the original unquantized weights to save memory."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#remove_quantized_params",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def remove_quantized_params(params, aqt_vars):\n  \"\"\"Remove param values with aqt tensors to Null to optimize memory.\"\"\"\n  quantized_param_paths = _get_aqt_key_paths(aqt_vars, params)\n  tree_flat, tree_struct = tree_flatten_with_path(params)\n  for i, (k, v) in enumerate(tree_flat):\n    if k in quantized_param_paths:\n      v = {}\n    tree_flat[i] = v\n  return tree_unflatten(tree_struct, tree_flat)",
        "analysis": {
            "module_type": "parameter_pruning_utility",
            "purpose": "Replaces the values of parameters that have corresponding AQT quantization states with empty dictionaries to optimize memory.",
            "input": {
                "shape": "Input 1 ('params'): A PyTree of model parameters. Input 2 ('aqt_vars'): A PyTree containing AQT quantization state variables.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Identifies the paths of quantized parameters by calling `_get_aqt_key_paths`.",
                "Flattens the input `params` PyTree into a list of (path, value) pairs using `tree_flatten_with_path`.",
                "Iterates through the flattened list of parameters.",
                "If a parameter's path is found in the list of quantized paths, its value is replaced with an empty dictionary `{}`.",
                "Reconstructs and returns the modified PyTree using `tree_unflatten`."
            ],
            "output": {
                "shape": "A PyTree with the same structure as the input `params`, where the values of quantized parameters have been replaced by empty dictionaries."
            },
            "dependencies": [
                "_get_aqt_key_paths",
                "jax.tree_util.tree_flatten_with_path",
                "jax.tree_util.tree_unflatten"
            ],
            "parameters": {},
            "notes": [
                "The primary goal of this function is to reduce memory footprint by discarding the original high-precision parameter values after they have been quantized.",
                "The function identifies which parameters to remove by matching the structure and paths of the `params` tree with the `aqt_vars` tree."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_kv_quant",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_kv_quant(config):\n  return None if not config.quantize_kvcache else KVQuant(config)",
        "analysis": {
            "module_type": "factory_function",
            "purpose": "Conditionally creates and returns a KVQuant object for Key-Value cache quantization based on a configuration flag.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the boolean value of `config.quantize_kvcache`.",
                "If `True`, instantiate the `KVQuant` class with the provided `config` and return the instance.",
                "If `False`, return `None`."
            ],
            "output": {
                "shape": "An instance of the KVQuant class or None."
            },
            "dependencies": [
                "MaxText.inference.kvcache.KVQuant"
            ],
            "parameters": {
                "config.quantize_kvcache": "A boolean flag that determines whether to enable Key-Value cache quantization."
            },
            "notes": [
                "This function acts as a factory for the KVQuant object, centralizing the logic for enabling or disabling KV cache quantization."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NvidaFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NvidaFp8Provider(qwix.QtProvider):\n  \"\"\"Wraps nn.Fp8DirectDotGeneralOp with Qwix's provider interface.\"\"\"\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.Fp8DirectDotGeneralOp(name=op_id)(*args, **kwargs)\n\n  def einsum(self, *args, **kwargs):\n    rule, op_id = self._get_current_rule_and_op_id(\"einsum\")\n    if rule is None:\n      return jnp.einsum(*args, **kwargs)\n    return nn.Fp8Einsum(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "nvidia_fp8_quantization_provider",
            "purpose": "Provides an interface for the Qwix quantization framework to replace standard JAX operations with NVIDIA-specific FP8 implementations from Flax.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax",
                "jax.numpy",
                "flax.linen.Fp8DirectDotGeneralOp",
                "flax.linen.Fp8Einsum"
            ],
            "parameters": {},
            "notes": [
                "This class inherits from `qwix.QtProvider` and is used to conditionally apply FP8 quantization based on rules defined within the Qwix framework.",
                "It overrides `dot_general` and `einsum` to intercept these calls and potentially replace them with their FP8 equivalents."
            ],
            "methods": {
                "dot_general": {
                    "purpose": "Conditionally replaces `jax.lax.dot_general` with `nn.Fp8DirectDotGeneralOp` if a quantization rule is active for the current operation.",
                    "input": {
                        "shape": "Variable, corresponds to the inputs of `jax.lax.dot_general`.",
                        "dtype": "Variable, corresponds to the inputs of `jax.lax.dot_general`."
                    },
                    "processing_steps": [
                        "Check for an active quantization rule for 'dot_general' using `_get_current_rule_and_op_id`.",
                        "If no rule is found, call the original `jax.lax.dot_general`.",
                        "If a rule is found, instantiate and call `nn.Fp8DirectDotGeneralOp` with the same arguments."
                    ],
                    "output": {
                        "shape": "Variable, corresponds to the output of the dot product operation."
                    },
                    "dependencies": [
                        "jax.lax.dot_general",
                        "nn.Fp8DirectDotGeneralOp"
                    ],
                    "notes": [
                        "The decision to quantize is delegated to the underlying Qwix provider's rule matching mechanism."
                    ]
                },
                "einsum": {
                    "purpose": "Conditionally replaces `jnp.einsum` with `nn.Fp8Einsum` if a quantization rule is active for the current operation.",
                    "input": {
                        "shape": "Variable, corresponds to the inputs of `jnp.einsum`.",
                        "dtype": "Variable, corresponds to the inputs of `jnp.einsum`."
                    },
                    "processing_steps": [
                        "Check for an active quantization rule for 'einsum' using `_get_current_rule_and_op_id`.",
                        "If no rule is found, call the original `jnp.einsum`.",
                        "If a rule is found, instantiate and call `nn.Fp8Einsum` with the same arguments."
                    ],
                    "output": {
                        "shape": "Variable, corresponds to the output of the einsum operation."
                    },
                    "dependencies": [
                        "jnp.einsum",
                        "nn.Fp8Einsum"
                    ],
                    "notes": [
                        "The decision to quantize is delegated to the underlying Qwix provider's rule matching mechanism."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Provider(qwix.QtProvider):\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.NANOOFp8DotGeneralOp(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "fp8_quantization_provider",
            "purpose": "Provides a conditional implementation for `dot_general` operations, dispatching to either the standard JAX implementation or a specialized NANOO FP8 quantized version based on active quantization rules from the Qwix framework.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The class is instantiated, typically with a list of quantization rules."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax",
                "flax.linen.nn.NANOOFp8DotGeneralOp"
            ],
            "parameters": {
                "rules": "A list of `qwix.QtRule` objects that determine when to apply quantization, passed during instantiation of the parent `qwix.QtProvider`."
            },
            "notes": [
                "This provider is specifically designed for NANOO FP8 quantization, which targets AMD MI300/MI325 GPUs.",
                "It overrides methods from `qwix.QtProvider` to intercept specific JAX operations."
            ],
            "methods": {
                "dot_general": {
                    "purpose": "Conditionally executes either a standard JAX `dot_general` or a quantized `nn.NANOOFp8DotGeneralOp` based on the active quantization rule.",
                    "input": {
                        "shape": "Variable, accepts the same arguments as `jax.lax.dot_general` (e.g., `(lhs, rhs, dimension_numbers, ...)`).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieves the current quantization rule and operation ID for 'dot_general' using `_get_current_rule_and_op_id`.",
                        "If no rule is found, calls `jax.lax.dot_general` with the provided arguments.",
                        "If a rule is found, instantiates and calls `nn.NANOOFp8DotGeneralOp` with the provided arguments."
                    ],
                    "output": {
                        "shape": "The shape resulting from the dot-general operation, identical to the output of a standard `jax.lax.dot_general` call."
                    },
                    "dependencies": [
                        "jax.lax.dot_general",
                        "flax.linen.nn.NANOOFp8DotGeneralOp"
                    ],
                    "notes": [
                        "The decision to quantize is delegated to the Qwix framework's rule-matching system via the `_get_current_rule_and_op_id` internal method."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quantization_rule",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quantization_rule(config: Config):\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.int8,\n          act_qtype=jnp.int8,\n          bwd_qtype=jnp.int8,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_full\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e5m2,\n          weight_calibration_method=config.quantization_calibration_method,\n          act_calibration_method=config.quantization_calibration_method,\n          bwd_calibration_method=config.quantization_calibration_method,\n          op_names=(\"dot_general\", \"gmm\"),\n      )\n    case \"fp8_gpu\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_nanoo\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"\":\n      return None",
        "analysis": {
            "module_type": "quantization_rule_factory",
            "purpose": "Creates and returns a specific `qwix.QtRule` quantization configuration based on the `quantization` string in the input config.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Read the `quantization` attribute from the input `config` object.",
                "Use a match-case statement to select the appropriate quantization configuration.",
                "For recognized quantization types ('int8', 'fp8', 'fp8_full', 'fp8_gpu', 'fp8_nanoo'), instantiate a `qwix.QtRule` with specific data types and parameters.",
                "If the `quantization` string is empty, return `None`."
            ],
            "output": {
                "shape": "Returns a `qwix.QtRule` object or `None`."
            },
            "dependencies": [
                "qwix.QtRule",
                "jax.numpy",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantization": "A string specifying the type of quantization to apply (e.g., 'int8', 'fp8').",
                "config.quantization_local_shard_count": "Used to calculate the backward pass weight gradient tile size.",
                "config.quantization_calibration_method": "Specifies the calibration method for weights, activations, and backward pass in the 'fp8_full' case."
            },
            "notes": [
                "This function acts as a factory for generating quantization rules for the `qwix` library.",
                "The `module_path` is hardcoded to 'decoder/.*layers.*', targeting the decoder layers of a model.",
                "If the `quantization` string in the config is empty, no rule is created, and `None` is returned."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_qt_provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_qt_provider(config):\n  \"\"\"Get quantization rules based on the config.\"\"\"\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_full\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_gpu\":\n      return NvidaFp8Provider([get_quantization_rule(config)])\n    case \"fp8_nanoo\":\n      return NANOOFp8Provider([get_quantization_rule(config)])\n  return None",
        "analysis": {
            "module_type": "quantization_provider_factory",
            "purpose": "Creates and returns a quantization provider instance based on the quantization type specified in the configuration object.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Read the `quantization` attribute from the input `config` object.",
                "Use a `match` statement to select the appropriate quantization provider based on the `config.quantization` string.",
                "Call `get_quantization_rule(config)` to generate the specific quantization rules.",
                "Instantiate the corresponding provider (`qwix.QtProvider`, `NvidaFp8Provider`, or `NANOOFp8Provider`) with the generated rule.",
                "Return the provider instance, or `None` if no case matches."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "NvidaFp8Provider",
                "NANOOFp8Provider",
                "get_quantization_rule",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantization": "A string specifying the quantization method. Supported values include 'int8', 'fp8', 'fp8_full', 'fp8_gpu', and 'fp8_nanoo'."
            },
            "notes": [
                "This function acts as a factory for creating quantization providers.",
                "If the `config.quantization` string does not match any of the predefined cases, the function returns `None`.",
                "The 'fp8_gpu' and 'fp8_nanoo' options return specialized providers for NVIDIA and AMD GPUs, respectively."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#maybe_quantize_model",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def maybe_quantize_model(model, config):\n  \"\"\"Quantize the model if quantization is enabled.\"\"\"\n  if config.use_qwix_quantization:\n    quantization_provider = get_qt_provider(config)\n    if quantization_provider:\n      model = qwix.quantize_model(model, quantization_provider)\n  return model",
        "analysis": {
            "functionality": "This function conditionally applies quantization to a given model using the Qwix library. The decision to quantize and the specific quantization provider used are determined by the provided configuration object.",
            "usage": "To use this function, pass a model object and a configuration object. If `config.use_qwix_quantization` is true, the function will attempt to quantize the model using a provider determined by `config.quantization` and return the modified model. Otherwise, it returns the original, unmodified model. \n\nInputs:\n- `model`: The model object (e.g., a Flax Linen module) to be potentially quantized.\n- `config`: A configuration object containing quantization settings, specifically `use_qwix_quantization` and `quantization` type.\n\nOutput:\n- A model object, which is either the original model or its quantized version."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#jax_chunk_gated_delta_rule",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "def jax_chunk_gated_delta_rule(\n    query: Array,\n    key: Array,\n    value: Array,\n    g: Array,\n    beta: Array,\n    chunk_size: int = 64,\n    initial_state: None | Array = None,\n    use_qk_norm_in_gdn: bool = False,\n) -> tuple[Array, None | Array]:\n  \"\"\"\n  A JAX implementation of the chunked Gated Delta Rule, a parallel scan algorithm.\n  This function implements the core recurrent logic of the Gated Delta Network in\n  a hardware-efficient way by splitting the sequence into chunks and using\n  jax.lax.scan for the recurrent part.\n\n  Tensor Shape Abbreviations:\n    B: batch_size, S: sequence_length, H: num_heads,\n    D_k: key/query_head_dim, D_v: value_head_dim,\n    N: num_chunks, C: chunk_size\n\n  Args:\n    query: Query tensor. Shape (B, S, H, D_k)\n    key: Key tensor. Shape (B, S, H, D_k)\n    value: Value tensor. Shape (B, S, H, D_v)\n    g: Log decay tensor. Shape (B, S, H)\n    beta: Gate tensor. Shape (B, S, H)\n    chunk_size: The size of each chunk for processing.\n    initial_state: Optional initial state for the recurrence. Shape (B, H, D_k, D_v)\n    use_qk_norm_in_gdn: Whether to apply L2 normalization to query and key.\n\n  Returns:\n    Output tensor. Shape (B, S, H, D_v)\n    Final recurrent state. Shape (B, H, D_k, D_v) or None\n  \"\"\"\n\n  # =========================================================================\n  # STAGE 1: PREPARATION & PADDING\n  # =========================================================================\n  initial_dtype = query.dtype\n  if use_qk_norm_in_gdn:\n    query = l2norm(query, dim=-1, eps=1e-6)\n    key = l2norm(key, dim=-1, eps=1e-6)\n\n  # Transpose (B, S, H, D) -> (B, H, S, D)\n  query = jnp.transpose(query, (0, 2, 1, 3)).astype(jnp.float32)\n  key = jnp.transpose(key, (0, 2, 1, 3)).astype(jnp.float32)\n  value = jnp.transpose(value, (0, 2, 1, 3)).astype(jnp.float32)\n  # Transpose (B, S, H) -> (B, H, S)\n  beta = jnp.transpose(beta, (0, 2, 1)).astype(jnp.float32)\n  g = jnp.transpose(g, (0, 2, 1)).astype(jnp.float32)\n\n  batch_size, num_heads, sequence_length, k_head_dim = key.shape\n  v_head_dim = value.shape[-1]\n  pad_size = (chunk_size - sequence_length % chunk_size) % chunk_size\n\n  # Padding to make sequence_length divisible by chunk_size\n  if pad_size > 0:\n    query = jnp.pad(query, ((0, 0), (0, 0), (0, pad_size), (0, 0)))  # (B, H, S_padded, D_k)\n    key = jnp.pad(key, ((0, 0), (0, 0), (0, pad_size), (0, 0)))  # (B, H, S_padded, D_k)\n    value = jnp.pad(value, ((0, 0), (0, 0), (0, pad_size), (0, 0)))  # (B, H, S_padded, D_v)\n    beta = jnp.pad(beta, ((0, 0), (0, 0), (0, pad_size)))  # (B, H, S_padded)\n    g = jnp.pad(g, ((0, 0), (0, 0), (0, pad_size)))  # (B, H, S_padded)\n\n  total_sequence_length = sequence_length + pad_size\n  # query shape: (B, H, S_padded, D_k)\n  scale = jax.lax.rsqrt(jnp.array(query.shape[-1]).astype(jnp.float32))\n  query = query * scale\n\n  v_beta = value * jnp.expand_dims(beta, -1)  # (B, H, S_padded, D_v)\n  k_beta = key * jnp.expand_dims(beta, -1)  # (B, H, S_padded, D_k)\n\n  # Reshape to chunks\n  num_chunks = total_sequence_length // chunk_size\n  # query_c shape: (B, H, N, C, D_k)\n  query_c = query.reshape(batch_size, num_heads, num_chunks, chunk_size, k_head_dim)\n  key_c = key.reshape(batch_size, num_heads, num_chunks, chunk_size, k_head_dim)\n  k_beta_c = k_beta.reshape(batch_size, num_heads, num_chunks, chunk_size, k_head_dim)\n  v_beta_c = v_beta.reshape(batch_size, num_heads, num_chunks, chunk_size, v_head_dim)\n  g_c = g.reshape(batch_size, num_heads, num_chunks, chunk_size)  # (B, H, N, C)\n\n  mask = jnp.triu(jnp.ones((chunk_size, chunk_size), dtype=bool), k=0)  # (C, C)\n\n  # =========================================================================\n  # STAGE 2: INTRA-CHUNK CALCULATION (PARALLEL)\n  # =========================================================================\n  # g_cumsum shape: (B, H, N, C)\n  g_cumsum = jnp.cumsum(g_c, axis=-1)\n  # g_diff shape: (B, H, N, C, C)\n  g_diff = jnp.expand_dims(g_cumsum, -1) - jnp.expand_dims(g_cumsum, -2)\n\n  # Apply tril to zero out the upper triangle of g_diff. This is crucial because\n  # the upper triangle contains large positive values that would cause exp() to overflow.\n  g_diff_tril = jnp.tril(g_diff)\n\n  # Exponentiate the lower triangular g_diff. Since these values are non-positive,\n  # exp() will not overflow and will produce values between 0 and 1.\n  g_diff_exp = jnp.exp(g_diff_tril).astype(jnp.float32)\n\n  # The result g_diff_exp is already lower triangular and serves as the decay_mask.\n  # decay_mask shape: (B, H, N, C, C)\n  decay_mask = g_diff_exp\n\n  # --- Precompute within-chunk attention ---\n  # NOTE: Precision set to HIGHEST for numerical accuracy.\n  prec = jax.lax.Precision.HIGHEST\n  # attn shape: (B, H, N, C, C)\n  attn = -jnp.matmul(k_beta_c, jnp.swapaxes(key_c, -1, -2), precision=prec) * decay_mask\n  attn = jnp.where(mask, 0.0, attn)\n\n  # Iterative refinement of the intra-chunk attention.\n  # This loop is equivalent to inverting (I - A) where A is the lower triangular part of attn.\n  def inner_attn_body(i, attn_val):\n    # indices: (C,)\n    indices = jnp.arange(chunk_size)\n    # col_mask: (C,)\n    col_mask = indices < i\n    # row: (B, H, N, C)\n    row = attn_val[..., i, :] * col_mask\n    # sub_mask: (C, C)\n    sub_mask = jnp.expand_dims(indices < i, -1) & (indices < i)\n    # sub: (B, H, N, C, C)\n    sub = attn_val * sub_mask\n    # row_exp: (B, H, N, C, 1)\n    row_exp = jnp.expand_dims(row, -1)\n    # term: (B, H, N, C, C)\n    term = row_exp * sub\n    # summed: (B, H, N, C)\n    summed = jnp.sum(term, axis=-2)\n    # update_val: (B, H, N, C)\n    update_val = row + summed\n    # original_row: (B, H, N, C)\n    original_row = attn_val[..., i, :]\n    # new_row: (B, H, N, C)\n    new_row = jnp.where(col_mask, update_val, original_row)\n    return attn_val.at[..., i, :].set(new_row)\n\n  attn = jax.lax.fori_loop(1, chunk_size, inner_attn_body, attn)\n\n  attn = attn + jnp.eye(chunk_size, dtype=attn.dtype)  # (B, H, N, C, C)\n  # value_intra shape: (B, H, N, C, D_v)\n  value_intra = jnp.matmul(attn, v_beta_c, precision=prec)\n  # k_cumdecay shape: (B, H, N, C, D_k)\n  k_cumdecay = jnp.matmul(attn, (k_beta_c * jnp.expand_dims(jnp.exp(g_cumsum), -1)), precision=prec)\n  # --- End Precompute ---\n\n  output_final_state = initial_state is not None\n  if initial_state is None:\n    # last_recurrent_state shape: (B, H, D_k, D_v)\n    last_recurrent_state = jnp.zeros((batch_size, num_heads, k_head_dim, v_head_dim), dtype=value_intra.dtype)\n  else:\n    last_recurrent_state = initial_state.astype(value_intra.dtype)\n\n  # mask_inter shape: (C, C)\n  mask_inter = jnp.triu(jnp.ones((chunk_size, chunk_size), dtype=bool), k=1)\n\n  # Transpose for scan: (B, H, N, C, D) -> (N, B, H, C, D)\n  query_scan = jnp.transpose(query_c, (2, 0, 1, 3, 4))\n  key_scan = jnp.transpose(key_c, (2, 0, 1, 3, 4))\n  value_scan = jnp.transpose(value_intra, (2, 0, 1, 3, 4))\n  k_cumdecay_scan = jnp.transpose(k_cumdecay, (2, 0, 1, 3, 4))\n  # Transpose for scan: (B, H, N, C) -> (N, B, H, C)\n  g_scan = jnp.transpose(g_cumsum, (2, 0, 1, 3))\n  decay_mask_scan = jnp.transpose(decay_mask, (2, 0, 1, 3, 4))\n\n  xs = (query_scan, key_scan, value_scan, k_cumdecay_scan, g_scan, decay_mask_scan)\n\n  # =========================================================================\n  # STAGE 3: INTER-CHUNK RECURRENCE (SEQUENTIAL VIA SCAN)\n  # =========================================================================\n  def scan_body(prev_state, x):\n    q_i, k_i, v_i, k_cumdecay_i, g_i, decay_mask_i = x\n    # prev_state shape: (B, H, D_k, D_v)\n    last_recurrent_state = prev_state\n    prec = jax.lax.Precision.HIGHEST\n\n    # Intra-chunk attention for the current chunk\n    # attn_i shape: (B, H, C, C)\n    attn_i = jnp.matmul(q_i, jnp.swapaxes(k_i, -1, -2), precision=prec) * decay_mask_i\n    attn_i = jnp.where(mask_inter, 0.0, attn_i)\n\n    # Interaction with the recurrent state\n    # v_prime shape: (B, H, C, D_v)\n    v_prime = jnp.matmul(k_cumdecay_i, last_recurrent_state, precision=prec)\n    # v_new shape: (B, H, C, D_v)\n    v_new = v_i - v_prime\n\n    # g_i is cumulative sum, so exp(g_i) is the decay factor\n    g_i_exp = jnp.exp(g_i)\n    # attn_inter shape: (B, H, C, D_v)\n    attn_inter = jnp.matmul(q_i * jnp.expand_dims(g_i_exp, -1), last_recurrent_state, precision=prec)\n\n    # core_attn_out_i shape: (B, H, C, D_v)\n    core_attn_out_i = attn_inter + jnp.matmul(attn_i, v_new, precision=prec)\n\n    # Update the recurrent state\n    # g_i_last_exp shape: (B, H, 1, 1)\n    g_i_last_exp = jnp.exp(g_i[..., -1, None, None])\n    # new_last_recurrent_state shape: (B, H, D_k, D_v)\n    new_last_recurrent_state = last_recurrent_state * g_i_last_exp\n\n    # g_diff_exp shape: (B, H, C, 1)\n    g_diff_exp = jnp.expand_dims(jnp.exp(jnp.expand_dims(g_i[..., -1], -1) - g_i), -1)\n    # k_i_g_diff shape: (B, H, C, D_k)\n    k_i_g_diff = k_i * g_diff_exp\n\n    # Update term shape: (B, H, D_k, D_v)\n    update_term = jnp.matmul(jnp.swapaxes(k_i_g_diff, -1, -2), v_new, precision=prec)\n    new_last_recurrent_state = new_last_recurrent_state + update_term\n\n    return new_last_recurrent_state, core_attn_out_i\n\n  # final_state shape: (B, H, D_k, D_v)\n  # core_attn_out_stacked shape: (N, B, H, C, D_v)\n  final_state, core_attn_out_stacked = jax.lax.scan(scan_body, last_recurrent_state, xs)\n\n  # =========================================================================\n  # STAGE 4: FINALIZATION\n  # =========================================================================\n  # core_attn_out shape: (B, H, N, C, D_v)\n  core_attn_out = jnp.transpose(core_attn_out_stacked, (1, 2, 0, 3, 4))\n\n  # core_attn_out shape: (B, H, S_padded, D_v)\n  core_attn_out = core_attn_out.reshape(batch_size, num_heads, -1, v_head_dim)\n  # Trim padding: (B, H, S, D_v)\n  core_attn_out = core_attn_out[:, :, :sequence_length, :]\n\n  # Transpose back to (B, S, H, D_v)\n  core_attn_out = jnp.transpose(core_attn_out, (0, 2, 1, 3)).astype(initial_dtype)\n\n  return core_attn_out, final_state if output_final_state else None",
        "analysis": {
            "module_type": "gated_delta_rule_chunked",
            "purpose": "Implements the core recurrent logic of the Gated Delta Network using a hardware-efficient chunked parallel scan algorithm in JAX.",
            "input": {
                "shape": "query: [B, S, H, D_k], key: [B, S, H, D_k], value: [B, S, H, D_v], g: [B, S, H], beta: [B, S, H], initial_state: [B, H, D_k, D_v] (optional). B=batch, S=sequence, H=heads, D_k=key_dim, D_v=value_dim.",
                "dtype": "float32 or bfloat16 (internally processed as float32)"
            },
            "processing_steps": [
                "Optionally apply L2 normalization to query and key tensors.",
                "Transpose input tensors from (B, S, H, D) to (B, H, S, D) for processing.",
                "Pad inputs along the sequence dimension to be divisible by `chunk_size`.",
                "Scale the query tensor by the inverse square root of its head dimension.",
                "Apply the `beta` gate to `value` and `key` tensors.",
                "Reshape tensors into chunks: (B, H, S_padded, D) -> (B, H, N, C, D), where N is num_chunks and C is chunk_size.",
                "Perform parallel intra-chunk calculations, including creating a decay mask and precomputing within-chunk attention.",
                "Refine the intra-chunk attention matrix using a `jax.lax.fori_loop`.",
                "Calculate intermediate values `value_intra` and `k_cumdecay` using the refined attention.",
                "Prepare tensors for `jax.lax.scan` by transposing the chunk dimension to the front.",
                "Execute the inter-chunk recurrence sequentially using `jax.lax.scan` with a custom `scan_body` function.",
                "The `scan_body` updates the recurrent state and computes the output for each chunk.",
                "Reshape the scanned output back into a single sequence.",
                "Trim the padding from the sequence dimension.",
                "Transpose the final output back to the original (B, S, H, D_v) format and cast to the initial data type."
            ],
            "output": {
                "shape": "A tuple containing: (1) Output tensor of shape [B, S, H, D_v], and (2) Final recurrent state of shape [B, H, D_k, D_v] or None."
            },
            "dependencies": [
                "jax.lax.scan",
                "jax.lax.fori_loop",
                "jnp.transpose",
                "jnp.pad",
                "jnp.reshape",
                "jnp.matmul",
                "jnp.cumsum",
                "jnp.exp",
                "l2norm"
            ],
            "parameters": {
                "chunk_size": "The size of each chunk for parallel processing, affecting performance and memory usage.",
                "use_qk_norm_in_gdn": "A boolean flag to determine whether to apply L2 normalization to query and key tensors before processing.",
                "initial_state": "An optional initial state for the recurrence. If not provided, a zero state is used."
            },
            "notes": [
                "The function is divided into four main stages: Preparation, Intra-Chunk Calculation (parallel), Inter-Chunk Recurrence (sequential), and Finalization.",
                "It uses a combination of parallel operations within chunks and a sequential `jax.lax.scan` across chunks to balance efficiency and correctness for the recurrent computation.",
                "Numerical precision is explicitly set to `jax.lax.Precision.HIGHEST` for matrix multiplications to ensure accuracy.",
                "The final recurrent state is only returned if an `initial_state` is provided as input."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextRMSNorm",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextRMSNorm(nnx.Module):\n  \"\"\"\n  Used for input and post attention layernorms\n  in Qwen3NextDecoderLayer.\n\n  This normalization layer is specific to Qwen3-Next. Key characteristics:\n  1.  The learnable scale parameter `weight` is initialized to ZEROS.\n  2.  The scale is applied as `(1.0 + self.weight)`, making the initial scale effectively 1.0.\n      This matches the PyTorch implementation of Qwen3NextRMSNorm.\n  3.  It is NOT a zero-centered normalization (as in the blog); it still uses the root mean square of the inputs.\n      The standard `MaxText.layers.normalizations.RMSNorm` also does not center the data.\n  4.  This differs from the standard MaxText `RMSNorm`\n      (MaxText.layers.normalizations.RMSNorm) which initializes its scale to ONES\n      and applies it multiplicatively (`y * scale`).\n  \"\"\"\n\n  def __init__(self, num_features: int, eps: float, dtype: DType, weight_dtype: DType, *, rngs: nnx.Rngs):\n    self.num_features = num_features\n    self.eps = eps\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n\n    self.weight = nnx.Param(linen_initializers.zeros(rngs.params(), (self.num_features,), self.weight_dtype))\n\n  def __call__(self, x: Array) -> Array:\n    \"\"\"Applies RMSNorm to the input tensor.\"\"\"\n    weight = self.weight.value\n    x_dtype = x.dtype\n    x = x.astype(jnp.float32)\n    variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)\n    x = x * jax.lax.rsqrt(variance + self.eps)\n    # Add 1.0 to the learnable weight\n    x = x * (1.0 + weight.astype(jnp.float32))\n    return x.astype(x_dtype)",
        "analysis": {
            "functionality": "Implements a specific Root Mean Square Normalization (RMSNorm) layer tailored for Qwen3-Next models. This variant initializes its learnable scale parameter (`weight`) to zeros and applies the scaling as `(1.0 + weight)`, differing from standard RMSNorm implementations which typically initialize weights to ones and apply them multiplicatively.",
            "usage": "Instantiate the class with the number of features, epsilon, and data types. Call the instance with an input tensor to apply the normalization. The input tensor should have the feature dimension as its last axis. For example: `norm_layer = Qwen3NextRMSNorm(num_features=1024, ...); output = norm_layer(input_tensor)` where `input_tensor` has a shape like `(batch, seq_len, 1024)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextRMSNormGated",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextRMSNormGated(nnx.Module):\n  \"\"\"\n  This applies RMS Normalization and then a gated activation function (SiLU).\n  This is used within the Qwen3NextGatedDeltaNet.\n\n  Attributes:\n    num_features: The number of features in the input.\n    eps: A small epsilon value to prevent division by zero in RMSNorm.\n    dtype: The datatype of the computation.\n    weight_dtype: The datatype of the weights.\n  \"\"\"\n\n  def __init__(self, num_features: int, eps: float, dtype: DType, weight_dtype: DType, *, rngs: nnx.Rngs):\n    self.num_features = num_features\n    self.eps = eps\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n\n    self.weight = nnx.Param(nnx.initializers.ones(rngs.params(), (self.num_features,), self.weight_dtype))\n\n  def __call__(self, hidden_states: Array, gate: Array) -> Array:\n    \"\"\"\n    Applies RMSNorm and then a SiLU gate.\n\n    Args:\n      hidden_states: The input array to be normalized (o). Shape: (..., F)\n      gate: The gating array for the activation (z). Shape: (..., F)\n            where F is num_features.\n\n    Returns:\n      The normalized and gated output array. Shape: (..., F)\n    \"\"\"\n    weight = self.weight.value\n\n    # RMS Normalization logic\n    hidden_states_f32 = hidden_states.astype(jnp.float32)\n    variance = jnp.mean(jnp.square(hidden_states_f32), axis=-1, keepdims=True)\n    normalized_states = hidden_states_f32 * jax.lax.rsqrt(variance + self.eps)\n    normalized_states = normalized_states * weight.astype(jnp.float32)\n\n    # Gated Activation using SiLU (Sigmoid-weighted Linear Unit)\n    gated_states = normalized_states * jax.nn.silu(gate.astype(jnp.float32))\n\n    return gated_states.astype(self.dtype)",
        "analysis": {
            "functionality": "The `Qwen3NextRMSNormGated` class is a neural network module that first applies Root Mean Square (RMS) Normalization to an input tensor (`hidden_states`) and then applies a gated activation function (SiLU) using a second input tensor (`gate`).",
            "usage": "Instantiate the class with the number of features, epsilon, and data types. Call the instance with two tensors: `hidden_states` to be normalized and `gate` for the activation. Both tensors should have the shape `(..., num_features)`. The module returns a tensor of the same shape, which is the result of the normalization and gating operations."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextGatedDeltaNet",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextGatedDeltaNet(nnx.Module):\n  \"\"\"\n  This module implements the full end-to-end logic of a Gated Delta Network layer.\n\n  End-to-End Equations Implemented:\n  Let `x` be the input `hidden_states`.\n\n  Step A: Input Projections\n  1. (q_raw, k_raw, v_raw, z) = Linear_qkvz(x)\n  2. (b, a) = Linear_ba(x)\n\n  Step B: 1D Convolution\n  1. qkv_conv = silu(Conv1D(concatenate(q_raw, k_raw, v_raw)))\n  2. (q, k, v) = split(qkv_conv)\n\n  Step C: Gated Delta Rule (Recurrent Core)\n  1. Gates: \u03b2=sigmoid(b), g = -exp(A_log) * softplus(a + dt_bias)\n  2. Core Calculation: core_attn_out = jax_chunk_gated_delta_rule(q, k, v, g, \u03b2)\n\n  Step D: Final Output Stage\n  1. y = RMSNorm(core_attn_out) * silu(z)\n  2. output = Linear_out(y)\n\n  Attributes:\n    config: MaxText configuration object.\n    dtype: The datatype of the computation.\n  \"\"\"\n\n  def __init__(self, config: Config, dtype: DType = jnp.float32, *, rngs: nnx.Rngs):\n    self.config = config\n    self.dtype = dtype\n    cfg = self.config\n\n    in_features = cfg.emb_dim\n    self.num_v_heads = cfg.gdn_num_value_heads\n    self.num_k_heads = cfg.gdn_num_key_heads\n    self.head_k_dim = cfg.gdn_key_head_dim\n    self.head_v_dim = cfg.gdn_value_head_dim\n    self.key_dim = self.head_k_dim * self.num_k_heads\n    self.value_dim = self.head_v_dim * self.num_v_heads\n    conv_dim = self.key_dim * 2 + self.value_dim\n    conv_kernel_size = cfg.gdn_conv_kernel_dim\n\n    # Submodule instantiations\n    self.in_proj_qkvz = linears.DenseGeneral(\n        in_features_shape=in_features,\n        out_features_shape=(self.key_dim * 2 + self.value_dim * 2),\n        dtype=cfg.dtype,\n        kernel_axes=(\"embed\", \"mlp\"),\n        matmul_precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n    self.in_proj_ba = linears.DenseGeneral(\n        in_features_shape=in_features,\n        out_features_shape=(self.num_v_heads * 2),\n        dtype=cfg.dtype,\n        kernel_axes=(\"embed\", \"mlp\"),\n        matmul_precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n\n    self.conv1d = nnx.Conv(\n        in_features=conv_dim,\n        out_features=conv_dim,\n        kernel_size=(conv_kernel_size,),\n        feature_group_count=conv_dim,  # Depthwise\n        padding=\"CAUSAL\",\n        use_bias=False,\n        dtype=cfg.dtype,\n        precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n\n    # Initialize A_log to match torch.log(torch.uniform(0, 16))\n    def a_log_init(key, shape, dtype=jnp.float32):\n      # Sample from Uniform(epsilon, 16) to avoid log(0)\n      a_vals = jax.random.uniform(key, shape=shape, dtype=dtype, minval=1e-9, maxval=16.0)\n      return jnp.log(a_vals)\n\n    self.A_log = nnx.Param(a_log_init(rngs.params(), (self.num_v_heads,)))\n    self.dt_bias = nnx.Param(nnx.initializers.ones(rngs.params(), (self.num_v_heads,)))\n\n    self.norm = Qwen3NextRMSNormGated(\n        num_features=self.head_v_dim,  # Normalize over the head dimension (D_v)\n        eps=cfg.normalization_layer_epsilon,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        rngs=rngs,\n    )\n    self.out_proj = linears.DenseGeneral(\n        in_features_shape=self.value_dim,\n        out_features_shape=(in_features,),\n        dtype=cfg.dtype,\n        kernel_axes=(\"mlp\", \"embed\"),\n        matmul_precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n\n  def __call__(self, hidden_states: Array) -> Array:\n    cfg = self.config\n\n    # =========================================================================\n    # STEP A: Input Projections\n    # =========================================================================\n    # hidden_states shape: (B, S, E)\n    # qkvz shape: (B, S, 2*key_dim + 2*value_dim)\n    qkvz = self.in_proj_qkvz(hidden_states)\n    # ba shape: (B, S, 2*H_v)\n    ba = self.in_proj_ba(hidden_states)\n\n    # q shape: (B, S, key_dim), k shape: (B, S, key_dim), v shape: (B, S, value_dim), z shape: (B, S, value_dim)\n    q, k, v, z = jnp.split(qkvz, [self.key_dim, 2 * self.key_dim, 2 * self.key_dim + self.value_dim], axis=-1)\n    # b shape: (B, S, H_v), a shape: (B, S, H_v)\n    b, a = jnp.split(ba, [self.num_v_heads], axis=-1)\n\n    # =========================================================================\n    # STEP B: 1D Convolution\n    # =========================================================================\n    # qkv shape: (B, S, conv_dim)\n    qkv = jnp.concatenate([q, k, v], axis=-1)\n\n    # TODO(parambole): Implement caching logic for conv_state and recurrent_state\n\n    # Input to conv_layer should be (B, S, C)\n    # qkv_conv shape: (B, S, conv_dim)\n    qkv_conv = jax.nn.silu(self.conv1d(qkv).astype(jnp.float32)).astype(cfg.dtype)\n    # q_conv shape: (B, S, key_dim), k_conv shape: (B, S, key_dim), v_conv shape: (B, S, value_dim)\n    q_conv, k_conv, v_conv = jnp.split(qkv_conv, [self.key_dim, 2 * self.key_dim], axis=-1)\n\n    # Reshape for multi-head processing\n    batch, seq_len, _ = hidden_states.shape\n    # query shape: (B, S, H_k, D_k)\n    query = q_conv.reshape(batch, seq_len, self.num_k_heads, self.head_k_dim)\n    # key shape: (B, S, H_k, D_k)\n    key = k_conv.reshape(batch, seq_len, self.num_k_heads, self.head_k_dim)\n    # value shape: (B, S, H_v, D_v)\n    value = v_conv.reshape(batch, seq_len, self.num_v_heads, self.head_v_dim)\n\n    # =========================================================================\n    # STEP C: Gated Delta Rule Recurrence\n    # =========================================================================\n    A_log = self.A_log.value\n    dt_bias = self.dt_bias.value\n    # beta shape: (B, S, H_v)\n    beta = jax.nn.sigmoid(b)\n    # g shape: (B, S, H_v)\n    g = -jnp.exp(A_log.astype(jnp.float32)) * jax.nn.softplus(a.astype(jnp.float32) + dt_bias.astype(jnp.float32))\n    g = g.astype(cfg.dtype)\n\n    if self.num_v_heads > self.num_k_heads and self.num_v_heads % self.num_k_heads == 0:\n      repeats = self.num_v_heads // self.num_k_heads\n      # query shape after repeat: (B, S, H_v, D_k)\n      query = jnp.repeat(query, repeats, axis=2)\n      # key shape after repeat: (B, S, H_v, D_k)\n      key = jnp.repeat(key, repeats, axis=2)\n    elif self.num_k_heads > self.num_v_heads and self.num_k_heads % self.num_v_heads == 0:\n      # This case might occur if key/query heads are more than value heads.\n      pass  # No repeating needed for query/key in this case\n\n    # TODO(parambole): Pass and update cache state for jax_chunk_gated_delta_rule\n    # core_attn_out shape: (B, S, H_v, D_v)\n    core_attn_out, _ = jax_chunk_gated_delta_rule(\n        query, key, value, g, beta, chunk_size=cfg.gdn_chunk_size, use_qk_norm_in_gdn=cfg.use_qk_norm_in_gdn\n    )\n\n    # =========================================================================\n    # STEP D: Final Output Stage\n    # =========================================================================\n    # The normalization and gating is applied per-head on the value dimension.\n    # We first reshape the `z` tensor to match the multi-head structure of `core_attn_out`.\n    # z shape from (B, S, value_dim) -> (B, S, H_v, D_v)\n    z_reshaped = z.reshape(batch, seq_len, self.num_v_heads, self.head_v_dim)\n\n    # Apply the norm and gate. Output shape: (B, S, H_v, D_v)\n    gated_output_reshaped = self.norm(core_attn_out, z_reshaped)\n\n    # Reshape back to a single feature dimension for the final projection.\n    # Shape from (B, S, H_v, D_v) -> (B, S, value_dim)\n    gated_output = gated_output_reshaped.reshape(batch, seq_len, -1)\n\n    # Final output shape: (B, S, E)\n    output = self.out_proj(gated_output)\n\n    return output",
        "analysis": {
            "module_type": "gated_delta_network",
            "purpose": "Implements the full end-to-end logic of a Gated Delta Network (GDN) layer, a type of linear attention mechanism.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "DType (configurable, e.g., jnp.float32)"
            },
            "processing_steps": [
                "Step A: Project input hidden_states into raw q, k, v, z, b, and a tensors using two separate dense layers.",
                "Step B: Apply a 1D depthwise causal convolution to the concatenated q, k, v tensors, followed by a SiLU activation to get processed q, k, v.",
                "Step C: Calculate gate tensors (beta, g) and execute the core recurrent logic using the `jax_chunk_gated_delta_rule` function.",
                "Step D: Apply a gated RMS normalization to the recurrent output, using z as the gate, and then project the result back to the embedding dimension."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]"
            },
            "dependencies": [
                "linears.DenseGeneral",
                "nnx.Conv",
                "Qwen3NextRMSNormGated",
                "jax_chunk_gated_delta_rule"
            ],
            "parameters": {
                "emb_dim": "The embedding dimension of the input and output.",
                "gdn_num_value_heads": "The number of value heads.",
                "gdn_num_key_heads": "The number of key heads.",
                "gdn_key_head_dim": "The dimension of each key head.",
                "gdn_value_head_dim": "The dimension of each value head.",
                "gdn_conv_kernel_dim": "The kernel size for the 1D convolution.",
                "gdn_chunk_size": "The chunk size used in the `jax_chunk_gated_delta_rule` for parallel processing.",
                "use_qk_norm_in_gdn": "A boolean flag to determine if L2 normalization should be applied to query and key tensors within the GDN rule."
            },
            "notes": [
                "This module is a key component of the Qwen3-Next architecture, serving as a linear attention replacement for standard full attention in most layers.",
                "It supports a form of grouped-query attention by repeating key and query heads if the number of value heads is a multiple of the number of key heads.",
                "The `A_log` parameter has a custom initialization to match the original PyTorch implementation.",
                "The code contains TODO comments indicating that state caching for the convolution and recurrent components during inference is not yet implemented."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gated Delta Network layer, setting up all submodules and parameters based on the provided configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize dimensions for key, value, and convolution based on config.",
                        "Instantiate `DenseGeneral` layers for input projections (`in_proj_qkvz`, `in_proj_ba`) and the output projection (`out_proj`).",
                        "Instantiate a depthwise causal `nnx.Conv` layer.",
                        "Initialize trainable parameters `A_log` and `dt_bias` for the recurrent gate calculation.",
                        "Instantiate the `Qwen3NextRMSNormGated` layer for the final output stage."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.DenseGeneral",
                        "nnx.Conv",
                        "Qwen3NextRMSNormGated",
                        "nnx.Param",
                        "jax.random.uniform"
                    ],
                    "notes": [
                        "The `A_log` parameter is initialized by taking the logarithm of values sampled from a uniform distribution between a small epsilon and 16.0, to match a specific behavior in the reference implementation."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the Gated Delta Network layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "DType"
                    },
                    "processing_steps": [
                        "Project input `hidden_states` into `qkvz` and `ba` tensors using `in_proj_qkvz` and `in_proj_ba`.",
                        "Split the projected tensors to get raw q, k, v, z, b, and a.",
                        "Concatenate raw q, k, v and apply a 1D depthwise causal convolution followed by a SiLU activation.",
                        "Split the convolved output into processed q_conv, k_conv, and v_conv.",
                        "Reshape q_conv, k_conv, v_conv into multi-head format: [batch, seq_len, num_heads, head_dim].",
                        "Calculate gate tensors `beta` (using sigmoid) and `g` (using `A_log`, `dt_bias`, softplus, and exp).",
                        "If `num_v_heads` > `num_k_heads`, repeat query and key heads to match the number of value heads.",
                        "Execute the core recurrence using `jax_chunk_gated_delta_rule`.",
                        "Reshape the `z` tensor into a multi-head format to be used as a gate.",
                        "Apply gated RMS normalization (`self.norm`) to the recurrent output using the reshaped `z`.",
                        "Reshape the gated output back to a 3D tensor.",
                        "Apply the final output projection layer `out_proj`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "jnp.split",
                        "jnp.concatenate",
                        "jax.nn.silu",
                        "jnp.repeat",
                        "jax.nn.sigmoid",
                        "jax.nn.softplus",
                        "jnp.exp",
                        "jax_chunk_gated_delta_rule"
                    ],
                    "notes": [
                        "The method's logic is explicitly structured into four main steps as documented in the class docstring: Input Projections, 1D Convolution, Gated Delta Rule Recurrence, and Final Output Stage."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextFullAttention",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextFullAttention(nnx.Module):\n  \"\"\"Placeholder for Qwen3-Next full attention.\"\"\"\n\n  def __init__(\n      self, config: Config, mesh: Mesh, model_mode: str, layer_idx: int, quant: None | Quant = None, *, rngs: nnx.Rngs\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.layer_idx = layer_idx\n    self.quant = quant\n    cfg = self.config\n\n    # TODO(rbierneni): Implement the actual Qwen3NextAttention logic.\n    # This is a placeholder. The actual Qwen3NextAttention in the PyTorch code has:\n    # 1.  q_proj projection: hidden_size -> num_attention_heads * head_dim * 2.\n    #     This output is chunked to get query_states and a 'gate'.\n    # 2.  Qwen3NextRMSNorm (self.q_norm and self.k_norm) is applied to query_states\n    #     and key_states *before* RoPE. These norms are on the head_dim.\n    # 3.  The final attention output is gated: attn_output = attn_output * torch.sigmoid(gate).\n    # 4.  k_proj and v_proj are standard Linear layers to num_key_value_heads * head_dim.\n    # 5.  RoPE is applied to the normed query and key.\n    # 6.  The o_proj maps from num_attention_heads * head_dim back to hidden_size.\n\n    # Placeholder call to standard MaxText attention\n    # NOTE: This will NOT match the Qwen3Next behavior or weights.\n\n    # Get mode-specific batch size and sequence length for shape\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(cfg, model_mode)\n    inputs_shape = (batch_size, seq_len, cfg.emb_dim)\n\n    self.attention_layer = attentions.Attention(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=inputs_shape,\n        inputs_kv_shape=inputs_shape,\n        mesh=self.mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_qk_norm=False,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      # TODO(parambole): Add cache arguments\n  ):\n\n    # TODO(parambole): Add caching in/out\n    attention_output = self.attention_layer(\n        inputs,\n        inputs,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    return attention_output",
        "analysis": {
            "module_type": "placeholder_full_attention",
            "purpose": "A placeholder module for Qwen3-Next full attention, which currently wraps a standard MaxText attention layer and does not implement the specific Qwen3-Next logic.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a standard `attentions.Attention` layer using configuration parameters."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "max_utils",
                "attentions.Attention"
            ],
            "parameters": {
                "config": "The model configuration object containing parameters like `num_query_heads`, `num_kv_heads`, `head_dim`, `emb_dim`, `dtype`, etc.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "The operational mode of the model (e.g., 'train', 'prefill', 'autoregressive')."
            },
            "notes": [
                "This is explicitly a placeholder and does not match the behavior or weights of the actual Qwen3-Next attention mechanism.",
                "The actual Qwen3-Next attention includes features like a gated query projection, RMSNorm applied to query/key states before RoPE, and a final gated output, which are not implemented here."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by creating a standard MaxText `Attention` layer as a placeholder.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (`config`, `mesh`, `model_mode`, etc.).",
                        "Determine the expected input tensor shape for the attention layer using `max_utils.get_batch_seq_len_for_mode`.",
                        "Instantiate a standard `attentions.Attention` layer with parameters derived from the configuration."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_utils.get_batch_seq_len_for_mode",
                        "attentions.Attention",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "The created `attention_layer` does not implement the specific logic of Qwen3-Next attention."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass using the underlying standard attention layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "The dtype specified in the config (e.g., jnp.float32)."
                    },
                    "processing_steps": [
                        "Calls the internal `self.attention_layer` with the provided inputs, positions, and segment IDs.",
                        "Returns the output of the attention layer."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "self.attention_layer"
                    ],
                    "notes": [
                        "Caching logic is not yet implemented, as indicated by a TODO comment."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextSparseMoeBlock",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextSparseMoeBlock(nnx.Module):\n  \"\"\"\n  This module encapsulates the unique MoE structure of Qwen3-Next, which includes:\n  1. A set of routed experts, where each token is sent to a subset of experts.\n  2. A single shared expert, which all tokens pass through.\n  3. A learnable gate that determines the contribution of the shared expert.\n\n  Attributes:\n    config: The model configuration object.\n    mesh: The device mesh for sharding.\n    quant: Optional quantization configuration.\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, quant: None | Quant = None, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    cfg = self.config\n\n    # 1. Instantiate and apply the routed experts block.\n    self.routed_experts = moe.RoutedMoE(\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.moe_mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n        rngs=rngs,\n    )\n\n    # 2. Instantiate and apply the shared expert.\n    self.shared_expert = linears.MlpBlock(\n        config=cfg,\n        in_features=cfg.emb_dim,\n        intermediate_dim=cfg.moe_mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n        model_mode=config.model_call_mode,\n        rngs=rngs,\n    )\n\n    # 3. Instantiate and apply the gate for the shared expert.\n    self.shared_expert_gate = linears.DenseGeneral(\n        in_features_shape=cfg.emb_dim,\n        out_features_shape=1,\n        use_bias=False,  # Qwen3-Next shared_expert_gate does not have a bias\n        dtype=cfg.dtype,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", \"vocab\"),\n        rngs=rngs,\n    )\n\n  def __call__(self, hidden_states: Array, deterministic: bool) -> tuple[Array, Array | None]:\n    \"\"\"\n    Applies the sparse MoE block to the input hidden states.\n\n    Args:\n      hidden_states: The input array from the previous layer. Shape: (batch, seq, embed_dim)\n      deterministic: If True, disables dropout.\n\n    Returns:\n      A tuple containing:\n        - The output array of the MoE block.\n        - The load balancing loss from the routed experts, if applicable during training.\n    \"\"\"\n    # 1. Apply the routed experts block.\n    routed_output, load_balance_loss = self.routed_experts(hidden_states)\n\n    # 2. Apply the shared expert.\n    shared_expert_output = self.shared_expert(hidden_states, deterministic=deterministic)\n\n    # 3. Apply the gate for the shared expert.\n    shared_gate_output = self.shared_expert_gate(hidden_states)\n\n    # 4. Combine the outputs.\n    final_output = routed_output + jax.nn.sigmoid(shared_gate_output) * shared_expert_output\n\n    return final_output, load_balance_loss",
        "analysis": {
            "module_type": "qwen3_next_sparse_moe_block",
            "purpose": "Encapsulates the unique Mixture-of-Experts (MoE) structure of Qwen3-Next, which combines a set of routed experts with a single shared expert controlled by a learnable gate.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a `RoutedMoE` block for the sparse experts.",
                "Initializes an `MlpBlock` for the shared expert.",
                "Initializes a `DenseGeneral` layer to act as a gate for the shared expert."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "moe.RoutedMoE",
                "linears.MlpBlock",
                "linears.DenseGeneral",
                "max_initializers",
                "jax.nn"
            ],
            "parameters": {
                "config": "The model configuration object containing parameters like `num_experts`, `num_experts_per_tok`, `moe_mlp_dim`, `emb_dim`, etc.",
                "mesh": "The device mesh for sharding model parameters and activations.",
                "quant": "Optional quantization configuration for the experts and gate."
            },
            "notes": [
                "This module's key feature is the parallel application of a standard sparse MoE and a dense shared expert, with their outputs combined additively.",
                "The contribution of the shared expert is weighted by a learned sigmoid gate."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the routed experts, the shared expert, and the gate for the shared expert.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiate `moe.RoutedMoE` as `self.routed_experts`.",
                        "Instantiate `linears.MlpBlock` as `self.shared_expert`.",
                        "Instantiate `linears.DenseGeneral` as `self.shared_expert_gate`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "moe.RoutedMoE",
                        "linears.MlpBlock",
                        "linears.DenseGeneral",
                        "max_initializers"
                    ],
                    "notes": [
                        "The gate for the shared expert is configured without a bias term, as specified in the Qwen3-Next architecture."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the sparse MoE block to the input hidden states.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embed_dim]",
                        "dtype": "Inferred from config.dtype (e.g., float32, bfloat16)"
                    },
                    "processing_steps": [
                        "Calculate the output and load balancing loss from the routed experts.",
                        "Calculate the output of the shared expert.",
                        "Calculate the gate value for the shared expert.",
                        "Combine the outputs: `final_output = routed_output + jax.nn.sigmoid(shared_gate_output) * shared_expert_output`."
                    ],
                    "output": {
                        "shape": "A tuple containing the final output tensor of shape `[batch_size, sequence_length, embed_dim]` and a scalar load balancing loss (`Array` or `None`)."
                    },
                    "dependencies": [
                        "jax.nn.sigmoid"
                    ],
                    "notes": [
                        "The method returns a load balancing loss from the routed experts, which is typically used as an auxiliary loss during training to encourage balanced expert utilization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextScannableBlock",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextScannableBlock(nnx.Module):\n  \"\"\"A scannable block of Qwen3-Next decoder layers.\n\n  This module contains a fixed number of heterogeneous decoder layers that form\n  a repeating pattern, as defined by `config.inhomogeneous_layer_cycle_interval`. It is\n  intended to be the body of an `nn.scan` transformation to construct the full\n  decoder stack efficiently.\n\n  Attributes:\n    config: The model configuration object.\n    mesh: The device mesh for sharding.\n    model_mode: The operational mode (e.g., 'train', 'prefill').\n    quant: Optional quantization configuration.\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, model_mode: str, quant: None | Quant = None, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n    cfg = self.config\n\n    # Instantiate each layer within the block in __init__\n    for i in range(cfg.inhomogeneous_layer_cycle_interval):\n      layer_rngs = self.rngs.fork()  # Fork RNGs for each layer\n      layer_name = f\"layer_{i}\"\n      layer = Qwen3NextDecoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=self.model_mode,\n          layer_idx=i,\n          rngs=layer_rngs,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(\n      self,\n      carry: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ) -> tuple[Array, None]:\n    \"\"\"Applies the block of decoder layers to the input carry.\n\n    Args:\n      carry: The input tensor from the previous scan iteration.\n      # ... other arguments are broadcasted to each iteration.\n\n    Returns:\n      A tuple containing the output of the block (the new carry) and an empty\n      value for the scan's `y` collection.\n    \"\"\"\n    cfg = self.config\n    x = carry\n\n    # Loop over the number of sub-layers that make up one repeating pattern.\n    for i in range(cfg.inhomogeneous_layer_cycle_interval):\n      layer = getattr(self, f\"layer_{i}\")\n      x = layer(\n          x,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk,\n          page_state,\n          slot,\n      )\n\n    # The output of the block is the carry for the next scan iteration.\n    return x, None",
        "analysis": {
            "module_type": "scannable_decoder_block",
            "purpose": "Encapsulates a fixed-size, repeating block of heterogeneous decoder layers, designed to be used as the body of a `jax.lax.scan` for efficient construction of the full decoder stack.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a fixed number of `Qwen3NextDecoderLayer` instances in `__init__` based on `config.inhomogeneous_layer_cycle_interval`.",
                "In `__call__`, sequentially applies each pre-instantiated `Qwen3NextDecoderLayer` to the input tensor."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "Qwen3NextDecoderLayer"
            ],
            "parameters": {
                "inhomogeneous_layer_cycle_interval": "The number of heterogeneous decoder layers that form one repeating pattern or block."
            },
            "notes": [
                "This module is specifically designed as the body for a `scan` transformation to improve compilation time and memory efficiency for deep models.",
                "The layers within the block are heterogeneous, meaning their internal structure (e.g., attention type) can vary, as determined by the `Qwen3NextDecoderLayer` logic."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the scannable block by creating and storing a sequence of `Qwen3NextDecoderLayer` instances.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates `config.inhomogeneous_layer_cycle_interval` times.",
                        "In each iteration, forks RNGs and instantiates a `Qwen3NextDecoderLayer`.",
                        "Assigns the created layer to an attribute of the instance (e.g., `self.layer_0`)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Qwen3NextDecoderLayer"
                    ],
                    "notes": [
                        "Pre-instantiates all layers required for one block, which is necessary for `scan` transformations."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the entire block of decoder layers to the input tensor (`carry`).",
                    "input": {
                        "shape": "carry: [batch_size, sequence_length, embed_dim]",
                        "dtype": "DType from config"
                    },
                    "processing_steps": [
                        "Initializes an intermediate tensor `x` with the input `carry`.",
                        "Iteratively applies each layer in the block to `x`.",
                        "Returns the final tensor `x` as the new carry for the scan, along with `None`."
                    ],
                    "output": {
                        "shape": "Tuple: ([batch_size, sequence_length, embed_dim], None)"
                    },
                    "dependencies": [
                        "Qwen3NextDecoderLayer"
                    ],
                    "notes": [
                        "Designed to be the `body` function for `jax.lax.scan`.",
                        "The second element of the return tuple (`None`) is for the scan's `y` collection, which is not used here."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextDecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextDecoderLayer(nnx.Module):\n  \"\"\"\n  This layer is a hybrid, capable of functioning as either:\n  1. A standard attention + MoE layer.\n  2. A linear attention + MoE layer.\n\n  NOTE: This implementation assumes every layer contains a MoE block, which is true for\n  models like Qwen3-Next-80B-A3B where `decoder_sparse_step=1`. For models that\n  interleave dense and sparse MLP layers, conditional logic would be needed here.\n\n  Attributes:\n    config: The model configuration object.\n    mesh: The device mesh for sharding.\n    model_mode: The operational mode (e.g., 'train', 'prefill').\n    layer_idx: The index of the current layer in the transformer stack.\n    quant: Optional quantization configuration.\n  \"\"\"\n\n  def __init__(\n      self, config: Config, mesh: Mesh, model_mode: str, layer_idx: int, quant: None | Quant = None, *, rngs: nnx.Rngs\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.layer_idx = layer_idx\n    self.quant = quant\n    cfg = self.config\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    # First LayerNorm, applied before the attention block.\n    self.input_layernorm = Qwen3NextRMSNorm(\n        num_features=cfg.emb_dim,\n        eps=cfg.normalization_layer_epsilon,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        rngs=rngs,\n    )\n\n    # Determine the type of attention mechanism for the current layer.\n    is_full_attention_layer = (self.layer_idx + 1) % cfg.inhomogeneous_layer_cycle_interval == 0\n\n    # Conditionally instantiate either the Linear Attention or Full Attention block.\n    if is_full_attention_layer:\n      self.attention = Qwen3NextFullAttention(\n          config=cfg,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=model_mode,\n          layer_idx=self.layer_idx,\n          rngs=rngs,\n      )\n    else:\n      self.attention = Qwen3NextGatedDeltaNet(config=cfg, dtype=cfg.dtype, rngs=rngs)\n\n    # Second LayerNorm, applied before the MoE block.\n    self.post_attention_layernorm = Qwen3NextRMSNorm(\n        num_features=cfg.emb_dim,\n        eps=cfg.normalization_layer_epsilon,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        rngs=rngs,\n    )\n\n    # Instantiate our `Qwen3NextSparseMoeBlock`.\n    self.mlp = Qwen3NextSparseMoeBlock(config=cfg, mesh=self.mesh, quant=self.quant, rngs=rngs)\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    residual = inputs\n\n    # First LayerNorm, applied before the attention block.\n    hidden_states = self.input_layernorm(inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # Conditionally apply either the Linear Attention or Full Attention block.\n    if isinstance(self.attention, Qwen3NextFullAttention):\n      attention_output = cast(Qwen3NextFullAttention, self.attention)(\n          hidden_states,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n    elif isinstance(self.attention, Qwen3NextGatedDeltaNet):\n      attention_output = cast(Qwen3NextGatedDeltaNet, self.attention)(hidden_states)\n    else:\n      raise TypeError(f\"Unexpected type for self.attention: {type(self.attention)}\")\n\n    # First residual connection after attention\n    hidden_states = residual + attention_output\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # Prepare for the MoE block by capturing the new residual\n    residual = hidden_states\n\n    # Second LayerNorm, applied before the MoE block.\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # Instantiate and call our `Qwen3NextSparseMoeBlock`.\n    mlp_output, load_balance_loss = self.mlp(hidden_states, deterministic=deterministic)\n\n    # We sow the load balancing loss so it can be collected and added to the total loss\n    # during training.\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    # Final residual connection (after the MoE block)\n    layer_output = residual + mlp_output\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        self.activation_axis_names,\n    )\n\n    return layer_output",
        "analysis": {
            "functionality": "The `Qwen3NextDecoderLayer` class implements a hybrid transformer decoder layer that can function as either a standard attention + MoE layer or a linear attention (Gated Delta Network) + MoE layer. The choice of attention mechanism is determined by the layer's index within the model.",
            "usage": "This class is used as a single layer within a larger transformer model. It is initialized with model configuration, a device mesh, and its specific layer index. During the forward pass (`__call__`), it takes an input tensor of shape `[batch_size, sequence_length, embedding_dim]` and returns an output tensor of the same shape after applying normalization, an attention mechanism, a sparse MoE block, and residual connections."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#AttentionWithNorm",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class AttentionWithNorm(nnx.Module):\n  \"\"\"Base class with shared common components: self-attention block with normalization.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    # Corresponds to Qwen3's `input_layernorm`\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    # Self-attention block\n    query_pre_attn_scalar = config.head_dim**-0.5  # Qwen3 specific scaling\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        use_qk_norm=config.use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    # Post Attention LayerNorm (corresponds to Qwen3's `post_attention_layernorm`)\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n  def apply_attention_with_norm(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n  ):\n    \"\"\"Applies self-attention with pre and post-layer normalization.\"\"\"\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # Pre attention norm\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n    # Self attention\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    # Residual connection after attention\n    intermediate_inputs = inputs + attention_lnx\n    # Post attention norm\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n    return hidden_states, intermediate_inputs",
        "analysis": {
            "module_type": "attention_with_normalization",
            "purpose": "A base module that encapsulates a self-attention block with pre- and post-layer normalization and a residual connection, a common pattern in transformer architectures.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a pre-attention RMSNorm layer.",
                "Initializes a multi-head/grouped-query Attention layer with a Qwen3-specific query scaling factor.",
                "Initializes a post-attention RMSNorm layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "max_utils",
                "RMSNorm",
                "Attention",
                "quantizations"
            ],
            "parameters": {
                "emb_dim": "The embedding dimension of the model.",
                "head_dim": "The dimension of each attention head.",
                "num_query_heads": "The number of query heads in the attention mechanism.",
                "num_kv_heads": "The number of key/value heads in the attention mechanism.",
                "normalization_layer_epsilon": "A small constant added to the variance in RMSNorm to avoid division by zero.",
                "attention": "The specific attention kernel to use (e.g., 'dot_product')."
            },
            "notes": [
                "This class implements the Pre-LN (Layer Normalization) transformer architecture.",
                "It is designed as a base class to be inherited by more complex decoder layers that add an MLP or MoE block."
            ],
            "methods": {
                "apply_attention_with_norm": {
                    "purpose": "Applies a pre-normalization, self-attention, residual connection, and post-normalization sequence to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "The data type specified in the config (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Pass the normalized tensor through the self-attention layer.",
                        "Add the original input tensor to the attention output (residual connection), creating `intermediate_inputs`.",
                        "Apply post-attention RMS normalization to `intermediate_inputs` to get `hidden_states`.",
                        "Return both `hidden_states` and `intermediate_inputs`."
                    ],
                    "output": {
                        "shape": "A tuple of two tensors, both with shape [batch_size, sequence_length, emb_dim]."
                    },
                    "dependencies": [
                        "self.pre_self_attention_layer_norm",
                        "self.self_attention",
                        "self.post_self_attention_layer_norm",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "The method returns both the final output and the intermediate result before the final normalization. The intermediate result is used by subclasses to add the output of a subsequent MLP/MoE block."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3DecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3DecoderLayer(AttentionWithNorm):\n  \"\"\"Qwen3 Transformer decoder layer (dense).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    super().__init__(config, mesh, model_mode, quant, rngs)\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        quant=quant,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    hidden_states, intermediate_inputs = self.apply_attention_with_norm(\n        inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode\n    )\n\n    mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = intermediate_inputs + mlp_lnx\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "transformer_decoder_layer_dense",
            "purpose": "Implements a standard dense Qwen3 transformer decoder layer, which includes a self-attention block and a feed-forward MLP block with residual connections and layer normalizations.",
            "input": {
                "shape": "[batch_size, sequence_length, emb_dim]",
                "dtype": "jnp.ndarray (as per config.dtype)"
            },
            "processing_steps": [
                "Initializes a self-attention block with pre and post-normalization by inheriting from `AttentionWithNorm`.",
                "Initializes a dense feed-forward network (`MlpBlock`).",
                "In the forward pass, it first applies the attention block with its normalizations and first residual connection.",
                "Then, it passes the result through the MLP block.",
                "Finally, it applies the second residual connection to produce the layer's output."
            ],
            "output": {
                "shape": "If config.scan_layers is false: [batch_size, sequence_length, emb_dim]. If true: ([batch_size, sequence_length, emb_dim], None)."
            },
            "dependencies": [
                "AttentionWithNorm",
                "MlpBlock",
                "flax.linen as nn"
            ],
            "parameters": {
                "config.emb_dim": "The embedding dimension of the model.",
                "config.mlp_dim": "The intermediate dimension of the MLP block.",
                "config.scan_layers": "A boolean flag that determines the output format to support `jax.lax.scan`."
            },
            "notes": [
                "This class represents a 'dense' decoder layer, meaning it uses a standard MLP block instead of a Mixture-of-Experts (MoE) block.",
                "It inherits the entire attention mechanism, including pre and post-layer normalization, from the `AttentionWithNorm` base class.",
                "The final output is calculated as `(inputs + attention_output) + mlp_output`, which is a common pre-norm residual connection pattern."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Qwen3 dense decoder layer, setting up the attention and MLP sub-modules.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the parent `AttentionWithNorm` constructor to initialize the self-attention block and its surrounding normalizations.",
                        "Instantiates the `MlpBlock` for the feed-forward network."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "AttentionWithNorm",
                        "MlpBlock"
                    ],
                    "notes": [
                        "This constructor takes configuration objects (`Config`, `Mesh`) and RNGs to set up the layer's parameters and sub-modules."
                    ]
                },
                "__call__": {
                    "purpose": "Processes an input tensor through one complete transformer decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Calls `self.apply_attention_with_norm` to perform pre-norm, self-attention, a first residual connection, and post-norm.",
                        "Passes the result of the attention block through the `MlpBlock`.",
                        "Adds the MLP output to the result of the first residual connection to compute the final layer output.",
                        "Applies logical constraints for tensor sharding.",
                        "Conditionally formats the return value based on the `config.scan_layers` flag."
                    ],
                    "output": {
                        "shape": "If config.scan_layers is false: [batch_size, sequence_length, emb_dim]. If true: ([batch_size, sequence_length, emb_dim], None)."
                    },
                    "dependencies": [
                        "self.apply_attention_with_norm",
                        "self.mlp"
                    ],
                    "notes": [
                        "The method accepts optional arguments like `page_state` and `slot` for inference, but they are not used in this layer's core logic and are passed down to the attention mechanism.",
                        "The return format changes to a tuple `(output, None)` when `config.scan_layers` is true, making it compatible with `jax.lax.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3MoeDecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3MoeDecoderLayer(AttentionWithNorm):\n  \"\"\"Qwen3 Transformer decoder layer (MoE).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    super().__init__(config, mesh, model_mode, quant, rngs)\n    self.moe_block = RoutedMoE(\n        config=config,\n        num_experts=config.num_experts,\n        num_experts_per_tok=config.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=config.moe_mlp_dim,  # same as config.mlp_dim\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=quant,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    hidden_states, intermediate_inputs = self.apply_attention_with_norm(\n        inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode\n    )\n\n    mlp_lnx, load_balance_loss = self.moe_block(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    layer_output = intermediate_inputs + mlp_lnx\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "qwen3_moe_decoder_layer",
            "purpose": "Implements a Qwen3 transformer decoder layer that uses a Mixture-of-Experts (MoE) block for the feed-forward network, inheriting attention and normalization logic from a base class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the self-attention and normalization components by calling the superclass constructor.",
                "Initializes a `RoutedMoE` block which serves as the feed-forward network for this layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "AttentionWithNorm",
                "RoutedMoE",
                "max_initializers.nd_dense_init"
            ],
            "parameters": {
                "config.num_experts": "The total number of experts in the MoE layer.",
                "config.num_experts_per_tok": "The number of experts to which each token is routed.",
                "config.moe_mlp_dim": "The intermediate dimension of the feed-forward network within each expert.",
                "config.scan_layers": "A boolean flag that determines the output format to be compatible with `jax.lax.scan`."
            },
            "notes": [
                "This class is a variant of a standard transformer decoder layer, replacing the dense MLP block with a sparse MoE block."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MoE decoder layer, including the attention/normalization blocks from the superclass and the `RoutedMoE` block.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `super().__init__` to initialize the `AttentionWithNorm` base class components.",
                        "Instantiates the `RoutedMoE` block with parameters from the configuration."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "AttentionWithNorm",
                        "RoutedMoE"
                    ],
                    "notes": [
                        "This constructor sets up all necessary sub-modules for the layer's forward pass."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass for the MoE decoder layer, applying attention, MoE, and residual connections.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Calls `self.apply_attention_with_norm` to compute the attention output and the first residual state.",
                        "Passes the normalized hidden states through the `self.moe_block` to get the MLP output and a load balancing loss.",
                        "Applies a logical constraint to the MLP output.",
                        "Sows the `load_balance_loss` to the 'intermediates' collection if it is not None.",
                        "Adds the MLP output to the first residual state to get the final layer output.",
                        "Applies a final logical constraint.",
                        "Returns the layer output, potentially in a tuple format if `config.scan_layers` is true."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is False: [batch_size, sequence_length, emb_dim]. If True: a tuple of ([batch_size, sequence_length, emb_dim], None)."
                    },
                    "dependencies": [
                        "self.apply_attention_with_norm",
                        "self.moe_block"
                    ],
                    "notes": [
                        "The load balancing loss is an auxiliary output generated during training that needs to be incorporated into the total loss function.",
                        "The return signature is conditional on `config.scan_layers` to support efficient compilation of the entire decoder stack using `jax.lax.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleDecoderLayer(nnx.Module):\n  \"\"\"Decoder layer consisting of a single [embed, embed] weight matrix.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ) -> None:\n\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.rngs = rngs\n    self.quant = quant\n\n    init_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), sharding=(\"embed\", \"mlp\"), mesh=self.mesh)\n\n    self.weights = nnx.Param(\n        init_fn(self.rngs.params(), (self.config.emb_dim, self.config.emb_dim)),\n    )\n\n  def __call__(\n      self, inputs: jnp.ndarray, positions, segmentation, deterministic, model_mode, previous_chunk=None, page_state=None\n  ):\n    if self.config.scan_layers:\n      return inputs @ self.weights.astype(inputs.dtype), None\n    return inputs @ self.weights.astype(inputs.dtype)",
        "analysis": {
            "module_type": "simple_decoder_layer",
            "purpose": "A simple decoder layer for testing that applies a single linear transformation (matrix multiplication) to an input tensor.",
            "input": {
                "shape": "[batch_size, sequence_length, emb_dim]",
                "dtype": "jnp.ndarray (e.g., float32)"
            },
            "processing_steps": [
                "Performs a matrix multiplication of the input tensor with the internal weight matrix."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, emb_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.initializers.lecun_normal",
                "nnx.with_partitioning"
            ],
            "parameters": {
                "config.emb_dim": "The embedding dimension, which determines the size of the square weight matrix `[emb_dim, emb_dim]`.",
                "config.scan_layers": "A boolean flag that, if true, changes the output to be a tuple `(output, None)` for compatibility with `nn.scan`."
            },
            "notes": [
                "This is a simplified layer intended for testing and debugging purposes.",
                "The weight matrix is partitioned across the 'embed' and 'mlp' mesh axes for distributed computation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the layer by creating a single learnable weight matrix with specified partitioning.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines a partitioned initializer using `lecun_normal` with `('embed', 'mlp')` sharding.",
                        "Initializes the `weights` parameter as a `(config.emb_dim, config.emb_dim)` matrix using the initializer."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "nnx.with_partitioning",
                        "nnx.initializers.lecun_normal",
                        "nnx.Param"
                    ],
                    "notes": [
                        "The `sharding` for the weights is hardcoded to `('embed', 'mlp')`."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the linear transformation to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Casts the layer's weights to the dtype of the input tensor.",
                        "Performs a matrix multiplication: `inputs @ self.weights`.",
                        "Conditionally returns a tuple `(output, None)` if `config.scan_layers` is true, otherwise returns just the output tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "jnp.ndarray"
                    ],
                    "notes": [
                        "Accepts several unused arguments (e.g., `positions`, `segmentation`, `deterministic`) to maintain a consistent API with more complex decoder layers.",
                        "The return signature varies based on the `config.scan_layers` flag."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleMlpDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleMlpDecoderLayer(nnx.Module):\n  \"\"\"Decoder layer consisting of [embed,mlp] followed by an [mlp,embed] matmul.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ) -> None:\n\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.rngs = rngs\n    self.quant = quant\n\n    init_ff1_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), sharding=(\"embed\", \"mlp\"), mesh=self.mesh)\n\n    self.ff_1 = nnx.Param(\n        init_ff1_fn(self.rngs.params(), (self.config.emb_dim, self.config.mlp_dim)),\n    )\n\n    init_ff2_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), sharding=(\"mlp\", \"embed\"), mesh=self.mesh)\n\n    self.ff_2 = nnx.Param(\n        init_ff2_fn(self.rngs.params(), (self.config.mlp_dim, self.config.emb_dim)),\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      positions,\n      segmentation,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=0,\n  ):\n    intermediate = inputs @ self.ff_1.astype(inputs.dtype)\n    output = intermediate @ self.ff_2.astype(inputs.dtype)\n    if self.config.scan_layers:\n      return output, None\n    return output",
        "analysis": {
            "module_type": "simple_mlp_decoder_layer",
            "purpose": "A simple decoder layer that applies a two-layer feed-forward network (MLP) without a non-linear activation function, consisting of an up-projection followed by a down-projection.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the first feed-forward weight matrix `ff_1` with shape (emb_dim, mlp_dim) and sharding ('embed', 'mlp').",
                "Initializes the second feed-forward weight matrix `ff_2` with shape (mlp_dim, emb_dim) and sharding ('mlp', 'embed')."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.with_partitioning",
                "nnx.initializers.lecun_normal"
            ],
            "parameters": {
                "config.emb_dim": "The embedding dimension of the input and output tensors.",
                "config.mlp_dim": "The intermediate dimension of the MLP.",
                "config.scan_layers": "A boolean flag that, if true, modifies the output format to be compatible with `jax.lax.scan`."
            },
            "notes": [
                "This module is likely intended for testing or debugging purposes due to its simplicity and lack of a non-linear activation function.",
                "The `quant` parameter is accepted but not used in the implementation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the two weight matrices for the MLP layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes `self.ff_1` as a sharded `nnx.Param` of shape (config.emb_dim, config.mlp_dim).",
                        "Initializes `self.ff_2` as a sharded `nnx.Param` of shape (config.mlp_dim, config.emb_dim)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.with_partitioning",
                        "nnx.initializers.lecun_normal"
                    ],
                    "notes": [
                        "The method takes a Config object, a JAX Mesh, model_mode string, and nnx.Rngs for initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the two linear transformations to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Performs a matrix multiplication of the input with the first weight matrix (`self.ff_1`).",
                        "Performs a matrix multiplication of the intermediate result with the second weight matrix (`self.ff_2`).",
                        "Conditionally returns a tuple `(output, None)` if `config.scan_layers` is True, otherwise returns just the output tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "jnp.ndarray"
                    ],
                    "notes": [
                        "Several arguments (positions, segmentation, deterministic, etc.) are accepted to maintain a consistent API with other decoder layers but are not used in the computation.",
                        "The output format changes based on the `config.scan_layers` flag."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "module_type": "configuration_validator",
            "purpose": "Validates specific configuration settings for the interleaved inference script, ensuring compatibility and logical consistency.",
            "input": {
                "shape": "N/A",
                "dtype": "pyconfig.Config"
            },
            "processing_steps": [
                "Assert that `config.load_full_state_path` is an empty string, ensuring a parameter-only checkpoint is used.",
                "Assert that the number of initial prefill streams is greater than 0 and less than or equal to the total number of streams."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "config.load_full_state_path": "The path to a full training state, which is disallowed for this decoding script.",
                "_INITIAL_PREFILL_STREAMS": "A global constant defining how many streams to prefill before starting generation.",
                "_NUM_STREAMS": "A global constant for the total number of concurrent streams."
            },
            "notes": [
                "This function does not return a value; it raises an AssertionError if validation fails, halting the program.",
                "It is a helper function called at the start of the main execution flow to prevent runtime errors due to misconfiguration."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#main",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "module_type": "interleaved_inference_runner",
            "purpose": "Runs a multi-stream inference loop that interleaves prefilling new requests with generating tokens for active requests, simulating a continuous batching scenario.",
            "input": {
                "shape": "N/A",
                "dtype": "Sequence[str]"
            },
            "processing_steps": [
                "Initialize JAX, environment, and configuration from command-line arguments.",
                "Instantiate the `maxengine.MaxEngine`.",
                "Load model parameters using `engine.load_params`.",
                "Tokenize the input prompt from the configuration.",
                "Initialize the decode state using `engine.init_decode_state`.",
                "Prefill an initial set of streams using `engine.prefill`.",
                "Insert the prefilled streams into the decode state using `engine.insert`.",
                "Enter a loop to generate tokens up to `max_target_length`.",
                "In each step, generate one token for all active streams simultaneously using `engine.generate`.",
                "Check for finished streams and release their resources using `engine.release_pages`.",
                "If capacity is available, prefill and insert new streams into the batch.",
                "After the loop, decode the generated token sequences for each stream.",
                "Print and assert the final output text."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "maxengine.MaxEngine",
                "pyconfig.initialize",
                "jax",
                "uuid",
                "_validate_config"
            ],
            "parameters": {
                "config.prompt": "The initial text prompt to be used for all streams.",
                "config.max_prefill_predict_length": "The maximum length of the tokenized prompt for the prefill stage.",
                "config.max_target_length": "The total maximum length of the generated sequence, including the prompt.",
                "config.per_device_batch_size": "The batch size per device, used to calculate the total batch size.",
                "config.autoregressive_decode_assert": "A string that the beginning of the generated output is expected to match for validation."
            },
            "notes": [
                "This function simulates a continuous batching inference server by managing multiple independent generation streams.",
                "It starts by prefilling a subset of streams (`_INITIAL_PREFILL_STREAMS`) and then enters a main loop.",
                "In the loop, it performs a single generation step for all active streams, and then uses any newly available capacity to prefill and insert new streams.",
                "The total number of streams to process is controlled by the global constant `_NUM_STREAMS`.",
                "The function does not return a value; it prints the results to standard output."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#reverse_transpose",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def reverse_transpose(transposed_array, transpose_axis_order):\n  return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)",
        "analysis": {
            "module_type": "tensor_manipulation",
            "purpose": "Reverses a transposition operation on a 4D array by moving its axes back to their original order.",
            "input": {
                "shape": "transposed_array: [dim0, dim1, dim2, dim3], transpose_axis_order: a tuple of 4 integers (e.g., (1, 2, 0, 3))",
                "dtype": "Any JAX array dtype"
            },
            "processing_steps": [
                "Calls `jax.numpy.moveaxis` to move the source axes (0, 1, 2, 3) to the destination positions specified by `transpose_axis_order`."
            ],
            "output": {
                "shape": "The input array with its axes permuted back to the original order. The rank remains 4."
            },
            "dependencies": [
                "jax.numpy.moveaxis"
            ],
            "parameters": {
                "transposed_array": "The 4D JAX array that has been transposed.",
                "transpose_axis_order": "A tuple of 4 integers representing the permutation to reverse. This should be the same axis order used in the original transpose operation."
            },
            "notes": [
                "This function implicitly assumes the input `transposed_array` is a 4-dimensional tensor, as indicated by the hardcoded source axes `(0, 1, 2, 3)`.",
                "It is the inverse operation of `jnp.transpose(original_array, transpose_axis_order)`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#transpose_tuple",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def transpose_tuple(items: tuple[Any, ...], axis_order: AxisIdxes) -> tuple[Any, ...]:\n  return tuple((items[i] for i in axis_order))",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Reorders the elements of a tuple based on a specified tuple of indices.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterates through the indices in `axis_order`.",
                "For each index `i`, it retrieves the element `items[i]`.",
                "Constructs a new tuple from the retrieved elements in the specified order."
            ],
            "output": {
                "shape": "A tuple with the same number of elements as `axis_order`."
            },
            "dependencies": [],
            "parameters": {
                "items": "The input tuple whose elements are to be reordered.",
                "axis_order": "A tuple of integers specifying the new order of elements from the `items` tuple."
            },
            "notes": [
                "This function is analogous to transposing axes of a multi-dimensional array, but for a simple tuple.",
                "The type hint `AxisIdxes` is an alias for `tuple[int, ...]`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVQuant",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVQuant:\n  \"\"\"Class to configure quantization for KV cache.\"\"\"\n\n  axis_cfg = \"\"\n  dtype = None\n\n  def __init__(self, config: Config):\n    assert config.quantize_kvcache\n    self.axis_cfg = config.kv_quant_axis\n    self.dtype = self._get_dtype(config.kv_quant_dtype)\n\n  def _get_dtype(self, dtype_cfg: str):\n    if dtype_cfg == \"int4\":\n      return jnp.int4\n    if dtype_cfg == \"int8\":\n      return jnp.int8\n    if dtype_cfg == \"fp8\":\n      return jnp.float8_e4m3fn\n    raise ValueError(f\"Invalid kv_quant_dtype: {dtype_cfg}\")\n\n  def _get_max_axis(self, axis_names: AxisNames):\n    if self.axis_cfg == \"dkv\":\n      return axis_names.index(CACHE_KV)\n    if self.axis_cfg == \"heads_and_dkv\":\n      return (axis_names.index(CACHE_HEADS), axis_names.index(CACHE_KV))\n    raise ValueError(f\"Invalid KV quant axis cfg: {self.axis_cfg}\")\n\n  def quantize(self, kv: Array, axis_names: AxisNames):\n    \"\"\"Quantize key/values stored in kvcache.\"\"\"\n    assert self.axis_cfg, \"KV quant axis cannot be None\"\n    max_axis = self._get_max_axis(axis_names)\n    scale = jnp.max(jnp.abs(kv), axis=max_axis, keepdims=True)\n    if self.dtype == jnp.int8:\n      value = jnp.int8(jnp.rint(kv * (MAX_INT8 / scale)))\n      return value, scale\n    if self.dtype == jnp.int4:\n      value = jnp.int4(jnp.rint(kv * (MAX_INT4 / scale)))\n      return value, scale\n    if self.dtype == jnp.float8_e4m3fn:\n      value = jnp.float8_e4m3fn(kv * (E4M3_MAX / scale))\n      return value, scale\n    raise ValueError(f\"Invalid KV quant dtype:{self.dtype}.\")\n\n  def einsum_fn_with_rhs_qtensor(\n      self,\n      rhs_dequant_mode=None,\n      rhs_calibration_mode=None,\n      lhs_dequant_mode=None,\n      lhs_calibration_mode=None,\n  ):\n    \"\"\"einsum function where QTensor is the right-hand-side\"\"\"\n    # Assumes kv is already quantized.\n    einsum = jnp.einsum\n    if self.dtype != jnp.float8_e4m3fn:\n      num_bits = 4 if self.dtype == jnp.int4 else 8\n      kv_cfg = aqt_config.dot_general_make(\n          lhs_bits=None,\n          rhs_bits=num_bits,\n          bwd_bits=None,\n          use_fwd_quant=False,\n      )\n    else:\n      kv_cfg = aqt_config.config_fwd_fp8()\n\n    if rhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, rhs_dequant_mode=rhs_dequant_mode)\n    if rhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          rhs_calibration_mode=rhs_calibration_mode,\n      )\n    if lhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, lhs_dequant_mode=lhs_dequant_mode)\n    if lhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          lhs_calibration_mode=lhs_calibration_mode,\n      )\n    einsum = aqt_flax.AqtEinsum(\n        rhs_quant_mode=aqt_flax.QuantMode.TRAIN,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        cfg=kv_cfg,\n    )\n    return einsum\n\n  def einsum_fn_with_rhs_qtensor_and_dequant(self):\n    \"\"\"Get einstein summation for different dequant modes.\"\"\"\n    if self.dtype == jnp.float8_e4m3fn:\n      return self.einsum_fn_with_rhs_qtensor(\n          lhs_dequant_mode=aqt_config.DequantMode.THIS_INPUT,\n          lhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )\n    else:\n      return self.einsum_fn_with_rhs_qtensor(\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )",
        "analysis": {
            "module_type": "kv_cache_quantization",
            "purpose": "A class to configure and apply quantization to the Key-Value (KV) cache in transformer models.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The class is initialized with a `Config` object.",
                "Asserts that `config.quantize_kvcache` is true.",
                "Sets the quantization axis configuration from `config.kv_quant_axis`.",
                "Sets the quantization data type by calling `_get_dtype` with `config.kv_quant_dtype`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.numpy",
                "aqt.jax.v2.config",
                "aqt.jax.v2.flax",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantize_kvcache": "A boolean that must be true to enable KV cache quantization.",
                "config.kv_quant_axis": "A string specifying the axis/axes for quantization (e.g., 'dkv', 'heads_and_dkv').",
                "config.kv_quant_dtype": "A string specifying the target data type for quantization (e.g., 'int4', 'int8', 'fp8')."
            },
            "notes": [
                "This class encapsulates all logic related to KV cache quantization, including the quantization process itself and the configuration of AQT einsum operations for dequantization during attention computation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVQuant configuration based on the provided `Config` object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert `config.quantize_kvcache` is True.",
                        "Store `config.kv_quant_axis` in `self.axis_cfg`.",
                        "Call `self._get_dtype` with `config.kv_quant_dtype` to determine and store the quantization dtype in `self.dtype`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "_get_dtype"
                    ],
                    "notes": [
                        "Raises an AssertionError if `config.quantize_kvcache` is False."
                    ]
                },
                "_get_dtype": {
                    "purpose": "Converts a string configuration for a data type into a `jnp` dtype object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "string"
                    },
                    "processing_steps": [
                        "Check the input string `dtype_cfg`.",
                        "Return `jnp.int4` for 'int4', `jnp.int8` for 'int8', or `jnp.float8_e4m3fn` for 'fp8'.",
                        "Raise a ValueError for any other string."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp"
                    ],
                    "notes": [
                        "Internal helper method. Raises ValueError for invalid `dtype_cfg`."
                    ]
                },
                "_get_max_axis": {
                    "purpose": "Determines the axis or axes over which to compute the maximum absolute value for quantization scaling, based on the `axis_cfg`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check `self.axis_cfg`.",
                        "If 'dkv', find the index of `CACHE_KV` in `axis_names`.",
                        "If 'heads_and_dkv', find the indices of `CACHE_HEADS` and `CACHE_KV` in `axis_names`.",
                        "Raise a ValueError for an invalid `axis_cfg`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "CACHE_KV",
                        "CACHE_HEADS"
                    ],
                    "notes": [
                        "Returns an integer or a tuple of integers representing the axis indices."
                    ]
                },
                "quantize": {
                    "purpose": "Quantizes the input key/value tensor (`kv`) to the configured data type.",
                    "input": {
                        "shape": "[batch, sequence, heads, features] or a transposed version of it.",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Call `_get_max_axis` to determine the quantization axes.",
                        "Calculate the scaling factor by finding the maximum absolute value of `kv` along the determined axes.",
                        "Scale the input `kv` tensor by the inverse of the scaling factor and a type-specific constant (e.g., MAX_INT8).",
                        "Round the scaled tensor to the nearest integer.",
                        "Cast the result to the target quantization dtype (`self.dtype`).",
                        "Return the quantized value and the scaling factor."
                    ],
                    "output": {
                        "shape": "A tuple containing the quantized tensor (same shape as input) and the scale tensor (shape broadcastable to input)."
                    },
                    "dependencies": [
                        "_get_max_axis",
                        "jnp.max",
                        "jnp.abs",
                        "jnp.rint"
                    ],
                    "notes": [
                        "Returns a tuple: (quantized_value, scale)."
                    ]
                },
                "einsum_fn_with_rhs_qtensor": {
                    "purpose": "Creates and configures an AQT (Quantized Tensor) einsum function where the right-hand-side operand is assumed to be a quantized tensor.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine the AQT configuration (`kv_cfg`) based on `self.dtype` (integer or fp8).",
                        "Optionally set dequantization and calibration modes for LHS and RHS using `aqt_config` functions.",
                        "Instantiate `aqt_flax.AqtEinsum` with the configured `kv_cfg`.",
                        "Return the `AqtEinsum` instance."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "aqt_config",
                        "aqt_flax.AqtEinsum"
                    ],
                    "notes": [
                        "This function prepares the einsum operation for attention, where the K or V tensor (the right-hand side) is quantized."
                    ]
                },
                "einsum_fn_with_rhs_qtensor_and_dequant": {
                    "purpose": "A convenience method to get a pre-configured AQT einsum function with specific dequantization modes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check `self.dtype`.",
                        "If `jnp.float8_e4m3fn`, call `einsum_fn_with_rhs_qtensor` with specific dequant/calibration modes for both LHS and RHS.",
                        "Otherwise (for int types), call `einsum_fn_with_rhs_qtensor` with specific dequant/calibration modes for RHS only.",
                        "Return the resulting `AqtEinsum` instance."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "einsum_fn_with_rhs_qtensor",
                        "aqt_config"
                    ],
                    "notes": [
                        "Provides a simplified interface for getting a commonly used einsum configuration for attention with a quantized KV cache."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_heads: int,\n    value_heads: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the KVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n    cache_logical_axis_names: The logical axis names for the cache.\n    cache_scale_logical_axis_names: The logical axis names for the cache scale.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    key_axis_order: The axis order for the key.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `KVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      KVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      kv_quant=kv_quant,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      key_axis_order=key_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "kv_cache_factory",
            "purpose": "Initializes a `KVCache` NNX module with the provided configuration and wraps it as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `KVCache` class into a Flax Linen module.",
                "Passes all configuration arguments to the `KVCache` constructor through the `to_linen` wrapper."
            ],
            "output": {
                "shape": "A Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "KVCache",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "max_prefill_length": "The maximum prefill length.",
                "max_target_length": "The maximum target length.",
                "batch": "The batch size.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "key_head_size": "The key head size.",
                "value_head_size": "The value head size.",
                "dtype": "The data type for the cache.",
                "kv_quant": "The KVQuant configuration for quantization, if any.",
                "model_mode": "The operational mode of the model (e.g., 'prefill', 'autoregressive')."
            },
            "notes": [
                "This function serves as a factory to create a Linen-compatible version of the `KVCache` NNX module.",
                "It forwards most of its arguments directly to the `KVCache` constructor.",
                "The `metadata_fn` is set to `variable_to_logically_partitioned` to handle tensor sharding."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVCache(nnx.Module):\n  \"\"\"Implementation of the KVCache.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len, key_heads,\n      # and value_heads from key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_heads: int,\n      value_heads: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in KVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the KVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      model_mode: The model mode.\n      use_chunked_prefill: Whether to use chunked prefill.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.max_prefill_length = max_prefill_length\n    self.max_target_length = max_target_length\n    self.batch = batch\n    self.key_seq_len = key_seq_len\n    self.value_seq_len = value_seq_len\n    self.key_heads = key_heads\n    self.value_heads = value_heads\n    self.key_head_size = key_head_size\n    self.value_head_size = value_head_size\n    self.dtype = dtype\n    self.kv_quant = kv_quant\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.key_axis_order = key_axis_order\n    self.model_mode = model_mode\n    self.use_chunked_prefill = use_chunked_prefill\n\n    if model_mode in (MODEL_MODE_PREFILL, MODEL_MODE_AUTOREGRESSIVE):\n      self._initialize_prefill_caches(model_mode)\n      self._initialize_ar_cache_vars(model_mode)\n\n  @property\n  def prefill_key_vars(self):\n    return (self.cached_prefill_key, self.cached_prefill_key_scale)\n\n  @property\n  def prefill_value_vars(self):\n    return (self.cached_prefill_value, self.cached_prefill_value_scale)\n\n  @property\n  def ar_key_vars(self):\n    return (self.cached_ar_key, self.cached_ar_key_scale)\n\n  @property\n  def ar_value_vars(self):\n    return (self.cached_ar_value, self.cached_ar_value_scale)\n\n  def _get_cached_kv_dtype(self):\n    return self.kv_quant.dtype if self.kv_quant else self.dtype\n\n  def _get_cache_scale_logical_shape(self, heads, cache_length):\n    assert self.kv_quant\n    if self.kv_quant.axis_cfg == \"dkv\":\n      return (self.batch, cache_length, heads, 1)\n    if self.kv_quant.axis_cfg == \"heads_and_dkv\":\n      return (self.batch, cache_length, 1, 1)\n    raise ValueError(f\"Invalid config for kv_quant_axis:{self.kv_quant.axis_cfg}\")\n\n  def _initialize_prefill_caches(self, model_mode):\n    \"\"\"Get a shaped abstraction of the state\"\"\"\n\n    cache_length = self.max_prefill_length\n    dtype = self._get_cached_kv_dtype()\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    self.cached_prefill_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_prefill_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n\n    self.cache_prefill_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      self.cached_prefill_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_prefill_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_prefill_key_scale = None\n      self.cached_prefill_value_scale = None\n\n  def _get_prefill_cache_vars(self):\n    return self.prefill_key_vars, self.prefill_value_vars, self.cache_prefill_segment_id\n\n  def _initialize_ar_cache_vars(self, model_mode):\n    \"\"\"get ar cache vars\"\"\"\n\n    dtype = self._get_cached_kv_dtype()\n    if self.max_target_length <= self.max_prefill_length:\n      raise ValueError(\n          f\"max_target_length: {self.max_target_length} should be greater than max_prefill_length:\"\n          f\" {self.max_prefill_length}!\"\n      )\n    cache_length = self.max_target_length - self.max_prefill_length\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    # TODO(b/339703100): investigate the issue why with_logical_partitioning doesn't enforce sharding\n    self.cached_ar_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_key.value = nn.with_logical_constraint(\n        self.cached_ar_key.value,\n        cache_axis_names,\n    )\n\n    self.cached_ar_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_value.value = nn.with_logical_constraint(\n        self.cached_ar_value.value,\n        cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n    self.cache_ar_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    self.cached_ar_lengths = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0],), dtype=jnp.int32),\n        sharding=(CACHE_BATCH,),\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      self.cached_ar_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_ar_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_ar_key_scale = None\n      self.cached_ar_value_scale = None\n\n    self.cache_ar_index = nnx.Cache(\n        jnp.zeros((1,), dtype=jnp.int32),\n        sharding=(),\n    )\n\n  def _get_ar_cache_vars(self):\n    return self.ar_key_vars, self.ar_value_vars, self.cache_ar_segment_id, self.cache_ar_index, self.cached_ar_lengths\n\n  def kv_cache_chunked_prefill(\n      self, key: Array, value: Array, decoder_segment_ids: Array, previous_chunk: None | Array = None\n  ):\n    \"\"\"Update the current kv cache into previous chunk and return needed length.\n\n    The previous chunk kv cache should be in the model's param.\n\n    Prefill cache need to be max prefill length to prevent different shape of kv cache.\n    Different shape of kv cache in previous chunk could produce different compiled graph.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n      previous_chunk:\n        In shape [b, s]. The tokens without padding in previous chunk.\n        Use to preserve the previous kv cache.\n\n    Returns:\n      key, value, decoder_segment_id.\n    \"\"\"\n\n    assert not self.kv_quant, \"Not support kv_quant now.\"\n    if decoder_segment_ids is not None:\n      _, segment_id_seq_len = decoder_segment_ids.shape\n      assert self.key_seq_len == segment_id_seq_len, f\"{self.key_seq_len=}, {segment_id_seq_len=} should match.\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n    assert self.key_seq_len == self.value_seq_len, f\"{self.key_seq_len=}, {self.value_seq_len=} should match.\"\n\n    next_pos = 0\n    if previous_chunk is not None:\n      # We only have 1 prompt in prefill mode.\n      next_pos = previous_chunk.shape[1]\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n    # TODO: Find a way to not enable the ar cache for prefill mode.\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    # For quantized kv cached. Could be get without transpose twice.\n    cached_key = self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order)\n    cached_value = self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order)\n    cached_key_value = jnp.transpose(cached_key, self.prefill_cache_axis_order)\n    cached_value_value = jnp.transpose(cached_value, self.prefill_cache_axis_order)\n\n    seq_axis = self.prefill_cache_logical_axis_names.index(CACHE_SEQUENCE)\n    cache_seq_axis = self.prefill_cache_axis_order.index(seq_axis)\n\n    assert next_pos + key_shaped_for_cache.shape[cache_seq_axis] <= self.max_prefill_length, (\n        f\"Previous kv cache[{next_pos}] + \"\n        f\"current kv cache[{key_shaped_for_cache.shape[cache_seq_axis]}] \"\n        f\"> max length[{self.max_prefill_length}]\"\n    )\n\n    # We don't zero out remain values. Use segment id to mask out.\n    cached_prefill_key_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_key_value, key_shaped_for_cache, next_pos, cache_seq_axis\n    )\n    cached_prefill_value_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_value_value, value_shaped_for_cache, next_pos, cache_seq_axis\n    )\n\n    if decoder_segment_ids is not None:\n      # Need zero out the remain values to prevent wrong mask in autoregressive.\n      previous_segment_id = cached_prefill_segment_id_var.value[:, :next_pos]\n      cached_prefill_segment_id_var.value = jnp.zeros_like(cached_prefill_segment_id_var.value, dtype=jnp.int32)\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, previous_segment_id, start_index=0, axis=1\n      )\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, decoder_segment_ids, next_pos, axis=1\n      )\n\n    # Return needed kv cache to reduce computation of attention.\n    needed_prefill_key_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_key_vars[0].value, start_index=0, slice_size=(next_pos + self.key_seq_len), axis=cache_seq_axis\n    )\n    needed_prefill_value_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_value_vars[0].value, start_index=0, slice_size=(next_pos + self.value_seq_len), axis=cache_seq_axis\n    )\n    needed_segment_id = None\n    if decoder_segment_ids is not None:\n      needed_segment_id = jax.lax.dynamic_slice_in_dim(\n          cached_prefill_segment_id_var.value, start_index=0, slice_size=(next_pos + segment_id_seq_len), axis=1\n      )\n\n    return (\n        jnp.transpose(needed_prefill_key_value, self.key_axis_order),\n        jnp.transpose(needed_prefill_value_value, self.key_axis_order),\n        needed_segment_id,\n    )\n\n  def kv_cache_prefill(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n  ):\n    \"\"\"In prefill mode, we zero out the existing cache, run the computation and\n    prepare the cache as necessary.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n\n    Returns:\n      key, value, decoder_segment_id.\n\n    \"\"\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    if self.kv_quant:\n      prefill_key_axis_names = transpose_tuple(self.cache_logical_axis_names, self.prefill_cache_axis_order)\n      key_shaped_for_cache, key_scale_shaped_for_cache = self.kv_quant.quantize(\n          key_shaped_for_cache, prefill_key_axis_names\n      )\n      value_shaped_for_cache, value_scale_shaped_for_cache = self.kv_quant.quantize(\n          value_shaped_for_cache, prefill_key_axis_names\n      )\n      cached_prefill_key_vars[1].value = key_scale_shaped_for_cache\n      cached_prefill_value_vars[1].value = value_scale_shaped_for_cache\n\n    cached_prefill_key_vars[0].value = key_shaped_for_cache\n    cached_prefill_value_vars[0].value = value_shaped_for_cache\n\n    if decoder_segment_ids is not None:\n      cached_prefill_segment_id_var.value = decoder_segment_ids\n    return key, value, decoder_segment_ids\n\n  def update_ar_key_value(\n      self,\n      one_token_key: Array,\n      one_token_value: Array,\n      key_caches: tuple[nnx.Cache, nnx.Cache | None],\n      value_caches: tuple[nnx.Cache, nnx.Cache | None],\n      one_hot_indices: Array,\n      lengths: Array,\n      use_ragged_attention: bool,\n  ) -> None:\n    \"\"\"Adds a single token's results to the ar kv cache\n\n    Args:\n        one_token_key (Array): Key of one token to add to the cache\n        one_token_value (Array): Value of one token to add to the cache\n        cached_ar_key (tuple[nnx.Cache, nnx.Cache|None],): Cached keys to add new token key to, possibly with scale\n        cached_ar_value (tuple[nnx.Cache, nnx.Cache|None],: Cached values to add new token value to, possible with scale\n        one_hot_indices (Array): Location of the new token within the cache\n\n    Returns:\n        tuple[Array, Array]: Updated caches for key and value with new token info added\n    \"\"\"\n\n    cached_key, cached_key_scale = key_caches\n    cached_value, cached_value_scale = value_caches\n\n    # In order to update the key, value caches with the current key and\n    # value, we reshape the one_token_key and one_token_value\n    one_token_key_shaped_for_cache = jnp.transpose(one_token_key, self.ar_cache_axis_order)\n    one_token_value_shaped_for_cache = jnp.transpose(one_token_value, self.ar_cache_axis_order)\n\n    ar_cache_axis_names = transpose_tuple(self.cache_logical_axis_names, self.ar_cache_axis_order)\n    if self.kv_quant:\n      one_token_key_shaped_for_cache, one_token_key_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_key_shaped_for_cache, ar_cache_axis_names\n      )\n      one_token_value_shaped_for_cache, one_token_value_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_value_shaped_for_cache, ar_cache_axis_names\n      )\n\n    ar_cache_update_idx = jnp.squeeze(one_hot_indices)\n    ar_cache_sequence_axis = ar_cache_update_axis = ar_cache_axis_names.index(CACHE_SEQUENCE)\n    ar_cache_batch_axis = ar_cache_axis_names.index(CACHE_BATCH)\n\n    if use_ragged_attention:\n      cache_locations = [slice(None)] * 4\n      new_token_locations = [slice(None)] * 4\n      new_token_locations[ar_cache_sequence_axis] = 0\n\n      def key_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_key_shaped_for_cache[tuple(new_token_locations)])\n\n      def value_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_value_shaped_for_cache[tuple(new_token_locations)])\n\n      cached_key.value = jax.lax.fori_loop(\n          0, one_token_key_shaped_for_cache.shape[0], key_body, cached_key.value, unroll=8\n      )\n      cached_value.value = jax.lax.fori_loop(\n          0, one_token_value_shaped_for_cache.shape[0], value_body, cached_value.value, unroll=8\n      )\n\n    else:\n      one_hot_indices = one_hot_indices.astype(int)\n\n      # Align batch size for cache with new token in decoding\n      if cached_key.value.shape[2] != one_token_key_shaped_for_cache.shape[2]:\n        cached_key.value = jnp.repeat(cached_key.value, one_token_key_shaped_for_cache.shape[2], axis=2)\n        cached_value.value = jnp.repeat(cached_value.value, one_token_value_shaped_for_cache.shape[2], axis=2)\n\n      cached_key.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key.value, one_token_key_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n      cached_value.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value.value, one_token_value_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n    cached_key.value = nn.with_logical_constraint(cached_key.value, ar_cache_axis_names)\n    cached_value.value = nn.with_logical_constraint(cached_value.value, ar_cache_axis_names)\n\n    if self.kv_quant:\n      ar_cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n      ar_cache_scale_update_axis = ar_cache_scale_axis_names.index(CACHE_SCALE_SEQUENCE)\n      assert cached_key_scale is not None, \"cached_key_scale_var cannot be None\"\n      assert cached_value_scale is not None, \"cached_value_scale_var cannot be None\"\n      cached_key_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key_scale.value, one_token_key_scale_shaped_for_cache, ar_cache_update_idx, ar_cache_scale_update_axis\n      )\n      cached_value_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value_scale.value,\n          one_token_value_scale_shaped_for_cache,\n          ar_cache_update_idx,\n          ar_cache_scale_update_axis,\n      )\n\n  def get_cached_values(self, cache_vars, target_dtype, cache_axis_order) -> jax.Array | KVTensor:\n    \"\"\"get cached values\"\"\"\n    cache_var, cache_scale_var = cache_vars\n    cache_value = cache_var.value\n    if cache_scale_var is not None:\n      scale_value = cache_scale_var.value\n      dtype = cache_value.dtype\n      if dtype == jnp.int8:\n        scale_value /= MAX_INT8\n      elif dtype == jnp.int4:\n        scale_value /= MAX_INT4\n      elif dtype == jnp.float8_e4m3fn:\n        scale_value /= E4M3_MAX\n\n      cache_value = KVTensor(qvalue=cache_value, scale=[scale_value], scale_t=None, dequant_dtype=target_dtype, bias=[])\n    cache_value_in_logical_shape = jax.tree.map(lambda x: reverse_transpose(x, cache_axis_order), cache_value)\n    return cache_value_in_logical_shape\n\n  def kv_cache_autoregressive(\n      self,\n      key: Array,\n      value: Array,\n      use_ragged_attention: bool = False,\n  ):\n    \"\"\"In autoregressive mode, we update the cache for this entry and\n       then return the full cache.\n\n    Args:\n      key: in shape [b, 1, n, d].\n      value: in shape [b, 1, n, d].\n      decoder_segment_ids: [b, 1] -- marking segment ids for tokens\n\n    Returns:\n      tuple of (key, value, segment_id) for both prefill and ar cache,\n    Raises:\n      ValueError: when key/value shape is not [batch, 1, num_heads, heads_dim].\n    \"\"\"\n    _, sequence, _, _ = value.shape\n    if sequence != 1:\n      raise ValueError(f\"Sequence length should be 1 during autoregression, got {sequence=}\")\n\n    cached_ar_key_vars, cached_ar_value_vars, cached_ar_segment_id_var, cache_ar_index_var, cache_ar_lengths_var = (\n        self._get_ar_cache_vars()\n    )\n\n    self.update_ar_key_value(\n        key,\n        value,\n        cached_ar_key_vars,\n        cached_ar_value_vars,\n        cache_ar_index_var.value,\n        cache_ar_lengths_var.value,\n        use_ragged_attention,\n    )\n    active_indicator = jnp.zeros((self.batch, 1), dtype=jnp.int32) + DECODING_ACTIVE_SEQUENCE_INDICATOR\n\n    # Align batch size for cached segment IDs with indicator in decoding\n    if cached_ar_segment_id_var.value.shape[0] != active_indicator.shape[0]:\n      cached_ar_segment_id_var.value = jnp.repeat(cached_ar_segment_id_var.value, active_indicator.shape[0], axis=0)\n\n    cached_ar_segment_id_var.value = jax.lax.dynamic_update_index_in_dim(\n        cached_ar_segment_id_var.value, active_indicator, jnp.squeeze(cache_ar_index_var.value), 1\n    )\n    cache_ar_index_var.value = jnp.mod(cache_ar_index_var.value + 1, self.max_target_length - self.max_prefill_length)\n    cache_ar_lengths_var.value = cache_ar_lengths_var.value.at[:].add(1)\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    cached_prefill = (\n        self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order),\n        self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order),\n        cached_prefill_segment_id_var.value,\n    )\n\n    cached_ar = (\n        self.get_cached_values(cached_ar_key_vars, key.dtype, self.ar_cache_axis_order),\n        self.get_cached_values(cached_ar_value_vars, value.dtype, self.ar_cache_axis_order),\n        cached_ar_segment_id_var.value,\n        cache_ar_lengths_var.value,\n    )\n    return cached_prefill, cached_ar\n\n  def __call__(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple:\n    \"\"\"KV cache takes the current state and updates the state accordingly.\n\n    The key and value have dimension [b, s, n_kv, d],\n    but we cache them with a reshape as defined in *_axis_order config as a TPU\n    fusion optimization. This also enables the \"scatter via one-hot\n    broadcast\" trick, which means we do a one-hot broadcast instead of a\n    scatter/gather operations, resulting in a 3-4x speedup in practice.\n\n    Args:\n      key: in shape [b, s, n_kv, d].\n      value: in shape [b, s, n_kv, d].\n      model_mode: model mode controlling model.\n\n    Returns:\n      two tuples of (k, v, decoder_segments) -- either can be Nones\n\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      if self.use_chunked_prefill:\n        return self.kv_cache_chunked_prefill(key, value, decoder_segment_ids, previous_chunk), None\n      else:\n        return self.kv_cache_prefill(key, value, decoder_segment_ids), None\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      return self.kv_cache_autoregressive(key, value, use_ragged_attention)\n    else:\n      raise ValueError(f\"Model Mode isn't supported! {model_mode=}\")",
        "analysis": {
            "module_type": "kv_cache",
            "purpose": "Manages the key-value cache for attention mechanisms during prefill and autoregressive decoding, handling different modes, chunking, and optional quantization.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "The `__call__` method is the main entry point, which dispatches to other methods based on `model_mode`.",
                "If `model_mode` is 'prefill', it calls `kv_cache_prefill` or `kv_cache_chunked_prefill` to populate the cache.",
                "If `model_mode` is 'autoregressive', it calls `kv_cache_autoregressive` to update the cache with a single token and return the full cache contents."
            ],
            "output": {
                "shape": "Returns a tuple of two tuples: `(prefill_cache, ar_cache)`. One of these is typically `None` depending on the `model_mode`. See the `__call__` method for details."
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "MaxText.layers.kv_cache.KVQuant",
                "MaxText.layers.kv_cache.KVTensor"
            ],
            "parameters": {
                "max_prefill_length": "The maximum prefill length.",
                "max_target_length": "The maximum target length.",
                "batch": "The batch size.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "key_head_size": "The key head size.",
                "value_head_size": "The value head size.",
                "dtype": "The data type for the cache.",
                "kv_quant": "Optional configuration for KV cache quantization.",
                "use_chunked_prefill": "Whether to use chunked prefill.",
                "model_mode": "The model mode (e.g., prefill, autoregressive) to initialize caches for."
            },
            "notes": [
                "The class initializes different cache variables (`cached_prefill_*`, `cached_ar_*`) for prefill and autoregressive decoding phases based on the `model_mode` during initialization.",
                "It uses axis order configurations (`*_axis_order`) to transpose tensors for potential TPU performance optimizations.",
                "The `__call__` method acts as a dispatcher, routing to the appropriate caching logic based on the `model_mode` argument."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVCache module, setting up dimensions, configurations, and initializing prefill and autoregressive caches.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store all configuration parameters as instance attributes.",
                        "Check if `model_mode` is `MODEL_MODE_PREFILL` or `MODEL_MODE_AUTOREGRESSIVE`.",
                        "If so, call `_initialize_prefill_caches`.",
                        "If so, call `_initialize_ar_cache_vars`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_initialize_prefill_caches",
                        "_initialize_ar_cache_vars"
                    ],
                    "notes": [
                        "The `rngs` argument is unused but present for compatibility with `nnx_wrappers.to_linen`."
                    ]
                },
                "kv_cache_chunked_prefill": {
                    "purpose": "Updates the prefill cache with a new chunk of key-value pairs, appending to any previous chunks.",
                    "input": {
                        "shape": "key: [b, s, n, d], value: [b, s, n, d], decoder_segment_ids: [b, s], previous_chunk: [b, prev_s] or None",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Determine the starting position (`next_pos`) based on `previous_chunk`.",
                        "Transpose input key and value to match cache layout.",
                        "Update the prefill key, value, and segment ID caches using `jax.lax.dynamic_update_slice_in_dim`.",
                        "Slice the updated caches to return only the currently relevant portion.",
                        "Transpose the sliced key and value back to the logical layout."
                    ],
                    "output": {
                        "shape": "A tuple of (key, value, decoder_segment_id). key/value shape: [b, next_pos + s, n, d], segment_id shape: [b, next_pos + s]"
                    },
                    "dependencies": [
                        "jax.lax.dynamic_update_slice_in_dim",
                        "jnp.transpose",
                        "get_cached_values"
                    ],
                    "notes": [
                        "This method does not support KV quantization."
                    ]
                },
                "kv_cache_prefill": {
                    "purpose": "Populates the prefill cache with the initial key-value pairs, overwriting any existing data.",
                    "input": {
                        "shape": "key: [b, s, n, d], value: [b, s, n, d], decoder_segment_ids: [b, s]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Transpose input key and value to match cache layout.",
                        "If quantization is enabled, quantize the key/value and update the scale caches.",
                        "Update the prefill key and value caches with the (possibly quantized) transposed inputs.",
                        "Update the segment ID cache.",
                        "Return the original, unmodified inputs."
                    ],
                    "output": {
                        "shape": "A tuple of (key, value, decoder_segment_id), identical to the input."
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "KVQuant.quantize"
                    ],
                    "notes": []
                },
                "update_ar_key_value": {
                    "purpose": "Updates the autoregressive key-value cache with a single new token's key and value.",
                    "input": {
                        "shape": "one_token_key: [b, 1, n, d], one_token_value: [b, 1, n, d], one_hot_indices: [1,], lengths: [b,]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Transpose the input key and value to match cache layout.",
                        "If quantization is enabled, quantize the key/value.",
                        "If `use_ragged_attention`, use a `jax.lax.fori_loop` to update the cache at the correct index for each item in the batch.",
                        "If not `use_ragged_attention`, use `jax.lax.dynamic_update_index_in_dim` to update the cache.",
                        "Apply logical constraints to the updated caches.",
                        "If quantization is enabled, update the scale caches."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "KVQuant.quantize",
                        "jax.lax.fori_loop",
                        "jax.lax.dynamic_update_index_in_dim",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "This method modifies the cache variables in place."
                    ]
                },
                "get_cached_values": {
                    "purpose": "Retrieves values from the cache, dequantizing them if necessary, and transposing them back to the logical layout.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the value and scale from the input cache variables.",
                        "If a scale exists, rescale it and wrap the value and scale in a `KVTensor` object.",
                        "Reverse the transpose operation to restore the logical shape."
                    ],
                    "output": {
                        "shape": "Logical shape of the cache tensor."
                    },
                    "dependencies": [
                        "KVTensor",
                        "reverse_transpose"
                    ],
                    "notes": []
                },
                "kv_cache_autoregressive": {
                    "purpose": "Performs a single step of autoregressive decoding: updates the AR cache with a new token and returns the full prefill and AR caches.",
                    "input": {
                        "shape": "key: [b, 1, n, d], value: [b, 1, n, d]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Validate that the input sequence length is 1.",
                        "Call `update_ar_key_value` to add the new key/value to the AR cache.",
                        "Update the AR segment ID cache with an active indicator.",
                        "Increment and wrap the AR cache index and increment lengths.",
                        "Call `get_cached_values` for both prefill and AR key/value caches to retrieve and dequantize them.",
                        "Return the full prefill and AR caches as two separate tuples."
                    ],
                    "output": {
                        "shape": "A tuple of two tuples: `(cached_prefill, cached_ar)`, where `cached_prefill` is `(key, value, segment_id)` and `cached_ar` is `(key, value, segment_id, lengths)`."
                    },
                    "dependencies": [
                        "update_ar_key_value",
                        "get_cached_values"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Main entry point that dispatches to the correct caching logic based on the model's current mode (prefill, chunked prefill, or autoregressive).",
                    "input": {
                        "shape": "key: [b, s, n_kv, d], value: [b, s, n_kv, d], decoder_segment_ids: [b, s]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "If `model_mode` is `MODEL_MODE_PREFILL`:",
                        "  If `self.use_chunked_prefill`, call `kv_cache_chunked_prefill`.",
                        "  Else, call `kv_cache_prefill`.",
                        "If `model_mode` is `MODEL_MODE_AUTOREGRESSIVE`, call `kv_cache_autoregressive`.",
                        "Otherwise, raise a `ValueError`."
                    ],
                    "output": {
                        "shape": "A tuple of two tuples: `(prefill_output, ar_output)`. In prefill mode, `ar_output` is `None`. In autoregressive mode, both are populated."
                    },
                    "dependencies": [
                        "kv_cache_chunked_prefill",
                        "kv_cache_prefill",
                        "kv_cache_autoregressive"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#mla_kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def mla_kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    key_heads: int = 1,\n    value_heads: int = 1,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the MlaKVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `MlaKVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MlaKVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      kv_quant=kv_quant,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "mla_kv_cache_factory",
            "purpose": "A factory function that initializes an NNX `MlaKVCache` module and wraps it as a Flax Linen module for use in a Linen model.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Receives configuration parameters for the `MlaKVCache` such as max lengths, batch size, dimensions, and data type.",
                "Calls `nnx_wrappers.to_linen` to wrap the `MlaKVCache` class.",
                "Passes all configuration parameters, along with a `metadata_fn` for logical partitioning and an `abstract_init` flag, to the wrapper function."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlaKVCache",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "max_target_length": "The maximum total sequence length (prefill + generated tokens) for the cache.",
                "model_mode": "Specifies the operational mode, such as 'prefill' or 'autoregressive', which determines the cache's behavior.",
                "use_chunked_prefill": "A boolean flag to indicate whether to use chunked prefill, which processes the prefill sequence in parts.",
                "kv_quant": "An optional configuration object for quantizing the key-value cache."
            },
            "notes": [
                "This function acts as a bridge, making an NNX-based module (`MlaKVCache`) compatible with the Flax Linen API.",
                "The returned object is a `flax.linen.Module` instance, ready to be used within a larger Linen model.",
                "The `metadata_fn` argument is used to apply logical partitioning rules to the cache variables for distributed computation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#MlaKVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class MlaKVCache(KVCache):\n  \"\"\"Implementation of the KVCache for MLA.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len,\n      # key_head_size, value_head_size, key_heads, and value_heads from\n      # key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      key_heads: int = 1,\n      value_heads: int = 1,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in MlaKVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the MlaKVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill\n        cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache\n        scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      use_chunked_prefill: Whether to use chunked prefill.\n      model_mode: The model mode.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    super().__init__(\n        max_prefill_length=max_prefill_length,\n        max_target_length=max_target_length,\n        batch=batch,\n        key_seq_len=key_seq_len,\n        value_seq_len=value_seq_len,\n        key_heads=key_heads,\n        value_heads=value_heads,\n        key_head_size=key_head_size,\n        value_head_size=value_head_size,\n        dtype=dtype,\n        kv_quant=kv_quant,\n        prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n        cache_logical_axis_names=cache_logical_axis_names,\n        cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        key_axis_order=key_axis_order,\n        use_chunked_prefill=use_chunked_prefill,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def key_latent_add_head_dim(self, key_latent: Array):\n    b, l, hz = key_latent.shape\n    return key_latent.reshape(b, l, 1, hz)\n\n  def key_latent_remove_head_dim(self, key_latent: Array):\n    b, l, _, hz = key_latent.shape\n    return key_latent.reshape(b, l, hz)\n\n  def __call__(\n      self,\n      key_latent: Array,\n      key_rope: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple[\n      None | tuple[Array, Array, Array],\n      None | tuple[Array, Array, Array, Array],\n  ]:\n    assert model_mode != MODEL_MODE_TRAIN, \"incorrectly updating kvcache in train mode.\"\n    assert self.kv_quant is None, \"kvcache quantization not supported with mla.\"\n    key_latent = self.key_latent_add_head_dim(key_latent)\n    prefill_cache, ar_cache = super().__call__(key_latent, key_rope, decoder_segment_ids, model_mode)\n    if prefill_cache:\n      key_latent, key_rope, decoder_segments_ids = prefill_cache\n      prefill_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n      )\n    if ar_cache:\n      key_latent, key_rope, decoder_segments_ids, lengths = ar_cache\n      ar_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n          lengths,\n      )\n    return prefill_cache, ar_cache",
        "analysis": {
            "module_type": "mla_kv_cache",
            "purpose": "An implementation of the Key-Value cache specifically for Multi-Layer Attention (MLA), which handles tensors that initially lack a head dimension.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the parent KVCache with provided configuration, including specific logical axis names for MLA."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "KVCache"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum total sequence length for generation.",
                "batch": "The batch size.",
                "key_head_size": "The dimension of the key head.",
                "model_mode": "The operating mode, e.g., 'prefill' or 'autoregressive'.",
                "prefill_cache_logical_axis_names": "Logical axis names for partitioning the prefill cache, defaulting to a layout suitable for MLA (using CACHE_HEADS_NONE)."
            },
            "notes": [
                "This class inherits from KVCache and adapts its functionality for MLA.",
                "It assumes the input `key_latent` does not have a head dimension and adds/removes it before/after calling the parent's logic.",
                "KV cache quantization is explicitly not supported, as checked by an assertion in the `__call__` method."
            ],
            "methods": {
                "key_latent_add_head_dim": {
                    "purpose": "Reshapes the input `key_latent` tensor to add a head dimension of size 1.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Reshapes the input tensor to insert a new dimension of size 1 for the heads."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, 1, hidden_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "This method makes the MLA `key_latent` tensor compatible with the parent `KVCache` which expects a head dimension."
                    ]
                },
                "key_latent_remove_head_dim": {
                    "purpose": "Reshapes the input `key_latent` tensor to remove its head dimension.",
                    "input": {
                        "shape": "[batch_size, sequence_length, 1, hidden_dim]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Reshapes the input tensor to remove the head dimension, assuming it is of size 1."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "This method restores the original shape of the MLA `key_latent` tensor after it has been processed by the parent `KVCache`."
                    ]
                },
                "__call__": {
                    "purpose": "Updates the key-value cache for MLA by adding a head dimension to `key_latent`, invoking the parent's cache update logic, and then removing the head dimension from the resulting cached tensor.",
                    "input": {
                        "shape": "{'key_latent': [batch, seq_len, hidden_dim], 'key_rope': [batch, seq_len, num_heads, head_dim], 'decoder_segment_ids': [batch, seq_len]}",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Assert that the model is not in training mode.",
                        "Assert that KV quantization is disabled.",
                        "Call `self.key_latent_add_head_dim` to add a head dimension to `key_latent`.",
                        "Call the parent `KVCache.__call__` method with the modified `key_latent`.",
                        "If `prefill_cache` is returned, call `self.key_latent_remove_head_dim` on its `key_latent` component.",
                        "If `ar_cache` is returned, call `self.key_latent_remove_head_dim` on its `key_latent` component.",
                        "Return the modified cache tuples."
                    ],
                    "output": {
                        "shape": "A tuple of (prefill_cache, ar_cache). Each cache is a tuple of tensors, where the `key_latent` component has its head dimension removed."
                    },
                    "dependencies": [
                        "KVCache.__call__",
                        "self.key_latent_add_head_dim",
                        "self.key_latent_remove_head_dim"
                    ],
                    "notes": [
                        "This method acts as a wrapper around the parent class's `__call__` to handle the specific shape requirements of MLA's `key_latent`.",
                        "The `key_rope` input is passed as the `value` argument to the parent's `__call__` method."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InputData",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InputData:\n  \"\"\"Container for input data and metadata.\n\n  Attributes:\n      id: Unique identifier for this input\n      tokens: JAX array containing the tokenized input\n      true_length: Actual length of the input before padding\n  \"\"\"\n\n  id: str\n  tokens: jax.Array | np.ndarray\n  true_length: int",
        "analysis": {
            "functionality": "This code block defines a dataclass named `InputData` that serves as a container for input data and its associated metadata for an inference engine.",
            "usage": "Instantiate this class to structure input for the inference process. It requires three attributes: `id` (a unique string identifier), `tokens` (a JAX or NumPy array of token IDs), and `true_length` (an integer representing the sequence length before padding)."
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#CompletionOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class CompletionOutput:\n  \"\"\"Container for model generation output.\n\n  Attributes:\n      index: The index of the output in the request.\n      token_ids: The token IDs of the prompt and generated output text.\n      logprobs: The log probabilities of the prompt and generated output tokens.\n      prompt_length: The number of prompt tokens.\n  \"\"\"\n\n  index: str\n  token_ids: np.ndarray\n  logprobs: np.ndarray\n  prompt_length: int",
        "analysis": {
            "functionality": "Defines a data class, `CompletionOutput`, to store the results of a model's text generation, including token IDs, log probabilities, and metadata.",
            "usage": "Instantiate this class to hold the output of a text generation task. The constructor takes `index` (str), `token_ids` (np.ndarray), `logprobs` (np.ndarray), and `prompt_length` (int) as arguments. It is used as a structured return object for inference functions."
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#TokenOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class TokenOutput:\n  \"\"\"Container for individual token generation result.\"\"\"\n\n  token: np.ndarray\n  log_prob: np.ndarray",
        "analysis": {
            "module_type": "token_result_container",
            "purpose": "A data class designed to store the output of a single token generation step, containing the generated token and its log probability.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "dataclasses"
            ],
            "parameters": {},
            "notes": [
                "This class is a simple data structure and does not perform any computation.",
                "It is used within the `InferenceWorker` to store the results of each decoding step for a given sequence."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#DetokenizationTask",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class DetokenizationTask:\n  \"\"\"Container for detokenization work to be done on background thread.\"\"\"\n\n  task_type: str  # \"prefill\" or \"decode\"\n  # For prefill tasks\n  result_tokens: Any = None\n  log_prob: Any = None\n  prompt_logp: Any = None\n  prompt_ids: list = None\n  slots: list = None\n  # For decode tasks\n  tokens_buffer: list = None\n  logprob_buffer: list = None",
        "analysis": {
            "module_type": "data_container",
            "purpose": "A data container for holding information about a detokenization task to be processed by a background thread.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "dataclasses"
            ],
            "parameters": {
                "task_type": "A string indicating the type of task, either 'prefill' or 'decode'.",
                "result_tokens": "For 'prefill' tasks, contains the generated tokens after the prompt is processed.",
                "log_prob": "For 'prefill' tasks, contains the log probabilities of the first generated tokens.",
                "prompt_logp": "For 'prefill' tasks, contains the log probabilities of the input prompt tokens.",
                "prompt_ids": "For 'prefill' tasks, a list of unique identifiers for the prompts being processed.",
                "slots": "For 'prefill' tasks, a list of KV cache slots assigned to the prompts.",
                "tokens_buffer": "For 'decode' tasks, a buffer of generated tokens from a single decode step across all active sequences.",
                "logprob_buffer": "For 'decode' tasks, a buffer of log probabilities corresponding to the tokens_buffer."
            },
            "notes": [
                "This is a dataclass used as a message object in a queue (`detokenization_queue`) for communication between the main inference thread and a background detokenization thread.",
                "The specific fields populated depend on the value of `task_type`."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#SafeThread",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class SafeThread(threading.Thread):\n  \"\"\"Thread class with exception handling to prevent silent failures.\"\"\"\n\n  def run(self):\n    try:\n      super().run()\n    except Exception as _:  # pylint: disable=broad-exception-caught\n      traceback.print_exc()\n      # Kill the process if a thread encounters an error\n      os.kill(os.getpid(), signal.SIGKILL)",
        "analysis": {
            "module_type": "exception_handling_thread",
            "purpose": "A subclass of threading.Thread that catches any unhandled exceptions in the thread's execution, prints a traceback, and then forcefully terminates the entire process to prevent silent failures.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a thread using standard threading.Thread arguments (e.g., target, args)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "threading.Thread",
                "os",
                "signal",
                "traceback"
            ],
            "parameters": {},
            "notes": [
                "This class is designed to make background thread failures catastrophic and visible, rather than allowing them to fail silently and potentially leave the main application in an inconsistent state.",
                "It uses os.kill with signal.SIGKILL to ensure the process is terminated immediately and cannot be ignored."
            ],
            "methods": {
                "run": {
                    "purpose": "Executes the thread's target function within a try-except block to handle any exceptions.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the parent class's `run` method to execute the thread's main logic.",
                        "If any `Exception` is caught, it prints the exception traceback.",
                        "It then gets the current process ID (PID) and sends a SIGKILL signal to it, terminating the process."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "super().run",
                        "traceback.print_exc",
                        "os.kill",
                        "os.getpid",
                        "signal.SIGKILL"
                    ],
                    "notes": [
                        "This method overrides the default `run` method of `threading.Thread` to add the exception handling and process termination logic."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillType",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillType(Enum):\n  \"\"\"Enumeration of supported prefill processing methods.\"\"\"\n\n  DEFAULT = \"default\"\n  BATCH = \"batch\"",
        "analysis": {
            "module_type": "enumeration",
            "purpose": "Defines an enumeration for the supported prefill processing methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This class provides named constants for different prefill strategies.",
                "The available members are DEFAULT ('default') and BATCH ('batch')."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillResult",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillResult:\n  \"\"\"Result from prefill processing operation.\"\"\"\n\n  result_tokens: \"jetstream.engine_api.ResultTokens\"\n  slot: int\n  prompt_logp: None | jax.Array",
        "analysis": {
            "module_type": "prefill_result_data_class",
            "purpose": "A data class to store the results of a prefill processing operation, including the first generated token, the assigned KV cache slot, and prompt log probabilities.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.Array",
                "jetstream.engine_api.ResultTokens"
            ],
            "parameters": {
                "result_tokens": "The generated tokens and their log probabilities from the prefill step, encapsulated in a ResultTokens object.",
                "slot": "The integer index of the decode slot in the KV cache assigned to this sequence.",
                "prompt_logp": "An optional JAX array containing the log probabilities of the input prompt tokens."
            },
            "notes": [
                "This is a dataclass used to structure the output from a prefill operation before it is passed to a callback or for further processing.",
                "It is instantiated within the `PrefillHelper` class and used in the `prefill_done` callback of the `InferenceWorker`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillHelper",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillHelper:\n  \"\"\"Abstraction layer for different prefill processing strategies.\n\n  Provides a unified interface for both default (single-sequence) and batch\n  (packed multi-sequence) prefill processing methods.\n  \"\"\"\n\n  def __init__(\n      self,\n      prefill_type: PrefillType,\n      engine: MaxEngine,\n      prefill_lengths: list[int],\n      batch_prefill_max_batch_size: int = 16,\n      rng=None,\n  ):\n    \"\"\"Initialize the PrefillHelper.\n\n    Args:\n        type: The type of prefill processor to use (\"default\" or \"batch\")\n        engine: The MaxEngine instance to use for prefill operations\n        prefill_lengths: list of prompt lengths to support\n        batch_prefill_max_batch_size: Maximum number of prompts in one packed\n            sequence for batch prefill\n    \"\"\"\n    self._type = prefill_type\n    self.engine = engine\n    self.prefill_lengths = sorted(prefill_lengths)\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    if prefill_type == PrefillType.DEFAULT:\n      self._processor = PrefillProcessor(engine)\n    elif prefill_type == PrefillType.BATCH:\n      self._batch_processor = BatchedPrefillProcessor(\n          engine=engine,\n          max_batch_size=batch_prefill_max_batch_size,\n          auto_layout_supported=False,\n      )\n      # Keep fallback processor for edge cases\n      self._processor = PrefillProcessor(engine)\n    else:\n      raise ValueError(f\"Invalid prefill type: {prefill_type}\")\n\n  @functools.partial(jax.jit, static_argnums=(0), donate_argnames=(\"decode_state\",))\n  def _jitted_single_prefill(\n      self, params, tokens, slot, true_length, decode_state, rng\n  ) -> tuple[jax.Array, jax.Array, DecodeState, jax.Array] | tuple[jax.Array, jax.Array, DecodeState]:\n    \"\"\"Prefill a single input.\"\"\"\n    # pylint: disable=protected-access\n    first_token, decode_state = self._processor._process(\n        params,\n        tokens,\n        slot,\n        true_length,\n        decode_state,\n        rng,\n        return_prompt_logp=True,\n    )\n\n    return (\n        first_token,\n        decode_state,\n        decode_state[\"prompt_logp\"],\n    )\n\n  def process(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      decode_slot: int,\n      input_id: int,\n      input_tokens_padded: jax.Array,\n      input_true_length: int,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Process an input through the appropriate prefill processor.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decode state\n        decode_slot: The decode slot index to use for this input\n        input_id: Unique identifier for this input\n        input_tokens_padded: Padded token array for the input\n        input_true_length: Actual length of the input before padding\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    padded_length = len(input_tokens_padded)\n    # Use default processor if configured or if input is already at max length\n    if self._type == PrefillType.DEFAULT or padded_length == self.max_prefill_length:\n      first_token, decode_state, prompt_logp = self._jitted_single_prefill(\n          model_params,\n          input_tokens_padded,\n          decode_slot,\n          input_true_length,\n          decode_state,\n          self.rng,\n      )\n      prefill_done(\n          [PrefillResult(first_token, decode_slot, prompt_logp)],\n          [input_id],\n          decode_state,\n      )\n    # Use batch processor for inputs that can benefit from prefill packing\n    elif self._type == PrefillType.BATCH:\n      self._batch_processor.process(\n          model_params,\n          decode_state,\n          decode_slot,\n          input_id,\n          input_tokens_padded[:input_true_length],\n          padded_length,\n          self.max_prefill_length,\n          prefill_done,\n          return_prompt_logp=True,\n      )\n\n  def finalize(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Finalize prefill operations, flushing any pending inputs.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decoder state\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    if self._type == PrefillType.DEFAULT:\n      # No finalization needed for default processor\n      pass\n    elif self._type == PrefillType.BATCH:\n      # Flush any remaining inputs in the batch processor\n      self._batch_processor.flush(model_params, decode_state, prefill_done, return_prompt_logp=True)",
        "analysis": {
            "module_type": "prefill_helper",
            "purpose": "Provides a unified interface for different prefill processing strategies, specifically default (single-sequence) and batch (packed multi-sequence) methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes either a `PrefillProcessor` for single-sequence prefill or a `BatchedPrefillProcessor` for packed prefill based on the `prefill_type`.",
                "The `process` method routes an input sequence to the appropriate processor.",
                "The `finalize` method flushes any pending inputs from the batched processor."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "MaxEngine",
                "PrefillProcessor",
                "BatchedPrefillProcessor",
                "PrefillType",
                "jax"
            ],
            "parameters": {
                "prefill_type": "The type of prefill processor to use, either 'default' or 'batch'.",
                "engine": "The MaxEngine instance to use for prefill operations.",
                "prefill_lengths": "A list of prompt lengths to support for padding and bucketing.",
                "batch_prefill_max_batch_size": "The maximum number of prompts to pack into a single sequence for batch prefill."
            },
            "notes": [
                "This class acts as an abstraction layer, hiding the complexity of whether prefill is happening one-by-one or in packed batches.",
                "When using the 'batch' type, it also instantiates a default `PrefillProcessor` as a fallback for edge cases (e.g., inputs that are already at the maximum prefill length)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PrefillHelper by creating the appropriate internal prefill processor(s) based on the specified `prefill_type`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like `prefill_type`, `engine`, and `prefill_lengths`.",
                        "Determine the maximum prefill length from `prefill_lengths`.",
                        "Initialize a JAX PRNG key.",
                        "If `prefill_type` is `DEFAULT`, create a `PrefillProcessor`.",
                        "If `prefill_type` is `BATCH`, create a `BatchedPrefillProcessor` and a fallback `PrefillProcessor`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillType",
                        "MaxEngine",
                        "PrefillProcessor",
                        "BatchedPrefillProcessor",
                        "jax.random.PRNGKey"
                    ],
                    "notes": [
                        "Raises a ValueError if an invalid `prefill_type` is provided."
                    ]
                },
                "_jitted_single_prefill": {
                    "purpose": "A JIT-compiled method to perform a prefill operation for a single input sequence.",
                    "input": {
                        "shape": "Inputs include `params` (Pytree), `tokens` (Array), `slot` (int), `true_length` (int), `decode_state` (Pytree), and `rng` (PRNGKey).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the internal `_process` method of its `PrefillProcessor` instance.",
                        "Returns the first generated token, the updated decode state, and the log probabilities of the prompt tokens."
                    ],
                    "output": {
                        "shape": "A tuple of (first_token, updated_decode_state, prompt_logp)."
                    },
                    "dependencies": [
                        "PrefillProcessor._process",
                        "jax.jit"
                    ],
                    "notes": [
                        "This method is decorated with `jax.jit` for performance.",
                        "It uses `donate_argnames=('decode_state',)` to allow JAX to optimize memory by modifying the decode state in-place."
                    ]
                },
                "process": {
                    "purpose": "Processes a single input sequence by routing it to the appropriate prefill processor (single or batched).",
                    "input": {
                        "shape": "Key input is `input_tokens_padded`, a JAX array representing the padded input sequence.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the prefill type is `DEFAULT` or if the input is already at the maximum prefill length.",
                        "If true, call `_jitted_single_prefill` and then the `prefill_done` callback with the results.",
                        "If false (and type is `BATCH`), call the `_batch_processor.process` method to add the input to a batch for later processing."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillType",
                        "PrefillResult",
                        "BatchedPrefillProcessor.process",
                        "self._jitted_single_prefill"
                    ],
                    "notes": [
                        "This method is asynchronous for batch prefill; it adds an item to a queue and the actual computation may happen later when the batch is full or flushed.",
                        "The results of the prefill operation are passed to the `prefill_done` callback function."
                    ]
                },
                "finalize": {
                    "purpose": "Finalizes prefill operations by flushing any pending inputs, which is primarily relevant for the batched prefill strategy.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If the prefill type is `BATCH`, call the `flush` method on the `_batch_processor` to process any remaining inputs.",
                        "If the prefill type is `DEFAULT`, this method is a no-op."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillType",
                        "BatchedPrefillProcessor.flush"
                    ],
                    "notes": [
                        "This must be called after all inputs have been submitted via `process` to ensure no inputs are left unprocessed in batch mode."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InferenceWorker",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InferenceWorker:\n  \"\"\"\n  InferenceWorker runs continuous batching over\n  a queue of inputs.\n\n    Continuous batching workflow:\n    1. Process inputs one at a time from queue\n    2. Prefill input and insert into KV cache\n    3. Continue prefilling until enough samples for batch decode\n    4. Decode until at least one sequence completes\n    5. Refill newly available decode slots with prefill\n    6. Repeat until all sequences complete\n\n    Prefill Packing:\n        When enable_batch_prefill is True, the prefill processor\n        will pack multiple inputs into a single sequence before\n        doing the prefill.\n\n        There are multiple buckets for packed sequences, where each bucket\n        contains inputs with the same padded length. Only inputs with the same\n        padded length can be packed together.\n\n        It is important to sort the inputs by padded length so that the\n        buckets fill up quickly.\n\n        When a decode slot frees up, the prefill processor will add the\n        sequence to a bucket. If the bucket becomes full, the packed sequence\n        will be prefilled.\n\n        E.g.\n        Bucket for length 64: [...seq1, ...seq2, ...seq3, ...seq4]\n        Bucket for length 128: [...seq1, ...seq2]\n        Bucket for length 256: [...seq1]\n\n  \"\"\"\n\n  def __init__(\n      self,\n      config: MaxTextConfig,\n      params: Params | None,\n      min_decode_steps: int,\n      enable_batch_prefill: bool,\n      devices: list[Any],\n      tokenizer: Any,\n      eos_ids: list[int],\n      prefill_lengths: list[int],\n      max_decode_length: int,\n      batch_prefill_max_batch_size: int,\n      is_pw_reshard: bool = True,\n      rng: jax.random.PRNGKey = None,\n      mesh: Mesh = None,\n      debug: bool = False,\n  ):\n    \"\"\"\n    Args:\n        config: MaxText configuration\n        params: Model parameters, if None, the params will be loaded from the config\n        min_decode_steps: Minimum number of decode steps to run at once\n        enable_batch_prefill: Whether to enable batch prefill\n        devices: JAX devices to use for this worker\n        tokenizer: Tokenizer to use\n        eos_ids: End-of-sequence token IDs\n        prefill_lengths: list of supported prefill lengths\n        max_decode_length: Maximum tokens to generate per sequence\n        batch_prefill_max_batch_size: Maximum batch size for batch prefill\n        run_as_a_thread: Whether to run in a separate thread\n        rng: Random number generator key\n        mesh: JAX mesh for distributed computation\n        is_pw_reshard: Whether to use Pathways for resharding\n    \"\"\"\n    # Configurations\n    self.config = config\n    self.params = params\n    self.devices = devices\n    self.is_pw_reshard = is_pw_reshard\n    self.enable_batch_prefill = enable_batch_prefill\n    self.prefill_type = PrefillType.BATCH if enable_batch_prefill else PrefillType.DEFAULT\n    self.prefill_lengths = prefill_lengths\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.max_decode_length = max_decode_length\n    self.eos_ids = eos_ids\n    self.tokenizer = tokenizer\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.min_decode_steps = min_decode_steps\n    self.mesh = mesh\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n\n    # Inference state (initialized later)\n    self.running = False\n    self.detokenization_queue = queue.Queue()\n    self.empty_decode_slots = set()\n    self.slot_to_id: dict[int, None | int] = {}\n    self.completed_sequences: set = set()\n\n    self.decode_state: DecodeState = None\n    self.completion_tokens_by_id: dict[Hashable, list[TokenOutput]] = {}\n    self.prompt_logprobs_by_id: dict[Hashable, list[np.ndarray]] = {}\n    self.true_lengths: dict[Hashable, int] = {}\n\n    # Model components (initialized later)\n    self.engine = None\n    self.decode_batch_size = None\n    self.prefill_helper = None\n    self.generate_fn = None\n\n    start_time = time.time()\n    # Initialize MaxEngine(s)\n    self.params, self.engine = self._init_engine(self.params)\n    self.tokenizer = self._init_tokenizer()\n    self.decode_batch_size = self.engine.max_concurrent_decodes\n    # Initialize prefill helper\n    self.prefill_helper = PrefillHelper(\n        self.prefill_type,\n        self.engine,\n        self.prefill_lengths,\n        self.batch_prefill_max_batch_size,\n        rng=self.rng,\n    )\n    # Initialize decode state\n    start_time_decode_state = time.time()\n    self.generate_fn = self.engine.generate\n    self.decode_state = self.engine.init_decode_state(self.rng)\n\n    if self.debug:\n      max_logging.log(f\"time taken to initialize decode_state: {time.time() - start_time_decode_state} seconds\")\n    max_logging.log(f\"Initialized Inference worker in {time.time() - start_time} seconds\")\n\n  def _init_engine(self, params):\n    \"\"\"Initialize the MaxEngine.\n\n    Args:\n        params: Model parameters\n\n    Returns:\n        tuple of (params, engine)\n    \"\"\"\n    start_time = time.time()\n    engine = MaxEngine(self.config, self.devices)\n    params = engine.load_params(params=params, rng=self.rng)\n    max_logging.log(f\"Time taken to initialize engine: {time.time() - start_time} seconds\")\n    return params, engine\n\n  def _init_tokenizer(self):\n    \"\"\"Initialize the tokenizer.\n\n    Returns:\n        Initialized tokenizer\n    \"\"\"\n    if self.eos_ids is None and self.tokenizer is None:\n      tokenizer_params = self.engine.get_tokenizer()\n      self.tokenizer = self.engine.build_tokenizer(tokenizer_params)\n    if self.eos_ids is None:\n      self.eos_ids = [self.tokenizer.eos_id]\n    return self.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n  ):\n    \"\"\"Update the model parameters\"\"\"\n    self.params = params\n\n  def reset_state(self):\n    \"\"\"Reset all worker state for a new inference run.\n\n    This allows reusing the same InferenceWorker instance across multiple\n    batch_inference calls without recreating the expensive engine components.\n    \"\"\"\n    max_logging.log(\"Resetting InferenceWorker state\")\n\n    # Reset inference state\n    self.running = False\n    self.completion_tokens_by_id = defaultdict(list)\n    self.prompt_logprobs_by_id = defaultdict(list)\n    self.empty_decode_slots = set()\n    for i in range(self.decode_batch_size):\n      self.empty_decode_slots.add(i)\n    self.slot_to_id = {}\n    self.true_lengths = {}\n    self.detokenization_queue = queue.Queue()\n    self.completed_sequences = set()\n\n    max_logging.log(\"InferenceWorker state reset complete\")\n\n  def run_inference(self, data: list[InputData], rng=None):\n    \"\"\"Start the inference process.\n\n    Args:\n        data: list of InputData objects containing input sequences\n        rng: Random number generator key. If None, the previous key will be used.\n    \"\"\"\n\n    # Reset state for new inference run\n    self.reset_state()\n\n    # Reset rng\n    if rng is not None:\n      self.rng = rng\n\n    # Set up state for this inference run\n    self.true_lengths = {input.id: input.true_length for input in data}\n    self.running = True\n\n    max_logging.log(\"Continuous batching started\")\n\n    self._run_continuous_batching(data)\n\n    return self._build_final_outputs(data)\n\n  def _run_continuous_batching(\n      self,\n      data: list[InputData],\n  ):\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects containing input sequences\n    \"\"\"\n\n    # Start detokenization thread\n    detokenization_thread = SafeThread(\n        target=self.background_detokenization,\n        name=\"detokenization\",\n    )\n    detokenization_thread.start()\n\n    # Process each input\n    for row in data:\n      # 1. Wait for an empty slot\n      while not self.empty_decode_slots:\n        self.decode()\n      # 2. Get an available slot\n      slot = self.empty_decode_slots.pop()\n      # 3. Prefill and insert kv cache\n      self.prefill_helper.process(\n          model_params=self.params,\n          decode_state=self.decode_state,\n          decode_slot=slot,\n          input_id=row.id,\n          input_tokens_padded=row.tokens,\n          input_true_length=row.true_length,\n          prefill_done=self.prefill_done,\n      )\n\n    # 4. Flush any pending inputs in batch prefill mode\n    self.prefill_helper.finalize(self.params, self.decode_state, self.prefill_done)\n\n    # 5. Continue decoding until all sequences are complete\n    while not all(value is None for value in self.slot_to_id.values()):\n      self.decode()\n\n    # Wait for detokenization to complete\n    self.running = False\n    max_logging.log(\"Inference worker: joining detokenization thread\")\n    start_time = time.time()\n\n    with jax.profiler.TraceAnnotation(\"Flushing detokenization thread\"):\n      detokenization_thread.join()\n\n    max_logging.log(f\"Inference worker: detokenization thread joined in {time.time() - start_time} seconds\")\n\n  def _build_final_outputs(self, input_data: list[InputData]) -> list[CompletionOutput]:\n    \"\"\"Build the final list of CompletionOutput.\"\"\"\n\n    with jax.profiler.TraceAnnotation(\"offline_engine.batch_inference.return_final_output\"):\n      completion_outputs = []\n      for row in input_data:\n        input_id = row.id\n        prompt_length = row.true_length\n        prompt_tokens = row.tokens[: row.true_length].squeeze()\n        completion_tokens = np.array(\n            [token_output.token for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        logprobs = np.array(\n            [token_output.log_prob.flatten() for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        prompt_logprobs = np.array(self.prompt_logprobs_by_id[input_id]).flatten()\n        completion_outputs.append(\n            CompletionOutput(\n                index=str(input_id),\n                prompt_length=prompt_length,\n                token_ids=np.concatenate(\n                    (\n                        prompt_tokens,\n                        completion_tokens,\n                    )\n                ),\n                logprobs=np.concatenate(\n                    (\n                        prompt_logprobs,\n                        logprobs,\n                    )\n                ),\n            )\n        )\n    return completion_outputs\n\n  def prefill_done(self, prefill_result: list[PrefillResult], prompt_ids: list[int], decode_state: DecodeState):\n    \"\"\"Callback function called when prefill completes.\n    This function queues the prefill data for background processing.\n\n    Args:\n        prefill_result: list of (token, slot) tuples\n        prompt_ids: list of prompt IDs\n        decode_state: Updated decode state\n    \"\"\"\n    # Update decode state\n    self.decode_state = decode_state\n    # Process each prefill result\n    slots = []\n    result_tokens_list = []\n    prompt_logp_list = []\n    for i, result in enumerate(prefill_result):\n      input_id = prompt_ids[i]\n      slot = result.slot\n      self.slot_to_id[slot] = input_id\n      slots.append(slot)\n      result_tokens_list.append(result.result_tokens)\n      prompt_logp_list.append(result.prompt_logp)\n\n    # Queue detokenization task\n    task = DetokenizationTask(\n        task_type=\"prefill\",\n        result_tokens=result_tokens_list,\n        prompt_logp=prompt_logp_list,\n        prompt_ids=prompt_ids,\n        slots=slots,\n    )\n    self.detokenization_queue.put_nowait(task)\n\n  def decode(self):\n    \"\"\"Run decode steps on current decoder state.\n\n    Performs `self.min_decode_steps` decode operations\n    and queues results for background processing.\n    \"\"\"\n\n    for i in range(self.min_decode_steps):\n      # Generate next tokens\n      self.decode_state, result_tokens, log_prob = self._jitted_generate_fn(self.params, self.decode_state, self.rng)\n      if i == self.min_decode_steps - 1:\n        # Block on the last token\n        jax.block_until_ready(result_tokens)\n\n      # Queue detokenization task\n      task = DetokenizationTask(\n          task_type=\"decode\",\n          tokens_buffer=result_tokens,\n          logprob_buffer=log_prob,\n      )\n\n      self.detokenization_queue.put_nowait(task)\n\n  @functools.partial(jax.jit, static_argnums=(0,), donate_argnums=(2,))\n  def _jitted_generate_fn(self, params, decode_state, rng):\n    decode_state, result_tokens = self.engine.generate(params, decode_state, rng=rng)\n    return decode_state, result_tokens.data[:, 0], result_tokens.log_prob\n\n  def background_detokenization(self):\n    \"\"\"Background thread that handles all GPU-to-CPU transfers and token emission.\n\n    This thread processes DetokenizationTask objects from the queue,\n    performs the numpy conversions, emits tokens, and manages decode slots.\n    \"\"\"\n    max_logging.log(\"Inference worker: starting detokenization thread\")\n\n    while True:\n      try:\n        task = self.detokenization_queue.get(timeout=0.1)\n      except queue.Empty:\n        if not self.running and self.detokenization_queue.empty():\n          break\n        continue\n\n      start_time = time.time()\n      newly_empty = []\n\n      if task.task_type == \"prefill\":\n\n        # Process prefill results - convert to numpy and emit\n        with jax.profiler.TraceAnnotation(\"convert_to_numpy_and_emit_prefill\"):\n          for i, result_tokens in enumerate(task.result_tokens):\n            prompt_id = task.prompt_ids[i]\n            slot = task.slots[i]\n\n            prompt_logp = task.prompt_logp[i]\n            true_length = self.true_lengths[prompt_id]\n\n            # Convert to numpy\n            first_token = np.array(result_tokens.data[:, 0])\n            log_prob = np.array(result_tokens.log_prob)\n            prompt_logp_np = np.array(prompt_logp)[:, :true_length]\n\n            # Emit token directly\n            should_terminate = self.emit_token(prompt_id, int(first_token), log_prob, prompt_logp=prompt_logp_np)\n            if should_terminate:\n              newly_empty.append(slot)\n\n      elif task.task_type == \"decode\":\n\n        # Check if there are any active sequences before expensive numpy conversion\n        active_slots = []\n        for slot, id_ in self.slot_to_id.items():\n          if id_ is not None and id_ not in self.completed_sequences:\n            active_slots.append((slot, id_))\n\n        # Skip processing entirely if no active sequences\n        if not active_slots:\n          continue\n\n        # Process single decode step - convert to numpy and emit\n        with jax.profiler.TraceAnnotation(\"convert_to_numpy_and_emit_decode_step\"):\n          result_tokens_step = np.array(task.tokens_buffer)  # Single step tokens\n          log_prob_step = np.array(task.logprob_buffer)  # Single step logprobs\n\n          for slot, id_ in active_slots:\n            log_prob_at_slot = log_prob_step[slot]\n            result_tokens_at_slot = result_tokens_step[slot]\n            should_terminate = self.emit_token(id_, int(result_tokens_at_slot), log_prob_at_slot)\n            if should_terminate:\n              newly_empty.append(slot)\n      # Update decode slots\n      for slot in newly_empty:\n        self.slot_to_id[slot] = None\n        self.empty_decode_slots.add(slot)\n\n      if self.debug:\n        max_logging.log(f\"Inference worker: detokenization in {time.time() - start_time} seconds\")\n\n  def emit_token(\n      self,\n      prompt_id,\n      result_token: int,\n      log_prob: float,\n      prompt_logp: None | np.ndarray = None,\n  ):\n    \"\"\"Adds the token to the results for the specified prompt ID and\n    determines if generation should terminate.\n\n    Args:\n        prompt_id: ID of the prompt\n        result_token: Token to emit\n        log_prob: Log probability of the token\n        prompt_logp: Log probabilities for the prompt tokens\n\n    Returns:\n        True if this token signals the end of generation, False otherwise\n    \"\"\"\n    # Skip if sequence already completed\n    if prompt_id in self.completed_sequences:\n      return True\n\n    index = len(self.completion_tokens_by_id[prompt_id])\n    if prompt_logp is not None:\n      self.prompt_logprobs_by_id[prompt_id] = [prompt_logp]\n\n    self.completion_tokens_by_id[prompt_id].append(TokenOutput(np.array(result_token), np.array(log_prob)))\n\n    # Check if this token completes the sequence\n    should_terminate = (result_token in self.eos_ids) or (index + 1 == self.max_decode_length)\n    if should_terminate:\n      self.completed_sequences.add(prompt_id)\n\n    return should_terminate",
        "analysis": {
            "module_type": "inference_worker",
            "purpose": "Manages a continuous batching inference process for a language model, handling prefilling of new inputs, autoregressive decoding of active sequences, and result aggregation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes model components like MaxEngine and PrefillHelper during `__init__`.",
                "The `run_inference` method orchestrates the entire process for a batch of inputs.",
                "A background thread is started for detokenization and post-processing.",
                "Iterates through input data, prefilling each into available KV cache slots using `PrefillHelper`.",
                "Continuously calls the `decode` method to generate new tokens for all active sequences.",
                "The background thread processes generated tokens, checks for completion (EOS or max length), and frees up slots.",
                "The cycle of prefilling new sequences into freed slots and decoding continues until all sequences are complete.",
                "Finally, aggregates and formats the results into a list of `CompletionOutput` objects."
            ],
            "output": {
                "shape": "The `run_inference` method returns a list of `CompletionOutput` objects."
            },
            "dependencies": [
                "MaxEngine",
                "PrefillHelper",
                "SafeThread",
                "queue.Queue",
                "InputData",
                "CompletionOutput",
                "DetokenizationTask",
                "PrefillResult"
            ],
            "parameters": {
                "config": "MaxText configuration object used to initialize the engine.",
                "min_decode_steps": "Minimum number of decode steps to run at once before checking for completion.",
                "enable_batch_prefill": "A boolean indicating whether to use prefill packing for multiple inputs.",
                "max_decode_length": "Maximum number of tokens to generate per sequence.",
                "prefill_lengths": "A list of supported padded lengths for input sequences.",
                "batch_prefill_max_batch_size": "Maximum number of inputs to pack into a single sequence when batch prefill is enabled."
            },
            "notes": [
                "Implements a continuous batching strategy to maximize hardware utilization by overlapping prefill and decode operations.",
                "A background thread (`background_detokenization`) is used to handle GPU-to-CPU data transfers and post-processing, preventing the main generation loop from blocking.",
                "The state of the worker can be reset using `reset_state()` to allow reuse for multiple inference calls without re-initializing expensive components."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the InferenceWorker, setting up configuration, state, and model components like the MaxEngine and PrefillHelper.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters from arguments.",
                        "Initialize internal state variables (queues, sets, dictionaries).",
                        "Call `_init_engine` to create a `MaxEngine` instance and load model parameters.",
                        "Call `_init_tokenizer` to set up the tokenizer and EOS IDs.",
                        "Instantiate `PrefillHelper` for managing prefill operations.",
                        "Call `engine.init_decode_state` to create the initial state for the decoder."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxEngine",
                        "PrefillHelper",
                        "PrefillType"
                    ],
                    "notes": [
                        "This method performs expensive initializations, including creating the JAX computation engine and loading model parameters."
                    ]
                },
                "reset_state": {
                    "purpose": "Resets all worker state for a new inference run, allowing the instance to be reused.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reset running flag.",
                        "Re-initialize dictionaries for completion tokens and logprobs.",
                        "Re-populate the set of empty decode slots.",
                        "Clear slot-to-id mapping, true lengths, and completed sequences.",
                        "Create a new detokenization queue."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "collections.defaultdict",
                        "queue.Queue"
                    ],
                    "notes": [
                        "This allows for efficient reuse of the InferenceWorker across multiple `batch_inference` calls without recreating the expensive engine components."
                    ]
                },
                "run_inference": {
                    "purpose": "Starts and manages a complete inference run for a given list of input data.",
                    "input": {
                        "shape": "list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `reset_state` to clear state from any previous runs.",
                        "Update the random number generator key if a new one is provided.",
                        "Set up the `true_lengths` dictionary from the input data.",
                        "Set the `running` flag to True.",
                        "Call `_run_continuous_batching` to execute the main inference loop.",
                        "Call `_build_final_outputs` to format and return the results."
                    ],
                    "output": {
                        "shape": "list[CompletionOutput]"
                    },
                    "dependencies": [
                        "InputData",
                        "CompletionOutput",
                        "self.reset_state",
                        "self._run_continuous_batching",
                        "self._build_final_outputs"
                    ],
                    "notes": [
                        "This is the main public entry point for executing an inference job on the worker."
                    ]
                },
                "_run_continuous_batching": {
                    "purpose": "Implements the core continuous batching logic, managing the interplay of prefill and decode steps.",
                    "input": {
                        "shape": "list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Start a `SafeThread` for the `background_detokenization` method.",
                        "Loop through each input data item.",
                        "Wait for an empty decode slot by calling `self.decode()` if the slot pool is empty.",
                        "Pop an available slot from `empty_decode_slots`.",
                        "Call `self.prefill_helper.process()` to handle the prefill for the current input.",
                        "After processing all inputs, call `self.prefill_helper.finalize()` to flush any pending batched prefills.",
                        "Enter a loop that calls `self.decode()` until all sequences are complete.",
                        "Set `self.running` to False and join the detokenization thread to ensure all post-processing is finished."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "SafeThread",
                        "self.background_detokenization",
                        "self.decode",
                        "self.prefill_helper"
                    ],
                    "notes": [
                        "This method orchestrates the main workflow of the inference process."
                    ]
                },
                "prefill_done": {
                    "purpose": "A callback function that is invoked when a prefill operation completes, queueing the results for background processing.",
                    "input": {
                        "shape": "prefill_result: list[PrefillResult], prompt_ids: list[int], decode_state: DecodeState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Update `self.decode_state` with the new state from the prefill operation.",
                        "Iterate through the prefill results.",
                        "Map the prompt ID to the decode slot in `self.slot_to_id`.",
                        "Aggregate result tokens and prompt log probabilities.",
                        "Create a `DetokenizationTask` of type 'prefill' with the aggregated results.",
                        "Put the task onto the `detokenization_queue`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillResult",
                        "DetokenizationTask"
                    ],
                    "notes": [
                        "This acts as a bridge between the `PrefillHelper` and the `InferenceWorker`'s background processing thread."
                    ]
                },
                "decode": {
                    "purpose": "Performs a fixed number of autoregressive decoding steps and queues the raw results for background processing.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Loop for `self.min_decode_steps` iterations.",
                        "Call the JIT-compiled `_jitted_generate_fn` to get the next tokens and log probabilities for all active sequences.",
                        "On the last iteration, block until the JAX computation is complete.",
                        "Create a `DetokenizationTask` of type 'decode' with the resulting token and logprob buffers.",
                        "Put the task into `self.detokenization_queue`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self._jitted_generate_fn",
                        "DetokenizationTask",
                        "jax.block_until_ready"
                    ],
                    "notes": [
                        "Executes multiple decode steps in a single method call to reduce Python overhead and improve performance."
                    ]
                },
                "_jitted_generate_fn": {
                    "purpose": "A JIT-compiled wrapper around the engine's generate function for a single autoregressive step.",
                    "input": {
                        "shape": "params: Pytree, decode_state: Pytree, rng: PRNGKey",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `self.engine.generate` with the current parameters, decode state, and RNG key.",
                        "Return the updated decode state, the generated tokens, and their log probabilities."
                    ],
                    "output": {
                        "shape": "(updated_decode_state, result_tokens, log_probs)"
                    },
                    "dependencies": [
                        "self.engine.generate",
                        "jax.jit"
                    ],
                    "notes": [
                        "Decorated with `jax.jit` for performance. The `decode_state` argument is donated (`donate_argnums`) to allow JAX to reuse its memory, optimizing memory usage."
                    ]
                },
                "background_detokenization": {
                    "purpose": "Runs in a separate thread to process results from the GPU, handle data transfers (JAX array to NumPy), and manage sequence completion.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Enter a loop that continues as long as inference is running or the queue has tasks.",
                        "Get a `DetokenizationTask` from the queue.",
                        "If the task is 'prefill', convert JAX arrays to NumPy and call `emit_token` for the first generated token of each new sequence.",
                        "If the task is 'decode', convert JAX arrays to NumPy and call `emit_token` for each active sequence's newly generated token.",
                        "If `emit_token` returns True (sequence terminated), add the corresponding slot to a list of newly empty slots.",
                        "Update the main worker's `empty_decode_slots` set with the newly freed slots."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "queue.Queue",
                        "DetokenizationTask",
                        "self.emit_token",
                        "numpy"
                    ],
                    "notes": [
                        "This method is designed to run in a background thread to overlap CPU-bound post-processing with GPU-bound generation."
                    ]
                },
                "emit_token": {
                    "purpose": "Appends a generated token to its corresponding sequence's output, stores log probabilities, and checks for termination conditions.",
                    "input": {
                        "shape": "prompt_id: Hashable, result_token: int, log_prob: float, prompt_logp: Optional[np.ndarray]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return immediately if the sequence is already marked as complete.",
                        "Store prompt log probabilities if provided (on the first token emission).",
                        "Append a `TokenOutput` object containing the new token and its log probability to the results for the given `prompt_id`.",
                        "Check if the token is an EOS token or if the `max_decode_length` has been reached.",
                        "If a termination condition is met, add the `prompt_id` to the `completed_sequences` set.",
                        "Return True if the sequence should terminate, False otherwise."
                    ],
                    "output": {
                        "shape": "A boolean indicating if the sequence has terminated."
                    },
                    "dependencies": [
                        "TokenOutput",
                        "numpy"
                    ],
                    "notes": [
                        "This method contains the core logic for tracking the state and output of each individual generation sequence."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#OfflineEngine",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class OfflineEngine:\n  \"\"\"Class for handling offline inference on batches of inputs.\"\"\"\n\n  def __init__(\n      self,\n      config: Any,\n      params: None | Params = None,\n      enable_batch_prefill: bool = False,\n      min_decode_steps: int = 10,\n      tokenizer: Any = None,\n      eos_ids: list[int] | None = None,\n      prefill_lengths: list[int] | str = \"auto\",\n      batch_prefill_max_batch_size: int = 16,\n      mesh: Mesh = None,\n      rng: jax.random.PRNGKey = None,\n      debug: bool = False,\n  ):\n    \"\"\"Initialize the OfflineEngine.\n\n    Args:\n        config: The MaxText config object which will be used to\n          create MaxEngine instance(s).\n        params: Model parameters (loaded from engine if None)\n        enable_batch_prefill: Whether to use prefill packing.\n            config.scan_layers must be False if this is True\n        min_decode_steps: Number of decode steps to perform at a time,\n            before checking for completion.\n        eos_ids: list of EOS token IDs for checking sequence completion.\n          If None, the tokenizer's EOS token will be used.\n        tokenizer: Tokenizer instance for encoding/decoding text. If None,\n          will be created using the config if eos_ids is not provided.\n        prefill_lengths: list of expected prefill lengths, or \"auto\" to\n            automatically determine appropriate lengths from the engine\n            config. Input sequences will be padded to the nearest length\n            in this list.\n        batch_prefill_max_batch_size: Maximum number of inputs to pack\n          into a single prefill. This is only used when enable_batch_prefill\n          is True.\n        mesh: JAX Mesh object. Use this\n          argument if you want to use only some of the devices for OfflineEngine and\n          reserve the rest for other tasks. If None, OfflineEngine will create the mesh\n          automatically.\n        rng: Random number generator key. If None, a new key will be created.\n    \"\"\"\n    max_logging.log(\"Initializing OfflineEngine\")\n    # Configurations\n    self.config = config\n    self.params = params\n    self.min_decode_steps = min_decode_steps\n    self.enable_batch_prefill = enable_batch_prefill\n    self.mesh = mesh\n    self.tokenizer = tokenizer\n    self.eos_ids = eos_ids\n    self.prefill_lengths = prefill_lengths\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.max_prefill_length = self.config.max_prefill_predict_length\n    self.max_decode_length = self.config.max_target_length - self.max_prefill_length\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n    self._validate_config()\n\n    # Create prefill buckets: [0, 64], (64, 128], (128, 256], ..., [max_length//2, max_length]\n    if prefill_lengths == \"auto\":\n      self.prefill_lengths = [2**i for i in range(6, max(6, (self.max_prefill_length - 1).bit_length()) + 1)]\n    else:\n      self.prefill_lengths = sorted(prefill_lengths)\n\n    # Create meshes\n    if not self.mesh:\n      self.mesh = OfflineEngine.create_mesh(jax.devices(), self.config)\n\n    self.worker = InferenceWorker(\n        config=self.config,\n        params=self.params,\n        min_decode_steps=self.min_decode_steps,\n        enable_batch_prefill=self.enable_batch_prefill,\n        mesh=self.mesh,\n        devices=self.mesh.devices.flatten(),\n        tokenizer=self.tokenizer,\n        eos_ids=self.eos_ids,\n        prefill_lengths=self.prefill_lengths,\n        max_decode_length=self.max_decode_length,\n        batch_prefill_max_batch_size=self.batch_prefill_max_batch_size,\n        rng=self.rng,\n        debug=self.debug,\n    )\n\n    self.tokenizer = self.worker.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n  ):\n    \"\"\"Update model weights.\"\"\"\n    self.worker.update_params(params)\n\n  def batch_inference(\n      self,\n      data: list[InputData] | list[jax.Array] | list[np.ndarray],\n      desc: str = \"\",\n      rng=None,\n  ) -> list[CompletionOutput]:\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays.\n            If input is JAX or numpy array, it must not contain padding tokens.\n        desc: Description string for logging\n        rng: Random number generator key. If None, the previous key will be used.\n\n    Returns:\n        list of CompletionOutput objects, one for each input in data\n    \"\"\"\n    data = self.prepare_data(data)\n\n    return self.worker.run_inference(data, rng)\n\n  def prepare_data(self, data: list[InputData | jax.Array | np.ndarray]) -> list[InputData]:\n    \"\"\"Pad and if batch prefill is enabled, sort data by length.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays\n\n    Returns:\n        list of prepared InputData objects\n    \"\"\"\n    # Convert JAX arrays to numpy arrays\n    if isinstance(data[0], jax.Array):\n      data = [np.array(array) for array in data]\n\n    # Convert numpy arrays to InputData objects\n    if isinstance(data[0], np.ndarray):\n      max_logging.log(\n          \"When you provide JAX/numpy arrays to Offline Engine, \"\n          \"make sure that the arrays are not padded with padding tokens.\"\n      )\n      data = [InputData(id=i, tokens=array, true_length=len(array)) for i, array in enumerate(data)]\n\n    # Make sure all data id is unique\n    if len(data) != len({item.id for item in data}):\n      raise ValueError(\"All data ids must be unique\")\n\n    data = self.pad_data(data)\n\n    if self.enable_batch_prefill:\n      return sorted(data, key=lambda x: x.tokens.shape[0])\n\n    return data\n\n  def pad_data(self, data: list[InputData]) -> list[InputData]:\n    \"\"\"For each input, pad it to the next length in self.prefill_lengths\n    that is greater than or equal to its true length.\n\n    Args:\n        data: list of InputData objects\n\n    Returns:\n        list of padded InputData objects\n    \"\"\"\n    padded_data = []\n\n    for item in data:\n      # Find the smallest prefill length that can accommodate this input\n      target_length = None\n      for length in self.prefill_lengths:\n        if length >= item.true_length:\n          target_length = length\n          break\n\n      # If no suitable length found, use the maximum prefill length\n      if target_length is None:\n        target_length = self.max_prefill_length\n\n      # Pad or truncate as needed\n      if len(item.tokens) < target_length:\n        # Pad with zeros\n        padded_tokens = np.zeros(target_length, dtype=item.tokens.dtype)\n        padded_tokens[: item.true_length] = item.tokens[: item.true_length]\n      else:\n        # Input is too long, truncate to max_prefill_length\n        padded_tokens = item.tokens[:target_length]\n\n      # Create new InputData with padded tokens\n      padded_data.append(InputData(id=item.id, tokens=padded_tokens, true_length=item.true_length))\n\n    return padded_data\n\n  @staticmethod\n  def create_mesh(devices, config):\n    \"\"\"Create data parallelism meshes for each Inference worker.\"\"\"\n    ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), len(devices), \"ICI\")\n    devices_array = mesh_utils.create_device_mesh(\n        ici_parallelism,\n        devices,\n        contiguous_submeshes=False,\n        allow_split_physical_axes=config.allow_split_physical_axes or False,\n    )\n    mesh = Mesh(devices_array.reshape(ici_parallelism), config.mesh_axes)\n    return mesh\n\n  def _validate_config(self):\n    \"\"\"Validate configuration parameters and check for incompatible settings.\"\"\"\n    if not self.config.return_log_prob:\n      raise ValueError(\"return_log_prob must be True when using OfflineEngine\")\n    if self.enable_batch_prefill and self.config.scan_layers:\n      raise ValueError(\"scan_layers must be False if enable_batch_prefill is True\")\n\n    if self.max_decode_length <= 0:\n      raise ValueError(\"Make sure max_target_length - max_prefill_predict_length is greater than 0\")\n    if self.config.scan_layers:\n      max_logging.log(\n          \"WARNING: scan_layers=True will result in slow step time. \" \"It is recommended for debugging purposes only.\"\n      )",
        "analysis": {
            "module_type": "offline_inference_engine",
            "purpose": "A high-level class for handling offline inference on batches of inputs by managing data preparation and an underlying `InferenceWorker`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes configuration parameters such as `min_decode_steps`, `enable_batch_prefill`, etc.",
                "Validates the configuration for incompatible settings via `_validate_config`.",
                "Determines the prefill bucket lengths, either automatically or from a provided list.",
                "Creates a JAX device mesh using `create_mesh` if one is not provided.",
                "Instantiates an `InferenceWorker` which handles the core continuous batching and inference logic.",
                "Sets the engine's tokenizer from the worker's tokenizer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "InferenceWorker",
                "jax",
                "jax.sharding.Mesh",
                "max_utils",
                "mesh_utils",
                "max_logging"
            ],
            "parameters": {
                "config": "The MaxText config object which will be used to create MaxEngine instance(s).",
                "params": "Model parameters (loaded from engine if None).",
                "enable_batch_prefill": "Whether to use prefill packing. `config.scan_layers` must be False if this is True.",
                "min_decode_steps": "Number of decode steps to perform at a time, before checking for completion.",
                "prefill_lengths": "List of expected prefill lengths, or 'auto' to automatically determine appropriate lengths from the engine config.",
                "mesh": "JAX Mesh object. If None, the engine will create one automatically using all available devices."
            },
            "notes": [
                "This class acts as a user-facing wrapper around the `InferenceWorker`, which performs the complex continuous batching logic.",
                "It simplifies the process by handling data preparation, including padding and sorting, before passing the data to the worker."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the OfflineEngine, sets up configuration, creates a JAX mesh, and instantiates the core InferenceWorker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (config, params, min_decode_steps, etc.).",
                        "Call `_validate_config` to check for incompatible settings.",
                        "Determine prefill bucket lengths, either from the provided list or automatically.",
                        "If no mesh is provided, call `create_mesh` to create one.",
                        "Instantiate an `InferenceWorker` to handle the core inference logic.",
                        "Update the engine's tokenizer reference from the newly created worker."
                    ],
                    "output": {
                        "shape": "An instance of OfflineEngine."
                    },
                    "dependencies": [
                        "InferenceWorker",
                        "jax.random.PRNGKey",
                        "OfflineEngine.create_mesh",
                        "OfflineEngine._validate_config"
                    ],
                    "notes": [
                        "The `prefill_lengths` can be set to 'auto' to automatically generate power-of-two bucket sizes for padding."
                    ]
                },
                "update_params": {
                    "purpose": "Updates the model weights in the underlying InferenceWorker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.worker.update_params` with the new parameters."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker.update_params"
                    ],
                    "notes": [
                        "This allows for hot-swapping model parameters without re-initializing the entire engine."
                    ]
                },
                "batch_inference": {
                    "purpose": "Runs inference on a batch of inputs and returns the generated completions.",
                    "input": {
                        "shape": "A list of 1D arrays, where each array is of shape `[sequence_length]`.",
                        "dtype": "int (token IDs)"
                    },
                    "processing_steps": [
                        "Calls `prepare_data` to convert inputs to `InputData` objects, pad them, and optionally sort them.",
                        "Calls `self.worker.run_inference` with the prepared data and returns the result."
                    ],
                    "output": {
                        "shape": "A list of `CompletionOutput` objects."
                    },
                    "dependencies": [
                        "self.prepare_data",
                        "self.worker.run_inference",
                        "InputData",
                        "CompletionOutput"
                    ],
                    "notes": [
                        "This is the main entry point for running inference with the engine. The input can be a list of `InputData` objects, JAX arrays, or NumPy arrays."
                    ]
                },
                "prepare_data": {
                    "purpose": "Converts various input formats to a list of padded `InputData` objects, and sorts them if batch prefill is enabled.",
                    "input": {
                        "shape": "A list of `InputData`, `jax.Array`, or `np.ndarray` objects.",
                        "dtype": "int (token IDs)"
                    },
                    "processing_steps": [
                        "If input is `jax.Array`, convert each element to `np.ndarray`.",
                        "If input is `np.ndarray`, convert each element to an `InputData` object.",
                        "Validate that all `InputData` IDs are unique.",
                        "Call `pad_data` to pad or truncate each input to a valid prefill length.",
                        "If `enable_batch_prefill` is True, sort the padded data by token length."
                    ],
                    "output": {
                        "shape": "A list of prepared `InputData` objects."
                    },
                    "dependencies": [
                        "numpy",
                        "InputData",
                        "self.pad_data"
                    ],
                    "notes": [
                        "Sorting the data when batch prefill is enabled helps to fill packing buckets more efficiently."
                    ]
                },
                "pad_data": {
                    "purpose": "Pads or truncates each input sequence to the nearest valid prefill length.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through each `InputData` item.",
                        "Find the smallest prefill length from `self.prefill_lengths` that is greater than or equal to the item's true length.",
                        "If no suitable length is found, use the maximum prefill length.",
                        "Create a new zero-padded numpy array of the target length.",
                        "Copy the original tokens into the new padded array.",
                        "Create and append a new `InputData` object with the padded tokens."
                    ],
                    "output": {
                        "shape": "A list of padded `InputData` objects."
                    },
                    "dependencies": [
                        "numpy",
                        "InputData"
                    ],
                    "notes": [
                        "Padding is done with zeros. Sequences longer than the maximum prefill length are truncated."
                    ]
                },
                "create_mesh": {
                    "purpose": "Creates a JAX device mesh for distributed computation based on the provided configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine ICI parallelism using `max_utils.fill_unspecified_mesh_axes`.",
                        "Create a device array using `mesh_utils.create_device_mesh`.",
                        "Instantiate and return a `jax.sharding.Mesh` object."
                    ],
                    "output": {
                        "shape": "A `jax.sharding.Mesh` object."
                    },
                    "dependencies": [
                        "max_utils.fill_unspecified_mesh_axes",
                        "mesh_utils.create_device_mesh",
                        "jax.sharding.Mesh"
                    ],
                    "notes": [
                        "This is a static method, meaning it can be called without an instance of the class."
                    ]
                },
                "_validate_config": {
                    "purpose": "Validates the configuration to ensure settings are compatible and raises errors for invalid combinations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `config.return_log_prob` is True.",
                        "Check for conflict between `enable_batch_prefill` and `config.scan_layers`.",
                        "Check if `max_decode_length` is positive.",
                        "Log a warning if `config.scan_layers` is True."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_logging"
                    ],
                    "notes": [
                        "This is a private helper method called during initialization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageState",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageState:\n  \"\"\"Represents the global state of memory pages managed by the `PageManager`.\n\n  This dataclass tracks the allocation status of each page across the entire system,\n  the mapping of pages to page groups (requests), and the current position within\n  each sequence's pages. State is managed globally, providing a single view\n  across all potential layers using this manager.\n\n  Attributes:\n    page_status: A `jnp.ndarray` of shape `[num_pages]`. Each element\n      indicates whether the corresponding page in the global pool is free (0)\n      or allocated (1).\n    page_map: A `jnp.ndarray` of shape `[max_page_groups, max_pages_per_group]`.\n      This array maps each page group to the indices (within the global pool)\n      of its allocated pages. Entries beyond `num_pages_used` for a group are invalid.\n    num_pages_used: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      tracks the number of pages currently allocated to each page group. This\n      determines the valid entries in `page_map` for each group.\n    sequence_lengths: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the current true length of each sequence (in tokens) associated\n      with a page group.\n    active_page: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the global index of the *currently active* page (the page where the\n      next token will be written) for each page group. Only valid if the\n      corresponding `has_active_page` is True.\n    has_active_page: A `jnp.ndarray` of shape `[max_page_groups]`. Boolean mask\n      indicating whether a page group currently represents an active sequence\n      and thus whether its `active_page` and `active_page_position` entries\n      are meaningful.\n    active_page_position: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the index (offset, 0 to tokens_per_page-1) of the next available\n      token *within the `active_page`* for each page group. Only valid if\n      `has_active_page` is True.\n  \"\"\"\n\n  page_status: PagesInt1d\n  page_map: GroupsPagesInt2d\n  num_pages_used: GroupsInt1d\n  sequence_lengths: GroupsInt1d\n  active_page: GroupsInt1d\n  has_active_page: GroupsBool1d\n  active_page_position: GroupsInt1d",
        "analysis": {
            "module_type": "paged_attention_state",
            "purpose": "A dataclass that represents and tracks the global state of memory pages for a paged attention mechanism, including allocation status, page-to-group mappings, and sequence positions.",
            "input": {
                "shape": "N/A (dataclass)",
                "dtype": "N/A (dataclass)"
            },
            "processing_steps": [
                "N/A (It is a data container)."
            ],
            "output": {
                "shape": "An instance of the PageState class, containing tensors with shapes like [num_pages], [max_page_groups], and [max_page_groups, max_pages_per_group]."
            },
            "dependencies": [
                "flax.struct",
                "jaxtyping",
                "PageManager"
            ],
            "parameters": {
                "page_status": "Shape: [num_pages]. An integer array where 0 indicates a free page and 1 indicates an allocated page.",
                "page_map": "Shape: [max_page_groups, max_pages_per_group]. Maps each page group to the global indices of its allocated pages.",
                "num_pages_used": "Shape: [max_page_groups]. Tracks the number of pages currently allocated to each page group.",
                "sequence_lengths": "Shape: [max_page_groups]. Stores the current length in tokens for each sequence.",
                "active_page": "Shape: [max_page_groups]. Stores the global index of the current page where the next token will be written for each group.",
                "has_active_page": "Shape: [max_page_groups]. A boolean mask indicating if a page group is active.",
                "active_page_position": "Shape: [max_page_groups]. Stores the offset (0 to tokens_per_page-1) for the next token write within the active page."
            },
            "notes": [
                "This is a Flax struct dataclass, designed to be a passive data container.",
                "The state is managed globally by the PageManager, not on a per-layer basis.",
                "The validity of `active_page` and `active_page_position` for a given group is conditional on the corresponding `has_active_page` flag being True."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#initialize_page_state",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def initialize_page_state(\n    num_pages: int,\n    max_page_groups: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Creates and initializes a global `PageState` object.\n\n  All pages in the global pool are initially marked as free (status 0), except\n  for page 0 which is marked used as a workaround. No pages are assigned to any\n  page group. Sequence lengths and page usage counts are initialized to zero.\n  Active page tracking is also reset.\n\n  Args:\n    num_pages: The total number of available pages in the global pool.\n    max_page_groups: The maximum number of page groups (concurrent sequences/requests)\n      the system can track.\n    max_pages_per_group: The maximum number of pages that can be allocated to\n      a single page group (determines the size of the second dimension of `page_map`).\n\n  Returns:\n    An initialized `PageState` object with all values set to their defaults (zeros/False).\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  initial_page_status = jnp.zeros((num_pages,), dtype=jnp.int32)\n  initial_page_status = initial_page_status.at[0].set(1)  # Workaround page 0\n  return PageState(\n      page_status=initial_page_status,\n      page_map=jnp.zeros((max_page_groups, max_pages_per_group), dtype=jnp.int32),\n      num_pages_used=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      sequence_lengths=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      active_page=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      has_active_page=jnp.zeros((max_page_groups,), dtype=jnp.bool_),\n      active_page_position=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n  )",
        "analysis": {
            "module_type": "page_state_initializer",
            "purpose": "Creates and initializes a global `PageState` object with default values for managing paged attention memory.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a zero-initialized integer array `initial_page_status` of shape `[num_pages]`.",
                "Set the value at index 0 of `initial_page_status` to 1, marking the first page as used.",
                "Instantiate a `PageState` object with the modified `page_status` array.",
                "Initialize `page_map` with zeros of shape `[max_page_groups, max_pages_per_group]`.",
                "Initialize `num_pages_used`, `sequence_lengths`, `active_page`, and `active_page_position` with zeros of shape `[max_page_groups]`.",
                "Initialize `has_active_page` with False values of shape `[max_page_groups]`."
            ],
            "output": {
                "shape": "An instance of the `PageState` dataclass."
            },
            "dependencies": [
                "PageState",
                "jax.numpy as jnp"
            ],
            "parameters": {
                "num_pages": "The total number of available pages in the global pool.",
                "max_page_groups": "The maximum number of page groups (concurrent sequences/requests) the system can track.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "All pages are initialized as free (status 0) except for page 0, which is marked as used (status 1) as a workaround.",
                "All other state arrays within the returned `PageState` object are initialized to zeros or False, representing an empty state with no active sequences."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_find_next_free_page_index",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _find_next_free_page_index(page_status: PagesInt1d) -> ScalarInt:\n  \"\"\"Finds the index of the next available free page in the global pool.\n\n  Searches the `page_status` array for the first occurrence of 0 (indicating\n  a free page), skipping index 0 due to potential issues.\n\n  Args:\n    page_status: A 1D `jnp.ndarray` representing the global status of pages\n      (0 for free, 1 for allocated). Should have shape [num_pages].\n\n  Returns:\n    A scalar `jnp.int32` array containing the index of the next free page\n    (the lowest index >= 1 where `page_status` is 0).\n    Returns -1 if no free pages (at index >= 1) are found.\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  search_status = page_status[1:]\n  overall_free_mask = search_status == 0\n\n  # argmax returns the index of the *first* True. If none are True, it returns 0.\n  next_free_relative = jnp.argmax(overall_free_mask)\n  # Add 1 to compensate for the slice [1:]\n  next_free_overall = next_free_relative + 1\n  # Check if a free page exists\n  has_free_overall = jnp.any(overall_free_mask)\n  # If a free page exists, return its index, otherwise return -1\n  return jnp.where(has_free_overall, next_free_overall, -1)",
        "analysis": {
            "module_type": "free_page_finder",
            "purpose": "Finds the index of the first available free page in a page status array, skipping the first element.",
            "input": {
                "shape": "[num_pages]",
                "dtype": "int32"
            },
            "processing_steps": [
                "Slice the input `page_status` array to exclude the first element (index 0).",
                "Create a boolean mask where `True` corresponds to a free page (value of 0).",
                "Use `jnp.argmax` on the mask to find the relative index of the first `True` value.",
                "Add 1 to the relative index to get the absolute index in the original array.",
                "Use `jnp.any` to check if any free pages were found in the sliced array.",
                "Use `jnp.where` to return the calculated absolute index if a free page was found, otherwise return -1."
            ],
            "output": {
                "shape": "[] (scalar)"
            },
            "dependencies": [
                "jax.numpy.argmax",
                "jax.numpy.any",
                "jax.numpy.where"
            ],
            "parameters": {},
            "notes": [
                "The function intentionally ignores the page at index 0, as noted by a TODO comment in the source code.",
                "It leverages the behavior that `jnp.argmax` on a boolean JAX array returns the index of the first `True` element, or 0 if no `True` elements exist.",
                "If no free pages are found (at index >= 1), the function returns -1."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_pages_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases all pages associated with a given page group.\n\n  This function iterates through the potential pages allocated to the specified\n  `page_group_id` (up to `max_pages_per_group`). For each page index actually\n  used by the group (determined by `num_pages_used`), it retrieves the global\n  page index from `page_map` and resets its status to 0 (free) in the global\n  `page_status` array. It also resets all state fields related to the\n  `page_group_id` (length, count, active status, etc.) to their initial values.\n\n  Args:\n    page_state: The current global `PageState`.\n    page_group_id: The index of the page group whose pages are to be released.\n    max_pages_per_group: The maximum number of pages a group can hold (used as\n      the loop bound).\n\n  Returns:\n    A new `PageState` object where the specified group's pages are marked as free\n    in `page_status`, and the group's specific state entries are reset.\n  \"\"\"\n  current_page_status = page_state.page_status\n  current_page_map = page_state.page_map\n  num_valid_pages = page_state.num_pages_used[page_group_id]\n\n  def release_page(i: int, status: PagesInt1d) -> PagesInt1d:\n    is_valid = i < num_valid_pages\n    page_idx = current_page_map[page_group_id, i]\n    # Only release if index 'i' points to a valid allocated page\n    should_release = jnp.logical_and(is_valid, page_idx > 0)\n\n    return jax.lax.cond(should_release, lambda s: s.at[page_idx].set(0), lambda s: s, status)\n\n  new_page_status = jax.lax.fori_loop(0, max_pages_per_group, release_page, current_page_status)\n\n  return page_state.replace(\n      page_status=new_page_status,\n      num_pages_used=page_state.num_pages_used.at[page_group_id].set(0),\n      sequence_lengths=page_state.sequence_lengths.at[page_group_id].set(0),\n      active_page=page_state.active_page.at[page_group_id].set(0),\n      has_active_page=page_state.has_active_page.at[page_group_id].set(False),\n      active_page_position=page_state.active_page_position.at[page_group_id].set(0),\n  )",
        "analysis": {
            "functionality": "Releases all memory pages associated with a specific page group, marking them as free in the global page status array and resetting the group's state attributes to their initial values.",
            "usage": "This function is used to deallocate resources for a finished sequence. Call it with the current `PageState`, the `page_group_id` of the sequence to release, and the static `max_pages_per_group` configuration. It returns a new `PageState` object reflecting the changes."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_reserve_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _reserve_pages_for_group(\n    released_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Reserves pages for a specific group, assuming true_length > 0.\n\n  PRECONDITION: `true_length` must be > 0. This function assumes the caller\n  (e.g., `PageManager.update_prefill_pages`) has validated this.\n\n  Calculates the number of pages required for `true_length`. Checks if enough\n  free pages exist globally and if the group has capacity based on the state\n  provided in `released_state`. If resources are sufficient, it iteratively\n  finds free pages, marks them allocated, records them in the map, and updates\n  the group's state fields. If resources are insufficient, it returns the\n  `released_state` unchanged (effectively leaving the group empty).\n\n  Args:\n      released_state: The global `PageState` after pages for `page_group_id`\n          have already been released.\n      page_group_id: The index of the page group to allocate pages for.\n      true_length: The target sequence length for the prefill. MUST BE > 0.\n      tokens_per_page: The capacity of each page.\n      max_pages_per_group: The maximum number of pages the group can hold.\n\n  Returns:\n      A new `PageState` with pages allocated for the group and its state updated,\n      or the input `released_state` if allocation failed due to resource limits.\n  \"\"\"\n  num_pages_needed = (true_length + tokens_per_page - 1) // tokens_per_page\n  last_token_abs_idx = true_length - 1\n  last_page_position_idx = last_token_abs_idx % tokens_per_page\n  next_write_position = (last_page_position_idx + 1) % tokens_per_page\n\n  current_page_status = released_state.page_status\n  current_page_map = released_state.page_map\n  current_num_pages_used = released_state.num_pages_used\n\n  num_free_pages = jnp.sum(current_page_status == 0)\n  group_has_capacity = jax.lax.le(num_pages_needed, max_pages_per_group)\n  sufficient_free_pages = jax.lax.ge(num_free_pages, num_pages_needed)\n  has_enough_resources = jnp.logical_and(sufficient_free_pages, group_has_capacity)\n\n  def allocate_and_update_state(initial_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]) -> PageState:\n    \"\"\"Allocates pages iteratively if resources are sufficient.\"\"\"\n    initial_status, initial_map, initial_num_used = initial_state_tuple\n\n    def allocate_one_page(\n        page_idx_in_group: ScalarInt, loop_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]\n    ) -> tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]:\n      \"\"\"Allocates a single page within the fori_loop.\"\"\"\n      current_loop_status, current_loop_map, current_loop_num_used = loop_state_tuple\n      next_free_page_global = _find_next_free_page_index(current_loop_status)\n      page_allocated = jax.lax.ge(next_free_page_global, 0)\n\n      new_loop_status = jax.lax.cond(\n          page_allocated,\n          lambda s: s.at[next_free_page_global].set(1),\n          lambda s: s,\n          current_loop_status,\n      )\n      new_loop_map = jax.lax.cond(\n          page_allocated,\n          lambda m: m.at[page_group_id, page_idx_in_group].set(next_free_page_global),\n          lambda m: m,\n          current_loop_map,\n      )\n      new_loop_num_used = jax.lax.cond(\n          page_allocated,\n          lambda n: n.at[page_group_id].add(1),\n          lambda n: n,\n          current_loop_num_used,\n      )\n      return new_loop_status, new_loop_map, new_loop_num_used\n\n    final_page_status, final_page_map, final_num_pages_used = jax.lax.fori_loop(\n        0,\n        num_pages_needed,\n        allocate_one_page,\n        (initial_status, initial_map, initial_num_used),\n    )\n    active_page_global_index = final_page_map[page_group_id, num_pages_needed - 1]\n\n    return released_state.replace(\n        page_status=final_page_status,\n        page_map=final_page_map,\n        num_pages_used=final_num_pages_used,\n        sequence_lengths=released_state.sequence_lengths.at[page_group_id].set(true_length),\n        active_page=released_state.active_page.at[page_group_id].set(active_page_global_index),\n        has_active_page=released_state.has_active_page.at[page_group_id].set(True),\n        active_page_position=released_state.active_page_position.at[page_group_id].set(next_write_position),\n    )\n\n  # Conditionally perform allocation or return the released state\n  final_state = jax.lax.cond(\n      has_enough_resources,\n      allocate_and_update_state,\n      lambda _: released_state,\n      operand=(current_page_status, current_page_map, current_num_pages_used),\n  )\n  return final_state",
        "analysis": {
            "module_type": "page_reservation_function",
            "purpose": "Calculates the required pages for a given sequence length, and if resources are sufficient, allocates them to a specific page group and updates the global page state.",
            "input": {
                "shape": "released_state: PageState object, page_group_id: [], true_length: []",
                "dtype": "PageState, int32, int32"
            },
            "processing_steps": [
                "Calculate the number of pages needed (`num_pages_needed`) based on `true_length` and `tokens_per_page`.",
                "Calculate the position for the next token write.",
                "Check for resource availability by comparing `num_pages_needed` against the number of free pages and `max_pages_per_group`.",
                "Define a nested function `allocate_and_update_state` to handle the allocation logic.",
                "Use `jax.lax.fori_loop` within `allocate_and_update_state` to iteratively find and allocate one page at a time for `num_pages_needed` iterations.",
                "In each loop iteration, call `_find_next_free_page_index` and update `page_status`, `page_map`, and `num_pages_used`.",
                "After the loop, update the group's specific state: `sequence_lengths`, `active_page`, `has_active_page`, and `active_page_position`.",
                "Use `jax.lax.cond` to execute `allocate_and_update_state` if resources are sufficient, otherwise return the input `released_state` unchanged."
            ],
            "output": {
                "shape": "A PageState object with the same tensor shapes as the input."
            },
            "dependencies": [
                "PageState",
                "_find_next_free_page_index",
                "jax.lax.cond",
                "jax.lax.fori_loop",
                "jax.numpy"
            ],
            "parameters": {
                "tokens_per_page": "The capacity of each page in tokens.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single group."
            },
            "notes": [
                "This function is JIT-compiled with `tokens_per_page` and `max_pages_per_group` as static arguments.",
                "It has a precondition that `true_length` must be greater than 0.",
                "If allocation fails due to insufficient resources, the function returns the input `released_state` unmodified, effectively leaving the target group empty."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_and_reserve_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_and_reserve_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases existing pages and reserves new pages for a group during prefill.\n\n  Assumes true_length > 0. Caller MUST validate inputs.\n  \"\"\"\n  released_state = _release_pages_for_group(page_state, page_group_id, max_pages_per_group)\n  final_state = _reserve_pages_for_group(released_state, page_group_id, true_length, tokens_per_page, max_pages_per_group)\n  return final_state",
        "analysis": {
            "module_type": "page_management_helper_function",
            "purpose": "Releases existing pages and reserves new pages for a specific group, typically during the prefill stage of sequence processing.",
            "input": {
                "shape": "page_state: PageState object, page_group_id: ScalarInt, true_length: ScalarInt, tokens_per_page: int, max_pages_per_group: int",
                "dtype": "PageState contains jax.numpy arrays (int32, bool), other arguments are integers."
            },
            "processing_steps": [
                "Calls `_release_pages_for_group` to free all pages currently associated with the specified `page_group_id`.",
                "Calls `_reserve_pages_for_group` on the resulting state to allocate a new set of pages based on the `true_length`.",
                "Returns the final, updated `PageState`."
            ],
            "output": {
                "shape": "Returns a PageState object with the same structure and shapes as the input `page_state`."
            },
            "dependencies": [
                "PageState",
                "_release_pages_for_group",
                "_reserve_pages_for_group"
            ],
            "parameters": {
                "page_state": "The current global state of memory pages.",
                "page_group_id": "The scalar integer ID of the page group (e.g., a specific request/sequence) to update.",
                "true_length": "The target sequence length for which to reserve pages. Assumed to be greater than 0.",
                "tokens_per_page": "The capacity of each memory page.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single group."
            },
            "notes": [
                "This function combines a release and a reserve operation into a single step for updating a group's page allocation.",
                "The docstring explicitly states that the caller is responsible for validating inputs, particularly ensuring `true_length` > 0."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_update_decode_pages_global",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _update_decode_pages_global(\n    page_state: PageState,\n    tokens_per_page: ScalarInt,\n    max_pages_per_group: ScalarInt,\n) -> PageState:\n  \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n  This function performs the following steps for all page groups simultaneously:\n  1. Increments `sequence_lengths` for groups marked as `has_active_page`.\n  2. Calculates the new `active_page_position` based on the incremented length.\n  3. Determines which active groups now require a new page because their sequence\n     length has crossed a page boundary (`required_pages > num_pages_used`) and\n     they still have capacity (`required_pages <= max_pages_per_group`).\n  4. For each group identified in step 3, it attempts to find a free page globally and\n     allocate it, updating `page_status`, `page_map`, `num_pages_used`, and\n     `active_page` for that group.\n\n  Args:\n    page_state: The current global `PageState`.\n    tokens_per_page: The capacity of each page.\n    max_pages_per_group: The maximum number of pages allowed per group.\n\n  Returns:\n    A new `PageState` object reflecting the state after the decode step, potentially\n    with new pages allocated to groups that crossed page boundaries.\n  \"\"\"\n  max_page_groups = page_state.sequence_lengths.shape[0]\n\n  seq_len_increment = jnp.where(page_state.has_active_page, 1, 0)\n  new_sequence_lengths = page_state.sequence_lengths + seq_len_increment\n\n  new_active_page_position = jnp.where(\n      page_state.has_active_page,\n      (new_sequence_lengths - 1) % tokens_per_page,\n      page_state.active_page_position,\n  )\n\n  required_pages_per_group = (new_sequence_lengths + tokens_per_page - 1) // tokens_per_page\n  needs_new_page_mask = jnp.logical_and(page_state.has_active_page, required_pages_per_group > page_state.num_pages_used)\n  has_capacity_mask = required_pages_per_group <= max_pages_per_group\n  needs_allocation_mask = jnp.logical_and(needs_new_page_mask, has_capacity_mask)\n\n  def allocate_for_group_if_needed(group_idx: ScalarInt, current_state: PageState) -> PageState:\n    \"\"\"Inner function for fori_loop to conditionally allocate a page.\"\"\"\n    current_status = current_state.page_status\n    current_map = current_state.page_map\n    current_num_used = current_state.num_pages_used\n    current_active_page = current_state.active_page\n\n    needs_alloc = needs_allocation_mask[group_idx]\n    next_free_page_global = _find_next_free_page_index(current_status)\n    can_allocate = jnp.logical_and(needs_alloc, next_free_page_global >= 0)\n\n    new_status = jax.lax.cond(can_allocate, lambda s: s.at[next_free_page_global].set(1), lambda s: s, current_status)\n\n    page_map_index = current_num_used[group_idx]\n    new_map = jax.lax.cond(\n        can_allocate, lambda m: m.at[group_idx, page_map_index].set(next_free_page_global), lambda m: m, current_map\n    )\n    new_num_used = jax.lax.cond(can_allocate, lambda n: n.at[group_idx].add(1), lambda n: n, current_num_used)\n    new_active_page = jax.lax.cond(\n        can_allocate, lambda a: a.at[group_idx].set(next_free_page_global), lambda a: a, current_active_page\n    )\n\n    # Reconstruct state for loop carry/return\n    return current_state.replace(\n        page_status=new_status,\n        page_map=new_map,\n        num_pages_used=new_num_used,\n        active_page=new_active_page,\n    )\n\n  # Initialize loop state with pre-calculated lengths and positions\n  initial_loop_state = page_state.replace(\n      sequence_lengths=new_sequence_lengths,\n      active_page_position=new_active_page_position,\n  )\n\n  # Apply conditional allocation across all groups\n  final_state = jax.lax.fori_loop(0, max_page_groups, allocate_for_group_if_needed, initial_loop_state)\n  return final_state",
        "analysis": {
            "functionality": "Updates the global page state for a single autoregressive decoding step by incrementing sequence lengths for active groups and allocating new pages to any group that crosses a page boundary.",
            "usage": "This function is called within a decoding loop. It takes the current `PageState`, the number of tokens per page, and the maximum pages per group as input. It returns a new `PageState` object reflecting the state after one token has been generated for all active sequences."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageManager",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageManager:\n  \"\"\"Manages the global allocation and release of pages for paged attention.\n\n  This class provides an interface for reserving pages during prefill and\n  decoding, and for releasing pages when a sequence (page group) is complete.\n  It encapsulates the logic for tracking page allocation globally and managing\n  the `PageState`. It uses the concept of page groups, where each group typically\n  corresponds to a single request or sequence being processed.\n\n  Example:\n    ```python\n    # Initialize a PageManager from configuration\n    config = YourConfig(...) # Set pagedattn_num_pages, etc.\n    page_manager = PageManager(config)\n\n    # Get initial page state (all pages free, except potentially page 0)\n    state = page_manager.get_initial_page_state()\n\n    # Update pages for prefill of a sequence in group 0 with length 16\n    state = page_manager.update_prefill_pages(\n        page_state=state,\n        page_group_id=0,\n        true_length=16\n    )\n\n    # Update pages for a single decode step (increments lengths, allocates if needed)\n    state = page_manager.update_decode_pages(state)\n\n    # Release pages associated with group 0 when the sequence is finished\n    state = page_manager.release_pages(\n        page_state=state,\n        page_group_id=0\n    )\n    ```\n  \"\"\"\n\n  def __init__(self, config: Config):\n    \"\"\"Initializes the `PageManager` from a configuration object.\n\n    Args:\n      config: A `Config` object containing the necessary parameters:\n        * `max_target_length`: The maximum sequence length supported.\n        * `pagedattn_num_pages`: The total number of pages available globally.\n        * `pagedattn_tokens_per_page`: The number of tokens each page can hold.\n        * `global_batch_size_to_load`: Used to determine the maximum number of concurrent\n          page groups (`max_page_groups`) the system can manage.\n        * `pagedattn_max_pages_per_group`: The maximum number of pages that can be\n          allocated to a single page group.\n\n    Raises:\n      ValueError: If the configuration parameters are invalid (e.g., non-positive\n        values, insufficient pages per group for max length).\n    \"\"\"\n    self.num_pages: int = config.pagedattn_num_pages\n    self.tokens_per_page: int = config.pagedattn_tokens_per_page\n    self.max_target_length: int = config.max_target_length\n    self.max_page_groups: int = config.global_batch_size_to_load\n    self.max_pages_per_group: int = config.pagedattn_max_pages_per_group\n    self._validate_init_params()\n\n  def _validate_init_params(self) -> None:\n    \"\"\"Validates initialization parameters for logical consistency.\"\"\"\n    if self.max_pages_per_group <= 0:\n      raise ValueError(\"`pagedattn_max_pages_per_group` must be positive.\")\n    min_required = (self.max_target_length + self.tokens_per_page - 1) // self.tokens_per_page\n    if self.max_pages_per_group < min_required:\n      raise ValueError(\n          f\"`pagedattn_max_pages_per_group` ({self.max_pages_per_group}) is insufficient for `max_target_length` \"\n          f\"({self.max_target_length}). Needs {min_required}.\"\n      )\n    # Check > 1 due to potential page 0 workaround\n    if self.num_pages <= 1:\n      raise ValueError(\"`pagedattn_num_pages` must be greater than 1.\")\n    if self.tokens_per_page <= 0:\n      raise ValueError(\"`pagedattn_tokens_per_page` must be positive.\")\n    if self.max_page_groups <= 0:\n      raise ValueError(\"`pagedattn_max_page_groups` must be positive.\")\n\n  def update_prefill_pages(self, page_state: PageState, page_group_id: int, true_length: int) -> PageState:\n    \"\"\"Reserves pages for a specific page group during prefill (global state).\n\n    This method first releases any pages currently allocated to the given\n    `page_group_id`. It then attempts to allocate the necessary number of pages\n    from the global pool to accommodate a sequence of `true_length`. If successful,\n    it updates the `PageState` to reflect the new allocation and marks the group\n    as active. If there are not enough free pages globally or the group exceeds\n    its `max_pages_per_group` limit, the group's state remains cleared (as after\n    the initial release). Input validation ensures `page_group_id` and `true_length`\n    are within valid ranges.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to allocate pages for. Must\n        be between 0 and `max_page_groups - 1`.\n      true_length: The sequence length to allocate pages for. Must be between 0\n        and `max_target_length`.\n\n    Returns:\n      The updated `PageState`. If allocation fails due to resource limits, the\n      returned state will have the specified `page_group_id` cleared.\n\n    Raises:\n      ValueError: If `page_group_id` or `true_length` are outside their valid ranges.\n\n    Example:\n      ```python\n      # Reserve pages for a 16-token sequence in group 0\n      state = page_manager.update_prefill_pages(\n          page_state=state,\n          page_group_id=0,\n          true_length=16\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    if true_length <= 0 or true_length > self.max_target_length:\n      raise ValueError(f\"PageManager: true_length ({true_length}) out of range (0, {self.max_target_length}]\")\n\n    return _release_and_reserve_for_group(\n        page_state, page_group_id, true_length, self.tokens_per_page, self.max_pages_per_group\n    )\n\n  def update_decode_pages(self, page_state: PageState) -> PageState:\n    \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n    This method advances the state for all active page groups. It increments\n    their sequence lengths by one and updates their position within the current\n    active page. If this increment causes a sequence to cross a page boundary\n    (i.e., it needs more pages than currently allocated), this method attempts\n    to allocate a new page from the global pool, provided the group has not\n    reached its `max_pages_per_group` limit and free pages are available.\n\n    Args:\n      page_state: The current global `PageState`.\n\n    Returns:\n      The updated `PageState` reflecting the state after the decode step.\n      Sequence lengths and active positions are updated for all active groups.\n      Groups that required and successfully obtained a new page will have their\n      `num_pages_used`, `page_map`, and `active_page` updated.\n\n    Example:\n      ```python\n      # Advance state for all active sequences by one decode step\n      state = page_manager.update_decode_pages(state)\n      ```\n    \"\"\"\n    return _update_decode_pages_global(page_state, self.tokens_per_page, self.max_pages_per_group)\n\n  def release_pages(self, page_state: PageState, page_group_id: int) -> PageState:\n    \"\"\"Releases all pages associated with a given page group (global state).\n\n    This method identifies all pages currently allocated to the specified\n    `page_group_id` using the `page_map` and `num_pages_used`. It marks these\n    pages as free (status 0) in the global `page_status` array. It also resets\n    all state information specific to the `page_group_id` (sequence length,\n    page count, active status, etc.) to their initial zero/False values.\n    Input validation ensures the `page_group_id` is within the valid range.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to release. Must be\n        between 0 and `max_page_groups - 1`.\n\n    Returns:\n      The updated `PageState` after releasing the pages and resetting the group's\n      state.\n\n    Raises:\n      ValueError: If `page_group_id` is outside its valid range.\n\n    Example:\n      ```python\n      # Release all pages currently held by group 0\n      state = page_manager.release_pages(\n          page_state=state,\n          page_group_id=0\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    return _release_pages_for_group(page_state, page_group_id, self.max_pages_per_group)\n\n  def get_initial_page_state(self) -> PageState:\n    \"\"\"Creates and returns an initial global `PageState`.\n\n    This is a convenience method that calls `initialize_page_state` with\n    the parameters (`num_pages`, `max_page_groups`, `max_pages_per_group`)\n    stored during the `PageManager` initialization.\n\n    Returns:\n      An initialized `PageState` object where all pages are free (except possibly 0)\n      and no groups are active.\n\n    Example:\n      ```python\n      # Get a fresh, empty page state\n      initial_state = page_manager.get_initial_page_state()\n      ```\n    \"\"\"\n    return initialize_page_state(\n        num_pages=self.num_pages,\n        max_page_groups=self.max_page_groups,\n        max_pages_per_group=self.max_pages_per_group,\n    )",
        "analysis": {
            "module_type": "paged_attention_manager",
            "purpose": "Manages the global allocation and release of memory pages for paged attention, providing an interface to reserve pages for prefill, update them during decoding, and release them when a sequence is complete.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes with configuration parameters defining the page system's dimensions.",
                "Provides methods to get an initial state, update the state for prefill and decode steps, and release pages for a given sequence."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Config",
                "PageState",
                "initialize_page_state",
                "_release_and_reserve_for_group",
                "_update_decode_pages_global",
                "_release_pages_for_group"
            ],
            "parameters": {
                "pagedattn_num_pages": "The total number of pages available globally.",
                "pagedattn_tokens_per_page": "The number of tokens each page can hold.",
                "max_target_length": "The maximum sequence length supported.",
                "global_batch_size_to_load": "Used to determine the maximum number of concurrent page groups.",
                "pagedattn_max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "This class encapsulates the logic for tracking page allocation globally and managing the `PageState`.",
                "It uses the concept of page groups, where each group typically corresponds to a single request or sequence being processed."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PageManager from a configuration object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "Config object"
                    },
                    "processing_steps": [
                        "Store paging-related parameters from the config object as instance attributes.",
                        "Call `_validate_init_params` to ensure parameter consistency."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "_validate_init_params"
                    ],
                    "notes": [
                        "Raises ValueError if the configuration parameters are invalid."
                    ]
                },
                "_validate_init_params": {
                    "purpose": "Validates initialization parameters for logical consistency.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `max_pages_per_group` is positive.",
                        "Check if `max_pages_per_group` is sufficient for `max_target_length`.",
                        "Check if `num_pages` is greater than 1.",
                        "Check if `tokens_per_page` is positive.",
                        "Check if `max_page_groups` is positive."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Raises ValueError if any of the validation checks fail."
                    ]
                },
                "update_prefill_pages": {
                    "purpose": "Reserves pages for a specific page group during prefill by first releasing any existing pages and then allocating new ones.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "PageState object, int, int"
                    },
                    "processing_steps": [
                        "Validate that `page_group_id` and `true_length` are within their valid ranges.",
                        "Call the JIT-compiled function `_release_and_reserve_for_group` to perform the state update."
                    ],
                    "output": {
                        "shape": "Returns an updated PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "_release_and_reserve_for_group"
                    ],
                    "notes": [
                        "If allocation fails due to resource limits, the returned state will have the specified `page_group_id` cleared."
                    ]
                },
                "update_decode_pages": {
                    "purpose": "Updates pages globally for one step of autoregressive decoding, allocating new pages as needed when sequences cross page boundaries.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "PageState object"
                    },
                    "processing_steps": [
                        "Call the JIT-compiled function `_update_decode_pages_global` to advance the state for all active page groups."
                    ],
                    "output": {
                        "shape": "Returns an updated PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "_update_decode_pages_global"
                    ],
                    "notes": [
                        "This method increments sequence lengths by one for all active groups and handles new page allocation if required."
                    ]
                },
                "release_pages": {
                    "purpose": "Releases all pages associated with a given page group and resets the group's state.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "PageState object, int"
                    },
                    "processing_steps": [
                        "Validate that `page_group_id` is within its valid range.",
                        "Call the JIT-compiled function `_release_pages_for_group` to free the pages and reset the group's state."
                    ],
                    "output": {
                        "shape": "Returns an updated PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "_release_pages_for_group"
                    ],
                    "notes": [
                        "This marks the pages as free in the global `page_status` array and resets the group's sequence length, page count, etc."
                    ]
                },
                "get_initial_page_state": {
                    "purpose": "Creates and returns an initial global PageState where all pages are free and no groups are active.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call the `initialize_page_state` function with the parameters stored during the PageManager's initialization."
                    ],
                    "output": {
                        "shape": "Returns an initialized PageState object."
                    },
                    "dependencies": [
                        "PageState",
                        "initialize_page_state"
                    ],
                    "notes": [
                        "This is a convenience method for creating a fresh, empty page state."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#paged_attention_op_as_linen",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "def paged_attention_op_as_linen(\n    *,\n    mesh: Mesh,\n    num_pages: int,\n    tokens_per_page: int,\n    max_pages_per_slot: int,\n    max_pages_per_prefill: int,\n    pages_per_compute_block: int,\n    num_kv_heads: int,\n    kv_head_dim_size: int,\n    dtype: DType = jnp.float32,\n    attn_logits_soft_cap: float | None = None,\n    query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n    kv_pages_axis_names: AxisNames = (\n        \"paged_kv_heads\",\n        \"num_pages\",\n        \"tokens_per_page\",\n        \"paged_kv_head_dim_size\",\n    ),\n):\n  \"\"\"A factory function to create a PagedAttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `PagedAttentionOp`\n  within a Linen model. It wraps the `PagedAttentionOp` module using\n  `nnx.bridge.to_linen`, making it compatible with the Linen API. This is\n  useful for gradual migration of a codebase from Linen to NNX.\n\n  Args:\n    mesh: The device mesh for sharding.\n    num_pages: The total number of pages in the KV cache.\n    tokens_per_page: The number of tokens each page can hold.\n    max_pages_per_slot: The maximum number of pages a single sequence can use.\n    max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n    pages_per_compute_block: The number of pages processed in one kernel block.\n    num_kv_heads: The number of key/value heads.\n    kv_head_dim_size: The dimension of each key/value head.\n    dtype: The data type for computations.\n    attn_logits_soft_cap: The soft cap for attention logits.\n    query_axis_names: The logical axis names for the query tensor.\n    kv_pages_axis_names: The logical axis names for the KV cache pages.\n\n  Returns:\n    A Linen module that wraps the NNX `PagedAttentionOp` module.\n  \"\"\"\n\n  return nnx.bridge.to_linen(\n      PagedAttentionOp,\n      mesh=mesh,\n      num_pages=num_pages,\n      tokens_per_page=tokens_per_page,\n      max_pages_per_slot=max_pages_per_slot,\n      max_pages_per_prefill=max_pages_per_prefill,\n      pages_per_compute_block=pages_per_compute_block,\n      num_kv_heads=num_kv_heads,\n      kv_head_dim_size=kv_head_dim_size,\n      dtype=dtype,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      query_axis_names=query_axis_names,\n      kv_pages_axis_names=kv_pages_axis_names,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "paged_attention_linen_factory",
            "purpose": "A factory function that wraps the NNX-based PagedAttentionOp class into a Flax Linen-compatible module, facilitating gradual migration from Linen to NNX.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx.bridge.to_linen` to wrap the `PagedAttentionOp` class, passing along all configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx.bridge.to_linen",
                "PagedAttentionOp",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "mesh": "The device mesh for sharding.",
                "num_pages": "The total number of pages in the KV cache.",
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_slot": "The maximum number of pages a single sequence can use.",
                "max_pages_per_prefill": "The maximum number of pages for a prefill sequence.",
                "pages_per_compute_block": "The number of pages processed in one kernel block.",
                "num_kv_heads": "The number of key/value heads.",
                "kv_head_dim_size": "The dimension of each key/value head.",
                "dtype": "The data type for computations.",
                "attn_logits_soft_cap": "The soft cap for attention logits."
            },
            "notes": [
                "This function serves as a compatibility bridge to use an NNX module within a Linen model.",
                "The returned object is a Linen module, not a tensor. All keyword arguments are forwarded to the `PagedAttentionOp` constructor."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#PagedAttentionOp",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "class PagedAttentionOp(nnx.Module):\n  \"\"\"An NNX module for paged attention.\n\n  This module implements the paged attention mechanism, which is an efficient\n  method for handling attention in autoregressive models with long sequences.\n  It divides the KV cache into fixed-size \"pages\" to manage memory dynamically.\n  \"\"\"\n\n  def __init__(\n      self,\n      mesh: Mesh,\n      num_pages: int,\n      tokens_per_page: int,\n      max_pages_per_slot: int,\n      max_pages_per_prefill: int,\n      pages_per_compute_block: int,\n      num_kv_heads: int,\n      kv_head_dim_size: int,\n      dtype: DType = jnp.float32,\n      attn_logits_soft_cap: float | None = None,\n      query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n      kv_pages_axis_names: AxisNames = (\n          \"paged_kv_heads\",\n          \"num_pages\",\n          \"tokens_per_page\",\n          \"paged_kv_head_dim_size\",\n      ),\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the PagedAttentionOp module.\n\n    Args:\n      mesh: The device mesh for sharding.\n      num_pages: The total number of pages in the KV cache.\n      tokens_per_page: The number of tokens each page can hold.\n      max_pages_per_slot: The maximum number of pages a single sequence can use.\n      max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n      pages_per_compute_block: The number of pages processed in one kernel block.\n      num_kv_heads: The number of key/value heads.\n      kv_head_dim_size: The dimension of each key/value head.\n      dtype: The data type for computations.\n      attn_logits_soft_cap: The soft cap for attention logits.\n      query_axis_names: The logical axis names for the query tensor.\n      kv_pages_axis_names: The logical axis names for the KV cache pages.\n      rngs: The random number generators for initialization (required by NNX).\n    \"\"\"\n\n    self.mesh = mesh\n    self.num_pages = num_pages\n    self.tokens_per_page = tokens_per_page\n    self.max_pages_per_slot = max_pages_per_slot\n    self.max_pages_per_prefill = max_pages_per_prefill\n    self.pages_per_compute_block = pages_per_compute_block\n    self.num_kv_heads = num_kv_heads\n    self.kv_head_dim_size = kv_head_dim_size\n    self.dtype = dtype\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.query_axis_names = query_axis_names\n    self.kv_pages_axis_names = kv_pages_axis_names\n\n    self.kv_pages_shape = (\n        self.num_kv_heads,\n        self.num_pages,\n        self.tokens_per_page,\n        self.kv_head_dim_size,\n    )\n\n    self.key_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n    self.value_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n\n  def _maybe_materialize_cache(self, cache: nnx.Cache) -> nnx.Cache:\n    \"\"\"Materializes the cache if it's currently a ShapeDtypeStruct.\"\"\"\n    if isinstance(cache.value, jax.ShapeDtypeStruct):\n      # This is needed because the Linen bridge lazily creates this state. We\n      # need to ensure the cache state is accessible at runtime.\n      # TODO: Delete this function when the to_linen bridge is no longer needed.\n      return nnx.Cache(\n          jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n          sharding=cache.sharding,\n      )\n    return cache\n\n  def get_kv_pages(self):\n    \"\"\"Retrieves the key and value page caches.\n\n    This method ensures the KV cache pages are materialized (if they are abstract\n    ShapeDtypeStructs, a temporary state during Linen bridge initialization) and\n    applies the necessary sharding constraints.\n\n    Returns:\n      A tuple containing the key pages and value pages caches (`nnx.Cache`).\n    \"\"\"\n\n    # TODO: Remove once to_linen bridge is no longer needed\n    self.key_pages = self._maybe_materialize_cache(self.key_pages)\n    self.value_pages = self._maybe_materialize_cache(self.value_pages)\n\n    self.key_pages.value = nn.with_logical_constraint(self.key_pages.value, self.kv_pages_axis_names)\n    self.value_pages.value = nn.with_logical_constraint(self.value_pages.value, self.kv_pages_axis_names)\n    return self.key_pages, self.value_pages\n\n  def pad_qkv(self, *qkv):\n    \"\"\"Pad input to kv_head_dim_size\"\"\"\n\n    def pad_to_kv_head_dim_size(x):\n      if x.shape[-1] != self.kv_head_dim_size:\n        return jnp.pad(\n            x,\n            ((0, 0), (0, 0), (0, 0), (0, self.kv_head_dim_size - x.shape[-1])),\n            mode=\"constant\",\n            constant_values=0.0,\n        )\n      else:\n        return x\n\n    # Align Q, K, V to the same head dim. This is required by the kernel.\n    return tuple(pad_to_kv_head_dim_size(x) for x in qkv)\n\n  def paged_dot_product_attention_with_max_and_sum(self, query, key, value):\n    \"\"\"paged dot product attention with max & sum\"\"\"\n    b, t, n, d = query.shape\n    _, s, n_kv, _ = key.shape\n    query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n\n    attn_weights = jnp.einsum(\"btkgd,bskd->bkgts\", query, key)\n\n    causal_mask = jnp.triu(jnp.ones((t, s)), k=1)\n    causal_mask = jnp.reshape(causal_mask, (1, 1, 1, t, s))\n    masked_weights = jnp.where(causal_mask, jnp.full_like(attn_weights, -1e10), attn_weights)\n\n    local_max = jnp.max(masked_weights, axis=-1, keepdims=True)\n    local_exps = jnp.exp(masked_weights - local_max)\n    local_sums = jnp.sum(local_exps, axis=-1, keepdims=True)\n\n    attn = jnp.einsum(\"bkgts,bskd->btkgd\", local_exps, value)\n    attn = jnp.reshape(attn, (b, t, n, d))\n\n    local_max = jnp.moveaxis(local_max, -2, 1)\n    local_max = jnp.reshape(local_max, (b, t, n, 1))\n\n    local_sums = jnp.moveaxis(local_sums, -2, 1)\n    local_sums = jnp.reshape(local_sums, (b, t, n, 1))\n\n    return attn, local_max, local_sums\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_prefill(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in prefill only. The assumption\n    is the batch_size is only 1\n    \"\"\"\n    assert query.shape[0] == 1  # ensure the batch size is 0\n    # shape of key_pages_cache.value is [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.array([0, page_state.sequence_lengths[0]])  # [0, prefill_true_length]\n    num_seqs = jnp.array([1])\n    query = query[0]  # [batch_size, max_num_tokens, num_kv_heads, head_dim] to [max_num_tokens, num_kv_heads, head_dim]\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,\n        kv_lens=jnp.array([query.shape[0]]),  # max_prefill_length\n        cu_q_lens=c_q_l,  # the accumulative real lengths of requests, starting from 0\n        page_indices=page_state.page_map,\n        num_seqs=num_seqs,\n        # TODO(rupliu) debug: repeated response when enabled below\n        # num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(result, axis=0)  # [batch_size, seq_len, n_kv_head, head_dim] and batch_size is 1 for now\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in decode only.\"\"\"\n    batch_size = query.shape[0]\n    query = jnp.squeeze(query, axis=1)  # [batch_size, seq_len, n_kv_head, head_dim] to [batch_size, n_kv_head, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.arange(batch_size + 1)  # one token per sequence\n    num_seqs = jnp.array([batch_size])  # real number of requests, set it to batch_size\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,  # [max_batched_num_tokens, num_kv_heads, head_dim]\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        kv_lens=page_state.sequence_lengths,  # [max_num_seqs]\n        cu_q_lens=c_q_l,  # [max_num_seqs+1]\n        page_indices=page_state.page_map,  # [max_num_seqs, pages_per_seq]\n        num_seqs=num_seqs,\n        num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(\n        result, axis=1\n    )  # [batch_size, n_kv_head, head_dim] to [batch_size, seq_len, n_kv_head, head_dim]\n\n  # v1 kernel has around 20% performance gain than v2 kernel in decode only task\n  def paged_attention_v1_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply Paged Attention v1 in decode only.\"\"\"\n    kv_pages_pspec = nn.logical_to_mesh_axes((\"paged_kv_heads\", None, None, None))\n    q_pspec = nn.logical_to_mesh_axes((None, None, \"paged_kv_heads\", None))\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            q_pspec,\n            kv_pages_pspec,\n            kv_pages_pspec,\n            P(None),\n            P(None, None),\n            None,\n        ),\n        out_specs=q_pspec,\n        check_vma=False,\n    )\n    def wrap_paged_attention(q, k_pages, v_pages, lengths, page_indices, pages_per_compute_block):\n      q = jnp.squeeze(q, axis=1)\n      result = paged_attention_kernel.paged_attention(\n          q=q,  # [batch_size, num_heads, head_dim]\n          k_pages=k_pages,\n          v_pages=v_pages,\n          lengths=lengths,\n          page_indices=page_indices,\n          pages_per_compute_block=pages_per_compute_block,\n      )\n      return jnp.expand_dims(result, axis=1)  # [batch_size, n_kv_head, head_dim] to [batch_size, 1, n_kv_head, head_dim]\n\n    return wrap_paged_attention(\n        query,\n        key_pages_cache.value,\n        value_pages_cache.value,\n        page_state.sequence_lengths,\n        page_state.page_map,\n        self.pages_per_compute_block,\n    )\n\n  def __call__(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    \"\"\"Applies the paged attention mechanism.\n\n    This is the main entry point for the module. It takes query, key, and value\n    tensors and performs paged attention based on the current model mode\n    (prefill or autoregressive).\n\n    Args:\n      query: The query tensor.\n      key: The key tensor for the current step.\n      value: The value tensor for the current step.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The current operational mode, either 'prefill' or\n        'autoregressive'.\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n      slot: The batch slot index for the current request.\n      page_state: The current state of the page manager.\n\n    Returns:\n        A tuple (output, exponentials_max, exponentials_sum) containing:\n        - The attention output tensor.\n        - The max of the exponentials (for prefill mode with dot-product attention).\n        - The sum of the exponentials (for prefill mode with dot-product attention).\n        The latter two are None for autoregressive mode, as this is handled\n        internally by the paged attention kernel.\n    \"\"\"\n\n    key_pages_cache, value_pages_cache = self.get_kv_pages()\n    query, key, value = self.pad_qkv(query, key, value)\n\n    # update kv pages and call page attention kernel\n    if model_mode == MODEL_MODE_PREFILL:\n      self.update_prefill_step_pages(key_pages_cache, value_pages_cache, key, value, slot, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_prefill(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return self.paged_dot_product_attention_with_max_and_sum(query, key, value)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and page_state is not None:\n      self.update_decode_step_pages(key_pages_cache, value_pages_cache, key, value, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_decode(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return (\n          self.paged_attention_v1_decode(query, key_pages_cache, value_pages_cache, page_state),\n          None,\n          None,\n      )\n    else:\n      raise NotImplementedError(model_mode)\n\n  def update_prefill_step_pages(\n      self,\n      key_pages_cache: nnx.Cache,  # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n      value_pages_cache: nnx.Cache,\n      key: Array,\n      value: Array,\n      slot: int,\n      page_state: page_manager.PageState,\n  ) -> None:\n    \"\"\"Update pages for prefill step.\"\"\"\n    assert (\n        key.shape == value.shape\n    ), f\"prefill_step key/value should have the same shape, but getting {key.shape=} and {value.shape=} instead\"\n    batch_size, seq_len, n_kv_head, head_dim = key.shape\n    assert seq_len % self.tokens_per_page == 0, f\"seq_length {seq_len} and  tokens_per_page {self.tokens_per_page}\"\n    assert key_pages_cache.value.shape == value_pages_cache.value.shape, (\n        f\"prefill_step key/value_pages_cache should have the same shape, but \"\n        f\"getting {key_pages_cache.shape=} and {value_pages_cache.shape=} instead\"\n    )\n\n    v_n_kv, _, v_p, v_d = key_pages_cache.value.shape\n    assert v_n_kv == n_kv_head, f\"{v_n_kv=} {n_kv_head=}\"\n    assert v_p == self.tokens_per_page, f\"{v_p=} {self.tokens_per_page=}\"\n    assert v_d == head_dim, f\"{v_d=} {head_dim=}\"\n    assert page_state.page_map.shape == (\n        page_state.num_pages_used.shape[0],\n        self.max_pages_per_slot,\n    )\n\n    # Handle both init (b>1) and runtime (b=1) cases\n    if batch_size == 1:\n      key = jnp.squeeze(key)  # [batch_size, seq_len, n_kv_head, head_dim] to [seq_len, n_kv_head, head_dim]\n      value = jnp.squeeze(value)\n    else:\n      key = key[0]\n      value = value[0]\n\n    key = jnp.transpose(key, axes=(1, 0, 2))\n    value = jnp.transpose(value, axes=(1, 0, 2))\n\n    key = jnp.reshape(\n        key,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n    value = jnp.reshape(\n        value,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n\n    key_pages_cache.value = nn.with_logical_constraint(key, self.kv_pages_axis_names)\n    value_pages_cache.value = nn.with_logical_constraint(value, self.kv_pages_axis_names)\n\n  def update_decode_step_pages(self, key_pages_cache, value_pages_cache, key, value, page_state):\n    \"\"\"Update decode-step pages\"\"\"\n    key_pages = key_pages_cache.value\n    value_pages = value_pages_cache.value\n\n    batch_size, _, kv_heads, head_dim = key.shape\n    kv_heads, _, _, head_dim = key_pages.shape\n\n    new_key = key.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_key = jnp.transpose(new_key, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n    new_value = value.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_value = jnp.transpose(new_value, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n\n    broadcast_pages = jnp.tile(page_state.active_page, (kv_heads, 1))  # [n_kv_heads, batch_size]\n    broadcast_pos = jnp.tile(page_state.active_page_position, (kv_heads, 1))  # [n_kv_heads, batch_size]\n\n    kv_indices = jnp.arange(kv_heads)[:, None]  # [n_kv_heads, 1]\n    kv_indices = jnp.tile(kv_indices, (1, batch_size))  # [n_kv_heads, batch_size]\n\n    # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    key_pages_updated = key_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_key)\n    value_pages_updated = value_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_value)\n\n    key_pages_cache.value = key_pages_updated\n    value_pages_cache.value = value_pages_updated\n    return key_pages_cache, value_pages_cache",
        "analysis": {
            "module_type": "paged_attention",
            "purpose": "An NNX module that implements the paged attention mechanism, an efficient method for handling attention in autoregressive models by dividing the KV cache into fixed-size 'pages' to manage memory dynamically.",
            "input": {
                "shape": "query: [batch_size, sequence_length, num_heads, head_dim], key: [batch_size, sequence_length, num_kv_heads, head_dim], value: [batch_size, sequence_length, num_kv_heads, head_dim]",
                "dtype": "float32"
            },
            "processing_steps": [
                "Retrieves and materializes the key and value page caches using `get_kv_pages`.",
                "Pads the query, key, and value tensors to a consistent head dimension using `pad_qkv`.",
                "Checks the `model_mode` to determine the execution path ('prefill' or 'autoregressive').",
                "For 'prefill' mode, updates the KV cache pages with the full sequence using `update_prefill_step_pages` and computes attention using either a custom kernel (`paged_attention_v2_prefill`) or a standard dot-product attention implementation.",
                "For 'autoregressive' mode, updates the KV cache pages with the single new token using `update_decode_step_pages` and computes attention using a custom paged attention kernel (`paged_attention_v1_decode` or `paged_attention_v2_decode`).",
                "Returns the attention output."
            ],
            "output": {
                "shape": "A tuple containing: (attention_output: [batch_size, sequence_length, num_heads, head_dim], exponentials_max: Optional[Array], exponentials_sum: Optional[Array])"
            },
            "dependencies": [
                "flax.nnx.Module",
                "flax.nnx.Cache",
                "MaxText.inference.page_manager.PageState",
                "jax.experimental.pallas.ops.tpu.paged_attention.paged_attention_kernel",
                "MaxText.inference.paged_attention_kernel_v2"
            ],
            "parameters": {
                "mesh": "The device mesh for sharding.",
                "num_pages": "The total number of pages in the KV cache.",
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_slot": "The maximum number of pages a single sequence can use.",
                "num_kv_heads": "The number of key/value heads.",
                "kv_head_dim_size": "The dimension of each key/value head.",
                "pages_per_compute_block": "The number of pages processed in one kernel block."
            },
            "notes": [
                "The module operates in two distinct modes: 'prefill' for processing initial prompts and 'autoregressive' for generating subsequent tokens.",
                "It maintains internal state for the key and value caches (`key_pages`, `value_pages`) as `nnx.Cache` objects.",
                "The choice of which paged attention kernel to use (v1, v2, or standard dot-product) is determined by the `model_mode` and a global `_use_kernel_v2` flag.",
                "Contains temporary logic (`_maybe_materialize_cache`) to handle lazy initialization when used with the `nnx.bridge.to_linen` compatibility layer."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PagedAttentionOp module, setting up configuration parameters and creating the paged KV caches.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (mesh, num_pages, tokens_per_page, etc.).",
                        "Define the shape of the KV page tensors.",
                        "Initialize `self.key_pages` and `self.value_pages` as `nnx.Cache` objects filled with zeros."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Cache",
                        "jax.numpy.zeros"
                    ],
                    "notes": [
                        "The `rngs` argument is required by the `nnx.bridge.to_linen` but is not used by this module's logic."
                    ]
                },
                "_maybe_materialize_cache": {
                    "purpose": "Materializes an `nnx.Cache` object by replacing an abstract `jax.ShapeDtypeStruct` with a concrete tensor of zeros.",
                    "input": {
                        "shape": "cache: nnx.Cache",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the `cache.value` is an instance of `jax.ShapeDtypeStruct`.",
                        "If it is, create a new `nnx.Cache` with a zero tensor of the correct shape and dtype.",
                        "Return the original or the newly created cache."
                    ],
                    "output": {
                        "shape": "nnx.Cache"
                    },
                    "dependencies": [
                        "jax.ShapeDtypeStruct",
                        "flax.nnx.Cache",
                        "jax.numpy.zeros"
                    ],
                    "notes": [
                        "This is a temporary helper function to work around lazy state creation by the `nnx.bridge.to_linen`."
                    ]
                },
                "get_kv_pages": {
                    "purpose": "Retrieves the key and value page caches, ensuring they are materialized and have the correct sharding constraints applied.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `_maybe_materialize_cache` for both `self.key_pages` and `self.value_pages`.",
                        "Apply logical sharding constraints to the cache tensors using `nn.with_logical_constraint`.",
                        "Return the prepared key and value page caches."
                    ],
                    "output": {
                        "shape": "A tuple of two `nnx.Cache` objects."
                    },
                    "dependencies": [
                        "self._maybe_materialize_cache",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": []
                },
                "pad_qkv": {
                    "purpose": "Pads the last dimension of input tensors (Q, K, V) to match the `kv_head_dim_size` required by the attention kernel.",
                    "input": {
                        "shape": "*qkv: A tuple of tensors, each with shape [batch, seq, heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Iterate through each input tensor.",
                        "If the size of the last dimension does not match `self.kv_head_dim_size`, pad it with zeros."
                    ],
                    "output": {
                        "shape": "A tuple of padded tensors, each with shape [batch, seq, heads, self.kv_head_dim_size]"
                    },
                    "dependencies": [
                        "jax.numpy.pad"
                    ],
                    "notes": []
                },
                "paged_dot_product_attention_with_max_and_sum": {
                    "purpose": "Computes standard dot-product attention with causal masking, returning the attention output along with the max and sum of exponentials for numerical stability.",
                    "input": {
                        "shape": "query: [b, t, n, d], key: [b, s, n_kv, d], value: [b, s, n_kv, d]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Reshape query for grouped query attention.",
                        "Compute attention weights using `jnp.einsum`.",
                        "Apply a causal mask.",
                        "Compute max, exponentiate, and sum for a stable softmax.",
                        "Compute the final attention output via `jnp.einsum`.",
                        "Reshape and return the output, max, and sum."
                    ],
                    "output": {
                        "shape": "A tuple (attn: [b, t, n, d], local_max: [b, t, n, 1], local_sums: [b, t, n, 1])"
                    },
                    "dependencies": [
                        "jax.numpy.einsum",
                        "jax.numpy.triu",
                        "jax.numpy.where"
                    ],
                    "notes": [
                        "This method is used as a fallback for prefill when not using the custom v2 kernel."
                    ]
                },
                "paged_attention_v2_prefill": {
                    "purpose": "Applies the v2 ragged paged attention kernel specifically for prefill sequences.",
                    "input": {
                        "shape": "query: [1, max_num_tokens, num_kv_heads, head_dim], key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, page_state: PageState",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Assert that the batch size of the query is 1.",
                        "Permute the dimensions of the KV cache pages to match the kernel's expected layout.",
                        "Prepare arguments for the kernel, such as `cu_q_lens` and `num_seqs`, from the `page_state`.",
                        "Call `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Expand the output's batch dimension to return a shape of [1, ...]."
                    ],
                    "output": {
                        "shape": "[1, seq_len, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "MaxText.inference.paged_attention_kernel_v2.ragged_paged_attention"
                    ],
                    "notes": [
                        "This method assumes the batch size is always 1 during prefill."
                    ]
                },
                "paged_attention_v2_decode": {
                    "purpose": "Applies the v2 ragged paged attention kernel specifically for decode (autoregressive) steps.",
                    "input": {
                        "shape": "query: [batch_size, 1, n_kv_head, head_dim], key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, page_state: PageState",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Squeeze the sequence length dimension from the query tensor.",
                        "Permute the dimensions of the KV cache pages.",
                        "Prepare arguments for the kernel from `page_state` and batch size.",
                        "Call `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Expand the sequence length dimension of the output to return a shape of [batch_size, 1, ...]."
                    ],
                    "output": {
                        "shape": "[batch_size, 1, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "MaxText.inference.paged_attention_kernel_v2.ragged_paged_attention"
                    ],
                    "notes": []
                },
                "paged_attention_v1_decode": {
                    "purpose": "Applies the v1 paged attention kernel for decode steps, using `jax.shard_map` for SPMD parallelism.",
                    "input": {
                        "shape": "query: [batch_size, 1, n_kv_head, head_dim], key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, page_state: PageState",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Define input and output partition specifications for `jax.shard_map`.",
                        "Define a wrapper function that calls `paged_attention_kernel.paged_attention`.",
                        "Invoke the wrapper function via `jax.shard_map` with the provided inputs."
                    ],
                    "output": {
                        "shape": "[batch_size, 1, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "jax.shard_map",
                        "jax.experimental.pallas.ops.tpu.paged_attention.paged_attention_kernel.paged_attention"
                    ],
                    "notes": [
                        "The docstring mentions this kernel has a performance advantage over v2 for decode-only tasks."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the paged attention mechanism by routing inputs to the correct implementation based on the model's operational mode.",
                    "input": {
                        "shape": "query: Array, key: Array, value: Array, model_mode: str, page_state: PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `get_kv_pages` to retrieve KV caches.",
                        "Call `pad_qkv` to pad inputs.",
                        "If `model_mode` is 'prefill', call `update_prefill_step_pages` and then an appropriate prefill attention function.",
                        "If `model_mode` is 'autoregressive', call `update_decode_step_pages` and then an appropriate decode attention function.",
                        "Return the attention output and optional max/sum values."
                    ],
                    "output": {
                        "shape": "A tuple (output, exponentials_max, exponentials_sum)."
                    },
                    "dependencies": [
                        "self.get_kv_pages",
                        "self.pad_qkv",
                        "self.update_prefill_step_pages",
                        "self.update_decode_step_pages",
                        "self.paged_attention_v2_prefill",
                        "self.paged_dot_product_attention_with_max_and_sum",
                        "self.paged_attention_v2_decode",
                        "self.paged_attention_v1_decode"
                    ],
                    "notes": [
                        "The `exponentials_max` and `exponentials_sum` are `None` for autoregressive mode."
                    ]
                },
                "update_prefill_step_pages": {
                    "purpose": "Updates the KV page caches with new key/value data for an entire sequence during a prefill step.",
                    "input": {
                        "shape": "key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, key: [batch, seq_len, ...], value: [batch, seq_len, ...], page_state: PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Perform assertions on input shapes.",
                        "Squeeze the batch dimension from key and value if it is 1.",
                        "Transpose and reshape the key and value tensors to match the paged layout [num_heads, num_pages, tokens_per_page, head_dim].",
                        "Overwrite the `.value` of the `key_pages_cache` and `value_pages_cache` with the reshaped tensors."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "jax.numpy.squeeze",
                        "jax.numpy.transpose",
                        "jax.numpy.reshape",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "This method modifies the cache objects in-place."
                    ]
                },
                "update_decode_step_pages": {
                    "purpose": "Updates the KV page caches with a single new token's key/value data for each sequence in the batch during a decode step.",
                    "input": {
                        "shape": "key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, key: [batch, 1, ...], value: [batch, 1, ...], page_state: PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape and transpose the new key and value tensors.",
                        "Get the target page and position indices for each sequence from `page_state`.",
                        "Use JAX's `.at[...].set(...)` syntax to perform a scattered update of the new key/value data into the cache tensors.",
                        "Update the `.value` of the cache objects with the updated tensors."
                    ],
                    "output": {
                        "shape": "A tuple of the updated (key_pages_cache, value_pages_cache)."
                    },
                    "dependencies": [
                        "jax.numpy.transpose",
                        "jax.numpy.tile"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#MultiPageAsyncCopyDescriptor",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "class MultiPageAsyncCopyDescriptor:\n  \"\"\"Descriptor for async copy of multiple K/V pages from HBM.\"\"\"\n\n  def __init__(\n      self,\n      pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads_per_blk, head_dim]\n      vmem_buf,  # [num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n      sem,\n      page_indices_ref,  # i32[max_num_seqs, pages_per_seq]\n      offset,  # [seq_idx, kv_pages_start]\n  ):\n    self._vmem_buf = vmem_buf\n    seq_id, kv_pages_start = offset\n    self._async_copies = [\n        pltpu.make_async_copy(\n            pages_hbm_ref.at[page_indices_ref[seq_id, kv_pages_start + i]],\n            vmem_buf.at[i],\n            sem,\n        )\n        for i in range(vmem_buf.shape[0])\n    ]\n\n  def start(self):\n    \"\"\"Starts the async copies.\"\"\"\n    for async_copy in self._async_copies:\n      async_copy.start()\n\n  def wait(self):\n    for async_copy in self._async_copies:\n      async_copy.wait()\n    return self._vmem_buf",
        "analysis": {
            "functionality": "The `MultiPageAsyncCopyDescriptor` class manages a set of asynchronous memory copy operations to transfer multiple key/value (K/V) pages from High Bandwidth Memory (HBM) to on-chip VMEM for a specific sequence. It encapsulates the setup, initiation, and completion of these parallel data transfers.",
            "usage": "Instantiate the class with references to the source HBM tensor (`pages_hbm_ref`), destination VMEM buffer (`vmem_buf`), a semaphore (`sem`), page indices (`page_indices_ref`), and a specific sequence offset (`offset`). Call the `start()` method to begin the non-blocking data transfers. Call the `wait()` method to block until all transfers are complete and retrieve the populated VMEM buffer."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ref_ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ref_ragged_paged_attention(\n    queries: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1],\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n):\n  \"\"\"Ref ragged paged attention.\"\"\"\n  _, _, num_kv_heads, head_dim = k_pages.shape\n  num_q_heads = queries.shape[1]\n  assert num_q_heads % num_kv_heads == 0\n  num_query_per_kv = num_q_heads // num_kv_heads\n  outputs = []\n  for i in range(num_seqs[0]):\n    q_start = cu_q_lens[i]\n    q_end = cu_q_lens[i + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens[i]\n    indices = page_indices[i]\n    q = queries[q_start:q_end]\n    k = k_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    v = v_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    k = jnp.repeat(k, num_query_per_kv, axis=1)\n    v = jnp.repeat(v, num_query_per_kv, axis=1)\n    attn = jnp.einsum(\"qhd,khd->hqk\", q, k, preferred_element_type=jnp.float32)\n    attn *= sm_scale\n    q_span = (kv_len - q_len) + jax.lax.broadcasted_iota(jnp.int32, attn.shape, 1)\n    kv_span = jax.lax.broadcasted_iota(jnp.int32, attn.shape, 2)\n    attn += jnp.where(q_span < kv_span, mask_value, 0.0)\n    attn = jax.nn.softmax(attn, axis=-1).astype(v.dtype)\n    out = jnp.einsum(\"hqk,khd->qhd\", attn, v).astype(queries.dtype)\n    outputs.append(out)\n\n  return jnp.concatenate(outputs, axis=0)",
        "analysis": {
            "module_type": "ragged_paged_attention_reference",
            "purpose": "Performs a reference implementation of ragged paged attention, calculating attention for a batch of sequences with varying lengths where keys and values are stored in a paged format.",
            "input": {
                "shape": "queries: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "queries/k_pages/v_pages: float (e.g., bfloat16, float32), kv_lens/page_indices/cu_q_lens/num_seqs: int32"
            },
            "processing_steps": [
                "Iterate through each sequence specified by `num_seqs`.",
                "For each sequence, extract its queries using `cu_q_lens`.",
                "Gather the corresponding key and value pages using `page_indices`.",
                "Reshape and truncate the gathered key/value pages to the correct sequence length `kv_len`.",
                "Repeat key and value heads to match the number of query heads (for Grouped Query Attention).",
                "Compute attention scores via `jnp.einsum` between queries and keys.",
                "Apply the `sm_scale` factor.",
                "Construct and apply a causal attention mask using `jax.lax.broadcasted_iota` and `jnp.where`.",
                "Apply softmax to the masked attention scores.",
                "Compute the output by taking a weighted sum of values using `jnp.einsum`.",
                "Append the output for the current sequence to a list.",
                "Concatenate the outputs of all sequences into a single tensor."
            ],
            "output": {
                "shape": "[max_num_batched_tokens, num_q_heads, head_dim]"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "jax.lax",
                "jax.nn"
            ],
            "parameters": {
                "sm_scale": "A float scaling factor applied to the attention scores before the softmax operation.",
                "mask_value": "A large negative float value used to mask out positions in the attention matrix for causal attention."
            },
            "notes": [
                "This is a reference implementation that processes sequences one by one in a Python loop, which is generally less efficient than a vectorized or kernel-based approach.",
                "It supports Grouped Query Attention (GQA) where the number of query heads is a multiple of the number of key/value heads.",
                "The implementation implicitly applies a causal mask, suitable for autoregressive decoding."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#validate_inputs_on_runtime",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def validate_inputs_on_runtime(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"validate inputs on runtime\"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  max_num_batched_tokens = q.shape[0]\n  page_size = k_pages.shape[1]\n  max_num_seqs, pages_per_seq = page_indices.shape\n  if num_seqs[0] > max_num_seqs:\n    raise ValueError(f\"{num_seqs[0]=} must be less or equal to {max_num_seqs=}\")\n  max_kv_len = jnp.max(kv_lens)\n  min_pages_per_seq = ceil_div(max_kv_len, page_size)\n  if pages_per_seq < min_pages_per_seq:\n    raise ValueError(\n        f\"{pages_per_seq=} must be greater or equal to\" f\" {min_pages_per_seq=} given {max_kv_len=} and {page_size=}.\"\n    )\n  if cu_q_lens[num_seqs[0]] > max_num_batched_tokens:\n    raise ValueError(f\"Total q tokens {cu_q_lens[num_seqs[0]]} must be less or equal to\" f\" {max_num_batched_tokens=}.\")\n  for i in range(num_seqs[0]):\n    q_len = cu_q_lens[i + 1] - cu_q_lens[i]\n    kv_len = kv_lens[i]\n    if q_len > kv_len:\n      raise ValueError(f\"{q_len=} must be less or equal to {kv_len=} at sequence {i}.\")",
        "analysis": {
            "module_type": "paged_attention_input_validator",
            "purpose": "Performs runtime validation of input tensors for the ragged paged attention kernel, raising ValueError on inconsistencies related to dynamic sequence lengths and buffer sizes.",
            "input": {
                "shape": "q: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "jax.Array (float types for q, k_pages, v_pages; int32 for kv_lens, page_indices, cu_q_lens, num_seqs)"
            },
            "processing_steps": [
                "Call `check_inputs_shapes` to validate static shapes and dtypes of all inputs.",
                "Verify that the dynamic number of sequences (`num_seqs`) does not exceed the maximum capacity (`max_num_seqs`).",
                "Find the maximum key-value length (`max_kv_len`) across all sequences.",
                "Calculate the minimum number of pages required for the longest sequence and check if enough pages are allocated.",
                "Ensure the total number of query tokens (derived from `cu_q_lens`) does not exceed the allocated buffer size (`max_num_batched_tokens`).",
                "Iterate through each active sequence to validate that its query length is not greater than its key-value length."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "check_inputs_shapes",
                "jnp.max",
                "ceil_div"
            ],
            "parameters": {},
            "notes": [
                "This function does not return a value. Its purpose is to raise a `ValueError` if any check fails.",
                "It complements `check_inputs_shapes` by checking dynamic values within the tensors (like actual sequence lengths), not just their static shapes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#check_inputs_shapes",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def check_inputs_shapes(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"check shapes of inputs\"\"\"\n  _, num_q_heads, head_dim = q.shape\n  _, _, num_kv_heads, head_dim_k = k_pages.shape\n  max_num_seqs, _ = page_indices.shape\n  if num_seqs.shape != (1,):\n    raise ValueError(f\"{num_seqs.shape=} must be (1,)\")\n  if k_pages.shape != v_pages.shape:\n    raise ValueError(f\"{k_pages.shape=} and {v_pages.shape=} must have the same shape.\")\n  if head_dim_k != head_dim:\n    raise ValueError(f\"Q head_dim {head_dim} must be the same as that of K/V {head_dim_k}.\")\n  if kv_lens.shape != (max_num_seqs,):\n    raise ValueError(\n        f\"Expected {kv_lens.shape=} to be ({max_num_seqs},) where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if cu_q_lens.shape != (max_num_seqs + 1,):\n    raise ValueError(\n        f\"Expected {cu_q_lens.shape=} to be ({max_num_seqs + 1},)  where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if kv_lens.dtype != jnp.int32 or page_indices.dtype != jnp.int32 or cu_q_lens.dtype != jnp.int32:\n    raise ValueError(\n        \"The dtype of `kv_lens`, `page_indices`, and `cu_q_lens` must be\"\n        f\" int32. Got {kv_lens.dtype=}, {page_indices.dtype=},\"\n        f\" {cu_q_lens.dtype=}.\"\n    )\n  if num_q_heads % num_kv_heads != 0:\n    raise ValueError(f\"{num_q_heads=} must be divisible by {num_kv_heads=}\")",
        "analysis": {
            "functionality": "This function validates the shapes, dtypes, and dimensional consistency of input tensors for a ragged paged attention operation. It raises a `ValueError` if any of the checks fail.",
            "usage": "Call this function with the query, key/value pages, and metadata tensors to ensure they are correctly formatted before being used in an attention computation. It takes seven `jax.Array` arguments: `q`, `k_pages`, `v_pages`, `kv_lens`, `page_indices`, `cu_q_lens`, and `num_seqs`. The function does not return any value; it either completes successfully or raises an error."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention_kernel",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention_kernel(\n    # Prefetch\n    kv_lens_ref,  # [max_num_seqs]\n    page_indices_ref,  # [max_num_seqs, pages_per_seq]\n    cu_q_lens_ref,  # [max_num_seqs + 1]\n    seq_buf_idx_ref,\n    # TODO(jevinjiang): if OOM in SMEM, consider pack to other scalar refs.\n    num_seqs_ref,\n    # Input\n    q_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    k_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    # Output\n    o_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    # Scratch\n    k_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    v_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    sems,  # [2, 2]\n    l_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    m_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    *,\n    sm_scale: float,\n    mask_value: float,\n):\n  \"\"\"ragged paged-attention kernel\"\"\"\n  num_q_per_blk, num_q_heads_per_blk, head_dim = q_ref.shape\n  num_seqs = num_seqs_ref[0]\n  _, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, _ = k_bufs.shape\n  num_kv_per_blk = num_kv_pages_per_blk * page_size\n  num_q_heads_per_kv_head = num_q_heads_per_blk // num_kv_heads_per_blk\n  heads_blk_idx, q_blk_idx = (\n      pl.program_id(0),\n      pl.program_id(1),\n  )\n  num_heads_blks = pl.num_programs(0)\n  init_seq_idx = seq_buf_idx_ref[0]\n  init_buf_idx = seq_buf_idx_ref[1]\n  q_len_start = q_blk_idx * num_q_per_blk\n  q_len_end = q_len_start + num_q_per_blk\n\n  def create_kv_async_copy_descriptors(heads_blk_idx, seq_idx, kv_blk_idx, buf_idx):\n    offset = (seq_idx, kv_blk_idx * num_kv_pages_per_blk)\n    heads_start = heads_blk_idx * num_kv_heads_per_blk\n    async_copy_k = MultiPageAsyncCopyDescriptor(\n        k_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        k_bufs.at[buf_idx],\n        sems.at[buf_idx, 0],\n        page_indices_ref,\n        offset,\n    )\n    async_copy_v = MultiPageAsyncCopyDescriptor(\n        v_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        v_bufs.at[buf_idx],\n        sems.at[buf_idx, 1],\n        page_indices_ref,\n        offset,\n    )\n    return async_copy_k, async_copy_v\n\n  # TODO(jevinjiang): Add these to Mosaic:\n  # 1. Support arbitrary strided load/store for any dtype.\n  # 2. Support arbitrary strided load/store for any last dimension.\n  def strided_load_kv(ref, start, step):\n    if ref.dtype == jnp.float32:\n      return ref[start::step, :]\n    packing = get_dtype_packing(ref.dtype)\n    assert ref.dtype == jnp.bfloat16\n    assert step % packing == 0\n    b_start = start // packing\n    b_offset = start % packing\n    b_step = step // packing\n    b_ref = ref.bitcast(jnp.int32)\n    b = b_ref[b_start::b_step, :]\n    bw = 32 // packing\n    b = jnp.right_shift(b, bw * b_offset)\n    b = jnp.left_shift(b, bw * (packing - 1))\n    return pltpu.bitcast(b, jnp.float32).astype(jnp.bfloat16)\n\n  def fold_on_2nd_minor(vec):\n    assert vec.dtype in (jnp.bfloat16, jnp.float32)\n    assert len(vec.shape) >= 2\n    last_dim = vec.shape[-1]\n    packing = get_dtype_packing(vec.dtype)\n    if vec.shape[-2] % packing != 0:\n      vec = vec.astype(jnp.float32)\n    return vec.reshape(-1, last_dim)\n\n  @pl.when(heads_blk_idx + q_blk_idx == 0)\n  def prefetch_first_kv_blk():\n    async_copy_k, async_copy_v = create_kv_async_copy_descriptors(heads_blk_idx, init_seq_idx, 0, init_buf_idx)\n    async_copy_k.start()\n    async_copy_v.start()\n\n  def is_cur_q_blk_needed(q_states):\n    done, cur_seq_idx, _ = q_states\n    return jnp.logical_and(done == 0, cur_seq_idx < num_seqs)\n\n  def compute_with_cur_q_blk(q_states):\n    done, cur_seq_idx, cur_buf_idx = q_states\n    q_start = cu_q_lens_ref[cur_seq_idx]\n    q_end = cu_q_lens_ref[cur_seq_idx + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens_ref[cur_seq_idx]\n\n    def get_next_prefetch_ids(heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx):\n      next_kv_blk_idx = kv_blk_idx + 1\n      is_last_kv_blk = next_kv_blk_idx * num_kv_per_blk >= kv_len\n      next_kv_blk_idx = lax.select(\n          is_last_kv_blk,\n          0,\n          next_kv_blk_idx,\n      )\n      is_cur_seq_end_in_cur_q_blk = q_end <= q_len_end\n      next_seq_idx = lax.select(\n          is_last_kv_blk,\n          lax.select(is_cur_seq_end_in_cur_q_blk, cur_seq_idx + 1, cur_seq_idx),\n          cur_seq_idx,\n      )\n      is_last_seq = next_seq_idx == num_seqs\n      next_seq_idx = lax.select(\n          is_last_seq,\n          0,\n          next_seq_idx,\n      )\n      next_heads_blk_idx = lax.select(\n          is_last_seq,\n          heads_blk_idx + 1,\n          heads_blk_idx,\n      )\n      next_buf_idx = lax.select(cur_buf_idx == 0, 1, 0)\n      return next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n\n    def flash_attention(\n        q,  # [num_q_per_blk * num_q_heads_per_kv_head, head_dim]\n        k,  # [num_kv_per_blk, head_dim]\n        v,  # [num_kv_per_blk, head_dim]\n        head_l_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_m_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_o_ref,  # [num_q_per_blk, num_q_heads_per_kv_head, head_dim]\n        *,\n        kv_blk_idx,\n    ):\n      assert q.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          head_dim,\n      )\n      assert k.shape == (\n          num_kv_per_blk,\n          head_dim,\n      ), f\"{k.shape=}, {(num_kv_per_blk, head_dim)=} {k.dtype=}\"\n      assert v.shape == (num_kv_per_blk, head_dim)\n      assert head_m_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_l_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_o_ref.shape == (\n          num_q_per_blk,\n          num_q_heads_per_kv_head,\n          head_dim,\n      )\n      kv_len_start = kv_blk_idx * num_kv_per_blk\n\n      def masked_store(ref, val, start, end, group=1):\n        iota = lax.broadcasted_iota(jnp.int32, ref.shape, 0) // group\n        mask = jnp.logical_and(iota >= start, iota < end)\n        pl.store(ref, tuple(slice(None) for _ in ref.shape), val, mask=mask)\n\n      qk = jnp.einsum(\"nd,md->nm\", q, k, preferred_element_type=jnp.float32) * sm_scale\n      store_start = jnp.maximum(q_start - q_len_start, 0)\n      store_end = jnp.minimum(q_end - q_len_start, num_q_per_blk)\n\n      @pl.when(kv_blk_idx == 0)\n      def init_scratch_ref():\n        masked_store(\n            head_m_ref,\n            jnp.full_like(head_m_ref, -jnp.inf),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_l_ref,\n            jnp.zeros_like(head_l_ref),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_o_ref,\n            jnp.zeros_like(head_o_ref),\n            store_start,\n            store_end,\n        )\n\n      row_ids = (\n          (kv_len - q_len)\n          + q_len_start\n          - q_start\n          + jax.lax.broadcasted_iota(\n              jnp.int32,\n              (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n              0,\n          )\n          // num_q_heads_per_kv_head\n      )\n      col_ids = kv_len_start + jax.lax.broadcasted_iota(\n          jnp.int32,\n          (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n          1,\n      )\n      causal_mask = row_ids < col_ids\n      qk += jnp.where(causal_mask, mask_value, 0.0)\n      m_curr = jnp.max(qk, axis=1, keepdims=True)\n      s_curr = jnp.exp(qk - m_curr)\n      qkv = jnp.dot(s_curr, v, preferred_element_type=jnp.float32)\n      lm_store_shape = head_m_ref.shape\n      m_curr = jnp.broadcast_to(m_curr, lm_store_shape)\n      l_curr = jnp.broadcast_to(s_curr.sum(axis=1, keepdims=True), lm_store_shape)\n      m_prev = head_m_ref[...]\n      l_prev = head_l_ref[...]\n      m_next = jnp.maximum(m_prev, m_curr)\n      masked_store(head_m_ref, m_next, store_start, store_end, num_q_heads_per_kv_head)\n      alpha = jnp.exp(m_prev - m_next)\n      beta = jnp.exp(m_curr - m_next)\n      l_alpha = alpha * l_prev\n      l_next = l_alpha + beta * l_curr\n      l_next_safe = jnp.where(l_next == 0.0, 1.0, l_next)\n      masked_store(\n          head_l_ref,\n          l_next_safe,\n          store_start,\n          store_end,\n          num_q_heads_per_kv_head,\n      )\n\n      def broadcast_to_shape(arr, shape):\n        if arr.shape == shape:\n          return arr\n        assert len(arr.shape) == len(shape)\n        assert arr.shape[0] == shape[0]\n        assert shape[1] % arr.shape[1] == 0\n        # no-op concatenation.\n        return jnp.concatenate([arr for _ in range(shape[1] // arr.shape[1])], axis=1)\n\n      o_curr = head_o_ref[...].reshape(-1, head_dim)\n      l_alpha = broadcast_to_shape(l_alpha, qkv.shape)\n      beta = broadcast_to_shape(beta, qkv.shape)\n      l_next_safe = broadcast_to_shape(l_next_safe, qkv.shape)\n      out = lax.div(\n          l_alpha * o_curr + beta * qkv,\n          l_next_safe,\n      ).astype(head_o_ref.dtype)\n      masked_store(\n          head_o_ref,\n          out.reshape(head_o_ref.shape),\n          store_start,\n          store_end,\n      )\n\n    def is_valid_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, _ = kv_states\n      return kv_blk_idx * num_kv_per_blk < kv_len\n\n    def compute_with_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, cur_buf_idx = kv_states\n      next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx = get_next_prefetch_ids(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n\n      @pl.when(next_heads_blk_idx < num_heads_blks)\n      def prefetch_next_kv_blk():\n        # TODO(jevinjiang): reuse the same buffer if it is already prefetched!\n        # TODO(jevinjiang): only fetch effective dynamic size to hold kv_len and\n        # DMA to fixed size buffer!\n        next_async_copy_k, next_async_copy_v = create_kv_async_copy_descriptors(\n            next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n        )\n        next_async_copy_k.start()\n        next_async_copy_v.start()\n\n      cur_async_copy_k, cur_async_copy_v = create_kv_async_copy_descriptors(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n      kv_to_load_shape = (\n          num_kv_pages_per_blk * page_size * num_kv_heads_per_blk,\n          head_dim,\n      )\n      k_ref = cur_async_copy_k.wait().reshape(kv_to_load_shape)\n      v_ref = cur_async_copy_v.wait().reshape(kv_to_load_shape)\n      for kv_head_idx in range(num_kv_heads_per_blk):\n        q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n        # TODO(jevinjiang): extra handling for packed type that can start at\n        # unaligned position!\n        q = fold_on_2nd_minor(q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :])\n        k = strided_load_kv(k_ref, kv_head_idx, num_kv_heads_per_blk)\n        v = strided_load_kv(v_ref, kv_head_idx, num_kv_heads_per_blk)\n        flash_attention(\n            q,\n            k,\n            v,\n            l_ref.at[kv_head_idx],\n            m_ref.at[kv_head_idx],\n            o_ref.at[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :],\n            kv_blk_idx=kv_blk_idx,\n        )\n      return kv_blk_idx + 1, next_buf_idx\n\n    _, next_buf_idx = lax.while_loop(\n        is_valid_kv_blk_in_cur_seq,\n        compute_with_kv_blk_in_cur_seq,\n        (0, cur_buf_idx),  # (kv_blk_idx, buf_idx)\n    )\n    next_seq_idx = lax.select(q_end <= q_len_end, cur_seq_idx + 1, cur_seq_idx)\n    done = lax.select(q_end < q_len_end, done, 1)\n    return done, next_seq_idx, next_buf_idx\n\n  _, seq_idx, buf_idx = lax.while_loop(\n      is_cur_q_blk_needed,\n      compute_with_cur_q_blk,\n      (0, init_seq_idx, init_buf_idx),  # (done, seq_idx, buf_idx)\n  )\n  # Reset seq_idx for next kv_heads_blk if run out of seqs!\n  seq_buf_idx_ref[0] = lax.select(seq_idx < num_seqs, seq_idx, 0)\n  seq_buf_idx_ref[1] = buf_idx",
        "analysis": {
            "functionality": "This code block defines a Pallas kernel for computing ragged paged attention. It processes blocks of queries against key-value pages fetched asynchronously from HBM, using a FlashAttention-style algorithm with online softmax to handle variable-length sequences efficiently on TPUs. The kernel is designed for mixed prefill and decode workloads.",
            "usage": "This function is not called directly but is launched by `jax.experimental.pallas.pallas_call`. It takes references to input tensors (queries, K/V pages), metadata (sequence lengths, page indices), and scratch buffers. It computes the attention output for a specific block of queries and heads and writes the result to the output reference `o_ref`. Key keyword arguments `sm_scale` and `mask_value` control the attention calculation."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ceil_div",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ceil_div(a, b):\n  assert b != 0\n  return (a + b - 1) // b",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Computes the ceiling of the integer division of two numbers (a / b).",
            "input": {
                "shape": "N/A",
                "dtype": "integer"
            },
            "processing_steps": [
                "Assert that the divisor `b` is not zero.",
                "Calculate `(a + b - 1)` and perform integer division of the result by `b`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This function implements ceiling division using only integer arithmetic, avoiding floating-point operations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_dtype_packing",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_dtype_packing(dtype):\n  if dtype == jnp.float32:\n    return 1\n  if dtype == jnp.bfloat16:\n    return 2\n  if dtype == jnp.int8:\n    return 4\n  if dtype == jnp.int4:\n    return 8\n  raise ValueError(f\"Not implemented: unsupported {dtype=}\")",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Returns an integer packing factor for a given JAX numpy data type, typically representing how many elements of that type fit into a 32-bit word.",
            "input": {
                "shape": "N/A",
                "dtype": "jax.numpy.dtype"
            },
            "processing_steps": [
                "Check if the input dtype is jnp.float32 and return 1.",
                "Check if the input dtype is jnp.bfloat16 and return 2.",
                "Check if the input dtype is jnp.int8 and return 4.",
                "Check if the input dtype is jnp.int4 and return 8.",
                "If the dtype is not supported, raise a ValueError."
            ],
            "output": {
                "shape": "Scalar integer."
            },
            "dependencies": [
                "jax.numpy as jnp"
            ],
            "parameters": {
                "dtype": "The JAX numpy data type for which to determine the packing factor."
            },
            "notes": [
                "This function is used to handle different data types in memory operations, particularly for packed data on TPUs.",
                "The packing factor is inversely proportional to the size of the data type (e.g., 32-bit / 8-bit = 4 for int8)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_min_heads_per_blk",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_min_heads_per_blk(num_q_heads, num_kv_heads, q_dtype, kv_dtype):\n  \"\"\"get min heads per block\"\"\"\n  q_packing = get_dtype_packing(q_dtype)\n  kv_packing = get_dtype_packing(kv_dtype)\n\n  def can_be_xla_fully_tiled(x, packing):\n    if x % packing != 0:\n      return False\n    x //= packing\n    return x in (1, 2, 4, 8) or x % 8 == 0\n\n  # TODO(jevinjiang): support unaligned number of heads!\n  if not can_be_xla_fully_tiled(num_kv_heads, kv_packing):\n    raise ValueError(f\"Not implemented: {num_kv_heads=} can not be XLA fully tiled.\")\n  assert num_q_heads % num_kv_heads == 0\n  ratio = num_q_heads // num_kv_heads\n  # TODO(jevinjiang): we can choose smaller tiling for packed type if large\n  # second minor tiling is not on.\n  max_kv_tiling = 8 * kv_packing\n  min_kv_heads = max_kv_tiling if num_kv_heads % max_kv_tiling == 0 else num_kv_heads\n  min_q_heads = min_kv_heads * ratio\n  if can_be_xla_fully_tiled(min_q_heads, q_packing):\n    return min_q_heads, min_kv_heads\n  return num_q_heads, num_kv_heads",
        "analysis": {
            "module_type": "attention_head_tiling_calculator",
            "purpose": "Calculates the minimum number of query and key/value heads per processing block that satisfies XLA tiling constraints for TPU execution.",
            "input": {
                "shape": "N/A",
                "dtype": "int, jax.numpy.dtype"
            },
            "processing_steps": [
                "Retrieve data type packing factors for query and KV heads using `get_dtype_packing`.",
                "Define and use an inner function `can_be_xla_fully_tiled` to check if the number of heads is compatible with hardware tiling requirements after accounting for packing.",
                "Validate that the total number of KV heads is XLA-tileable; raise a ValueError otherwise.",
                "Calculate the ratio of query heads to KV heads.",
                "Determine a maximum tiling size for KV heads (`max_kv_tiling`) based on its data type packing.",
                "Calculate a potential `min_kv_heads` by using `max_kv_tiling` if `num_kv_heads` is divisible by it, otherwise using the original `num_kv_heads`.",
                "Calculate the corresponding `min_q_heads` based on the `min_kv_heads` and the head ratio.",
                "If the calculated `min_q_heads` is also XLA-tileable, return the new minimum head counts.",
                "Otherwise, fall back and return the original total number of query and KV heads."
            ],
            "output": {
                "shape": "A tuple of two integers: (min_q_heads, min_kv_heads)."
            },
            "dependencies": [
                "get_dtype_packing"
            ],
            "parameters": {
                "num_q_heads": "The total number of query heads.",
                "num_kv_heads": "The total number of key/value heads.",
                "q_dtype": "The data type of the query tensors.",
                "kv_dtype": "The data type of the key/value tensors."
            },
            "notes": [
                "This function is a heuristic to determine an efficient block size for distributing attention head computations across TPU cores.",
                "The logic is specific to XLA's tiling preferences, where dimensions that are powers of 2 or multiples of 8 (after accounting for data type packing) are more efficient.",
                "The function contains TODOs indicating that support for unaligned head counts and more flexible tiling strategies are planned improvements.",
                "It prioritizes finding a valid tiling for KV heads first, then checks if the corresponding Q head count is also valid before returning the smaller tiling."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    # TODO(jevinjiang): create a write_to_kv_cache kernel!\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1]\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n    num_kv_pages_per_block: int = 16,\n    num_queries_per_block: int = 128,\n    vmem_limit_bytes: int | None = None,\n):\n  \"\"\"Ragged paged attention that supports mixed prefill and decode.\n\n  Args:\n    q: concatenated all sequences' queries.\n    k_pages: paged K cache. Normally in HBM.\n    v_pages: paged V cache. Normally in HBM.\n    kv_lens: padded kv lengths. Only the first num_seqs values are valid.\n    page_indices: the first index indicates which page to use in the kv cache\n      for each sequence. Only the first num_seqs values are valid.\n    cu_q_lens: the cumulative sum of the effective query lengths. Similar to\n      kv_lens, only the first num_seqs+1 values are valid.\n    num_seqs: the dynamic number of sequences.\n    sm_scale: the softmax scale which will be applied to the Q@K^T.\n    mask_value: mask value for causal mask.\n    num_kv_pages_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    num_queries_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    vmem_limit_bytes: the vmem limit for the pallas kernel.\n\n  Returns:\n    The output of the attention.\n  \"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  _, num_q_heads, head_dim = q.shape\n  _, page_size, num_kv_heads, _ = k_pages.shape\n  num_q_per_blk = num_queries_per_block\n  num_kv_pages_per_blk = num_kv_pages_per_block\n  num_q_heads_per_kv_head = num_q_heads // num_kv_heads\n  num_q_blks = ceil_div(cu_q_lens[num_seqs[0]], num_q_per_blk)\n  num_q_heads_per_blk, num_kv_heads_per_blk = get_min_heads_per_blk(num_q_heads, num_kv_heads, q.dtype, k_pages.dtype)\n  assert num_q_heads_per_blk % num_q_heads_per_kv_head == 0\n  num_heads_blks = num_q_heads // num_q_heads_per_blk\n  grid = (num_heads_blks, num_q_blks)\n\n  def q_index_map(heads_blk_idx, q_blk_idx, *_):\n    return (q_blk_idx, heads_blk_idx, 0)\n\n  q_block_spec = pl.BlockSpec(\n      (num_q_per_blk, num_q_heads_per_blk, head_dim),\n      q_index_map,\n  )\n  in_specs = [\n      q_block_spec,\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n  ]\n  out_specs = q_block_spec\n  lm_scratch = pltpu.VMEM(\n      # TODO(jevinjiang): use 128 instead of 1 is due to Mosaic does not support\n      # unaligned slicing!\n      (num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128),\n      jnp.float32,\n  )\n  double_buf_scratch = pltpu.VMEM(\n      (\n          2,  # For double buffering during DMA copies.\n          num_kv_pages_per_blk,\n          page_size,\n          num_kv_heads_per_blk,\n          head_dim,\n      ),\n      k_pages.dtype,\n  )\n  scratch_shapes = [\n      double_buf_scratch,  # k_bufs\n      double_buf_scratch,  # v_bufs\n      pltpu.SemaphoreType.DMA((2, 2)),  # [double_buffers, k_sem/v_sem]\n      lm_scratch,  # l_ref\n      lm_scratch,  # m_ref\n  ]\n  scalar_prefetches = (\n      kv_lens,\n      page_indices,\n      cu_q_lens,\n      jnp.array((0, 0), jnp.int32),  # seq_idx, buf_idx\n      num_seqs,\n  )\n  kernel = pl.pallas_call(\n      functools.partial(\n          ragged_paged_attention_kernel,\n          sm_scale=sm_scale,\n          mask_value=mask_value,\n      ),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n          num_scalar_prefetch=len(scalar_prefetches),\n          in_specs=in_specs,\n          out_specs=out_specs,\n          grid=grid,\n          scratch_shapes=scratch_shapes,\n      ),\n      compiler_params=pltpu.CompilerParams(\n          dimension_semantics=(\n              \"arbitrary\",\n              \"arbitrary\",\n          ),\n          vmem_limit_bytes=vmem_limit_bytes,\n      ),\n      out_shape=jax.ShapeDtypeStruct(shape=q.shape, dtype=jnp.float32),\n      name=\"ragged_paged_attention_kernel\",\n  )\n  # TODO(jevinjiang): Use f32 acc scratch for output! So we only need\n  # to transfer output with desired dtype back to HBM.\n  return kernel(*scalar_prefetches, q, k_pages, v_pages).astype(q.dtype)",
        "analysis": {
            "module_type": "ragged_paged_attention",
            "purpose": "Performs ragged paged attention, supporting mixed prefill and decode workloads, by launching a custom Pallas kernel on TPU.",
            "input": {
                "shape": "q: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "q, k_pages, v_pages: float (e.g., bfloat16, float32); kv_lens, page_indices, cu_q_lens, num_seqs: int32"
            },
            "processing_steps": [
                "Validate input tensor shapes using `check_inputs_shapes`.",
                "Calculate grid dimensions and block sizes for the Pallas kernel based on input shapes and parameters.",
                "Determine the minimum number of heads per block using `get_min_heads_per_blk` for efficient tiling.",
                "Define memory layouts (`BlockSpec`), scratch memory shapes (`VMEM`), and scalar prefetches for the kernel.",
                "Create a Pallas kernel by wrapping `ragged_paged_attention_kernel` with `pl.pallas_call`, providing the grid specification and compiler parameters.",
                "Execute the Pallas kernel with the provided inputs.",
                "Cast the kernel's float32 output back to the original query data type."
            ],
            "output": {
                "shape": "[max_num_batched_tokens, num_q_heads, head_dim]"
            },
            "dependencies": [
                "jax.experimental.pallas.pallas_call",
                "jax.experimental.pallas.BlockSpec",
                "jax.experimental.pallas.tpu.VMEM",
                "jax.experimental.pallas.tpu.PrefetchScalarGridSpec",
                "ragged_paged_attention_kernel",
                "check_inputs_shapes",
                "get_min_heads_per_blk",
                "ceil_div"
            ],
            "parameters": {
                "sm_scale": "The softmax scaling factor applied to the QK^T matrix.",
                "mask_value": "The value used for masking out attention scores in the causal mask.",
                "num_kv_pages_per_block": "The number of key/value cache pages processed in a single block by the Pallas kernel.",
                "num_queries_per_block": "The number of queries processed in a single block by the Pallas kernel.",
                "vmem_limit_bytes": "The explicit memory limit for the kernel's on-chip vector memory (VMEM)."
            },
            "notes": [
                "This function is a JIT-compiled wrapper that sets up and launches a custom Pallas kernel (`ragged_paged_attention_kernel`) for efficient execution on TPUs.",
                "It handles ragged inputs (sequences of different lengths) by using cumulative query lengths (`cu_q_lens`) and paged key-value caches (`k_pages`, `v_pages`, `page_indices`).",
                "The function is decorated with `@functools.partial(jax.jit, static_argnames=[...])` to compile the kernel setup and execution for specific static parameters."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "module_type": "configuration_validator",
            "purpose": "Validates specific configuration settings for the interleaved inference script, ensuring it runs with correct parameters.",
            "input": {
                "shape": "N/A",
                "dtype": "pyconfig.Config object"
            },
            "processing_steps": [
                "Assert that `config.load_full_state_path` is an empty string, ensuring a parameter-only checkpoint is used for decoding.",
                "Assert that the number of initial prefill streams (`_INITIAL_PREFILL_STREAMS`) is greater than 0 and less than or equal to the total number of streams (`_NUM_STREAMS`)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_INITIAL_PREFILL_STREAMS (global constant)",
                "_NUM_STREAMS (global constant)"
            ],
            "parameters": {
                "config.load_full_state_path": "The path for loading a full model state. This must be empty because decoding operates on parameter-only checkpoints.",
                "_INITIAL_PREFILL_STREAMS": "The number of streams to prefill before starting generation. Must be > 0.",
                "_NUM_STREAMS": "The total number of concurrent streams. Must be >= _INITIAL_PREFILL_STREAMS."
            },
            "notes": [
                "This function does not return a value; it raises an AssertionError if any validation check fails.",
                "It is called at the start of the main execution to perform critical pre-run checks."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#main",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "module_type": "interleaved_inference_runner",
            "purpose": "To run a multi-stream, interleaved prefill and generation inference loop, demonstrating a continuous batching scenario.",
            "input": {
                "shape": "N/A",
                "dtype": "Sequence[str]"
            },
            "processing_steps": [
                "Initialize configuration, JAX environment, and the MaxEngine.",
                "Load model parameters and build the tokenizer.",
                "Tokenize the input prompt specified in the configuration.",
                "Initialize a batched decode state using `engine.init_decode_state()`.",
                "Prefill an initial subset of streams using `engine.prefill()`.",
                "Insert the state of the prefilled streams into the main decode state using `engine.insert()`.",
                "Enter a generation loop that runs for a specified number of steps.",
                "In each step, call `engine.generate()` to produce one token for all active streams.",
                "Store the generated tokens and check if any streams have reached their target length.",
                "If a stream finishes, release its resources using `engine.release_pages()`.",
                "If there are available slots in the batch and more streams to process, prefill and insert a new stream.",
                "After the loop, decode the complete token sequences for each stream.",
                "Print the final generated text for each stream and perform an assertion check on the first stream's output."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "maxengine.MaxEngine",
                "pyconfig",
                "max_utils",
                "_validate_config",
                "uuid"
            ],
            "parameters": {
                "prompt": "The initial text prompt to feed to the model for all streams.",
                "max_prefill_predict_length": "The maximum length of the initial prompt tokens.",
                "max_target_length": "The total maximum length of the generated sequence, including the prompt.",
                "per_device_batch_size": "The batch size per device, used to calculate the total available slots for streams.",
                "autoregressive_decode_assert": "An expected string that the beginning of the first stream's generated output should match for validation."
            },
            "notes": [
                "This function demonstrates a continuous batching pattern where new requests (streams) are prefilled and added to the batch while generation proceeds for existing requests.",
                "The total number of streams to process (`_NUM_STREAMS`) and the number of streams to prefill before starting the generation loop (`_INITIAL_PREFILL_STREAMS`) are hardcoded module-level constants.",
                "The function's primary output is printed to standard output; it has a `None` return type.",
                "It manages the state of multiple independent generation streams within a single batched `decode_state`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#latency_bound_comms",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def latency_bound_comms(comm: float, latency=1e-6):\n  return max(comm, latency)",
        "analysis": {
            "module_type": "communication_latency_model",
            "purpose": "Calculates communication time, ensuring it is at least a specified minimum latency value.",
            "input": {
                "shape": "N/A",
                "dtype": "float"
            },
            "processing_steps": [
                "Return the maximum value between the input communication time (`comm`) and the minimum latency (`latency`)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "comm": "The calculated communication time based on bandwidth and volume.",
                "latency": "The minimum latency overhead for a communication operation, defaults to 1e-6."
            },
            "notes": [
                "This function models the physical constraint that any communication operation has a non-zero latency, regardless of the data volume."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#calculate_matmul_resources",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def calculate_matmul_resources(\n    activations_shape: tuple[int, ...],\n    weights_shape: tuple[int, ...],\n    ici_bandwidth: float,\n    peak_flops: float,\n    sD: int = 1,\n    sK: int = 1,\n    sW: int = 1,\n    sF: int = 1,\n    sE: int = 1,\n    activation_size_bytes: int = 2,\n    weight_size_bytes: int = 2,\n    ici_latency: float = 1e-6,\n    all_gather_axes: Sequence[str] = tuple(),\n    debug=True,\n) -> dict[str, float]:\n  \"\"\"\n  Calculates estimated FLOPs, communication volume, and memory for a distributed matrix multiplication.\n\n  The multiplication is A @ W.\n  A (activations) has shape (M, K).\n  W (weights) has shape (G, K, F).\n\n  Sharding strategy assumed:\n  - Data Parallelism: `sD` shards the M dim of A.\n  - Embedding Parallelism: `sK` shards on the embedding dim of A.\n  - Tensor Parallelism for W dim: `sK` shards the W dimension of W.\n  - Tensor Parallelism for F dim: `sF` shards the second weight dim of W.\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n                     G is the number of groups if this is a GMM (e.g in MoE layer).\n      sD: Number of data parallel shards (sD). Must be >= 1.\n      sK: Sharding factor for the activation embedding dimension.\n      sW: Sharding factor for the first weight dimension.\n      sF: Sharding factor for the second weight dimension.\n      sE: Sharding factor to split up expert weights.\n      activation_size_bytes: Size of a single element in bytes for the activations.\n      weight_size_bytes: Size of a single element in bytes for the weights.\n      ici_latency: The latency overhead of communicating between TPUs.\n      all_gather_axes: Optional additional output axes that need to be all-gathered (e.g. \"M\", \"F\").\n      debug: Whether to print intermediate resource calculations.\n\n  Returns:\n      A dictionary with keys:\n          \"t_flops\": Estimated FLOPs latency.\n          \"t_comms\": Estimated communication latency.\n          \"memory\": Estimated memory footprint per device for storing\n                                   local shards of activations, weights, and output (bytes).\n  \"\"\"\n\n  M, K_act = activations_shape[0], activations_shape[-1]\n  # Intermediate activation shape\n  I = np.prod(np.array(activations_shape[1:-1]))\n  if len(weights_shape) == 3:\n    G, K_w, F = weights_shape\n  elif len(weights_shape) == 2:\n    K_w, F = weights_shape\n    G = 1\n  else:\n    raise ValueError(f\"weights_shape={weights_shape} is not supported!.\")\n\n  def _gather_dim_to_shard():\n    # # Used to map all-gather arguments to the respective shardings.\n    return {\"D\": sD, \"K\": sK, \"W\": sW, \"F\": sF, \"E\": sE}\n\n  gather_dim_to_shard = _gather_dim_to_shard()\n\n  def _validate_shardings_and_shapes():\n    if not (sD >= 1 and sK >= 1 and sW >= 1 and sF >= 1 and sE >= 1):\n      raise ValueError(\"All sharding amounts must be >= 1.\")\n    if sK > 1 and sF > 1:\n      raise ValueError(\"Cannot have both sK & sF > 1!\")\n    if K_act != K_w:\n      raise ValueError(f\"K dimension of activations ({K_act}) must match K dimension of weights ({K_w})\")\n    if sK > 1 and sW > 1 and sK != sW:\n      raise ValueError(\"Sharding amounts between embedding dim and first weight matricx dim are different!.\")\n    # Warnings for non-divisibility. Calculations proceed with float division,\n    # implying an average or approximation if not perfectly divisible.\n    if M % sD != 0:\n      print(\n          f\"Warning: Activations M dimension ({M}) is not perfectly divisible by sharding amount {sD}.\",\n          \"Results are approximate.\",\n      )\n    if K_act % sK != 0:\n      print(\n          f\"Warning: Common K dimension ({K_act}) is not perfectly divisible by sharding amount {sK}.\",\n          \"Results are approximate.\",\n      )\n    if K_w % sW != 0:\n      print(\n          f\"Warning: Common W dimension ({K_w}) is not perfectly divisible by sharding amount {sW}. Results are approximate.\"\n      )\n    if F % sF != 0:\n      print(\n          f\"Warning: Weights F dimension ({F}) is not perfectly divisible by sharding amount {sF}. Results are approximate.\"\n      )\n    if G % sE != 0:\n      print(\n          f\"Warning: Experts G dimension ({G}) is not perfectly divisible by sharding amount {sE}. Results are approximate.\"\n      )\n\n  _validate_shardings_and_shapes()\n  K = K_act\n\n  # Implied all-gather flags\n  is_fsdp_act = sK > 1 and sW == 1\n  is_fsdp_weight = sK == 1 and sW > 1\n\n  # Local device dimensions\n  local_M_dim = M // sD\n  local_K_dim = K // sK\n  local_W_dim = K // sW\n  local_G_dim = G // sE\n  local_F_dim = F // sF\n\n  # 1. Total FLOPs\n  # For A(M,K) @ W(K,F), FLOPs = 2 * M * K * F\n  total_flops = 2.0 * np.prod(activations_shape) * G * F / (sF * sE * sD * sK * sW)\n  if debug:\n    print(f\"Total GFlops = {total_flops/1e9}\")\n  if is_fsdp_act:\n    total_flops *= sK\n    if debug:\n      print(f\"Total GFlops after activation all-gather = {total_flops/1e9}\")\n  elif is_fsdp_weight:\n    total_flops *= sW\n    if debug:\n      print(f\"Total GFlops after weights all-gather = {total_flops/1e9}\")\n  t_flops = total_flops / peak_flops\n\n  # 2. Memory per device\n  # A_local: (M/sD, K/sK)\n  # W_local: (G/gE, K/sK, N/sF)\n  # Out_local: (M/sD, N/sF) (buffer for local output)\n  mem_activations_bytes = local_M_dim * I * local_K_dim * activation_size_bytes\n  mem_weights_bytes = local_G_dim * local_W_dim * local_F_dim * weight_size_bytes\n  if debug:\n    print(f\"Activation memory (GB): {mem_activations_bytes/1e9}\")\n    print(f\"Weights memory (GB): {mem_weights_bytes/1e9}\")\n  # All-gather\n  if is_fsdp_act:\n    mem_activations_bytes *= sK\n    if debug:\n      print(f\"Activation memory (GB) after all-gather: {mem_activations_bytes/1e9}\")\n  elif is_fsdp_weight:\n    mem_weights_bytes *= sW\n    if debug:\n      print(f\"Weight memory (GB) after all-gather: {mem_weights_bytes/1e9}\")\n\n  local_output_bytes = local_M_dim * I * local_G_dim * local_F_dim * max(activation_size_bytes, weight_size_bytes)\n  if debug:\n    print(f\"Output memory (GB): {local_output_bytes/1e9}\")\n\n  gathered_output_bytes = local_output_bytes * np.prod([gather_dim_to_shard[axis] for axis in all_gather_axes])\n  if debug:\n    print(f\"Output memory (GB) after additional axes gathers: {gathered_output_bytes/1e9}\")\n  memory_per_TPU_bytes = mem_activations_bytes + mem_weights_bytes + gathered_output_bytes\n  if debug:\n    print(f\"Total memory (GB): {memory_per_TPU_bytes/1e9}\")\n\n  # 3. Communication Volume per TPU\n  t_comms = 0.0\n\n  # For FSDP-style comms, all-gather the tensor.\n  if is_fsdp_act:\n    communication_volume_per_TPU_bytes = np.prod(np.array(activations_shape)) / sK * activation_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for activation all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif is_fsdp_weight:\n    communication_volume_per_TPU_bytes = np.prod(np.array(weights_shape)) / sW * weight_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sW - 1)\n    if debug:\n      print(f\"Per-TPU comms for weights all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif sK > 1 and sW > 1:\n    # Perform reduce-scatter on the output.\n    t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for all-reduce (GB): {local_output_bytes/1e9}\")\n\n  # All-to-all on the output during expert parallelism (assuming equal loads. i.e. 1/4 * comms(all-gather))\n  if sE > 1:\n    t_comms += latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sE - 1) / 4\n    if debug:\n      print(f\"Per-TPU comms for all-to-all (GB): {local_output_bytes/1e9}\")\n\n  for axis in all_gather_axes:\n    current_output_bytes = local_output_bytes\n    current_sharding = gather_dim_to_shard[axis]\n    t_comms += latency_bound_comms(current_output_bytes / ici_bandwidth, ici_latency) * (current_sharding - 1)\n    if debug:\n      print(f\"Per-TPU comms for axis {axis} all-gather (GB): {current_output_bytes/1e9}\")\n    current_output_bytes *= current_sharding\n\n  return {\n      \"t_flops\": t_flops,\n      \"t_comms\": t_comms,\n      \"memory_per_TPU_bytes\": memory_per_TPU_bytes,\n  }",
        "analysis": {
            "module_type": "distributed_matmul_resource_calculator",
            "purpose": "Calculates the estimated latency from FLOPs, communication, and the per-device memory footprint for a distributed matrix multiplication based on various sharding strategies.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Parse and validate input tensor shapes and sharding parameters.",
                "Determine local device tensor dimensions based on sharding factors (sD, sK, sW, sF, sE).",
                "Calculate the total FLOPs for the local matrix multiplication, accounting for any all-gather operations required by the sharding strategy (e.g., FSDP).",
                "Calculate the FLOPs latency (t_flops) by dividing total FLOPs by peak_flops.",
                "Calculate the memory required per device for local shards of activations, weights, and output, including memory expansion from all-gather operations.",
                "Calculate the communication latency (t_comms) by modeling the data volume and latency of collective operations like all-gather, reduce-scatter, or all-to-all based on the sharding strategy.",
                "Return a dictionary containing the calculated FLOPs latency, communication latency, and per-device memory."
            ],
            "output": {
                "shape": "A dictionary with keys: 't_flops', 't_comms', 'memory_per_TPU_bytes'."
            },
            "dependencies": [
                "numpy",
                "latency_bound_comms"
            ],
            "parameters": {
                "activations_shape": "Shape of the input activation tensor, typically (M, K).",
                "weights_shape": "Shape of the weight tensor, typically (G, K, F) for MoE or (K, F).",
                "ici_bandwidth": "The inter-chip interconnect bandwidth of the hardware in bytes/sec.",
                "peak_flops": "The peak floating-point operations per second of the hardware.",
                "sD": "Sharding factor for Data parallelism (shards the 'M' dimension).",
                "sK": "Sharding factor for the activation embedding dimension ('K').",
                "sW": "Sharding factor for the first weight dimension ('K').",
                "sF": "Sharding factor for the second weight dimension ('F').",
                "sE": "Sharding factor for expert parallelism (shards the 'G' dimension).",
                "all_gather_axes": "Specifies any additional output dimensions that require an all-gather collective, impacting communication and memory."
            },
            "notes": [
                "The function models several common parallelism strategies including data parallelism, tensor parallelism (FSDP-style and 2D), and expert parallelism.",
                "It raises ValueError for unsupported or conflicting sharding configurations (e.g., sharding both K and F dimensions simultaneously).",
                "The calculations assume a matrix multiplication of the form A @ W.",
                "Prints warnings if tensor dimensions are not perfectly divisible by their corresponding sharding factors, indicating that the results are approximate."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#plot_sharding_scheme_comparison",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def plot_sharding_scheme_comparison(\n    calc_resource_func,\n    activations_shape,\n    weights_shape,\n    sharding_schemes: list[dict],\n):\n  \"\"\"\n  Generates plots comparing different sharding schemes:\n  1. Communication latency vs. FLOPs latency\n  2. Communication latency / memory per device\n  3. Memory & Communication Latency\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n      sharding_schemes: A list of dictionaries. Each dictionary must contain:\n          - \"label\": A string label for the scheme (e.g., \"DP=8\").\n          - \"shard_settings\": A dictionary with sharding parameters used for calc_resource_func().\n          E.g:\n          [\n              {\n                  \"label\": \"DP=8\", # Pure Data Parallelism\n                  \"shard_settings\": {\n                      \"sD\": 8,\n                      \"all_gather_axes\": (\"D\",)\n                  }\n              },\n          ]\n      element_size_bytes: Size of a single element in bytes.\n  \"\"\"\n  results = []\n  valid_schemes_labels = []\n\n  print(\"Calculating resources for sharding schemes...\")\n  for scheme in sharding_schemes:\n    label = scheme.get(\"label\", \"Unknown Scheme\")\n    shard_settings = scheme.get(\"shard_settings\")\n\n    print(f\"\\n--- Scheme: {label} ---\")\n    try:\n      # Clear previous warnings for divisibility for cleaner output per iteration\n      with warnings.catch_warnings(record=True) as caught_warnings:\n        del caught_warnings\n        warnings.simplefilter(\"always\")  # Catch all warnings\n\n        # Call the resource calculation function\n        res = calc_resource_func(activations_shape, weights_shape, **shard_settings)\n        print(\"Workload stats:\\n\")\n        pprint.PrettyPrinter(indent=4).pprint(res)\n\n      results.append(res)\n      valid_schemes_labels.append(label)\n    except ValueError as e:\n      print(f\"Error calculating resources for scheme '{label}': {e}. Skipping.\")\n    except (TypeError, KeyError, ZeroDivisionError, AttributeError) as e:\n      print(f\"An unexpected error occurred for scheme '{label}': {e}. Skipping.\")\n\n  if not results:\n    print(\"No valid data points generated. Cannot create plots.\")\n    return\n\n  # Extract data for plotting\n  t_flops_list = np.array([r[\"t_flops\"] for r in results])\n  t_comms_list = np.array([r[\"t_comms\"] for r in results])\n  mem_list = np.array([r[\"memory_per_TPU_bytes\"] for r in results]) / (1024**3)  # GB\n  title_suffix_context = f\": A{activations_shape} @ W{weights_shape}\"\n  num_schemes = len(valid_schemes_labels)  # Number of successfully processed schemes\n  colors = plt.cm.viridis(np.linspace(0, 1, num_schemes)) if num_schemes > 0 else []\n\n  # Calculate FLOPs/Communication ratio\n  flops_per_comm_ratio = np.zeros(num_schemes)\n  has_infinite_ratio = [False] * num_schemes\n  for i in range(num_schemes):\n    if t_comms_list[i] > 1e-9:  # Threshold to avoid near-zero division issues\n      flops_per_comm_ratio[i] = t_flops_list[i] / t_comms_list[i]\n    elif t_flops_list[i] > 1e-9:  # Positive FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = np.inf\n      has_infinite_ratio[i] = True\n    else:  # Zero FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = 0\n\n  finite_ratios = flops_per_comm_ratio[np.isfinite(flops_per_comm_ratio)]\n  placeholder_for_inf = 0\n  if finite_ratios.size > 0:\n    placeholder_for_inf = np.max(finite_ratios) * 1.5 if np.max(finite_ratios) > 0 else 1000\n  elif np.any(has_infinite_ratio):\n    placeholder_for_inf = 1000\n\n  plot_ratios = np.array(\n      [placeholder_for_inf if r_inf else r_val for r_val, r_inf in zip(flops_per_comm_ratio, has_infinite_ratio)]\n  )\n  plot_ratios = np.nan_to_num(plot_ratios, nan=0.0, posinf=placeholder_for_inf, neginf=-placeholder_for_inf)\n\n  # --- Create Plots ---\n  categorical_x = np.arange(num_schemes)  # For categorical x-axis\n\n  # Plot 1: FLOPs & Communication (Grouped Bar Plot)\n  grouped_bar_width_fc = 0.35\n  fig_flops_comm_grouped, ax_flops_comm_grouped = plt.subplots(figsize=(max(10, num_schemes * 1.7), 7))\n\n  rects_flops = ax_flops_comm_grouped.bar(\n      categorical_x - grouped_bar_width_fc / 2,\n      t_flops_list,\n      grouped_bar_width_fc,\n      label=\"T_flops\",\n      color=\"mediumseagreen\",\n  )\n  rects_comms_grouped = ax_flops_comm_grouped.bar(\n      categorical_x + grouped_bar_width_fc / 2, t_comms_list, grouped_bar_width_fc, label=\"T_comms\", color=\"deepskyblue\"\n  )\n\n  ax_flops_comm_grouped.set_xlabel(\"Sharding Scheme\")\n  ax_flops_comm_grouped.set_ylabel(\"Seconds\")\n  ax_flops_comm_grouped.set_title(f\"T_flops & T_comms by Sharding Scheme{title_suffix_context}\", fontsize=14)\n  ax_flops_comm_grouped.set_xticks(categorical_x)\n  ax_flops_comm_grouped.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  if num_schemes > 0:\n    ax_flops_comm_grouped.legend(fontsize=10)\n\n  ax_flops_comm_grouped.bar_label(rects_flops, padding=3, fmt=\"%.2e\", fontsize=9)\n  ax_flops_comm_grouped.bar_label(rects_comms_grouped, padding=3, fmt=\"%.2e\", fontsize=9)\n\n  ax_flops_comm_grouped.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  max_y_val_fc = 0\n  if t_flops_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_flops_list))\n  if t_comms_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_comms_list))\n  print(f\"max_y_val_fc = {max_y_val_fc}\")\n  ax_flops_comm_grouped.set_ylim(0, max_y_val_fc * 1.15)\n\n  fig_flops_comm_grouped.tight_layout()\n  plt.show()\n\n  # Plot 2: FLOPs/Communication Ratio\n  fig_ratio, ax_ratio = plt.subplots(figsize=(max(10, num_schemes * 1.1), 7))\n\n  bars_ratio = ax_ratio.bar(categorical_x, plot_ratios, width=0.6, color=colors, alpha=0.9)\n\n  ax_ratio.set_xlabel(\"Sharding Scheme\")\n  ax_ratio.set_ylabel(\"T_flops / T_comms\")\n  ax_ratio.set_title(f\"Roofline (T_flops vs. T_comms) for {title_suffix_context}\", fontsize=14)\n  ax_ratio.set_xticks(categorical_x)\n  ax_ratio.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_ratio.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  for i, bar in enumerate(bars_ratio):\n    yval = bar.get_height()\n    label_text = f\"{yval:.2f}\"\n    if has_infinite_ratio[i] and yval == placeholder_for_inf:\n      label_text = f\"> {np.max(finite_ratios):.2f}\\n(Effectively Inf)\" if finite_ratios.size > 0 else \"Very High\"\n    ax_ratio.text(bar.get_x() + bar.get_width() / 2.0, yval, label_text, va=\"bottom\", ha=\"center\", fontsize=9)\n\n  if plot_ratios.size > 0:\n    max_ratio_plot_val = np.max(plot_ratios)\n    ax_ratio.set_ylim(0, max_ratio_plot_val * 1.15)\n\n  fig_ratio.tight_layout()\n  plt.show()\n\n  # Plot 3: Memory vs. Communication (Bars positioned by Communication Volume)\n  fig_mem, ax_mem = plt.subplots(figsize=(max(10, num_schemes * 1.3), 7))  # Slightly wider for labels\n  bar_width_mem = 0.6\n\n  ax_mem.bar(\n      categorical_x,\n      mem_list,\n      width=bar_width_mem,\n      color=colors,\n      alpha=0.85,\n      edgecolor=[np.array(c[:3]) * 0.6 for c in colors],\n  )\n\n  ax_mem.set_xlabel(\"Sharding Scheme\")\n  ax_mem.set_ylabel(\"Memory per TPU (GB)\")\n  ax_mem.set_title(f\"Memory & Comm. by Sharding Scheme{title_suffix_context}\", fontsize=14)  # Updated title\n  ax_mem.set_xticks(categorical_x)\n  ax_mem.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_mem.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  # Add custom labels in scientific notation\n  for i in range(num_schemes):\n    mem_val = mem_list[i]\n    comm_val = t_comms_list[i]  # This is assumed to be in MB\n\n    # Format the label string as requested\n    # Using \\n for a new line to make it more readable on the plot\n    label_text = f\"mem: {mem_val:.2e} GB\\nt_comms: {comm_val:.2e} sec\"\n\n    ax_mem.text(\n        categorical_x[i],  # x-position: center of the bar\n        mem_val,  # y-position: top of the bar\n        label_text,\n        ha=\"center\",  # Horizontal alignment\n        va=\"bottom\",  # Vertical alignment (anchor at bottom of text, so text is above y)\n        fontsize=8,\n        rotation=0,\n        bbox={\"facecolor\": \"white\", \"alpha\": 0.6, \"pad\": 2, \"boxstyle\": \"round,pad=0.3\"},  # Added bbox\n    )\n\n  if mem_list.size > 0:\n    max_mem_val = np.max(mem_list)\n    # Adjust y-limit to accommodate multi-line labels; factor might need tuning\n    ax_mem.set_ylim(0, max_mem_val * 1.35)  # Increased padding for multi-line labels\n  else:\n    ax_mem.set_ylim(0, 1)\n\n  fig_mem.tight_layout()\n  plt.show()",
        "analysis": {
            "module_type": "sharding_scheme_visualizer",
            "purpose": "Calculates and plots a comparison of performance metrics (latency, memory) for various tensor sharding schemes.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through each provided sharding scheme.",
                "For each scheme, call the `calc_resource_func` to compute FLOPs latency, communication latency, and memory usage.",
                "Collect the results for all valid schemes, skipping any that cause errors during calculation.",
                "Extract and process the collected data into numpy arrays (e.g., convert memory to GB, calculate FLOPs/communication ratio).",
                "Generate and display a grouped bar plot comparing FLOPs latency (T_flops) and communication latency (T_comms).",
                "Generate and display a bar plot of the T_flops / T_comms ratio.",
                "Generate and display a bar plot of memory per TPU, with annotations for memory and communication values."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "matplotlib.pyplot",
                "pprint",
                "warnings"
            ],
            "parameters": {
                "calc_resource_func": "A callable function that takes tensor shapes and sharding settings, and returns a dictionary with 't_flops', 't_comms', and 'memory_per_TPU_bytes'.",
                "activations_shape": "A tuple representing the shape of the activation tensor (e.g., (M, K)).",
                "weights_shape": "A tuple representing the shape of the weight tensor (e.g., (G, K, F)).",
                "sharding_schemes": "A list of dictionaries, where each dictionary defines a sharding strategy with a 'label' and 'shard_settings'."
            },
            "notes": [
                "The function does not return any value; its side effect is displaying three matplotlib plots.",
                "It includes error handling to skip sharding schemes that are invalid or cause calculation errors.",
                "Handles division-by-zero for the FLOPs/communication ratio, representing infinite ratios with a placeholder value on the plot."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/test_sharding_utils.py#ShardingTests",
        "file_path": "src/MaxText/inference/scripts/test_sharding_utils.py",
        "code_block": "class ShardingTests(unittest.TestCase):\n  \"\"\"Test suite for sharding resource calculation utilities.\"\"\"\n\n  def test_no_sharding(self):\n    \"\"\"Tests the basic case with no sharding.\"\"\"\n    sD, sK, sW, sF, sE = 1, 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    # Total FLOPs = 2 * M * K * F\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF(self):\n    \"\"\"Tests sharding on the F dimension of weights (sF > 1).\"\"\"\n    sF = 4\n    sD, sK, sW, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_data_parallelism_sD(self):\n    \"\"\"Tests sharding on the M dimension of activations (sD).\"\"\"\n    sD = 4\n    sK, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs:\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_activation_sharding_sK(self):\n    \"\"\"Tests FSDP-style sharding on the K dimension of activations (sK).\n\n    In this scenario, the weights are not sharded (sW=1).\n    \"\"\"\n    sK = 4\n    sD, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (M * K / sK) * activation_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_weight_sharding_sW(self):\n    \"\"\"Tests FSDP-style sharding on the W dimension of weights (sW).\n\n    In this scenario, the activations are not sharded (sK=1).\n    \"\"\"\n    sW = 4\n    sD, sK, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (K * F / sW) * weight_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sW - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_tensor_parallel_sK_sW(self):\n    \"\"\"Tests tensor parallelism where both sK and sW are used.\n\n    This test assumes sK == sW and a reduce-scatter operation for partial\n    results.\n    \"\"\"\n    sK = 2\n    sW = 2  # Must be equal to sK for this path\n    sD, sF, sE = 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF_with_all_gather_F(self):\n    \"\"\"Tests sF sharding with a subsequent all-gather on the F dimension.\"\"\"\n    sF = 4  # Shard the output feature dimension\n    sD, sK, sW, sE = 1, 1, 1, 1  # Isolate sF effect\n    all_gather_axes = [\"F\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_for_gather = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_for_gather / ici_bandwidth_val, ici_latency_val) * (sF - 1)\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU:\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Outputs\n    expected_mem_output_gathered = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output_gathered\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_expert_parallelism_sE(self):\n    \"\"\"Tests expert parallelism sharding on the G dimension (sE).\"\"\"\n    G_val = 8\n    weights_shape_3d = (G_val, K, F)\n    sE = 4\n    sD, sK, sW, sF = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_3d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F * G_val) / (peak_flops_val * sE)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sE - 1) / 4\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (G_val / sE) * K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_mixed_sharding_sD_sK_sW(self):\n    \"\"\"Tests a mix of data and tensor parallelism (reduce-scatter).\"\"\"\n    sD = 2\n    sK = 2\n    sW = 2  # sK == sW\n    sF, sE = 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * (M / sD) * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_additional_all_gather_axes_D(self):\n    \"\"\"Tests an additional all-gather on the 'D' dimension of the output.\"\"\"\n    sD = 2\n    sK, sW, sF, sE = 1, 1, 1, 1\n    all_gather_axes = [\"D\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_base = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_base / ici_bandwidth_val, ici_latency_val) * (sD - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE",
        "analysis": {
            "module_type": "unit_test_suite",
            "purpose": "A test suite for verifying the correctness of sharding resource calculation utilities, specifically for FLOPs, communication costs, and memory usage.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes common test parameters like tensor dimensions, bandwidth, and FLOPs.",
                "Defines and runs multiple test methods, each corresponding to a specific sharding strategy.",
                "Each test method calls `calculate_matmul_resources` with a specific sharding configuration.",
                "Asserts that the returned FLOPs, communication time, and memory usage match pre-calculated expected values."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "unittest.TestCase",
                "MaxText.inference.scripts.sharding_utils.calculate_matmul_resources",
                "MaxText.inference.scripts.sharding_utils.latency_bound_comms"
            ],
            "parameters": {},
            "notes": [
                "This class systematically tests various parallelism strategies including data parallelism, tensor parallelism (FSDP, reduce-scatter), output feature parallelism, and expert parallelism.",
                "It also tests scenarios with additional communication operations like all-gather on specific output dimensions."
            ],
            "methods": {
                "test_no_sharding": {
                    "purpose": "Tests the baseline case with no sharding applied, where all sharding factors are 1.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with all sharding factors set to 1.",
                        "Calculate the expected total FLOPs, communication time (zero), and memory per TPU.",
                        "Assert that the results from the function call match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": [
                        "Establishes the baseline for resource usage without any parallelism."
                    ]
                },
                "test_output_feature_parallelism_sF": {
                    "purpose": "Tests sharding on the output feature dimension 'F' of the weights (sF > 1).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sF` > 1.",
                        "Calculate expected FLOPs, communication time (zero), and memory, accounting for the sharded 'F' dimension.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": [
                        "This form of parallelism typically does not introduce communication costs unless an all-gather is explicitly added."
                    ]
                },
                "test_data_parallelism_sD": {
                    "purpose": "Tests sharding on the batch dimension 'M' of the activations (sD > 1).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sD` > 1.",
                        "Calculate expected FLOPs, communication time (zero), and memory, accounting for the sharded 'M' dimension.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": [
                        "This is a standard data parallelism test where each device processes a slice of the input batch."
                    ]
                },
                "test_fsdp_activation_sharding_sK": {
                    "purpose": "Tests FSDP-style sharding on the contracting dimension 'K' of activations, which requires an all-gather operation.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sK` > 1.",
                        "Calculate expected FLOPs (unchanged), communication time for the activation all-gather, and memory (full tensors materialized).",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This scenario assumes weights are not sharded (`sW=1`) and models the communication overhead of FSDP on activations."
                    ]
                },
                "test_fsdp_weight_sharding_sW": {
                    "purpose": "Tests FSDP-style sharding on the contracting dimension 'K' of weights, which requires an all-gather operation.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sW` > 1.",
                        "Calculate expected FLOPs (unchanged), communication time for the weight all-gather, and memory (full tensors materialized).",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This scenario assumes activations are not sharded (`sK=1`) and models the communication overhead of FSDP on weights."
                    ]
                },
                "test_tensor_parallel_sK_sW": {
                    "purpose": "Tests tensor parallelism where both activations and weights are sharded on the contracting dimension 'K', requiring a reduce-scatter.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sK` > 1 and `sW` > 1, where `sK` == `sW`.",
                        "Calculate expected FLOPs, communication time for the reduce-scatter on partial results, and memory for sharded inputs and full output.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This models a common tensor parallelism strategy where the matmul is split and partial results are aggregated."
                    ]
                },
                "test_output_feature_parallelism_sF_with_all_gather_F": {
                    "purpose": "Tests output feature parallelism (`sF` > 1) followed by an explicit all-gather on the sharded 'F' dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sF` > 1 and `all_gather_axes=['F']`.",
                        "Calculate expected FLOPs, communication time for the output all-gather, and memory for sharded weights but a fully gathered output.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This test verifies the calculation of communication and memory costs when a sharded output needs to be fully materialized on all devices."
                    ]
                },
                "test_expert_parallelism_sE": {
                    "purpose": "Tests expert parallelism sharding on the 'G' (group/expert) dimension of a 3D weight tensor.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Use a 3D weight shape to simulate experts.",
                        "Call `calculate_matmul_resources` with `sE` > 1.",
                        "Calculate expected FLOPs, communication time for the all-to-all operation, and memory for sharded weights and output.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This models the resource usage of a Mixture-of-Experts (MoE) layer."
                    ]
                },
                "test_mixed_sharding_sD_sK_sW": {
                    "purpose": "Tests a combination of data parallelism (`sD`) and tensor parallelism (`sK`, `sW`) with a reduce-scatter.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sD`, `sK`, and `sW` all greater than 1.",
                        "Calculate expected FLOPs, communication time, and memory, accounting for the combined effects of both parallelism types.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "Verifies that the calculation correctly handles interactions between different sharding strategies."
                    ]
                },
                "test_additional_all_gather_axes_D": {
                    "purpose": "Tests data parallelism (`sD` > 1) followed by an explicit all-gather on the sharded 'D' (batch) dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `calculate_matmul_resources` with `sD` > 1 and `all_gather_axes=['D']`.",
                        "Calculate expected FLOPs, communication time for the output all-gather, and memory for sharded activations but a fully gathered output.",
                        "Assert that the results match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This scenario is less common but tests the flexibility of the calculation for arbitrary all-gather operations."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#DecoderBlockType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class DecoderBlockType(enum.Enum):\n  \"\"\"Decoder block types.\"\"\"\n\n  DEFAULT = \"default\"\n  LLAMA2 = \"llama2\"\n  MISTRAL = \"mistral\"\n  MIXTRAL = \"mixtral\"\n  DEEPSEEK = \"deepseek\"\n  GEMMA = \"gemma\"\n  GEMMA2 = \"gemma2\"\n  GEMMA3 = \"gemma3\"\n  QWEN3 = \"qwen3\"\n  QWEN3_MOE = \"qwen3_moe\"\n  QWEN3_NEXT = \"qwen3_next\"\n  GPT3 = \"gpt3\"\n  GPT_OSS = \"gpt_oss\"\n  SIMPLE = \"simple\"\n  SIMPLE_MLP = \"simple_mlp\"\n  LLAMA4 = \"llama4\"",
        "analysis": {
            "module_type": "decoder_block_type_enum",
            "purpose": "Defines a set of named constants to represent different architectural variations of a transformer decoder block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This is a standard Python enumeration used for configuration.",
                "The members correspond to specific model architectures (e.g., LLAMA2, MISTRAL, GEMMA) or generic decoder types (e.g., DEFAULT, SIMPLE).",
                "Each member's value is a string representation of its name."
            ]
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#AttentionType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class AttentionType(enum.Enum):\n  GLOBAL = \"global\"  # default, with causality\n  LOCAL_SLIDING = \"local_sliding\"\n  CHUNK = \"chunk\"\n  MLA = \"mla\"\n  FULL = \"full\"",
        "analysis": {
            "module_type": "attention_type_enum",
            "purpose": "Defines a set of named constants representing different types of attention mechanisms.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum"
            ],
            "parameters": {},
            "notes": [
                "This is an enumeration class that defines the following members:",
                "GLOBAL: 'global' (commented as the default, with causality)",
                "LOCAL_SLIDING: 'local_sliding'",
                "CHUNK: 'chunk'",
                "MLA: 'mla'",
                "FULL: 'full'"
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_input_data_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_input_data_sharding(config, mesh):\n  \"\"\"Get the input data sharding for the model\"\"\"\n  return nn.logical_to_mesh_sharding(P(*config.input_data_sharding_logical_axes), mesh, config.logical_axis_rules)",
        "analysis": {
            "module_type": "sharding_utility",
            "purpose": "Converts a logical axis specification for input data into a physical mesh sharding configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a `jax.sharding.PartitionSpec` from the logical axes defined in `config.input_data_sharding_logical_axes`.",
                "Calls `nn.logical_to_mesh_sharding` to map the logical `PartitionSpec` to the physical device `mesh` using the rules from `config.logical_axis_rules`.",
                "Returns the resulting mesh sharding object."
            ],
            "output": {
                "shape": "Returns a JAX sharding object (e.g., `jax.sharding.NamedSharding`), not a tensor."
            },
            "dependencies": [
                "flax.linen.nn.logical_to_mesh_sharding",
                "jax.sharding.PartitionSpec"
            ],
            "parameters": {
                "config.input_data_sharding_logical_axes": "A tuple or list of strings representing the logical axes for sharding the input data (e.g., ('data',)).",
                "config.logical_axis_rules": "A set of rules that map logical axis names to physical mesh axis names.",
                "mesh": "A JAX device mesh object representing the physical hardware topology."
            },
            "notes": [
                "This function is a helper to centralize the creation of input data sharding configurations based on high-level settings."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_train_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_train_with_signature(\n    train_step, data_sharding, state_mesh_shardings, model, config, params_shardings=None\n):\n  \"\"\"Get the shardings (both state and data) for `train_step`.\"\"\"\n  functional_train = functools.partial(train_step, model, config, state_mesh_shardings, params_shardings)\n  functional_train.__name__ = \"train_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = (state_mesh_shardings, None)  # State, metrics\n  static_argnums = ()  # We partial out the static argnums of model and config\n  donate_argnums = 0  # This is the index of the state - we allow the compiler to make use of this memory.\n  return functional_train, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "pjit_signature_wrapper",
            "purpose": "Wraps a training step function with `functools.partial` to fix static arguments and defines the input/output sharding specifications, static argument indices, and donated argument indices required for JAX's `pjit` compilation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a partial function `functional_train` from the input `train_step`, binding the `model`, `config`, `state_mesh_shardings`, and `params_shardings` arguments.",
                "Set the `__name__` of the partial function to 'train_step' for better profiling.",
                "Define `in_shardings` as a tuple specifying the sharding for the training state, data batch, and RNG key.",
                "Define `out_shardings` as a tuple specifying the sharding for the output training state and metrics.",
                "Define `static_argnums` as an empty tuple, as static arguments are handled by `functools.partial`.",
                "Define `donate_argnums` to be 0, indicating the input training state's memory can be reused for the output state.",
                "Return the partial function and the sharding/compilation configurations."
            ],
            "output": {
                "shape": "Returns a tuple: (partial_function, in_shardings_tuple, out_shardings_tuple, static_argnums_tuple, donate_argnums_int)."
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "train_step": "The core training step function to be wrapped.",
                "data_sharding": "The sharding specification for the input data batch.",
                "state_mesh_shardings": "The sharding specification for the model's training state.",
                "model": "The model object, which will be a static argument to the train step.",
                "config": "The configuration object, which will be a static argument to the train step.",
                "params_shardings": "Optional sharding specification for model parameters."
            },
            "notes": [
                "This function prepares a training step for efficient compilation with `jax.pjit` by separating static configuration from dynamic inputs.",
                "The `donate_argnums=0` is a performance optimization that allows the JAX compiler to perform an in-place update of the training state, saving memory."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_eval_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_eval_with_signature(eval_step, data_sharding, state_mesh_shardings, model, config):\n  \"\"\"Get the shardings (both state and data) for `eval_step`.\"\"\"\n  functional_eval = functools.partial(eval_step, model, config)\n  functional_eval.__name__ = \"eval_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = None  # metrics\n  static_argnums = ()  # We partial out the static argnums of model, config\n  donate_argnums = ()  # state will be kept instead of being donated in eval_step\n  return functional_eval, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "function_signature_wrapper",
            "purpose": "Wraps an evaluation step function with its corresponding JAX pjit signature, including input/output shardings and other compilation arguments.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a partial function `functional_eval` by binding the `model` and `config` arguments to the input `eval_step` function using `functools.partial`.",
                "Sets the `__name__` of the new partial function to 'eval_step'.",
                "Defines a tuple `in_shardings` specifying the sharding for the state, batch, and RNG inputs.",
                "Sets `out_shardings` to `None`, indicating the output (metrics) is fully replicated.",
                "Defines `static_argnums` as an empty tuple because the static arguments (`model`, `config`) have been pre-applied.",
                "Defines `donate_argnums` as an empty tuple to ensure the model state is not modified in-place during evaluation.",
                "Returns the created partial function along with its sharding and compilation configurations."
            ],
            "output": {
                "shape": "A tuple containing (functional_eval, in_shardings, out_shardings, static_argnums, donate_argnums)."
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "eval_step": "The evaluation function to be wrapped.",
                "data_sharding": "The sharding specification for the input data batch.",
                "state_mesh_shardings": "The sharding specification for the model's state.",
                "model": "The model object, which is partially applied to `eval_step`.",
                "config": "The configuration object, which is partially applied to `eval_step`."
            },
            "notes": [
                "This function is a utility to prepare an evaluation function for `jax.pjit` by packaging it with all necessary sharding and compilation metadata.",
                "The returned tuple is designed to be passed directly to `pjit` to create a parallelized evaluation function."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_shaped_batch",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_shaped_batch(config):\n  \"\"\"Return the shape of the batch - this is what eval_shape would return for the\n  output of create_data_iterator, but eval_shape doesn't work, see b/306901078.\"\"\"\n  batch_shape = (config.global_batch_size_to_load, config.max_target_length)\n  shaped_batch = {}\n  shaped_batch[\"inputs\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  if config.use_multimodal:\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n    shaped_batch[\"images\"] = jax.ShapeDtypeStruct(image_shape, jnp.int32)\n    shaped_batch[\"image_masks\"] = jax.ShapeDtypeStruct(image_shape[:2], jnp.int32)\n  return shaped_batch",
        "analysis": {
            "module_type": "batch_shape_generator",
            "purpose": "Constructs a dictionary of JAX ShapeDtypeStruct objects representing the shape and data type of a typical data batch, based on configuration parameters.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a base batch shape from `config.global_batch_size_to_load` and `config.max_target_length`.",
                "Create `jax.ShapeDtypeStruct` objects for standard text inputs and targets ('inputs', 'targets', 'inputs_position', etc.) using the base shape and int32 dtype.",
                "Check if `config.use_multimodal` is true.",
                "If multimodal, call `multimodal_utils.get_dummy_image_shape_for_init` to determine the image shape.",
                "If multimodal, create and add `jax.ShapeDtypeStruct` objects for 'images' and 'image_masks' to the batch dictionary.",
                "Return the dictionary of `ShapeDtypeStruct` objects."
            ],
            "output": {
                "shape": "A dictionary mapping string keys (e.g., 'inputs', 'targets', 'images') to `jax.ShapeDtypeStruct` objects."
            },
            "dependencies": [
                "jax.ShapeDtypeStruct",
                "jax.numpy",
                "multimodal_utils.get_dummy_image_shape_for_init"
            ],
            "parameters": {
                "config.global_batch_size_to_load": "The total batch size to be loaded across all devices.",
                "config.max_target_length": "The maximum sequence length for text inputs and targets.",
                "config.use_multimodal": "A boolean flag indicating whether to include shapes for multimodal image data.",
                "config.model_name": "The name of the model, used to determine the dummy image shape for initialization.",
                "config.micro_batch_size_to_train_on": "The per-device batch size, used to determine the dummy image shape for initialization."
            },
            "notes": [
                "This function serves as a workaround for `eval_shape` not functioning correctly with the data iterator, as mentioned in the docstring (b/306901078).",
                "The output is not actual data but a pytree of shape and dtype specifications, which is useful for JAX transformations like `jax.jit` or for model initialization without needing a real data batch."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#should_prevent_cse_in_remat",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def should_prevent_cse_in_remat(config):\n  \"\"\"Determines whether to prevent common subexpression elimination (CSE) in remat.\n\n  CSE should not be prevented when:\n  1. Layers are being scanned (scan_layers=True), OR\n  2. Gradient accumulation is enabled (gradient_accumulation_steps > 1) on GPU hardware\n\n  Args:\n    config: Configuration object with scan_layers, gradient_accumulation_steps, and hardware\n\n  Returns:\n    bool: True if CSE should be prevented, False otherwise\n  \"\"\"\n  if config.scan_layers:\n    return False\n\n  if config.gradient_accumulation_steps > 1 and config.hardware in (\"gpu\", \"gpu_multiprocess\"):\n    return False\n\n  return True",
        "analysis": {
            "module_type": "configuration_utility",
            "purpose": "Determines whether to prevent common subexpression elimination (CSE) during rematerialization based on specific training configurations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.scan_layers` is true.",
                "Check if `config.gradient_accumulation_steps` is greater than 1 and `config.hardware` is 'gpu' or 'gpu_multiprocess'.",
                "Return False if either of the above conditions is met, otherwise return True."
            ],
            "output": {
                "shape": "Scalar (boolean)"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing `scan_layers`, `gradient_accumulation_steps`, and `hardware` attributes."
            },
            "notes": [
                "CSE is allowed (function returns False) if layers are scanned or if gradient accumulation is used on GPUs.",
                "The default behavior is to prevent CSE (function returns True)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#load_compiled",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def load_compiled(config, partial_train, state):\n  \"\"\"# Loading a serialized compiled train step function.\"\"\"\n\n  # Currently partial_train and state  are needed to reconstruct\n  # input/output shapes to construct the in_trees and out_trees for load API\n  # Parker is working on a serializing these\n  def load_serialized_compiled(save_name):\n    with open(save_name, \"rb\") as f:\n      serialized_compiled = pickle.load(f)\n    return serialized_compiled\n\n  def get_train_input_output_trees(func, input_args, input_kwargs):\n    _, in_tree_recreated = jax.tree_util.tree_flatten((input_args, input_kwargs))\n    out_shaped = jax.eval_shape(func, *input_args, **input_kwargs)\n    _, out_tree_recreated = jax.tree_util.tree_flatten(out_shaped)\n    return in_tree_recreated, out_tree_recreated\n\n  serialized_compiled = load_serialized_compiled(config.compiled_trainstep_file)\n  shaped_batch = get_shaped_batch(config)\n  example_rng = jax.random.PRNGKey(0)\n  shaped_input_args = (state, shaped_batch, example_rng)\n  shaped_input_kwargs = {}\n  in_tree, out_tree = get_train_input_output_trees(partial_train, shaped_input_args, shaped_input_kwargs)\n  p_train_step = deserialize_and_load(serialized_compiled, in_tree, out_tree)\n  return p_train_step",
        "analysis": {
            "module_type": "compiled_function_loader",
            "purpose": "Loads a pre-compiled and serialized JAX training step function from a file.",
            "input": {
                "shape": "config: Configuration object, partial_train: Callable, state: JAX TrainState PyTree",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a helper function `load_serialized_compiled` to load a pickled file.",
                "Define a helper function `get_train_input_output_trees` to determine the input/output PyTree structure of a function using `jax.eval_shape`.",
                "Load the serialized compiled function from the file specified in `config.compiled_trainstep_file`.",
                "Generate a dummy batch with the correct shapes using `get_shaped_batch`.",
                "Construct shaped input arguments using the provided `state`, the dummy batch, and a dummy RNG key.",
                "Determine the input and output PyTree definitions (`in_tree`, `out_tree`) for the `partial_train` function.",
                "Deserialize the loaded function and apply the PyTree definitions using `deserialize_and_load`.",
                "Return the deserialized and loaded parallel train step function."
            ],
            "output": {
                "shape": "A callable JAX pjit-ed function."
            },
            "dependencies": [
                "pickle.load",
                "jax.tree_util.tree_flatten",
                "jax.eval_shape",
                "jax.random.PRNGKey",
                "jax.experimental.serialize_executable.deserialize_and_load",
                "get_shaped_batch"
            ],
            "parameters": {
                "compiled_trainstep_file": "The file path from which to load the serialized compiled training step function."
            },
            "notes": [
                "This function is necessary because the input/output PyTree structures (in_trees and out_trees) are not saved with the serialized compiled function and must be reconstructed at load time.",
                "It uses a `partial_train` function, a `state` object, and a dummy batch to infer the required shapes and tree structures for deserialization."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tokens_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tokens_training_per_device(config):\n  \"\"\"Calculate training Tokens per device\"\"\"\n  return config.max_target_length * config.per_device_batch_size * config.gradient_accumulation_steps",
        "analysis": {
            "functionality": "Calculates the total number of training tokens processed per device for a single logical batch (i.e., one optimizer step).",
            "usage": "Call this function with a configuration object that has `max_target_length`, `per_device_batch_size`, and `gradient_accumulation_steps` attributes. It returns an integer representing the total tokens per device."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma2_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma2_tflops_training_per_device(config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops):\n  \"\"\"\n  Calculate training TFLOP for Gemma2 as in Gemma2 we combine [local_attention, global_attention] into one decoder\n  layer and we use sliding window attention in local_attention\n  \"\"\"\n  noncausal_attention_flops = (\n      # global attention\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n      +\n      # local attention\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  # multiply num_decoder_layers by 2 because we combine [local_attention, global_attention] into one decoder layer\n  learnable_weight_tflops = (\n      ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers * 2 + embedding_flops) * 3 / 10**12\n  )\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "functionality": "Calculates the training TeraFLOPs (TFLOPs) per device for a Gemma2-style model, separating the computation into attention-related FLOPs and learnable weight-related FLOPs.",
            "usage": "This function is used to estimate the computational cost of training a Gemma2 model. It requires a configuration object with model parameters (e.g., batch size, sequence length, attention heads) and pre-calculated FLOPs for various model components. It outputs a tuple of the total TFLOPs for attention operations and for learnable weight computations (forward, backward, and optimizer steps included)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mixed_attention_model_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mixed_attention_model_tflops_training_per_device(\n    config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length\n):\n  \"\"\"\n  Calculate training TFLOPs for models with a mixed attention pattern of local\n  and global attention layers, like Gemma3 and GPT-OSS.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n\n  num_global_layers = num_layers // attention_pattern_length\n  num_local_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention)\n  # Formula: 4 * batch_size * seq_len^2 * num_heads * head_dim\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single local attention layer (sliding window)\n  # Formula: 4 * batch_size * seq_len * window_size * num_heads * head_dim\n  local_attention_flops_per_layer = (\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n\n  # Total attention FLOPs = (num_global_layers * FLOPs_per_global) + (num_local_layers * FLOPs_per_local)\n  noncausal_attention_flops = (\n      num_global_layers * global_attention_flops_per_layer + num_local_layers * local_attention_flops_per_layer\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Convert to TFLOPs and multiply by 3 for fwd/bwd pass\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  # Learnable weights (FFN, QKV, Projections) are present in every layer.\n  learnable_weight_tflops = ((total_ffn_flops + qkv_flops + projection_flops) * num_layers + embedding_flops) * 3 / 10**12\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "functionality": "Calculates the training TeraFLOPs (TFLOPs) per device for transformer models that use a mixed pattern of global (full) and local (sliding window) attention layers.",
            "usage": "This function is used for performance modeling. Call it with a configuration object and pre-calculated FLOPs for FFN, QKV, projection, and embedding layers, along with the attention pattern length. It returns a tuple containing the TFLOPs for the attention mechanism and the TFLOPs for learnable weight computations."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_calculate_chunked_attention_flops_per_layer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size):\n  \"\"\"Calculates the non-causal FLOPs for a single layer of chunked attention.\"\"\"\n  num_chunks = seq_len // chunk_size\n  rem_chunk_size = seq_len % chunk_size\n  # The complexity of chunked attention is the sum of squares of chunk lengths.\n  chunked_complexity = (num_chunks * chunk_size**2) + (rem_chunk_size**2)\n  # The formula for non-causal attention FLOPs is 4 * B * complexity * H * D,\n  # where B=batch_size, H=num_heads, D=head_dim.\n  return 4 * config.per_device_batch_size * chunked_complexity * config.num_query_heads * config.head_dim",
        "analysis": {
            "module_type": "chunked_attention_flops_calculator",
            "purpose": "Calculates the non-causal Floating Point Operations (FLOPs) for a single layer of chunked attention.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the number of full chunks and the size of the remaining chunk based on sequence length and chunk size.",
                "Compute the chunked complexity as the sum of the squares of the chunk lengths.",
                "Calculate the total non-causal FLOPs using the formula: 4 * batch_size * complexity * num_heads * head_dim."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [],
            "parameters": {
                "config.per_device_batch_size": "The batch size processed by a single device.",
                "config.num_query_heads": "The number of query heads in the attention layer.",
                "config.head_dim": "The dimension of each attention head.",
                "seq_len": "The total length of the input sequence.",
                "chunk_size": "The size of each chunk for the attention computation."
            },
            "notes": [
                "The function calculates FLOPs for non-causal attention, which would be halved for causal attention.",
                "The complexity formula `(num_chunks * chunk_size**2) + (rem_chunk_size**2)` is an approximation for the sum of squares of chunk lengths, which is more efficient than the standard attention complexity of `seq_len**2`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_attention_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_attention_tflops(config):\n  \"\"\"\n  Calculates attention-only training TFLOPs for Llama4's specific architecture,\n  which has an alternating pattern of global and chunked attention layers.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n  seq_len = config.max_target_length\n  chunk_size = config.chunk_attn_window_size\n\n  # Determine number of global vs. chunked layers based on the NoPE interval.\n  # A \"NoPE\" layer uses global attention.\n  num_global_layers = num_layers // config.nope_layer_interval\n  num_chunked_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention, non-causal)\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * seq_len**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single chunked attention layer (non-causal)\n  chunked_attention_flops_per_layer = _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size)\n\n  # Total non-causal attention FLOPs is the sum of all global and all chunked layers\n  noncausal_attention_flops = (num_global_layers * global_attention_flops_per_layer) + (\n      num_chunked_layers * chunked_attention_flops_per_layer\n  )\n\n  # Apply causal mask and convert to TFLOPs (multiply by 3 for fwd/bwd pass)\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  return attention_tflops",
        "analysis": {
            "functionality": "Calculates the attention-only training TeraFLOPs (TFLOPs) for a Llama4-style model architecture, which features an alternating pattern of global and chunked attention layers.",
            "usage": "This function is used for performance estimation. Call it with a configuration object that contains model parameters such as `num_decoder_layers`, `max_target_length`, `chunk_attn_window_size`, `nope_layer_interval`, `per_device_batch_size`, `num_query_heads`, and `head_dim`. It returns a single float value representing the calculated TFLOPs."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mla_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mla_tflops_per_device(config):\n  \"\"\"Calculate Multi-Head Latent Attention TFLOP\"\"\"\n  batch_len = config.per_device_batch_size * config.max_target_length\n  qk_head_dim_sum = config.qk_nope_head_dim + config.qk_rope_head_dim\n  # calculate mla query projection\n  if config.q_lora_rank == 0:\n    q_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * qk_head_dim_sum\n  else:\n    # calculate query down and up flops\n    q_flops = (\n        2\n        * batch_len\n        * (config.emb_dim * config.q_lora_rank + config.q_lora_rank * config.num_query_heads * qk_head_dim_sum)\n    )\n  # calculate mla kv projection with down and up flops\n  kv_flops = (\n      2\n      * batch_len\n      * (\n          config.emb_dim * (config.kv_lora_rank + config.qk_rope_head_dim)\n          + config.kv_lora_rank * config.num_query_heads * (config.qk_nope_head_dim + config.v_head_dim)\n      )\n  )\n  qkv_flops = q_flops + kv_flops\n\n  attention_flops = (\n      2 * batch_len * config.max_target_length * config.num_query_heads * (qk_head_dim_sum + config.v_head_dim)\n  )\n  projection_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * config.v_head_dim\n  return qkv_flops, attention_flops, projection_flops",
        "analysis": {
            "functionality": "This function calculates the per-device Floating Point Operations (FLOPs) for a Multi-Head Latent Attention (MLA) layer, breaking down the cost into three components: QKV projection, the attention mechanism itself, and the final output projection.",
            "usage": "To use this function, pass a configuration object containing model parameters such as batch size, sequence length, embedding dimensions, head counts, and LoRA ranks. It returns a tuple of three scalar values: (qkv_flops, attention_flops, projection_flops)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_ffn_mamtul_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_ffn_mamtul_tflops_per_device(config, mlp_dim):\n  \"\"\"Helper function to calculate matmul TFLOP in ffn based on MLP dimension.\n\n  Applies to:\n    - Dense FFN layers (mlp_dim = config.mlp_dim).\n    - MoE FFN layers (mlp_dim = config.moe_mlp_dim),\n      need to scale by shared_experts or num_experts_per_tok.\n  \"\"\"\n  ffn1_flops = (\n      2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim * len(config.mlp_activations)\n  )\n  ffn2_flops = 2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim\n  return ffn1_flops + ffn2_flops",
        "analysis": {
            "module_type": "ffn_matmul_tflops_calculator",
            "purpose": "Calculates the total theoretical floating-point operations (FLOPs) for the matrix multiplications within a single Feed-Forward Network (FFN) layer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the FLOPs for the first FFN matrix multiplication(s), scaling by the number of MLP activations.",
                "Calculate the FLOPs for the second FFN matrix multiplication.",
                "Return the sum of the FLOPs from both multiplications."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model and training parameters like `per_device_batch_size`, `max_target_length`, `emb_dim`, and `mlp_activations`.",
                "mlp_dim": "The intermediate dimension of the MLP layer, which could be for a dense FFN or a single expert in an MoE layer."
            },
            "notes": [
                "This function is a helper used to calculate FLOPs for both standard dense FFN layers and individual experts in Mixture-of-Experts (MoE) layers.",
                "The calculation for `ffn1_flops` is scaled by `len(config.mlp_activations)`, which accommodates gated MLP architectures (like SwiGLU) that use multiple parallel linear layers in the first stage.",
                "The function returns the raw FLOP count; conversion to TFLOPs (dividing by 10^12) is handled by the calling functions."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_routed_and_shared_ffn_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_routed_and_shared_ffn_tflops_per_device(config):\n  \"\"\"Helper function to calculate DeepSeek-style ffn TFLOP\"\"\"\n  gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n  # Due to the mixed decoder layers, the flops is multiplied by num of layers for both dense and moe\n  num_dense_layers, num_moe_layers = get_dense_moe_layers(config)\n  dense_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * num_dense_layers\n  shared_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.shared_experts\n  routed_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.num_experts_per_tok\n  moe_ffn_flops = (gate_flops + shared_experts_flops + routed_experts_flops) * num_moe_layers\n  total_ffn_flops = dense_ffn_flops + moe_ffn_flops\n  return total_ffn_flops",
        "analysis": {
            "module_type": "tflops_calculator",
            "purpose": "Calculates the total TeraFLOPs for the Feed-Forward Network (FFN) component of a model with a DeepSeek-style mixed architecture of dense and Mixture-of-Experts (MoE) layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the TFLOPs for the MoE gating mechanism.",
                "Call `get_dense_moe_layers` to determine the number of dense and MoE layers based on the model configuration.",
                "Calculate the TFLOPs for all dense FFN layers by calling `calculate_ffn_mamtul_tflops_per_device` and scaling by the number of dense layers.",
                "Calculate the TFLOPs for the shared experts within the MoE layers.",
                "Calculate the TFLOPs for the routed (non-shared) experts activated per token within the MoE layers.",
                "Sum the gate, shared, and routed expert TFLOPs and scale by the number of MoE layers.",
                "Sum the TFLOPs from the dense layers and the MoE layers to get the total FFN TFLOPs.",
                "Return the total FFN TFLOPs."
            ],
            "output": {
                "shape": "Scalar (total FFN TFLOPs)"
            },
            "dependencies": [
                "get_dense_moe_layers",
                "calculate_ffn_mamtul_tflops_per_device"
            ],
            "parameters": {
                "config": "A configuration object containing model dimensions and architecture details like `per_device_batch_size`, `max_target_length`, `emb_dim`, `num_experts`, `mlp_dim`, `moe_mlp_dim`, `shared_experts`, and `num_experts_per_tok`."
            },
            "notes": [
                "This function is specifically designed for models with a 'DeepSeek-style' architecture, which mixes standard dense FFN layers with MoE layers.",
                "The calculation separately computes the contributions from dense layers, MoE gating, MoE shared experts, and MoE routed experts before summing them."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_dense_moe_layers",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_dense_moe_layers(config):\n  \"\"\"Helper function to calculate number of dense and moe layers\"\"\"\n  if config.decoder_block == DecoderBlockType.DEEPSEEK:\n    num_dense_layers = config.first_num_dense_layers\n    num_moe_layers = config.num_decoder_layers - config.first_num_dense_layers\n    return num_dense_layers, num_moe_layers\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    num_moe_layers = config.num_decoder_layers // config.interleave_moe_layer_step\n    num_dense_layers = config.num_decoder_layers - num_moe_layers\n  else:\n    raise ValueError(\"Currently we only support DeepSeek and Llama4 calculation.\")\n\n  return num_dense_layers, num_moe_layers",
        "analysis": {
            "functionality": "Calculates the number of dense and Mixture-of-Experts (MoE) layers for specific model architectures (DeepSeek, Llama4) based on a configuration object.",
            "usage": "Call this function with a configuration object that specifies the `decoder_block` type and other relevant parameters (e.g., `num_decoder_layers`, `first_num_dense_layers`, `interleave_moe_layer_step`). It returns a tuple containing the number of dense layers and the number of MoE layers, e.g., `(num_dense, num_moe)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma3_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma3_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Gemma3 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.image_size_for_vit  # Gemma3 default 896\n  embed_dim = config.emb_dim  # text embedding dim after projection\n  # Values below are hardcoded in Gemma3VisionEncoderLayer\n  patch_size = 14\n  hidden_dim = 1152\n  intermediate_dim = 4304\n  num_layers = 27\n  vision_exit_pooling_window = 4\n\n  # 1. Patch embedding (Conv2D)\n  num_patches_h = H // patch_size\n  num_patches_w = W // patch_size\n  seq_len = num_patches_h * num_patches_w  # 64*64=4096\n  patch_embed_flops = 2 * B * seq_len * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. gemma3.Encoder: num_layers * gemma3.Encoder1DBlock\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 4. VisionEmbedder\n  seq_len_after_pooling = (num_patches_h // vision_exit_pooling_window) * (num_patches_w // vision_exit_pooling_window)\n  vision_embedder_flops = 2 * B * seq_len_after_pooling * hidden_dim * embed_dim  # One linear projection\n\n  # Learnable weights summation\n  learnable_weight_flops = patch_embed_flops + encoder_flops + vision_embedder_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * vision_embedder_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "module_type": "tflops_calculator",
            "purpose": "Estimates the total, learnable weight, and attention TFLOPs per device for a Gemma3 vision encoder based on a given configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract parameters from the config object and define hardcoded model dimensions.",
                "Calculate FLOPs for the patch embedding layer (Conv2D).",
                "Calculate per-layer FLOPs for QKV projection, attention matrix multiplication, output projection, and MLP layers.",
                "Sum FLOPs across all layers for learnable weights (QKV, projections, MLP) and attention multiplications separately.",
                "Calculate FLOPs for the final vision embedder projection.",
                "Aggregate all learnable weight FLOPs from patch embedding, encoder weights, and the vision embedder.",
                "Multiply the learnable weight FLOPs by 3 to account for forward, backward, and optimizer passes, with special handling if `freeze_vision_encoder_params` is true.",
                "Convert learnable weight FLOPs and attention FLOPs to TFLOPs by dividing by 1e12.",
                "Sum the learnable weight TFLOPs and attention TFLOPs to get the total."
            ],
            "output": {
                "shape": "A tuple of three scalar values: (total_tflops, learnable_weight_tflops, attention_tflops)."
            },
            "dependencies": [],
            "parameters": {
                "per_device_batch_size": "The batch size used on a single device.",
                "num_channels_for_vit": "The number of channels in the input image (e.g., 3 for RGB).",
                "image_size_for_vit": "The height and width of the square input image.",
                "emb_dim": "The dimension of the text embedding that the vision features are projected into.",
                "freeze_vision_encoder_params": "Boolean flag indicating if the vision encoder parameters are frozen during training, which affects the TFLOPs calculation for backward and optimizer steps."
            },
            "notes": [
                "The function uses hardcoded architectural values specific to the Gemma3 vision encoder, such as `patch_size=14`, `hidden_dim=1152`, and `num_layers=27`.",
                "The returned `learnable_weight_tflops` accounts for the forward pass, backward pass, and optimizer update (approximated by a 3x multiplier).",
                "The returned `attention_tflops` only accounts for the attention matrix multiplication FLOPs (forward pass) and is not multiplied to account for backward/optimizer steps.",
                "The returned `total_tflops` is the sum of `learnable_weight_tflops` and `attention_tflops`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Llama4 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.tile_size_for_vit\n  patch_size = config.patch_size_for_vit\n  hidden_dim = config.hidden_size_for_vit\n  intermediate_dim = config.intermediate_size_for_vit\n  num_layers = config.num_hidden_layers_for_vit\n  pixel_shuffle_fc1_out_dim = config.projector_input_dim_for_vit  # 4096\n  pixel_shuffle_fc2_out_dim = config.projector_output_dim_for_vit  # 4096\n  base_emb_dim = config.base_emb_dim\n  pixel_shuffle_ratio = config.pixel_shuffle_ratio_for_vit  # 0.5\n  num_patches = (H // patch_size) * (W // patch_size)  # 24*24 = 576\n  pixel_shuffle_tokens = num_patches * pixel_shuffle_ratio**2  # 144\n\n  # 1. Llama4UnfoldConvolution (flops by linear projection)\n  # lax.conv_general_dilated_patches extracts patches through reshaping/indexing without flops\n  # Each patch: C * patch_size * patch_size -> hidden_dim\n  patch_embed_flops = 2 * B * num_patches * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. Llama4VisionEncoder: num_layers * (qkv + att_projection + mlp)\n  seq_len = num_patches + 1  # +1 for class token, so 577\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)  # Q, K, V projections\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim  # Attention scores and weighted sum\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  vision_encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 3. Llama4VisionPixelShuffleMLP\n  # (B, 144, 5632) -> (B, 144, 4096) -> (B, 144, 4096)\n  pixel_shuffle_fc1_flops = 2 * B * pixel_shuffle_tokens * intermediate_dim * pixel_shuffle_fc1_out_dim\n  pixel_shuffle_fc2_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * pixel_shuffle_fc2_out_dim\n  pixel_shuffle_total_flops = pixel_shuffle_fc1_flops + pixel_shuffle_fc2_flops\n\n  # 4. Llama4MultiModalProjector: (B, 144, 5120) x (5120, base_emb_dim)\n  projector_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * base_emb_dim\n\n  # Learnable weights: all matmuls above\n  learnable_weight_flops = patch_embed_flops + vision_encoder_flops + pixel_shuffle_total_flops + projector_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * projector_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "module_type": "llama4_vision_tflops_calculator",
            "purpose": "Estimates the TeraFLOPs (TFLOPs) per device for the Llama4 vision encoder, breaking down the calculation into learnable weights and attention components.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract vision model parameters (e.g., batch size, image size, patch size, hidden dimensions) from the input config object.",
                "Calculate the number of patches and tokens after pixel shuffling.",
                "Compute FLOPs for the patch embedding projection.",
                "Compute FLOPs for each layer of the vision encoder, including QKV projections, attention score calculations, output projections, and MLP layers.",
                "Compute FLOPs for the pixel shuffle MLP.",
                "Compute FLOPs for the final multi-modal projector.",
                "Sum the FLOPs from all learnable weight components.",
                "Adjust the learnable weight FLOPs for training (forward, backward, optimizer) based on whether the vision encoder is frozen.",
                "Convert FLOP counts to TFLOPs by dividing by 1e12.",
                "Return the total TFLOPs, learnable weight TFLOPs, and attention TFLOPs."
            ],
            "output": {
                "shape": "A tuple of three scalar floats: (total_tflops, learnable_weight_tflops, attention_tflops)."
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing parameters for the Llama4 vision model, such as `per_device_batch_size`, `tile_size_for_vit`, `patch_size_for_vit`, `hidden_size_for_vit`, `num_hidden_layers_for_vit`, and `freeze_vision_encoder_params`."
            },
            "notes": [
                "The function provides a theoretical estimate of computational cost for a ViT-style vision encoder.",
                "The calculation for learnable weights is multiplied by 3 to account for the forward pass, backward pass, and optimizer update, unless `config.freeze_vision_encoder_params` is true.",
                "The calculation assumes a class token is added to the sequence of patches.",
                "The FLOPs are separated into those from matrix multiplications on learnable weights and those from the attention mechanism's score computation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_vision_encoder_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_vision_encoder_tflops(config):\n  \"\"\"Calculate vision encoder TFLOPs per prefill step per device.\"\"\"\n  if config.model_name.startswith(\"gemma3\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_gemma3_vision_layers_tflops_per_device(\n        config\n    )\n  elif config.model_name.startswith(\"llama4\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_llama4_vision_layers_tflops_per_device(\n        config\n    )\n  else:\n    max_logging.log(\n        f\"Vision encoder TFLOPs calculation not implemented for model {config.model_name}, counting as 0 for now.\"\n    )\n    mm_total_tflops = mm_learnable_weight_tflops = mm_attention_tflops = 0\n\n  return mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops",
        "analysis": {
            "functionality": "Calculates the TeraFLOPs (TFLOPs) for a vision encoder during a prefill step on a single device, dispatching the calculation based on the model name specified in the configuration.",
            "usage": "Call this function with a configuration object. It returns a tuple containing the total TFLOPs, learnable weight TFLOPs, and attention TFLOPs. The `config` object must have a `model_name` attribute (e.g., 'gemma3', 'llama4') to select the correct calculation method. If the model is unsupported, it returns (0, 0, 0)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tflops_training_per_device(config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  # MLP flops\n  if config.num_experts > 1:\n    # calculation based on dropless implementation\n    if config.decoder_block in (DecoderBlockType.DEEPSEEK, DecoderBlockType.LLAMA4):\n      total_ffn_flops = calculate_routed_and_shared_ffn_tflops_per_device(config)\n    else:\n      gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n      total_ffn_flops = (\n          gate_flops + calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * config.num_experts_per_tok\n      )\n  else:\n    total_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim)\n\n  # Attention flops\n  if config.attention_type == \"mla\":\n    qkv_flops, noncausal_attention_flops, projection_flops = calculate_mla_tflops_per_device(config)\n  else:\n    qkv_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * (config.num_query_heads + 2 * config.num_kv_heads)\n        * config.head_dim\n    )\n    noncausal_attention_flops = (\n        4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n    )\n    projection_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * config.num_query_heads\n        * config.head_dim\n    )\n\n  # Divide attention flops by 2 due to causal mask\n  # References:\n  # NVIDIA/Megatron-LM (2025 March): https://github.com/NVIDIA/Megatron-LM/blob/250b79415dcc4b660521273c87f15334c804eeae/megatron/training/training.py#L361-L362\n  # NVIDIA/NeMo (2025 April): https://github.com/NVIDIA/NeMo/blob/ba4d6d116463de512ff0cfc14641aa6cf4577a42/nemo/utils/flops_formulas.py#L259-L272\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Embedding flops\n  embedding_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.vocab_size\n\n  # Combine flops with number of decoder layers\n  if config.decoder_block == DecoderBlockType.GEMMA2:\n    attention_tflops, learnable_weight_tflops = calculate_gemma2_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops\n    )\n  elif config.decoder_block == DecoderBlockType.GEMMA3:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=6\n    )\n  elif config.decoder_block == DecoderBlockType.GPT_OSS:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=2\n    )\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    # Use the new helper to calculate attention TFLOPs correctly.\n    attention_tflops = calculate_llama4_attention_tflops(config)\n    # The learnable weight calculation remains the same as it correctly handles Llama4's MoE structure.\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n  elif config.decoder_block == DecoderBlockType.DEEPSEEK:\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n  else:\n    # multiply by 3 for both feed forward and back propagation flops\n    learnable_weight_tflops = (\n        ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  learnable_weight_tflops = learnable_weight_tflops * config.gradient_accumulation_steps\n  attention_tflops = attention_tflops * config.gradient_accumulation_steps\n\n  # DPO includes one additional forward pass per gradient accumulation step\n  if config.use_dpo:\n    reference_model_tflops = learnable_weight_tflops / 3  # additional forward pass\n    reference_model_attention_tflops = attention_tflops / 3\n    attention_tflops = attention_tflops + reference_model_attention_tflops\n  else:\n    reference_model_tflops = 0\n\n  total_tflops = learnable_weight_tflops + attention_tflops + reference_model_tflops\n\n  if config.use_multimodal:\n    # Add vision layers TFLOPs for multimodal models\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_vision_encoder_tflops(config)\n    if log:\n      print(\n          f\"{config.model_name} vision layers per train step:\\n\",\n          f\"Total TFLOPs: {mm_total_tflops:.2f} \\n\",\n          f\"split as {100 * mm_learnable_weight_tflops/mm_total_tflops:.2f}% learnable weight flops\",\n          f\"and {100 * mm_attention_tflops/mm_total_tflops:.2f}% attention flops;\\n\",\n          f\"learnable weight {mm_learnable_weight_tflops:.2f} TFLOPs, attention {mm_attention_tflops:.2f} TFLOPs\",\n      )\n    total_tflops += mm_total_tflops\n    learnable_weight_tflops += mm_learnable_weight_tflops\n    attention_tflops += mm_attention_tflops\n\n  if log:\n    print(\n        \"Per train step:\\n\",\n        f\"Total TFLOPs: {total_tflops:.2f} \\n\",\n        f\"split as {100 * learnable_weight_tflops/total_tflops:.2f}% learnable weight flops\",\n        f\"and {100 * attention_tflops/total_tflops:.2f}% attention flops\",\n    )\n  return total_tflops, learnable_weight_tflops, attention_tflops",
        "analysis": {
            "functionality": "Calculates the total TeraFLOPs (TFLOPs) per device for a single training step, breaking it down into learnable weight and attention TFLOPs. It supports various model architectures, including Mixture-of-Experts (MoE), different attention types, and multimodal vision encoders.",
            "usage": "Call the function with a configuration object containing model and training hyperparameters. It returns a tuple of floats: (total_tflops, learnable_weight_tflops, attention_tflops). The optional `log` argument controls whether the results are printed to the console."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_prefill_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_prefill_tflops_per_device(num_model_parameters, prefill_length, config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  learnable_weight_tflops = 2 * num_model_parameters * prefill_length / jax.device_count() / 1e12\n  noncausal_attention_flops = (\n      4\n      * config.num_query_heads\n      * config.num_decoder_layers\n      * config.head_dim\n      * prefill_length**2\n      / jax.device_count()\n      / 1e12\n  )\n  causal_attention_tflops = noncausal_attention_flops / 2  # due to causality in attention\n  total_tflops = learnable_weight_tflops + causal_attention_tflops\n\n  if log:\n    print(\n        \"Per prefill step per device: \\n\",\n        f\"\\tTotal TFLOPs: {total_tflops:.2f} \\n\",\n        f\"\\t\\tLearnable weight TFLOPs: {learnable_weight_tflops:.2f} \",\n        f\"({100 * learnable_weight_tflops/total_tflops:.2f})% of Total\\n\",\n        f\"\\t\\tCausal attention TFLOPs: {causal_attention_tflops:.2f} \",\n        f\"({100 * causal_attention_tflops/total_tflops:.2f})% of Total\",\n    )\n  return total_tflops, learnable_weight_tflops, causal_attention_tflops",
        "analysis": {
            "module_type": "prefill_tflops_calculator",
            "purpose": "Calculates the total, learnable weight, and causal attention TeraFLOPs per device for a single prefill (forward pass) step.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate TFLOPs for learnable weights using the formula: 2 * num_model_parameters * prefill_length / num_devices / 1e12.",
                "Calculate non-causal attention FLOPs based on model configuration (num_query_heads, num_decoder_layers, head_dim) and prefill_length.",
                "Halve the non-causal attention FLOPs to account for the causal attention mask.",
                "Sum the learnable weight TFLOPs and causal attention TFLOPs to get the total.",
                "Optionally log a detailed breakdown of the TFLOPs.",
                "Return the total, learnable weight, and causal attention TFLOPs."
            ],
            "output": {
                "shape": "A tuple of three scalar float values: (total_tflops, learnable_weight_tflops, causal_attention_tflops)."
            },
            "dependencies": [
                "jax.device_count"
            ],
            "parameters": {
                "num_model_parameters": "The total number of learnable parameters in the model.",
                "prefill_length": "The length of the input sequence for the prefill computation.",
                "config": "A configuration object containing model architecture details like num_query_heads, num_decoder_layers, and head_dim.",
                "log": "A boolean flag to enable or disable printing the TFLOPs breakdown."
            },
            "notes": [
                "The calculation is performed on a per-device basis.",
                "The formula assumes a single forward pass, which is typical for prefill/inference, hence the factor of 2 for learnable weights (vs. 6 for training).",
                "The calculation is based on Appendix B of the PaLM paper (https://arxiv.org/pdf/2204.02311.pdf)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_mesh_axes_used_by_tensor_spec",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_mesh_axes_used_by_tensor_spec(tensor_sharding_spec):\n  \"\"\"\n  Extracts the set of mesh axis names that a tensor's PartitionSpec uses.\n\n  This function inspects a tensor's sharding specification (PartitionSpec) and\n  identifies which mesh axes are actively used for sharding. If a tensor is not\n  sharded (i.e., fully replicated), the resulting set will be empty.\n\n  Args:\n    tensor_sharding_spec: The PartitionSpec of a tensor, which defines how it's partitioned across the mesh.\n    It can be None or contain strings and iterables representing the mesh axes.\n    all_mesh_axis_names: A collection of all available mesh axis names in the current device mesh.\n\n  Returns:\n    A set of strings, where each string is a mesh axis name used by the\n    tensor's sharding spec. Returns an empty set for unsharded tensors.\n  \"\"\"\n  # Flatten the sharding spec, as it can contain nested iterables (e.g., ('data', 'mdl')).\n  tensor_sharding_spec = sum(\n      [\n          [axis] if isinstance(axis, str) else list(axis) if isinstance(axis, Iterable) else []\n          for axis in tensor_sharding_spec\n      ],\n      [],\n  )\n  return tensor_sharding_spec",
        "analysis": {
            "module_type": "sharding_utility",
            "purpose": "Extracts and flattens all mesh axis names from a tensor's sharding specification (PartitionSpec).",
            "input": {
                "shape": "A JAX PartitionSpec, which is typically a tuple or list of strings, None, or other iterables (e.g., P('data', ('mdl1', 'mdl2'))). Not a tensor, so shape is not applicable.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through the elements of the input `tensor_sharding_spec`.",
                "For each element, convert it to a list of axis names (strings) or an empty list if it's not a string or an iterable.",
                "Flatten the resulting list of lists into a single list of all axis names using `sum`."
            ],
            "output": {
                "shape": "A flat list of strings, where each string is a mesh axis name. The length is variable."
            },
            "dependencies": [
                "collections.abc.Iterable"
            ],
            "parameters": {
                "tensor_sharding_spec": "The PartitionSpec of a tensor, which defines how it's partitioned across the mesh. It can contain strings, iterables of strings, and None."
            },
            "notes": [
                "This function is designed to handle nested iterables within the PartitionSpec, such as `('data', 'mdl')`.",
                "It uses the `sum(..., [])` idiom to flatten the list of axis specifications.",
                "If a tensor is fully replicated (i.e., the spec contains no axis names), the function returns an empty list.",
                "Although the docstring mentions returning a set, the implementation returns a list which may contain duplicate axis names if they appear multiple times in the spec."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_get_nontrival_mesh_axes",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _get_nontrival_mesh_axes(mesh):\n  \"\"\"\n  Returns mesh axes from config that are valid and have more than one shard.\n\n  This function identifies which of the predefined potential sharding axes are\n  actually present in the current device mesh and are configured with a size\n  greater than one (i.e., are actually sharded).\n\n  Args:\n    mesh: The device mesh object, which contains information about the mesh topology, including axis names and their sizes.\n\n  Returns:\n    A set of strings, where each string is a mesh axis name that is both\n    pre-configured as a target for sharding and has more than one shard in the mesh.\n  \"\"\"\n\n  target_sharding_axes_config = [\n      \"fsdp\",\n      \"fsdp_transpose\",\n      \"sequence\",\n      \"context\",\n      \"context_autoregressive\",\n      \"tensor\",\n      \"tensor_transpose\",\n      \"tensor_sequence\",\n      \"stage\",\n      \"expert\",\n  ]\n\n  # Filter the target axes to find those that exist in the current mesh\n  # and have a size greater than 1, meaning they are actually used for sharding.\n  return {axis for axis in target_sharding_axes_config if axis in mesh.axis_names and mesh.shape[axis] > 1}",
        "analysis": {
            "module_type": "sharding_axis_filter",
            "purpose": "Filters a predefined list of potential sharding axes to find those that are actively used (size > 1) in the provided device mesh.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a static list of target sharding axis names.",
                "Iterate through the target list using a set comprehension.",
                "For each axis, check if it exists in the input `mesh.axis_names`.",
                "For each existing axis, check if its size in `mesh.shape` is greater than 1.",
                "Collect all axes that satisfy both conditions into a set and return it."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.sharding.Mesh"
            ],
            "parameters": {},
            "notes": [
                "The input `mesh` is expected to be a jax.sharding.Mesh object.",
                "The function identifies axes that are 'nontrivial', meaning they are actually used for sharding (size > 1).",
                "The list of potential sharding axes is hardcoded within the function.",
                "The return type is a Python `set` of strings, not a tensor."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_analyze_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _analyze_sharding(params, mesh, valid_target_mesh_axes):\n  \"\"\"\n  Analyzes parameters to find which are unsharded on any valid mesh axis.\n\n  This function iterates through all parameters in a model, checking their\n  sharding specifications. It identifies parameters that are not sharded along any\n  of the provided valid target axes (i.e., they are fully replicated across these axes).\n\n  Args:\n    params: A PyTree of model parameters.\n    mesh: The device mesh object.\n    valid_target_mesh_axes: A set of mesh axis names that are considered valid targets for sharding.\n\n  Returns:\n    A tuple containing:\n      - unsharded_params_total_size (int): The total size (number of elements) of all parameters found to be\n        unsharded on the target axes.\n      - problematic_tensors_details (list): A list of dictionaries, where each\n        dictionary contains details about a tensor that is not sharded on any of the target axes.\n  \"\"\"\n  unsharded_params_total_size = 0  # Initialize a counter for the size of unsharded parameters.\n  problematic_tensors_details = []  # Initialize a list to store details of problematic tensors.\n\n  # Get a flattened list of all parameters (leaves) in the PyTree, along with their paths.\n  all_params_leaves = jtu.tree_leaves_with_path(params)\n\n  for path, p_leaf in all_params_leaves:  # Iterate over each parameter leaf\n    param_name_str = jtu.keystr(path)  # Convert the tree path to a readable string\n\n    # Check that sharding and spec exist and are valid\n    sharding = getattr(p_leaf, \"sharding\", None)\n    spec = getattr(sharding, \"spec\", None)\n    assert sharding is not None and spec is not None and isinstance(spec, P), (\n        f\"Parameter '{param_name_str}' is missing a valid '.sharding.spec'.\"\n        \"Expected 'p_leaf.sharding.spec' to be a non-null 'partitionspec'.\"\n    )\n\n    current_sharding_spec = p_leaf.sharding.spec  # Extract the current tensor's sharding spec\n    # Identify axes used for sharding\n    mesh_axes_used = get_mesh_axes_used_by_tensor_spec(current_sharding_spec)\n    # Check if the parameter is sharded on all the valid target axes.\n    is_sharded_on_all_target_axis = all(axis in mesh_axes_used for axis in valid_target_mesh_axes)\n\n    # If the parameter is not sharded on all of the target axes, it's considered \"problematic.\"\n    if not is_sharded_on_all_target_axis:\n      unsharded_params_total_size += p_leaf.size  # Add to total unsharded parameter size\n      unsharded_axes = set(valid_target_mesh_axes) - set(mesh_axes_used)\n      # Add detailed info to list of problematic tensors\n      problematic_tensors_details.append(\n          {\n              \"name\": param_name_str,  # Tensor name\n              \"size\": p_leaf.size,  # tensor size\n              \"shape\": p_leaf.shape,  # tensor shape\n              \"spec\": str(current_sharding_spec),  # Tensor sharding spec as string\n              \"available_axes\": sorted(list(valid_target_mesh_axes)),  # Axes that could be used for sharding\n              \"unsharded_axes\": sorted(list(unsharded_axes)),  # Unsharded axes\n          }\n      )\n  # Return the total size of unsharded parameters and the list of problematic tensors.\n  return unsharded_params_total_size, problematic_tensors_details",
        "analysis": {
            "functionality": "Analyzes a PyTree of model parameters to identify and report on tensors that are not sharded along a specified set of valid mesh axes.",
            "usage": "This function is used to audit the sharding configuration of a model. It takes a PyTree of parameters (`params`), a device mesh (`mesh`), and a set of target mesh axes (`valid_target_mesh_axes`). It returns a tuple containing the total size of all unsharded parameters and a list of dictionaries with detailed information about each unsharded tensor. This information can be used to ensure the model is sufficiently sharded for efficient distributed training."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_raise_if_unsharded_exceeds_tolerance",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _raise_if_unsharded_exceeds_tolerance(unsharded_size, total_size, tolerance, problematic_tensors_details):\n  \"\"\"\n  Raises an AssertionError if the percentage of unsharded parameters exceeds the given tolerance.\n\n  This function calculates the proportion of model parameters that are unsharded\n  and compares it against a specified tolerance. If the tolerance is exceeded,\n  it constructs and raises a detailed error message.\n\n  Args:\n    unsharded_size: The total size of parameters not sharded on target axes.\n    total_size: The total size of all parameters in the model.\n    tolerance: A float (e.g., 0.05 for 5%) representing the maximum allowed percentage of unsharded parameters.\n    problematic_tensors_details: A list of details about the unsharded tensors,\n    used to generate an informative error message.\n\n  Raises:\n    AssertionError: If the percentage of unsharded parameters is greater than the tolerance.\n  \"\"\"\n  if total_size <= 0:\n    raise ValueError(\"Total size must be greater than zero.\")\n\n  # Calculate the percentage of unsharded parameters.\n  unsharded_param_perc = unsharded_size / total_size\n\n  # If the percentage is over the tolerance, prepare and raise an error.\n  if unsharded_param_perc > tolerance:\n    # Sort the problematic tensors by size to show the largest ones first.\n    problematic_tensors_details.sort(key=lambda x: x[\"size\"], reverse=True)\n\n    # Begin constructing the error message.\n    error_msg_lines = [\n        f\"Unsharded parameter percentage ({unsharded_param_perc:.2%})\" f\"exceeds tolerance ({tolerance:.2%}).\"\n    ]\n    # Add a header explaining the issue.\n    error_msg_lines.append(\n        \"The following large tensors are replicated (unsharded) but could be sharded on at \"\n        \"least one of the available axes:\"\n    )\n    # Add details for the top 5 largest problematic tensors.\n    for detail in problematic_tensors_details[:5]:  # Show top 5 largest problematic tensors\n      error_msg_lines.append(\n          f\" - Name: {detail['name']}(Size: {detail['size']}, Shape: {detail['spec']}, Spec: {detail['spec']}) \"\n          f\" is unsharded on axis: {detail['unsharded_axes']}\"\n          f\" could be sharded on: {detail['available_axes']}\"\n      )\n\n    # Raise the assertion error with the combined, formatted message.\n    raise AssertionError(\"\\n\".join(error_msg_lines))",
        "analysis": {
            "module_type": "sharding_validation_helper",
            "purpose": "Calculates the percentage of unsharded model parameters and raises a detailed AssertionError if this percentage exceeds a specified tolerance.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Validate that `total_size` is greater than zero, raising a ValueError if not.",
                "Calculate the percentage of unsharded parameters by dividing `unsharded_size` by `total_size`.",
                "Check if the calculated percentage is greater than the `tolerance`.",
                "If the tolerance is exceeded, sort the `problematic_tensors_details` list by tensor size in descending order.",
                "Construct a detailed, multi-line error message that includes the unsharded percentage, the tolerance, and details of the top 5 largest problematic tensors.",
                "Raise an AssertionError with the formatted error message."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "tolerance": "A float (e.g., 0.05 for 5%) representing the maximum allowed percentage of unsharded parameters."
            },
            "notes": [
                "This function does not return a value; its purpose is to raise an exception if the validation fails.",
                "The error message is specifically formatted to be informative, listing the top 5 largest unsharded tensors to aid in debugging sharding configurations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#assert_params_sufficiently_sharded",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def assert_params_sufficiently_sharded(params, mesh, tolerance):\n  \"\"\"\n  Asserts that the total size of replicated parameters is within a given tolerance.\n\n  This is the main function that orchestrates the sharding analysis. It determines\n  the total number of parameters, identifies valid sharding axes, analyzes the\n  sharding of all parameters, and then raises an error if the amount of\n  unsharded parameters exceeds the specified tolerance.\n\n  Args:\n    params: A PyTree of model parameters.\n    mesh: The device mesh object.\n    tolerance: A float representing the maximum allowed percentage of unsharded parameters.\n  \"\"\"\n  # Calculate the total size of all parameters in the model.\n  total_num_params = max_utils.calculate_bytes_from_pytree(params)\n\n  # Get the set of nontrival mesh axes that can be used for sharding.\n  valid_target_mesh_axes = _get_nontrival_mesh_axes(mesh)\n  # If there are no valid axes to shard along, there's nothing to check, so we can exit.\n  if not valid_target_mesh_axes:\n    return  # Exit early\n\n  # Analyze the parameters to find the total size of unsharded parameters\n  # and get details on which tensors are problematic.\n  unsharded_params_total_size, problematic_tensors_details = _analyze_sharding(params, mesh, valid_target_mesh_axes)\n\n  # Check if the amount of unsharded parameters is within the tolerance and\n  # raise an exception if it is not.\n  _raise_if_unsharded_exceeds_tolerance(\n      unsharded_params_total_size, total_num_params, tolerance, problematic_tensors_details\n  )",
        "analysis": {
            "functionality": "Asserts that the total size of replicated (unsharded) model parameters is within a specified tolerance by analyzing the sharding of each parameter against the available device mesh axes.",
            "usage": "Call this function with a PyTree of model parameters, a JAX device mesh, and a float tolerance (e.g., 0.05 for 5%). It will raise an AssertionError if the percentage of unsharded parameters exceeds the tolerance, providing details on the largest unsharded tensors. It returns nothing if the check passes."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#apply_gradient_clipping",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def apply_gradient_clipping(raw_grads, state, clipping_threshold):\n  \"\"\"Applies gradient clipping to raw gradients, with special handing for FLAX fp8 stats.\n\n  Args:\n    raw_grads: A pytree of raw gradients.\n    state: The current optimizer state.\n    clipping_threshold: The gradient clipping threshold.\n\n  Returns:\n    A pytree of clipped gradients.\n  \"\"\"\n  gradient_clip_transformation = optax.clip_by_global_norm(clipping_threshold)\n  if OVERWRITE_WITH_GRADIENT in raw_grads:\n    # Scales + Amax History for Delayed Tensor Scaling SHOULD NOT be clipped or affect clipping\n    fp8_stats = raw_grads.pop(OVERWRITE_WITH_GRADIENT)\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n    grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n    raw_grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n  else:\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n\n  return grads",
        "analysis": {
            "module_type": "gradient_clipping_utility",
            "purpose": "Applies global norm gradient clipping to a pytree of gradients, with special handling to exclude FLAX FP8 statistics from the clipping process.",
            "input": {
                "shape": "A pytree of gradients, where leaves are tensors of various shapes.",
                "dtype": "float"
            },
            "processing_steps": [
                "Initializes an `optax.clip_by_global_norm` transformation with the given threshold.",
                "Checks if the `raw_grads` pytree contains the special key `_overwrite_with_gradient` for FP8 statistics.",
                "If the key exists, it separates the FP8 stats from the gradients to prevent them from being clipped.",
                "Applies the `optax` clipping transformation to the gradients (excluding FP8 stats if present).",
                "If FP8 stats were separated, they are re-inserted into the clipped gradients pytree.",
                "Returns the pytree of clipped gradients."
            ],
            "output": {
                "shape": "A pytree of clipped gradients with the same structure and shape as the input `raw_grads`."
            },
            "dependencies": [
                "optax.clip_by_global_norm"
            ],
            "parameters": {
                "raw_grads": "A pytree of raw gradients to be clipped.",
                "state": "The current optimizer state, required by the `optax` update function.",
                "clipping_threshold": "The maximum global norm for the gradients.",
                "OVERWRITE_WITH_GRADIENT": "A constant string key ('_overwrite_with_gradient') used to identify and separate FP8 statistics (scales and amax history) from the gradients that should be clipped."
            },
            "notes": [
                "This function is designed to work with FLAX's FP8 training, where certain statistics related to delayed tensor scaling should not be part of the gradient norm calculation or be clipped.",
                "The function modifies the input `raw_grads` pytree in-place by adding back the FP8 stats if they were popped."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_nested_value",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_nested_value(dictionary, nested_key, default=None):\n  \"\"\"\n  Retrieves a value from a nested key in a dictionary.\n\n  Args:\n      dictionary: The dictionary to search in.\n      nested_key: A tuple representing the nested key, e.g., ('level1', 'level2', 'key').\n      default: The value to return if the nested key is not found.\n\n  Returns:\n      The value associated with the nested key, or the default value if not found.\n  \"\"\"\n  current_level = dictionary\n\n  for key in nested_key:\n    if not isinstance(current_level, dict) or key not in current_level:\n      return default\n    current_level = current_level[key]\n  return current_level",
        "analysis": {
            "module_type": "nested_dictionary_accessor",
            "purpose": "Retrieves a value from a nested dictionary using a tuple of keys representing the path.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize `current_level` with the input `dictionary`.",
                "Iterate through each key in the `nested_key` tuple.",
                "Check if the `current_level` is a dictionary and if the current `key` exists within it.",
                "If the check fails, return the `default` value immediately.",
                "If the check passes, update `current_level` to the value at the current key.",
                "After the loop finishes, return the final `current_level`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "dictionary": "The dictionary to search in.",
                "nested_key": "A tuple representing the nested key path, e.g., ('level1', 'level2', 'key').",
                "default": "The value to return if the nested key is not found."
            },
            "notes": [
                "The function safely traverses the dictionary, returning a default value if any intermediate key is missing or if an intermediate value is not a dictionary, thus preventing potential KeyErrors or TypeErrors."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_decode_state(apply_fn, params) -> train_state.TrainState:\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState(step=0, apply_fn=apply_fn, params=params, tx=None, opt_state={})  # type: ignore\n  return state",
        "analysis": {
            "module_type": "state_initializer",
            "purpose": "Initializes a Flax TrainState object specifically for decoding/inference, with a null optimizer state.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Instantiates a `flax.training.train_state.TrainState` object.",
                "Initializes the state with `step=0`, the provided `apply_fn` and `params`, a null optimizer `tx=None`, and an empty `opt_state`.",
                "Returns the created `TrainState` object."
            ],
            "output": {
                "shape": "An instance of flax.training.train_state.TrainState."
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {
                "apply_fn": "The model's forward pass function, typically `model.apply`.",
                "params": "A PyTree of the model's learned parameters."
            },
            "notes": [
                "This function is used to create a state for inference where no optimizer is needed.",
                "The optimizer-related fields `tx` and `opt_state` are explicitly set to null values (`None` and `{}`)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_training_state(apply_fn, params, tx):\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState.create(apply_fn=apply_fn, params=params, tx=tx)\n  return state",
        "analysis": {
            "module_type": "state_initializer",
            "purpose": "Initializes a Flax TrainState object for training, bundling the model's apply function, parameters, and an Optax optimizer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `train_state.TrainState.create` with the provided `apply_fn`, `params`, and `tx`."
            ],
            "output": {
                "shape": "Returns a `flax.training.train_state.TrainState` object, which is a PyTree."
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {
                "apply_fn": "The model's forward pass function, typically `model.apply`.",
                "params": "A PyTree containing the model's learnable parameters.",
                "tx": "An Optax gradient transformation (optimizer) to be used for training."
            },
            "notes": [
                "The docstring 'Init train state with null opt state for decode.' appears to be incorrect for this function, as it initializes a full training state including the optimizer state via the `.create` method. The function `init_decode_state` is used for creating a state without an optimizer for decoding/inference."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_initial_state(model, tx, config, is_training, key):\n  \"\"\"\n  We pass in \"static\" objects like model, tx, config as JAX compares them by\n  object hash, and instantiating them inside causes pjit top-level annotations\n  to fail to match as pytree prefixes if we re-instantiate.\n\n  Args: model, tx, config, is_training, key\n  \"\"\"\n  input_shape = (config.micro_batch_size_to_train_on, config.max_target_length)\n  image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n      config.model_name, batch_size=config.micro_batch_size_to_train_on\n  )\n  model_vars = model.init(\n      {\"params\": key, \"dropout\": key, \"aqt\": key},\n      np.ones(input_shape, dtype=jnp.int32),\n      np.ones(input_shape, dtype=jnp.int32),\n      encoder_images=np.ones(image_shape, dtype=jnp.int32) if config.use_multimodal else None,\n      # nnx_method=\"no_op\",\n  )\n  if is_training:\n    return init_training_state(model.apply, model_vars, tx)\n  return init_decode_state(model.apply, model_vars)",
        "analysis": {
            "module_type": "model_state_initializer",
            "purpose": "Initializes the model variables and creates either a training state (with an optimizer) or a decoding state (without an optimizer) based on the `is_training` flag.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define dummy input shapes for text and optionally for images based on the configuration.",
                "Initialize the model variables by calling `model.init` with dummy numpy arrays and a JAX PRNG key.",
                "If `is_training` is true, call `init_training_state` to create a `flax.training.train_state.TrainState` with an optimizer state.",
                "If `is_training` is false, call `init_decode_state` to create a `flax.training.train_state.TrainState` without an optimizer state."
            ],
            "output": {
                "shape": "Returns a `flax.training.train_state.TrainState` object."
            },
            "dependencies": [
                "multimodal_utils.get_dummy_image_shape_for_init",
                "init_training_state",
                "init_decode_state",
                "numpy",
                "jax.numpy"
            ],
            "parameters": {
                "is_training": "A boolean flag that determines whether to create a full training state with an optimizer or a simpler decoding state for inference.",
                "config.use_multimodal": "A boolean flag in the config object that determines whether to initialize with dummy image inputs.",
                "config.micro_batch_size_to_train_on": "Used to determine the batch size dimension for the dummy input shapes.",
                "config.max_target_length": "Used to determine the sequence length dimension for the dummy text input shape."
            },
            "notes": [
                "The function uses dummy `numpy.ones` arrays with shapes derived from the config to initialize the model's parameters.",
                "The docstring highlights that `model`, `tx`, and `config` are passed in as static objects to maintain object identity for JAX's pjit compilation, preventing potential mismatches in pytree prefixes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_decode_state(model, config, rng, mesh, checkpoint_manager):\n  \"\"\"Setup decode state by loading params from a checkpoint.\n  Args:\n    model: the flax model to initialize\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: Checkpoint manager\n\n  Returns:\n    state: state with decode params loaded from the checkpoint\n    state_mesh_annotations: the mesh annotations for the state\n  \"\"\"\n  if not config.load_parameters_path:\n    # generate random params\n    max_logging.log(\"No decode checkpoint specified - generating random weights.\")\n    state, state_mesh_annotations, _, _ = setup_initial_state(\n        model, None, None, config, rng, mesh, checkpoint_manager, False\n    )\n  else:\n    # Load params from checkpoint\n    max_logging.log(f\"Loading decode params from {config.load_parameters_path}\")\n    unboxed_abstract_state, state_mesh_annotations, _ = get_abstract_state(model, None, config, rng, mesh, False)\n    with nn_partitioning.axis_rules(config.logical_axis_rules):\n      params = checkpointing.load_params_from_path(\n          config.load_parameters_path,\n          unboxed_abstract_state.params,\n          config.checkpoint_storage_concurrent_gb,\n          config.checkpoint_storage_use_ocdbt,\n          config.checkpoint_storage_use_zarr3,\n      )\n    state = init_decode_state(None, params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n  return state, state_mesh_annotations",
        "analysis": {
            "module_type": "decode_state_setup",
            "purpose": "Initializes a model state for decoding (inference), either by loading parameters from a checkpoint or by creating random weights.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.load_parameters_path` is provided.",
                "If a path is not provided, call `setup_initial_state` to generate a state with random parameters.",
                "If a path is provided, call `get_abstract_state` to determine the target parameter structure and sharding.",
                "Load parameters from the specified path using `checkpointing.load_params_from_path`.",
                "Initialize a `TrainState` object for decoding using `init_decode_state` with the loaded parameters.",
                "Unbox any `flax.LogicallyPartitioned` objects in the state using `max_utils.unbox_logicallypartioned`.",
                "Return the final state and its corresponding mesh annotations."
            ],
            "output": {
                "shape": "A tuple containing (state, state_mesh_annotations), where 'state' is a PyTree (flax.training.train_state.TrainState) and 'state_mesh_annotations' is a PyTree of sharding specifications."
            },
            "dependencies": [
                "setup_initial_state",
                "get_abstract_state",
                "checkpointing.load_params_from_path",
                "init_decode_state",
                "max_utils.unbox_logicallypartioned",
                "flax.linen.partitioning.axis_rules",
                "max_logging"
            ],
            "parameters": {
                "load_parameters_path": "Path to a checkpoint to load model parameters from. If not provided, random parameters are generated.",
                "logical_axis_rules": "Rules for mapping logical axis names to physical mesh axes for sharding.",
                "checkpoint_storage_concurrent_gb": "Concurrent gigabytes to use for checkpoint loading.",
                "checkpoint_storage_use_ocdbt": "Boolean flag to indicate if OCDBT should be used for checkpoint loading.",
                "checkpoint_storage_use_zarr3": "Boolean flag to indicate if Zarr3 should be used for checkpoint loading."
            },
            "notes": [
                "The function's primary control flow depends on the presence of `config.load_parameters_path`.",
                "When loading from a checkpoint, it creates a 'decode state' which is a `TrainState` object with a null optimizer (`tx=None`, `opt_state={}`).",
                "When no checkpoint is specified, it reuses the training state setup logic (`setup_initial_state`) but with `is_training=False`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_training_state(model, data_iterator, tx, config, rng, mesh, checkpoint_manager):\n  is_training = True\n  return setup_initial_state(\n      model,\n      data_iterator,\n      tx,\n      config,\n      rng,\n      mesh,\n      checkpoint_manager,\n      is_training,\n  )",
        "analysis": {
            "module_type": "training_state_setup",
            "purpose": "A convenience function that initializes the model and optimizer state for training by calling `setup_initial_state` with `is_training` set to True.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Set the `is_training` flag to True.",
                "Call the `setup_initial_state` function with all provided arguments and the `is_training` flag.",
                "Return the result from `setup_initial_state`."
            ],
            "output": {
                "shape": "A tuple containing the initialized TrainState, state mesh annotations, state mesh shardings, and the data iterator."
            },
            "dependencies": [
                "setup_initial_state"
            ],
            "parameters": {
                "model": "The Flax model to initialize.",
                "data_iterator": "The iterator for the training data, which may be modified if restoring from a checkpoint.",
                "tx": "The Optax gradient transformation (optimizer).",
                "config": "The configuration object containing settings for initialization and checkpointing.",
                "rng": "The JAX PRNG key for random weight initialization.",
                "mesh": "The JAX device mesh for distributing the state.",
                "checkpoint_manager": "The Orbax checkpoint manager for loading a saved state."
            },
            "notes": [
                "This function is a wrapper for `setup_initial_state`, simplifying the call for the training use case by hardcoding `is_training` to `True`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#unbox_logicallypartioned",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def unbox_logicallypartioned(boxed_pytree):\n  \"\"\"Unboxes the flax.LogicallyPartitioned pieces\n  Args:\n    boxed_pytree: a pytree that includes LogicallyPartitioned\n      leaves.\n  Returns:\n    a pytree where all all LogicallyPartitioned leaves have been unboxed.\n  \"\"\"\n  return jax.tree_util.tree_map(\n      lambda x: x.unbox() if isinstance(x, nn.spmd.LogicallyPartitioned) else x,\n      boxed_pytree,\n      is_leaf=lambda k: isinstance(k, nn.spmd.LogicallyPartitioned),\n  )",
        "analysis": {
            "module_type": "pytree_utility",
            "purpose": "Recursively traverses a PyTree and calls the `.unbox()` method on any leaf that is a `flax.linen.spmd.LogicallyPartitioned` object, effectively removing the wrapper.",
            "input": {
                "shape": "Any PyTree structure containing `flax.linen.spmd.LogicallyPartitioned` objects as leaves.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply `jax.tree_util.tree_map` to the input `boxed_pytree`.",
                "Specify that `nn.spmd.LogicallyPartitioned` instances should be treated as leaves for the traversal.",
                "For each leaf, check if it is an instance of `nn.spmd.LogicallyPartitioned`.",
                "If it is, call its `.unbox()` method.",
                "Otherwise, return the leaf unchanged."
            ],
            "output": {
                "shape": "A PyTree with the same structure as the input, where all `LogicallyPartitioned` leaves have been replaced by their unboxed values."
            },
            "dependencies": [
                "jax.tree_util.tree_map",
                "flax.linen.spmd.LogicallyPartitioned"
            ],
            "parameters": {},
            "notes": [
                "The `is_leaf` argument in `tree_map` is critical, as it prevents the function from traversing into the `LogicallyPartitioned` object's internal structure, ensuring the `.unbox()` method is called on the object itself."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#add_data_to_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def add_data_to_sharding(mesh, path, aval, sharding):\n  \"\"\"Adds 'data' dimension to sharding spec if compatible and not already present.\n\n  This function attempts to add data parallelism to a sharding specification by finding\n  a dimension that is divisible by the 'data' mesh axis size and doesn't conflict with\n  existing partitioning (e.g., tensor parallelism).\n  This function is mainly used to add data parallelism to the optimizer state for Zero-1 style sharding.\n\n  Args:\n    mesh: The device mesh\n    path: JAX tree path to the value being sharded\n    aval: Abstract value with shape information\n    sharding: Current NamedSharding to potentially augment\n\n  Returns:\n    NamedSharding: Updated sharding with 'data' dimension added, or original if unchanged\n\n  Raises:\n    AssertionError: If sharding is not NamedSharding or shape cannot be sharded\n  \"\"\"\n  if not isinstance(sharding, jax.sharding.NamedSharding):\n    raise AssertionError(f\"Expected NamedSharding, found {sharding} of {type(sharding)=} at {jax.tree_util.keystr(path)}\")\n  try:\n    sharded_shape = sharding.shard_shape(aval.shape)\n  except Exception as e:\n    raise AssertionError(f\"Could not shard {jax.tree_util.keystr(path)} of shape={aval.shape} with {sharding=}\") from e\n  pspec = sharding.spec\n\n  if \"data\" in jax.tree.leaves(pspec):\n    return sharding\n\n  for idx, (size, partition) in enumerate(zip(sharded_shape, pspec)):\n    if partition is None:\n      partition = ()\n\n    if isinstance(partition, str):\n      partition = (partition,)\n\n    if size % mesh.shape[\"data\"] == 0 and (partition is None or \"tensor\" not in partition):\n      added_component = (\"data\",) + partition\n      new_pspec = jax.sharding.PartitionSpec(*(pspec[:idx] + (added_component,) + pspec[idx + 1 :]))\n      new_sharding = jax.sharding.NamedSharding(sharding.mesh, new_pspec)\n      return new_sharding\n  return sharding",
        "analysis": {
            "module_type": "sharding_utility_function",
            "purpose": "Adds a 'data' parallelism dimension to a JAX NamedSharding specification if a compatible tensor dimension is found and it's not already present, primarily for Zero-1 style optimizer state sharding.",
            "input": {
                "shape": "mesh: jax.sharding.Mesh, path: JAX tree path, aval: jax.core.AbstractValue with a .shape attribute, sharding: jax.sharding.NamedSharding",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Verify the input `sharding` is an instance of `jax.sharding.NamedSharding`.",
                "Check if the 'data' axis already exists in the sharding spec; if so, return the original sharding.",
                "Iterate through each dimension of the tensor's abstract shape and its corresponding partition spec.",
                "For each dimension, check if its size is divisible by the 'data' mesh axis size and if it is not already sharded by a 'tensor' axis.",
                "If a suitable dimension is found, create a new `PartitionSpec` by prepending 'data' to that dimension's sharding tuple.",
                "Construct and return a new `jax.sharding.NamedSharding` object with the updated partition spec.",
                "If the loop completes without finding a suitable dimension, return the original `sharding` object."
            ],
            "output": {
                "shape": "Returns a `jax.sharding.NamedSharding` object, which is not a tensor. Shape is N/A."
            },
            "dependencies": [
                "jax.sharding.NamedSharding",
                "jax.sharding.PartitionSpec",
                "jax.tree.leaves",
                "jax.tree_util.keystr"
            ],
            "parameters": {
                "mesh": "The device mesh object, which contains the size of the 'data' axis.",
                "path": "The JAX tree path to the value being sharded, used for error messages.",
                "aval": "The abstract value (e.g., from `jax.eval_shape`) of the tensor, containing its shape.",
                "sharding": "The current `NamedSharding` object to be potentially augmented."
            },
            "notes": [
                "This function is primarily used to add data parallelism to optimizer states for Zero-1 style sharding.",
                "It specifically avoids adding data parallelism to dimensions that are already used for tensor parallelism to prevent conflicts.",
                "Raises an AssertionError if the input `sharding` is not a `NamedSharding` or if the `aval.shape` cannot be sharded with the given spec."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#maybe_update_params_sharding_with_opt",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def maybe_update_params_sharding_with_opt(config, state_mesh_shardings):\n  \"\"\"Updates parameter sharding configuration when optimizer state sharding is enabled.\n\n  When shard_optimizer_over_data is enabled (Zero-1 style sharding), this function\n  extracts the optimizer state shardings from the Adam optimizer's first moment (mu)\n  and merges them with the parameter shardings. This ensures parameter sharding is\n  consistent with how the optimizer state is distributed across the compute mesh.\n\n  Args:\n    config: Configuration object with shard_optimizer_over_data flag\n    state_mesh_shardings: Train state mesh shardings containing params and opt_state\n\n  Returns:\n    A tuple of (prev_params_shardings, updated_state_mesh_shardings):\n      - prev_params_shardings: Original parameter shardings before the update\n      - updated_state_mesh_shardings: State mesh shardings with updated params field\n        (unchanged if shard_optimizer_over_data is False)\n  \"\"\"\n  prev_params_shardings = state_mesh_shardings.params\n  if config.shard_optimizer_over_data:\n    if isinstance(state_mesh_shardings.opt_state, optax.ScaleByAdamState):\n      sharded_fp32_params = state_mesh_shardings.opt_state.mu\n    elif isinstance(state_mesh_shardings.opt_state, tuple) and isinstance(\n        state_mesh_shardings.opt_state[0], optax.ScaleByAdamState\n    ):\n      sharded_fp32_params = state_mesh_shardings.opt_state[0].mu\n    else:\n      raise NotImplementedError(f\"Could not find optimizer state shardings from {type(state_mesh_shardings.opt_state)}\")\n    if \"params\" not in sharded_fp32_params.keys():\n      # When quantization=fp8 is enabled the sharded_fp32_params\n      # are not wrapped in `params`. Here we wrap them back.\n      sharded_fp32_params = {\"params\": sharded_fp32_params}\n    state_mesh_shardings = state_mesh_shardings.replace(params=dict(prev_params_shardings, **sharded_fp32_params))\n  return prev_params_shardings, state_mesh_shardings",
        "analysis": {
            "module_type": "sharding_utility_function",
            "purpose": "Updates parameter sharding specifications to match the optimizer state sharding when Zero-1 style sharding is enabled.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Cache the original parameter shardings from `state_mesh_shardings.params`.",
                "Conditionally proceed if `config.shard_optimizer_over_data` is True.",
                "Extract the optimizer's first moment (`mu`) sharding from `state_mesh_shardings.opt_state`, handling both direct `optax.ScaleByAdamState` and tuple-wrapped instances.",
                "Raise `NotImplementedError` if the optimizer state type is unrecognized.",
                "Handle a special case for fp8 quantization by wrapping the extracted shardings in a 'params' dictionary if it's missing.",
                "Merge the extracted optimizer state shardings with the original parameter shardings.",
                "Update `state_mesh_shardings` by replacing its `params` field with the merged shardings.",
                "Return the original parameter shardings and the updated state mesh shardings."
            ],
            "output": {
                "shape": "A tuple containing the original parameter shardings and the potentially updated state mesh shardings."
            },
            "dependencies": [
                "optax.ScaleByAdamState"
            ],
            "parameters": {
                "shard_optimizer_over_data": "A boolean flag in the config that, when True, enables Zero-1 style sharding and triggers the update of parameter shardings to match the optimizer state shardings."
            },
            "notes": [
                "This function implements logic for Zero-1 style sharding, where optimizer states are sharded across the data-parallel dimension.",
                "The parameter sharding is updated to be consistent with the sharding of the Adam optimizer's first moment (`mu`).",
                "If `shard_optimizer_over_data` is False, the function returns the original, unmodified inputs.",
                "Includes a specific check to handle fp8 quantization, where the optimizer state structure may differ."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_initial_state(\n    model,\n    data_iterator,\n    tx,\n    config,\n    rng,\n    mesh,\n    checkpoint_manager,\n    is_training=True,\n):\n  \"\"\"We initialize the model and optimizer state, and optionally load from a\n  checkpoint as necessary.\n\n  Args:\n    model: the flax model to initialize\n    tx: the optax.GradientTransformation\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: an Orbax checkpointing.CheckpointManager object\n    is_training: True to initialize training state, False for decode state\n\n  Returns:\n    state: the initialized train state\n    state_mesh_annotations: the mesh annotations for the train state\n  \"\"\"\n\n  unboxed_abstract_state, state_mesh_annotations, state_mesh_shardings = get_abstract_state(\n      model, tx, config, rng, mesh, is_training\n  )\n\n  # Initialization\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    restored, raw_params = checkpointing.load_state_if_possible(\n        checkpoint_manager,\n        data_iterator,\n        config.load_parameters_path,\n        config.load_full_state_path,\n        config.checkpoint_storage_concurrent_gb,\n        unboxed_abstract_state,\n        config.enable_single_replica_ckpt_restoring,\n        config.dataset_type,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n        enable_orbax_v1=config.enable_orbax_v1,\n        checkpoint_conversion_fn=config.checkpoint_conversion_fn,\n        source_checkpoint_layout=config.source_checkpoint_layout,\n    )\n\n    if restored:\n      if isinstance(\n          checkpoint_manager,\n          (\n              emergency_checkpoint_manager.CheckpointManager,\n              emergency_replicator_checkpoint_manager.ReplicatorCheckpointManager,\n          ),\n      ):\n        state = restored\n      else:\n        if \"iter\" in restored and restored[\"iter\"] is not None:\n          data_iterator.local_iterator = restored[\"iter\"]\n        state = restored[\"items\"]\n    else:\n      init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training)\n      init_state_partial.__name__ = \"initialize_state\"\n      # pylint: disable=not-callable\n      state = jax.jit(\n          init_state_partial,\n          in_shardings=None,\n          out_shardings=state_mesh_shardings,\n      )(rng)\n      if raw_params:  # If we loaded a partial state, we need to merge it.\n        state = state.replace(params=raw_params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n\n  return state, state_mesh_annotations, state_mesh_shardings, data_iterator",
        "analysis": {
            "functionality": "Initializes the model and optimizer state, either from scratch or by restoring from a checkpoint.",
            "usage": "This function is called at the beginning of a training or decoding process to set up the initial state. It takes a Flax model, an optimizer (`tx`), a configuration object, a JAX random key, a device mesh, and a checkpoint manager. It first attempts to load a state from the checkpoint paths specified in the config. If unsuccessful, it initializes a new state with random weights. It returns the initialized state, its corresponding mesh annotations and shardings, and the data iterator (which may be updated with the step from the checkpoint)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_abstract_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_abstract_state(model, tx, config, rng, mesh, is_training=True):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n  init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training, rng)\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    abstract_state = jax.eval_shape(init_state_partial)\n\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n\n  state_mesh_shardings = nn.logical_to_mesh_sharding(state_logical_annotations, mesh, config.logical_axis_rules)\n  if is_training and config.shard_optimizer_over_data:\n    # Add data to sharding for optimizer state\n    state_mesh_shardings = state_mesh_shardings.replace(\n        opt_state=jax.tree.map_with_path(\n            functools.partial(add_data_to_sharding, mesh),\n            unbox_logicallypartioned(abstract_state).opt_state,\n            state_mesh_shardings.opt_state,\n        )\n    )\n  if is_training and config.optimizer_memory_host_offload:\n    opt_state = jax.tree_util.tree_map(lambda x: x.with_memory_kind(kind=\"pinned_host\"), state_mesh_shardings.opt_state)\n    state_mesh_shardings = state_mesh_shardings.replace(opt_state=opt_state)\n  if is_training and config.parameter_memory_host_offload:\n    assert config.param_scan_axis == 0, \"You must set the scan axis 0 to enable parameter offloading.\"\n\n    def move(path, x):\n      max_logging.log(f\"max_utils.py: Moving {path} to host\")\n      return x.with_memory_kind(kind=\"pinned_host\")\n\n    params = jax.tree_util.tree_map_with_path(move, state_mesh_shardings.params)\n    state_mesh_shardings = state_mesh_shardings.replace(params=params)\n\n  abstract_sharded_state = jax.jit(init_state_partial, in_shardings=None, out_shardings=state_mesh_shardings).eval_shape()\n\n  unboxed_abstract_sharded_state = max_utils.unbox_logicallypartioned(abstract_sharded_state)\n  # Initialization\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return (\n      unboxed_abstract_sharded_state,\n      state_mesh_annotations,\n      state_mesh_shardings,\n  )",
        "analysis": {
            "module_type": "state_abstraction_utility",
            "purpose": "Calculates the abstract shape, logical partitioning, and physical mesh sharding for a model's state (parameters and optimizer state) without performing the actual initialization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a partial function `init_state_partial` for state initialization.",
                "Use `jax.eval_shape` to get the abstract shape and structure of the state.",
                "Determine the logical partition specifications for the abstract state using `nn.get_partition_spec`.",
                "Convert logical specifications to physical mesh shardings using `nn.logical_to_mesh_sharding`.",
                "Conditionally modify the optimizer state and parameter shardings for data parallelism (ZeRO-1) and host memory offloading based on the config.",
                "Use `jax.jit(...).eval_shape()` with the calculated output shardings to get the final abstract sharded state.",
                "Unbox any `LogicallyPartitioned` wrappers from the abstract state.",
                "Convert logical annotations to mesh annotations.",
                "Return the unboxed abstract sharded state, mesh annotations, and mesh shardings."
            ],
            "output": {
                "shape": "A tuple containing (unboxed_abstract_sharded_state, state_mesh_annotations, state_mesh_shardings), which are PyTrees of abstract shapes and sharding specifications."
            },
            "dependencies": [
                "functools.partial",
                "jax.eval_shape",
                "jax.jit",
                "flax.linen.partitioning as nn_partitioning",
                "flax.linen as nn",
                "init_initial_state",
                "add_data_to_sharding",
                "max_utils.unbox_logicallypartioned"
            ],
            "parameters": {
                "model": "The Flax model to be initialized.",
                "tx": "The Optax gradient transformation (optimizer).",
                "config": "A configuration object containing settings for sharding, memory offloading, and model dimensions.",
                "rng": "A JAX PRNG key for initialization.",
                "mesh": "The JAX device mesh for distributed computation.",
                "is_training": "A boolean indicating whether to prepare a training state (with optimizer) or a decoding state (without optimizer)."
            },
            "notes": [
                "This function is crucial for setting up distributed training by determining how the model and optimizer states will be partitioned across devices before allocating memory for them.",
                "It leverages `jax.eval_shape` to efficiently compute shapes and dtypes without incurring the cost of actual computation.",
                "The returned sharding specifications are used to JIT-compile the actual state initialization function with the correct input/output shardings."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_prefill_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_prefill_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (\n        config.micro_batch_size_to_train_on,\n        config.max_prefill_predict_length,\n    )\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_PREFILL,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "functionality": "This function calculates the mesh sharding annotations for a model's Key-Value (KV) cache during the prefill stage of inference. It does this by performing an abstract initialization of the model to determine the shape and structure of the cache, and then maps logical sharding rules to the physical device mesh.",
            "usage": "Call this function with a Flax model, a configuration object, a JAX PRNG key, and a device mesh. It returns a PyTree of sharding annotations that can be used to correctly partition the KV cache tensors across devices for the prefill operation. \n\nInputs:\n- `model`: The Flax model.\n- `config`: A configuration object containing parameters like `logical_axis_rules`, `max_prefill_predict_length`, etc.\n- `rng`: A JAX PRNG key for initialization.\n- `mesh`: The JAX device mesh.\n- `page_state` (optional): State for paged attention.\n\nOutput:\nA PyTree with the same structure as the model's KV cache, where each leaf is a mesh sharding annotation object specifying how that tensor should be distributed."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (config.micro_batch_size_to_train_on, 1)\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_AUTOREGRESSIVE,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "module_type": "kv_cache_annotation_generator",
            "purpose": "Determines the physical sharding annotations for the model's key-value (KV) cache for autoregressive decoding by performing a shape-only initialization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a nested function `init_kv_cache` to initialize the model in `MODEL_MODE_AUTOREGRESSIVE` with a dummy input tensor of shape [micro_batch_size, 1].",
                "Extract the 'cache' component from the initialized model variables within the nested function.",
                "Use `jax.eval_shape` on the `init_kv_cache` function to get the abstract shape and dtype of the KV cache without actual computation.",
                "Use `nn.get_partition_spec` to determine the logical sharding annotations for the abstract KV cache based on configured axis rules.",
                "Use `nn.logical_to_mesh` to convert the logical annotations into physical mesh sharding annotations.",
                "Return the physical mesh annotations for the KV cache."
            ],
            "output": {
                "shape": "A PyTree with the same structure as the model's KV cache, where leaves are `jax.sharding.NamedSharding` objects specifying the physical device layout."
            },
            "dependencies": [
                "jax.eval_shape",
                "flax.linen.partitioning.get_partition_spec",
                "flax.linen.partitioning.logical_to_mesh",
                "multimodal_utils.get_dummy_image_shape_for_init",
                "PageState"
            ],
            "parameters": {
                "model": "The Flax model instance to be analyzed.",
                "config": "A configuration object containing model and sharding parameters like `logical_axis_rules` and `micro_batch_size_to_train_on`.",
                "rng": "A JAX PRNG key for model initialization.",
                "mesh": "The JAX device mesh for determining physical sharding.",
                "page_state": "An optional PageState object for paged attention KV cache initialization."
            },
            "notes": [
                "This function is specifically for the autoregressive decoding step, as indicated by `model_mode=MODEL_MODE_AUTOREGRESSIVE` and an input sequence length of 1.",
                "It does not perform any actual computation or memory allocation for the model weights or cache, focusing solely on shape and sharding analysis.",
                "The returned annotations are crucial for correctly sharding the KV cache during inference to ensure efficient memory usage and computation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#save_quantized_checkpoint_if_configured",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def save_quantized_checkpoint_if_configured(config, params):\n  \"\"\"Save quantized checkpoint if configured\"\"\"\n  assert config.quantization, \"quantization must be configured\"\n  if config.save_quantized_params_path:\n    checkpointing.save_params_to_path(\n        checkpoint_dir=config.save_quantized_params_path,\n        params=params,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n    )\n  else:\n    max_logging.log(\"Skipping saving quantized checkpoint as save_quantized_params_path is null.\")",
        "analysis": {
            "functionality": "Saves a checkpoint of quantized model parameters to a specified path if the configuration directs it to.",
            "usage": "Call this function with a configuration object and a PyTree of model parameters. The function will check `config.quantization` and `config.save_quantized_params_path`. If both are appropriately set, it saves the parameters to the specified path using storage options also defined in the config. Otherwise, it either raises an assertion error or logs that it's skipping the save."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#add_config_to_summary_writer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def add_config_to_summary_writer(config, summary_writer):\n  \"\"\"Writes config params to tensorboard\"\"\"\n  if jax.process_index() == 0:\n    for key, value in config.get_keys().items():\n      max_utils.add_text_to_summary_writer(key, str(value), summary_writer)",
        "analysis": {
            "functionality": "This function writes configuration parameters to a TensorBoard summary writer. It iterates through the key-value pairs of a given configuration object and logs each one as text in TensorBoard.",
            "usage": "To use this function, pass a configuration object and a TensorBoard summary writer instance. The function will only execute on the primary JAX process (index 0) to prevent duplicate logging in a multi-process environment. It has no return value. Example: `add_config_to_summary_writer(my_config, my_summary_writer)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#logical_axis_rules_pp_act_as_dp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def logical_axis_rules_pp_act_as_dp(logical_rules):\n  \"\"\"Add stage as a physical axes before data for each rule, so stage acts just like data instead of PP.\n  This is used when we want to pipeline only a subset of layers, and leave the rest like DP.\n  \"\"\"\n  new_rules = []\n  for key, physical_axes in logical_rules:\n    if isinstance(physical_axes, str):\n      physical_axes = (physical_axes,)\n    else:\n      physical_axes = tuple(physical_axes)\n    new_physical_axes = tuple(axis for axis in physical_axes if axis != \"stage\")\n    if \"data\" in new_physical_axes:\n      data_idx = new_physical_axes.index(\"data\")\n      new_physical_axes = new_physical_axes[0:data_idx] + (\"stage\",) + new_physical_axes[data_idx:]\n    new_rules.append((key, new_physical_axes))\n  return tuple(new_rules)",
        "analysis": {
            "module_type": "logical_axis_rule_transformer",
            "purpose": "Modifies logical axis rules to make the 'stage' physical axis behave like the 'data' axis, effectively treating pipeline parallelism as data parallelism for a subset of layers.",
            "input": {
                "shape": "A list or tuple of rules, where each rule is a tuple of (key_string, physical_axes). `physical_axes` can be a string or a tuple of strings. e.g., [('embed', ('data', 'fsdp'))]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through each rule in the input `logical_rules`.",
                "Ensure the physical axes component of the rule is a tuple.",
                "Create a new tuple of physical axes by removing any 'stage' axis.",
                "If the 'data' axis exists in the new tuple, find its index and insert the 'stage' axis immediately before it.",
                "Append the modified rule (key and new physical axes) to a new list.",
                "Return the list of new rules as a tuple."
            ],
            "output": {
                "shape": "A tuple of modified rules with the same structure as the input, e.g., (('embed', ('stage', 'data', 'fsdp')), ...)"
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This function is used to configure hybrid parallelism schemes where some layers are pipelined and others are data-parallel.",
                "It effectively re-purposes the 'stage' mesh dimension to act as an additional 'data' dimension by placing it before the 'data' axis in the physical sharding specification."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_device_mesh",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_device_mesh(config, devices=None):\n  \"\"\"Creates a device mesh with each slice in its own data parallel group. If there is only one slice, uses two replicas\"\"\"\n  if devices is None:\n    devices = jax.devices()\n  if config.subslice_shape and config.enable_single_controller and config.num_slices == 1:\n    max_logging.log(f\"Trying to create a subslice with shape: {config.subslice_shape}\")\n    subslice_shape = tuple(int(x) for x in config.subslice_shape.split(\",\"))\n    device_coords = [device.coords for device in devices]\n    device_coords_np = np.array(device_coords)\n\n    # Find the minimum coordinates to start the subslice\n    min_coords = device_coords_np.min(axis=0)\n\n    subslice_devices = []\n    for device in devices:\n      coords = device.coords\n      if all(min_coords[i] <= coords[i] < min_coords[i] + subslice_shape[i] for i in range(len(subslice_shape))):\n        subslice_devices.append(device)\n    devices = subslice_devices\n\n  num_devices = len(devices)\n  num_slices = 1 if config.inference_benchmark_test else config.num_slices\n  num_devices_per_slice = num_devices // num_slices\n\n  multi_slice_env = num_slices > 1\n\n  # Find possible unspecified parallelisms\n  ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), num_devices_per_slice, \"ICI\")\n\n  allow_split_physical_axes = config.allow_split_physical_axes if config.allow_split_physical_axes else False\n\n  if multi_slice_env:\n    dcn_parallelism = max_utils.fill_unspecified_mesh_axes(config.dcn_parallelism.copy(), num_slices, \"DCN\")\n    if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n      mesh = max_utils.create_custom_device_mesh(ici_parallelism, dcn_parallelism, devices, config.custom_mesh)\n    else:\n      mesh = mesh_utils.create_hybrid_device_mesh(\n          ici_parallelism,\n          dcn_parallelism,\n          devices,\n          allow_split_physical_axes=allow_split_physical_axes,\n      )\n  else:\n    if allow_split_physical_axes:\n      if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n        mesh = mesh_utils.create_device_mesh(\n            [16, 16],\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=False,\n        )\n        mesh = max_utils.reshape_mesh_to_rings(mesh, config.custom_mesh)\n        mesh = np.reshape(mesh, ici_parallelism)\n      else:\n        mesh = mesh_utils.create_device_mesh(\n            ici_parallelism,\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=allow_split_physical_axes,\n        )\n    else:\n      mesh = mesh_utils.create_device_mesh(\n          ici_parallelism,\n          devices,\n      )\n      if config.optimize_mesh_for_tpu_v6e:\n        mesh = max_utils.optimize_mesh_for_tpu_v6e(mesh, devices)\n\n  max_logging.log(f\"Num_devices: {num_devices}, shape {mesh.shape}\")\n\n  return mesh",
        "analysis": {
            "functionality": "Creates a JAX device mesh based on configuration, handling single-slice, multi-slice, and custom hardware topologies.",
            "usage": "Call this function with a configuration object and an optional list of JAX devices. It returns a NumPy array representing the device mesh, which is used to distribute computations across hardware. The configuration object should specify parameters like `num_slices`, `ici_parallelism`, and `dcn_parallelism`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_learning_rate_schedule",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_learning_rate_schedule(config):\n  \"\"\"Creates a warmup and cosine decay learning rate schedule:\n  We take inspiration from Llama2's learning rate (LR) schedule, see https://arxiv.org/pdf/2307.09288.pdf section 2.2\n  Learning rate schedule has either two or three parts:\n  1) Linear warmup from 0 to [learning_rate] over steps 0 to [learning_rate_schedule_steps * warmup_steps_fraction]\n  2) Cosine from [learning_rate] to [learning_rate * cosine_learning_rate_final_fraction] until learning_rate_schedule_steps\n  3) Constant learning rate of 0 from learning_rate_schedule_steps to steps.\n  The zero learning rate section can be used to more accurately measure the fully trained model's performance.\n  \"\"\"\n\n  def make_cos_schedule(init_lr, final_lr, len_steps):\n    def schedule(step):\n      pct = (step) / len_steps\n      a = 0.5 * (jnp.cos(jnp.pi * pct) + 1)\n      lr = init_lr * a + final_lr * (1 - a)\n      return lr\n\n    return schedule\n\n  lr = config.learning_rate\n  cos_final_lr = lr * config.cosine_learning_rate_final_fraction\n\n  warmup_steps = int(config.learning_rate_schedule_steps * config.warmup_steps_fraction)\n  cos_steps = config.learning_rate_schedule_steps - warmup_steps\n  constant_zero_steps = config.steps - config.learning_rate_schedule_steps\n\n  warmup_schedule = optax.linear_schedule(init_value=0.0, end_value=lr, transition_steps=warmup_steps)\n  cos_schedule = make_cos_schedule(lr, cos_final_lr, cos_steps)\n  constant_schedule = optax.constant_schedule(0.0)\n\n  pieces = [warmup_schedule, cos_schedule]\n  boundaries = [\n      warmup_steps,\n      warmup_steps + cos_steps,\n  ]\n\n  if constant_zero_steps > 0:\n    pieces.append(constant_schedule)\n    boundaries.append(warmup_steps + cos_steps + constant_zero_steps)\n\n  return optax.join_schedules(pieces, boundaries)",
        "analysis": {
            "module_type": "learning_rate_schedule_generator",
            "purpose": "Creates a multi-part learning rate schedule consisting of a linear warmup, a cosine decay, and an optional constant zero phase at the end.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define an inner helper function `make_cos_schedule` to create a cosine decay schedule.",
                "Calculate the number of steps for the warmup, cosine decay, and optional constant zero phases based on the config.",
                "Create a linear warmup schedule using `optax.linear_schedule`.",
                "Create a cosine decay schedule using the `make_cos_schedule` helper.",
                "Create a constant zero learning rate schedule using `optax.constant_schedule`.",
                "Combine the warmup and cosine schedules into a list of pieces and boundaries.",
                "Conditionally append the constant zero schedule and its boundary if the total steps exceed the schedule steps.",
                "Return a single piecewise schedule by combining the schedules using `optax.join_schedules`."
            ],
            "output": {
                "shape": "A callable function that takes an integer step and returns a float learning rate."
            },
            "dependencies": [
                "optax.linear_schedule",
                "optax.constant_schedule",
                "optax.join_schedules",
                "jax.numpy"
            ],
            "parameters": {
                "config.learning_rate": "The peak learning rate after warmup.",
                "config.cosine_learning_rate_final_fraction": "The fraction of the peak learning rate to decay to at the end of the cosine phase.",
                "config.learning_rate_schedule_steps": "The total number of steps for the combined warmup and cosine decay phases.",
                "config.warmup_steps_fraction": "The fraction of `learning_rate_schedule_steps` to be used for the linear warmup.",
                "config.steps": "The total number of training steps, used to determine the length of the optional final constant zero phase."
            },
            "notes": [
                "The learning rate schedule is inspired by the Llama2 paper.",
                "The schedule is composed of two or three parts: 1) Linear warmup, 2) Cosine decay, and 3) Optional constant zero learning rate.",
                "The final constant zero learning rate phase is intended to allow for more accurate performance measurement of the fully trained model."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_formatted_sharding_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_formatted_sharding_annotations(params, mesh=None):\n  \"\"\"\n  Generates a readable string report of sharding annotations for all parameters.\n\n  This function iterates through a PyTree of model parameters and inspects the\n  sharding information attached to each parameter (leaf). It creates a\n  human-readable summary that is useful for debugging sharding configurations.\n\n  Args:\n    params: The PyTree of model parameters to inspect.\n    mesh: (Optional) The device mesh. If provided, its axis names and shape\n          are included in the report for additional context.\n\n  Returns:\n    A single string containing the formatted report of sharding annotations\n    for every parameter, with each entry on a new line.\n  \"\"\"\n  # Initialize a list to hold the lines of the report, starting with a title.\n  annotation_lines = [\"Comprehensice Weight Sharding Annotations:\"]\n\n  # If a mesh object is provided, add its details to the report header.\n  if mesh:\n    annotation_lines.append(f\"Mesh axes: {mesh.axis_names}, Mesh shape: {mesh.shape}\")\n    annotation_lines.append(\"-\" * 30)\n\n  # Get a flattened list of all parameters (leaves) and their corresponding paths in the PyTree.\n  all_params_leaves = jtu.tree_leaves_with_path(params)\n\n  # Loop through each parameter leaf in the flattened list.\n  for path, p_leaf in all_params_leaves:\n    # Convert the parameter's path (a sequence of keys) into a readable string name.\n    param_name_str = jtu.keystr(path)\n    # Get the shape of the parameter as a string.\n    shape_str = str(p_leaf.shape)\n    # Set a default description for sharding, in case none is found.\n    sharding_desc = \"N/A\"\n\n    # Check if the parameter leaf has a 'sharding' attribute.\n    if hasattr(p_leaf, \"sharding\"):\n      # Case 1: Standard JAX sharding with a PartitionSpec.\n      if hasattr(p_leaf.sharding, \"spec\") and p_leaf.sharding.spec is not None:\n        # The spec is a tuple (PartitionSpec), format it for readability.\n        spec_parts = []\n        for item in p_leaf.sharding.spec:\n          # Represent None as \"Replicated\" to make it explicit.\n          spec_parts.append(str(item) if item is not None else \"Replicated\")\n        sharding_desc = f\"PartitionSpec({', '.join(spec_parts)})\"\n      # Case 2: The parameter is explicitly marked as fully replicated.\n      elif hasattr(p_leaf.sharding, \"spec\") and p_leaf.sharding.spec is None:\n        sharding_desc = \"Fully Replicated (spec is None)\"\n      # Case 3: A generic fallback if a sharding object exists but has no recognized spec attribute.\n      else:\n        # Print the string representation of the sharding object itself.\n        sharding_desc = str(p_leaf.sharding)\n    # Case 4: The parameter has no .sharding attribute at all.\n    else:\n      sharding_desc = \"No .sharding attribute found\"\n\n    # Append the formatted details for the current parameter to our list of lines.\n    annotation_lines.append(f\" - Param: {param_name_str}\\n\" f\"   Shape: {shape_str}\\n\" f\"   Sharding: {sharding_desc}\")\n  # Join all the collected lines into a single string, separated by newlines.\n  return \"\\n\".join(annotation_lines)",
        "analysis": {
            "functionality": "Generates a human-readable string report detailing the sharding annotations for each parameter in a PyTree. It inspects the sharding information attached to each parameter leaf and formats it for debugging purposes.",
            "usage": "Call this function with a PyTree of model parameters (e.g., a Flax TrainState's `params`) and an optional JAX mesh object. It returns a single formatted string that can be printed to the console to verify sharding configurations."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_physical_spec_no_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_physical_spec_no_fsdp(full_logical, mesh, logical_axis_rules):\n  \"\"\"\n  Generates a physical sharding spec for fully replicated weights.\n\n  This function computes a target sharding layout where model parameters are fully\n  replicated across the 'fsdp' mesh axis. It starts with the original logical\n  sharding and removes any rules that shard along the 'fsdp' or\n  'fsdp_transpose' axes.\n\n  Replacing a sharding axis with `None` in a PartitionSpec instructs JAX to\n  replicate the array data along that physical mesh dimension. The resulting\n  specification is used as a target layout for an all-gather operation.\n\n  Args:\n    full_logical: A PyTree of logical PartitionSpecs for the model parameters.\n    mesh: The JAX device mesh.\n    logical_axis_rules: Rules for converting logical axes to physical mesh axes.\n\n  Returns:\n    A PyTree of physical `jax.sharding.NamedSharding` objects that describe a\n    layout where parameters are fully gathered (replicated) across the 'fsdp'\n    mesh axis.\n  \"\"\"\n\n  def remove_fsdp_sharding(sharding_tree):\n    \"\"\"Recursively traverses the sharding tree to remove fsdp axes.\"\"\"\n\n    def _remove_fsdp_from_partition_spec(named_sharding):\n      \"\"\"Removes 'fsdp' and 'fsdp_transpose' from a PartitionSpec.\"\"\"\n      if isinstance(named_sharding, jax.sharding.NamedSharding):\n        new_spec = []\n        # Iterate through each axis in the original PartitionSpec.\n        for axis in named_sharding.spec:\n          if axis is None:\n            new_spec.append(None)\n          elif isinstance(axis, str):\n            # If the axis is 'fsdp', replace it with None to signify replication.\n            if axis not in (\"fsdp\", \"fsdp_transpose\"):\n              new_spec.append(axis)\n            else:\n              new_spec.append(None)\n          elif isinstance(axis, (list, tuple)):\n            # If the axis is a collection, filter out 'fsdp'.\n            new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n            new_spec.append(tuple(new_axis))\n          else:\n            raise ValueError(f\"Unsupported_axis_type: {type(axis)}\")\n          # Return a new sharding object with the modified spec.\n        return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n      return named_sharding\n\n    return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n  # Convert the high-level logical spec to a physical one using default rules.\n  physical = nn.logical_to_mesh_sharding(full_logical, mesh=mesh, rules=logical_axis_rules)\n  # Apply the function to remove the FSDP sharding, defining our target layout.\n  physical_no_fsdp = remove_fsdp_sharding(physical)\n  return physical_no_fsdp",
        "analysis": {
            "functionality": "Generates a physical sharding specification where model parameters are fully replicated across the 'fsdp' and 'fsdp_transpose' mesh axes by modifying an existing logical specification.",
            "usage": "Call this function with a PyTree of logical PartitionSpecs, a JAX device mesh, and logical axis rules. It returns a new PyTree of `jax.sharding.NamedSharding` objects that can be used as a target layout for an all-gather operation, effectively replicating the parameters across the FSDP mesh dimension."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#all_gather_over_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def all_gather_over_fsdp(variables, sharding_info, mesh, logical_axis_rules):\n  \"\"\"Performs an all-gather on FSDP-sharded variables via a sharding constraint.\n  This function triggers an all-gather operation on the model's parameters.\n  It does so by applying a sharding constraint that specifies a fully\n  replicated layout.\n\n  The JAX compiler satisfies this constraint by automatically inserting the\n  necessary `all-gather` collective communication operations into the\n  computation graph, effectively gathering the sharded weights.\n\n  Args:\n    variables: The PyTree of model parameters, currently sharded across devices.\n    sharding_info: The logical partition spec of the currently sharded `variables`.\n    mesh: The JAX device mesh.\n    logical_axis_rules: Rules for converting logical axes to physical mesh axes.\n\n  Returns:\n    The model's variables with the all-gather operation applied, resulting\n    in the weights being fully replicated on all devices in the 'fsdp' mesh.\n  \"\"\"\n  # Get the target physical layout (weights fully replicated).\n  physical_constraint_no_fsdp = get_physical_spec_no_fsdp(sharding_info, mesh, logical_axis_rules)\n  # Apply the constraint to the model's current variables. This tells JAX to\n  # gather the weights into this layout.\n  return jax.lax.with_sharding_constraint(variables, physical_constraint_no_fsdp)",
        "analysis": {
            "module_type": "fsdp_all_gather_utility",
            "purpose": "Performs an all-gather on FSDP-sharded variables by applying a sharding constraint that specifies a fully replicated layout, which implicitly triggers the necessary collective communication operations by the JAX compiler.",
            "input": {
                "shape": "A PyTree of model parameters (`variables`), currently sharded across devices.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Call `get_physical_spec_no_fsdp` to compute a target physical sharding layout where parameters are fully replicated across the 'fsdp' mesh axis.",
                "Apply the computed sharding layout as a constraint to the input variables using `jax.lax.with_sharding_constraint`."
            ],
            "output": {
                "shape": "A PyTree with the same structure and tensor shapes as the input `variables`, but with a new sharding layout where weights are replicated on all devices in the 'fsdp' mesh."
            },
            "dependencies": [
                "get_physical_spec_no_fsdp",
                "jax.lax.with_sharding_constraint"
            ],
            "parameters": {
                "variables": "The PyTree of model parameters, currently sharded across devices.",
                "sharding_info": "The logical partition spec of the currently sharded `variables`.",
                "mesh": "The JAX device mesh.",
                "logical_axis_rules": "Rules for converting logical axes to physical mesh axes."
            },
            "notes": [
                "This function does not perform the all-gather directly but rather hints to the JAX compiler to insert the necessary `all-gather` collective communication operations by imposing a sharding constraint.",
                "The JAX compiler satisfies the constraint by gathering the sharded weights into the specified fully replicated layout."
            ]
        }
    }
]