[
    {
        "block_name": "src/MaxText/layers/attention_mla.py#mla_as_linen",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "def mla_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    q_lora_rank: int = 0,\n    kv_lora_rank: int = 512,\n    qk_nope_head_dim: int = 128,\n    qk_rope_head_dim: int = 64,\n    v_head_dim: int = 128,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    mscale: float = 1.0,  # scaling factor for softmax\n    rope_factor: float = 40.0,  # rotary embedding factor\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an MLA as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `MLA` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MLA,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      q_lora_rank=q_lora_rank,\n      kv_lora_rank=kv_lora_rank,\n      qk_nope_head_dim=qk_nope_head_dim,\n      qk_rope_head_dim=qk_rope_head_dim,\n      v_head_dim=v_head_dim,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      mscale=mscale,\n      rope_factor=rope_factor,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "functionality": "Creates a Flax Linen module for Multi-Head Latent Attention (MLA) by wrapping an NNX-based MLA implementation.",
            "usage": "This function acts as a factory to instantiate an MLA layer compatible with Flax Linen models. It takes numerous configuration parameters related to attention heads, dimensions, sequence lengths, sharding, and various attention mechanisms (like LoRA, RoPE, Paged Attention). It returns a Linen module that can be used within a Flax model. The module expects query and key-value inputs, along with optional positional and segment ID information, and outputs the attention-transformed tensor."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_mla.py#MLA",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "class MLA(Attention):\n  \"\"\"Multi-Head Latent Attention (MLA) layer.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      q_lora_rank: int = 0,\n      kv_lora_rank: int = 512,\n      qk_nope_head_dim: int = 128,\n      qk_rope_head_dim: int = 64,\n      v_head_dim: int = 128,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      mscale: float = 1.0,  # scaling factor for softmax\n      rope_factor: float = 40.0,  # rotary embedding factor\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the MLA module.\n\n    Args:\n      config: The model configuration.\n      ... and other configuration parameters for MLA attention.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    base_kv_cache = config.attention != \"paged\" and config.mla_naive_kvcache\n\n    # Setting these before call to super because a field is used in super\n    self.q_lora_rank = q_lora_rank\n    self.kv_lora_rank = kv_lora_rank\n    self.qk_nope_head_dim = qk_nope_head_dim\n    self.qk_rope_head_dim = qk_rope_head_dim\n    self.v_head_dim = v_head_dim\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.mscale = mscale\n    self.rope_factor = rope_factor\n\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n\n    super().__init__(\n        config=config,\n        num_query_heads=num_query_heads,\n        num_kv_heads=num_kv_heads,\n        head_dim=head_dim,\n        max_target_length=max_target_length,\n        mesh=mesh,\n        attention_kernel=attention_kernel,\n        inputs_q_shape=inputs_q_shape,\n        inputs_kv_shape=inputs_kv_shape,\n        dtype=dtype,\n        weight_dtype=weight_dtype,\n        max_prefill_predict_length=max_prefill_predict_length,\n        dropout_rate=dropout_rate,\n        kernel_init=kernel_init,\n        float32_qk_product=float32_qk_product,\n        float32_logits=float32_logits,\n        quant=quant,\n        kv_quant=kv_quant,\n        attention_type=attention_type,\n        attn_logits_soft_cap=attn_logits_soft_cap,\n        sliding_window_size=sliding_window_size,\n        use_ragged_attention=use_ragged_attention,\n        ragged_block_size=ragged_block_size,\n        use_qk_norm=use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        use_bias_in_projections=use_bias_in_projections,\n        temperature_tuning=temperature_tuning,\n        temperature_tuning_scale=temperature_tuning_scale,\n        temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n        prefill_query_axis_names=prefill_query_axis_names,\n        prefill_key_axis_names=prefill_key_axis_names,\n        prefill_value_axis_names=prefill_value_axis_names,\n        query_axis_names=query_axis_names,\n        key_axis_names=key_axis_names,\n        value_axis_names=value_axis_names,\n        ep_query_axis_names=ep_query_axis_names,\n        ep_key_axis_names=ep_key_axis_names,\n        ep_value_axis_names=ep_value_axis_names,\n        input_axis_names=input_axis_names,\n        ep_input_axis_names=ep_input_axis_names,\n        out_axis_names=out_axis_names,\n        ep_out_axis_names=ep_out_axis_names,\n        prefill_input_axis_names=prefill_input_axis_names,\n        decode_input_axis_names=decode_input_axis_names,\n        prefill_out_axis_names=prefill_out_axis_names,\n        decode_out_axis_names=decode_out_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        compute_axis_order=compute_axis_order,\n        reshape_q=reshape_q,\n        is_nope_layer=is_nope_layer,\n        is_vision=is_vision,\n        model_mode=model_mode,\n        base_kv_cache=base_kv_cache,\n        rngs=rngs,\n    )\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.MlaKVCache_0 = self.init_mla_kv_caches(inputs_kv_shape) if model_mode != MODEL_MODE_TRAIN else None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the MLA-specific projections.\"\"\"\n    # Assert required configuration parameters for MLA attention.\n    assert (\n        self.config.attention_type == AttentionType.MLA.value\n    ), f\"MLA requires MLA attention type {AttentionType.MLA.value}\"\n    assert self.kv_lora_rank > 0, \"KV LoRA rank must be > 0\"\n    assert self.qk_nope_head_dim > 0, \"QK NoPe head dim must be > 0\"\n    assert self.qk_rope_head_dim > 0, \"QK RoPE head dim must be > 0\"\n    assert self.v_head_dim > 0, \"V head dim must be > 0\"\n    assert self.num_query_heads == self.num_kv_heads, \"MLA requires equal number of query and kv heads\"\n    assert not self.config.fused_qkv, \"Fused QKV is not supported for MLA\"\n\n    if self.q_lora_rank == 0:\n      # Standard Q projection (without LoRA).\n      self.query = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n    else:\n      # LoRA path for Q.\n      self.wq_a = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=self.q_lora_rank,\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_lora_up_proj\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n      self.q_norm = RMSNorm(\n          num_features=self.q_lora_rank,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.wq_b = DenseGeneral(\n          in_features_shape=self.q_lora_rank,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"q_lora\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n\n    # KV LoRA path.\n    self.wkv_a = DenseGeneral(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.kv_lora_rank + self.qk_rope_head_dim,\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"kv_lora_up_proj\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.kv_norm = RMSNorm(\n        num_features=self.kv_lora_rank,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.wkv_b = DenseGeneral(\n        in_features_shape=self.kv_lora_rank,\n        out_features_shape=(\n            self.num_query_heads,\n            (self.qk_nope_head_dim + self.v_head_dim),\n        ),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"kv_lora\", \"kv_heads\", \"kv_head_dim\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # Set softmax scaling.\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n    # Setup paged attention op\n    if self.config.attention == \"paged\":\n      # Set head_dim to the max of qk_head_dim and v_head_dim. The current paged\n      # attention kernel requires the head_dim to be the same for q, k, v.\n      head_dim = max(self.qk_head_dim, self.v_head_dim)\n      # Align head_dim to the pagedattn_head_dim_alignment if specified.\n      if self.config.pagedattn_head_dim_alignment > 0:\n        alignment = self.config.pagedattn_head_dim_alignment\n        head_dim = (head_dim + alignment - 1) // alignment * alignment\n      self.ds_paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n  def mla_query_projection(self, inputs_q: Array, inputs_positions: Array, model_mode) -> Array:\n    \"\"\"Query projection for MLA, e.g. includes LoRA if q_lora_rank > 0.\"\"\"\n    # Set softmax scaling.\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    if self.q_lora_rank == 0:\n      q = self.query(inputs_q)\n    else:\n      # LoRA path\n      low_rank_q = self.wq_a(inputs_q)  # [B, L, q_lora_rank]\n      low_rank_q = self.q_norm(low_rank_q)  # RMSNorm on low rank\n      q = self.wq_b(low_rank_q)  # [B, L, n_heads * qk_head_dim]\n\n    # Split into non-positional and rotary parts.\n    q_nope, q_pe = jnp.split(q, [self.qk_nope_head_dim], axis=-1)\n    q_pe = self.apply_rotary_embedding(q_pe, inputs_positions=inputs_positions)\n    # Query projection is scaled by self.softmax_scale to be consistent MaxText implementation.\n    # DeepSeek v3 was doing it in attention score computation.\n    query = jnp.concatenate([q_nope, q_pe], axis=-1) * self.softmax_scale\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n    return query\n\n  def mla_get_key_value(self, low_rank_main, key_rope, model_mode):\n    \"\"\"get (key,value) pair from mla\"\"\"\n    kv_out = self.wkv_b(low_rank_main)\n\n    # Split kv_out into key_nope and value parts.\n    key_nope, value = jnp.split(kv_out, [self.qk_nope_head_dim], axis=-1)\n    key_rope = jnp.broadcast_to(key_rope, (key_nope.shape[0], key_nope.shape[1], self.num_query_heads, key_rope.shape[3]))\n\n    key = jnp.concatenate([key_nope, key_rope], axis=-1)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n    return key, value\n\n  def init_mla_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes MlaKVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      An MlaKVCache module instance.\n\n    Raises:\n      ValueError: If the configuration is invalid.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in MlaKVCache.\n    # However, MlaKVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # MlaKVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.MlaKVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_head_size=self.kv_lora_rank,\n        value_head_size=self.qk_rope_head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        model_mode=self.model_mode,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        rngs=self.rngs,\n    )\n\n  def update_mla_kv_caches(self, low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk=None):\n    \"\"\"Updates the MLA (Multi-Head Latent Attention) KV caches.\n\n    This method is specific to the MLA attention mechanism. It calls the\n    `mla_kv_cache_as_linen` module to update and retrieve the caches, which\n    store latent representations (`low_rank_main`) and RoPE-applied keys\n    (`key_rope`). It then reconstructs the full key and value tensors from\n    the cached components.\n\n    Args:\n      low_rank_main: The main latent component of the key.\n      key_rope: The RoPE-applied component of the key.\n      decoder_segment_ids: Segment IDs for decoder masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, reconstructed from the MLA cache, or None.\n      - The autoregressive key-value cache, reconstructed from the MLA cache, or None.\n    \"\"\"\n\n    prefill_mla_cache, ar_mla_cache = self.MlaKVCache_0(\n        key_latent=low_rank_main,\n        key_rope=key_rope,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n\n    if prefill_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids = prefill_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      prefill_kv_cache = key, value, decoder_segment_ids\n    else:\n      prefill_kv_cache = None\n\n    if ar_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids, lengths = ar_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      ar_kv_cache = key, value, decoder_segment_ids, lengths\n    else:\n      ar_kv_cache = None\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def mla_kv_projection(self, inputs: Array, inputs_positions: Array, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"MLA key/value projection with integrated rotary embedding.\"\"\"\n    low_rank = self.wkv_a(inputs)\n    low_rank_main, low_rank_rope = jnp.split(low_rank, [self.kv_lora_rank], axis=-1)\n    low_rank_main = self.kv_norm(low_rank_main)\n\n    # Apply rotary embedding to key_rope.\n    key_rope = jnp.expand_dims(low_rank_rope, axis=2)\n    key_rope = self.apply_rotary_embedding(key_rope, inputs_positions=inputs_positions)\n\n    key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n    cached_values = [None, None]\n    if self.config.attention != \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      if self.config.mla_naive_kvcache:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      else:\n        cached_values = self.update_mla_kv_caches(\n            low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk\n        )\n\n    return key, value, cached_values\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      out_sharding: NamedSharding | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Optional[Any] = None,\n  ) -> Array:\n    \"\"\"Forward pass for MLA, reusing `AttentionOp` for the actual attention.\n\n    Args:\n      inputs_q: Query input [batch, q_length, embed_dim].\n      inputs_kv: KV input   [batch, kv_length, embed_dim].\n      inputs_positions: Positions for rotary embeddings or similar.\n      decoder_segment_ids: Segment IDs for masking, if any.\n      model_mode: \"train\", \"prefill\", or \"autoregressive\".\n      deterministic: Disables dropout if set to True.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      A tensor of shape [batch, length, embed_dim] containing the\n      MLA-attended outputs.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n\n    query = self.mla_query_projection(inputs_q, inputs_positions, model_mode)\n    key, value, cached_values = self.mla_kv_projection(\n        inputs_kv, inputs_positions, decoder_segment_ids, model_mode, previous_chunk\n    )\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.ds_paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      unnormalized_out = unnormalized_out[..., : self.v_head_dim]\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      out = self.attention_op(query, key, value, decoder_segment_ids, model_mode, cached_values)\n\n    if model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    out = self.out_projection(out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "multi_head_latent_attention",
            "purpose": "Implements the Multi-Head Latent Attention (MLA) layer, a variant of attention mechanism with specific optimizations for latent representations and LoRA integration.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Query projection (potentially with LoRA).",
                "Key and Value projection (potentially with LoRA and Rotary Embeddings).",
                "KV cache update (if not in training mode and not using paged attention).",
                "Attention operation (using AttentionOp or PagedAttentionOp).",
                "Output projection."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Attention",
                "DenseGeneral",
                "RMSNorm",
                "kvcache",
                "paged_attention",
                "nnx_wrappers",
                "jax.numpy",
                "jax.sharding",
                "flax.linen"
            ],
            "parameters": {
                "config": "Model configuration object containing various settings.",
                "num_query_heads": "Number of heads for the query.",
                "num_kv_heads": "Number of heads for key and value.",
                "head_dim": "Dimension of each attention head.",
                "max_target_length": "Maximum sequence length the attention layer can handle.",
                "mesh": "JAX mesh for distributed computation.",
                "attention_kernel": "Name of the attention kernel to use.",
                "inputs_q_shape": "Shape of the query input tensor.",
                "inputs_kv_shape": "Shape of the key/value input tensor.",
                "dtype": "Data type for computations.",
                "weight_dtype": "Data type for weights.",
                "max_prefill_predict_length": "Maximum length for prefill prediction.",
                "dropout_rate": "Dropout rate for regularization.",
                "kernel_init": "Initializer for the kernel weights.",
                "float32_qk_product": "Whether to compute QK product in float32 for stability.",
                "float32_logits": "Whether to cast logits to float32 for stability.",
                "quant": "Quantization configuration for query.",
                "kv_quant": "Quantization configuration for key/value.",
                "attention_type": "Type of attention mechanism (defaults to MLA).",
                "attn_logits_soft_cap": "Soft cap for attention logits.",
                "sliding_window_size": "Size of the sliding window for attention.",
                "use_ragged_attention": "Whether to use ragged attention.",
                "ragged_block_size": "Block size for ragged attention.",
                "use_qk_norm": "Whether to use normalization on QK.",
                "query_pre_attn_scalar": "Scalar to apply before attention score calculation for query.",
                "use_bias_in_projections": "Whether to use bias in projection layers.",
                "temperature_tuning": "Whether to use temperature tuning.",
                "temperature_tuning_scale": "Scale factor for temperature tuning.",
                "temperature_tuning_floor_scale": "Floor scale for temperature tuning.",
                "prefill_query_axis_names": "Axis names for query during prefill.",
                "prefill_key_axis_names": "Axis names for key during prefill.",
                "prefill_value_axis_names": "Axis names for value during prefill.",
                "query_axis_names": "Axis names for query during normal operation.",
                "key_axis_names": "Axis names for key during normal operation.",
                "value_axis_names": "Axis names for value during normal operation.",
                "ep_query_axis_names": "Axis names for query in expert context.",
                "ep_key_axis_names": "Axis names for key in expert context.",
                "ep_value_axis_names": "Axis names for value in expert context.",
                "input_axis_names": "Axis names for input during normal operation.",
                "ep_input_axis_names": "Axis names for input in expert context.",
                "out_axis_names": "Axis names for output during normal operation.",
                "ep_out_axis_names": "Axis names for output in expert context.",
                "prefill_input_axis_names": "Axis names for input during prefill.",
                "decode_input_axis_names": "Axis names for input during decoding.",
                "prefill_out_axis_names": "Axis names for output during prefill.",
                "decode_out_axis_names": "Axis names for output during decoding.",
                "prefill_cache_axis_order": "Axis order for prefill cache.",
                "ar_cache_axis_order": "Axis order for autoregressive cache.",
                "compute_axis_order": "Axis order for computation.",
                "reshape_q": "Whether to reshape query.",
                "is_nope_layer": "Whether this is a 'nope' layer.",
                "is_vision": "Whether this layer is for vision tasks.",
                "model_mode": "Current model mode (train, prefill, autoregressive).",
                "q_lora_rank": "Rank for LoRA projection of query.",
                "kv_lora_rank": "Rank for LoRA projection of key/value.",
                "qk_nope_head_dim": "Dimension for the non-rotary part of QK heads.",
                "qk_rope_head_dim": "Dimension for the rotary part of QK heads.",
                "v_head_dim": "Dimension for the value heads.",
                "max_position_embeddings": "Maximum position embeddings supported.",
                "original_max_position_embeddings": "Original maximum position embeddings.",
                "mscale": "Scaling factor for softmax.",
                "rope_factor": "Factor for rotary embeddings.",
                "name": "Name of the module.",
                "rngs": "Random number generators for initialization."
            },
            "notes": [
                "This class implements the MLA attention mechanism, which is a specialized form of attention.",
                "It supports LoRA (Low-Rank Adaptation) for query and key/value projections.",
                "It integrates rotary positional embeddings (RoPE).",
                "It handles different model modes: training, prefill, and autoregressive decoding.",
                "It can utilize paged attention for efficient inference.",
                "The `_init_projections` method sets up the specific linear layers and normalization based on configuration.",
                "The `__call__` method orchestrates the forward pass, calling projection methods and the core attention operation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLA module with configuration parameters and sets up internal layers.",
                    "input": {
                        "shape": "N/A (takes configuration and dimension parameters)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set instance attributes from input parameters.",
                        "Initialize base KV cache if applicable.",
                        "Call the parent class constructor (`super().__init__`).",
                        "Initialize MLA KV caches if not in training mode."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "super().__init__",
                        "init_mla_kv_caches"
                    ],
                    "notes": [
                        "Stores various configuration parameters like LoRA ranks, head dimensions, and positional embedding details.",
                        "Initializes `MlaKVCache_0` for KV caching during inference."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the specific linear projection layers (query, key, value) and normalization layers required for MLA attention.",
                    "input": {
                        "shape": "Tuple representing shapes of query and KV inputs.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Asserts required configuration parameters for MLA.",
                        "Initializes standard or LoRA-based query projection (`self.query` or `self.wq_a`, `self.q_norm`, `self.wq_b`).",
                        "Initializes key/value projection using LoRA (`self.wkv_a`, `self.kv_norm`, `self.wkv_b`).",
                        "Sets softmax scaling factor.",
                        "Initializes the output projection layer (`self.out`).",
                        "Sets up `PagedAttentionOp` if paged attention is configured."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "RMSNorm",
                        "paged_attention.PagedAttentionOp",
                        "AttentionType"
                    ],
                    "notes": [
                        "Handles conditional initialization based on `q_lora_rank`.",
                        "Configures `PagedAttentionOp` with specific parameters like page size and number of heads."
                    ]
                },
                "mla_query_projection": {
                    "purpose": "Performs the query projection, including LoRA and rotary embedding application.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dimension] for inputs_q, [batch_size, sequence_length] for inputs_positions",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Recalculates softmax scale.",
                        "Applies standard or LoRA query projection.",
                        "Splits the projected query into non-positional and rotary parts.",
                        "Applies rotary embedding to the positional part.",
                        "Concatenates the parts and scales the result.",
                        "Applies logical constraints based on `model_mode`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_query_heads * qk_head_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.split",
                        "apply_rotary_embedding",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The query projection is scaled by `self.softmax_scale`."
                    ]
                },
                "mla_get_key_value": {
                    "purpose": "Constructs the key and value tensors from latent representations and applies sharding constraints.",
                    "input": {
                        "shape": "Tensors representing latent key components and RoPE-applied key components.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the `wkv_b` projection to `low_rank_main`.",
                        "Splits the output into `key_nope` and `value`.",
                        "Broadcasts `key_rope` to match the key shape.",
                        "Concatenates `key_nope` and `key_rope` to form the final key.",
                        "Applies logical constraints to key and value based on `model_mode`."
                    ],
                    "output": {
                        "shape": "Tuple containing key and value tensors.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.split",
                        "jnp.concatenate",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "This is a helper method to assemble key and value from their components."
                    ]
                },
                "init_mla_kv_caches": {
                    "purpose": "Initializes the MlaKVCache module, which manages KV caches for MLA attention.",
                    "input": {
                        "shape": "Tuple representing the shape of key/value inputs for initialization.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extracts batch size from `inputs_kv_shape`.",
                        "Instantiates `kvcache.MlaKVCache` with relevant parameters."
                    ],
                    "output": {
                        "shape": "An instance of `kvcache.MlaKVCache`.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "kvcache.MlaKVCache"
                    ],
                    "notes": [
                        "Uses placeholder sequence length during initialization as internal cache shapes are determined by `max_prefill_length` and `max_target_length`."
                    ]
                },
                "update_mla_kv_caches": {
                    "purpose": "Updates the MLA KV caches using the `MlaKVCache_0` instance.",
                    "input": {
                        "shape": "Tensors for latent key, RoPE key, decoder segment IDs, and optional `previous_chunk`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.MlaKVCache_0` to update prefill and autoregressive caches.",
                        "If prefill cache is updated, reconstructs key and value using `mla_get_key_value`.",
                        "If autoregressive cache is updated, reconstructs key and value using `mla_get_key_value`.",
                        "Returns the reconstructed KV caches."
                    ],
                    "output": {
                        "shape": "List containing prefill and autoregressive KV cache information.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "mla_get_key_value"
                    ],
                    "notes": [
                        "This method is specific to MLA attention and handles the caching logic."
                    ]
                },
                "mla_kv_projection": {
                    "purpose": "Performs the key/value projection, including LoRA, rotary embedding, and KV cache updates.",
                    "input": {
                        "shape": "Tensors for inputs, inputs_positions, decoder_segment_ids, and optional `previous_chunk`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies `wkv_a` projection to inputs.",
                        "Splits the result into main and rotary components.",
                        "Applies normalization to the main component.",
                        "Applies rotary embedding to the rotary component.",
                        "Constructs key and value using `mla_get_key_value`.",
                        "Updates KV caches if not in training mode and not using paged attention."
                    ],
                    "output": {
                        "shape": "Tuple containing key, value, and cached_values tensors.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "mla_get_key_value",
                        "apply_rotary_embedding",
                        "update_mla_kv_caches",
                        "update_kv_caches"
                    ],
                    "notes": [
                        "Handles the logic for updating KV caches based on the model mode and configuration."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the MLA layer, orchestrating query/key/value projections and the attention operation.",
                    "input": {
                        "shape": "Query inputs, KV inputs, optional positions, segment IDs, and other inference-specific parameters.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints to input tensors based on `model_mode`.",
                        "Performs query projection using `mla_query_projection`.",
                        "Performs key/value projection and cache updates using `mla_kv_projection`.",
                        "Applies checkpointing names to query, key, and value.",
                        "Executes the attention operation using `self.attention_op` or `self.ds_paged_attention_op`.",
                        "Applies logical constraints to the output tensor.",
                        "Performs the final output projection.",
                        "Applies checkpointing name to the output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dimension]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "mla_query_projection",
                        "mla_kv_projection",
                        "checkpoint_name",
                        "self.attention_op",
                        "self.ds_paged_attention_op",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "This is the main entry point for using the MLA layer.",
                        "Handles different attention mechanisms (standard vs. paged) based on configuration.",
                        "Supports various inference-time parameters like `slot` and `page_state` for paged attention."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_compute_axis_order",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_compute_axis_order(s: AxisIdxes) -> None:\n  valid_compute_axis_order = ((0, 1, 2, 3), (0, 2, 1, 3))\n  if s not in valid_compute_axis_order:  # currently supported compute_axis_order\n    raise ValueError(\"Invalid compute_axis_order was passed. Valid options \", valid_compute_axis_order)",
        "analysis": {
            "module_type": "validate_compute_axis_order",
            "purpose": "Validates if the provided compute_axis_order is one of the supported configurations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a tuple of valid compute axis orders.",
                "Check if the input 's' is present in the valid compute axis orders.",
                "Raise a ValueError if 's' is not in the valid options."
            ],
            "output": {
                "shape": "None"
            },
            "dependencies": [
                "AxisIdxes (type hint)"
            ],
            "parameters": {
                "s": "The compute_axis_order to validate, expected to be a tuple of integers."
            },
            "notes": [
                "The function does not return any value; it only raises an error if the input is invalid.",
                "The currently supported compute_axis_order configurations are (0, 1, 2, 3) and (0, 2, 1, 3)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#apply_mask_to_logits",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def apply_mask_to_logits(logits: Array, mask: Array):\n  \"\"\"Applies a floating-point mask to a set of logits.\n\n  The mask is represented as a tensor with some dtype where 0 represents true and values\n  below a large negative number (here set to\n  get_large_negative_number(logits.dtype) / 2) represent false. Applying the mask\n  leaves the logits alone in the true case and replaces them by\n  get_large_negative_number(logits.dtype) in the false case. Previously, this was\n  done by adding the logits to the mask; however, this leads to a bad fusion\n  decision in the compiler that saves the values in memory rather than\n  just the predicate. This implementation avoids that problem.\n\n  from https://github.com/google/praxis/blob/4712a6b9ee13e224b86e235ff55f7c6bab9fbab3/praxis/py_utils.py#L706\n\n  Args:\n    logits: A JTensor of logit values.\n    mask: A JTensor of mask values with the encoding described in the\n      function documentation.\n\n  Returns:\n    Masked logits.\n  \"\"\"\n  return jnp.where((mask >= DEFAULT_MASK_VALUE * 0.5), logits, DEFAULT_MASK_VALUE)",
        "analysis": {
            "functionality": "Applies a floating-point mask to logits, setting masked-out positions to a large negative value.",
            "usage": "This function takes two JAX arrays, `logits` and `mask`, as input. The `mask` array determines which positions in `logits` should be masked. Positions where `mask` is greater than or equal to half of `DEFAULT_MASK_VALUE` retain their original `logits` value. Otherwise, the corresponding `logits` value is replaced by `DEFAULT_MASK_VALUE`. It returns the modified `logits` array."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_flash_attention_with_sinks_on_gpu",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_flash_attention_with_sinks_on_gpu(sinks: Array | None) -> None:\n  \"\"\"Helper function to check for sinks with flash attention on GPU.\"\"\"\n  if sinks is not None:\n    raise ValueError(\"The flash attention with sinks is not supported on GPU yet.\")",
        "analysis": {
            "functionality": "Validates that 'sinks' is None when using flash attention on a GPU.",
            "usage": "This function is a helper to check for unsupported configurations. It raises a ValueError if 'sinks' is not None, indicating that flash attention with sinks is not yet supported on GPU. It takes an optional 'sinks' argument of type Array or None."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#ChunkedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class ChunkedCausalMask(splash_attention_mask._ComputableMask):  # pylint: disable=protected-access\n  \"\"\"Lazy chunked causal mask.\n\n  Attention is causal within each chunk (0, K), (K, 2K), (2K, 3K), ... tokens attend to each other but not across chunks.\n  Llama4 models use interleaved chunk attention along with global attention.\n\n  This mask class inherits from splash_attention_mask._ComputableMask and is designed to be used with Splash Attention.\n  It allows the mask logic to be computed on-the-fly or fused into the attention kernel, avoiding the memory cost of\n  materializing the full (sequence_length, sequence_length) boolean mask array, which can be prohibitive for long sequences.\n\n  Attributes:\n    chunk_size: The size of each attention chunk.\n  \"\"\"\n\n  chunk_size: int\n\n  def __init__(\n      self,\n      shape: tuple[int, int],\n      chunk_size: int,\n      shard_count: int = 1,\n  ):\n    if chunk_size <= 0:\n      raise ValueError(\"chunk_size must be positive\")\n    self.chunk_size = chunk_size\n\n    # Define the mask function for chunk attention\n    def chunked_causal_mask_function(q_ids, kv_ids):\n      \"\"\"Computes the mask logic for the given slice indices.\"\"\"\n      if q_ids.size == 0 or kv_ids.size == 0:\n        return np.empty((q_ids.shape[0], kv_ids.shape[1]), dtype=np.bool_)\n\n      # Condition 1: Same chunk\n      q_chunk = q_ids // self.chunk_size\n      kv_chunk = kv_ids // self.chunk_size\n      same_chunk = q_chunk == kv_chunk\n\n      # Condition 2: Causal\n      causal = q_ids >= kv_ids\n\n      return same_chunk & causal\n\n    # Initialize the parent ComputableMask with this function\n    super().__init__(\n        shape=shape,\n        mask_function=chunked_causal_mask_function,\n        shard_count=shard_count,\n    )\n\n  # Implement equality and hashing based on relevant attributes\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n    # Compare shape, chunk_size, and the underlying q_sequence array\n    return (\n        self.shape == other.shape\n        and self.chunk_size == other.chunk_size\n        and np.array_equal(self.q_sequence, other.q_sequence)\n    )\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.chunk_size,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "chunked_causal_mask",
            "purpose": "Implements a lazy chunked causal mask for attention mechanisms, where attention is restricted within defined chunks and causally within those chunks.",
            "input": {
                "shape": "N/A (constructor takes shape, chunk_size, shard_count)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes with shape, chunk_size, and shard_count.",
                "Defines a `chunked_causal_mask_function` that computes mask logic based on query and key IDs relative to chunk size.",
                "Initializes the parent `_ComputableMask` with the defined mask function."
            ],
            "output": {
                "shape": "N/A (The mask function computes boolean values for given indices)"
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "numpy"
            ],
            "parameters": {
                "chunk_size": "The size of each attention chunk. Must be a positive integer."
            },
            "notes": [
                "This mask is designed to be computed on-the-fly or fused into attention kernels to save memory for long sequences.",
                "It inherits from `_ComputableMask` and overrides equality and hashing methods.",
                "The `chunked_causal_mask_function` checks if query and key IDs are in the same chunk and if the query ID is greater than or equal to the key ID."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the ChunkedCausalMask with shape, chunk size, and shard count.",
                    "input": {
                        "shape": "shape: tuple[int, int], chunk_size: int, shard_count: int = 1",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates that chunk_size is positive.",
                        "Stores chunk_size.",
                        "Defines the internal `chunked_causal_mask_function`.",
                        "Calls the parent `_ComputableMask` constructor."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "splash_attention_mask._ComputableMask",
                        "numpy"
                    ],
                    "notes": [
                        "The `chunked_causal_mask_function` determines attention validity based on chunk membership and causality."
                    ]
                },
                "__eq__": {
                    "purpose": "Compares this ChunkedCausalMask instance with another object for equality.",
                    "input": {
                        "shape": "other: object",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the other object is of the same type.",
                        "Compares shape, chunk_size, and the underlying q_sequence array."
                    ],
                    "output": {
                        "shape": "bool",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "numpy"
                    ],
                    "notes": [
                        "Equality is determined by shape, chunk_size, and the content of q_sequence."
                    ]
                },
                "__hash__": {
                    "purpose": "Computes the hash of the ChunkedCausalMask instance.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Hashes the type, shape, chunk_size, and the byte representation of q_sequence."
                    ],
                    "output": {
                        "shape": "int",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Ensures that equal instances have the same hash value."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_generate_chunk_attention_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _generate_chunk_attention_mask(mask_shape: tuple[int, int], chunk_size: int, q_offset: int = 0) -> jax.Array:\n  \"\"\"Generates an explicit boolean mask for chunked causal attention.\n\n  This function computes the full boolean mask array where True indicates\n  attention is allowed based on chunked causal rules (tokens attend only\n  within the same chunk, and causally within that chunk).\n\n  Args:\n    mask_shape: The desired shape of the mask (q_seq_len, kv_seq_len).\n    chunk_size: The size of the attention chunks.\n\n  Returns:\n    A boolean mask of shape `mask_shape` where True indicates attention is\n    allowed according to chunked causal rules, and False otherwise.\n\n  Raises:\n    ValueError: If chunk_window_size is None or not positive.\n  \"\"\"\n\n  row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0) + q_offset\n  col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n  if chunk_size <= 0:\n    raise ValueError(\"chunk_size must be positive\")\n\n  # chunk mask calculation\n  same_chunk = (row_ids // chunk_size) == (col_ids // chunk_size)\n  chunk_mask = same_chunk & (row_ids >= col_ids)\n  return chunk_mask",
        "analysis": {
            "module_type": "chunked_causal_attention_mask_generator",
            "purpose": "Generates a boolean mask for chunked causal attention, allowing tokens to attend only within their own chunk and causally within that chunk.",
            "input": {
                "shape": "[q_seq_len, kv_seq_len]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Generate row and column IDs using broadcasted_iota.",
                "Check if chunk_size is positive.",
                "Calculate 'same_chunk' mask: checks if row and column IDs belong to the same chunk.",
                "Calculate 'chunk_mask': combines 'same_chunk' with a causal check (row_ids >= col_ids)."
            ],
            "output": {
                "shape": "[q_seq_len, kv_seq_len]"
            },
            "dependencies": [
                "jax.lax.broadcasted_iota",
                "jnp.int32"
            ],
            "parameters": {
                "mask_shape": "The desired shape of the mask (q_seq_len, kv_seq_len).",
                "chunk_size": "The size of the attention chunks.",
                "q_offset": "An optional offset for query IDs."
            },
            "notes": [
                "The generated mask is boolean, where True indicates allowed attention.",
                "Raises ValueError if chunk_size is not positive."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_block_mask_indices",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_block_mask_indices(bidirectional_mask):\n  \"\"\"Creates block mask identifying segments based on a bidirectional mask.\n\n  Args:\n    bidirectional_mask: boolean mask, e.g. [011110011010].\n\n  Returns:\n    block mask for segments, e.g. [011110022030].\n  \"\"\"\n  # Left pad 0.\n  padded_mask = jnp.pad(bidirectional_mask, [(0, 0), (1, 0)], constant_values=0)\n  boundary = padded_mask[..., 1:] > padded_mask[..., :-1]\n  numbered_boundary = jnp.cumsum(boundary, axis=-1)\n  return bidirectional_mask * numbered_boundary",
        "analysis": {
            "functionality": "Creates a block mask that identifies segments based on a bidirectional mask.",
            "usage": "Takes a bidirectional boolean mask as input and returns a block mask where segments are numbered. For example, an input like [011110011010] might produce [011110022030]."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_bidirectional_block_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_bidirectional_block_mask(bidirectional_mask):\n  \"\"\"Creates bidirectional block mask from bidirectional_mask, where True corresponds to image tokens.\n  bidirectional_mask shape: [B, L]\n  bidirectional_block_mask shape: [B, L, L]\n  Examples:\n  bidirectional_mask = [[0, 1, 1, 1, 0, 0]]\n  bidirectional_block_mask = [[\n      [False, False, False, False, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False, False, False, False, False, False],\n      [False, False, False, False, False, False],\n  ]]\n  \"\"\"\n  q_block_indices = _make_block_mask_indices(bidirectional_mask)\n  kv_block_indices = q_block_indices\n  bidirectional_block_mask = (kv_block_indices[:, None, :] == q_block_indices[..., None]) & (\n      q_block_indices[..., None] > 0\n  )\n  return bidirectional_block_mask",
        "analysis": {
            "functionality": "Creates a bidirectional block mask from a given bidirectional mask.",
            "usage": "The function takes a 2D boolean array `bidirectional_mask` (shape [B, L]) as input. It returns a 3D boolean array `bidirectional_block_mask` (shape [B, L, L]) where `True` indicates that attention is allowed between corresponding tokens based on block segmentation derived from the input mask. Specifically, it identifies segments in the input mask and creates a mask where tokens within the same segment can attend to each other."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#attention_op_as_linen",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def attention_op_as_linen(\n    *,\n    config: Config,\n    mesh: Mesh,\n    attention_kernel: str,\n    max_target_length: int,\n    num_query_heads: int,\n    num_kv_heads: int,\n    float32_qk_product: bool = False,\n    max_prefill_predict_length: int = -1,\n    float32_logits: bool = False,\n    flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n    flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n    flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n    flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n    ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    reshape_q: bool = False,\n    dropout_rate: float = 0.0,\n    dtype: DType = jnp.float32,\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    chunk_attn_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n):\n  \"\"\"A factory function to create an AttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `AttentionOp` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      AttentionOp,\n      config=config,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      max_target_length=max_target_length,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      float32_qk_product=float32_qk_product,\n      max_prefill_predict_length=max_prefill_predict_length,\n      float32_logits=float32_logits,\n      flash_axis_names_q=flash_axis_names_q,\n      flash_axis_names_q_ep=flash_axis_names_q_ep,\n      flash_axis_names_kv=flash_axis_names_kv,\n      flash_axis_names_kv_ep=flash_axis_names_kv_ep,\n      flash_axis_names_splash_kernel=flash_axis_names_splash_kernel,\n      flash_axis_names_splash_kernel_ep=flash_axis_names_splash_kernel_ep,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      ragged_qkv_axis_names=ragged_qkv_axis_names,\n      ragged_lengths_names=ragged_lengths_names,\n      compute_axis_order=compute_axis_order,\n      key_axis_order=key_axis_order,\n      reshape_q=reshape_q,\n      dropout_rate=dropout_rate,\n      dtype=dtype,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      chunk_attn_window_size=chunk_attn_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "attention_op_factory",
            "purpose": "A factory function to create an AttentionOp as a Linen module, bridging NNX-based AttentionOp with Linen models.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `AttentionOp` class into a Linen module.",
                "Passes all arguments to the `AttentionOp` constructor."
            ],
            "output": {
                "shape": "N/A (returns a Linen module)"
            },
            "dependencies": [
                "flax.linen as nn",
                "flax.nnx",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.initializers.variable_to_logically_partitioned",
                "MaxText.common_types",
                "jax.numpy as jnp"
            ],
            "parameters": {
                "config": "Configuration object for the attention operation.",
                "mesh": "Device mesh for distributed computation.",
                "attention_kernel": "Specifies the attention implementation to use (e.g., 'flash', 'dot_product').",
                "max_target_length": "Maximum sequence length for the target (decoder) side.",
                "num_query_heads": "Number of heads for the query.",
                "num_kv_heads": "Number of heads for the key/value.",
                "dtype": "Data type for computations.",
                "attention_type": "Type of attention (e.g., GLOBAL, LOCAL_SLIDING, CHUNK)."
            },
            "notes": [
                "This function acts as a wrapper to instantiate and configure the `AttentionOp` within a Flax Linen model.",
                "It forwards a large number of configuration parameters to the underlying `AttentionOp`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#AttentionOp",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class AttentionOp(nnx.Module):\n  \"\"\"Attention operation\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      attention_kernel: str,\n      max_target_length: int,\n      num_query_heads: int,\n      num_kv_heads: int,\n      float32_qk_product: bool = False,\n      max_prefill_predict_length: int = -1,\n      float32_logits: bool = False,\n      flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n      flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n      flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n      flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n      ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      reshape_q: bool = False,\n      dropout_rate: float = 0.0,\n      dtype: DType = jnp.float32,\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      chunk_attn_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      rngs: nnx.Rngs | None = None,\n  ):\n    \"\"\"Initializes the AttentionOp module.\n\n    Args:\n      config: The configuration for the model.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use.\n      max_target_length: The maximum target length.\n      num_query_heads: The number of query heads.\n      num_kv_heads: The number of key/value heads.\n      float32_qk_product: Whether to compute qk_product in float32.\n      max_prefill_predict_length: The maximum prefill predict length.\n      float32_logits: Whether to compute logits in float32.\n      flash_axis_names_kv: The logical axis names for the KV cache in flash attention.\n      flash_axis_names_q: The logical axis names for the query in flash attention.\n      flash_axis_names_splash_kernel: The logical axis names for the splash attention kernel.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      ragged_qkv_axis_names: The logical axis names for ragged QKV tensors.\n      ragged_lengths_names: The logical axis names for ragged lengths.\n      compute_axis_order: The order of axes for computation.\n      key_axis_order: The order of axes for the key.\n      ... and other configuration parameters.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.max_target_length = max_target_length\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.float32_qk_product = float32_qk_product\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.float32_logits = float32_logits\n    self.flash_axis_names_q = flash_axis_names_q\n    self.flash_axis_names_q_ep = flash_axis_names_q_ep\n    self.flash_axis_names_kv = flash_axis_names_kv\n    self.flash_axis_names_kv_ep = flash_axis_names_kv_ep\n    self.flash_axis_names_splash_kernel = flash_axis_names_splash_kernel\n    self.flash_axis_names_splash_kernel_ep = flash_axis_names_splash_kernel_ep\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.ragged_qkv_axis_names = ragged_qkv_axis_names\n    self.ragged_lengths_names = ragged_lengths_names\n    self.compute_axis_order = compute_axis_order\n    self.key_axis_order = key_axis_order\n    self.reshape_q = reshape_q\n    self.dropout_rate = dropout_rate\n    self.dtype = dtype\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.chunk_attn_window_size = chunk_attn_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n    self.rngs = rngs\n\n    def maybe_create_nnx(einsum, *args):\n      if isinstance(einsum, nn.Module):\n        return nnx_wrappers.ToNNX(einsum, rngs=rngs).lazy_init(*args)\n      return einsum\n\n    # qk_product\n    if self.kv_quant:\n      # Dummy inputs for lazy initialization\n      b = 1\n      t_prefill = self.max_prefill_predict_length\n      t_ar = 1  # Autoregressive mode has a query length of 1\n      n = self.num_query_heads\n      n_kv = self.num_kv_heads\n      d = self.config.head_dim\n      g = n // n_kv\n      s_prefill = self.max_prefill_predict_length\n      s_ar = self.max_target_length\n\n      # Dummy query/key/value shapes as before...\n      dummy_query_prefill = jnp.zeros((b, t_prefill, n_kv, g, d), dtype=self.dtype)\n      dummy_key_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_query_ar = jnp.zeros((b, t_ar, n_kv, g, d), dtype=self.dtype)\n      dummy_key_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      dummy_attn_weights_prefill = jnp.zeros((b, n_kv, g, t_prefill, s_prefill), dtype=jnp.float32)\n      dummy_value_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_attn_weights_ar = jnp.zeros((b, n_kv, g, t_ar, s_ar), dtype=jnp.float32)\n      dummy_value_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      # Prefill AqtEinsum instances\n      self.AqtEinsum_0 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_prefill, dummy_key_prefill\n      )\n      self.AqtEinsum_1 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_prefill,\n          dummy_value_prefill,\n      )\n      # Autoregressive AqtEinsum instances\n      self.AqtEinsum_2 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_ar, dummy_key_ar\n      )\n      self.AqtEinsum_3 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_ar,\n          dummy_value_ar,\n      )\n    else:\n      self.AqtEinsum_0 = jnp.einsum\n      self.AqtEinsum_1 = jnp.einsum\n      self.AqtEinsum_2 = jnp.einsum\n      self.AqtEinsum_3 = jnp.einsum\n\n  def check_attention_inputs(self, query: Array, key: Array | KVTensor, value: Array | KVTensor) -> None:\n    \"\"\"Check attention inputs.\"\"\"\n\n    assert key.ndim == value.ndim, f\"k (dim {key.ndim}), v (dim {value.ndim}) must have same rank.\"\n    assert (\n        query.shape[:-3] == key.shape[:-3] == value.shape[:-3]\n    ), f\"{query.shape[:-3]=}, {key.shape[:-3]=}, {value.shape[:-3]=} batch dims must match.\"\n    assert key.shape[-2] == value.shape[-2], \"k, v num_kv_heads must match.\"\n    assert key.shape[-3] == value.shape[-3], \"k, v lengths must match.\"\n    assert query.shape[-1] == key.shape[-1], \"q, k depths must match.\"\n\n  def generate_attention_mask(\n      self,\n      query,\n      key,\n      decoder_segment_ids: Array | None,\n      model_mode: str,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n  ) -> Array | None:\n    \"\"\"Generates a combined attention mask for Transformer models.\n\n    This function constructs an attention mask by potentially combining\n    several types of masks based on the input parameters and model\n    configuration. The generated mask dictates which query-key pairs are\n    allowed to attend to each other.\n\n    The masking logic can enforce:\n    1.  **Sequence Separation:** Using `decoder_segment_ids`, attention is\n      confined within distinct sequences in a batch. This is crucial when\n      multiple unrelated sequences are packed together.\n    2.  **Causality:** Preventing attention to future positions. This is\n      standard for autoregressive decoding. For chunked prefill, as\n      described in the SARATHI paper [2], causality is adjusted based\n      on `previous_chunk` information.\n    3.  **Specialized Attention Patterns:** Depending on `self.attention_type`,\n      it can apply:\n      * Local Sliding Window Attention: Restricts attention to a\n          fixed-size window around each query position.\n      * Chunk Attention: Divides sequences into chunks and applies\n          masking at the chunk level.\n    4.  **Bidirectional Attention for Sub-sequences:** If `bidirectional_mask`\n      is provided (e.g., for image tokens in a multimodal model),\n      those parts of the sequence can attend bidirectionally, and this\n      mask is OR-ed with other generated masks.\n\n    The overall approach and specific masking techniques are influenced by\n    efficient attention mechanisms like those found in the Pallas MHA\n    Flash Attention reference [1].\n\n    Args:\n      query: The query tensor, typically of shape\n          `[batch_size, q_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      key: The key tensor, typically of shape\n          `[batch_size, kv_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      decoder_segment_ids: Optional `Array` of shape `[batch_size, q_sequence_length]`.\n          Identifies distinct sequences within the batch. Attention is\n          restricted to elements within the same segment ID. In autoregressive\n          mode, specific values (e.g., `common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR`)\n          can mark the currently active sequence for decoding.\n      model_mode: A string (e.g., `common_types.MODEL_MODE_AUTOREGRESSIVE`,\n          `MODEL_MODE_PREFILL`) indicating the operational\n          mode. This significantly influences mask generation, particularly\n          how causality and segment separation are handled.\n      previous_chunk: Optional. Information about previously processed\n          key/value chunks, often a tensor representing the previous keys/values.\n          Used to correctly offset causal masks in chunked attention or\n          streaming scenarios. Its shape might be\n          `[batch_size, prev_kv_sequence_length, ...]`.\n      bidirectional_mask: Optional `Array` of shape `[batch_size, kv_sequence_length]`.\n          If provided, this boolean mask indicates tokens (e.g., image tokens)\n          that are allowed to attend bidirectionally. The resulting\n          block-wise bidirectional mask is combined with other masks using a\n          logical OR.\n\n    Returns:\n      An `Array` representing the attention mask, broadcastable to the shape\n      `[batch_size, num_heads, q_sequence_length, kv_sequence_length]`.\n      Positions with `0.0` allow attention, while positions with\n      `DEFAULT_MASK_VALUE` (a large negative number) prevent it.\n      Returns `None` if no masking is determined to be necessary based on\n      the inputs and configuration.\n\n    References:\n      [1] JAX Pallas MHA Flash Attention:\n          https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py\n      [2] SARATHI: Efficient LLM Inference by Piggybacking Decodes with\n          Chunked Prefills - ArXiv:2308.16369 (https://arxiv.org/abs/2308.16369)\n    \"\"\"\n    mask = None\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      mask = decoder_segment_ids[:, None, None, None, :] == DECODING_ACTIVE_SEQUENCE_INDICATOR\n    elif decoder_segment_ids is not None:\n      mask = decoder_segment_ids[:, :, None] == decoder_segment_ids[:, None, :]\n      mask = mask[:, None, None, :, :]\n\n    _, q_seq_len, _, _ = query.shape\n    _, kv_seq_len, _, _ = key.shape\n    next_pos = 0\n    if previous_chunk is not None:\n      next_pos = previous_chunk.shape[1]\n      if mask is not None:\n        mask = mask[:, :, :, next_pos : next_pos + q_seq_len, :]\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and q_seq_len == 1:\n      # In autoregression, the query position is the last position in the KV sequence.\n      next_pos = kv_seq_len - 1\n\n    causal_mask = None\n    # We enforce causality except for AUTOREGRESSION\n    if model_mode != MODEL_MODE_AUTOREGRESSIVE and self.attention_type != AttentionType.FULL:\n      mask_shape = (q_seq_len, kv_seq_len)\n      # row_ids indicates the position of query\n      # col_ids indicates the position of kv\n      row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)\n      col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n      # Attention mask for chunked prefill is generated in the same way\n      # as mentioned in SARATHI - https://arxiv.org/abs/2308.16369\n      causal_mask = (col_ids <= row_ids + next_pos)[None, None, None, :, :]\n\n    output_mask = None\n    if (mask is not None) and (causal_mask is not None):\n      output_mask = jnp.logical_and(mask, causal_mask)\n    elif mask is not None:\n      output_mask = mask\n    elif causal_mask is not None:\n      output_mask = causal_mask\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and output_mask is not None:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n\n      row_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (q_seq_len, 1), 0) + next_pos\n      col_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (1, kv_seq_len), 1)\n      sliding_mask = (col_ids_sliding > (row_ids_sliding - self.sliding_window_size)) & (\n          col_ids_sliding <= row_ids_sliding\n      )\n      output_mask = sliding_mask * output_mask\n    elif self.attention_type == AttentionType.CHUNK and output_mask is not None:\n      mask_shape = (q_seq_len, kv_seq_len)\n      chunk_mask = _generate_chunk_attention_mask(\n          mask_shape=(q_seq_len, kv_seq_len), chunk_size=self.chunk_attn_window_size, q_offset=next_pos\n      )\n      output_mask = chunk_mask * output_mask\n\n    if bidirectional_mask is not None:\n      image_mask = _make_bidirectional_block_mask(bidirectional_mask)\n      output_mask = output_mask | image_mask[:, None, None, ...]\n\n    return jnp.where(output_mask, 0.0, DEFAULT_MASK_VALUE) if output_mask is not None else None\n\n  def calculate_moba_gate_logic(self, q_item, k_item, q_pos_item):\n    \"\"\"Computes the block-level MoBA gating intermediates for one batch item.\n\n    Args:\n      q_item: Query tensor shaped `[q_len, n_q_heads, head_dim]`.\n      k_item: Key tensor shaped `[kv_len, n_kv_heads, head_dim]`.\n      q_pos_item: Absolute query positions shaped `[q_len]`, used to derive the\n        chunk index for each query.\n        For example, during prefill after 128 tokens\n        have been processed `q_pos_item` is `jnp.arange(128, 128 + q_len)`,\n        while in autoregressive decode with a single query token it is\n        `jnp.array([kv_len - 1])`.\n\n    Returns:\n      `need_attend`, a boolean mask of shape `[n_kv_heads, g, q_len, num_block]`\n      indicating which key blocks each query should attend to. The additional\n      values in the returned tuple are debug intermediates used for logging and\n      diagnostics when inspecting the gating behaviour.\n    \"\"\"\n    q_len, n_q_heads, head_dim = q_item.shape\n    kv_len, n_kv_heads, _ = k_item.shape\n    g = n_q_heads // n_kv_heads\n\n    q_item_f32 = q_item.astype(jnp.float32).reshape(q_len, n_kv_heads, g, head_dim)  # grouped-query attention (GQA)\n\n    moba_chunk_size = self.config.moba_chunk_size\n    moba_topk = self.config.moba_topk\n\n    num_block = math.ceil(kv_len / moba_chunk_size)\n\n    block_ids = jnp.arange(kv_len, dtype=jnp.int32) // moba_chunk_size  # chunk index for each key position\n    # Sum key vectors per chunk so we can later average within each block.\n    key_gate_weight_sum = jax.ops.segment_sum(\n        k_item.astype(jnp.float32), block_ids, num_segments=num_block\n    )  # [num_block, n_kv_heads, head_dim]\n    # Count how many tokens end up in each chunk so we can take the mean.\n    block_counts = jax.ops.segment_sum(\n        jnp.ones((kv_len,), dtype=jnp.float32), block_ids, num_segments=num_block\n    )  # [num_block]\n    # Mean Pooling, Avoid division by zero for empty blocks.\n    key_gate_weight = key_gate_weight_sum / jnp.maximum(\n        block_counts[:, None, None], 1\n    )  # [num_block, n_kv_heads, head_dim]\n\n    # Take the dot product between each query and every key chunk to get a score.\n    gate = jnp.einsum(\"skgd,Nkd->kgsN\", q_item_f32, key_gate_weight)  # [n_kv_heads, g, q_len, num_block]\n    gate_before_masking = gate\n\n    q_block_idx = q_pos_item // moba_chunk_size  # chunk id for each query\n    block_indices = jnp.arange(num_block)  # list every key chunk index\n\n    q_block_idx_b = jnp.expand_dims(q_block_idx, axis=-1)  # [q_len, 1]\n    block_indices_b = jnp.expand_dims(block_indices, axis=0)  # [1, num_block]\n\n    # Block-causal masking: a query can't attend to future key blocks,\n    # and must attend to its own key block.\n    mask_future = q_block_idx_b > block_indices_b\n    gate = jnp.where(mask_future, gate, -float(\"inf\"))\n    mask_diag = q_block_idx_b == block_indices_b\n    gate = jnp.where(mask_diag, float(\"inf\"), gate)\n    gate_after_masking = gate\n\n    k_for_topk = min(moba_topk, num_block)\n    gate_top_k_val, gate_top_k_idx = jax.lax.top_k(gate, k=k_for_topk)  # [n_kv_heads, g, q_len, k_for_topk]\n    gate_top_k_val_min = jnp.min(gate_top_k_val, axis=-1, keepdims=True)  # [n_kv_heads, g, q_len, 1]\n    need_attend_threshold_mask = gate >= gate_top_k_val_min  # [n_kv_heads, g, q_len, num_block]\n\n    # Tie-breaking: if multiple blocks have the same gate value as the k-th\n    # block, we only select the ones that appear in the top-k indices.\n    gate_idx_mask = jnp.sum(\n        jax.nn.one_hot(gate_top_k_idx, num_block, dtype=jnp.bool_), axis=-2\n    )  # [n_kv_heads, g, q_len, num_block]\n    need_attend = jnp.logical_and(need_attend_threshold_mask, gate_idx_mask)  # [n_kv_heads, g, q_len, num_block]\n\n    return (\n        key_gate_weight,\n        gate_before_masking,\n        gate_after_masking,\n        gate_top_k_val,\n        gate_top_k_idx,\n        gate_top_k_val_min,\n        need_attend_threshold_mask,\n        gate_idx_mask,\n        need_attend,  # [n_kv_heads, g, q_len, num_block]\n    )\n\n  def generate_moba_mask_single_item(self, q_item, k_item, q_positions):\n    \"\"\"Generates the token-level MoBA additive mask for a single batch item.\"\"\"\n    q_len, _, _ = q_item.shape\n    kv_len, _, _ = k_item.shape\n    moba_chunk_size = self.config.moba_chunk_size\n\n    # Run the gating logic to find which key blocks this query cares about.\n    *_, need_attend = self.calculate_moba_gate_logic(q_item, k_item, q_positions)\n\n    # Expand the block-level `need_attend` mask to a token-level mask.\n    k_block_indices = jnp.arange(kv_len, dtype=jnp.int32) // moba_chunk_size\n    token_level_need_attend = need_attend[..., k_block_indices]\n\n    # Convert the boolean mask to float mask values.\n    gate = jnp.where(token_level_need_attend, 0.0, -float(\"inf\"))\n\n    # Apply a final per-token causal mask to ensure causality within chunks.\n    k_indices = jax.lax.broadcasted_iota(jnp.int32, (q_len, kv_len), 1)\n    q_indices = q_positions[:, None]\n    causal_mask = q_indices >= k_indices\n    gate = jnp.where(causal_mask, gate, -float(\"inf\"))\n\n    # Return the additive mask for this batch item.\n    return gate\n\n  def _generate_moba_mask(self, query: Array, key: Array, q_positions: Array) -> Array:\n    \"\"\"Builds the token-level MoBA additive mask for the whole batch.\n\n    Args:\n      query: Query tensor shaped `[batch, q_len, n_q_heads, head_dim]`.\n      key: Key tensor shaped `[batch, kv_len, n_kv_heads, head_dim]`.\n      q_positions: Absolute query positions shaped `[q_len]`, shared across the\n        batch, identifying the starting offset of each query token.\n        For example, in prefill after 128 tokens we pass\n        `jnp.arange(128, 128 + q_len)`, while in autoregressive decode with a\n        single new token the vector is `[kv_len - 1]` for each batch element.\n\n    Returns:\n      Additive attention mask with shape\n      `[batch, n_kv_heads, n_q_heads // n_kv_heads, q_len, kv_len]` containing\n      `0.` for permitted positions and `-inf` for masked ones.\n    \"\"\"\n    # vmap over the batch dimension of query and key. q_positions is constant across the batch.\n    moba_mask = jax.vmap(self.generate_moba_mask_single_item, in_axes=(0, 0, None))(query, key, q_positions)\n    return moba_mask\n\n  def apply_attention(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      lengths: Array | None,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply attention\"\"\"\n    self.check_attention_inputs(query, key, value)\n    length = query.shape[-3]\n    target_hardware = self.mesh.devices[(0,) * self.mesh.devices.ndim].platform\n\n    if use_ragged_attention and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      if lengths is None:\n        lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      if target_hardware == \"tpu\":\n        impl = self.tpu_ragged_attention\n      elif target_hardware == \"gpu\":\n        impl = self.gpu_ragged_attention\n      else:\n        raise NotImplementedError(target_hardware)\n      return impl(query, key, value, lengths, self.ragged_block_size)\n\n    elif (\n        self.attention_kernel == \"dot_product\"\n        or (self.attention_kernel == \"autoselected\" and model_mode == MODEL_MODE_AUTOREGRESSIVE)\n        or (self.attention_kernel == \"autoselected\" and length < 128)\n        or (self.attention_kernel == \"paged\")\n    ):\n      return self.apply_attention_dot(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          previous_chunk,\n          bidirectional_mask=bidirectional_mask,\n          sinks=sinks,\n          qk_product_einsum=qk_product_einsum,\n          wv_product_einsum=wv_product_einsum,\n      )\n    elif self.attention_kernel in (\"flash\", \"autoselected\"):\n      if target_hardware == \"tpu\":\n        if isinstance(key, KVTensor):\n          key = key.dequant()\n        if isinstance(value, KVTensor):\n          value = value.dequant()\n\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          raise ValueError(\n              \"\"\"Decode not supported with flash attention.\n                              Use `dot_product` instead.\"\"\"\n          )\n        return (\n            self.tpu_flash_attention(query, key, value, decoder_segment_ids, self.attn_logits_soft_cap, sinks),\n            None,\n            None,\n        )\n      else:\n        validate_flash_attention_with_sinks_on_gpu(sinks)\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          # fallback to dot_product as pallas gpu flash attention doesn't support decode stage\n          return self.apply_attention_dot(\n              query,\n              key,\n              value,\n              decoder_segment_ids,\n              model_mode,\n              bidirectional_mask=bidirectional_mask,\n              qk_product_einsum=qk_product_einsum,\n              wv_product_einsum=wv_product_einsum,\n          )\n        else:\n          head_axis = -2\n          num_query_heads = query.shape[head_axis]\n          num_kv_heads = key.shape[head_axis]\n          if num_query_heads != num_kv_heads:\n            # Handle cases where the number of query heads is different from the number of key/value heads.\n            if num_query_heads % num_kv_heads != 0:\n              raise ValueError(\n                  f\"Number of query heads ({num_query_heads}) must be divisible by number of key/value heads ({num_kv_heads}).\"\n              )\n            # TODO Investigate if the KV copy can be eliminated. It's likely redundant.\n            q_heads_per_kv_head = num_query_heads // num_kv_heads\n\n            key = jnp.repeat(\n                key, q_heads_per_kv_head, axis=head_axis\n            )  # key shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n            value = jnp.repeat(\n                value, q_heads_per_kv_head, axis=head_axis\n            )  # value shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n          out = gpu_pallas_attention.mha(query, key, value, decoder_segment_ids, sm_scale=1.0, causal=True)\n          return out, None, None\n    elif self.attention_kernel == \"cudnn_flash_te\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n        raise ValueError(\n            \"\"\"Decode not supported with flash attention.\n                           Use `dot_product` instead.\"\"\"\n        )\n      return self.cudnn_flash_attention(query, key, value, decoder_segment_ids, model_mode), None, None\n    elif self.attention_kernel == \"cudnn_flash_jax\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      return *self.cudnn_jax_flash_attention(query, key, value, decoder_segment_ids, model_mode), None\n    else:\n      raise ValueError(f\"Unexpected attention kernel {self.attention_kernel=}.\")\n\n  def gpu_ragged_attention(self, q: Array, k: Array | KVTensor, v: Array | KVTensor, lengths: Array, block_size: int):\n    \"\"\"gpu ragged attention\"\"\"\n    batch_size, q_length, q_heads, head_dim = q.shape\n\n    # Reshape q to match gqa's expected shape\n    q_for_gqa = q.squeeze(axis=1)\n\n    # Define logical axis names - clearer and avoids repeated calls.\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n    bnd = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS, CACHE_KV))\n    bn = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS))\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(bnd, bsnd, bsnd, b, None),\n        out_specs=(bnd, bn, bn),\n        check_vma=False,\n    )\n    def wrap_ragged_attention(\n        q: Array, k: Array, v: Array, lengths: Array, block_size: int\n    ) -> Tuple[Array, Array, Array]:\n      # Use the original gqa function to get the attention output\n      \"\"\"\n      Wraps the GQA function with appropriate sharding.\n\n      Args:\n          q: Query tensor.\n          k: Key tensor.\n          v: Value tensor.\n          lengths: Sequence lengths.\n          block_size: Block size for attention.\n\n      Returns:\n          A tuple containing the output, max, and sum tensors.\n      \"\"\"\n      # Use the original gqa function to get the attention output\n      local_out, (local_sum, local_max) = gpu_pallas_decode_attention.gqa(\n          q=q,\n          k=k,\n          v=v,\n          kv_seq_len=lengths,\n          block_k=block_size,\n          sm_scale=1.0,\n          return_residuals=True,\n          normalize_output=False,\n      )\n      return local_out, local_max, local_sum\n\n    local_out, local_max, local_sum = wrap_ragged_attention(q_for_gqa, k, v, lengths, block_size)\n\n    # Reshape local_out, local_max and local_sum to match Maxtext requirements\n    local_out = local_out.reshape(batch_size, q_length, q_heads, head_dim)\n    local_max = local_max.reshape(batch_size, q_length, q_heads, 1)\n    local_sum = local_sum.reshape(batch_size, q_length, q_heads, 1)\n    return local_out, local_max, local_sum\n\n  def tpu_ragged_attention(\n      self, query: Array, key: Array | KVTensor, value: Array | KVTensor, lengths: Array, block_size: int\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Ragged Attention.\"\"\"\n    if isinstance(query, KVTensor):\n      raise TypeError(\"Ragged attention does not currently support quantized tensors.\")\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            bsnd,\n            bsnd,\n            bsnd,\n            b,\n            None,\n        ),\n        out_specs=bsnd,\n        check_vma=False,\n    )\n    def wrap_ragged_attention(query, key, value, lengths, block_size):\n      if query.shape[-2] == key.shape[-2]:\n        return ragged_mha(query, key, value, lengths, block_size=block_size)\n      else:\n        return ragged_gqa(query, key, value, lengths, block_size=block_size)\n\n    return wrap_ragged_attention(query, key, value, lengths, block_size)\n\n  def tpu_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      attn_logits_soft_cap: float | None = None,\n      sinks: Array | None = None,\n  ) -> Array:\n    \"\"\"TPU Flash Attention.\"\"\"\n\n    cp_size = self.config.context_parallel_size\n    load_balanced_context_parallel = self.config.context_parallel_load_balance\n\n    # Transpose to ('batch', 'heads', 'length', 'kv')\n    query = jnp.transpose(query, axes=(0, 2, 1, 3))\n    key = jnp.transpose(key, axes=(0, 2, 1, 3))\n    value = jnp.transpose(value, axes=(0, 2, 1, 3))\n    segment_axis_names_q = None\n    segment_axis_names_kv = None\n    sink_axis_names = nn.logical_to_mesh_axes((HEAD,))\n    if decoder_segment_ids is not None:\n      if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH_NO_EXP, Q_LENGTH))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH_NO_EXP, KV_LENGTH))\n      else:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH, Q_LENGTH_NO_EXP))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH, KV_LENGTH))\n\n    if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel_ep)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q_ep)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv_ep)\n    else:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv)\n\n    global global_block_q, global_block_kv, global_block_kv_compute, global_block_q_dkv, global_block_kv_dkv\n    global global_block_kv_dkv_compute, global_block_q_dq, global_block_kv_dq, global_use_fused_bwd_kernel\n    global global_q_layout, global_k_layout, global_v_layout\n    global_block_q = self.config.sa_block_q\n    global_block_kv = self.config.sa_block_kv\n    global_block_kv_compute = self.config.sa_block_kv_compute\n    global_block_q_dkv = self.config.sa_block_q_dkv\n    global_block_kv_dkv = self.config.sa_block_kv_dkv\n    global_block_kv_dkv_compute = self.config.sa_block_kv_dkv_compute\n    global_block_q_dq = self.config.sa_block_q_dq\n    global_block_kv_dq = self.config.sa_block_kv_dq\n    global_use_fused_bwd_kernel = self.config.sa_use_fused_bwd_kernel\n    global_q_layout = self.config.sa_q_layout\n    global_k_layout = self.config.sa_k_layout\n    global_v_layout = self.config.sa_v_layout\n\n    devices_in_data_fsdp = self.mesh.shape[\"data\"] * self.mesh.shape[\"fsdp\"]\n    assert (query.shape[0] / devices_in_data_fsdp).is_integer(), (\n        \"Batch dimension should be shardable among the devices in data and fsdp\"\n        \" axis\"\n        f\" got {query.shape[0]=}/{devices_in_data_fsdp=}\"\n    )\n\n    # create_splash_attention config\n    def create_sa_config(config, query, key, attn_logits_soft_cap):\n      if config.use_tokamax_splash:\n        sa_config = tokamax_splash_kernel.SplashConfig(\n            block_q=min(global_block_q, query.shape[2]),\n            block_kv=min(global_block_kv, key.shape[2]),\n            block_kv_compute=min(global_block_kv_compute, key.shape[2]),\n            block_q_dkv=min(global_block_q_dkv, query.shape[2]),\n            block_kv_dkv=min(global_block_kv_dkv, key.shape[2]),\n            block_kv_dkv_compute=min(global_block_kv_dkv_compute, query.shape[2]),\n            use_fused_bwd_kernel=True,  # tokamax only supports fused bwd kernel\n            q_layout=tokamax_splash_kernel.QKVLayout[global_q_layout],\n            k_layout=tokamax_splash_kernel.QKVLayout[global_k_layout],\n            v_layout=tokamax_splash_kernel.QKVLayout[global_v_layout],\n            attn_logits_soft_cap=attn_logits_soft_cap,\n            residual_checkpoint_name=\"context\",\n            fwd_cost_estimate=pl.CostEstimate(\n                flops=config.cost_estimate_flops_fwd,\n                transcendentals=0,\n                bytes_accessed=0,\n            )\n            if config.cost_estimate_flops_fwd >= 0\n            else None,\n            bwd_cost_estimate=pl.CostEstimate(\n                flops=config.cost_estimate_flops_bwd,\n                transcendentals=0,\n                bytes_accessed=0,\n            )\n            if config.cost_estimate_flops_bwd >= 0\n            else None,\n            dq_reduction_steps=config.dq_reduction_steps if config.dq_reduction_steps > 0 else None,\n        )\n      else:\n        sa_config = splash_attention_kernel.BlockSizes(\n            block_q=min(global_block_q, query.shape[2]),\n            block_kv=min(global_block_kv, key.shape[2]),\n            block_kv_compute=min(global_block_kv_compute, key.shape[2]),\n            block_q_dkv=min(global_block_q_dkv, query.shape[2]),\n            block_kv_dkv=min(global_block_kv_dkv, key.shape[2]),\n            block_kv_dkv_compute=min(global_block_kv_dkv_compute, query.shape[2]),\n            block_q_dq=None if global_use_fused_bwd_kernel else min(global_block_q_dq, query.shape[2]),\n            block_kv_dq=None if global_use_fused_bwd_kernel else min(global_block_kv_dq, query.shape[2]),\n            use_fused_bwd_kernel=global_use_fused_bwd_kernel,\n            q_layout=splash_attention_kernel.QKVLayout[global_q_layout],\n            k_layout=splash_attention_kernel.QKVLayout[global_k_layout],\n            v_layout=splash_attention_kernel.QKVLayout[global_v_layout],\n        )\n      return sa_config\n\n    sa_config = create_sa_config(self.config, query, key, attn_logits_soft_cap)\n    mask_shape = (query.shape[2], key.shape[2])  # (q_seq_len, kv_seq_len)\n    mask_module = tokamax_splash_mask if self.config.use_tokamax_splash else splash_attention_mask\n    if self.attention_type == AttentionType.FULL:\n      mask = mask_module.FullMask(mask_shape)\n    else:\n      mask = mask_module.CausalMask(shape=mask_shape)\n\n    # Create LoadBalancedCausalMask if cp and load_balancing\n    if cp_size > 1 and load_balanced_context_parallel:\n      mask = LoadBalancedCausalMask(shape=mask_shape, cp_size=cp_size)\n\n    # TODO: figure out local_sliding attention + load_balancing, default is global\n    # Apply local masking if local sliding attention is enabled.\n    if self.attention_type == AttentionType.LOCAL_SLIDING:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n      mask &= mask_module.LocalMask(\n          shape=(query.shape[2], key.shape[2]),\n          window_size=(self.sliding_window_size, self.sliding_window_size),\n          offset=0,\n      )\n    elif self.attention_type == AttentionType.CHUNK:\n      if self.chunk_attn_window_size is None:\n        raise ValueError(\"chunk_attn_window_size must be set for chunk attention type\")\n\n      mask &= ChunkedCausalMask(shape=(query.shape[2], key.shape[2]), chunk_size=self.chunk_attn_window_size)\n\n    max_logit_value = None\n    if self.config.use_tokamax_splash:\n      # Create mask\n      single_head_mask = mask  # tokamax now just uses a single mask and assumes broadcast to all heads\n      if self.config.use_max_logit_estimate > 0:\n        sa_config = dataclasses.replace(sa_config, max_logit_const=self.config.use_max_logit_estimate)\n\n      # Create the splash attention kernel object separately, jit it for performance\n      @partial(\n          jax.jit,\n          static_argnames=[\n              \"single_head_mask\",\n              \"shard_head_size\",\n          ],\n      )\n      def wrap_splash_kernel(single_head_mask, shard_head_size=1):\n        splash_kernel = tokamax_splash_kernel.make_splash_mha(\n            mask=single_head_mask,\n            config=sa_config,\n            q_seq_shards=cp_size,  # axis for sequence sharding,\n        )\n        return splash_kernel\n\n      logical_axis_rules_head = np.array(\n          [self.mesh.shape[physical_axes] for physical_axes in dict(self.config.logical_axis_rules)[HEAD]]\n      )\n      shard_head_size = np.prod(logical_axis_rules_head)\n      splash_kernel = wrap_splash_kernel(single_head_mask, int(shard_head_size))\n      if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n        segment_axis_names_splash_kernel = nn.logical_to_mesh_axes((Q_LENGTH,))\n      else:\n        segment_axis_names_splash_kernel = nn.logical_to_mesh_axes((Q_LENGTH_NO_EXP,))\n    else:\n      # Create multi-head mask\n      multi_head_mask = splash_attention_mask.MultiHeadMask(masks=(mask,) * query.shape[1])\n\n      # Create the splash attention kernel object separately, jit it for performance\n      @partial(\n          jax.jit,\n          static_argnames=[\n              \"multi_head_mask\",\n              \"shard_head_size\",\n          ],\n      )\n      def wrap_splash_kernel(multi_head_mask, shard_head_size=1):\n        splash_kernel = splash_attention_kernel.make_splash_mha(\n            mask=multi_head_mask,\n            head_shards=shard_head_size,  # the size of the axis if sharding over heads\n            q_seq_shards=cp_size,  # axis for sequence sharding\n            block_sizes=sa_config,\n            attn_logits_soft_cap=attn_logits_soft_cap,\n            residual_checkpoint_name=\"context\",\n        )\n        return splash_kernel\n\n      logical_axis_rules_head = np.array(\n          [self.mesh.shape[physical_axes] for physical_axes in dict(self.config.logical_axis_rules)[HEAD]]\n      )\n      shard_head_size = np.prod(logical_axis_rules_head)\n      splash_kernel = wrap_splash_kernel(multi_head_mask, int(shard_head_size))\n      named_sharding = jax.sharding.NamedSharding(self.mesh, axis_names_splash_kernel)\n      segment_axis_names_splash_kernel = splash_kernel.manual_sharding_spec(named_sharding)\n\n    # Now call the function wrap_flash_attention which does the actual computation.\n    # The splash kernel is passed as a parameter to the function. Since we have the shard map\n    # decorating the wrap_flash_attention function, the data will be correctly sharded\n    # meaning q will be sharded over sequence aka context length but K and V will be duplicated\n    # The shardings are specified in the in_specs and out_specs of the shard_map decorator:\n    # 'segment_axis_names_q' maps to ['activation_q_length', ['context']] meaning that q is sharded over the context axis\n    #  'segment_axis_names_kv' maps to ['activation_kv_length', []] meaning that K and V are not sharded\n    # splash_kernel is sharded over (HEAD, LENGTH)\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            axis_names_q,\n            axis_names_kv,\n            axis_names_kv,\n            segment_axis_names_q,\n            segment_axis_names_kv,\n            segment_axis_names_splash_kernel,\n            None,  # no sharding for cp_size\n            None,  # no sharding for load_balanced_context_parallel\n            sink_axis_names,  # sharding align with query heads\n        ),\n        out_specs=axis_names_q,\n        check_vma=False,\n    )\n    def wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids_q,\n        decoder_segment_ids_kv,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    ):\n      # If load_balanced_context_parallel is enabled, reorder the key and value tensors\n      # to ensure that they are contiguous in memory.\n      # This is necessary for the splash attention kernel to work correctly because it expects\n      # the K and V to be contiguous. Note that K and V are not sharded over the sequence aka context axis\n      # This was we get the unsharded unpermuted key and value tensors\n      if cp_size > 1 and load_balanced_context_parallel:\n        key = max_utils.reorder_sequence(tensor=key, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        value = max_utils.reorder_sequence(tensor=value, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        decoder_segment_ids_unpermuted = max_utils.reorder_sequence(\n            tensor=decoder_segment_ids_kv, cp_size=cp_size, seq_dim=1, to_contiguous=True\n        )\n\n      if decoder_segment_ids_q is not None:\n        if cp_size > 1 and load_balanced_context_parallel:\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(\n              decoder_segment_ids_q, decoder_segment_ids_unpermuted\n          )\n        else:\n          # if cp=1, decoder_segment_ids_q is the same as decoder_segment_ids_kv\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(decoder_segment_ids_q, decoder_segment_ids_kv)\n      else:\n        decoder_segment_ids_tuple = None\n\n      if self.config.use_tokamax_splash:\n        if max_logit_value is not None:\n          attention_output = jax.vmap(partial(splash_kernel, max_logit_value=max_logit_value))(\n              query, key, value, decoder_segment_ids_tuple\n          )\n        else:\n          attention_output = jax.vmap(splash_kernel)(query, key, value, decoder_segment_ids_tuple)\n      else:\n        attention_output = jax.vmap(splash_kernel, in_axes=(0, 0, 0, 0, None))(\n            query, key, value, decoder_segment_ids_tuple, sinks\n        )\n      return attention_output\n\n    def _maybe_shard_with_pspec(inputs, pspec: jax.sharding.PartitionSpec | None):\n      # decoder_segment_ids can be None\n      if pspec is None:\n        return None\n      sharding = NamedSharding(self.mesh, pspec)\n      return maybe_shard_with_name(inputs, sharding, shard_mode=self.config.shard_mode)\n\n    query = _maybe_shard_with_pspec(query, axis_names_q)\n    key = _maybe_shard_with_pspec(key, axis_names_kv)\n    value = _maybe_shard_with_pspec(value, axis_names_kv)\n    decoder_segment_ids_q = _maybe_shard_with_pspec(decoder_segment_ids, segment_axis_names_q)\n    decoder_segment_ids_kv = _maybe_shard_with_pspec(decoder_segment_ids, segment_axis_names_kv)\n    sinks = _maybe_shard_with_pspec(sinks, sink_axis_names)\n\n    x = wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids_q,\n        decoder_segment_ids_kv,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    )\n\n    x = jnp.transpose(x, axes=(0, 2, 1, 3))\n\n    return x\n\n  def cudnn_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> Array:\n    \"\"\"CUDNN Flash Attention with Transformer Engine.\n    1. Stable API, supports GQA, SWA (only with causal masking)\n    2. Head_dim = 256 is also supported from TE-1.12 stable release with CUDNN 12.6\n    \"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from transformer_engine.jax.flax.transformer import DotProductAttention  # pytype: disable=import-error\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    using_context_parallelism = self.mesh.shape[\"context\"] > 1\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and using_context_parallelism:\n      raise AssertionError(\"Sliding window attention is not supported when context parallelism is enabled\")\n\n    sliding_window_size = None\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or not self.config.enable_padding_causal_mask:\n      sliding_window_size = [self.sliding_window_size, 0]\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or using_context_parallelism:\n      mask_type = \"causal\"  # SWA and Context Parallelism only work with causal masking\n      attn_mask = None\n      dummy_attn_mask = None\n    else:\n      # generate attn_mask\n      mask_type = \"padding_causal\"  # only padding_causal mask type can take a created mask\n      dummy_attn_mask = jnp.zeros((1, 1, 1, self.max_target_length, self.max_target_length), dtype=jnp.uint8)\n      attn_mask = self.generate_attention_mask(query, key, decoder_segment_ids, model_mode)\n\n    if attn_mask is not None:\n      attn_mask = jnp.where((attn_mask >= DEFAULT_MASK_VALUE * 0.5), 0, 1).astype(jnp.uint8)\n\n    dpa_layer = DotProductAttention(\n        head_dim=head_dim,\n        num_attention_heads=self.num_query_heads,\n        num_gqa_groups=self.num_kv_heads,\n        attn_mask_type=mask_type,  # 'no_mask', 'padding', 'causal', or 'padding_causal'\n        attn_bias_type=\"no_bias\",  # 'no_bias', 'pre_scale_bias' or 'post_scale_bias'\n        attention_dropout=self.dropout_rate,\n        dropout_rng_name=\"aqt\",\n        dtype=self.dtype,\n        float32_logits=self.float32_logits,\n        qkv_layout=\"BSHD_BSHD_BSHD\",  # 'BS3HD', 'BSHD_BS2HD' or 'BSHD_BSHD_BSHD'\n        scale_factor=1.0,\n        transpose_batch_sequence=False,\n        window_size=sliding_window_size,\n        context_parallel_causal_load_balanced=self.config.context_parallel_load_balance,\n        context_parallel_axis=\"context\",\n    )\n\n    dpa_layer = nnx_wrappers.ToNNX(dpa_layer, rngs=self.rngs)\n    dummy_query_prefill = jnp.zeros(\n        (1, self.max_target_length, self.num_query_heads, self.config.head_dim), dtype=self.dtype\n    )\n    dummy_key_prefill = jnp.zeros((1, self.max_target_length, self.num_kv_heads, self.config.head_dim), dtype=self.dtype)\n    dummy_value_prefill = jnp.zeros(\n        (1, self.max_target_length, self.num_kv_heads, self.config.head_dim), dtype=self.dtype\n    )\n\n    dpa_layer.lazy_init(dummy_query_prefill, dummy_key_prefill, dummy_value_prefill, mask=dummy_attn_mask)\n    return dpa_layer(query, key, value, mask=attn_mask)\n\n  def cudnn_jax_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> tuple[Array, Array]:\n    \"\"\"CUDNN Flash Attention with JAX SDPA API.\"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from jax._src.cudnn.fused_attention_stablehlo import (\n        dot_product_attention,\n        MaskType,\n    )\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          q_seqlen=lengths,\n          kv_seqlen=lengths,\n          mask_type=MaskType.PADDING,\n          scale=1.0,\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    else:\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          mask_type=MaskType.CAUSAL,\n          scale=1.0 / math.sqrt(head_dim),\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    output = checkpoint_name(output, \"context\")\n    lse = checkpoint_name(lse, \"context\")\n    return output, lse\n\n  def compute_local_attention(\n      self,\n      attn_weights: Array,\n      value: Array | KVTensor,\n      q_seq_len: int,\n      model_mode: str,\n      wv_product_einsum: Callable[..., Array],\n      sinks: Array | None = None,\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Computes the attention of a local subset of the kv cache.\n    Local attention results will need to be combined with any other local attentions and normalized\n    Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n\n    Args:\n        attn_weights (Array): Product of query and key\n        value (Array): Current value\n        aqt_rng (PRNGKey | None): Optional rng\n\n    Returns:\n        (local_out, local_max,): where\n          local_out is local unnormalized output\n          local_max is the local max of exponentials\n          local_sum is the sum of exponentials for this chunk, divided by exp(local_max).\n    \"\"\"\n    b, n_kv, g, t, s = attn_weights.shape\n    n_q = n_kv * g\n    logits = jnp.reshape(attn_weights, (b, n_q, t, s))\n    if sinks is not None:\n      # broadcast sinks to match the attn weights dimension and combine\n      sinks_param = sinks.astype(attn_weights.dtype)  # (n_q,)\n      sinks_logits = sinks_param[jnp.newaxis, :, jnp.newaxis, jnp.newaxis]  # (1, n_q, 1, 1)\n      sinks_logits = jnp.broadcast_to(sinks_logits, (b, n_q, t, 1))\n      logits = jnp.concatenate([logits, sinks_logits], axis=-1)\n\n    # softmax\n    local_max = jnp.max(logits, axis=-1, keepdims=True)\n    local_exps_combined = jnp.exp(logits - local_max)\n    local_sum = jnp.sum(local_exps_combined, axis=-1, keepdims=True)\n\n    # reshape and transpose\n    local_exps = local_exps_combined[..., :s]\n    local_exps = jnp.reshape(local_exps, (b, n_kv, g, t, s))\n    local_max = jnp.transpose(local_max, (0, 2, 1, 3))  # (b, t, n_q, 1)\n    local_sum = jnp.transpose(local_sum, (0, 2, 1, 3))  # (b, t, n_q, 1)\n\n    local_out = self.wv_product(local_exps, value, model_mode, wv_product_einsum)\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n    elif model_mode == MODEL_MODE_PREFILL:\n      local_out = partitioning.with_sharding_constraint(local_out, (BATCH, KV_LENGTH, HEAD, D_KV))\n\n    if self.reshape_q and q_seq_len == 1:\n      local_max = local_max[:, 0:1, :, :]\n      local_sum = local_sum[:, 0:1, :, :]\n      local_out = local_out[:, 0:1, :, :]\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_max = partitioning.with_sharding_constraint(local_max, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_sum = partitioning.with_sharding_constraint(local_sum, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n\n    return local_out, local_max, local_sum\n\n  def is_partition_in_decode(self, seq_len):\n    return self.config.ici_context_autoregressive_parallelism > 0 and seq_len == 1\n\n  def apply_attention_dot(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply Attention.\"\"\"\n    validate_compute_axis_order(self.compute_axis_order)\n    # Casting qk_product and softmaxt computation for float32 for model stability.\n    if self.float32_qk_product:\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      query = query.astype(jnp.float32)\n      key = key.astype(jnp.float32)\n\n    # special sharding for decode\n    q_seq_len = query.shape[1]\n    prefill_qkv_sharding = (BATCH, PREFILL_LENGTH, HEAD, D_KV)\n    decode_qkv_sharding = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV)\n    if self.is_partition_in_decode(q_seq_len):\n      query = partitioning.with_sharding_constraint(query, decode_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, decode_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, decode_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, decode_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, decode_qkv_sharding)\n    elif model_mode == MODEL_MODE_PREFILL:\n      query = partitioning.with_sharding_constraint(query, prefill_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, prefill_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, prefill_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, prefill_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, prefill_qkv_sharding)\n\n    attn_weights = self.qk_product(query, key, q_seq_len, model_mode, qk_product_einsum)\n    if self.is_partition_in_decode(q_seq_len):\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n\n    if self.attn_logits_soft_cap:\n      attn_weights = jnp.tanh(attn_weights / self.attn_logits_soft_cap)\n      attn_weights = attn_weights * self.attn_logits_soft_cap\n\n    # Casting softmaxt computation for float32 for model stability.\n    if self.float32_logits:\n      attn_weights = attn_weights.astype(jnp.float32)\n\n    attn_mask = self.generate_attention_mask(\n        query, key, decoder_segment_ids, model_mode, previous_chunk, bidirectional_mask\n    )\n    if self.config.moba:\n      kv_seq_len = key.shape[1]\n      # This logic for `next_pos` is duplicated from `generate_attention_mask`.\n      # It determines the starting position of the query sequence.\n      next_pos = 0\n      if previous_chunk is not None:\n        next_pos = previous_chunk.shape[1]\n      elif model_mode == MODEL_MODE_AUTOREGRESSIVE and q_seq_len == 1:\n        next_pos = kv_seq_len - 1\n      q_positions = jnp.arange(next_pos, next_pos + q_seq_len)\n\n      # The gate calculation in MoBA uses the unscaled query.\n      # With scaled query, the gate values are scaled, but since the top-k selection\n      # is scale-invariant, we can use the scaled query directly.\n      moba_mask = self._generate_moba_mask(query, key, q_positions)\n      attn_weights += moba_mask\n\n    if self.is_partition_in_decode(q_seq_len):\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n    if attn_mask is not None:\n      attn_weights = apply_mask_to_logits(attn_weights, attn_mask)\n    return self.compute_local_attention(attn_weights, value, q_seq_len, model_mode, wv_product_einsum, sinks)\n\n  def qk_product(\n      self, query: Array, key: Array | KVTensor, q_seq_len: int, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"Query-Key product.\n\n    Args:\n      query: Query projection, in shape of [b, t, n, d]\n      key: Key projection in shape of [b, s, n_kv, d]\n\n    Returns:\n      results in shape [b, n_kv, n // n_kv, t, s].\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n    b, t, n, d = query.shape\n    n_kv = key.shape[-2]\n    assert n_kv == self.num_kv_heads\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, 2, n_kv, n // n_kv, d))\n      result = einsum(\"btkgd,bskd->bkgts\", query, key, **precision_kwargs)\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      query = jnp.transpose(query, axes=self.compute_axis_order)\n      key = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), key)\n      query = jnp.reshape(query, (b, n_kv, n // n_kv, t, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, n_kv, n // n_kv, 2, d))\n      result = einsum(\"bkgtd,bksd->bkgts\", query, key, **precision_kwargs)\n    else:\n      raise NotImplementedError(self.compute_axis_order)\n    return result\n\n  def wv_product(\n      self, attn_weights: Array, value: Array | KVTensor, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"weighted value product.\n\n    Args:\n      attn_weights: Computed results of qk_einsum, in shape [b, n_kv, n // n_kv, t, s]\n      value: Value projection, in shape of [b, s, n_kv, d]\n\n    Returns:\n      result in shape [b, t, n, d]\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if self.kv_quant:\n      # manually cast to bf16 to avoid the fp32 XLA ops for speedup\n      if isinstance(value, KVTensor) and self.kv_quant.dtype == jnp.float8_e4m3fn:\n        value.qvalue = value.qvalue.astype(jnp.bfloat16)\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      out = einsum(\"bkgts,bskd->btkgd\", attn_weights, value, **precision_kwargs)\n      b, t, n_kv, g, d = out.shape\n      result = jnp.reshape(out, (b, t, n_kv * g, d))\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      value = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), value)\n      out = einsum(\"bkgts,bksd->bkgtd\", attn_weights, value, **precision_kwargs)\n      b, n_kv, g, t, d = out.shape\n      result = jnp.reshape(out, (b, n_kv * g, t, d))\n      result = self.reverse_transepose(result, self.compute_axis_order)\n    return result\n\n  def reverse_transepose(self, transposed_array, transpose_axis_order):\n    return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)\n\n  def normalize_cudnn_attention(self, local_outs, local_stats):\n    \"\"\"Normalize across two cuDNN attentions\n\n    Args:\n        local_outs (list): List of outputs entries for each cudnn attention\n          in shape [b, t, n, d].\n        local_stats (list): List of logsumexp entries for each cudnn attention\n          in shape [b, n, t].\n\n    Returns:\n        Array: Combined attention that has been normalized in shape [b, t, n, d].\n    \"\"\"\n    # reshape stat to have shape [b, n, t, 1]\n    stat0 = local_stats[0].reshape((*local_stats[0].shape, 1))\n    stat1 = local_stats[1].reshape((*local_stats[1].shape, 1))\n    global_stat = jnp.log(jnp.exp(stat0) + jnp.exp(stat1))\n    # # transpose stat to have shape [b, t, n, 1] for elemenwise multiplication\n    attn_out = local_outs[0].astype(jnp.float32) * jnp.exp(stat0 - global_stat).transpose((0, 2, 1, 3)) + local_outs[\n        1\n    ].astype(jnp.float32) * jnp.exp(stat1 - global_stat).transpose((0, 2, 1, 3))\n    return attn_out.astype(local_stats[0].dtype)\n\n  def normalize_attention(self, local_outs, local_maxes, local_sums):\n    \"\"\"Normalize across multiple localized attentions\n\n    Args:\n        local_outs (list): List of unnormalized outputs entries for each local attention\n        local_maxes (list): List of max exponentials entries for each local attention\n        local_sums (list): List of exponential sum entries for each local attention\n\n    Returns:\n        Array: Combined attention that has been normalized\n    \"\"\"\n    # Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n    global_max = functools.reduce(jnp.maximum, local_maxes)\n    global_sum = sum(\n        (jnp.exp(local_max - global_max) * local_sum for (local_sum, local_max) in zip(local_sums, local_maxes))\n    )\n\n    attn_out = 0\n    for local_max, local_out in zip(local_maxes, local_outs):\n      local_normalizer = jnp.exp(local_max - global_max) / global_sum\n      attn_out += local_normalizer * local_out\n    return attn_out\n\n  def __call__(\n      self,\n      query,\n      key,\n      value,\n      decoder_segment_ids,\n      model_mode,\n      cached_values=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n      sinks=None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n  ):\n    if cached_values is None:\n      prefill_kv_cache, ar_kv_cache = None, None\n    else:\n      prefill_kv_cache, ar_kv_cache = cached_values[0], cached_values[1]\n    if model_mode != MODEL_MODE_TRAIN:\n      assert prefill_kv_cache\n      key, value, decoder_segment_ids = prefill_kv_cache\n\n    prefill_unnormalized_output, prefill_exponentials_max, prefill_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=None,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n        bidirectional_mask=bidirectional_mask,\n        sinks=sinks,\n        qk_product_einsum=self.AqtEinsum_0,\n        wv_product_einsum=self.AqtEinsum_1,\n    )\n\n    # Return the \"prefill\" cache if it actually the combined prefill+ar kv cache\n    if ar_kv_cache is None:\n      if prefill_exponentials_sum is not None:\n        return prefill_unnormalized_output / prefill_exponentials_sum\n      return prefill_unnormalized_output\n\n    key, value, decoder_segment_ids, lengths = ar_kv_cache\n    ar_unnormalized_output, ar_exponentials_max, ar_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=lengths,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        bidirectional_mask=bidirectional_mask,\n        qk_product_einsum=self.AqtEinsum_2,\n        wv_product_einsum=self.AqtEinsum_3,\n    )\n\n    if ar_unnormalized_output is not None:\n      unnormalized_outputs = [prefill_unnormalized_output, ar_unnormalized_output]\n      exponentials_maxes = [prefill_exponentials_max, ar_exponentials_max]\n      exponentials_sums = [prefill_exponentials_sum, ar_exponentials_sum]\n      if prefill_exponentials_max is not None and prefill_exponentials_sum is None:\n        prefill_stat = prefill_exponentials_max\n        ar_stat = ar_exponentials_max\n        stats = [prefill_stat, ar_stat]\n        return self.normalize_cudnn_attention(unnormalized_outputs, stats)\n      else:\n        return self.normalize_attention(unnormalized_outputs, exponentials_maxes, exponentials_sums)\n    else:\n      return prefill_unnormalized_output / prefill_exponentials_sum",
        "analysis": {
            "module_type": "attention_op",
            "purpose": "Implements the attention operation, a core component of Transformer models, supporting various attention mechanisms and optimizations.",
            "input": {
                "shape": "query: [batch_size, sequence_length, num_heads, head_dim], key: [batch_size, kv_sequence_length, num_kv_heads, head_dim], value: [batch_size, kv_sequence_length, num_kv_heads, head_dim]. Other inputs like decoder_segment_ids, lengths, etc., are optional and depend on the specific attention mechanism and model mode.",
                "dtype": "float32 or bfloat16 (depending on configuration)"
            },
            "processing_steps": [
                "Input validation (check_attention_inputs)",
                "Determine attention kernel and hardware implementation",
                "Apply specific attention logic (e.g., ragged, dot product, flash, CUDNN)",
                "Generate attention mask (generate_attention_mask)",
                "Compute query-key product (qk_product)",
                "Apply attention mask to logits",
                "Compute weighted value product (wv_product)",
                "Normalize attention outputs (normalize_attention or normalize_cudnn_attention)"
            ],
            "output": {
                "shape": "Typically [batch_size, sequence_length, num_heads, head_dim]. Specific return values can include attention outputs, and intermediate statistics like max exponentials and sums for normalization."
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.linen",
                "flax.nnx",
                "transformer_engine.jax.flax.transformer",
                "jax.experimental.pallas",
                "MaxText.common_types",
                "MaxText.kernels.ragged_attention",
                "MaxText.layers.nnx_wrappers",
                "MaxText.inference.kvcache"
            ],
            "parameters": {
                "config": "Configuration object containing various model and attention-specific settings.",
                "mesh": "Device mesh for distributed computation.",
                "attention_kernel": "Specifies the attention implementation to use (e.g., 'dot_product', 'flash', 'autoselected').",
                "num_query_heads": "Number of heads for the query.",
                "num_kv_heads": "Number of heads for key/value.",
                "dtype": "Data type for computations.",
                "attention_type": "Type of attention (e.g., GLOBAL, LOCAL_SLIDING, CHUNK).",
                "kv_quant": "Optional KV quantization configuration.",
                "float32_qk_product": "If True, computes the query-key product in float32 for stability.",
                "float32_logits": "If True, computes logits in float32 for stability."
            },
            "notes": [
                "The class supports multiple attention implementations including dot product, flash attention (TPU and GPU), CUDNN flash attention, and ragged attention.",
                "It handles different model modes like training, prefill, and autoregressive decoding.",
                "Supports various masking strategies: causal, padding, local sliding window, chunked, and bidirectional.",
                "Includes optimizations for specific hardware (TPU, GPU) and advanced features like MoBA (Multi-Object-Based Attention) and quantization.",
                "The `__call__` method orchestrates the attention computation, handling prefill and autoregressive KV cache logic."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the AttentionOp module with configuration, mesh, and attention-specific parameters.",
                    "input": {
                        "shape": "N/A (constructor)",
                        "dtype": "N/A (constructor)"
                    },
                    "processing_steps": [
                        "Store configuration and parameters.",
                        "Initialize einsum functions, potentially wrapping them with NNX for quantization."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "check_attention_inputs": {
                    "purpose": "Validates the shapes and dimensions of query, key, and value tensors.",
                    "input": {
                        "shape": "query: [..., num_heads, head_dim], key: [..., num_kv_heads, head_dim], value: [..., num_kv_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert that key and value have the same number of dimensions.",
                        "Assert that batch dimensions of query, key, and value match.",
                        "Assert that num_kv_heads match for key and value.",
                        "Assert that sequence lengths match for key and value.",
                        "Assert that the head dimensions of query and key match."
                    ],
                    "output": {
                        "shape": "None (raises AssertionError on failure)"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "generate_attention_mask": {
                    "purpose": "Generates a combined attention mask based on model mode, segment IDs, and attention type.",
                    "input": {
                        "shape": "query: [batch_size, q_sequence_length, num_heads, head_dim], key: [batch_size, kv_sequence_length, num_heads, head_dim], decoder_segment_ids: [batch_size, q_sequence_length] (optional), model_mode: str, previous_chunk: Any (optional), bidirectional_mask: Any (optional)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize mask to None.",
                        "Apply segment ID masking if decoder_segment_ids are provided.",
                        "Determine causal mask based on model_mode and attention_type.",
                        "Combine segment and causal masks.",
                        "Apply local sliding window or chunk attention masking if specified.",
                        "Incorporate bidirectional mask if provided.",
                        "Convert boolean mask to float mask values (0.0 for allowed, DEFAULT_MASK_VALUE for disallowed)."
                    ],
                    "output": {
                        "shape": "[batch_size, num_heads, q_sequence_length, kv_sequence_length] or None",
                        "dtype": "float32"
                    },
                    "dependencies": [
                        "_generate_chunk_attention_mask",
                        "_make_bidirectional_block_mask"
                    ],
                    "notes": [
                        "Handles causality, sequence separation, local attention, and bidirectional attention.",
                        "References Pallas MHA Flash Attention and SARATHI paper."
                    ]
                },
                "calculate_moba_gate_logic": {
                    "purpose": "Computes block-level gating intermediates for Multi-Object-Based Attention (MoBA).",
                    "input": {
                        "shape": "q_item: [q_len, n_q_heads, head_dim], k_item: [kv_len, n_kv_heads, head_dim], q_pos_item: [q_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape query for GQA.",
                        "Calculate chunk IDs for keys.",
                        "Sum key vectors per chunk.",
                        "Calculate mean key vectors per chunk.",
                        "Compute gate scores by dot product of queries and key chunk means.",
                        "Apply block-causal masking.",
                        "Apply top-k selection to identify relevant key blocks.",
                        "Generate a boolean mask indicating which key blocks queries should attend to."
                    ],
                    "output": {
                        "shape": "Returns multiple intermediate arrays, including a boolean mask 'need_attend' of shape [n_kv_heads, g, q_len, num_block].",
                        "dtype": "float32"
                    },
                    "dependencies": [
                        "jax.ops.segment_sum",
                        "jax.lax.top_k",
                        "jax.nn.one_hot"
                    ],
                    "notes": [
                        "Used for MoBA attention mechanism."
                    ]
                },
                "generate_moba_mask_single_item": {
                    "purpose": "Generates the token-level MoBA additive mask for a single batch item.",
                    "input": {
                        "shape": "q_item: [q_len, n_q_heads, head_dim], k_item: [kv_len, n_kv_heads, head_dim], q_positions: [q_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call calculate_moba_gate_logic to get block-level attention decisions.",
                        "Expand block-level decisions to token-level.",
                        "Convert boolean mask to additive float mask (-inf for masked positions).",
                        "Apply a final per-token causal mask.",
                        "Return the additive mask."
                    ],
                    "output": {
                        "shape": "[n_kv_heads, g, q_len, kv_len]",
                        "dtype": "float32"
                    },
                    "dependencies": [
                        "calculate_moba_gate_logic"
                    ],
                    "notes": []
                },
                "_generate_moba_mask": {
                    "purpose": "Builds the token-level MoBA additive mask for the entire batch.",
                    "input": {
                        "shape": "query: [batch, q_len, n_q_heads, head_dim], key: [batch, kv_len, n_kv_heads, head_dim], q_positions: [q_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Use jax.vmap to apply generate_moba_mask_single_item across the batch dimension."
                    ],
                    "output": {
                        "shape": "[batch, n_kv_heads, g, q_len, kv_len]",
                        "dtype": "float32"
                    },
                    "dependencies": [
                        "generate_moba_mask_single_item"
                    ],
                    "notes": []
                },
                "apply_attention": {
                    "purpose": "Applies the attention mechanism based on the selected kernel and model mode.",
                    "input": {
                        "shape": "query: [batch, seq_len, n_heads, head_dim], key: [batch, kv_seq_len, n_kv_heads, head_dim], value: [batch, kv_seq_len, n_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len] (optional), lengths: [batch] (optional), model_mode: str, use_ragged_attention: bool, previous_chunk: Any (optional), bidirectional_mask: Any (optional), sinks: Any (optional), qk_product_einsum: Callable, wv_product_einsum: Callable",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate attention inputs.",
                        "Select implementation based on use_ragged_attention, model_mode, attention_kernel, and hardware.",
                        "Call the appropriate attention implementation (e.g., tpu_ragged_attention, apply_attention_dot, tpu_flash_attention)."
                    ],
                    "output": {
                        "shape": "Tuple containing attention output and potentially intermediate statistics (e.g., max exponentials, sums).",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "check_attention_inputs",
                        "tpu_ragged_attention",
                        "gpu_ragged_attention",
                        "apply_attention_dot",
                        "tpu_flash_attention",
                        "cudnn_flash_attention",
                        "cudnn_jax_flash_attention"
                    ],
                    "notes": [
                        "This is a dispatcher method that routes to specific attention implementations."
                    ]
                },
                "gpu_ragged_attention": {
                    "purpose": "Applies ragged attention on GPU using Pallas GQA kernel.",
                    "input": {
                        "shape": "q: [batch_size, q_length, q_heads, head_dim], k: [batch_size, kv_seq_len, n_kv_heads, head_dim], v: [batch_size, kv_seq_len, n_kv_heads, head_dim], lengths: [batch_size], block_size: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape query for GQA.",
                        "Define logical axis names for sharding.",
                        "Wrap the Pallas GQA kernel with jax.shard_map for distributed execution.",
                        "Call the wrapped kernel.",
                        "Reshape output, max, and sum tensors to match MaxText requirements."
                    ],
                    "output": {
                        "shape": "[batch_size, q_length, q_heads, head_dim], [batch_size, q_length, q_heads, 1], [batch_size, q_length, q_heads, 1]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.shard_map",
                        "gpu_pallas_decode_attention.gqa",
                        "nnx_wrappers.logical_to_mesh_axes"
                    ],
                    "notes": []
                },
                "tpu_ragged_attention": {
                    "purpose": "Applies ragged attention on TPU using custom kernels.",
                    "input": {
                        "shape": "query: [batch, seq_len, n_heads, head_dim], key: [batch, kv_seq_len, n_kv_heads, head_dim], value: [batch, kv_seq_len, n_kv_heads, head_dim], lengths: [batch], block_size: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define logical axis names for sharding.",
                        "Wrap ragged_mha or ragged_gqa kernel with jax.shard_map.",
                        "Call the wrapped kernel."
                    ],
                    "output": {
                        "shape": "[batch, seq_len, n_heads, head_dim], [batch, seq_len, n_heads, 1], [batch, seq_len, n_heads, 1]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.shard_map",
                        "ragged_mha",
                        "ragged_gqa",
                        "nnx_wrappers.logical_to_mesh_axes"
                    ],
                    "notes": [
                        "Does not support quantized tensors."
                    ]
                },
                "tpu_flash_attention": {
                    "purpose": "Implements Flash Attention on TPU using the Splash Attention kernel.",
                    "input": {
                        "shape": "query: [batch, seq_len, num_heads, head_dim], key: [batch, kv_seq_len, num_kv_heads, head_dim], value: [batch, kv_seq_len, num_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len] (optional), attn_logits_soft_cap: float (optional), sinks: Any (optional)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Transpose query, key, and value tensors.",
                        "Determine axis names for sharding based on configuration.",
                        "Create SplashConfig based on model configuration.",
                        "Define and JIT compile the Splash Attention kernel.",
                        "Define a shard_map wrapper for the attention computation.",
                        "Potentially reorder key and value tensors for load balancing.",
                        "Call the wrapped attention kernel.",
                        "Transpose the output tensor back."
                    ],
                    "output": {
                        "shape": "[batch, seq_len, num_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.jit",
                        "jax.shard_map",
                        "tokamax_splash_kernel",
                        "splash_attention_kernel",
                        "splash_attention_mask",
                        "max_utils.reorder_sequence",
                        "nnx_wrappers.logical_to_mesh_axes"
                    ],
                    "notes": [
                        "Supports context parallelism and load balancing.",
                        "Handles different attention types (FULL, CAUSAL, LOCAL_SLIDING, CHUNK)."
                    ]
                },
                "cudnn_flash_attention": {
                    "purpose": "Applies Flash Attention on GPU using Transformer Engine's DotProductAttention.",
                    "input": {
                        "shape": "query: [batch, seq_len, num_heads, head_dim], key: [batch, kv_seq_len, num_kv_heads, head_dim], value: [batch, kv_seq_len, num_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len] (optional), model_mode: str",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine mask type and generate attention mask if needed.",
                        "Instantiate DotProductAttention layer from Transformer Engine.",
                        "Lazy initialize the layer with dummy inputs.",
                        "Call the layer with query, key, value, and mask."
                    ],
                    "output": {
                        "shape": "[batch, seq_len, num_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "transformer_engine.jax.flax.transformer.DotProductAttention",
                        "nnx_wrappers.ToNNX",
                        "generate_attention_mask"
                    ],
                    "notes": [
                        "Requires GPU build and Transformer Engine installation.",
                        "Supports GQA and SWA (with causal masking).",
                        "Does not support sliding window attention with context parallelism."
                    ]
                },
                "cudnn_jax_flash_attention": {
                    "purpose": "Applies Flash Attention on GPU using JAX's SDPA API.",
                    "input": {
                        "shape": "query: [batch, seq_len, num_heads, head_dim], key: [batch, kv_seq_len, num_kv_heads, head_dim], value: [batch, kv_seq_len, num_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len] (optional), model_mode: str",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine mask type based on model_mode.",
                        "Call jax._src.cudnn.fused_attention_stablehlo.dot_product_attention.",
                        "Apply checkpointing to the output and log-sum-exp."
                    ],
                    "output": {
                        "shape": "Tuple: ([batch, seq_len, num_heads, head_dim], [batch, num_heads, seq_len])",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax._src.cudnn.fused_attention_stablehlo.dot_product_attention",
                        "jax.ad_checkpoint.checkpoint_name"
                    ],
                    "notes": [
                        "Requires GPU build."
                    ]
                },
                "compute_local_attention": {
                    "purpose": "Computes the attention for a local subset of the KV cache and normalizes it.",
                    "input": {
                        "shape": "attn_weights: [b, n_kv, g, t, s], value: [b, s, n_kv, d], q_seq_len: int, model_mode: str, wv_product_einsum: Callable, sinks: Any (optional)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape attention weights to include query sequence length.",
                        "Concatenate sink logits if sinks are provided.",
                        "Compute softmax (local_max, local_exps_combined, local_sum).",
                        "Reshape and transpose intermediate results.",
                        "Compute weighted value product using wv_product_einsum.",
                        "Apply sharding constraints based on model_mode.",
                        "Handle reshape_q logic for single query tokens.",
                        "Apply sharding constraints again for local_max, local_sum, and local_out."
                    ],
                    "output": {
                        "shape": "Tuple: (local_out, local_max, local_sum)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "wv_product",
                        "partitioning.with_sharding_constraint"
                    ],
                    "notes": [
                        "Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py"
                    ]
                },
                "is_partition_in_decode": {
                    "purpose": "Checks if the current partition is in the decode stage of autoregressive parallelism.",
                    "input": {
                        "shape": "seq_len: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if context autoregressive parallelism is enabled and sequence length is 1."
                    ],
                    "output": {
                        "shape": "bool",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "apply_attention_dot": {
                    "purpose": "Applies dot-product attention, handling sharding and potential quantization.",
                    "input": {
                        "shape": "query: [batch, seq_len, n_heads, head_dim], key: [batch, kv_seq_len, n_kv_heads, head_dim], value: [batch, kv_seq_len, n_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len] (optional), model_mode: str, previous_chunk: Any (optional), bidirectional_mask: Any (optional), sinks: Any (optional), qk_product_einsum: Callable, wv_product_einsum: Callable",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate compute_axis_order.",
                        "Optionally cast query and key to float32 for qk_product.",
                        "Apply sharding constraints for decode or prefill modes.",
                        "Compute query-key product (qk_product).",
                        "Apply sharding constraints to attention weights.",
                        "Optionally apply soft capping to attention weights.",
                        "Optionally cast attention weights to float32 for logits.",
                        "Generate attention mask.",
                        "If MoBA is enabled, generate MoBA mask and add it to attention weights.",
                        "Apply attention mask to logits.",
                        "Compute local attention outputs."
                    ],
                    "output": {
                        "shape": "Tuple: (local_out, local_max, local_sum)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "validate_compute_axis_order",
                        "partitioning.with_sharding_constraint",
                        "qk_product",
                        "generate_attention_mask",
                        "_generate_moba_mask",
                        "apply_mask_to_logits",
                        "compute_local_attention"
                    ],
                    "notes": [
                        "Handles specific sharding for decode and prefill stages.",
                        "Integrates MoBA attention if configured."
                    ]
                },
                "qk_product": {
                    "purpose": "Computes the dot product between query and key projections.",
                    "input": {
                        "shape": "query: [b, t, n, d], key: [b, s, n_kv, d], q_seq_len: int, model_mode: str, einsum: Callable",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape query tensor based on compute_axis_order.",
                        "Transpose key tensor if compute_axis_order is not (0, 1, 2, 3).",
                        "Perform einsum operation for query-key product.",
                        "Optionally broadcast query for reshape_q logic."
                    ],
                    "output": {
                        "shape": "[b, n_kv, n // n_kv, t, s]",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Supports different compute_axis_order for optimized computation."
                    ]
                },
                "wv_product": {
                    "purpose": "Computes the weighted sum of value vectors using attention weights.",
                    "input": {
                        "shape": "attn_weights: [b, n_kv, g, t, s], value: [b, s, n_kv, d], model_mode: str, einsum: Callable",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Optionally cast value to bfloat16 if kv_quant is used.",
                        "Perform einsum operation for weighted value product.",
                        "Reshape the output tensor.",
                        "Transpose the result if compute_axis_order requires it."
                    ],
                    "output": {
                        "shape": "[b, t, n, d]",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Supports different compute_axis_order for optimized computation."
                    ]
                },
                "reverse_transepose": {
                    "purpose": "Reverses a transpose operation applied to an array.",
                    "input": {
                        "shape": "transposed_array: Array, transpose_axis_order: AxisIdxes",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Use jax.numpy.moveaxis to reverse the transpose operation."
                    ],
                    "output": {
                        "shape": "The original array shape before transpose.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.numpy.moveaxis"
                    ],
                    "notes": []
                },
                "normalize_cudnn_attention": {
                    "purpose": "Normalizes attention outputs from two cuDNN attention computations.",
                    "input": {
                        "shape": "local_outs: list of [b, t, n, d], local_stats: list of [b, n, t]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape stats to [b, n, t, 1].",
                        "Compute global stat using log-sum-exp.",
                        "Combine local outputs using normalized exponential weights.",
                        "Transpose stats for element-wise multiplication.",
                        "Cast to float32 and then back to original dtype."
                    ],
                    "output": {
                        "shape": "[b, t, n, d]",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "normalize_attention": {
                    "purpose": "Normalizes attention outputs from multiple localized attention computations.",
                    "input": {
                        "shape": "local_outs: list of [b, t, n, d], local_maxes: list of [b, t, n, 1], local_sums: list of [b, t, n, 1]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Compute global_max using jnp.maximum.",
                        "Compute global_sum by combining local sums and maxes.",
                        "Iterate through local attentions, compute normalization factor, and accumulate weighted local outputs."
                    ],
                    "output": {
                        "shape": "[b, t, n, d]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "functools.reduce"
                    ],
                    "notes": [
                        "Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py"
                    ]
                },
                "__call__": {
                    "purpose": "Executes the attention operation, handling prefill and autoregressive KV cache logic.",
                    "input": {
                        "shape": "query: [batch, seq_len, n_heads, head_dim], key: [batch, kv_seq_len, n_kv_heads, head_dim], value: [batch, kv_seq_len, n_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len] (optional), model_mode: str, cached_values: tuple (optional), previous_chunk: Any (optional), bidirectional_mask: Any (optional), sinks: Any (optional), slot: Optional[int], page_state: Optional[page_manager.PageState]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract prefill and autoregressive KV caches from cached_values.",
                        "If not in training mode, use prefill cache for key, value, and segment IDs.",
                        "Call apply_attention for prefill computation.",
                        "If autoregressive cache exists, call apply_attention for AR computation.",
                        "Combine and normalize outputs from prefill and AR computations if both exist.",
                        "Return normalized output or prefill output."
                    ],
                    "output": {
                        "shape": "Attention output tensor.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "apply_attention",
                        "normalize_cudnn_attention",
                        "normalize_attention"
                    ],
                    "notes": [
                        "Handles the logic for combining prefill and autoregressive attention results.",
                        "Uses different einsum functions (AqtEinsum_0/1 for prefill, AqtEinsum_2/3 for AR) if KV quantization is enabled."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#LoadBalancedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class LoadBalancedCausalMask(splash_attention_mask._ComputableMask):\n  \"\"\"Lazy causal mask, prevents the model from attending to future tokens.\n  Attributes:\n    offset: Offset of q start wrt kv. A positive offset shifts the bottom\n      triangle upward, a negative one shifts it downward. A negative offset\n      makes the first 'offset' rows of the attention matrix all 0s which leads\n      to undefined softmax.\n  \"\"\"\n\n  offset: int\n  shape: tuple[int, int]\n  cp_size: int\n\n  def __init__(self, shape: tuple[int, int], offset: int = 0, shard_count: int = 1, cp_size: int = 4):\n    self.offset = offset\n\n    def causal_mask_function(q_ids, kv_ids):\n      if self.offset == 0:\n        return q_ids >= kv_ids\n      else:\n        return q_ids + self.offset >= kv_ids\n\n    arr = np.arange(shape[0])\n    # we reorder the mask to be load balanced following the same approach as\n    # used to reorder the input tokens\n    out = max_utils.reorder_mask_load_balancing(arr[None, :, None, None], cp_size, 1)\n    q_sequence = out[0, :, 0, 0]\n\n    mask_function = causal_mask_function\n\n    super().__init__(\n        shape=shape,\n        mask_function=mask_function,\n        shard_count=shard_count,\n    )\n    self.q_sequence = q_sequence\n\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n\n    return self.shape == other.shape and self.offset == other.offset and np.array_equal(self.q_sequence, other.q_sequence)\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.offset,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "load_balanced_causal_mask",
            "purpose": "A lazy causal mask that prevents attention to future tokens, optimized for load balancing in context parallel settings.",
            "input": {
                "shape": "N/A (parameters are passed during initialization)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines a `causal_mask_function` based on the `offset` parameter.",
                "Reorders a sequence of indices (`arr`) for load balancing using `max_utils.reorder_mask_load_balancing`.",
                "Stores the reordered sequence as `q_sequence`.",
                "Initializes the parent `_ComputableMask` with the defined `mask_function` and `shape`."
            ],
            "output": {
                "shape": "N/A (This is a mask class, its output is used internally by attention mechanisms)"
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "numpy",
                "max_utils.reorder_mask_load_balancing"
            ],
            "parameters": {
                "shape": "The desired shape of the mask (e.g., [query_length, kv_length]).",
                "offset": "An integer offset to adjust the causal mask. Defaults to 0.",
                "shard_count": "Number of shards for the mask. Defaults to 1.",
                "cp_size": "Context parallel size, used for load balancing. Defaults to 4."
            },
            "notes": [
                "The `q_sequence` is generated to facilitate load balancing across context parallel replicas.",
                "Implements `__eq__` and `__hash__` for equality comparison and hashing based on shape, offset, and `q_sequence`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#L2Norm",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class L2Norm(nnx.Module):\n  \"\"\"\n  Implementation of L2Norm in JAX.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n\n  eps: float = 1e-6\n  rngs: nnx.Rngs = None  # Not used in L2Norm but passed in by nnx.bridge.to_linen\n\n  def __call__(self, x):\n    return x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)",
        "analysis": {
            "module_type": "l2_norm",
            "purpose": "Applies L2 normalization to an input tensor for numerical stability.",
            "input": {
                "shape": "[batch_size, ..., hidden_dim]",
                "dtype": "float32 or float64"
            },
            "processing_steps": [
                "Calculate the mean of the squared input tensor along the last axis.",
                "Add epsilon for numerical stability.",
                "Compute the inverse square root of the result.",
                "Multiply the input tensor by the computed inverse square root."
            ],
            "output": {
                "shape": "[batch_size, ..., hidden_dim]"
            },
            "dependencies": [
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "eps": "A small float value added to the denominator for numerical stability. Defaults to 1e-6."
            },
            "notes": [
                "The normalization is applied along the last dimension of the input tensor.",
                "The `rngs` parameter is present but not used by this module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#l2_norm_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def l2_norm_as_linen(self, eps: float = 1e-6):\n  \"\"\"\n  Initializes the L2Norm module and returns it as a Linen module.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n  return nnx_wrappers.to_linen(L2Norm, eps=eps, metadata_fn=variable_to_logically_partitioned)",
        "analysis": {
            "functionality": "Converts an L2Norm module (defined in NNX) into a Linen module.",
            "usage": "Call this function with an optional epsilon value to get a Linen-compatible L2Norm module. Example: `linen_l2norm = l2_norm_as_linen(eps=1e-5)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#attention_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def attention_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an Attention as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Attention` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Attention,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "attention_as_linen",
            "purpose": "A factory function to create an Attention module compatible with Flax Linen.",
            "input": {
                "shape": "N/A (This is a factory function, not a module that directly takes input tensors)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert an NNX-based `Attention` module into a Linen-compatible module.",
                "Passes all arguments to the `Attention` constructor."
            ],
            "output": {
                "shape": "N/A (Returns a Flax Linen module)",
                "dtype": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Attention (NNX module)"
            ],
            "parameters": {
                "config": "Configuration object for the attention module.",
                "num_query_heads": "Number of query attention heads.",
                "num_kv_heads": "Number of key-value attention heads.",
                "head_dim": "The dimension of each attention head.",
                "max_target_length": "Maximum sequence length.",
                "mesh": "The device mesh for distributed computation.",
                "attention_kernel": "The attention kernel to use (e.g., 'dot_product', 'flash').",
                "inputs_q_shape": "Query inputs shape for initialization.",
                "inputs_kv_shape": "Key/value inputs shape for initialization.",
                "dtype": "The data type for computation.",
                "weight_dtype": "The data type for weights.",
                "dropout_rate": "The dropout rate.",
                "attention_type": "The type of attention (e.g., 'global', 'local_sliding').",
                "model_mode": "The model's operational mode (e.g., 'train', 'prefill')."
            },
            "notes": [
                "This function acts as a bridge to integrate NNX-based attention mechanisms into Flax Linen models.",
                "It takes a comprehensive set of configuration parameters to customize the attention behavior."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#Attention",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class Attention(nnx.Module):\n  \"\"\"Attention Module.\n\n  This module implements multi-headed attention as described in the\n  original Transformer paper. It projects the inputs into query, key, and\n  value vectors, applies the attention mechanism, and projects the results to\n  an output vector.\n\n  Attributes:\n    config: The model configuration.\n    num_query_heads: Number of query attention heads.\n    num_kv_heads: Number of key-value attention heads.\n    head_dim: The dimension of each attention head.\n    max_target_length: Maximum sequence length.\n    mesh: The device mesh.\n    attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n    inputs_q_shape: Query inputs shape for initialization, required by NNX.\n    inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n    dtype: The data type for computation.\n    weight_dtype: The data type for weights.\n    max_prefill_predict_length: Maximum length for prefill.\n    dropout_rate: The dropout rate.\n    kernel_init: Initializer for the kernel of the dense layers.\n    float32_qk_product: If True, compute query-key product in float32.\n    float32_logits: If True, cast logits to float32 before softmax.\n    quant: Quantization configuration.\n    kv_quant: KV cache quantization configuration.\n    attention_type: The type of attention (e.g., 'global', 'local_sliding').\n    attn_logits_soft_cap: Soft cap for attention logits.\n    ... and other configuration parameters.\n  \"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      base_kv_cache: bool = True,\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the Attention module.\n\n    Attributes:\n      config: The model configuration.\n      num_query_heads: Number of query attention heads.\n      num_kv_heads: Number of key-value attention heads.\n      head_dim: The dimension of each attention head.\n      max_target_length: Maximum sequence length.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n      inputs_q_shape: Query inputs shape for initialization, required by NNX.\n      inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n      dtype: The data type for computation.\n      weight_dtype: The data type for weights.\n      max_prefill_predict_length: Maximum length for prefill.\n      dropout_rate: The dropout rate.\n      kernel_init: Initializer for the kernel of the dense layers.\n      float32_qk_product: If True, compute query-key product in float32.\n      float32_logits: If True, cast logits to float32 before softmax.\n      quant: Quantization configuration.\n      kv_quant: KV cache quantization configuration.\n      attention_type: The type of attention (e.g., 'global', 'local_sliding').\n      attn_logits_soft_cap: Soft cap for attention logits.\n      sliding_window_size: The size of the sliding window for local attention.\n      use_ragged_attention: Whether to use ragged attention for decoding.\n      ragged_block_size: The block size for ragged attention.\n      use_qk_norm: Whether to apply normalization to query and key.\n      query_pre_attn_scalar: Scalar to apply to query before attention.\n      use_bias_in_projections: Whether to use bias in Q, K, V, and output projections.\n      temperature_tuning: Whether to use temperature tuning for attention.\n      temperature_tuning_scale: The scale for temperature tuning.\n      temperature_tuning_floor_scale: The floor scale for temperature tuning.\n      ... other configuration parameters.\n      is_nope_layer: Whether this is a \"NoPE\" (No Position-Embedding) layer.\n      is_vision: Whether this is a vision attention layer.\n      model_mode: The model's operational mode (e.g., 'train', 'prefill').\n      base_kv_cache: Whether to use base (non-MLA) kv cache, if KVCache is used\n      rngs: RNG state for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n\n    self.config = config\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.head_dim = head_dim\n    self.max_target_length = max_target_length\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.dropout_rate = dropout_rate\n    self.kernel_init = kernel_init\n    self.float32_qk_product = float32_qk_product\n    self.float32_logits = float32_logits\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n    self.use_qk_norm = use_qk_norm\n    self.query_pre_attn_scalar = query_pre_attn_scalar\n    self.use_bias_in_projections = use_bias_in_projections\n    self.temperature_tuning = temperature_tuning\n    self.temperature_tuning_scale = temperature_tuning_scale\n    self.temperature_tuning_floor_scale = temperature_tuning_floor_scale\n    self.prefill_query_axis_names = prefill_query_axis_names\n    self.prefill_key_axis_names = prefill_key_axis_names\n    self.prefill_value_axis_names = prefill_value_axis_names\n    self.query_axis_names = query_axis_names\n    self.key_axis_names = key_axis_names\n    self.value_axis_names = value_axis_names\n    self.ep_query_axis_names = ep_query_axis_names\n    self.ep_key_axis_names = ep_key_axis_names\n    self.ep_value_axis_names = ep_value_axis_names\n    self.input_axis_names = input_axis_names\n    self.ep_input_axis_names = ep_input_axis_names\n    self.out_axis_names = out_axis_names\n    self.ep_out_axis_names = ep_out_axis_names\n    self.prefill_input_axis_names = prefill_input_axis_names\n    self.decode_input_axis_names = decode_input_axis_names\n    self.prefill_out_axis_names = prefill_out_axis_names\n    self.decode_out_axis_names = decode_out_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.compute_axis_order = compute_axis_order\n    self.reshape_q = reshape_q\n    self.is_nope_layer = is_nope_layer\n    self.is_vision = is_vision\n    self.model_mode = model_mode\n    self.rngs = rngs\n\n    self.is_qwen3_next = self.config.decoder_block == DecoderBlockType.QWEN3_NEXT\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.KVCache_0 = (\n        self.init_kv_caches(inputs_kv_shape=inputs_kv_shape)\n        if self.model_mode != MODEL_MODE_TRAIN and base_kv_cache\n        else None\n    )\n\n    self.rotary_embedding = self.init_rotary_embedding()\n\n    self.attention_op = AttentionOp(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        max_prefill_predict_length=self.max_prefill_predict_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_query_heads,\n        num_kv_heads=self.num_kv_heads,\n        dropout_rate=self.dropout_rate,\n        dtype=self.dtype,\n        compute_axis_order=self.compute_axis_order,\n        reshape_q=self.reshape_q,\n        attention_type=self.attention_type,\n        attn_logits_soft_cap=self.attn_logits_soft_cap,\n        sliding_window_size=self.sliding_window_size,\n        chunk_attn_window_size=self.config.chunk_attn_window_size,\n        use_ragged_attention=self.use_ragged_attention,\n        ragged_block_size=self.ragged_block_size,\n        rngs=self.rngs,\n    )\n    # When paged attention is enabled, paged attention op is used for all model modes except TRAIN,\n    # which uses default attention op.\n    if self.config.attention == \"paged\":\n      self.paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=self.head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n    self._init_projections(inputs_q_shape, inputs_kv_shape)\n\n    if self.config.attention_sink:\n      self.sinks = nnx.Param(\n          default_bias_init(self.rngs.params(), (self.config.num_query_heads,), self.weight_dtype),\n          sharding=(None,),\n      )\n    else:\n      self.sinks = None\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      self.query_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          shard_mode=self.config.shard_mode,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.key_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          shard_mode=self.config.shard_mode,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    elif self.is_qwen3_next:\n      self.query_norm = Qwen3NextRMSNorm(\n          num_features=self.config.head_dim,\n          eps=self.config.normalization_layer_epsilon,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          rngs=self.rngs,\n      )\n      self.key_norm = Qwen3NextRMSNorm(\n          num_features=self.config.head_dim,\n          eps=self.config.normalization_layer_epsilon,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          rngs=self.rngs,\n      )\n    else:\n      self.query_norm = None\n      self.key_norm = None\n\n    self._maybe_shard_with_logical = functools.partial(\n        maybe_shard_with_logical,\n        mesh=mesh,\n        shard_mode=config.shard_mode,\n    )\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the query, key, value, and output projections.\"\"\"\n    if self.config.fused_qkv:\n      self.qkv_proj = self.init_qkv_w(inputs_shape=inputs_q_shape)\n    else:\n      self.query = self.init_query_w(inputs_q_shape=inputs_q_shape)\n      self.key = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n      self.value = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n  def init_query_w(self, inputs_q_shape: Tuple) -> nnx.Module:\n    \"\"\"Query projection initialization.\"\"\"\n\n    # NOTE: T5 does not explicitly rescale the attention logits by\n    #       1/sqrt(depth_kq)!  This is folded into the initializers of the\n    #       linear transformations, which is equivalent under Adafactor.\n    # We disable depth_scaling when using qk_norm or a query_pre_attn_scalar\n    # to avoid applying scaling twice.\n    if self.config.use_qk_norm or (self.query_pre_attn_scalar is not None and self.query_pre_attn_scalar != 1.0):\n      depth_scaling = 1.0\n    else:\n      depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n\n    def query_init(*args):\n      # pylint: disable=no-value-for-parameter\n      return self.kernel_init(*args) / depth_scaling\n\n    kernel_axes = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"embed\", \"q_heads\", \"kv\")\n    )\n    in_features = self.convert_dense_general_inputs_shape(inputs_q_shape)\n    out_features = (self.num_query_heads, self.head_dim)\n\n    if self.is_qwen3_next:\n      out_features = (self.num_query_heads, self.head_dim * 2)\n\n    return DenseGeneral(\n        in_features_shape=in_features,\n        out_features_shape=out_features,\n        axis=-1,\n        kernel_init=query_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        shard_mode=self.config.shard_mode,\n        rngs=self.rngs,\n    )\n\n  def query_projection(self, inputs_q: Array, out_sharding: NamedSharding | None = None) -> Array:\n    \"\"\"Query projection.\"\"\"\n\n    return self.query(inputs_q, out_sharding=out_sharding)\n\n  def init_kv_w(self, inputs_kv_shape: Tuple) -> nnx.Module:\n    \"\"\"Initializes the key or value projection.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A DenseGeneral module that performs the key or value projection.\n    \"\"\"\n    if self.num_kv_heads == -1:\n      raise ValueError(\"num_kv_heads is not defined.\")\n\n    if self.num_query_heads % self.num_kv_heads != 0:\n      raise ValueError(\"Invalid num_kv_heads for GQA.\")\n\n    kernel_axes = (\n        (None, None, None)\n        if self.config.ici_context_autoregressive_parallelism > 1\n        else (\"embed\", \"kv_heads\", \"kv_head_dim\")\n    )\n\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_kv_shape),\n        out_features_shape=(self.num_kv_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        shard_mode=self.config.shard_mode,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def kv_projection(self, inputs_kv: Array, proj_name: str, out_sharding: NamedSharding | None = None) -> nnx.Module:\n    \"\"\"Applies the key or value projection.\n\n    Args:\n      inputs_kv: The input tensor to project.\n      proj_name: The name of the projection (\"key\" or \"value\").\n\n    Returns:\n      The projected key or value tensor.\n\n    Raises:\n      ValueError: If `proj_name` is not one of the supported values\n        (\"key\", \"value\").\n\n    \"\"\"\n    if proj_name == \"key\":\n      return self.key(inputs_kv, out_sharding=out_sharding)\n    elif proj_name == \"value\":\n      return self.value(inputs_kv, out_sharding=out_sharding)\n    else:\n      raise ValueError(f\"proj_name must be 'key' or 'value', but got {proj_name}\")\n\n  def init_qkv_w(self, inputs_shape: Tuple) -> nnx.Module:\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_shape),\n        out_features_shape=(3, self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        shard_mode=self.config.shard_mode,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def qkv_projection(self, inputs: Array, proj_name: str, out_sharding: NamedSharding | None = None):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = self.qkv_proj(inputs, out_sharding)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def init_out_w(self, output_dim: int) -> nnx.Module:\n    \"\"\"out projection\"\"\"\n    in_features = (self.num_query_heads, self.head_dim)\n    out_features = output_dim\n    out_kernel_axis = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"heads\", \"kv\", \"embed\")\n    )\n    axis = (-2, -1)\n\n    if self.is_qwen3_next:\n      in_features = self.num_query_heads * self.head_dim\n      out_kernel_axis = (\"mlp\", \"embed\")\n      axis = (-1,)\n\n    return DenseGeneral(\n        in_features_shape=in_features,\n        out_features_shape=out_features,\n        axis=axis,\n        kernel_init=self.kernel_init,\n        kernel_axes=out_kernel_axis,  # trade speed with memory\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        shard_mode=self.config.shard_mode,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def out_projection(self, out: Array, out_sharding: NamedSharding | None = None) -> Array:\n    \"\"\"out projection\"\"\"\n    return self.out(out, out_sharding=out_sharding)\n\n  def convert_dense_general_inputs_shape(\n      self,\n      inputs_shape: tuple[int, ...] | None = None,\n      axis: Union[Iterable[int], int] = -1,\n  ) -> Union[Iterable[int], int]:\n    axis = canonicalize_tuple(axis)\n    return tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n\n  def init_rotary_embedding(self):\n    \"\"\"Initializes the rotary embeddings, handling different model types.\n\n    Returns:\n      The rotary embedding module that will be used in the model.\n    \"\"\"\n    if self.config.attention_type == AttentionType.MLA.value:\n      # For MLA attention RoPE is applied to only `self.qk_rope_head_dim` portion the heads.\n      rope_embedding_dims = self.qk_rope_head_dim\n    else:\n      rope_embedding_dims = self.head_dim\n\n    rope_type = self.config.rope_type.lower()\n    rope_use_scale = self.config.rope_use_scale\n    if self.is_vision:\n      rotary_embedding = LlamaVisionRotaryEmbedding(\n          image_size=self.config.image_size_for_vit,\n          patch_size=self.config.patch_size_for_vit,\n          hidden_size=self.config.hidden_size_for_vit,\n          num_attention_heads=self.config.num_attention_heads_for_vit,\n          rope_theta=self.config.rope_theta_for_vit,\n          fprop_dtype=self.dtype,\n          rngs=self.rngs,\n      )\n    elif self.config.model_name.startswith(\"llama3.1\") or rope_type.startswith(\"llama3.1\"):\n      rotary_embedding = LLaMARotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=self.config.rope_max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          use_scale=rope_use_scale,\n          rngs=self.rngs,\n      )\n    elif rope_type.startswith(\"yarn\"):\n      rotary_embedding = YarnRotaryEmbedding(\n          max_position_embeddings=self.config.max_position_embeddings,\n          original_max_position_embeddings=self.config.original_max_position_embeddings,\n          beta_fast=self.config.beta_fast,\n          beta_slow=self.config.beta_slow,\n          rope_theta=self.config.rope_max_timescale,\n          rope_factor=self.config.rope_factor,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          interleave=self.config.rope_interleave,\n          truncate=self.config.rope_truncate,\n          attention_scaling=self.config.rope_attention_scaling,\n          rngs=self.rngs,\n      )\n    elif self.is_qwen3_next:\n      rotary_embedding = Qwen3NextRotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=self.config.rope_max_timescale,\n          embedding_dims=self.config.head_dim,\n          partial_rotary_factor=self.config.partial_rotary_factor,\n          cast_as_fprop_dtype=True,\n          fprop_dtype=self.config.dtype,\n          rngs=self.rngs,\n      )\n    else:\n      max_timescale = self.config.rope_max_timescale\n      # For local attention use local_rope_max_timescale if it's is positive\n      if self.attention_type == AttentionType.LOCAL_SLIDING and self.config.local_rope_max_timescale > 0:\n        max_timescale = self.config.local_rope_max_timescale\n\n      rope_linear_scaling_factor = self.config.rope_linear_scaling_factor\n      # In gemma3, linear scaling factor does not apply to local sliding layers.\n      if self.config.model_name.startswith(\"gemma3\") and self.attention_type == AttentionType.LOCAL_SLIDING:\n        rope_linear_scaling_factor = 1.0\n\n      rotary_embedding = RotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          rope_linear_scaling_factor=rope_linear_scaling_factor,\n          rngs=self.rngs,\n      )\n    return rotary_embedding\n\n  def apply_rotary_embedding(self, inputs: Array, inputs_positions: Optional[Array | None] = None):\n    \"\"\"Applies rotary embeddings, handling different model types.\n\n    Args:\n      inputs: The input tensor to apply rotary embeddings to.\n      inputs_positions: The positions of the inputs.\n      name: A name for the embedding layer.\n\n    Returns:\n      The input tensor with rotary embeddings applied.\n    \"\"\"\n    return self.rotary_embedding(inputs, inputs_positions)\n\n  def init_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes KVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A KVCache module instance.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in KVCache.\n    # However, KVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # KVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.KVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_heads=self.num_kv_heads,\n        value_heads=self.num_kv_heads,\n        key_head_size=self.head_dim,\n        value_head_size=self.head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n  def update_kv_caches(self, key, value, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"Updates the KV caches for prefill and autoregressive modes.\n\n    This method uses a kvcache module to update and retrieve the key-value\n    caches based on the current operational mode.\n\n    Args:\n      key: The key tensor for the current attention computation.\n      value: The value tensor for the current attention computation.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, or None.\n      - The autoregressive key-value cache, or None.\n    \"\"\"\n    prefill_kv_cache, ar_kv_cache = self.KVCache_0(\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      out_sharding: NamedSharding | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Any = None,\n  ):\n    \"\"\"Applies Attention on the input data.\n\n    Projects the inputs into multi-headed query, key, and value vectors,\n    applies dot-product attention, and project the results to an output vector.\n\n    This method handles three modes:\n    1.  **Training**: The KV cache is ignored.\n    2.  **Prefill**: The KV cache is filled with the key-value pairs from the input sequence.\n    3.  **Autoregressive Decoding**: The KV cache is used to provide context from previous steps.\n\n    In the cache initialization call, `inputs_q` has a shape [batch, length,\n    q_features] and `inputs_kv`: [batch, length, kv_features]. During the\n    incremental decoding stage, query, key and value all have the shape [batch,\n    1, qkv_features] corresponding to a single step.\n\n    Args:\n      inputs_q: Input queries of shape `[batch, q_length, q_features]`.\n      inputs_kv: Key/values of shape `[batch, kv_length, kv_features]`.\n      inputs_positions: Input positions for rotary embeddings.\n      decoder_segment_ids: Segment IDs for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      deterministic: If True, disables dropout.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      output of shape `[batch, length, q_features]`.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = self._maybe_shard_with_logical(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = self._maybe_shard_with_logical(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = self._maybe_shard_with_logical(inputs_q, self.ep_input_axis_names)\n      inputs_kv = self._maybe_shard_with_logical(inputs_kv, self.ep_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      inputs_q = self._maybe_shard_with_logical(inputs_q, self.input_axis_names)\n      inputs_kv = self._maybe_shard_with_logical(inputs_kv, self.input_axis_names)\n    else:\n      inputs_q = self._maybe_shard_with_logical(inputs_q, self.decode_input_axis_names)\n      inputs_kv = self._maybe_shard_with_logical(inputs_kv, self.decode_input_axis_names)\n\n    # apply projection.\n    if self.config.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query_sharding = NamedSharding(self.mesh, nn.logical_to_mesh_axes(self.query_axis_names))\n      query = self.query_projection(inputs_q, out_sharding=query_sharding)\n      key_sharding = NamedSharding(self.mesh, nn.logical_to_mesh_axes(self.key_axis_names))\n      key = self.kv_projection(inputs_kv, proj_name=\"key\", out_sharding=key_sharding)\n      value_sharding = NamedSharding(self.mesh, nn.logical_to_mesh_axes(self.value_axis_names))\n      value = self.kv_projection(inputs_kv, proj_name=\"value\", out_sharding=value_sharding)\n\n    gate = None\n    if self.is_qwen3_next:\n      # Split query into query & gate.\n      query, gate = jnp.split(query, 2, axis=-1)\n      batch_size, seq_len, _, _ = gate.shape\n      gate = gate.reshape(batch_size, seq_len, self.config.num_query_heads * self.config.head_dim)\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    # NOTE: llama 4 does L2 normalization after RoPE\n    # Apply Qwen3Next specific RMS Norm\n    if (self.use_qk_norm and not is_llama4_decoder_block) or self.is_qwen3_next:\n      query = self.query_norm(query)\n      key = self.key_norm(key)\n\n    # NOTE: is_nope_layer should be used in attention mask and also used in attention tuning\n    use_rope = not self.is_nope_layer\n    use_qk_norm = self.use_qk_norm and use_rope\n\n    if use_rope:\n      query = self.apply_rotary_embedding(query, inputs_positions=inputs_positions)\n      key = self.apply_rotary_embedding(key, inputs_positions=inputs_positions)\n\n    if use_qk_norm and is_llama4_decoder_block:\n      l2_norm = L2Norm(eps=self.config.normalization_layer_epsilon)\n      query = l2_norm(query)\n      key = l2_norm(key)\n\n    # apply query_pre_attn_scalar if it's present.\n    if self.query_pre_attn_scalar and self.query_pre_attn_scalar != 1.0:\n      query = query * self.query_pre_attn_scalar\n\n    if self.temperature_tuning and not use_rope:\n      attn_scales = (\n          jnp.log(jnp.floor((inputs_positions.astype(self.dtype) + 1.0) / self.temperature_tuning_floor_scale) + 1.0)\n          * self.temperature_tuning_scale\n          + 1.0\n      )\n      query = (query * attn_scales[:, :, jnp.newaxis, jnp.newaxis]).astype(self.dtype)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = self._maybe_shard_with_logical(query, self.prefill_query_axis_names)\n      key = self._maybe_shard_with_logical(key, self.prefill_key_axis_names)\n      value = self._maybe_shard_with_logical(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      query = self._maybe_shard_with_logical(query, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      key = self._maybe_shard_with_logical(key, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n      value = self._maybe_shard_with_logical(value, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = self._maybe_shard_with_logical(query, self.ep_query_axis_names)\n      key = self._maybe_shard_with_logical(key, self.ep_key_axis_names)\n      value = self._maybe_shard_with_logical(value, self.ep_value_axis_names)\n    else:\n      query = self._maybe_shard_with_logical(query, self.query_axis_names)\n      key = self._maybe_shard_with_logical(key, self.key_axis_names)\n      value = self._maybe_shard_with_logical(value, self.value_axis_names)\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    assert not self.config.quantize_kvcache or self.kv_quant\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      cached_values = [None, None]\n      if model_mode != MODEL_MODE_TRAIN:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      out = self.attention_op(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          cached_values,\n          previous_chunk,\n          bidirectional_mask,\n          self.sinks,\n      )\n    if self.is_qwen3_next:\n      out = out.reshape(batch_size, seq_len, self.config.num_query_heads * self.config.head_dim)\n      out = out * jax.nn.sigmoid(gate)\n    if model_mode == MODEL_MODE_PREFILL:\n      out = self._maybe_shard_with_logical(out, self.prefill_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = self._maybe_shard_with_logical(out, self.ep_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      out = self._maybe_shard_with_logical(out, self.out_axis_names)\n    else:\n      out = self._maybe_shard_with_logical(out, self.decode_out_axis_names)\n    out = self.out_projection(out, out_sharding=out_sharding)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "attention_module",
            "purpose": "Implements multi-headed attention, projecting inputs to query, key, and value vectors, applying the attention mechanism, and projecting the results to an output vector.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize attention parameters and sub-modules (projections, attention ops, rotary embeddings, KV caches).",
                "Project inputs to query, key, and value vectors.",
                "Apply rotary embeddings (optional).",
                "Apply normalization (optional).",
                "Compute attention scores and context.",
                "Update KV cache (for prefill/autoregressive modes).",
                "Project the attention output to the final output vector."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "DType",
                "Array",
                "AttentionOp",
                "PagedAttentionOp",
                "KVCache",
                "RotaryEmbedding",
                "DenseGeneral",
                "RMSNorm",
                "Qwen3NextRMSNorm",
                "L2Norm"
            ],
            "parameters": {
                "config": "Model configuration object containing various attention-related settings.",
                "num_query_heads": "Number of query attention heads.",
                "num_kv_heads": "Number of key-value attention heads.",
                "head_dim": "Dimension of each attention head.",
                "max_target_length": "Maximum sequence length for attention.",
                "mesh": "Device mesh for distributed computation.",
                "attention_kernel": "Type of attention kernel to use (e.g., 'dot_product', 'flash').",
                "inputs_q_shape": "Shape of query inputs for NNX initialization.",
                "inputs_kv_shape": "Shape of key/value inputs for NNX initialization.",
                "dtype": "Data type for computation.",
                "weight_dtype": "Data type for weights.",
                "max_prefill_predict_length": "Maximum length for prefill.",
                "dropout_rate": "Dropout rate for regularization.",
                "kernel_init": "Initializer for the kernel of dense layers.",
                "float32_qk_product": "Whether to compute query-key product in float32 for stability.",
                "float32_logits": "Whether to cast logits to float32 before softmax for stability.",
                "quant": "Quantization configuration.",
                "kv_quant": "KV cache quantization configuration.",
                "attention_type": "Type of attention (e.g., 'global', 'local_sliding').",
                "attn_logits_soft_cap": "Soft cap for attention logits.",
                "sliding_window_size": "Size of the sliding window for local attention.",
                "use_ragged_attention": "Whether to use ragged attention for decoding.",
                "ragged_block_size": "Block size for ragged attention.",
                "use_qk_norm": "Whether to apply normalization to query and key.",
                "query_pre_attn_scalar": "Scalar to apply to query before attention.",
                "use_bias_in_projections": "Whether to use bias in Q, K, V, and output projections.",
                "temperature_tuning": "Whether to use temperature tuning for attention.",
                "temperature_tuning_scale": "Scale for temperature tuning.",
                "temperature_tuning_floor_scale": "Floor scale for temperature tuning.",
                "prefill_query_axis_names": "Axis names for sharding query during prefill.",
                "prefill_key_axis_names": "Axis names for sharding key during prefill.",
                "prefill_value_axis_names": "Axis names for sharding value during prefill.",
                "query_axis_names": "Axis names for sharding query during training.",
                "key_axis_names": "Axis names for sharding key during training.",
                "value_axis_names": "Axis names for sharding value during training.",
                "ep_query_axis_names": "Axis names for sharding query in expert parallelism context.",
                "ep_key_axis_names": "Axis names for sharding key in expert parallelism context.",
                "ep_value_axis_names": "Axis names for sharding value in expert parallelism context.",
                "input_axis_names": "Axis names for sharding input during training.",
                "ep_input_axis_names": "Axis names for sharding input in expert parallelism context.",
                "out_axis_names": "Axis names for sharding output during training.",
                "ep_out_axis_names": "Axis names for sharding output in expert parallelism context.",
                "prefill_input_axis_names": "Axis names for sharding input during prefill.",
                "decode_input_axis_names": "Axis names for sharding input during decoding.",
                "prefill_out_axis_names": "Axis names for sharding output during prefill.",
                "decode_out_axis_names": "Axis names for sharding output during decoding.",
                "prefill_cache_axis_order": "Axis order for prefill KV cache.",
                "ar_cache_axis_order": "Axis order for autoregressive KV cache.",
                "compute_axis_order": "Axis order for computation.",
                "reshape_q": "Whether to reshape query.",
                "is_nope_layer": "Whether this is a 'NoPE' (No Position-Embedding) layer.",
                "is_vision": "Whether this is a vision attention layer.",
                "model_mode": "The model's operational mode ('train', 'prefill', 'autoregressive').",
                "base_kv_cache": "Whether to use base KV cache.",
                "rngs": "RNG state for initialization."
            },
            "notes": [
                "The module supports different attention types (global, local_sliding) and kernels (dot_product, flash).",
                "It handles training, prefill, and autoregressive decoding modes, utilizing KV caching.",
                "Supports fused QKV projections and paged attention.",
                "Includes options for rotary embeddings, query-key normalization, and temperature tuning.",
                "Handles various sharding strategies based on model configuration and mode."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Attention module with various configuration parameters and sub-modules.",
                    "input": {
                        "shape": "N/A (takes configuration and dimension parameters)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration and dimension parameters.",
                        "Initializes KV caches if not in training mode and base_kv_cache is True.",
                        "Initializes rotary embedding module.",
                        "Initializes AttentionOp and PagedAttentionOp (if applicable).",
                        "Initializes query, key, value, and output projection layers.",
                        "Initializes normalization layers (RMSNorm, Qwen3NextRMSNorm) if use_qk_norm is True.",
                        "Sets up a partial function for sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "DType",
                        "AttentionOp",
                        "PagedAttentionOp",
                        "KVCache",
                        "RotaryEmbedding",
                        "DenseGeneral",
                        "RMSNorm",
                        "Qwen3NextRMSNorm",
                        "functools.partial"
                    ],
                    "notes": [
                        "Module attribute names are designed to match Linen for checkpointing.",
                        "Conditional initialization of KV caches and normalization layers based on model mode and configuration."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the query, key, value, and output projection layers based on fused or separate QKV configuration.",
                    "input": {
                        "shape": "inputs_q_shape: Tuple, inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if fused_qkv is enabled.",
                        "Initializes qkv_proj if fused_qkv is True.",
                        "Initializes separate query, key, and value projection layers if fused_qkv is False.",
                        "Initializes the output projection layer."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "DenseGeneral"
                    ],
                    "notes": []
                },
                "init_query_w": {
                    "purpose": "Initializes the DenseGeneral module for the query projection.",
                    "input": {
                        "shape": "inputs_q_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines depth scaling factor, considering qk_norm and query_pre_attn_scalar.",
                        "Defines a custom kernel initializer that applies scaling.",
                        "Sets kernel axes based on ICI context.",
                        "Calculates input and output features.",
                        "Initializes and returns a DenseGeneral module."
                    ],
                    "output": {
                        "shape": "DenseGeneral module"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "jnp"
                    ],
                    "notes": [
                        "Handles specific scaling for T5-like models and when qk_norm or query_pre_attn_scalar is used."
                    ]
                },
                "query_projection": {
                    "purpose": "Applies the query projection to the input tensor.",
                    "input": {
                        "shape": "inputs_q: Array, out_sharding: Optional[NamedSharding]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the query DenseGeneral module with the input tensor and optional sharding."
                    ],
                    "output": {
                        "shape": "Projected query tensor"
                    },
                    "dependencies": [
                        "Array",
                        "NamedSharding"
                    ],
                    "notes": []
                },
                "init_kv_w": {
                    "purpose": "Initializes the DenseGeneral module for the key or value projection.",
                    "input": {
                        "shape": "inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates num_kv_heads.",
                        "Sets kernel axes based on ICI context.",
                        "Initializes and returns a DenseGeneral module for key/value projection."
                    ],
                    "output": {
                        "shape": "DenseGeneral module"
                    },
                    "dependencies": [
                        "DenseGeneral"
                    ],
                    "notes": [
                        "Raises ValueError if num_kv_heads is not defined or invalid for GQA."
                    ]
                },
                "kv_projection": {
                    "purpose": "Applies the key or value projection to the input tensor.",
                    "input": {
                        "shape": "inputs_kv: Array, proj_name: str, out_sharding: Optional[NamedSharding]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Selects the appropriate projection ('key' or 'value') based on proj_name.",
                        "Applies the projection using the corresponding DenseGeneral module."
                    ],
                    "output": {
                        "shape": "Projected key or value tensor"
                    },
                    "dependencies": [
                        "Array",
                        "NamedSharding"
                    ],
                    "notes": [
                        "Raises ValueError if proj_name is not 'key' or 'value'."
                    ]
                },
                "init_qkv_w": {
                    "purpose": "Initializes the DenseGeneral module for fused QKV projection.",
                    "input": {
                        "shape": "inputs_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes and returns a DenseGeneral module for fused QKV projection."
                    ],
                    "output": {
                        "shape": "DenseGeneral module"
                    },
                    "dependencies": [
                        "DenseGeneral"
                    ],
                    "notes": []
                },
                "qkv_projection": {
                    "purpose": "Performs fused QKV projection and splits the output into query, key, and value.",
                    "input": {
                        "shape": "inputs: Array, proj_name: str, out_sharding: Optional[NamedSharding]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the fused qkv_proj.",
                        "Applies checkpointing name.",
                        "Splits the result into query, key, and value tensors."
                    ],
                    "output": {
                        "shape": "Tuple of (query, key, value) tensors"
                    },
                    "dependencies": [
                        "Array",
                        "NamedSharding",
                        "checkpoint_name"
                    ],
                    "notes": []
                },
                "init_out_w": {
                    "purpose": "Initializes the DenseGeneral module for the output projection.",
                    "input": {
                        "shape": "output_dim: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines input and output features and kernel axes based on configuration (e.g., Qwen3Next).",
                        "Initializes and returns a DenseGeneral module for the output projection."
                    ],
                    "output": {
                        "shape": "DenseGeneral module"
                    },
                    "dependencies": [
                        "DenseGeneral"
                    ],
                    "notes": []
                },
                "out_projection": {
                    "purpose": "Applies the output projection to the attention output.",
                    "input": {
                        "shape": "out: Array, out_sharding: Optional[NamedSharding]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the out DenseGeneral module with the input tensor and optional sharding."
                    ],
                    "output": {
                        "shape": "Projected output tensor"
                    },
                    "dependencies": [
                        "Array",
                        "NamedSharding"
                    ],
                    "notes": []
                },
                "convert_dense_general_inputs_shape": {
                    "purpose": "Converts input shapes for DenseGeneral based on specified axes.",
                    "input": {
                        "shape": "inputs_shape: Optional[Tuple], axis: Union[Iterable[int], int]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Canonicalizes the axis input.",
                        "Normalizes axes based on the input shape's rank.",
                        "Returns a tuple of input shapes corresponding to the normalized axes."
                    ],
                    "output": {
                        "shape": "Union[Iterable[int], int]"
                    },
                    "dependencies": [
                        "canonicalize_tuple",
                        "normalize_axes"
                    ],
                    "notes": []
                },
                "init_rotary_embedding": {
                    "purpose": "Initializes the appropriate rotary embedding module based on model configuration.",
                    "input": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines embedding dimensions based on attention type (MLA vs. others).",
                        "Selects and initializes a specific rotary embedding class (LlamaVisionRotaryEmbedding, LLaMARotaryEmbedding, YarnRotaryEmbedding, Qwen3NextRotaryEmbedding, RotaryEmbedding) based on config parameters like model name, rope type, and is_vision flag.",
                        "Configures parameters like timescales, embedding dimensions, and scaling factors for the chosen rotary embedding."
                    ],
                    "output": {
                        "shape": "Rotary embedding module instance"
                    },
                    "dependencies": [
                        "AttentionType",
                        "LlamaVisionRotaryEmbedding",
                        "LLaMARotaryEmbedding",
                        "YarnRotaryEmbedding",
                        "Qwen3NextRotaryEmbedding",
                        "RotaryEmbedding",
                        "jnp"
                    ],
                    "notes": [
                        "Handles specific configurations for different models (e.g., Llama, Yarn, Qwen3Next, Gemma3) and attention types."
                    ]
                },
                "apply_rotary_embedding": {
                    "purpose": "Applies the initialized rotary embeddings to the input tensor.",
                    "input": {
                        "shape": "inputs: Array, inputs_positions: Optional[Array | None]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the initialized rotary embedding module with the input tensor and positions."
                    ],
                    "output": {
                        "shape": "Input tensor with rotary embeddings applied"
                    },
                    "dependencies": [
                        "Array"
                    ],
                    "notes": []
                },
                "init_kv_caches": {
                    "purpose": "Initializes the KVCache module for storing key-value pairs during inference.",
                    "input": {
                        "shape": "inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extracts batch size from inputs_kv_shape.",
                        "Initializes and returns a KVCache module with parameters like max lengths, batch size, head dimensions, and quantization settings."
                    ],
                    "output": {
                        "shape": "KVCache module instance"
                    },
                    "dependencies": [
                        "kvcache.KVCache"
                    ],
                    "notes": [
                        "Uses placeholder sequence length during initialization as internal cache shapes are based on max lengths."
                    ]
                },
                "update_kv_caches": {
                    "purpose": "Updates and retrieves key-value caches using the KVCache module for different model modes.",
                    "input": {
                        "shape": "key: Array, value: Array, decoder_segment_ids: Array | None, model_mode: str, previous_chunk: Any",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the KVCache module (self.KVCache_0) with key, value, segment IDs, model mode, and chunk information.",
                        "Returns the updated prefill and autoregressive KV caches."
                    ],
                    "output": {
                        "shape": "List containing prefill_kv_cache and ar_kv_cache"
                    },
                    "dependencies": [
                        "Array"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Applies the Attention module to input data, handling different operational modes (train, prefill, autoregressive).",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, q_features], inputs_kv: [batch, kv_length, kv_features], inputs_positions: Optional[Array], decoder_segment_ids: Optional[Array], out_sharding: Optional[NamedSharding], model_mode: str, deterministic: bool, previous_chunk: Any, slot: Optional[int], page_state: Optional[page_manager.PageState], bidirectional_mask: Any",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies sharding to input tensors based on model_mode.",
                        "Projects inputs to query, key, and value using fused or separate projections.",
                        "Handles Qwen3Next specific query/gate splitting.",
                        "Applies normalization (QK norm, Qwen3Next RMS Norm, L2 Norm) if configured.",
                        "Applies rotary embeddings if not a 'nope' layer.",
                        "Applies query_pre_attn_scalar and temperature tuning if configured.",
                        "Applies sharding to query, key, and value tensors based on model_mode.",
                        "Applies checkpointing names to projected tensors.",
                        "Selects between PagedAttentionOp and AttentionOp based on configuration and model_mode.",
                        "Updates KV caches if not in training mode.",
                        "Applies the chosen attention operation.",
                        "Handles Qwen3Next specific output scaling with gate.",
                        "Applies output sharding based on model_mode.",
                        "Projects the attention output using the output projection layer.",
                        "Applies checkpointing name to the final output."
                    ],
                    "output": {
                        "shape": "[batch, length, q_features]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "Array",
                        "NamedSharding",
                        "MODEL_MODE_TRAIN",
                        "MODEL_MODE_PREFILL",
                        "MODEL_MODE_AUTOREGRESSIVE",
                        "checkpoint_name",
                        "paged_attention.PagedAttentionOp",
                        "AttentionOp",
                        "kvcache.KVCache",
                        "jax.nn.sigmoid",
                        "jnp"
                    ],
                    "notes": [
                        "Handles different sharding strategies for inputs and outputs based on model mode.",
                        "Supports paged attention for inference.",
                        "Conditional application of rotary embeddings, normalization, and other features.",
                        "The output shape is typically [batch, length, q_features]."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#DecoderLayer",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class DecoderLayer(nn.Module):\n  \"\"\"\n  Transformer decoder layer that attends to the encoder.\n  This is the core, reusable building block for both the main model's\n  decoder stack and the auxiliary MTP layers.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    _maybe_shard_with_logical = functools.partial(\n        sharding.maybe_shard_with_logical,\n        mesh=mesh,\n        shard_mode=cfg.shard_mode,\n    )\n\n    if self.model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_length\", \"activation_embed\")\n    elif self.config.expert_shard_attention_option == EP_AS_CONTEXT and self.model_mode == MODEL_MODE_TRAIN:\n      logical_axis_names = (\"activation_batch_no_exp\", \"activation_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_length_no_exp\", \"activation_embed\")\n\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs = _maybe_shard_with_logical(inputs, logical_axis_names)\n    else:\n      inputs = _maybe_shard_with_logical(inputs, logical_axis_names)\n\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    if model_mode == MODEL_MODE_PREFILL:\n      lnx = _maybe_shard_with_logical(lnx, logical_axis_names)\n    else:\n      lnx = _maybe_shard_with_logical(lnx, logical_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=self.config,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      attention_lnx = _maybe_shard_with_logical(attention_lnx, logical_axis_names)\n    else:\n      attention_lnx = _maybe_shard_with_logical(attention_lnx, logical_axis_names)\n\n    # MLP block.\n    mlp_lnx = linears.mlp_block(\n        in_features=lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        model_mode=model_mode,\n        config=cfg,\n        quant=self.quant,\n        mesh=self.mesh,\n    )(lnx, deterministic=deterministic)\n    if model_mode == MODEL_MODE_PREFILL:\n      mlp_lnx = _maybe_shard_with_logical(mlp_lnx, logical_axis_names)\n    else:\n      mlp_lnx = _maybe_shard_with_logical(mlp_lnx, logical_axis_names)\n\n    next_layer_addition = mlp_lnx + attention_lnx\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out + inputs\n    if model_mode == MODEL_MODE_PREFILL:\n      layer_output = _maybe_shard_with_logical(\n          layer_output,\n          logical_axis_names,\n      )\n    else:\n      layer_output = _maybe_shard_with_logical(\n          layer_output,\n          logical_axis_names,\n      )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    return layer_output, None if cfg.scan_layers else layer_output",
        "analysis": {
            "module_type": "decoder_layer",
            "purpose": "Represents a single layer within a Transformer decoder stack, responsible for processing input sequences and attending to encoder outputs.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies RMS normalization to the input.",
                "Performs self-attention.",
                "Applies a Multi-Layer Perceptron (MLP) block.",
                "Combines the outputs of self-attention and MLP.",
                "Applies dropout.",
                "Adds the residual connection from the input."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "dependencies": [
                "flax.linen as nn",
                "jax.numpy as jnp",
                "functools",
                "sharding.maybe_shard_with_logical",
                "checkpoint_name",
                "rms_norm",
                "attention_as_linen",
                "linears.mlp_block",
                "nn.Dropout"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters.",
                "mesh": "JAX Mesh object for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'prefill', 'train', 'autoregressive').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The layer supports different model modes (prefill, train, autoregressive) which can affect sharding and attention mechanisms.",
                "It integrates with a configuration object (`Config`) for various settings like dimensions, dropout rates, and normalization epsilon.",
                "The layer can optionally record internal metrics like activation mean, standard deviation, and fraction of zeros.",
                "The output is a tuple, where the first element is the layer's output, and the second is `None` if `scan_layers` is `True` in the config, otherwise it's the layer's output again."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dimension], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str, previous_chunk: Optional, slot: Optional, page_state: Optional",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine logical axis names based on model mode and configuration.",
                        "Shard inputs using `_maybe_shard_with_logical`.",
                        "Apply checkpointing to the input.",
                        "Apply RMS normalization.",
                        "Shard normalized inputs.",
                        "Instantiate and call the self-attention layer.",
                        "Shard attention output.",
                        "Instantiate and call the MLP block.",
                        "Shard MLP output.",
                        "Add MLP and attention outputs.",
                        "Apply dropout.",
                        "Add residual connection.",
                        "Shard the final layer output.",
                        "Optionally record internal metrics.",
                        "Return the layer output and a secondary output based on `cfg.scan_layers`."
                    ],
                    "output": {
                        "shape": "Tuple: (output_tensor, secondary_output_tensor)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "functools.partial",
                        "sharding.maybe_shard_with_logical",
                        "checkpoint_name",
                        "rms_norm",
                        "attention_as_linen",
                        "linears.mlp_block",
                        "nn.Dropout",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "The `model_mode` parameter significantly influences the sharding strategy and attention kernel used.",
                        "Conditional logic based on `model_mode` and `config.expert_shard_attention_option` determines `logical_axis_names`.",
                        "The `deterministic` flag controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#SequentialBlockDecoderLayers",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class SequentialBlockDecoderLayers(nn.Module):\n  \"\"\"Sequential unscanned series of decoder layers.\"\"\"\n\n  decoder_layer: Any\n  num_decoder_layers: int\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  model_mode: str\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic: bool,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ) -> jnp.ndarray:\n    for lyr in range(self.num_decoder_layers):\n      inputs = self.decoder_layer(\n          config=self.config, mesh=self.mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=model_mode\n      )(\n          inputs,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          slot=slot,\n          page_state=page_state,\n      )\n      if self.config.scan_layers:\n        inputs = inputs[0]  #  When scan_layers is True the decoder layers return (outputs, None).\n    if self.config.scan_layers:\n      return inputs, None  # pytype: disable=bad-return-type\n    else:\n      return inputs",
        "analysis": {
            "module_type": "sequential_block_decoder_layers",
            "purpose": "Applies a sequential series of decoder layers to the input.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "float32 or bfloat16"
            },
            "processing_steps": [
                "Iterates through the specified number of decoder layers.",
                "Applies each decoder layer to the input.",
                "If `config.scan_layers` is True, it extracts the first element of the decoder layer's output.",
                "Returns the final processed input."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None) if config.scan_layers is True"
            },
            "dependencies": [
                "nn.Module",
                "Config",
                "Mesh",
                "Quant"
            ],
            "parameters": {
                "decoder_layer": "The decoder layer module to be applied sequentially.",
                "num_decoder_layers": "The total number of decoder layers to apply.",
                "config": "Configuration object containing model settings, including `scan_layers`.",
                "mesh": "Mesh object for distributed computation.",
                "quant": "Quantization configuration.",
                "model_mode": "The current mode of the model (e.g., train, prefill, autoregressive)."
            },
            "notes": [
                "This module is designed to apply multiple decoder layers in a loop.",
                "The behavior changes based on the `config.scan_layers` flag, affecting the output format."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#Decoder",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class Decoder(nn.Module):\n  \"\"\"A stack of decoder layers as a part of an encoder-decoder architecture.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: None | Quant = None\n  model_mode: str = MODEL_MODE_TRAIN\n\n  def setup(self):\n    \"\"\"Initialize decoder layer.\"\"\"\n    self.decoder_layer = self.get_decoder_layers()\n    self.norm_layer = self.get_norm_layer(num_features=self.config.emb_dim)\n    if self.config.using_pipeline_parallelism:\n      pipeline_stage_module = self.get_pipeline_stage_module(self.decoder_layer)\n      remat_policy = self.get_remat_policy()\n      self.pipeline_module = pipeline.Pipeline(\n          config=self.config, mesh=self.mesh, layers=pipeline_stage_module, remat_policy=remat_policy\n      )\n\n  def minimal_policy(self, with_context=False):\n    \"\"\"Helper for creating minimal checkpoint policies.\"\"\"\n    names = [\n        \"query_proj\",\n        \"value_proj\",\n        \"key_proj\",\n        \"qkv_proj\",\n        \"out_proj\",\n        \"mlpwi_0\",\n        \"mlpwi_1\",\n        \"mlpwi\",\n        \"mlpwo\",\n    ]\n    if with_context:\n      names.append(\"context\")\n    return jax.checkpoint_policies.save_only_these_names(*names)\n\n  def get_remat_policy(self):\n    \"\"\"Get remat policy\"\"\"\n    policy = None\n    cfg = self.config\n    if cfg.remat_policy != \"none\":\n      if cfg.remat_policy in (\"minimal_with_context\", \"minimal_flash\"):\n        # save all\n        if cfg.remat_policy == \"minimal_flash\":\n          max_logging.log(\"WARNING: 'minimal_flash' will be deprecated soon, please use 'minimal_with_context' instead.\")\n          max_logging.log(\"WARNING: 'minimal_flash' will be deprecated soon, please use 'minimal_with_context' instead.\")\n        policy = self.minimal_policy(with_context=True)\n      elif cfg.remat_policy == \"minimal\":\n        # save all except context\n        policy = self.minimal_policy()\n      elif cfg.remat_policy == \"save_dot_with_context_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"context\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlpwi\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n            \"mlpwo\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_qkv_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n        )\n      elif cfg.remat_policy == \"qkv_proj_offloaded\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\"query_proj\", \"value_proj\", \"key_proj\"],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"minimal_offloaded\":\n        # offload all except context\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\n                \"query_proj\",\n                \"value_proj\",\n                \"key_proj\",\n                \"qkv_proj\",\n                \"out_proj\",\n                \"mlpwi_0\",\n                \"mlpwi_1\",\n                \"mlpwi\",\n                \"mlpwo\",\n            ],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"custom\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=cfg.tensors_on_device,\n            names_which_can_be_offloaded=cfg.tensors_to_offload,\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"save_out_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"out_proj\",\n        )\n      else:\n        assert cfg.remat_policy == \"full\", \"Remat policy needs to be on list of remat policies\"\n        policy = None\n    return policy\n\n  def get_decoder_layers(self):\n    \"\"\"Retrieves a list of decoder layer classes based on the `decoder_block` config.\n\n    Returns:\n        A list containing one or more `nn.Module` classes for the decoder.\n    \"\"\"\n    match self.config.decoder_block:\n      case DecoderBlockType.DEFAULT:\n        return [DecoderLayer]\n      case DecoderBlockType.LLAMA2:\n        return [llama2.LlamaDecoderLayerToLinen]\n      case DecoderBlockType.MISTRAL:\n        # TODO(ranran): update to Mistral with sliding window attention\n        return [mistral.MistralDecoderLayerToLinen]\n      case DecoderBlockType.MIXTRAL:\n        return [mixtral.MixtralDecoderLayerToLinen]\n      case DecoderBlockType.DEEPSEEK:\n        if self.config.use_batch_split_schedule:\n          return [deepseek_batchsplit.DeepSeekDenseLayer, deepseek_batchsplit.DeepSeekMoELayer]\n        else:\n          return [deepseek.DeepSeekDenseLayer, deepseek.DeepSeekMoELayer]\n      case DecoderBlockType.GEMMA:\n        return [gemma.GemmaDecoderLayerToLinen]\n      case DecoderBlockType.GEMMA2:\n        return [gemma2.Gemma2DecoderLayerToLinen]\n      case DecoderBlockType.GEMMA3:\n        return [gemma3.Gemma3DecoderLayerToLinen]\n      case DecoderBlockType.GPT3:\n        return [gpt3.Gpt3DecoderLayer]\n      case DecoderBlockType.GPT_OSS:\n        return [gpt_oss.GptOssScannableBlock] if self.config.scan_layers else [gpt_oss.GptOssDecoderLayer]\n      case DecoderBlockType.QWEN3:\n        return [qwen3.Qwen3DecoderLayerToLinen]\n      case DecoderBlockType.QWEN3_MOE:\n        return [qwen3.Qwen3MoeDecoderLayerToLinen]\n      case DecoderBlockType.QWEN3_NEXT:\n        return [qwen3.Qwen3NextScannableBlockToLinen] if self.config.scan_layers else [qwen3.Qwen3NextDecoderLayerToLinen]\n      case DecoderBlockType.SIMPLE:\n        return [simple_layer.SimpleDecoderLayerToLinen]\n      case DecoderBlockType.SIMPLE_MLP:\n        return [simple_layer.SimpleMlpDecoderLayerToLinen]\n      case DecoderBlockType.LLAMA4:\n        return [llama4.Llama4ScannableBlockToLinen] if self.config.scan_layers else [llama4.Llama4DecoderLayerToLinen]\n      case _:\n        # Default case to handle any unknown decoder block types.\n        raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def set_remat_policy(self, block_layers, policy):\n    \"\"\"Set remat policy\"\"\"\n    RemattedBlockLayers = []\n    for block_layer in block_layers:\n      if self.config.parameter_memory_host_offload:\n        # Define parameter movement with mesh-based sharding\n        def move_to_device(variables):\n          \"\"\"Move parameters to device with proper sharding.\"\"\"\n\n          def map_fn(path, value):\n            max_logging.log(f\"models.py: Moving parameter {path} to device\")\n            return jax.device_put(value, max_utils.device_space())\n\n          return jax.tree_util.tree_map_with_path(map_fn, variables)\n\n        # Transform layer class before remat\n        block_layer = nn.map_variables(block_layer, [\"params\"], move_to_device, mutable=True)\n\n      # Apply remat policy to layer\n      layer = nn.remat(\n          block_layer,\n          prevent_cse=maxtext_utils.should_prevent_cse_in_remat(self.config),\n          policy=policy,\n          static_argnums=(4, 5),  # Deterministic and model mode are static arguments.\n      )\n      RemattedBlockLayers.append(layer)\n    return RemattedBlockLayers\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer (return type inherits from nn.Module)\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.QWEN3_MOE,\n        DecoderBlockType.QWEN3_NEXT,\n        DecoderBlockType.GPT_OSS,\n        DecoderBlockType.SIMPLE,\n        DecoderBlockType.SIMPLE_MLP,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(rms_norm, num_features=num_features, shard_mode=self.config.shard_mode)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      return functools.partial(gpt3.gpt3_layer_norm, num_features=num_features, reductions_in_fp32=False, use_bias=True)\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def scan_decoder_layers(self, cfg, decoder_layer, length, metadata_axis_name, mesh, in_axes_tuple, **kwargs):\n    \"\"\"scan decoder layers, calls `flax.linen.transforms.scan`\"\"\"\n    initializing = self.is_mutable_collection(\"params\")\n    params_spec = cfg.param_scan_axis if initializing else ScanIn(cfg.param_scan_axis)\n    cache_spec = 0\n    scan_fn = nn.scan(\n        decoder_layer,\n        variable_axes={\n            \"params\": params_spec,\n            \"cache\": cache_spec,\n            \"intermediates\": 0,\n            \"aqt\": 0,\n            \"_overwrite_with_gradient\": 0,\n        },\n        split_rngs={\n            \"params\": True,\n            \"dropout\": cfg.enable_dropout,\n        },\n        in_axes=in_axes_tuple,\n        length=length,\n        metadata_params={nn.PARTITION_NAME: metadata_axis_name},\n    )\n    return scan_fn(\n        config=cfg, mesh=mesh, name=metadata_axis_name, quant=self.quant, **kwargs  # pytype: disable=wrong-keyword-args\n    )\n\n  def get_pipeline_stage_module(self, decoder_blocks):\n    \"\"\"get pipeline stage module\"\"\"\n\n    def get_layer_to_pipeline(blocks, cfg):\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        return blocks[1]  # return the sparse block\n      else:\n        return blocks[0]\n\n    cfg = self.config\n    base_stage = get_layer_to_pipeline(decoder_blocks, cfg)\n    if cfg.set_remat_policy_on_layers_per_stage:\n      policy = self.get_remat_policy()\n      base_stage = self.set_remat_policy([base_stage], policy)[0]\n    if cfg.num_layers_per_pipeline_stage == 1:\n      stage_module = base_stage(config=cfg, mesh=self.mesh, quant=self.quant, model_mode=self.model_mode)\n    elif cfg.scan_layers_per_stage:\n      stage_module = self.scan_decoder_layers(\n          cfg,\n          base_stage,\n          cfg.num_layers_per_pipeline_stage,\n          \"layers_per_stage\",\n          self.mesh,\n          in_axes_tuple=(nn.broadcast,) * 4,\n      )\n    else:\n      stage_module = SequentialBlockDecoderLayers(\n          decoder_layer=base_stage,\n          num_decoder_layers=cfg.num_layers_per_pipeline_stage,\n          config=cfg,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=self.model_mode,\n      )\n    return stage_module\n\n  @nn.compact\n  def _apply_embedding(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      image_embeddings=None,\n      bidirectional_mask=None,\n      image_masks=None,\n  ):\n    \"\"\"Applies token and positional embeddings to the input tokens.\"\"\"\n    cfg = self.config\n\n    y = shared_embedding(decoder_input_tokens.astype(\"int32\"), model_mode=model_mode)\n\n    # Merge the image embeddings with the text embeddings for multimodal models\n    if image_embeddings is not None and cfg.use_multimodal:\n      if cfg.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\", \"llama4-17b-16e\", \"llama4-17b-128e\"]:\n        y = multimodal_utils.merge_mm_embeddings(\n            text_embeddings=y,\n            vision_embeddings=image_embeddings,\n            mask=bidirectional_mask,\n            image_masks=image_masks,\n        )\n      # TODO(hengtaoguo): Add support for other multimodal models such as Llama4, refactor if needed\n      else:\n        raise ValueError(f\"Unsupported model_name for multimodal: {cfg.model_name}\")\n\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n    y = y.astype(cfg.dtype)\n\n    if cfg.use_untrainable_positional_embedding:\n      y = positional_embedding_as_linen(embedding_dims=cfg.base_emb_dim)(y, decoder_positions)\n\n    if cfg.trainable_position_size > 0:\n      y += embed_as_linen(\n          num_embeddings=cfg.trainable_position_size,\n          num_features=cfg.emb_dim,\n          dtype=cfg.dtype,\n          embedding_init=nn.initializers.normal(stddev=1.0),\n          name=\"position_embedder\",\n          config=cfg,\n          mesh=self.mesh,\n      )(decoder_positions, model_mode=model_mode)\n    return y\n\n  @nn.compact\n  def _apply_output_head(self, shared_embedding: nn.Module | nnx.Module, y, deterministic, model_mode):\n    \"\"\"Applies final normalization and projects hidden states to logits.\"\"\"\n\n    cfg = self.config\n    if cfg.shard_mode == ShardMode.EXPLICIT:\n      norm_out_sharding = NamedSharding(\n          self.mesh,\n          nn.logical_to_mesh_axes(\n              (\n                  \"activation_batch\",\n                  \"activation_length_no_exp\",\n                  \"activation_embed\",\n              )\n          ),\n      )\n    else:\n      norm_out_sharding = None\n\n    y = self.get_norm_layer(num_features=y.shape[-1])(\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"decoder_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n    )(y, out_sharding=norm_out_sharding)\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n\n    if model_mode in (MODEL_MODE_PREFILL, MODEL_MODE_AUTOREGRESSIVE):\n      out_sharding = NamedSharding(self.mesh, nn.logical_to_mesh_axes((None, None, \"activation_vocab\")))\n    else:\n      out_sharding = NamedSharding(\n          self.mesh,\n          nn.logical_to_mesh_axes(\n              (\n                  \"activation_embed_and_logits_batch\",\n                  \"activation_length_no_exp\",\n                  \"activation_vocab\",\n              )\n          ),\n      )\n\n    # [batch, length, emb_dim] -> [batch, length, vocab_size]\n    if cfg.logits_via_embedding:\n      # Use the transpose of embedding matrix for logit transform.\n      if isinstance(shared_embedding, nnx.Module):\n        embedding_table = shared_embedding.embedding.value\n      else:\n        embedding_table = shared_embedding.variables[\"params\"][\"embedding\"]\n      if isinstance(embedding_table, nn.spmd.LogicallyPartitioned):\n        embedding_table = embedding_table.unbox()\n      attend_dtype = jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype\n      logits = attend_on_embedding(y, embedding_table, attend_dtype, self.config, out_sharding)\n\n      if self.config.normalize_embedding_logits:\n        # Correctly normalize pre-softmax logits for this shared case.\n        logits = logits / jnp.sqrt(y.shape[-1])\n      if cfg.final_logits_soft_cap:\n        logits = logits / cfg.final_logits_soft_cap\n        logits = jnp.tanh(logits) * cfg.final_logits_soft_cap\n    else:\n      logits = linears.dense_general(\n          inputs_shape=y.shape,\n          out_features_shape=cfg.vocab_size,\n          weight_dtype=cfg.weight_dtype,\n          dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n          kernel_axes=(\"embed\", \"vocab\"),\n          shard_mode=cfg.shard_mode,\n          name=\"logits_dense\",\n          matmul_precision=self.config.matmul_precision,\n          parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n      )(\n          y,\n          out_sharding=out_sharding,\n      )  # We do not quantize the logits matmul.\n\n    if self.config.cast_logits_to_fp32:\n      logits = logits.astype(jnp.float32)\n\n    return logits\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      decoder_segment_ids=None,\n      deterministic=False,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      bidirectional_mask: None | Any = None,\n      image_embeddings: None | jnp.ndarray = None,\n      image_masks: None | jnp.ndarray = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    assert decoder_input_tokens.ndim == 2  # [batch, len]\n\n    # [batch, length] -> [batch, length, emb_dim]\n    y = self._apply_embedding(\n        shared_embedding,\n        decoder_input_tokens,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        image_embeddings,\n        bidirectional_mask,\n        image_masks,\n    )\n\n    policy = self.get_remat_policy()\n    RemattedBlockLayers = self.set_remat_policy(self.decoder_layer, policy)\n    # scan does not support kwargs in layer call, passing broadcast_args as positional arg\n    broadcast_args = (\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n    )\n    if cfg.using_pipeline_parallelism:\n      if cfg.pipeline_fsdp_ag_once:\n        partition_spec = self.pipeline_module.get_weight_sharding(\n            y, decoder_segment_ids, decoder_positions, deterministic, model_mode\n        )\n      else:\n        partition_spec = None  # This partition spec is only used for the fsdp_ag_once feature.\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n        dense_layer = RemattedBlockLayers[0]\n        moe_layer = RemattedBlockLayers[1]\n        num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n        num_moe_layers_outside_pp = num_moe_layers - self.config.pipeline_parallel_layers\n        logical_axis_rules_pp_as_dp = sharding.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n        # We chose not to pipeline the dense layers, only sparse for SPMD.\n        with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n          )(y, *broadcast_args)\n          if num_moe_layers_outside_pp > 0:\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                moe_layer,\n                num_moe_layers_outside_pp,\n                \"moe_layers\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n                model_mode=model_mode,\n            )(y, *broadcast_args)\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n      else:  # Not DeepSeek\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n        remaining_layers = self.config.num_decoder_layers - self.config.pipeline_parallel_layers\n        if remaining_layers > 0:\n          logical_axis_rules_pp_as_dp = sharding.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n          with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                RemattedBlockLayers[0],\n                remaining_layers,\n                \"layers_outside_pipeline\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n                model_mode=model_mode,\n            )(y, *broadcast_args)\n    else:\n      if cfg.scan_layers:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n          layer_call_kwargs = {\n              \"page_state\": page_state,\n              \"previous_chunk\": previous_chunk,\n              \"slot\": slot,\n          }\n          dense_layer = RemattedBlockLayers[0]\n          dense_layer.__call__ = functools.partial(dense_layer.__call__, **layer_call_kwargs)\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n          )(y, *broadcast_args)\n          moe_layer = RemattedBlockLayers[1]\n          moe_layer.__call__ = functools.partial(moe_layer.__call__, **layer_call_kwargs)\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              moe_layer,\n              num_moe_layers,\n              \"moe_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n          )(y, *broadcast_args)\n        elif cfg.decoder_block == DecoderBlockType.GEMMA3:\n          y = self._apply_gemma3_scanned_blocks(\n              y,\n              decoder_segment_ids,\n              decoder_positions,\n              deterministic,\n              model_mode,\n              bidirectional_mask,\n              previous_chunk,\n              page_state,\n              slot,\n          )\n        else:\n          RemattedBlockLayer = RemattedBlockLayers[0]\n          scan_length = int(cfg.num_decoder_layers / cfg.inhomogeneous_layer_cycle_interval)\n          layer_kwargs = {}\n          if cfg.decoder_block == DecoderBlockType.LLAMA4:\n            layer_kwargs = {\n                \"nope_layer_interval\": self.config.nope_layer_interval,\n                \"interleave_moe_layer_step\": self.config.interleave_moe_layer_step,\n            }\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              RemattedBlockLayer,\n              scan_length,\n              \"layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              model_mode=model_mode,\n              **layer_kwargs,\n          )(y, *broadcast_args)\n      else:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Unscanned layers must have a length of 2 using deepseek.\"\n          dense_layer = RemattedBlockLayers[0]\n          moe_layer = RemattedBlockLayers[1]\n\n          layers = [dense_layer, moe_layer]\n          layer_prefixes = [\"dense_layers\", \"moe_layers\"]\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          num_layers_list = [cfg.first_num_dense_layers, num_moe_layers]\n          # Iterate over the two layer groups (dense and MoE) and apply layer transformation\n          for layer, num_layers, layer_prefix in zip(layers, num_layers_list, layer_prefixes):\n            for index in range(num_layers):\n              y = layer(\n                  config=cfg, mesh=mesh, name=f\"{layer_prefix}_{index}\", quant=self.quant, model_mode=self.model_mode\n              )(\n                  y,\n                  decoder_segment_ids,\n                  decoder_positions,\n                  deterministic,\n                  model_mode,\n                  previous_chunk=previous_chunk,\n                  page_state=page_state,\n                  slot=slot,\n              )\n        else:\n          for lyr in range(cfg.num_decoder_layers):\n            RemattedBlockLayer = RemattedBlockLayers[0]\n            layer_kwargs = {}\n            layer_call_kwargs = {}\n            if cfg.decoder_block == DecoderBlockType.GEMMA3:\n              # Gemma3 uses both global and sliding window attention depending on the layer index.\n              layer_kwargs = {\"attention_type\": gemma3.get_attention_type(layer_id=lyr)}\n              layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n            if cfg.decoder_block == DecoderBlockType.LLAMA4:\n              layer_kwargs = {\n                  \"is_nope_layer\": llama4.determine_is_nope_layer(lyr, self.config.nope_layer_interval),\n                  \"is_moe_layer\": llama4.determine_is_moe_layer(lyr, self.config.interleave_moe_layer_step),\n              }\n            if cfg.decoder_block == DecoderBlockType.QWEN3_NEXT:\n              layer_kwargs = {\"layer_idx\": lyr}\n            if cfg.decoder_block == DecoderBlockType.GPT_OSS:\n              layer_kwargs = {\"attention_type\": gpt_oss.get_attention_type(layer_id=lyr)}\n            layer = RemattedBlockLayer(\n                config=cfg, mesh=mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=self.model_mode, **layer_kwargs\n            )\n            y = layer(\n                y,\n                decoder_segment_ids,\n                decoder_positions,\n                deterministic,\n                model_mode,\n                previous_chunk=previous_chunk,\n                page_state=page_state,\n                slot=slot,\n                **layer_call_kwargs,\n            )\n\n    assert isinstance(y, jax.Array)\n\n    # After the final transformer layer, `y` holds the raw, un-normalized hidden state.\n    hidden_state = y\n\n    # When vocab tiling is enabled in training mode, full logits won't generate to reduce memory\n    # Instead, we keep track on the hidden states, which has smaller size compared to full logits\n    if cfg.num_vocab_tiling > 1 and self.model_mode == MODEL_MODE_TRAIN:\n      logits = None\n      self.sow(\"intermediates\", \"hidden_states\", hidden_state)\n    else:\n      logits = self._apply_output_head(shared_embedding, hidden_state, deterministic, model_mode)\n\n    # The API of the Decoder is now a tuple, providing both the main output\n    # and the raw hidden state needed for auxiliary tasks.\n    return logits, hidden_state\n\n  def _apply_gemma3_scanned_blocks(\n      self,\n      y,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      bidirectional_mask,\n      previous_chunk,\n      page_state,\n      slot,\n  ):\n    \"\"\"Applies Gemma3 scanned decoder blocks, handling main scan and remainders.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n\n    # Define the repeating pattern length and calculate how many full blocks to scan\n    attention_pattern_length = len(gemma3.GEMMA3_ATTENTION_PATTERN)\n    scan_length = cfg.num_decoder_layers // attention_pattern_length\n\n    policy = self.get_remat_policy()\n    RemattedGemma3Block = self.set_remat_policy([gemma3.Gemma3ScannableBlockToLinen], policy)[0]\n\n    layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n    layer_kwargs = {\"num_of_layers\": attention_pattern_length}\n\n    # Apply the main scan over the full blocks\n    if scan_length > 0:\n      broadcast_args = (\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n      y, _ = self.scan_decoder_layers(\n          cfg,\n          RemattedGemma3Block,\n          scan_length,\n          \"layers\",\n          mesh,\n          in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          model_mode=self.model_mode,\n          **layer_kwargs,\n      )(y, *broadcast_args, **layer_call_kwargs)\n\n    # Apply any remaining layers that did not fit into a full scanned block\n    num_remaining_layers = cfg.num_decoder_layers % attention_pattern_length\n    if num_remaining_layers > 0:\n      # We name the remainder block with a 'remainder' suffix to avoid parameter name collisions\n      rem_layer_kwargs = {\"num_of_layers\": num_remaining_layers}\n      # pytype: disable=wrong-keyword-args\n      layer = RemattedGemma3Block(\n          config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode, name=\"layers_remainder\", **rem_layer_kwargs\n      )\n      y, _ = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          **layer_call_kwargs,\n      )\n    return y",
        "analysis": {
            "module_type": "decoder",
            "purpose": "A stack of transformer decoder layers forming the core of an encoder-decoder architecture.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply token and positional embeddings.",
                "Process through a stack of decoder layers (potentially using scanning or pipeline parallelism).",
                "Apply final normalization and project to logits."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, vocab_size] (logits) and [batch_size, sequence_length, embedding_dimension] (hidden_state)",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "Quant",
                "MODEL_MODE_TRAIN",
                "MODEL_MODE_PREFILL",
                "MODEL_MODE_AUTOREGRESSIVE",
                "nn.Module",
                "nn.compact",
                "jax.Array",
                "functools",
                "flax.linen",
                "flax.nnx",
                "MaxText.common_types",
                "MaxText.layers.pipeline",
                "MaxText.layers.linears",
                "MaxText.layers.attentions",
                "MaxText.layers.normalizations",
                "MaxText.layers.embeddings",
                "MaxText.layers.quantizations",
                "MaxText.maxtext_utils",
                "MaxText.multimodal_utils",
                "MaxText.sharding",
                "MaxText.inference.page_manager",
                "MaxText.max_logging",
                "MaxText.max_utils"
            ],
            "parameters": {
                "config": "Configuration object for the decoder, specifying architecture details, parallelism, etc.",
                "mesh": "JAX mesh for distributed computation.",
                "quant": "Optional quantization configuration.",
                "model_mode": "The current operating mode (e.g., training, prefill, autoregressive)."
            },
            "notes": [
                "The `setup` method initializes the decoder layers, normalization layer, and pipeline module if applicable.",
                "The `get_decoder_layers` method dynamically selects the appropriate decoder layer implementation based on the `config.decoder_block`.",
                "The `get_remat_policy` method configures checkpointing strategies.",
                "The `__call__` method orchestrates the forward pass, handling embeddings, decoder layers (with options for scanning and pipeline parallelism), and the output head.",
                "Supports multimodal inputs via `image_embeddings` and `image_masks`.",
                "Handles different model architectures and configurations through conditional logic.",
                "Returns both logits and the raw hidden state."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the decoder layers, normalization layer, and pipeline module if parallelism is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get decoder layer classes based on config.",
                        "Get normalization layer.",
                        "Initialize pipeline module if `using_pipeline_parallelism` is true."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "get_decoder_layers",
                        "get_norm_layer",
                        "get_pipeline_stage_module",
                        "get_remat_policy"
                    ],
                    "notes": []
                },
                "minimal_policy": {
                    "purpose": "Helper function to create a minimal checkpointing policy by specifying which tensors to save.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define a list of tensor names to save.",
                        "Optionally include 'context' if `with_context` is True.",
                        "Return a JAX checkpoint policy."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jax.checkpoint_policies.save_only_these_names"
                    ],
                    "notes": []
                },
                "get_remat_policy": {
                    "purpose": "Determines and returns the appropriate recomputation (remat) policy based on configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check `config.remat_policy`.",
                        "Select and configure a JAX checkpoint policy based on the policy name (e.g., 'minimal', 'save_dot_with_context_except_mlp', 'qkv_proj_offloaded').",
                        "Handle 'none' and 'full' policies.",
                        "Return the selected policy or None."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "minimal_policy",
                        "jax.checkpoint_policies.save_only_these_names",
                        "jax.checkpoint_policies.save_and_offload_only_these_names",
                        "max_logging"
                    ],
                    "notes": [
                        "Supports various strategies for saving and offloading tensors during recomputation."
                    ]
                },
                "get_decoder_layers": {
                    "purpose": "Retrieves a list of decoder layer classes based on the `decoder_block` configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Use a match statement on `self.config.decoder_block`.",
                        "Return the corresponding list of `nn.Module` classes for different architectures (e.g., DEFAULT, LLAMA2, MISTRAL, DEEPSEEK, GEMMA, etc.).",
                        "Raise a ValueError for unknown decoder block types."
                    ],
                    "output": {
                        "shape": "List of nn.Module classes."
                    },
                    "dependencies": [
                        "DecoderLayer",
                        "llama2.LlamaDecoderLayerToLinen",
                        "mistral.MistralDecoderLayerToLinen",
                        "mixtral.MixtralDecoderLayerToLinen",
                        "deepseek.DeepSeekDenseLayer",
                        "deepseek.DeepSeekMoELayer",
                        "deepseek_batchsplit.DeepSeekDenseLayer",
                        "deepseek_batchsplit.DeepSeekMoELayer",
                        "gemma.GemmaDecoderLayerToLinen",
                        "gemma2.Gemma2DecoderLayerToLinen",
                        "gemma3.Gemma3DecoderLayerToLinen",
                        "gpt3.Gpt3DecoderLayer",
                        "gpt_oss.GptOssScannableBlock",
                        "gpt_oss.GptOssDecoderLayer",
                        "qwen3.Qwen3DecoderLayerToLinen",
                        "qwen3.Qwen3MoeDecoderLayerToLinen",
                        "qwen3.Qwen3NextScannableBlockToLinen",
                        "qwen3.Qwen3NextDecoderLayerToLinen",
                        "simple_layer.SimpleDecoderLayerToLinen",
                        "simple_layer.SimpleMlpDecoderLayerToLinen",
                        "llama4.Llama4ScannableBlockToLinen",
                        "llama4.Llama4DecoderLayerToLinen",
                        "DecoderBlockType"
                    ],
                    "notes": []
                },
                "set_remat_policy": {
                    "purpose": "Applies the specified recomputation (remat) policy to a list of decoder layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through each `block_layer` in the input list.",
                        "If `parameter_memory_host_offload` is enabled, apply a function to move parameters to the device.",
                        "Apply `nn.remat` to the layer with the given policy and prevent_cse setting.",
                        "Append the rematted layer to a new list.",
                        "Return the list of rematted layers."
                    ],
                    "output": {
                        "shape": "List of rematted nn.Module classes."
                    },
                    "dependencies": [
                        "nn.remat",
                        "nn.map_variables",
                        "maxtext_utils.should_prevent_cse_in_remat",
                        "max_utils.device_space",
                        "jax.device_put",
                        "jax.tree_util.tree_map_with_path"
                    ],
                    "notes": []
                },
                "get_norm_layer": {
                    "purpose": "Returns a partial function for the appropriate normalization layer based on the decoder block type.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check `self.config.decoder_block` against a list of known types.",
                        "If it matches, return `functools.partial(rms_norm, ...)`.",
                        "If it's GPT3, return `functools.partial(gpt3.gpt3_layer_norm, ...)`.",
                        "Raise a ValueError for unsupported decoder block types."
                    ],
                    "output": {
                        "shape": "functools.partial object."
                    },
                    "dependencies": [
                        "functools.partial",
                        "rms_norm",
                        "gpt3.gpt3_layer_norm",
                        "DecoderBlockType",
                        "ShardMode"
                    ],
                    "notes": []
                },
                "scan_decoder_layers": {
                    "purpose": "Applies `flax.linen.transforms.scan` to a decoder layer for efficient processing of sequences.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine `params_spec` based on initialization state.",
                        "Define `variable_axes` and `split_rngs` for the scan.",
                        "Create a `scan_fn` using `nn.scan`.",
                        "Return the `scan_fn` configured with layer-specific parameters."
                    ],
                    "output": {
                        "shape": "Callable scan function."
                    },
                    "dependencies": [
                        "nn.scan",
                        "ScanIn",
                        "flax.linen.PARTITION_NAME"
                    ],
                    "notes": []
                },
                "get_pipeline_stage_module": {
                    "purpose": "Constructs the module for a pipeline stage, handling different layer configurations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine the base decoder layer to use for the pipeline stage.",
                        "Optionally apply recomputation policy if `set_remat_policy_on_layers_per_stage` is true.",
                        "Instantiate the stage module: either a single layer, a scanned block, or a sequential block, based on configuration.",
                        "Return the constructed pipeline stage module."
                    ],
                    "output": {
                        "shape": "nn.Module instance."
                    },
                    "dependencies": [
                        "get_layer_to_pipeline",
                        "set_remat_policy",
                        "scan_decoder_layers",
                        "SequentialBlockDecoderLayers",
                        "DecoderBlockType"
                    ],
                    "notes": []
                },
                "_apply_embedding": {
                    "purpose": "Applies token and positional embeddings to the input tokens, with support for multimodal inputs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply shared embedding to input tokens.",
                        "Merge image embeddings if `use_multimodal` is true.",
                        "Apply dropout.",
                        "Cast to the configured dtype.",
                        "Apply positional embeddings (untrainable or trainable).",
                        "Return the embedded output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dimension]"
                    },
                    "dependencies": [
                        "nn.Dropout",
                        "positional_embedding_as_linen",
                        "embed_as_linen",
                        "multimodal_utils.merge_mm_embeddings"
                    ],
                    "notes": []
                },
                "_apply_output_head": {
                    "purpose": "Applies final normalization and projects hidden states to logits.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply final normalization layer.",
                        "Apply dropout.",
                        "Project hidden states to vocabulary size using either a dense layer or the transpose of the embedding matrix.",
                        "Optionally cast logits to float32.",
                        "Return the logits."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "get_norm_layer",
                        "nn.Dropout",
                        "linears.dense_general",
                        "attend_on_embedding",
                        "nn.spmd.LogicallyPartitioned",
                        "jnp.sqrt",
                        "jnp.tanh",
                        "NamedSharding",
                        "ShardMode",
                        "MODEL_MODE_PREFILL",
                        "MODEL_MODE_AUTOREGRESSIVE"
                    ],
                    "notes": [
                        "Handles different sharding strategies for the output.",
                        "Supports using the embedding matrix transpose for logit calculation (`logits_via_embedding`)."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder, processing input tokens through embeddings, decoder layers, and the output head.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply embeddings to input tokens.",
                        "Determine and apply recomputation policy to decoder layers.",
                        "Process through decoder layers, handling pipeline parallelism, scanning, or sequential execution based on configuration.",
                        "Apply the output head to get logits.",
                        "Return logits and hidden state."
                    ],
                    "output": {
                        "shape": "Tuple of (logits, hidden_state)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "_apply_embedding",
                        "get_remat_policy",
                        "set_remat_policy",
                        "scan_decoder_layers",
                        "pipeline.Pipeline",
                        "_apply_output_head",
                        "DecoderBlockType",
                        "MODEL_MODE_TRAIN",
                        "MODEL_MODE_PREFILL",
                        "MODEL_MODE_AUTOREGRESSIVE",
                        "sharding.logical_axis_rules_pp_act_as_dp",
                        "nn.partitioning.axis_rules",
                        "functools.partial",
                        "llama4.determine_is_nope_layer",
                        "llama4.determine_is_moe_layer",
                        "gemma3.get_attention_type",
                        "gpt_oss.get_attention_type"
                    ],
                    "notes": [
                        "Handles complex logic for different parallelism strategies (pipeline, scanning).",
                        "Includes specific logic for architectures like DeepSeek, Gemma3, Llama4, Qwen3_Next, and GPT_OSS.",
                        "Returns hidden states when vocab tiling is enabled during training to save memory."
                    ]
                },
                "_apply_gemma3_scanned_blocks": {
                    "purpose": "Applies Gemma3 scanned decoder blocks, handling both full scanned blocks and any remaining layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the number of full blocks to scan based on `attention_pattern_length`.",
                        "Get and apply recomputation policy to `Gemma3ScannableBlockToLinen`.",
                        "Apply the main scan over the full blocks using `scan_decoder_layers`.",
                        "Apply any remaining layers that do not form a full block.",
                        "Return the processed output."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "scan_decoder_layers",
                        "set_remat_policy",
                        "gemma3.GEMMA3_ATTENTION_PATTERN",
                        "gemma3.Gemma3ScannableBlockToLinen"
                    ],
                    "notes": [
                        "Specifically designed for the Gemma3 architecture's attention pattern."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#self_attention_with_norm",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def self_attention_with_norm(\n    inputs,\n    cfg,\n    mesh,\n    quant,\n    decoder_segment_ids,\n    decoder_positions,\n    deterministic,\n    model_mode,\n    previous_chunk=None,\n    page_state: None | page_manager.PageState = None,\n    slot: None | int = None,\n):\n  \"\"\"self-attention with normalization\"\"\"\n  # Normalization\n  lnx_rms = rms_norm(\n      num_features=inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"pre_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )\n  lnx = lnx_rms(inputs)\n  if model_mode == MODEL_MODE_PREFILL:\n    logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n  else:\n    logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n  attention_layer = attention_mla.mla_as_linen(\n      config=cfg,\n      num_query_heads=cfg.num_query_heads,\n      num_kv_heads=cfg.num_kv_heads,\n      head_dim=cfg.head_dim,\n      max_target_length=cfg.max_target_length,\n      max_prefill_predict_length=cfg.max_prefill_predict_length,\n      attention_kernel=cfg.attention,\n      attention_type=cfg.attention_type,\n      inputs_q_shape=lnx.shape,\n      inputs_kv_shape=lnx.shape,\n      mesh=mesh,\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      dropout_rate=cfg.dropout_rate,\n      name=\"self_attention\",\n      quant=quant,\n      kv_quant=quantizations.configure_kv_quant(cfg),\n      q_lora_rank=cfg.q_lora_rank,\n      kv_lora_rank=cfg.kv_lora_rank,\n      qk_nope_head_dim=cfg.qk_nope_head_dim,\n      qk_rope_head_dim=cfg.qk_rope_head_dim,\n      v_head_dim=cfg.v_head_dim,\n      max_position_embeddings=cfg.max_position_embeddings,\n      original_max_position_embeddings=cfg.original_max_position_embeddings,\n      mscale=cfg.mscale,\n      rope_factor=cfg.rope_factor,\n      model_mode=model_mode,\n  )\n\n  attention_lnx = attention_layer(\n      lnx,\n      lnx,\n      decoder_positions,\n      decoder_segment_ids=decoder_segment_ids,\n      deterministic=deterministic,\n      model_mode=model_mode,\n      previous_chunk=previous_chunk,\n      page_state=page_state,\n      slot=slot,\n  )\n\n  attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n  intermediate_inputs = inputs + attention_lnx\n\n  # Normalization\n  hidden_states = rms_norm(\n      num_features=intermediate_inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"post_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )(intermediate_inputs)\n  hidden_states = nn.with_logical_constraint(hidden_states, logical_axis_names)\n  return hidden_states, intermediate_inputs",
        "analysis": {
            "module_type": "self_attention_with_norm",
            "purpose": "Applies RMS normalization, self-attention, and another RMS normalization to the input tensor.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply pre-layer normalization (rms_norm).",
                "Apply logical constraint to normalized input.",
                "Instantiate a Multi-Head Latent Attention (MLA) layer.",
                "Apply the attention layer to the normalized input.",
                "Apply logical constraint to attention output.",
                "Add attention output to the original input (residual connection).",
                "Apply post-layer normalization (rms_norm).",
                "Apply logical constraint to the final output."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "rms_norm",
                "attention_mla.mla_as_linen",
                "nn.with_logical_constraint"
            ],
            "parameters": {
                "cfg": "Configuration object containing parameters like dtype, normalization epsilon, attention heads, etc.",
                "mesh": "JAX mesh for distributed computation.",
                "quant": "Quantization configuration.",
                "decoder_segment_ids": "Segment IDs for decoder input.",
                "decoder_positions": "Positional embeddings for decoder input.",
                "deterministic": "Boolean flag to control dropout and other stochastic behaviors.",
                "model_mode": "String indicating the current model mode (e.g., 'prefill').",
                "previous_chunk": "Optional previous chunk for attention computation.",
                "page_state": "Optional page state for attention computation.",
                "slot": "Optional slot index for attention computation."
            },
            "notes": [
                "The logical axis names for constraints are determined by the `model_mode`.",
                "Returns both the final hidden states and the intermediate result after the attention layer."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#post_process",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def post_process(cfg, layer_output, sow):\n  \"\"\"postprocessing.\"\"\"\n  if cfg.record_internal_nn_metrics:\n    sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n    sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n    sow(\n        \"intermediates\",\n        \"activation_fraction_zero\",\n        jnp.sum(layer_output == 0) / jnp.size(layer_output),\n    )\n\n  if cfg.scan_layers:\n    return layer_output, None\n  else:\n    return layer_output",
        "analysis": {
            "functionality": "Performs post-processing on a layer's output, optionally recording internal neural network metrics and handling layer scanning.",
            "usage": "Call this function with a configuration object 'cfg', the 'layer_output' tensor, and a 'sow' function. If 'cfg.record_internal_nn_metrics' is true, it records mean, standard deviation, and fraction of zeros for the 'layer_output'. If 'cfg.scan_layers' is true, it returns the 'layer_output' and None; otherwise, it returns only the 'layer_output'."
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekDenseLayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekDenseLayer(nn.Module):\n  \"\"\"DeepSeek-style dense layer with Multi-Head Latent Attention.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n    mlp_lnx = linears.mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        mesh=self.mesh,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "functionality": "Implements a DeepSeek-style dense layer that includes self-attention and a multi-layer perceptron (MLP) block.",
            "usage": "This class is a PyTorch nn.Module that takes input tensors and various configuration parameters. It processes the inputs through self-attention and an MLP, returning the processed output. Key inputs include 'inputs', 'decoder_segment_ids', 'decoder_positions', 'deterministic', and 'model_mode'. The output is the processed tensor after attention and MLP operations."
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekMoELayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekMoELayer(nn.Module):\n  \"\"\"DeepSeek-style MoE layer with Multi-Head Latent Attention.\n  Supports dropless and dropping base on configs.\n  Uses a bias in routing instead of load balancing loss.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        self.config,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx = moe.get_routed_and_shared_moe(\n        name=\"DeepSeekMoeBlock_0\",\n        config=cfg,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "module_type": "deepseek_moe_layer",
            "purpose": "Implements a DeepSeek-style Mixture-of-Experts (MoE) layer with Multi-Head Latent Attention.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply logical constraint to inputs.",
                "Checkpoint input for layer.",
                "Perform self-attention with normalization.",
                "Route and share MoE components.",
                "Apply logical constraint to MoE output.",
                "Add MoE output to intermediate inputs.",
                "Apply dropout.",
                "Apply logical constraint to final output.",
                "Post-process the layer output."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "Quant",
                "nn.Module",
                "nn.with_logical_constraint",
                "checkpoint_name",
                "self_attention_with_norm",
                "moe.get_routed_and_shared_moe",
                "nn.Dropout",
                "post_process",
                "initializers.nd_dense_init"
            ],
            "parameters": {
                "config": "Configuration object for the layer.",
                "mesh": "Mesh object for distributed computation.",
                "model_mode": "String indicating the model mode (e.g., 'prefill' or 'decode').",
                "quant": "Quantization configuration, if any."
            },
            "notes": [
                "Supports dropless and dropping based on configurations.",
                "Uses bias in routing instead of load balancing loss.",
                "The naming mismatch for 'DeepSeekMoeBlock_0' is for reverse compatibility with existing checkpoints."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the DeepSeek MoE layer.",
                    "input": {
                        "shape": "N/A (takes inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode, and optional previous_chunk, page_state, slot)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine logical axis names based on model_mode.",
                        "Apply logical constraint to inputs.",
                        "Checkpoint inputs.",
                        "Call self_attention_with_norm to get hidden_states and intermediate_inputs.",
                        "Call moe.get_routed_and_shared_moe to get mlp_lnx.",
                        "Apply logical constraint to mlp_lnx.",
                        "Calculate layer_output by adding mlp_lnx and intermediate_inputs.",
                        "Apply dropout to layer_output.",
                        "Apply logical constraint to the final layer_output.",
                        "Return the result of post_process."
                    ],
                    "output": {
                        "shape": "N/A (depends on post_process, typically [batch_size, sequence_length, hidden_dim])",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "moe.get_routed_and_shared_moe",
                        "nn.Dropout",
                        "post_process",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "Handles 'prefill' and other model modes differently for logical constraints.",
                        "The `name` parameter in `moe.get_routed_and_shared_moe` is for checkpoint compatibility."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek_batchsplit.py#DeepSeekGenericLayer",
        "file_path": "src/MaxText/layers/deepseek_batchsplit.py",
        "code_block": "class DeepSeekGenericLayer(nn.Module):\n  \"\"\"Generic DeepSeek layer with Multi-Head Latent Attention.\n\n  This is to be used as a base class for DeepSeek layers with dense/sparse MLPs.\n\n  This class follows a pattern of separating module creation from execution.\n  `*_layer()` methods (e.g., `attention_layer`) are factories for `nn.Module`s,\n  called in `setup()` to initialize sub-layers. The module instances are stored\n  in `*_op` attributes (e.g., `self.attention_op`). The corresponding methods\n  (e.g., `attention`) are called during execution in `__call__` and wrap the\n  `*_op` modules with logic like logical constraints. This keeps `__call__`\n  clean and readable.\n  \"\"\"\n\n  config: common_types.Config\n  mesh: jax.sharding.Mesh\n  model_mode: str\n  quant: None | quantizations.AqtQuantization = None\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    x = self.with_logical_constraint(inputs)\n    x = jax.ad_checkpoint.checkpoint_name(x, \"decoder_layer_input\")\n\n    x += self.attention(\n        self.pre_attention_norm(x),\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n\n    x += self.mlp(self.post_attention_norm(x), deterministic)\n    x = self.dropout(x, deterministic)\n    return self.post_process(x)\n\n  def setup(self):\n    self.pre_attention_norm_op = self.rms_norm_layer(\"pre_attention_layer_norm\")\n    self.post_attention_norm_op = self.rms_norm_layer(\n        \"post_attention_layer_norm\"\n    )\n    self.attention_op = self.attention_layer()\n    self.mlp_op = self.mlp_layer()\n    self.dropout_op = self.dropout_layer()\n\n  @property\n  def logical_axis_names(self):\n    if self.model_mode == common_types.MODEL_MODE_PREFILL:\n      return (\n          \"activation_batch\",\n          \"prefill_activation_norm_length\",\n          \"activation_embed\",\n      )\n    else:\n      return (\n          \"activation_batch\",\n          \"activation_norm_length\",\n          \"activation_embed\",\n      )\n\n  def with_logical_constraint(self, x):\n    return nn.with_logical_constraint(x, self.logical_axis_names)\n\n  def rms_norm_layer(self, name):\n    return normalizations.rms_norm(\n        num_features=self.config.base_emb_dim,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        name=name,\n        kernel_axes=(\"norm\",),\n        epsilon=self.config.normalization_layer_epsilon,\n    )\n\n  def pre_attention_norm(self, x):\n    return self.with_logical_constraint(self.pre_attention_norm_op(x))\n\n  def post_attention_norm(self, x):\n    return self.with_logical_constraint(self.post_attention_norm_op(x))\n\n  def attention_layer(self):\n    inputs_shape = (\n        self.config.per_device_batch_size,\n        self.config.max_target_length,\n        self.config.base_emb_dim,\n    )\n    return attention_mla.mla_as_linen(\n        config=self.config,\n        num_query_heads=self.config.num_query_heads,\n        num_kv_heads=self.config.num_kv_heads,\n        head_dim=self.config.head_dim,\n        max_target_length=self.config.max_target_length,\n        max_prefill_predict_length=self.config.max_prefill_predict_length,\n        attention_kernel=self.config.attention,\n        attention_type=self.config.attention_type,\n        inputs_q_shape=inputs_shape,\n        inputs_kv_shape=inputs_shape,\n        mesh=self.mesh,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        dropout_rate=self.config.dropout_rate,\n        name=\"self_attention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(self.config),\n        q_lora_rank=self.config.q_lora_rank,\n        kv_lora_rank=self.config.kv_lora_rank,\n        qk_nope_head_dim=self.config.qk_nope_head_dim,\n        qk_rope_head_dim=self.config.qk_rope_head_dim,\n        v_head_dim=self.config.v_head_dim,\n        max_position_embeddings=self.config.max_position_embeddings,\n        original_max_position_embeddings=self.config.original_max_position_embeddings,\n        mscale=self.config.mscale,\n        rope_factor=self.config.rope_factor,\n        model_mode=self.model_mode,\n    )\n\n  def attention(\n      self,\n      x,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    \"\"\"Executes the attention layer.\"\"\"\n    return self.with_logical_constraint(\n        self.attention_op(\n            x,\n            x,\n            decoder_positions,\n            decoder_segment_ids=decoder_segment_ids,\n            deterministic=deterministic,\n            model_mode=self.model_mode,\n            previous_chunk=previous_chunk,\n            page_state=page_state,\n            slot=slot,\n        )\n    )\n\n  def mlp_layer(self):\n    raise NotImplementedError()\n\n  def mlp(self, x, deterministic):\n    raise NotImplementedError()\n\n  def dropout_layer(self):\n    return nn.Dropout(rate=self.config.dropout_rate, broadcast_dims=(-2,))\n\n  def dropout(self, x, deterministic):\n    return self.with_logical_constraint(\n        self.dropout_op(x, deterministic=deterministic)\n    )\n\n  def post_process(self, x):\n    \"\"\"Collect statistics about the output of the layer.\"\"\"\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(x))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(x))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(x == 0) / jnp.size(x),\n      )\n\n    if self.config.scan_layers:\n      return x, None\n    else:\n      return x",
        "analysis": {
            "module_type": "deepseek_generic_layer",
            "purpose": "Represents a generic DeepSeek layer that serves as a base class for layers with dense or sparse MLPs, incorporating Multi-Head Latent Attention.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies logical constraint to inputs.",
                "Applies checkpointing to the input.",
                "Performs pre-attention normalization.",
                "Applies attention mechanism.",
                "Adds attention output to the input.",
                "Performs post-attention normalization.",
                "Applies MLP (feed-forward network).",
                "Adds MLP output to the intermediate result.",
                "Applies dropout.",
                "Performs post-processing, potentially recording metrics or handling scan layers."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "jax",
                "flax.linen as nn",
                "MaxText.common_types",
                "MaxText.inference.page_manager",
                "MaxText.layers.attention_mla",
                "MaxText.layers.normalizations",
                "MaxText.layers.quantizations"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like embedding dimensions, attention heads, etc.",
                "mesh": "JAX mesh for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'prefill', 'decode').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This class separates module creation (in `setup` and `*_layer` methods) from execution (in `__call__`).",
                "Sub-layers like attention, MLP, and normalization are created as separate modules and then called.",
                "The `mlp_layer` and `mlp` methods are abstract and must be implemented by subclasses.",
                "Supports different logical axis names based on `model_mode` for distributed training.",
                "Includes optional recording of internal neural network metrics."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the DeepSeek generic layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str, previous_chunk: N/A, page_state: N/A, slot: N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint to inputs.",
                        "Checkpoint the input tensor.",
                        "Apply pre-attention normalization.",
                        "Call the attention module.",
                        "Add attention output to the input.",
                        "Apply post-attention normalization.",
                        "Call the MLP module.",
                        "Add MLP output to the intermediate result.",
                        "Apply dropout.",
                        "Call post_process."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "with_logical_constraint",
                        "jax.ad_checkpoint.checkpoint_name",
                        "pre_attention_norm",
                        "attention",
                        "post_attention_norm",
                        "mlp",
                        "dropout",
                        "post_process"
                    ],
                    "notes": [
                        "Handles optional `previous_chunk`, `page_state`, and `slot` for specific inference scenarios.",
                        "The `model_mode` parameter influences the behavior of some internal operations."
                    ]
                },
                "setup": {
                    "purpose": "Initializes the sub-modules (layers) of the DeepSeek generic layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create and store the pre-attention RMS normalization layer.",
                        "Create and store the post-attention RMS normalization layer.",
                        "Create and store the attention layer module.",
                        "Create and store the MLP layer module (abstract, implemented by subclasses).",
                        "Create and store the dropout layer module."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "rms_norm_layer",
                        "attention_layer",
                        "mlp_layer",
                        "dropout_layer"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during module initialization."
                    ]
                },
                "logical_axis_names": {
                    "purpose": "Returns the logical axis names for sharding based on the model mode.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks the `model_mode` attribute.",
                        "Returns a tuple of axis names corresponding to 'prefill' or other modes."
                    ],
                    "output": {
                        "shape": "tuple of strings",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "with_logical_constraint": {
                    "purpose": "Applies a logical constraint to a tensor based on the layer's logical axis names.",
                    "input": {
                        "shape": "x: tensor",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `nn.with_logical_constraint` with the input tensor and `self.logical_axis_names`."
                    ],
                    "output": {
                        "shape": "tensor with logical constraint applied",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint"
                    ],
                    "notes": []
                },
                "rms_norm_layer": {
                    "purpose": "Factory method to create an RMS normalization layer.",
                    "input": {
                        "shape": "name: str",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `normalizations.rms_norm` with configuration parameters."
                    ],
                    "output": {
                        "shape": "nn.Module (RMS norm layer)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "normalizations.rms_norm"
                    ],
                    "notes": [
                        "Uses configuration from `self.config` for normalization parameters."
                    ]
                },
                "pre_attention_norm": {
                    "purpose": "Applies RMS normalization before the attention layer.",
                    "input": {
                        "shape": "x: tensor",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the pre-attention normalization operation (`self.pre_attention_norm_op`).",
                        "Applies a logical constraint to the normalized output."
                    ],
                    "output": {
                        "shape": "tensor",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "with_logical_constraint"
                    ],
                    "notes": []
                },
                "post_attention_norm": {
                    "purpose": "Applies RMS normalization after the attention layer and before the MLP.",
                    "input": {
                        "shape": "x: tensor",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the post-attention normalization operation (`self.post_attention_norm_op`).",
                        "Applies a logical constraint to the normalized output."
                    ],
                    "output": {
                        "shape": "tensor",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "with_logical_constraint"
                    ],
                    "notes": []
                },
                "attention_layer": {
                    "purpose": "Factory method to create the multi-head attention layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines the expected input shape for attention.",
                        "Calls `attention_mla.mla_as_linen` with various configuration parameters."
                    ],
                    "output": {
                        "shape": "nn.Module (attention layer)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "attention_mla.mla_as_linen",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "Configures attention with parameters like number of heads, head dimension, attention kernel, etc.",
                        "Handles quantization and LoRA ranks if configured."
                    ]
                },
                "attention": {
                    "purpose": "Executes the attention layer's forward pass.",
                    "input": {
                        "shape": "x: tensor, decoder_segment_ids: tensor, decoder_positions: tensor, deterministic: bool, previous_chunk: N/A, page_state: N/A, slot: N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the attention operation module (`self.attention_op`) with inputs and attention-specific arguments.",
                        "Applies a logical constraint to the output of the attention operation."
                    ],
                    "output": {
                        "shape": "tensor",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "with_logical_constraint"
                    ],
                    "notes": [
                        "Passes `x` as both query and key/value inputs for self-attention.",
                        "Includes parameters for inference optimizations like `previous_chunk`, `page_state`, and `slot`."
                    ]
                },
                "mlp_layer": {
                    "purpose": "Abstract factory method to create the MLP layer. Must be implemented by subclasses.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Raises NotImplementedError."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "mlp": {
                    "purpose": "Abstract method to execute the MLP layer's forward pass. Must be implemented by subclasses.",
                    "input": {
                        "shape": "x: tensor, deterministic: bool",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Raises NotImplementedError."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "dropout_layer": {
                    "purpose": "Factory method to create a dropout layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Creates an `nn.Dropout` layer with a rate specified in `self.config.dropout_rate` and broadcast dimensions."
                    ],
                    "output": {
                        "shape": "nn.Module (Dropout layer)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nn.Dropout"
                    ],
                    "notes": []
                },
                "dropout": {
                    "purpose": "Applies dropout to the tensor.",
                    "input": {
                        "shape": "x: tensor, deterministic: bool",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the dropout operation (`self.dropout_op`) to the input tensor.",
                        "Applies a logical constraint to the output of the dropout operation."
                    ],
                    "output": {
                        "shape": "tensor",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "with_logical_constraint"
                    ],
                    "notes": []
                },
                "post_process": {
                    "purpose": "Performs post-processing on the layer's output, including optional metric recording and handling of scan layers.",
                    "input": {
                        "shape": "x: tensor",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Conditionally records activation statistics (mean, stdev, fraction zero) if `record_internal_nn_metrics` is true.",
                        "Conditionally returns a tuple `(x, None)` if `scan_layers` is true, otherwise returns `x`."
                    ],
                    "output": {
                        "shape": "tensor or tuple (tensor, None)",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Uses `self.sow` for recording intermediate metrics."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek_batchsplit.py#DeepSeekDenseLayer",
        "file_path": "src/MaxText/layers/deepseek_batchsplit.py",
        "code_block": "class DeepSeekDenseLayer(DeepSeekGenericLayer):\n  \"\"\"DeepSeek layer with dense MLP.\"\"\"\n\n  def mlp_layer(self):\n    return linears.mlp_block(\n        in_features=self.config.base_emb_dim,\n        intermediate_dim=self.config.mlp_dim,\n        activations=self.config.mlp_activations,\n        intermediate_dropout_rate=self.config.dropout_rate,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        name=\"mlp\",\n        config=self.config,\n        quant=self.quant,\n        mesh=self.mesh,\n    )\n\n  def mlp(self, x, deterministic):\n    return self.with_logical_constraint(self.mlp_op(x, deterministic))",
        "analysis": {
            "functionality": "Defines a DeepSeek layer with a dense Multi-Layer Perceptron (MLP) block.",
            "usage": "This class is a specific implementation of `DeepSeekGenericLayer` that configures and uses a dense MLP. It is intended to be used within a larger DeepSeek model architecture. The `mlp_layer` method creates the MLP block using `linears.mlp_block`, and the `mlp` method applies this block to the input tensor `x`."
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek_batchsplit.py#DeepSeekMoELayer",
        "file_path": "src/MaxText/layers/deepseek_batchsplit.py",
        "code_block": "class DeepSeekMoELayer(DeepSeekGenericLayer):\n  \"\"\"DeepSeek MoE layer that uses a batch-split schedule.\"\"\"\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n      split_factor: int = 2,\n  ):\n    x = self.with_logical_constraint(inputs)\n    x = jax.ad_checkpoint.checkpoint_name(x, \"decoder_layer_input\")\n\n    # Helper functions.\n    def _split(x):\n      if x is None:\n        return [None] * split_factor\n      else:\n        return jnp.split(x, split_factor, axis=0)\n\n    def _merge(x):\n      return jnp.concatenate(x, axis=0)\n\n    def _attn(x, decoder_segment_ids, decoder_positions):\n      return self.attention(\n          self.pre_attention_norm(x),\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          previous_chunk,\n          page_state,\n          slot,\n      )\n\n    def _moe(x):\n      return self.mlp(self.post_attention_norm(x), deterministic)\n\n    # Split the inputs into micro-batches.\n    x = _split(x)\n    dpos = _split(decoder_positions)\n    dseg = _split(decoder_segment_ids)\n\n    # Attention.\n    x = [xi + _attn(xi, yi, zi) for xi, yi, zi in zip(x, dseg, dpos)]\n\n    # Mixture-of-experts.\n    x = [xi + _moe(xi) for xi in x]\n\n    # Merge the micro-batches back into a single batch.\n    x = _merge(x)\n\n    x = self.dropout(x, deterministic)\n    return self.post_process(x)\n\n  def init(self, *args, **kwargs):\n    # Calls the parent init method for testing parity.\n    return super().init(*args, **kwargs, method=super().__call__)\n\n  def mlp_layer(self):\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with\n    # existing checkpoints. The `name` represents the weight name in\n    # JAX/checkpoints and so the class name is just for readability.\n    return moe.get_routed_and_shared_moe(\n        name=\"DeepSeekMoeBlock_0\",\n        config=self.config,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(\n            1.0, \"fan_in\", \"truncated_normal\"\n        ),\n        kernel_axes=(\"embed\", None),\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        quant=self.quant,\n    )\n\n  def mlp(self, x, _):\n    return self.with_logical_constraint(self.mlp_op(x))",
        "analysis": {
            "module_type": "deepseek_moe_layer",
            "purpose": "Implements a DeepSeek layer with a Mixture-of-Experts (MoE) component that utilizes a batch-split schedule.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply logical constraint to inputs.",
                "Checkpoint the input tensor.",
                "Split inputs (activations, decoder positions, decoder segment IDs) into micro-batches based on `split_factor`.",
                "Process each micro-batch through attention and MoE MLP.",
                "Add attention output to the input.",
                "Add MoE MLP output to the result.",
                "Merge the processed micro-batches back into a single batch.",
                "Apply dropout.",
                "Perform post-processing (e.g., metric collection)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "DeepSeekGenericLayer",
                "jax",
                "jax.numpy",
                "moe",
                "initializers"
            ],
            "parameters": {
                "split_factor": "The number of ways to split the batch for processing."
            },
            "notes": [
                "This layer inherits from `DeepSeekGenericLayer`.",
                "It customizes the `__call__` method to implement the batch-split MoE logic.",
                "The `mlp_layer` method is overridden to use `moe.get_routed_and_shared_moe`.",
                "The `mlp` method is simplified to not pass the `deterministic` flag to the MoE operation."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the DeepSeek MoE layer using a batch-split schedule.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str, previous_chunk: N/A, page_state: N/A, slot: N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint.",
                        "Checkpoint input.",
                        "Define helper functions `_split`, `_merge`, `_attn`, `_moe`.",
                        "Split inputs `x`, `decoder_positions`, `decoder_segment_ids` by `split_factor`.",
                        "Apply attention to each split micro-batch.",
                        "Apply MoE MLP to each split micro-batch.",
                        "Merge the processed micro-batches.",
                        "Apply dropout.",
                        "Apply post-processing."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self.with_logical_constraint",
                        "jax.ad_checkpoint.checkpoint_name",
                        "jnp.split",
                        "jnp.concatenate",
                        "self.attention",
                        "self.mlp",
                        "self.dropout",
                        "self.post_process"
                    ],
                    "notes": [
                        "The batch is split along axis 0.",
                        "Residual connections are added after attention and MoE MLP.",
                        "The `split_factor` defaults to 2."
                    ]
                },
                "init": {
                    "purpose": "Initializes the layer, calling the parent init method for testing parity.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `super().init` with `method=super().__call__`."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "super().init"
                    ],
                    "notes": [
                        "This method is primarily for testing."
                    ]
                },
                "mlp_layer": {
                    "purpose": "Factory method to create the Mixture-of-Experts (MoE) MLP layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `moe.get_routed_and_shared_moe` with specific configuration."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "moe.get_routed_and_shared_moe",
                        "initializers.nd_dense_init"
                    ],
                    "notes": [
                        "The `name` 'DeepSeekMoeBlock_0' is used for reverse compatibility with checkpoints.",
                        "Configures kernel initialization and axes."
                    ]
                },
                "mlp": {
                    "purpose": "Applies the MoE MLP operation with a logical constraint.",
                    "input": {
                        "shape": "x: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply the MoE operation (`self.mlp_op`).",
                        "Apply logical constraint."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self.with_logical_constraint",
                        "self.mlp_op"
                    ],
                    "notes": [
                        "The `deterministic` flag is ignored in this implementation, indicated by `_`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#_maybe_move_embedding_to_device",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def _maybe_move_embedding_to_device(embedding_table: Array, config: Config) -> Array:\n  \"\"\"Moves embedding table to device if parameter offloading is enabled.\"\"\"\n  if config.parameter_memory_host_offload:\n    max_logging.log(\"embeddings.py: Moving embedding parameter to device\")\n    return jax.device_put(embedding_table, max_utils.device_space())\n  return embedding_table",
        "analysis": {
            "module_type": "embedding_device_transfer",
            "purpose": "Conditionally moves an embedding table to a specified device if parameter offloading is enabled.",
            "input": {
                "shape": "[N/A]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.parameter_memory_host_offload` is True.",
                "If True, log a message indicating the move.",
                "Use `jax.device_put` to move the `embedding_table` to the device specified by `max_utils.device_space()`.",
                "Return the potentially moved `embedding_table`."
            ],
            "output": {
                "shape": "[N/A]"
            },
            "dependencies": [
                "jax",
                "max_logging",
                "max_utils"
            ],
            "parameters": {
                "config.parameter_memory_host_offload": "A boolean flag that determines if parameter offloading is enabled."
            },
            "notes": [
                "The function returns the original `embedding_table` if `config.parameter_memory_host_offload` is False."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#embed_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def embed_as_linen(\n    *,\n    num_embeddings: int,\n    num_features: int,\n    config: Config,\n    mesh: Mesh,\n    cast_input_dtype: None | DType = None,\n    dtype: DType = jnp.float32,\n    attend_dtype: None | DType = None,\n    embedding_init: Initializer = default_embed_init,\n    name: str | None = None,\n):\n  \"\"\"Initializes the Embed NNX module and returns it as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Embed` module within\n  a Linen model. It wraps the `Embed` module using `nnx.bridge.to_linen`,\n  making it compatible with the Linen API.\n\n  Args:\n    num_embeddings: The number of embeddings.\n    num_features: The number of feature dimensions for each embedding.\n    config: The model configuration.\n    cast_input_dtype: The dtype to cast the input to, if any.\n    dtype: The dtype of the embedding vectors.\n    attend_dtype: The dtype for the `attend` method.\n    embedding_init: The initializer for the embedding matrix.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `Embed` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Embed,\n      num_embeddings=num_embeddings,\n      num_features=num_features,\n      config=config,\n      mesh=mesh,\n      cast_input_dtype=cast_input_dtype,\n      dtype=dtype,\n      attend_dtype=attend_dtype,\n      embedding_init=embedding_init,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "embed_as_linen",
            "purpose": "Initializes and returns an NNX-based Embed module as a Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `Embed` module.",
                "Passes `Embed` class and initialization arguments to `to_linen`."
            ],
            "output": {
                "shape": "N/A (returns a Linen module)"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Embed"
            ],
            "parameters": {
                "num_embeddings": "The number of embeddings.",
                "num_features": "The number of feature dimensions for each embedding.",
                "config": "The model configuration.",
                "mesh": "The mesh for distributed computation.",
                "cast_input_dtype": "The dtype to cast the input to, if any.",
                "dtype": "The dtype of the embedding vectors.",
                "attend_dtype": "The dtype for the `attend` method.",
                "embedding_init": "The initializer for the embedding matrix.",
                "name": "The name of the Linen module."
            },
            "notes": [
                "This function acts as a bridge to use the NNX `Embed` module within a Linen model.",
                "It makes the NNX `Embed` module compatible with the Linen API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#Embed",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class Embed(nnx.Module):\n  \"\"\"A parameterized function from integers [0, n) to d-dimensional vectors.\"\"\"\n\n  def __init__(\n      self,\n      num_embeddings: int,\n      num_features: int,\n      config: Config,\n      mesh: Mesh,\n      cast_input_dtype: None | DType = None,\n      dtype: DType = jnp.float32,\n      attend_dtype: None | DType = None,\n      embedding_init: Initializer = default_embed_init,\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the Embed module.\n\n    Args:\n      num_embeddings: The number of embeddings.\n      num_features: The number of feature dimensions for each embedding.\n      config: The model configuration.\n      cast_input_dtype: The dtype to cast the input to, if any.\n      dtype: The dtype of the embedding vectors.\n      attend_dtype: The dtype for the `attend` method.\n      embedding_init: The initializer for the embedding matrix.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.num_embeddings = num_embeddings\n    self.num_features = num_features\n    self.config = config\n    self.mesh = mesh\n    self.cast_input_dtype = cast_input_dtype\n    self.dtype = dtype\n    self.attend_dtype = attend_dtype\n\n    self.embedding = nnx.Param(\n        embedding_init(rngs.params(), (self.num_embeddings, self.num_features), self.config.weight_dtype),\n        sharding=(\"vocab\", \"embed\"),\n    )\n\n  def __call__(self, inputs: Array, model_mode: str = MODEL_MODE_TRAIN) -> Array:\n    \"\"\"Embeds the inputs along the last dimension.\n\n    Args:\n      inputs: input data, all dimensions are considered batch dimensions.\n\n    Returns:\n      Output which is embedded input data.  The output shape follows the input,\n      with an additional `num_features` dimension appended.\n    \"\"\"\n    cfg = self.config\n    if self.cast_input_dtype:\n      inputs = inputs.astype(self.cast_input_dtype)\n    if not jnp.issubdtype(inputs.dtype, jnp.integer):\n      raise ValueError(\"Input type must be an integer or unsigned integer.\")\n\n    embedding = jnp.asarray(\n        _maybe_move_embedding_to_device(self.embedding.value, self.config),\n        self.dtype,\n    )\n\n    output_axis_names = (\n        (\"activation_embed_and_logits_batch\", \"prefill_activation_length\", \"activation_embed\")\n        if model_mode == MODEL_MODE_PREFILL\n        else (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_embed\")\n    )\n    out_pspec = nn.logical_to_mesh_axes(output_axis_names)\n\n    out_sharding = NamedSharding(self.mesh, out_pspec) if self.config.shard_mode == ShardMode.EXPLICIT else None\n\n    if cfg.use_iota_embed:\n      iota = lax.iota(jnp.int32, self.num_embeddings)\n      one_hot = jnp.array(inputs[..., jnp.newaxis] == iota, dtype=self.dtype)\n      output = jnp.dot(one_hot, embedding, out_sharding=out_sharding)\n    else:\n      output = embedding.at[inputs].get(out_sharding=out_sharding)\n\n    return output\n\n  def attend(self, query: Array, out_sharding: NamedSharding | None = None) -> Array:\n    \"\"\"Attend over the embedding using a query array.\n\n    Args:\n      query: array with last dimension equal the feature depth `num_features` of the\n        embedding.\n      out_sharding: NamedSharding object indicating how the output gets sharded\n\n    Returns:\n      An array with final dim `num_embeddings` corresponding to the batched\n      inner-product of the array of query vectors against each embedding.\n      Commonly used for weight-sharing between embeddings and logit transform\n      in NLP models.\n    \"\"\"\n    embedding = self.embedding.value\n    attend_dtype = self.attend_dtype if self.attend_dtype is not None else self.dtype\n    return attend_on_embedding(query, embedding, attend_dtype, self.config, out_sharding)",
        "analysis": {
            "module_type": "embed",
            "purpose": "A parameterized module that maps integer indices to dense embedding vectors.",
            "input": {
                "shape": "[batch_size, ...]",
                "dtype": "integer or unsigned integer"
            },
            "processing_steps": [
                "Optionally cast input dtype.",
                "Validate input dtype.",
                "Retrieve embedding matrix.",
                "Handle different embedding lookup methods (iota_embed or direct indexing).",
                "Apply sharding if configured."
            ],
            "output": {
                "shape": "[batch_size, ..., num_features]"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.nnx",
                "flax.linen",
                "MaxText.common_types",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "num_embeddings": "The total number of unique embeddings (vocabulary size).",
                "num_features": "The dimensionality of each embedding vector.",
                "config": "Model configuration object, used for shard_mode, use_iota_embed, etc.",
                "mesh": "JAX mesh object for distributed computation.",
                "cast_input_dtype": "Optional dtype to cast input indices to.",
                "dtype": "The data type of the output embedding vectors.",
                "attend_dtype": "The data type used in the `attend` method.",
                "embedding_init": "Initializer function for the embedding matrix."
            },
            "notes": [
                "The `__call__` method handles the embedding lookup.",
                "Supports two modes for embedding lookup: `use_iota_embed` (one-hot encoding and dot product) and direct indexing.",
                "The `attend` method performs an attention-like operation using the embedding matrix.",
                "The `rngs` parameter is a placeholder for nnx.bridge compatibility and not directly used."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Embed module with embedding dimensions, configuration, and sharding information.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Initialize the embedding matrix using the provided initializer and RNGs.",
                        "Set sharding for the embedding parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "default_embed_init",
                        "nnx.Rngs",
                        "Mesh",
                        "NamedSharding"
                    ],
                    "notes": [
                        "The `rngs` argument is required by nnx.bridge but not used internally.",
                        "The embedding matrix is sharded based on 'vocab' and 'embed' axes."
                    ]
                },
                "__call__": {
                    "purpose": "Embeds input integer sequences into dense vectors.",
                    "input": {
                        "shape": "[batch_size, ...]",
                        "dtype": "integer or unsigned integer"
                    },
                    "processing_steps": [
                        "Optionally cast input to `cast_input_dtype`.",
                        "Validate that input is an integer type.",
                        "Retrieve and cast the embedding matrix to the module's `dtype`.",
                        "Determine output sharding based on `model_mode` and `config.shard_mode`.",
                        "Perform embedding lookup using either one-hot encoding (`use_iota_embed=True`) or direct indexing.",
                        "Apply output sharding."
                    ],
                    "output": {
                        "shape": "[batch_size, ..., num_features]"
                    },
                    "dependencies": [
                        "jnp.issubdtype",
                        "jnp.asarray",
                        "lax.iota",
                        "jnp.dot",
                        "jnp.array",
                        "NamedSharding"
                    ],
                    "notes": [
                        "Handles different `model_mode` values (e.g., `MODEL_MODE_PREFILL`) for output sharding.",
                        "Raises a ValueError if the input dtype is not an integer type."
                    ]
                },
                "attend": {
                    "purpose": "Performs an attention-like operation using the embedding matrix as keys/values.",
                    "input": {
                        "shape": "[batch_size, ..., num_features]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieve the embedding matrix.",
                        "Determine the dtype for the attention computation.",
                        "Call the `attend_on_embedding` function."
                    ],
                    "output": {
                        "shape": "[batch_size, ..., num_embeddings]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "attend_on_embedding"
                    ],
                    "notes": [
                        "This method is commonly used for weight-sharing between embeddings and logit transformations."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#attend_on_embedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def attend_on_embedding(\n    query: Array,\n    embedding_table: Array,\n    attend_dtype: DType,\n    config: Config,\n    out_sharding: NamedSharding | None = None,\n) -> Array:\n  \"\"\"Attend over an embedding table using a query array.\n\n  TODO: Remove this method when Embed bridge to Linen is no longer needed\n\n  Args:\n    query: An array with a last dimension equal to the feature depth of the embedding.\n    embedding_table: The embedding table to attend over.\n    attend_dtype: The data type for the attention computation.\n    config: The model configuration, used to check for parameter offloading.\n    out_sharding: NamedSharding object indicating the output sharding\n\n  Returns:\n    An array with a final dimension equal to `num_embeddings`, corresponding to the\n    batched inner-product of the query vectors against each embedding.\n  \"\"\"\n  # out_sharding must be None under auto shard_mode\n  if config.shard_mode != ShardMode.EXPLICIT:\n    out_sharding = None\n  embedding_table = _maybe_move_embedding_to_device(embedding_table, config)\n  return jnp.dot(\n      query,\n      jnp.asarray(embedding_table, jnp.bfloat16).T,\n      preferred_element_type=attend_dtype,\n      out_sharding=out_sharding,\n  )",
        "analysis": {
            "functionality": "Performs an attention-like operation by computing the dot product between a query array and an embedding table.",
            "usage": "This function takes a query array, an embedding table, a desired data type for computation, and a configuration object. It computes the batched inner product of the query vectors against each embedding in the table. The output shape will have a final dimension equal to the number of embeddings. It's intended for use within the `Embed` module's `attend` method."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the RotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      RotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "rotary_embedding_initializer",
            "purpose": "Initializes and returns a RotaryEmbedding module wrapped as a Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RotaryEmbedding` class into a Linen module.",
                "Passes initialization arguments to the `RotaryEmbedding` class."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, determining the periodicity of the added signal.",
                "max_timescale": "End of the geometric index, determining the frequency of the added signal.",
                "embedding_dims": "Dimension of the embedding to be generated.",
                "cast_as_fprop_dtype": "Whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The dtype of the output.",
                "name": "Name of the Linen module."
            },
            "notes": [
                "This function acts as a factory or initializer for the RotaryEmbedding module, making it usable within a Flax Linen model structure."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#RotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class RotaryEmbedding(nnx.Module):\n  \"\"\"Rotary Position Embedding.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      # Not used in RotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rope_linear_scaling_factor: float = 1.0,\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the RotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    self.min_timescale = min_timescale\n    self.max_timescale = max_timescale\n    self.embedding_dims = embedding_dims\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.rope_linear_scaling_factor = rope_linear_scaling_factor\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def timescale(self):\n    \"\"\"Returns the timescale for the rotary embedding.\"\"\"\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n    if self.rope_linear_scaling_factor != 1.0:\n      timescale = timescale * self.rope_linear_scaling_factor\n    return timescale\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      inputs: jax.Array,\n      position: None | jax.Array = None,\n  ) -> jax.Array:\n    \"\"\"Generates a jax.Array of sinusoids with different frequencies.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. Since rotary position embeddings are applied to query and\n        keys after projection, it is assumed of shape [B, S, N, H].\n      position: Optional position jax.Array which denotes the position of each\n        token in the sequence. This only needs to be supplied when the sequence\n        is packed. It is of shape [B, S].\n\n    Returns:\n      a jax.Array of shape [B, S, N, H] which includes the inputs together with\n      the rotary position embedding incorporated in it.\n    \"\"\"\n    assert position is not None\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape\" \"[batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding\" \"must match the hidden dimension of the inputs.\"\n      )\n\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n    sin = jnp.sin(sinusoid_inp).astype(inputs.dtype)\n    cos = jnp.cos(sinusoid_inp).astype(inputs.dtype)\n    first_half, second_half = jnp.split(inputs, 2, axis=-1)\n    first_part = first_half * cos - second_half * sin\n    second_part = second_half * cos + first_half * sin\n    if self.cast_as_fprop_dtype:\n      first_part = first_part.astype(self.fprop_dtype)\n      second_part = second_part.astype(self.fprop_dtype)\n    x_out = jnp.concatenate((first_part, second_part), axis=-1)\n    return x_out",
        "analysis": {
            "functionality": "Applies rotary position embeddings to input tensors.",
            "usage": "Instantiate RotaryEmbedding with timescale parameters and embedding dimensions. Then, call the instance with input tensors and their corresponding positions to apply the rotary embedding transformation."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    use_scale: bool = True,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LLaMARotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    use_scale: Whether to apply LLaMA3.1 scaling factor.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LLaMARotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      use_scale=use_scale,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "llama_rotary_embedding_as_linen",
            "purpose": "Initializes and returns a LLaMARotaryEmbedding module wrapped as a Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `LLaMARotaryEmbedding` class into a Linen module.",
                "Passes initialization arguments to the `LLaMARotaryEmbedding` constructor.",
                "Sets `metadata_fn` to `variable_to_logically_partitioned` for metadata handling."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "LLaMARotaryEmbedding",
                "variable_to_logically_partitioned",
                "jnp"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index. Determines the periodicity of the added signal.",
                "max_timescale": "End of the geometric index. Determines the frequency of the added signal.",
                "embedding_dims": "Dimension of the embedding to be generated.",
                "cast_as_fprop_dtype": "Whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The dtype of the output.",
                "use_scale": "Whether to apply LLaMA3.1 scaling factor.",
                "name": "Name of the Linen module."
            },
            "notes": [
                "This function acts as a factory or a bridge to create a Linen-compatible version of the LLaMARotaryEmbedding module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#qwen3_next_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def qwen3_next_rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    partial_rotary_factor: float = 0.25,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the Qwen3NextRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    partial_rotary_factor: Ratio of dimensions to apply ROPE to.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Qwen3NextRotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      partial_rotary_factor=partial_rotary_factor,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "functionality": "This function acts as a factory to create and return a Qwen3NextRotaryEmbedding module, configured to be used within a Flax Linen model.",
            "usage": "Call this function with parameters like min_timescale, max_timescale, embedding_dims, partial_rotary_factor, cast_as_fprop_dtype, and fprop_dtype to initialize and obtain a Linen-compatible Qwen3NextRotaryEmbedding module. The module can then be used in a Flax model to apply rotary position embeddings with Qwen3Next specific configurations."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#Qwen3NextRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class Qwen3NextRotaryEmbedding(RotaryEmbedding):\n  \"\"\"Qwen3 Next variant of ROPE (partial ROPE)\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      partial_rotary_factor: float = 0.25,\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the Qwen3NextRotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      partial_rotary_factor: Ratio of dimensions to apply ROPE to\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    self.head_dim = embedding_dims\n    self.partial_rotary_factor = partial_rotary_factor\n    self.rotary_dim = int(self.head_dim * self.partial_rotary_factor)\n\n    super().__init__(\n        min_timescale=min_timescale,\n        max_timescale=max_timescale,\n        embedding_dims=self.rotary_dim,\n        cast_as_fprop_dtype=cast_as_fprop_dtype,\n        fprop_dtype=fprop_dtype,\n        rngs=rngs,\n    )\n\n  def __call__(self, inputs: jax.Array, position: None | jax.Array = None) -> jax.Array:\n    \"\"\"Applies LLaMA variant of rotary position embedding.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. It is assumed of shape [B, S, H, D].\n      position: Optional position array [B, S]. Only needed when the sequence\n        is packed.\n\n    Returns:\n      A jax.Array of shape [B, S, H, D - rotary_dim] with rotary position embeddings applied.\n    \"\"\"\n    inputs_rot, inputs_pass = jnp.split(inputs, [self.rotary_dim], axis=-1)\n    inputs_rot = super().__call__(inputs_rot, position)\n    inputs = jnp.concatenate([inputs_rot, inputs_pass], axis=-1)\n    return inputs",
        "analysis": {
            "functionality": "Applies a partial rotary position embedding (ROPE) to a portion of the input tensor's last dimension.",
            "usage": "Instantiate `Qwen3NextRotaryEmbedding` with `min_timescale`, `max_timescale`, and `embedding_dims`. Call the instance with an input tensor of shape `[B, S, H, D]` and an optional `position` array. The ROPE is applied to the first `rotary_dim` (calculated as `embedding_dims * partial_rotary_factor`) dimensions, while the remaining dimensions are passed through unchanged. The output tensor has the same shape as the input tensor."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LLaMARotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LLaMARotaryEmbedding(RotaryEmbedding):\n  \"\"\"LLaMA variant of ROPE.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      use_scale: bool = True,\n      # Not used in LLaMARotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the LLaMARotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      use_scale: Whether to apply LLaMA3.1 scaling factor.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    super().__init__(\n        min_timescale=min_timescale,\n        max_timescale=max_timescale,\n        embedding_dims=embedding_dims,\n        cast_as_fprop_dtype=cast_as_fprop_dtype,\n        fprop_dtype=fprop_dtype,\n        rngs=rngs,\n    )\n\n    # LLaMA3.1 ROPE scaling, see the original pytorch implementation:\n    # https://github.com/meta-llama/llama-models/blob/301ca3a2b3b10e94ddcd1fdd2c57e52f812e1cac/models/llama3/reference_impl/model.py#L45C5-L45C18\n    self.use_scale = use_scale\n\n  @property\n  def timescale(self):\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    fraction = jnp.repeat(fraction, 2)\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n\n    # Apply scaling factor if enabled\n    if self.use_scale:\n      timescale = 1.0 / jax.vmap(self._apply_scaling_factor)(1.0 / timescale)\n\n    # Expand timescale dimensions for broadcasting\n    return timescale[jnp.newaxis, jnp.newaxis, jnp.newaxis, :]\n\n  def _apply_scaling_factor(self, freq):\n    \"\"\"apply scaling factor to rotary position embedding.\"\"\"\n    scale_factor = 8\n    low_freq_factor = 1\n    high_freq_factor = 4\n    old_context_len = 8192  # original llama3 length\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n    wavelen = 2 * jnp.pi / freq\n\n    def lower_wavelen(freq):\n      return freq\n\n    def bigger_or_equal_wavelen(freq):\n      def bigger_wavelen(freq):\n        return freq / scale_factor\n\n      def equal_wavelen(freq):\n        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n        return (1 - smooth) * freq / scale_factor + smooth * freq\n\n      bigger_wavelen_cond = wavelen > low_freq_wavelen\n      return jax.lax.cond(bigger_wavelen_cond, bigger_wavelen, equal_wavelen, freq)\n\n    lower_wavelen_cond = wavelen < high_freq_wavelen\n    return jax.lax.cond(lower_wavelen_cond, lower_wavelen, bigger_or_equal_wavelen, freq)\n\n  def __call__(self, inputs: jax.Array, position: None | jax.Array = None) -> jax.Array:\n    \"\"\"Applies LLaMA variant of rotary position embedding.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. It is assumed of shape [B, S, N, H].\n      position: Optional position array [B, S]. Only needed when the sequence\n        is packed.\n\n    Returns:\n      A jax.Array of shape [B, S, N, H] with rotary position embeddings applied.\n    \"\"\"\n    # Ensure input is 4D\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [B, S, N, H].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\"\n      )\n\n    # Shift the inputs left and right as per LLaMA's specific behavior\n    inputs_shifted_left = jnp.concatenate([inputs[..., 1:], inputs[..., :1]], axis=-1)\n    inputs_shifted_right = jnp.concatenate([inputs[..., -1:], inputs[..., :-1]], axis=-1)\n    inputs_shifted = jax.lax.select(\n        jnp.tile(\n            jnp.mod(jnp.arange(self.embedding_dims, dtype=jnp.int32), 2),\n            inputs.shape[:-1] + (1,),\n        ),\n        inputs_shifted_right,\n        inputs_shifted_left,\n    )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.float32)[jnp.newaxis, :]\n\n    # Calculate sinusoidal input\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n\n    sin = jnp.sin(sinusoid_inp)\n    cos = jnp.cos(sinusoid_inp)\n\n    # Apply alternating sign\n    sign = jnp.tile(jnp.array([-1, 1]), self.embedding_dims // 2)\n\n    # Combine original inputs with sinusoidal information\n    outputs = inputs * cos + inputs_shifted * sin * sign\n\n    if self.cast_as_fprop_dtype:\n      outputs = outputs.astype(self.fprop_dtype)\n\n    return outputs",
        "analysis": {
            "functionality": "Applies LLaMA-specific Rotary Position Embeddings (ROPE) to input sequences, incorporating a scaling factor for LLaMA3.1.",
            "usage": "Instantiate LLaMARotaryEmbedding with timescale parameters and embedding dimensions. Call the instance with an input tensor of shape [B, S, N, H] and an optional position array. The module returns the input tensor with ROPE applied, with the output shape matching the input shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#yarn_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def yarn_rotary_embedding_as_linen(\n    *,\n    embedding_dims: int,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    beta_fast: float = 32,\n    beta_slow: float = 1,\n    rope_theta: float = 10000.0,\n    rope_factor: float = 40,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n    interleave: bool = True,\n    truncate: bool = True,\n    attention_scaling: bool = False,\n):\n  \"\"\"Initializes the YarnRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_position_embeddings: The maximum number of positions.\n    original_max_position_embeddings: The original maximum number of positions.\n    beta_fast: The fast beta parameter for YaRN.\n    beta_slow: The slow beta parameter for YaRN.\n    rope_theta: The base for the rotary frequencies.\n    rope_factor: The scaling factor for RoPE.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    name: The name of the module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      YarnRotaryEmbedding,\n      embedding_dims=embedding_dims,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      beta_fast=beta_fast,\n      beta_slow=beta_slow,\n      rope_theta=rope_theta,\n      rope_factor=rope_factor,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      interleave=interleave,\n      truncate=truncate,\n      attention_scaling=attention_scaling,\n  )",
        "analysis": {
            "functionality": "Initializes and returns a YarnRotaryEmbedding module wrapped as a Flax Linen module.",
            "usage": "Call this function with the desired parameters to create a Linen-compatible YarnRotaryEmbedding module. Key parameters include `embedding_dims`, `max_position_embeddings`, and various YaRN-specific parameters like `beta_fast`, `beta_slow`, `rope_theta`, and `rope_factor`. The returned module can then be used within a Flax Linen model."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#YarnRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class YarnRotaryEmbedding(nnx.Module):\n  \"\"\"Yarn rotary embedding.\n\n  Based on https://arxiv.org/abs/2309.00071\n  This implementation uses DeepSeek-v3 PyTorch as reference\n  https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L294\n\n  Attributes:\n    embedding_dims: Dimension of the embedding to be generated.\n    max_position_embeddings: The maximum sequence length that will be encountered.\n    original_max_position_embeddings: The sequence length for which the base frequencies were defined.\n    beta_fast: Lower bound parameter for correction.\n    beta_slow: Upper bound parameter for correction.\n    rope_theta: The base theta value for the frequency computation.\n    rope_factor: Factor applied to adjust the frequencies.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    rope_interleave: Whether complex representation is interleaved or concatenated.\n    rope_truncate: Whether or not to floor lower bound and ceil upper bound for correction range.\n    rope_attention_scaling: Whether or not to scale the rotary embedding output.\n    rngs: rng keys passed in by nnx.bridge.to_linen.\n  \"\"\"\n\n  def __init__(\n      self,\n      embedding_dims: int,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      beta_fast: float = 32,\n      beta_slow: float = 1,\n      rope_theta: float = 10000.0,\n      rope_factor: float = 40,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      interleave=True,\n      truncate=True,\n      attention_scaling=False,\n      # Not used in YarnRotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the YarnRotaryEmbedding module.\"\"\"\n    self.embedding_dims = embedding_dims\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.beta_fast = beta_fast\n    self.beta_slow = beta_slow\n    self.rope_theta = rope_theta\n    self.rope_factor = rope_factor\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.interleave = interleave\n    self.truncate = truncate\n    self.attention_scaling = attention_scaling\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    half_dim = self.embedding_dims // 2\n    # Compute base frequencies for each (even-indexed) dimension.\n    # (Note: We use jnp.arange with float32 for precision.)\n    freqs = 1.0 / (self.rope_theta ** (2.0 * jnp.arange(0, half_dim, dtype=jnp.float32) / self.embedding_dims))\n\n    low, high = self._find_correction_range(\n        self.beta_fast,\n        self.beta_slow,\n        self.embedding_dims,\n        self.rope_theta,\n        self.original_max_position_embeddings,\n        self.truncate,\n    )\n    smooth = 1 - self._linear_ramp_factor(low, high, half_dim)\n    # The corrected frequency is a weighted mix of the scaled and base values.\n    freqs = freqs / self.rope_factor * (1 - smooth) + freqs * smooth\n\n    # Precompute frequencies for all positions by taking the outer product.\n    t = jnp.arange(self.max_position_embeddings, dtype=jnp.float32)  # shape [max_position_embeddings]\n    # This gives a [max_position_embeddings, half_dim] tensor with rows as time steps.\n    freqs = jnp.outer(t, freqs)\n\n    # Compute the complex \u201ccis\u201d values: exp(i * theta).\n    return jnp.exp(1j * freqs)  # shape [max_position_embeddings, half_dim]\n\n  def _find_correction_dim(self, num_rotations: float, dim: int, base: float, max_position_embeddings: int) -> float:\n    \"\"\"Compute the correction dimension for a given number of rotations.\"\"\"\n    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n\n  def _find_correction_range(\n      self, low_rot: float, high_rot: float, dim: int, base: float, max_position_embeddings: int, truncate: bool\n  ):\n    \"\"\"Computes the range of correction dimensions for rotary positional embeddings.\n\n    Args:\n        low_rot (float): Lower bound for the number of rotations.\n        high_rot (float): Upper bound for the number of rotations.\n        dim (int): Dimensionality of the embedding space.\n        base (float): Base value for the exponential computation.\n        max_position_embeddings (int): Maximum sequence length.\n        truncate (bool): Whether to floor lower bound and ceil upper bound.\n\n    Returns:\n        tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n    \"\"\"\n    low = self._find_correction_dim(low_rot, dim, base, max_position_embeddings)\n    high = self._find_correction_dim(high_rot, dim, base, max_position_embeddings)\n    if truncate:\n      low = math.floor(low)\n      high = math.ceil(high)\n    low = max(low, 0)\n    high = min(high, dim - 1)\n    return low, high\n\n  def _linear_ramp_factor(self, min_val: float, max_val: float, dim: int) -> Array:\n    \"\"\"Computes a linear ramp over the dimension.\n\n    Returns a jax.Array of shape (dim,) with values between 0 and 1.\n    \"\"\"\n    if min_val == max_val:\n      max_val += 0.001  # Avoid division by zero.\n    linear_func = (jnp.arange(dim, dtype=jnp.float32) - min_val) / (max_val - min_val)\n    return jnp.clip(linear_func, 0, 1)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies the rotary positional embedding using the precomputed complex frequencies.\n\n    Args:\n      inputs: jax.Array of shape [B, S, N, H]. (H must equal self.embedding_dims.)\n      position: jax.Array of shape [B, S] with integer positions (indexes into precomputed freqs).\n\n    Returns:\n      jax.Array of shape [B, S, N, H] with the rotary embedding applied.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\"\n      )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.int32)[jnp.newaxis, :]\n    else:\n      position = position.astype(jnp.int32)\n\n    # Lookup the precomputed frequencies using the position indices.\n    # self.freqs_cis has shape [max_position_embeddings, half_dim] so we use jnp.take along axis 0.\n    # After indexing, shape becomes [B, S, half_dim]; we then add an axis for the heads.\n    freqs = jnp.take(self.freqs_cis, position, axis=0)  # shape: [B, S, half_dim]\n    freqs = freqs[:, :, jnp.newaxis, :]  # shape: [B, S, 1, half_dim]\n\n    if self.interleave:\n      # Inputs with interleaved format [real1, img1, real2, img2, ...] at last dimension\n      # Convert the last dimension into a complex representation.\n      # First reshape so that each pair of numbers represents the real and imaginary parts.\n      B, S, N, H = inputs.shape\n      half_dim = H // 2\n      inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n      first_half, second_half = inputs_reshaped[..., 0], inputs_reshaped[..., 1]\n    else:\n      # Inputs with concatenated format [real1, real2, ..., img1, img2, ...] at last dimension\n      first_half, second_half = jnp.split(inputs, 2, axis=-1)\n\n    inputs_complex = first_half + 1j * second_half  # shape: [B, S, N, half_dim]\n    # Apply the rotary transformation via complex multiplication.\n    rotated = inputs_complex * freqs  # shape: [B, S, N, half_dim]\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    # [real1, real2, ..., img1, img2, ...]\n    output = jnp.concatenate([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n\n    if self.attention_scaling:\n      attention_scaling = 1.0 if self.rope_factor <= 1 else (0.1 * math.log(self.rope_factor) + 1.0)\n      output = output * attention_scaling\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n    return output",
        "analysis": {
            "module_type": "yarn_rotary_embedding",
            "purpose": "Applies Yarn Rotary Positional Embedding to input tensors.",
            "input": {
                "shape": "[B, S, N, H] where H must equal self.embedding_dims",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine positions if not provided.",
                "Lookup precomputed frequencies using position indices.",
                "Reshape frequencies for broadcasting.",
                "Convert input to complex representation (interleaved or concatenated).",
                "Apply rotary transformation via complex multiplication.",
                "Convert complex result back to a real tensor.",
                "Apply attention scaling if enabled.",
                "Cast output to fprop_dtype if enabled."
            ],
            "output": {
                "shape": "[B, S, N, H]"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "math",
                "nnx"
            ],
            "parameters": {
                "embedding_dims": "Dimension of the embedding to be generated.",
                "max_position_embeddings": "The maximum sequence length that will be encountered.",
                "original_max_position_embeddings": "The sequence length for which the base frequencies were defined.",
                "beta_fast": "Lower bound parameter for correction.",
                "beta_slow": "Upper bound parameter for correction.",
                "rope_theta": "The base theta value for the frequency computation.",
                "rope_factor": "Factor applied to adjust the frequencies.",
                "cast_as_fprop_dtype": "Whether to cast the output to `fprop_dtype`.",
                "fprop_dtype": "The forward pass dtype.",
                "interleave": "Whether complex representation is interleaved or concatenated.",
                "truncate": "Whether or not to floor lower bound and ceil upper bound for correction range.",
                "attention_scaling": "Whether or not to scale the rotary embedding output."
            },
            "notes": [
                "The implementation is based on https://arxiv.org/abs/2309.00071 and uses DeepSeek-v3 PyTorch as reference.",
                "Raises ValueError if embedding_dims is not a multiple of 2.",
                "The `freqs_cis` property precomputes complex frequencies.",
                "Helper methods `_find_correction_dim`, `_find_correction_range`, and `_linear_ramp_factor` are used for frequency calculation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#positional_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def positional_embedding_as_linen(*, embedding_dims: int, max_wavelength: int = _MAX_WAVELENGTH):\n  \"\"\"Initializes the PositionalEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      PositionalEmbedding,\n      embedding_dims=embedding_dims,\n      max_wavelength=max_wavelength,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "Initializes and returns a Flax Linen module for PositionalEmbedding.",
            "usage": "Call this function with `embedding_dims` and optionally `max_wavelength` to get a Linen-compatible PositionalEmbedding module. This module is used to add sinusoidal positional embeddings to input embeddings based on their position."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#PositionalEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class PositionalEmbedding(nnx.Module):\n  \"\"\"A layer that adds sinusoidal positional embeddings to the input.\n\n  Attributes:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  \"\"\"\n\n  embedding_dims: int\n  max_wavelength: int = _MAX_WAVELENGTH\n\n  rngs: nnx.Rngs = None  # Not used in PositionalEmbedding but passed in by nnx.bridge.to_linen\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      input_embedding: jax.Array,\n      position: jax.Array,\n  ) -> jax.Array:\n    num_timescales = self.embedding_dims // 2\n    log_timescale_increment = jnp.log(float(self.max_wavelength)) / jnp.maximum(\n        jnp.asarray(num_timescales, dtype=jnp.float32) - 1, 1\n    )\n    inv_timescales = jnp.exp(jnp.arange(num_timescales, dtype=jnp.float32) * -log_timescale_increment)\n    position = position[:, :, jnp.newaxis]\n    inv_timescales = inv_timescales[jnp.newaxis, jnp.newaxis, :]\n    scaled_time = position * inv_timescales\n    signal = jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)], axis=-1)\n    # signal = jnp.pad(signal, [[0, jnp.mod(self.embedding_dims, 2)]])\n    position_embedding = signal.astype(jnp.float32)\n    return input_embedding + position_embedding",
        "analysis": {
            "module_type": "positional_embedding",
            "purpose": "Adds sinusoidal positional embeddings to the input.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dims]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate number of timescales.",
                "Calculate log of timescale increment.",
                "Calculate inverse timescales.",
                "Reshape position tensor.",
                "Reshape inverse timescales tensor.",
                "Scale position by inverse timescales.",
                "Concatenate sine and cosine signals.",
                "Cast signal to float32.",
                "Add position embedding to input embedding."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dims]"
            },
            "dependencies": [
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "The `rngs` parameter is passed by nnx.bridge.to_linen but not used.",
                "The commented-out line `signal = jnp.pad(signal, [[0, jnp.mod(self.embedding_dims, 2)]])` suggests potential padding logic that is currently disabled."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies sinusoidal positional embeddings to the input embedding.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dims], [batch_size, sequence_length]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate number of timescales.",
                        "Calculate log of timescale increment.",
                        "Calculate inverse timescales.",
                        "Reshape position tensor.",
                        "Reshape inverse timescales tensor.",
                        "Scale position by inverse timescales.",
                        "Concatenate sine and cosine signals.",
                        "Cast signal to float32.",
                        "Add position embedding to input embedding."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dims]"
                    },
                    "dependencies": [
                        "jnp.log",
                        "jnp.maximum",
                        "jnp.asarray",
                        "jnp.exp",
                        "jnp.arange",
                        "jnp.newaxis",
                        "jnp.sin",
                        "jnp.cos",
                        "jnp.concatenate",
                        "astype"
                    ],
                    "notes": [
                        "The `position` input is expected to be a 2D array representing the position of each token.",
                        "The output shape matches the `input_embedding` shape."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_vision_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_vision_rotary_embedding_as_linen(\n    *,\n    image_size: int,\n    patch_size: int,\n    hidden_size: int,\n    num_attention_heads: int,\n    rope_theta: float = 10000.0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LlamaVisionRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    image_size: The size of the input image.\n    patch_size: The size of the image patches.\n    hidden_size: The size of the hidden dimension.\n    num_attention_heads: The number of attention heads.\n    rope_theta: The base theta value for the frequency computation.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: The name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LlamaVisionRotaryEmbedding,\n      image_size=image_size,\n      patch_size=patch_size,\n      hidden_size=hidden_size,\n      num_attention_heads=num_attention_heads,\n      rope_theta=rope_theta,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "functionality": "Initializes and returns a LlamaVisionRotaryEmbedding module as a Linen module.",
            "usage": "Call this function with parameters like image_size, patch_size, hidden_size, and num_attention_heads to create a Linen-compatible rotary embedding module for Llama vision encoders. The module can then be used to apply rotary position embeddings to input tensors."
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LlamaVisionRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LlamaVisionRotaryEmbedding(nnx.Module):\n  \"\"\"Rotary position embedding for Llama4 vision encoder.\n\n  Based on Pytorch Reference\n  https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n  This implementation follows the Llama4 vision encoder's rotary embedding approach,\n  which uses 2D coordinates (x, y) to generate rotary position embeddings.\n\n  Attributes:\n    image_size: int size of the input image\n    patch_size: int size of the image patches\n    hidden_size: int size of the hidden dimension\n    num_attention_heads: int number of attention heads\n    rope_theta: float = 10000.0 base theta value for the frequency computation\n    cast_as_fprop_dtype: bool = True whether to cast the output to the fprop dtype\n    fprop_dtype: DType = jnp.bfloat16 the dtype of the output\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  Returns:\n    jax.Array of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n    where vision rotary position embeddings are applied.\n  \"\"\"\n\n  image_size: int\n  patch_size: int\n  hidden_size: int\n  num_attention_heads: int\n  rope_theta: float = 10000.0\n  cast_as_fprop_dtype: bool = True\n  fprop_dtype: DType = jnp.bfloat16\n  # Not used in LlamaVisionRotaryEmbedding but passed in by nnx.bridge.to_linen.\n  # TODO: Remove when bridge no longer needed\n  rngs: nnx.Rngs = None\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    idx = self.image_size // self.patch_size\n    img_idx = jnp.arange(idx**2, dtype=jnp.int32).reshape(idx**2, 1)\n    img_idx = jnp.concatenate([img_idx, img_idx[:1]], axis=0)\n    img_idx = img_idx.at[-1, -1].set(-2)  # ID_CLS_TOKEN\n\n    # Get 2D coordinates\n    frequencies_x = img_idx % idx  # x coordinates\n    frequencies_y = img_idx // idx  # y coordinates\n\n    # Compute frequency dimensions\n    freq_dim = self.hidden_size // self.num_attention_heads // 2\n    rope_freq = 1.0 / (self.rope_theta ** (jnp.arange(0, freq_dim, 2)[: (freq_dim // 2)].astype(jnp.float32) / freq_dim))\n\n    # Compute frequencies for x and y coordinates\n    freqs_x = (frequencies_x + 1)[..., None] * rope_freq[None, None, :]\n    freqs_y = (frequencies_y + 1)[..., None] * rope_freq[None, None, :]\n\n    # Interleave x and y frequencies\n    freqs_x = jnp.repeat(freqs_x, 2, axis=-1)\n    freqs_y = jnp.repeat(freqs_y, 2, axis=-1)\n\n    # Combine frequencies\n    freqs = jnp.concatenate([freqs_x, freqs_y], axis=-1).astype(jnp.float32)\n    freqs = freqs[..., ::2]\n\n    # Mask out invalid positions\n    freqs = jnp.where(img_idx.reshape(-1, 1, 1) < 0, 0, freqs)\n    # Convert to complex representation\n    return jnp.exp(1j * freqs)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies rotary embeddings to the input tensor for Llama4 vision encoder.\n\n    Args:\n      inputs: Input tensor of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n\n    Returns:\n      Tensor with rotary embeddings applied, maintaining the same shape as input.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\n          \"\"\"Input is assumed to be a rank 4 tensor of shape [batch_size_times_tiles, num_patches_incl_cls, \n          num_heads, head_dim].\"\"\"\n      )\n\n    # Reshape inputs to complex representation\n    B, S, N, H = inputs.shape\n    half_dim = H // 2\n\n    # Convert the last dimension into a complex representation.\n    # First reshape so that each pair of numbers represents the real and imaginary parts.\n    inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n    inputs_complex = inputs_reshaped[..., 0] + 1j * inputs_reshaped[..., 1]\n\n    # Reshape freqs_ci for broadcasting\n    freqs_ci = self.freqs_cis[jnp.newaxis, :, :, :]\n\n    # Apply rotary transformation\n    rotated = inputs_complex * freqs_ci\n\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    rotated_real = jnp.stack([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n    output = rotated_real.reshape(B, S, N, H)\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n\n    return output",
        "analysis": {
            "module_type": "llama_vision_rotary_embedding",
            "purpose": "Applies rotary position embeddings to input tensors for a Llama vision encoder, utilizing 2D coordinates.",
            "input": {
                "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Reshape input to complex representation.",
                "Calculate frequencies based on image and patch size.",
                "Apply rotary transformation using complex multiplication.",
                "Convert complex result back to real tensor.",
                "Optionally cast output to specified dtype."
            ],
            "output": {
                "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]"
            },
            "dependencies": [
                "jax",
                "jax.numpy as jnp",
                "flax.nnx as nnx"
            ],
            "parameters": {
                "image_size": "Size of the input image.",
                "patch_size": "Size of the image patches.",
                "hidden_size": "Size of the hidden dimension.",
                "num_attention_heads": "Number of attention heads.",
                "rope_theta": "Base theta value for frequency computation.",
                "cast_as_fprop_dtype": "Whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The dtype of the output."
            },
            "notes": [
                "The module calculates frequencies based on 2D coordinates derived from image and patch sizes.",
                "It handles a special token ID_CLS_TOKEN.",
                "The input tensor is expected to be rank 4."
            ],
            "methods": {
                "freqs_cis": {
                    "purpose": "Calculates and returns the complex frequencies used for rotary embedding.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine grid index from image and patch size.",
                        "Generate 2D coordinates (x, y).",
                        "Compute base frequencies.",
                        "Calculate frequencies for x and y coordinates.",
                        "Interleave and combine frequencies.",
                        "Mask out invalid positions.",
                        "Convert to complex representation."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jax.numpy as jnp"
                    ],
                    "notes": [
                        "Handles a special class token ID (-2)."
                    ]
                },
                "__call__": {
                    "purpose": "Applies rotary embeddings to the input tensor.",
                    "input": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate input tensor rank.",
                        "Reshape input to complex representation.",
                        "Retrieve pre-calculated frequencies.",
                        "Apply rotary transformation by complex multiplication.",
                        "Convert complex result back to real tensor.",
                        "Optionally cast output to the specified fprop_dtype."
                    ],
                    "output": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "jax.numpy as jnp"
                    ],
                    "notes": [
                        "The input tensor must be rank 4.",
                        "The last dimension of the input is split into real and imaginary parts."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/encoders.py#VisionEncoder",
        "file_path": "src/MaxText/layers/encoders.py",
        "code_block": "class VisionEncoder(nn.Module):\n  \"\"\"Vision encoder to encode images into soft tokens.\"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    self.vision_encoder_layer = self.get_vision_encoder_layers()\n\n  def get_vision_encoder_layers(self):\n    \"\"\"Get vision encoder layers specific to the model, classes of nn.Module type.\"\"\"\n    if self.config.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\"]:\n      from MaxText.layers import gemma3  # pylint: disable=import-outside-toplevel\n\n      return [gemma3.gemma3visionencoder_as_linen, gemma3.visionembedder_as_linen]\n    elif self.config.model_name in [\"llama4-17b-16e\", \"llama4-17b-128e\"]:\n      from MaxText.layers import llama4  # pylint: disable=import-outside-toplevel\n\n      return [llama4.llama4visionmodel_as_linen, llama4.llama4multimodalprojector_as_linen]\n    else:\n      raise ValueError(f\"No VisionEncoder implemented for {self.config.model_name} yet\")\n\n  @nn.compact\n  def __call__(self, input_images, deterministic=False):\n    cfg = self.config\n    mesh = self.mesh\n    # vision encoder output, frozen params in many cases\n    embeddings = self.vision_encoder_layer[0](config=cfg, mesh=mesh)(input_images, deterministic=deterministic)\n    if cfg.freeze_vision_encoder_params:\n      embeddings = jax.lax.stop_gradient(embeddings)\n\n    if len(self.vision_encoder_layer) > 1:\n      # vision embedder / projection layer, not frozen in most cases, trained / finetuned together with main model\n      embeddings = self.vision_encoder_layer[1](config=cfg, mesh=mesh)(embeddings)\n    return embeddings",
        "analysis": {
            "module_type": "vision_encoder",
            "purpose": "Encodes input images into soft token embeddings using model-specific vision encoder layers.",
            "input": {
                "shape": "[batch_size, height, width, channels]",
                "dtype": "float32 (assumed)"
            },
            "processing_steps": [
                "Selects appropriate vision encoder layers based on the model configuration.",
                "Passes input images through the first vision encoder layer to get initial embeddings.",
                "Optionally stops gradient flow if `freeze_vision_encoder_params` is true.",
                "If a second vision embedder/projector layer exists, passes the embeddings through it."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "float32 (assumed)"
            },
            "dependencies": [
                "flax.linen as nn",
                "jax",
                "jax.sharding.Mesh",
                "MaxText.common_types.Config",
                "MaxText.layers.gemma3",
                "MaxText.layers.llama4"
            ],
            "parameters": {
                "config": "Configuration object containing model details like `model_name` and `freeze_vision_encoder_params`.",
                "mesh": "JAX Mesh object for distributed computation."
            },
            "notes": [
                "The specific vision encoder layers used depend on the `config.model_name`.",
                "Supports 'gemma3-4b', 'gemma3-12b', 'gemma3-27b', 'llama4-17b-16e', and 'llama4-17b-128e' models.",
                "The vision encoder parameters can be frozen during training.",
                "The output embeddings can be further processed by a vision embedder/projector if available."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the vision encoder layers based on the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `get_vision_encoder_layers()` to obtain the necessary layers and stores them in `self.vision_encoder_layer`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.get_vision_encoder_layers"
                    ],
                    "notes": []
                },
                "get_vision_encoder_layers": {
                    "purpose": "Retrieves the appropriate vision encoder and embedder layers based on the model name.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks `self.config.model_name`.",
                        "Imports and returns specific layer functions from `MaxText.layers.gemma3` or `MaxText.layers.llama4`.",
                        "Raises a ValueError if the model name is not supported."
                    ],
                    "output": {
                        "shape": "List of layer functions (nn.Module types)."
                    },
                    "dependencies": [
                        "MaxText.layers.gemma3",
                        "MaxText.layers.llama4"
                    ],
                    "notes": [
                        "Uses `import-outside-toplevel` for dynamic imports."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the vision encoder to generate image embeddings.",
                    "input": {
                        "shape": "[batch_size, height, width, channels]",
                        "dtype": "float32 (assumed)"
                    },
                    "processing_steps": [
                        "Retrieves `config` and `mesh` from `self`.",
                        "Applies the first vision encoder layer (`self.vision_encoder_layer[0]`) to `input_images`.",
                        "Optionally applies `jax.lax.stop_gradient` if `cfg.freeze_vision_encoder_params` is true.",
                        "If a second layer exists (`self.vision_encoder_layer[1]`), applies it to the intermediate embeddings.",
                        "Returns the final embeddings."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]",
                        "dtype": "float32 (assumed)"
                    },
                    "dependencies": [
                        "jax.lax.stop_gradient",
                        "self.vision_encoder_layer"
                    ],
                    "notes": [
                        "The `deterministic` argument is passed to the first encoder layer.",
                        "The second layer (embedder/projector) is typically trained/finetuned."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma.py#GemmaDecoderLayer",
        "file_path": "src/MaxText/layers/gemma.py",
        "code_block": "class GemmaDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: Optional[Quant] = None,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=self.mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    self.pre_ffw_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp = MlpBlock(\n        config=config,\n        mesh=self.mesh,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_manager=None,\n      page_state=None,\n      slot=None,\n  ):\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = self.pre_self_attention_norm(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm(attention_lnx)\n\n    mlp_lnx = self.mlp(attn_output, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        self.activation_axis_names,\n    )\n\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma_decoder_layer",
            "purpose": "Represents a single decoder layer in the Gemma Transformer model, responsible for self-attention and feed-forward processing.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies Layer Normalization (RMSNorm) before self-attention.",
                "Performs self-attention using the Attention module.",
                "Adds the output of self-attention to the original inputs (residual connection).",
                "Applies Layer Normalization (RMSNorm) before the feed-forward network.",
                "Processes the output through a Multi-Layer Perceptron (MLPBlock).",
                "Adds the output of the MLP to the residual connection from the attention step.",
                "Applies dropout.",
                "Optionally records internal metrics if configured.",
                "Returns the final output of the layer."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "nnx.Module",
                "nnx.Rngs",
                "jax.sharding.Mesh",
                "jax.numpy as jnp",
                "jax.ad_checkpoint.checkpoint_name",
                "MaxText.max_utils.get_batch_seq_len_for_mode"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions, attention parameters, dropout rates, etc.",
                "mesh": "JAX Mesh object for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'train', 'inference').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The layer uses residual connections and layer normalization.",
                "It supports different attention mechanisms based on the config.",
                "The `__call__` method accepts various arguments for controlling attention, caching, and determinism.",
                "Logical constraints are applied to intermediate tensors.",
                "Internal metrics can be recorded if `config.record_internal_nn_metrics` is True.",
                "If `config.scan_layers` is True, it returns an additional None value."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the GemmaDecoderLayer with configuration, mesh, and other necessary parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, model_mode, and quantization settings.",
                        "Determines dummy input shapes based on config and model_mode.",
                        "Initializes RMSNorm layers for pre-attention and pre-feedforward.",
                        "Initializes the Attention module.",
                        "Initializes the MlpBlock module.",
                        "Initializes the Dropout layer.",
                        "Sets activation axis names for logical constraints."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "nnx.Module",
                        "nnx.Rngs",
                        "MaxText.max_utils.get_batch_seq_len_for_mode",
                        "MaxText.layers.quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "Requires `rngs` for parameter initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the decoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dimension] for inputs, plus decoder_segment_ids, decoder_positions, deterministic, model_mode, and optional caching arguments.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints and checkpointing to inputs.",
                        "Applies pre-self-attention normalization.",
                        "Performs self-attention.",
                        "Applies residual connection after self-attention.",
                        "Applies pre-feedforward normalization.",
                        "Processes through the MLP block.",
                        "Applies residual connection after MLP.",
                        "Applies dropout.",
                        "Applies logical constraints to the output.",
                        "Optionally records internal metrics.",
                        "Returns the layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dimension] or ([batch_size, sequence_length, embedding_dimension], None) if config.scan_layers is True.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "Handles optional arguments for KV caching (`previous_chunk`, `page_manager`, `page_state`, `slot`).",
                        "The `deterministic` flag controls dropout behavior.",
                        "The `model_mode` is passed to the attention module."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma2.py#Gemma2DecoderLayer",
        "file_path": "src/MaxText/layers/gemma2.py",
        "code_block": "class Gemma2DecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: Optional[Quant] = None,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_norm_local = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.self_attention_local = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=self.mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        attention_type=attentions.AttentionType.LOCAL_SLIDING,\n        sliding_window_size=config.sliding_window_size,\n        attn_logits_soft_cap=config.attn_logits_soft_cap,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_attn_norm:\n      self.post_self_attention_norm_local = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    self.pre_ffw_norm_local = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp_local = MlpBlock(\n        config=config,\n        mesh=self.mesh,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_ffw_norm:\n      self.post_ffw_norm_local = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n\n    self.pre_self_attention_norm_global = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.self_attention_global = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=self.mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=True,\n        float32_logits=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        attention_type=attentions.AttentionType.GLOBAL,\n        attn_logits_soft_cap=config.attn_logits_soft_cap,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_attn_norm:\n      self.post_self_attention_norm_global = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    self.pre_ffw_norm_global = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp_global = MlpBlock(\n        config=config,\n        mesh=self.mesh,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n    if config.use_post_ffw_norm:\n      self.post_ffw_norm_global = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = self.pre_self_attention_norm_local(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention_local(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if self.config.use_post_attn_norm:\n      attention_lnx = self.post_self_attention_norm_local(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm_local(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = self.mlp_local(attn_output, deterministic=deterministic)\n\n    if self.config.use_post_ffw_norm:\n      mlp_lnx = self.post_ffw_norm_local(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    ### global part\n    inputs = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = self.pre_self_attention_norm_global(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention_global(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if self.config.use_post_attn_norm:\n      attention_lnx = self.post_self_attention_norm_global(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm_global(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = self.mlp_global(attn_output, deterministic=deterministic)\n    if self.config.use_post_ffw_norm:\n      mlp_lnx = self.post_ffw_norm_global(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma2_decoder_layer",
            "purpose": "Represents a single decoder layer in the Gemma2 Transformer model, incorporating both local and global attention mechanisms.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies local attention mechanism.",
                "Applies feed-forward network (MLP) for local attention output.",
                "Applies global attention mechanism.",
                "Applies feed-forward network (MLP) for global attention output.",
                "Applies dropout.",
                "Optionally records internal metrics."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "max_utils.get_batch_seq_len_for_mode",
                "attentions.AttentionType.LOCAL_SLIDING",
                "attentions.AttentionType.GLOBAL",
                "nn.with_logical_constraint",
                "checkpoint_name"
            ],
            "parameters": {
                "config": "Configuration object for the model, containing parameters like embedding dimension, number of heads, etc.",
                "mesh": "Mesh object for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'prefill', 'decode').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The layer processes inputs through two main sub-layers: one for local attention and one for global attention.",
                "Residual connections are used after both local and global attention and MLP blocks.",
                "Conditional normalization layers (post_attn_norm, post_ffw_norm) are applied based on the config.",
                "Handles different axis names for activation constraints based on the model mode (prefill vs. other).",
                "Can optionally return an additional None value if `scan_layers` is True in the config."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gemma2DecoderLayer with configuration, mesh, model mode, and optional quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes configuration, mesh, model_mode, and quantization attributes.",
                        "Determines dummy input shapes based on config and model_mode.",
                        "Initializes RMSNorm layers for pre-attention and pre-feedforward normalization (local and global).",
                        "Initializes Attention layers for local and global attention.",
                        "Initializes MlpBlock layers for local and global feed-forward processing.",
                        "Initializes Dropout layer.",
                        "Sets activation axis names based on model_mode."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode",
                        "quantizations.configure_kv_quant",
                        "attentions.AttentionType"
                    ],
                    "notes": [
                        "Conditional initialization of post-attention and post-feedforward normalization layers based on `config.use_post_attn_norm` and `config.use_post_ffw_norm`."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the decoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim] for inputs, plus decoder_segment_ids, decoder_positions, deterministic flag, model_mode, and optional previous_chunk, page_state, slot.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints and checkpointing to inputs.",
                        "Applies local attention normalization, attention, and residual connection.",
                        "Applies local MLP normalization, MLP, and residual connection.",
                        "Applies dropout.",
                        "Applies global attention normalization, attention, and residual connection.",
                        "Applies global MLP normalization, MLP, and residual connection.",
                        "Applies dropout.",
                        "Optionally records internal metrics.",
                        "Returns the final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None) if scan_layers is True.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "RMSNorm",
                        "Attention",
                        "Dropout",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior.",
                        "The `model_mode` parameter influences the attention mechanisms.",
                        "The `previous_chunk`, `page_state`, and `slot` parameters are passed to the attention layers but their specific usage is not detailed within this method's scope."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_attention_type",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_attention_type(layer_id):\n  layer_id %= len(GEMMA3_ATTENTION_PATTERN)\n  return GEMMA3_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "functionality": "Determines the attention type for a given layer ID based on a predefined pattern.",
            "usage": "Call the function with an integer layer_id to get the corresponding AttentionType from GEMMA3_ATTENTION_PATTERN. The layer_id is taken modulo the length of the pattern to cycle through the available attention types."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_query_pre_attn_scalar",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_query_pre_attn_scalar(config) -> float:\n  \"\"\"Returns the scalar to multiply the query by before attention.\"\"\"\n  if config.model_name in [\"gemma3-4b\", \"gemma3-12b\"]:\n    return config.head_dim**-0.5\n  elif config.model_name == \"gemma3-27b\":\n    return (config.base_emb_dim // config.base_num_query_heads) ** -0.5\n  else:\n    raise ValueError(f\"Unsupported model name: {config.model_name}\")",
        "analysis": {
            "functionality": "Calculates a scalar value used to scale query vectors before the attention mechanism in Gemma 3 models.",
            "usage": "Call this function with a configuration object. It returns a float representing the pre-attention scalar. The scalar depends on the 'model_name' attribute of the config object, using either 'head_dim' or a combination of 'base_emb_dim' and 'base_num_query_heads'."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3DecoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3DecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer for Gemma3.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      attention_type: AttentionType = AttentionType.LOCAL_SLIDING,\n  ):\n    \"\"\"Initializes the Gemma3DecoderLayer.\n\n    Args:\n      config: The Config object with model hyperparameters.\n      mesh: The device mesh for distributed training.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: The random number generators for initialization.\n      quant: The quantization configuration.\n      attention_type: The type of attention to use.\n    \"\"\"\n\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.rngs = rngs\n    self.attention_type = attention_type\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    query_pre_attn_scalar = get_query_pre_attn_scalar(config)\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        attention_type=self.attention_type,\n        sliding_window_size=config.sliding_window_size,\n        attn_logits_soft_cap=config.attn_logits_soft_cap,\n        use_qk_norm=True,  # Gemma 3 models use query, key normalizations\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    if self.config.use_post_attn_norm:\n      self.post_self_attention_norm = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.post_self_attention_norm = None\n\n    self.pre_ffw_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        quant=self.quant,\n        model_mode=model_mode,\n        mesh=mesh,\n        rngs=self.rngs,\n    )\n\n    if self.config.use_post_ffw_norm:\n      self.post_ffw_norm = RMSNorm(\n          num_features=config.emb_dim,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.post_ffw_norm = None\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n      bidirectional_mask=None,\n  ):\n    cfg = self.config\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    lnx = self.pre_self_attention_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    # Self-attention block\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        bidirectional_mask=bidirectional_mask,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = self.post_self_attention_norm(attention_lnx)\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = self.pre_ffw_norm(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = self.mlp(attn_output, deterministic=deterministic)\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = self.post_ffw_norm(mlp_lnx)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n    next_layer_addition_dropped_out = self.dropout(next_layer_addition, deterministic=deterministic)\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma3_decoder_layer",
            "purpose": "Represents a single decoder layer in the Gemma3 Transformer model.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies pre-self-attention normalization.",
                "Performs self-attention.",
                "Applies post-self-attention normalization (if enabled).",
                "Adds the residual connection from the input.",
                "Applies pre-feed-forward normalization.",
                "Processes the output through an MLP block.",
                "Applies post-feed-forward normalization (if enabled).",
                "Adds the residual connection from the self-attention output.",
                "Applies dropout."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "nnx.Rngs",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "max_utils.get_batch_seq_len_for_mode",
                "get_query_pre_attn_scalar",
                "quantizations.configure_kv_quant"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters.",
                "mesh": "Device mesh for distributed training.",
                "model_mode": "Specifies the operational mode (train, prefill, autoregressive).",
                "rngs": "Random number generators for parameter initialization.",
                "quant": "Optional quantization configuration.",
                "attention_type": "Type of attention mechanism to use."
            },
            "notes": [
                "The layer includes optional post-attention and post-feed-forward normalization layers based on configuration.",
                "Supports different model modes which affect input shape calculations.",
                "Can record internal metrics if configured.",
                "Handles conditional return based on `scan_layers` configuration."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gemma3DecoderLayer with configuration and components.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration and other initialization parameters.",
                        "Determines dummy input shapes based on model mode.",
                        "Initializes RMSNorm layers for pre-attention and pre-feed-forward.",
                        "Initializes the Attention module.",
                        "Initializes post-attention and post-feed-forward normalization layers conditionally.",
                        "Initializes the MLP block.",
                        "Initializes the Dropout layer.",
                        "Sets activation axis names based on model mode."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode",
                        "get_query_pre_attn_scalar",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "The `attention_type` parameter defaults to `AttentionType.LOCAL_SLIDING`."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the decoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim] for inputs, plus other control/positional arguments.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints and checkpointing to inputs.",
                        "Applies pre-self-attention normalization.",
                        "Passes normalized inputs through the self-attention module.",
                        "Applies post-self-attention normalization if enabled.",
                        "Adds the self-attention output to the original inputs (residual connection).",
                        "Applies pre-feed-forward normalization to the result.",
                        "Passes the normalized output through the MLP block.",
                        "Applies post-feed-forward normalization if enabled.",
                        "Adds the MLP output to the residual connection from the attention block.",
                        "Applies dropout.",
                        "Applies logical constraints to the final output.",
                        "Optionally sows internal metrics.",
                        "Returns the layer output, potentially with an auxiliary value if `scan_layers` is true."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim] or a tuple if scan_layers is true."
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "self.pre_self_attention_norm",
                        "self.self_attention",
                        "self.post_self_attention_norm",
                        "self.pre_ffw_norm",
                        "self.mlp",
                        "self.post_ffw_norm",
                        "self.dropout",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior.",
                        "Various optional arguments like `previous_chunk`, `page_state`, `slot`, and `bidirectional_mask` are passed to the attention module.",
                        "The output shape can be a tuple `(output, None)` if `cfg.scan_layers` is true."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3ScannableBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3ScannableBlock(nnx.Module):\n  \"\"\"A repeatable block of Gemma3 decoder layers.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      num_of_layers: int = 1,\n  ):\n    \"\"\"Initializes the Gemma3ScannableBlock.\n\n    Args:\n      config: The Config object with model hyperparameters.\n      mesh: The device mesh for distributed training.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: The random number generators for initialization.\n      quant: The quantization configuration.\n      num_of_layers: The number of layers in the model.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n    self.num_of_layers = num_of_layers\n\n    for layer_id in range(self.num_of_layers):\n      attention_type = get_attention_type(layer_id)\n      layer_name = f\"layers_{layer_id}\"\n      layer = Gemma3DecoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          model_mode=self.model_mode,\n          rngs=self.rngs,\n          quant=self.quant,\n          attention_type=attention_type,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot=None,\n      page_state=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n  ):\n\n    cfg = self.config\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n\n    for layer_id in range(self.num_of_layers):\n      y = getattr(self, f\"layers_{layer_id}\")(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          bidirectional_mask=bidirectional_mask,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gemma3_scannable_block",
            "purpose": "A repeatable block of Gemma3 decoder layers that can be scanned.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize multiple Gemma3DecoderLayer instances.",
                "Iterate through the initialized layers, applying each to the input.",
                "Optionally extract the first element if scan_layers is True.",
                "Return the final output, potentially as a tuple if scan_layers is True."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "nnx.Module",
                "Gemma3DecoderLayer",
                "get_attention_type"
            ],
            "parameters": {
                "config": "The Config object with model hyperparameters.",
                "mesh": "The device mesh for distributed training.",
                "model_mode": "One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.",
                "rngs": "The random number generators for initialization.",
                "quant": "The quantization configuration.",
                "num_of_layers": "The number of layers in the model."
            },
            "notes": [
                "The `scan_layers` attribute in the config determines if the output is a tuple and if intermediate results are processed differently.",
                "Layers are dynamically created and assigned as attributes based on `num_of_layers`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gemma3ScannableBlock by creating and setting up multiple Gemma3DecoderLayer instances.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration, mesh, model mode, RNGs, quantization, and number of layers.",
                        "Loop from 0 to num_of_layers - 1.",
                        "Determine attention type for each layer.",
                        "Instantiate Gemma3DecoderLayer with provided configurations.",
                        "Set the instantiated layer as an attribute of the Gemma3ScannableBlock instance (e.g., self.layers_0)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "Quant",
                        "Gemma3DecoderLayer",
                        "get_attention_type"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Processes input through a sequence of Gemma3DecoderLayer instances.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint and checkpoint name to inputs.",
                        "Initialize output `y` with inputs.",
                        "Loop through each decoder layer (from layer_id 0 to num_of_layers - 1).",
                        "Apply the current decoder layer to `y` and other arguments.",
                        "If `cfg.scan_layers` is True, update `y` to be the first element of the layer's output.",
                        "If `cfg.scan_layers` is True, return `y` and None.",
                        "Otherwise, return `y`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.with_logical_constraint",
                        "checkpoint_name",
                        "getattr"
                    ],
                    "notes": [
                        "The behavior of the output depends on the `cfg.scan_layers` configuration.",
                        "Passes various arguments like `decoder_segment_ids`, `decoder_positions`, `deterministic`, `model_mode`, `slot`, `page_state`, `previous_chunk`, and `bidirectional_mask` to each decoder layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#_posemb_sincos_2d",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def _posemb_sincos_2d(\n    h: int,\n    w: int,\n    *,\n    width: int,\n    temperature: float = 10_000.0,\n    precision: str = \"default\",\n    dtype: jnp.dtype = jnp.float32,\n):\n  \"\"\"Follows the MoCo v3 logic.\"\"\"\n  y, x = jnp.mgrid[:h, :w]  # pylint: disable=unpacking-non-sequence\n\n  assert width % 4 == 0, \"Width must be mult of 4 for sincos posemb\"\n  omega = jnp.arange(width // 4) / (width // 4 - 1)\n  omega = 1.0 / (temperature**omega)\n  y = jnp.einsum(\"m,d->md\", y.flatten(), omega, precision=precision)\n  x = jnp.einsum(\"m,d->md\", x.flatten(), omega, precision=precision)\n  pe = jnp.concatenate([jnp.sin(x), jnp.cos(x), jnp.sin(y), jnp.cos(y)], axis=1)\n  return jnp.asarray(pe, dtype)[None, :, :]",
        "analysis": {
            "functionality": "Generates 2D sinusoidal positional embeddings.",
            "usage": "This function takes height (h), width (w), and the embedding width (width) as input, along with optional temperature, precision, and data type. It returns a JAX array containing the 2D sinusoidal positional embeddings, shaped as [1, h*w, width]."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#MlpBlockViT",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class MlpBlockViT(nnx.Module):\n  \"\"\"NNX version of Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.block_id = block_id\n    self.rngs = rngs\n\n    self.Dense_0 = DenseGeneral(\n        in_features_shape=self.config.hidden_size_for_vit,\n        out_features_shape=self.config.intermediate_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(rate=self.config.dropout_rate)\n    self.Dense_1 = DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    \"\"\"Applies the Transformer MlpBlock module.\"\"\"\n    x = self.Dense_0(x)\n    x = nnx.gelu(x)\n    x = self.Dropout_0(x, deterministic=deterministic)\n    x = self.Dense_1(x)\n    return x",
        "analysis": {
            "module_type": "mlp_block_vit",
            "purpose": "Implements a feed-forward network (MLP) block for a Vision Transformer, consisting of two linear layers with a GELU activation and dropout in between.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies the first DenseGeneral layer.",
                "Applies GELU activation.",
                "Applies Dropout.",
                "Applies the second DenseGeneral layer."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Rngs",
                "nnx.Dropout",
                "DenseGeneral",
                "nnx.gelu"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like hidden_size_for_vit, intermediate_size_for_vit, dtype_mm, dropout_rate, and matmul_precision.",
                "block_id": "Identifier for the MLP block, used for potential logging or specific configurations.",
                "rngs": "Random number generators for parameter initialization."
            },
            "notes": [
                "The input and output hidden dimensions are determined by config.hidden_size_for_vit.",
                "The intermediate dimension is determined by config.intermediate_size_for_vit.",
                "Dropout is applied conditionally based on the 'deterministic' flag during the forward pass."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MlpBlockViT with configuration and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores config, block_id, and rngs.",
                        "Initializes the first DenseGeneral layer (Dense_0).",
                        "Initializes the Dropout layer (Dropout_0).",
                        "Initializes the second DenseGeneral layer (Dense_1)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "nnx.Rngs",
                        "DenseGeneral",
                        "nnx.Dropout"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Applies the Transformer MlpBlock module to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Passes input through Dense_0.",
                        "Applies GELU activation.",
                        "Passes through Dropout_0.",
                        "Passes through Dense_1."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "nnx.gelu",
                        "nnx.Dropout",
                        "DenseGeneral"
                    ],
                    "notes": [
                        "The 'deterministic' flag controls whether dropout is applied."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder1DBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder1DBlock(nnx.Module):\n  \"\"\"Single transformer encoder block (MHSA + MLP).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.block_id = block_id\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.seq_len = (self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2\n\n    self.LayerNorm_0 = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.MultiHeadDotProductAttention_0 = Attention(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=self.seq_len,\n        float32_qk_product=self.config.float32_qk_product,\n        float32_logits=self.config.float32_logits,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        mesh=self.mesh,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        inputs_kv_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        dropout_rate=0,\n        is_nope_layer=True,\n        use_bias_in_projections=True,\n        attention_type=AttentionType.FULL,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / (self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit) ** 0.5,\n        model_mode=\"train\",\n        is_vision=True,\n        rngs=self.rngs,\n    )\n    self.LayerNorm_1 = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.MlpBlockViT_0 = MlpBlockViT(\n        block_id=self.block_id,\n        config=self.config,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, deterministic: bool = False) -> jax.Array:\n    y = self.LayerNorm_0(x)\n\n    y = self.MultiHeadDotProductAttention_0(inputs_q=y, inputs_kv=y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n\n    y = self.LayerNorm_1(x)\n    y = self.MlpBlockViT_0(y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n    return x",
        "analysis": {
            "module_type": "encoder_1d_block",
            "purpose": "Represents a single block within a Transformer encoder, combining Multi-Head Self-Attention (MHSA) and a Feed-Forward Network (MLP).",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "jax.Array"
            },
            "processing_steps": [
                "Applies Layer Normalization.",
                "Performs Multi-Head Dot-Product Attention.",
                "Applies Dropout.",
                "Adds the attention output to the input (residual connection).",
                "Applies another Layer Normalization.",
                "Processes the output through an MLP block.",
                "Applies Dropout again.",
                "Adds the MLP output to the result of the first residual connection."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "nnx.LayerNorm",
                "Attention",
                "nnx.Dropout",
                "MlpBlockViT",
                "Config",
                "Mesh"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like hidden size, number of attention heads, dropout rate, etc.",
                "mesh": "Device mesh for distributed computation.",
                "block_id": "Identifier for the specific encoder block.",
                "rngs": "Random number generators for parameter initialization and dropout."
            },
            "notes": [
                "This block is designed for vision transformers (ViT) as indicated by `is_vision=True` in the Attention layer.",
                "The attention type is set to `AttentionType.FULL`.",
                "Dropout is applied after both the attention and MLP sub-layers.",
                "Residual connections are used around both the attention and MLP sub-layers."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Encoder1DBlock with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A (constructor)",
                        "dtype": "N/A (constructor)"
                    },
                    "processing_steps": [
                        "Initializes LayerNorm layers.",
                        "Initializes the MultiHeadDotProductAttention layer.",
                        "Initializes the MlpBlockViT layer.",
                        "Initializes the Dropout layer."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.LayerNorm",
                        "Attention",
                        "MlpBlockViT",
                        "nnx.Dropout",
                        "Config",
                        "Mesh",
                        "nnx.Rngs"
                    ],
                    "notes": [
                        "Calculates `seq_len` based on `image_size_for_vit` and `patch_size_for_vit`.",
                        "Sets `dropout_rate=0` for the attention layer itself, relying on the separate Dropout layer."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the encoder block, applying attention and MLP layers with residual connections.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Apply LayerNorm_0 to input `x`.",
                        "Pass the normalized input through MultiHeadDotProductAttention_0.",
                        "Apply Dropout_0 to the attention output.",
                        "Add the result to the original input `x` (first residual connection).",
                        "Apply LayerNorm_1 to the result.",
                        "Pass the normalized result through MlpBlockViT_0.",
                        "Apply Dropout_0 to the MLP output.",
                        "Add the result to the output of the first residual connection (second residual connection)."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "LayerNorm_0",
                        "MultiHeadDotProductAttention_0",
                        "Dropout_0",
                        "LayerNorm_1",
                        "MlpBlockViT_0"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior.",
                        "The output `x` is the result after both attention and MLP sub-layers with residual connections."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder(nnx.Module):\n  \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"encoderblock_{lyr}\"\n      layer = Encoder1DBlock(\n          block_id=lyr,\n          config=self.config,\n          mesh=self.mesh,\n          rngs=self.rngs,\n      )\n      setattr(self, layer_name, layer)\n    self.encoder_norm = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    # TODO(aireenmei, hengtaoguo): add if-scan branch to enable scan support for vision encoder\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      x = getattr(self, f\"encoderblock_{lyr}\")(x, deterministic=deterministic)\n    x = self.encoder_norm(x)\n    return x",
        "analysis": {
            "module_type": "encoder",
            "purpose": "The Encoder module represents the encoder part of a Transformer model, specifically designed for sequence-to-sequence translation tasks.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterates through a specified number of Encoder1DBlock layers.",
                "Applies each Encoder1DBlock sequentially to the input.",
                "Applies a final LayerNorm to the output of the last block."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "nnx.Module",
                "nnx.Rngs",
                "nnx.LayerNorm",
                "Encoder1DBlock"
            ],
            "parameters": {
                "config.num_hidden_layers_for_vit": "The number of Encoder1DBlock layers to instantiate.",
                "config.hidden_size_for_vit": "The dimensionality of the hidden states in the encoder.",
                "config.normalization_layer_epsilon": "Epsilon value for LayerNorm."
            },
            "notes": [
                "The module dynamically creates Encoder1DBlock instances based on the configuration.",
                "A TODO comment indicates potential future support for scan operations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Einsum",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Einsum(nnx.Module):\n  \"\"\"Einsum is a convenience module for parameterized tensor multiplication.\"\"\"\n\n  def __init__(\n      self,\n      shape: tuple[int, ...],\n      initializer: nnx.initializers.Initializer = nnx.initializers.normal(),\n      dtype: jnp.dtype | None = None,\n      precision: str = \"default\",\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.precision = precision\n    self.w = nnx.Param(initializer(rngs.params(), shape, dtype))\n\n  def __call__(self, eqn: str, x: jax.Array) -> jax.Array:\n    return jnp.einsum(eqn, x, self.w, precision=self.precision)",
        "analysis": {
            "module_type": "einsum_module",
            "purpose": "A convenience module for parameterized tensor multiplication using einsum.",
            "input": {
                "shape": "[batch_size, ..., input_dim], [batch_size, ..., output_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a learnable parameter 'w' with a given shape and initializer.",
                "Performs tensor multiplication using jnp.einsum with a specified equation and the input tensor 'x' and the learned parameter 'w'."
            ],
            "output": {
                "shape": "Output shape depends on the einsum equation and input shapes."
            },
            "dependencies": [
                "jax.numpy as jnp",
                "flax.nnx as nnx"
            ],
            "parameters": {
                "shape": "The shape of the learnable weight parameter 'w'.",
                "initializer": "The initializer for the weight parameter 'w'. Defaults to normal distribution.",
                "dtype": "The data type of the weight parameter 'w'.",
                "precision": "The precision setting for the einsum operation."
            },
            "notes": [
                "The 'eqn' parameter in the __call__ method defines the einsum operation.",
                "The module stores a learnable weight parameter 'w'."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionEmbedder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionEmbedder(nnx.Module):\n  \"\"\"Projects image embeddings to the embedding space of the text encoder.\"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.mm_soft_embedding_norm = RMSNorm(\n        num_features=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.mm_input_projection = Einsum(\n        shape=(self.config.hidden_size_for_vit, self.config.emb_dim),\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, eqn: str = \"...tm,md->...td\") -> jax.Array:\n    x = self.mm_soft_embedding_norm(x)\n    x = self.mm_input_projection(eqn, x)\n    return x",
        "analysis": {
            "module_type": "vision_embedder",
            "purpose": "Projects image embeddings to the embedding space of the text encoder.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies RMSNorm to the input tensor.",
                "Projects the normalized tensor using an Einsum operation for input projection."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, emb_dim]"
            },
            "dependencies": [
                "RMSNorm",
                "Einsum"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like hidden_size_for_vit, emb_dim, dtype_mm, weight_dtype, normalization_layer_epsilon, and matmul_precision.",
                "mesh": "Device mesh for distributed computation.",
                "rngs": "Random number generators for parameter initialization."
            },
            "notes": [
                "The `eqn` parameter in the `__call__` method defaults to '...tm,md->...td', which is a common einsum notation for matrix multiplication.",
                "This module is designed to bridge the gap between vision model outputs and text model inputs."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the VisionEmbedder module with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the config, mesh, and rngs.",
                        "Initializes an RMSNorm layer.",
                        "Initializes an Einsum layer for input projection."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RMSNorm",
                        "Einsum",
                        "Config",
                        "Mesh",
                        "nnx.Rngs"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Applies the vision embedding projection to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies `mm_soft_embedding_norm`.",
                        "Applies `mm_input_projection` using the provided or default equation."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `eqn` parameter controls the einsum operation for projection."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#visionembedder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def visionembedder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a VisionEmbedder module.\"\"\"\n  return nnx_wrappers.to_linen(\n      VisionEmbedder,\n      config,\n      mesh=mesh,\n      name=\"VisionEmbedder_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "Creates a VisionEmbedder module that can be used with Flax Linen.",
            "usage": "Call this function with a Config object and a Mesh object to get a Linen-compatible VisionEmbedder module. The VisionEmbedder module projects image embeddings to the embedding space of the text encoder."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionExit",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionExit(nnx.Module):\n  \"\"\"The vision exit layer.\n\n  Possibly downsample the soft tokens to a required output length.\n\n  Attributes:\n    output_length: The embed will be spatially avg-pooled to this output length.\n  \"\"\"\n\n  def __init__(self, output_length: int = 256, *, rngs: nnx.Rngs):\n    self.output_length = output_length\n    self.rngs = rngs\n\n  def __call__(self, x):\n    cur_length = x.shape[1]\n    if cur_length == self.output_length:\n      return x\n    cur_width = int(cur_length**0.5)\n    assert cur_width**2 == cur_length\n    output_width = int(self.output_length**0.5)\n    assert output_width**2 == self.output_length, f\"Cannot pool {x.shape=} to {self.output_length}=!\"\n    batch_size = x.shape[0]\n    embed_dim = x.shape[-1]\n    x = jnp.reshape(x, (batch_size, cur_width, cur_width, embed_dim))\n    assert not cur_width % output_width, f\"{cur_width=} {output_width=}\"\n    window = cur_width // output_width\n    window_shape = (window, window)\n    x = nnx.avg_pool(x, window_shape=window_shape, strides=window_shape)\n    batch_size, height, width, embed_dim = x.shape\n    return jnp.reshape(x, (batch_size, height * width, embed_dim))",
        "analysis": {
            "module_type": "vision_exit",
            "purpose": "Downsamples spatial embeddings to a specified output length using average pooling.",
            "input": {
                "shape": "[batch_size, sequence_length, embed_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if current sequence length matches the desired output length.",
                "Reshape input to [batch_size, cur_width, cur_width, embed_dim].",
                "Perform average pooling with a window size derived from the ratio of current to output length.",
                "Reshape output to [batch_size, output_length, embed_dim]."
            ],
            "output": {
                "shape": "[batch_size, output_length, embed_dim]"
            },
            "dependencies": [
                "jax.numpy",
                "flax.nnx"
            ],
            "parameters": {
                "output_length": "The target sequence length after pooling."
            },
            "notes": [
                "Assumes the input sequence length is a perfect square.",
                "Assumes the output length is a perfect square.",
                "Assumes the current width is divisible by the output width."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#vision_exit_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def vision_exit_as_linen(x: jax.Array, output_length: int) -> jax.Array:\n  \"\"\"A wrapper to use VisionExit as a function.\"\"\"\n  return nnx.bridge.to_linen(VisionExit, output_length=output_length)(x)",
        "analysis": {
            "functionality": "This function acts as a bridge to convert a VisionExit NNX module into a Flax Linen module.",
            "usage": "It takes an input JAX array 'x' and an integer 'output_length' as arguments. It then instantiates a VisionExit module with the specified output_length and converts it to a Linen module, which is then applied to the input 'x'. The output is a JAX array representing the processed input."
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3VisionEncoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3VisionEncoderLayer(nnx.Module):\n  \"\"\"gemma 3 vision encoder layer\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.embedding = nnx.Conv(\n        in_features=self.config.num_channels_for_vit,\n        out_features=self.config.hidden_size_for_vit,\n        kernel_size=(self.config.patch_size_for_vit, self.config.patch_size_for_vit),\n        strides=self.config.conv_stride_for_vit,\n        padding=\"VALID\",\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.pos_embedding = self._get_posemb(\n        self.config.posemb_type_for_vit,\n        seqshape=(\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n        ),\n        width=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n    self.Transformer = Encoder(\n        config=self.config,\n        mesh=self.mesh,\n        rngs=self.rngs,\n    )\n    self.VisionExit = VisionExit(output_length=256, rngs=self.rngs)\n\n  def _get_posemb(\n      self,\n      typ: str,\n      *,\n      seqshape: tuple[int, int],\n      width: int,\n      dtype: jnp.dtype = jnp.float32,\n  ):\n    \"\"\"Returns the position embedding.\"\"\"\n    if typ == \"learn\":\n      shape = (1, seqshape[0] * seqshape[1], width)\n      initializer = nnx.initializers.normal(stddev=1 / (width**0.5))\n      return nnx.Param(initializer(self.rngs.params(), shape, dtype))\n    elif typ == \"sincos2d\":\n      return _posemb_sincos_2d(*seqshape, width=width, dtype=dtype, precision=self.config.matmul_precision)\n    else:\n      raise ValueError(f\"Unknown posemb type: {typ}\")\n\n  def __call__(self, inputs, deterministic, train=False):\n    \"\"\"ViT model that transforms image inputs to image embeddings.\n    Args:\n      inputs: jnp.array shaped [B, N, H, W, C], e.g. [4, 1, 896, 896, 3]\n    Returns:\n      jnp.array for image embeddings, shaped [B, N, P, D], e.g. [4, 1, 256, 1152]\n    \"\"\"\n    # currently only supports N=1, the inputs shape is [B, H, W, C]\n    if len(inputs.shape) == 4:\n      inputs = inputs[:, None, :]\n    b, n, h, w, c = inputs.shape\n    x = jnp.reshape(inputs, [b * n, h, w, c])\n    # Gemma3 uses conv2d with stride 14 and kernel size 14 to extract patches.\n    x = self.embedding(x)\n    bn, h, w, c = x.shape\n    x = jnp.reshape(x, [bn, h * w, c])\n\n    x = self.pos_embedding + x\n    x = self.Dropout_0(x)\n\n    # Transformer encoder to extract image features.\n    x = self.Transformer(x, deterministic=deterministic)\n\n    # Gemma3 use a vision exit layer to downsample the soft tokens to a required output length.\n    x = self.VisionExit(x)\n    bn, l, c = x.shape\n    x = jnp.reshape(x, [b, n, l, c])\n    return x",
        "analysis": {
            "module_type": "gemma_3_vision_encoder_layer",
            "purpose": "Processes image inputs through a convolutional embedding, positional encoding, a Transformer encoder, and a vision exit layer to produce image embeddings.",
            "input": {
                "shape": "[batch_size, sequence_length, height, width, channels] (e.g., [4, 1, 896, 896, 3])",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Reshape input to [batch_size * sequence_length, height, width, channels]",
                "Apply convolutional embedding",
                "Reshape to [batch_size * sequence_length, num_patches, hidden_size]",
                "Add positional embeddings",
                "Apply dropout",
                "Pass through Transformer encoder",
                "Apply VisionExit layer for downsampling",
                "Reshape output to [batch_size, sequence_length, output_length, hidden_size]"
            ],
            "output": {
                "shape": "[batch_size, sequence_length, output_length, hidden_size] (e.g., [4, 1, 256, 1152])",
                "dtype": "N/A"
            },
            "dependencies": [
                "nnx.Conv",
                "nnx.Dropout",
                "Encoder",
                "VisionExit",
                "_posemb_sincos_2d"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like hidden size, patch size, dropout rate, etc.",
                "mesh": "Device mesh for distributed computation.",
                "rngs": "Random number generators for parameter initialization."
            },
            "notes": [
                "The module currently supports sequence_length (N) of 1.",
                "The convolutional embedding uses a kernel size and stride defined by config.patch_size_for_vit and config.conv_stride_for_vit respectively.",
                "Positional embeddings can be learned or use a 2D sine-cosine encoding.",
                "The VisionExit layer downsamples the output of the Transformer encoder to a fixed output_length (default 256)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gemma3VisionEncoderLayer with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, and rngs.",
                        "Initialize convolutional embedding layer.",
                        "Initialize positional embedding.",
                        "Initialize dropout layer.",
                        "Initialize Transformer encoder.",
                        "Initialize VisionExit layer."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Conv",
                        "nnx.Dropout",
                        "Encoder",
                        "VisionExit",
                        "_get_posemb"
                    ],
                    "notes": []
                },
                "_get_posemb": {
                    "purpose": "Generates positional embeddings based on the specified type.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if type is 'learn': create a learnable parameter.",
                        "Check if type is 'sincos2d': call _posemb_sincos_2d.",
                        "Raise ValueError for unknown types."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.normal",
                        "_posemb_sincos_2d"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the vision encoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, height, width, channels]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Handle input shape if sequence_length is not present.",
                        "Reshape input for convolutional embedding.",
                        "Apply convolutional embedding.",
                        "Reshape for positional embedding and dropout.",
                        "Add positional embeddings.",
                        "Apply dropout.",
                        "Pass through Transformer encoder.",
                        "Apply VisionExit layer.",
                        "Reshape output to final format."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, output_length, hidden_size]"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "self.embedding",
                        "self.pos_embedding",
                        "self.Dropout_0",
                        "self.Transformer",
                        "self.VisionExit"
                    ],
                    "notes": [
                        "The `train` argument is present but not used in the method body.",
                        "The `deterministic` argument controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#gemma3visionencoder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def gemma3visionencoder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a Gemma3VisionEncoder module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      Gemma3VisionEncoderLayer,\n      config=config,\n      mesh=mesh,\n      name=\"Gemma3VisionEncoderLayer_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "functionality": "This function acts as a factory to create a Flax Linen module for a Gemma3 Vision Encoder layer.",
            "usage": "It takes a configuration object and a JAX Mesh object as input and returns a Linen module that can be used within a larger Flax model. The module is initialized using the `Gemma3VisionEncoderLayer` class and wrapped with `nnx_wrappers.to_linen` for compatibility with Flax Linen."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3LayerNorm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3LayerNorm(nnx.Module):\n  \"\"\"GPT3 Layer normalization operating on the last axis of the input data.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.zeros,\n      use_bias: bool = True,\n      reductions_in_fp32: bool = False,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.use_bias = use_bias\n    self.reductions_in_fp32 = reductions_in_fp32\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    self.scale = nnx.Param(\n        self.scale_init(rngs.params(), (num_features,), self.weight_dtype),\n        sharding=self.kernel_axes,\n    )\n    if self.use_bias:\n      self.bias = nnx.Param(\n          initializers.default_bias_init(rngs.params(), (num_features,), self.weight_dtype), sharding=self.kernel_axes\n      )\n    else:\n      self.bias = None\n\n  def __call__(self, x: jnp.ndarray, out_sharding: NamedSharding | None = None) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    if self.reductions_in_fp32:\n      x = jnp.asarray(x, jnp.float32)\n    mean = jnp.mean(x, axis=[-1], keepdims=True)\n    var = jnp.mean(jnp.square(x - mean), axis=[-1], keepdims=True)\n    normed_inputs = (x - mean) * lax.rsqrt(var + self.epsilon)\n    if self.reductions_in_fp32:\n      normed_inputs = normed_inputs.astype(self.dtype)\n\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"gpt3.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    # broadcast second inputs and element-wise mul\n    output = jnp.einsum(\n        \"i...k,...k->i...k\",\n        normed_inputs,\n        scale + 1,\n        out_sharding=out_sharding,\n    )\n\n    if self.bias is not None:\n      bias = self.bias.value\n      bias = jnp.asarray(bias, self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "functionality": "Implements GPT-3 style Layer Normalization, which normalizes the input tensor along its last axis.",
            "usage": "Instantiate Gpt3LayerNorm with the number of features and optional parameters like epsilon, dtype, and weight initialization. Then, call the instance with an input JAX array to apply the layer normalization. The output is a normalized JAX array."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#gpt3_layer_norm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "def gpt3_layer_norm(\n    *,\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.zeros,\n    use_bias: bool = True,\n    reductions_in_fp32: bool = False,\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Initializes the gpt3_layer_norm module.\n\n  Args:\n    num_features: the number of features.\n    epsilon: the epsilon for the layer norm.\n    dtype: the dtype of the computation (default: float32).\n    weight_dtype: the dtype of the weights (default: float32).\n    kernel_axes: logical axes for partitioning the kernel.\n    scale_init: initializer for the scale.\n    use_bias: whether to add bias in linear transformation.\n    reductions_in_fp32: whether to do reductions in fp32.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n\n  module = nnx_wrappers.to_linen(\n      Gpt3LayerNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      use_bias=use_bias,\n      reductions_in_fp32=reductions_in_fp32,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "functionality": "Initializes a GPT-3 style Layer Normalization module using nnx_wrappers.to_linen.",
            "usage": "Call this function with `num_features` and optional parameters like `epsilon`, `dtype`, `weight_dtype`, `scale_init`, `use_bias`, etc., to create a Linen module that can be used for layer normalization in a GPT-3 model. It returns a callable module."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3MultiHeadAttention",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3MultiHeadAttention(nn.Module):\n  \"\"\"Multi-head attention in gpt3.\n\n  Attributes:\n    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n      should be divisible by the number of heads.\n    head_dim: dimension of each head.\n    max_target_length: maximum length of output\n    max_prefill_predict_length: size of the maximum prefill\n    mesh: device mesh\n    dtype: the dtype of the computation.\n    dropout_rate: dropout rate\n    kernel_init: initializer for the kernel of the Dense layers.\n    float32_qk_product: bool, if True then compute logits via float32 qk_product to avoid\n      numerical issues with bfloat16.\n    float32_logits: bool, if True then cast logits to float32 before softmax to avoid\n      numerical issues with bfloat16.\n    fused_qkv: whether to fuse query, key and value into one projection.\n    quant: Quant, stores quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n  \"\"\"\n\n  config: Config\n  num_heads: int\n  head_dim: int\n  max_target_length: int\n  max_prefill_predict_length: int\n  mesh: Mesh\n  attention_kernel: str\n  dtype: DType = jnp.float32\n  weight_dtype: DType = jnp.float32\n  dropout_rate: float = 0.0\n  kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\")\n  float32_qk_product: bool = False  # computes logits in float32 for stability.\n  float32_logits: bool = True  # cast logits in float32 for stability.\n  fused_qkv: bool = True\n  quant: None | Quant = None\n  kv_quant: None | KVQuant = None\n  use_bias: bool = True\n\n  input_axis_names: AxisNames = (BATCH, LENGTH, EMBED)\n  query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  key_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  value_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  out_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(3, self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def projection(self, inputs: Array, proj_name: str) -> Array:\n    \"\"\"individual projection for one of q, k and v.\"\"\"\n    proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    return proj\n\n  def out_projection(self, output_dim: int, out: Array) -> Array:\n    \"\"\"output projection\"\"\"\n    out_proj = dense_general(\n        inputs_shape=out.shape,\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"heads\", \"kv\", \"embed\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=\"out\",\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(out)\n    return out_proj\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs_q: Array,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n  ):\n    inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n    if self.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.projection(inputs_q, proj_name=\"query\")\n      key = self.projection(inputs_q, proj_name=\"key\")\n      value = self.projection(inputs_q, proj_name=\"value\")\n\n    depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n    query /= depth_scaling\n\n    # annotate with sharding constraint.\n    query = nn.with_logical_constraint(query, self.query_axis_names)\n    query = checkpoint_name(query, \"query_proj\")\n    key = nn.with_logical_constraint(key, self.key_axis_names)\n    key = checkpoint_name(key, \"key_proj\")\n    value = nn.with_logical_constraint(value, self.value_axis_names)\n    value = checkpoint_name(value, \"value_proj\")\n\n    attention_op = attention_op_as_linen(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_heads,\n        num_kv_heads=self.num_heads,\n        dtype=self.dtype,\n    )\n\n    out = attention_op(query, key, value, decoder_segment_ids, model_mode)\n\n    out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    # apply output projection,  output dim is set to the input dim.\n    out = self.out_projection(inputs_q.shape[-1], out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": {
            "module_type": "gpt3_multi_head_attention",
            "purpose": "Implements the multi-head attention mechanism as used in GPT-3.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies a fused QKV projection if enabled, otherwise separate projections for Q, K, V.",
                "Scales the query by the square root of the head dimension.",
                "Applies attention operation using a configured attention kernel.",
                "Applies an output projection."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "jax",
                "flax.linen as nn",
                "MaxText.layers.linears.dense_general",
                "MaxText.layers.attention_op.attention_op_as_linen",
                "MaxText.common_types.Array",
                "MaxText.common_types.AxisNames",
                "MaxText.common_types.Config",
                "MaxText.common_types.DType",
                "MaxText.common_types.Mesh",
                "MaxText.common_types.MODEL_MODE_TRAIN"
            ],
            "parameters": {
                "num_heads": "Number of attention heads. Input features must be divisible by this.",
                "head_dim": "Dimension of each attention head.",
                "max_target_length": "Maximum length of the output sequence.",
                "max_prefill_predict_length": "Size of the maximum prefill.",
                "mesh": "Device mesh for distributed computation.",
                "dtype": "Data type for computation.",
                "dropout_rate": "Dropout rate for regularization.",
                "kernel_init": "Initializer for the kernel of Dense layers.",
                "float32_qk_product": "If True, compute logits via float32 qk_product for stability.",
                "float32_logits": "If True, cast logits to float32 before softmax for stability.",
                "fused_qkv": "If True, fuses query, key, and value into a single projection.",
                "use_bias": "Whether to add bias in linear transformations."
            },
            "notes": [
                "The module supports fused QKV projection for efficiency.",
                "It includes options for numerical stability by using float32 for QK product and logits.",
                "Output dimension of the final projection matches the input dimension."
            ],
            "methods": {
                "qkv_projection": {
                    "purpose": "Performs a fused projection for query, key, and value.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies dense_general to project inputs into QKV.",
                        "Splits the projected output into query, key, and value tensors."
                    ],
                    "output": {
                        "shape": "Tuple of three arrays: (query, key, value)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "dense_general",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "Assumes fused_qkv is True."
                    ]
                },
                "projection": {
                    "purpose": "Performs an individual projection for query, key, or value.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies dense_general to project inputs into a single Q, K, or V tensor."
                    ],
                    "output": {
                        "shape": "Projected tensor (query, key, or value)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "dense_general"
                    ],
                    "notes": [
                        "Used when fused_qkv is False."
                    ]
                },
                "out_projection": {
                    "purpose": "Applies the output projection after the attention mechanism.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies dense_general to project the attention output.",
                        "The output dimension is set to match the input dimension."
                    ],
                    "output": {
                        "shape": "Projected output tensor",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "dense_general"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Executes the multi-head attention computation.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints to input.",
                        "Performs QKV projection (fused or separate).",
                        "Scales query tensor.",
                        "Applies logical constraints and checkpointing to Q, K, V.",
                        "Initializes and calls the attention operation.",
                        "Applies logical constraints to attention output.",
                        "Applies output projection.",
                        "Applies checkpointing to output projection."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "attention_op_as_linen",
                        "jnp.sqrt",
                        "qkv_projection",
                        "projection",
                        "out_projection"
                    ],
                    "notes": [
                        "Accepts optional decoder_segment_ids.",
                        "Can operate in different model modes (e.g., training, inference).",
                        "Deterministic flag controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3DecoderLayer",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = gpt3_layer_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n        reductions_in_fp32=False,\n        use_bias=True,\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    assert (\n        cfg.num_query_heads == cfg.num_kv_heads\n    ), f\"{cfg.num_query_heads=} should be the same as {cfg.num_kv_heads=} in gpt3\"\n    attention_layer = Gpt3MultiHeadAttention(\n        config=cfg,\n        num_heads=cfg.num_query_heads,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        mesh=mesh,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        fused_qkv=cfg.fused_qkv,\n        use_bias=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n    )\n\n    attention_lnx = attention_layer(\n        lnx, decoder_segment_ids=decoder_segment_ids, model_mode=model_mode, deterministic=deterministic\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attention_lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        use_bias=True,\n        use_pre_norm=True,\n        config=cfg,\n        quant=self.quant,\n        mesh=self.mesh,\n    )(attention_lnx, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = attention_lnx + mlp_lnx\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gpt3_decoder_layer",
            "purpose": "Represents a single layer of the GPT-3 decoder, including self-attention and a feed-forward network.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply pre-layer normalization.",
                "Perform self-attention using Gpt3MultiHeadAttention.",
                "Add the self-attention output to the original input (residual connection).",
                "Apply MLP block.",
                "Add the MLP output to the result of the self-attention block (residual connection).",
                "Apply dropout.",
                "Optionally record internal metrics.",
                "Return the layer output."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "models.Config",
                "Mesh",
                "Quant",
                "gpt3_layer_norm",
                "Gpt3MultiHeadAttention",
                "mlp_block",
                "nn.Dropout",
                "checkpoint_name",
                "nn.with_logical_constraint"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like hidden dimensions, number of heads, etc.",
                "mesh": "Device mesh for distributed computation.",
                "model_mode": "The current mode of the model (e.g., training, inference).",
                "quant": "Quantization configuration, if any."
            },
            "notes": [
                "The layer normalizations and attention mechanisms are specific to the GPT-3 architecture.",
                "Residual connections are used after both the self-attention and MLP blocks.",
                "Dropout is applied to the final output of the layer.",
                "Internal metrics can be recorded if `cfg.record_internal_nn_metrics` is True.",
                "The output shape is expected to be the same as the input shape."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs a forward pass through the GPT-3 decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str, previous_chunk: N/A, page_state: N/A, slot: N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint to inputs.",
                        "Checkpoint the input for gradient computation.",
                        "Apply pre-self-attention layer normalization.",
                        "Apply logical constraint to normalized inputs.",
                        "Instantiate and call Gpt3MultiHeadAttention for self-attention.",
                        "Apply logical constraint to attention output.",
                        "Add attention output to the original inputs (residual connection).",
                        "Instantiate and call mlp_block for the feed-forward network.",
                        "Apply logical constraint to MLP output.",
                        "Add MLP output to the attention output (residual connection).",
                        "Apply dropout.",
                        "Apply logical constraint to the final layer output.",
                        "Optionally sow internal metrics.",
                        "Return the final layer output, and None if scan_layers is False."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "gpt3_layer_norm",
                        "Gpt3MultiHeadAttention",
                        "mlp_block",
                        "nn.Dropout",
                        "checkpoint_name",
                        "nn.with_logical_constraint",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum"
                    ],
                    "notes": [
                        "The `decoder_positions` argument is not explicitly used within the `__call__` method but is passed as an argument.",
                        "The `previous_chunk`, `page_state`, and `slot` arguments are also not used in this implementation.",
                        "The `assert` statement ensures that `num_query_heads` and `num_kv_heads` are equal, as expected in GPT-3.",
                        "If `cfg.scan_layers` is True, the method returns a tuple containing the layer output and None."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#get_attention_type",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "def get_attention_type(layer_id):\n  \"\"\"Get attention type based on layer ID.\"\"\"\n  layer_id %= len(GPT_OSS_ATTENTION_PATTERN)\n  return GPT_OSS_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "functionality": "Determines the attention type for a given layer ID based on a predefined pattern.",
            "usage": "Call this function with an integer `layer_id` to get the corresponding `AttentionType` from the `GPT_OSS_ATTENTION_PATTERN` list. The `layer_id` is taken modulo the length of the pattern list to cycle through available attention types."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssDecoderLayer",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  attention_type: AttentionType\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"GptOssAttention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_bias_in_projections=cfg.attention_bias,\n        attention_type=self.attention_type,\n        sliding_window_size=cfg.sliding_window_size,\n        query_pre_attn_scalar=(cfg.head_dim**-0.5),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    mlp_lnx, load_balance_loss = moe.get_routed_moe(\n        name=\"GptOssMlp\",\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "Implements a single Transformer decoder layer for GPT OSS models, including self-attention and a Mixture-of-Experts (MoE) feed-forward network.",
            "usage": "This class is a Flax nn.Module that represents a decoder layer. It takes input tensors, decoder segment IDs, decoder positions, a deterministic flag, and the model mode as arguments. It outputs the processed tensor after passing through the attention and MLP blocks. The layer also handles optional quantization and internal metric recording based on the configuration."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssScannableBlock",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssScannableBlock(nn.Module):\n  \"\"\"A repeatable block of GPT OSS decoder layers.\n\n    This block applies multiple decoder layers sequentially, using the attention\n    pattern defined by GPT_OSS_ATTENTION_PATTERN. It's designed to be\n    used with `nn.scan` for efficient compilation.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    num_of_layers: int, number of decoder layers in the block\n    quant: Optional[Quant], quantization config\n  \"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      attention_type = get_attention_type(layer_id)\n      layer = GptOssDecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          model_mode=model_mode,\n          name=f\"layers_{layer_id}\",\n          attention_type=attention_type,\n          quant=self.quant,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gpt_oss_scannable_block",
            "purpose": "A repeatable block of GPT OSS decoder layers designed for efficient compilation with nn.scan.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply logical constraints to inputs.",
                "Checkpoint inputs.",
                "Iterate through decoder layers (based on config.inhomogeneous_layer_cycle_interval).",
                "Instantiate GptOssDecoderLayer for each iteration.",
                "Call the GptOssDecoderLayer with inputs and other relevant arguments.",
                "If config.scan_layers is True, extract the first element of the layer's output.",
                "Return the final output, potentially with an additional None if config.scan_layers is True."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None)",
                "dtype": "N/A"
            },
            "dependencies": [
                "GptOssDecoderLayer",
                "nn.Module",
                "nn.with_logical_constraint",
                "checkpoint_name",
                "get_attention_type"
            ],
            "parameters": {
                "config": "MaxText model configuration object.",
                "mesh": "JAX device mesh for sharding.",
                "model_mode": "String indicating the current model mode (e.g., 'train', 'eval').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The number of layers processed is determined by `config.inhomogeneous_layer_cycle_interval`.",
                "The attention type for each layer is determined dynamically using `get_attention_type`.",
                "If `config.scan_layers` is True, the output format changes, and the block is designed for use with `nn.scan`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the GptOssScannableBlock.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraints and checkpointing to the input tensor.",
                        "Loop through a configurable number of decoder layers.",
                        "For each layer, determine the attention type and instantiate a GptOssDecoderLayer.",
                        "Pass the current tensor through the GptOssDecoderLayer.",
                        "If `cfg.scan_layers` is enabled, extract the primary output from the layer's return tuple.",
                        "Return the final processed tensor and potentially None if `cfg.scan_layers` is enabled."
                    ],
                    "output": {
                        "shape": "A tensor representing the output of the decoder block, or a tuple containing the tensor and None if `cfg.scan_layers` is True.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "GptOssDecoderLayer",
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "get_attention_type"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior.",
                        "The `model_mode` parameter influences layer behavior.",
                        "The `cfg.scan_layers` flag significantly alters the output structure and is intended for use with `nn.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#nd_dense_init",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def nd_dense_init(scale, mode, distribution):\n  \"\"\"Creates a variance-scaling initializer with dynamic in/out axes.\n\n  This function is a factory that returns an initializer function. The returned\n  function is a wrapper around `jax.nn.initializers.variance_scaling` that\n  allows the `in_axis` and `out_axis` to be specified at call time, rather\n  than at creation time.\n\n  Args:\n    scale: The scaling factor for the variance.\n    mode: The mode for variance scaling ('fan_in', 'fan_out', 'fan_avg').\n    distribution: The distribution to sample from ('normal', 'uniform', etc.).\n\n  Returns:\n    A function that takes a PRNG key, shape, dtype, in_axis, and out_axis,\n    and returns an initialized array.\n  \"\"\"\n\n  def init_fn(key, shape, dtype, in_axis, out_axis):\n    \"\"\"Initializes an array using variance scaling with specified axes.\"\"\"\n    fn = jax.nn.initializers.variance_scaling(scale, mode, distribution, in_axis, out_axis)\n    return fn(key, shape, dtype)\n\n  return init_fn",
        "analysis": {
            "functionality": "Creates a factory function for variance scaling initializers that allows dynamic specification of input and output axes.",
            "usage": "Call `nd_dense_init` with `scale`, `mode`, and `distribution` to get an initializer function. This returned function then takes a PRNG key, shape, dtype, `in_axis`, and `out_axis` to initialize an array using JAX's variance scaling."
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#variable_to_logically_partitioned",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def variable_to_logically_partitioned(variable: nnx.VariableState):\n  \"\"\"Wraps an NNX variable's value in `nn.LogicallyPartitioned`.\n\n  This function inspects the metadata of an `nnx.VariableState` object. If\n  sharding information ('sharding' or 'sharding_names') is present, it wraps\n  the variable's value in `nn.LogicallyPartitioned` to apply the specified\n  sharding constraints.\n\n  It handles special cases for `aqt_tensor.QTensor` and variables of type\n  `_overwrite_with_gradient` by returning their values directly without\n  wrapping.\n\n  Args:\n    variable: The `nnx.VariableState` object to process.\n\n  Returns:\n    The variable's value, potentially wrapped in `nn.LogicallyPartitioned`.\n  \"\"\"\n  if isinstance(variable.value, aqt_tensor.QTensor):\n    return variable.value\n\n  if variable.type.__name__ == \"_overwrite_with_gradient\":\n    return variable.value\n\n  metadata = variable.get_metadata()\n  if \"sharding\" in metadata or \"sharding_names\" in metadata:\n    if \"sharding_names\" in metadata:\n      sharding_names = metadata[\"sharding_names\"]\n    else:\n      sharding_names = metadata[\"sharding\"]\n    return nn.LogicallyPartitioned(  # type: ignore[wrong-keyword-args]\n        variable.value,\n        sharding_names,  # type: ignore[arg-type]\n        mesh=metadata.get(\"mesh\"),\n        rules=metadata.get(\"rules\"),\n    )\n  else:\n    return variable.value",
        "analysis": {
            "functionality": "Conditionally wraps an NNX variable's value in `nn.LogicallyPartitioned` based on its metadata.",
            "usage": "Pass an `nnx.VariableState` object to the function. If the variable has 'sharding' or 'sharding_names' in its metadata, its value will be wrapped in `nn.LogicallyPartitioned` with the specified sharding, mesh, and rules. Special types like `aqt_tensor.QTensor` or `_overwrite_with_gradient` are returned directly. The function returns the (potentially wrapped) variable value."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_convert_to_activation_function",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _convert_to_activation_function(fn_or_string: str | Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Convert a string to an activation function.\"\"\"\n  if fn_or_string == \"linear\":\n    return lambda x: x\n  elif isinstance(fn_or_string, str):\n    return getattr(nn, fn_or_string)\n  elif callable(fn_or_string):\n    return fn_or_string\n  else:\n    raise ValueError(\n        f\"\"\"Don't know how to convert {fn_or_string}\n                         to an activation function\"\"\"\n    )",
        "analysis": {
            "functionality": "Converts a string or a callable to an activation function.",
            "usage": "Pass a string (e.g., 'relu', 'gelu') or a callable function to this function. It returns a callable activation function. If the input is 'linear', it returns an identity function. It raises a ValueError for unknown inputs."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#normalize_axes",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def normalize_axes(axes: Iterable[int], ndim: int) -> tuple[int, ...]:\n  # A tuple by convention. len(axes_tuple) then also gives the rank efficiently.\n  return tuple(ax if ax >= 0 else ndim + ax for ax in axes)",
        "analysis": {
            "functionality": "Normalizes axes by converting negative indices to positive ones based on the total number of dimensions.",
            "usage": "Takes an iterable of integer axes and the total number of dimensions (ndim) as input. It returns a tuple of normalized axes where negative axes are converted to their positive equivalents. For example, -1 becomes ndim - 1."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#canonicalize_tuple",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def canonicalize_tuple(x):\n  if isinstance(x, Iterable):\n    return tuple(x)\n  else:\n    return (x,)",
        "analysis": {
            "functionality": "Ensures that the input is a tuple, converting non-iterable inputs into a single-element tuple.",
            "usage": "Pass any object to this function. If the object is an iterable (like a list or another tuple), it will be returned as a tuple. If it's not an iterable (like an integer or a string), it will be wrapped in a tuple."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general(inputs, kernel, kernel_axes, axis, contract_ind, matmul_precision, quant):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant:\n    dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n    dot_general = dot_general_cls()\n    return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "functionality": "Computes a dot_general operation, with optional quantization support.",
            "usage": "This function is used to perform matrix multiplication between 'inputs' and 'kernel'. It supports optional quantization via the 'quant' parameter. The 'kernel_axes', 'axis', 'contract_ind', and 'matmul_precision' parameters control the specifics of the dot_general operation. It returns the result of the dot_general operation."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general_nnx",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general_nnx(\n    inputs,\n    kernel,\n    axis,\n    contract_ind,\n    matmul_precision,\n    quant_dot_general: nnx_wrappers.ToNNX | None,\n    initializing: bool,\n    out_sharding: NamedSharding | None = None,\n):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant_dot_general is not None:\n    if initializing:\n      quant_dot_general.lazy_init(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n    return quant_dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None, mutable=[\"aqt\"])\n\n  return dot_general(\n      inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision, out_sharding=out_sharding\n  )",
        "analysis": {
            "functionality": "Computes a dot_general operation, with optional quantization support.",
            "usage": "This function takes input tensors, a kernel, and various parameters to perform a matrix multiplication. It can optionally apply quantization if `quant_dot_general` is provided. The `initializing` flag controls lazy initialization of the quantizer. The output is the result of the dot_general operation."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#DenseGeneral",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class DenseGeneral(nnx.Module):\n  \"\"\"A linear transformation with flexible axes.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Iterable[int] | int,\n      out_features_shape: Iterable[int] | int,\n      axis: Iterable[int] | int = -1,\n      weight_dtype: DType = jnp.float32,\n      dtype: DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: tuple[None | str, ...] = (),\n      quant: None | Quant = None,\n      use_bias: bool = False,\n      shard_mode: ShardMode = ShardMode.AUTO,\n      matmul_precision: str = \"default\",\n      parameter_memory_host_offload: bool = False,\n      *,  # Following arguments are keyword-only\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the DenseGeneral module.\n\n    Args:\n      in_features_shape: tuple with numbers of input features for axes specified in\n        'axis'.\n      out_features_shape: tuple with numbers of output features.\n      axis: tuple with axes to apply the transformation on.\n      weight_dtype: the dtype of the weights (default: float32).\n      dtype: the dtype of the computation (default: float32).\n      kernel_init: initializer function for the weight matrix.\n      kernel_axes: logical axes for partitioning the kernel.\n      quant: quantization config, defaults to None implying no quantization.\n      use_bias: whether to add bias in linear transformation.\n      shard_mode: auto or explicit shard mode.\n      matmul_precision: Precision for matrix multiplication.\n      parameter_memory_host_offload: Determines whether to offload params to host\n      rngs: RNG state for initialization in nnx.\n    \"\"\"\n    self.in_features_shape = canonicalize_tuple(in_features_shape)\n    self.out_features_shape = canonicalize_tuple(out_features_shape)\n    self.axis = canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.quant = quant\n    self.use_bias = use_bias\n    self.shard_mode = shard_mode\n    self.matmul_precision = matmul_precision\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: Array, _initializing: bool = False, out_sharding: NamedSharding | None = None) -> Array:\n    \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n\n    Args:\n      inputs: The nd-array to be transformed.\n\n    Returns:\n      The transformed input.\n    \"\"\"\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = normalize_axes(self.axis, inputs.ndim)\n\n    for i, ax in enumerate(norm_axis):\n      if inputs.shape[ax] != self.in_features_shape[i]:\n        raise ValueError(\n            f\"Input dimension {inputs.shape[ax]} at axis {ax} \"\n            f\"does not match expected input feature size {self.in_features_shape[i]}\"\n        )\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n      # Move logit_dense kernel to device if parameter offloading is enabled\n      if self.parameter_memory_host_offload:\n        max_logging.log(\"linear.py: Moving parameter logits_dense kernel to device\")\n        kernel = jax.device_put(kernel, max_utils.device_space())\n      kernel = jnp.asarray(kernel, self.dtype)\n\n    # out_sharding should be None for auto mesh axis\n    if self.shard_mode != ShardMode.EXPLICIT:\n      out_sharding = None\n\n    contract_ind = tuple(range(0, len(self.axis)))\n    output = _compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n        out_sharding,\n    )\n\n    if self.bias is not None:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "module_type": "dense_general",
            "purpose": "Applies a linear transformation to the inputs along multiple dimensions.",
            "input": {
                "shape": "[batch_size, ..., in_features_shape, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Convert input to specified dtype.",
                "Normalize input axes.",
                "Validate input dimensions against expected in_features_shape.",
                "Retrieve kernel weights.",
                "Perform matrix multiplication (dot_general) between inputs and kernel.",
                "Add bias if use_bias is True."
            ],
            "output": {
                "shape": "[batch_size, ..., out_features_shape, ...]"
            },
            "dependencies": [
                "jax",
                "flax.nnx",
                "numpy",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.quantizations",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "in_features_shape": "Shape of the input features for the axes to be transformed.",
                "out_features_shape": "Shape of the output features.",
                "axis": "Axes to apply the transformation on.",
                "weight_dtype": "Data type of the kernel weights.",
                "dtype": "Data type for computation.",
                "kernel_init": "Initializer for the kernel weights.",
                "kernel_axes": "Logical axes for partitioning the kernel.",
                "quant": "Quantization configuration.",
                "use_bias": "Whether to use bias.",
                "shard_mode": "Mode for sharding parameters.",
                "matmul_precision": "Precision setting for matrix multiplication.",
                "parameter_memory_host_offload": "Whether to offload parameters to host memory."
            },
            "notes": [
                "Can handle transformations across multiple input dimensions specified by `axis`.",
                "Supports optional bias addition.",
                "Integrates with quantization mechanisms.",
                "Handles parameter sharding and offloading."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the DenseGeneral module with specified configurations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Initialize kernel weights using `kernel_init`.",
                        "Initialize bias if `use_bias` is True.",
                        "Initialize quantization-related components if `quant` is provided."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Module",
                        "nnx.Param",
                        "nnx.Rngs",
                        "jax.numpy",
                        "numpy",
                        "MaxText.layers.quantizations",
                        "MaxText.layers.nnx_wrappers",
                        "MaxText.layers.initializers"
                    ],
                    "notes": [
                        "Parameters (`kernel`, `bias`) are initialized using `nnx.Param`.",
                        "Quantization setup involves creating and setting a `quant_dot_general` attribute."
                    ]
                },
                "quant_dot_general": {
                    "purpose": "Provides access to the quantized dot general function if quantization is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieve the quantized dot general function from the module's attributes."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "Returns None if quantization is not configured."
                    ]
                },
                "__call__": {
                    "purpose": "Applies a linear transformation to the inputs along multiple dimensions.",
                    "input": {
                        "shape": "[batch_size, ..., in_features_shape, ...]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Convert inputs to the module's dtype.",
                        "Normalize the axes of the input tensor.",
                        "Validate that input dimensions match `in_features_shape` for the specified axes.",
                        "Retrieve the kernel weights (or a zero tensor if in serve mode with quantization).",
                        "Optionally move kernel to device if `parameter_memory_host_offload` is True.",
                        "Convert kernel to the module's dtype.",
                        "Determine output sharding based on `shard_mode`.",
                        "Compute the dot product using `_compute_dot_general_nnx`.",
                        "Add bias to the output if `use_bias` is True."
                    ],
                    "output": {
                        "shape": "[batch_size, ..., out_features_shape, ...]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "MaxText.layers.quantizations",
                        "MaxText.layers.nnx_wrappers",
                        "MaxText.max_utils",
                        "MaxText.max_logging"
                    ],
                    "notes": [
                        "The `_initializing` flag is used for lazy initialization of quantized operations.",
                        "Raises a `ValueError` if input dimensions do not match expected `in_features_shape`.",
                        "Handles `out_sharding` based on `shard_mode`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#dense_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def dense_general(\n    *,\n    inputs_shape: tuple[int, ...] | None = None,\n    in_features_shape: tuple[int, ...] | int | None = None,\n    out_features_shape: Iterable[int] | int,\n    axis: Iterable[int] | int = -1,\n    weight_dtype: DType = jnp.float32,\n    dtype: DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: tuple[None | str, ...] = (),\n    quant: None | Quant = None,\n    use_bias: bool = False,\n    shard_mode: ShardMode = ShardMode.AUTO,\n    matmul_precision: str = \"default\",\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Creates a DenseGeneral Linen module using nnx.bridge.to_linen.\n\n  Args:\n    inputs_shape: tuple with the shape of the inputs\n    in_features_shape: tuple with numbers of input features for axes specified in\n      'axis'.\n    out_features_shape: tuple with numbers of output features.\n    axis: tuple with axes to apply the transformation on.\n    weight_dtype: the dtype of the weights (default: float32).\n    dtype: the dtype of the computation (default: float32).\n    kernel_init: initializer function for the weight matrix.\n    kernel_axes: logical axes for partitioning the kernel.\n    quant: quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n    shard_mode: indicating the shard mode\n    matmul_precision: Precision for matrix multiplication.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n  if not (inputs_shape is not None) ^ (in_features_shape is not None):\n    raise ValueError(\"Exactly one of inputs_shape or in_features must be specified.\")\n\n  if inputs_shape is not None:\n    axis = canonicalize_tuple(axis)\n    in_features_shape = tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n  else:\n    assert in_features_shape is not None\n  module = nnx_wrappers.to_linen(\n      DenseGeneral,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      quant=quant,\n      use_bias=use_bias,\n      shard_mode=shard_mode,\n      matmul_precision=matmul_precision,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "functionality": "Creates a DenseGeneral Linen module using nnx.bridge.to_linen.",
            "usage": "This function acts as a factory to create a DenseGeneral module, which performs a linear transformation. It requires either the input shape or the input features shape, along with the output features shape and optionally the axis for transformation. Other parameters control weight initialization, data types, quantization, bias usage, sharding, and precision."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#Dropout",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class Dropout(nnx.Dropout):\n  \"\"\"Forked nnx.Dropout that is easier to use with bridge\"\"\"\n\n  def __init__(  # pylint: disable=super-init-not-called\n      self,\n      rate: float,\n      *,\n      broadcast_dims: Sequence[int] = (),\n      deterministic: bool = False,\n      rng_collection: str = \"dropout\",\n      rngs: nnx.Rngs | None = None,\n  ):\n    self.rate = rate\n    self.broadcast_dims = broadcast_dims\n    self.deterministic = deterministic\n    self.rng_collection = rng_collection\n\n    if isinstance(rngs, nnx.Rngs):\n      self.rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else rngs\n    else:\n      raise TypeError(f\"rngs must be a Rngs, RngStream or None, but got {type(rngs)}.\")",
        "analysis": {
            "module_type": "dropout",
            "purpose": "A custom Dropout layer that is designed for easier integration with a 'bridge' mechanism, likely for model serialization or distributed training frameworks.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the Dropout layer with rate, broadcast dimensions, deterministic flag, and RNG collection.",
                "Handles RNG state by forking if a nnx.Rngs object is provided."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.Dropout",
                "typing.Sequence",
                "nnx.Rngs"
            ],
            "parameters": {
                "rate": "The probability of an element to be zeroed.",
                "broadcast_dims": "Dimensions to broadcast the dropout mask over.",
                "deterministic": "If True, dropout is not applied.",
                "rng_collection": "The name of the RNG collection to use for dropout."
            },
            "notes": [
                "This class inherits from nnx.Dropout but overrides the __init__ method.",
                "The `super().__init__()` call is explicitly commented out, suggesting a custom initialization logic.",
                "It raises a TypeError if `rngs` is not an instance of `nnx.Rngs` or `None`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#MlpBlock",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class MlpBlock(nnx.Module):\n  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      in_features: int,\n      intermediate_dim: int = 2048,\n      activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      intermediate_dropout_rate: float = 0.1,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      use_bias: bool = False,\n      use_pre_norm: bool = False,\n      quant: None | Quant = None,\n      model_mode: None | str = None,\n      *,\n      rngs: nnx.Rngs,\n  ) -> None:\n    \"\"\"A MlpBlock module.\n\n    Args:\n      config: Config object containing model parameters.\n      mesh: Mesh object of device and physical axes information\n      in_features: Number of input features.\n      intermediate_dim: Shared dimension of hidden layers.\n      activations: Type of activations for each layer.  Each element is either\n        'linear', a string function name in flax.linen, or a function.\n      kernel_init: Kernel function, passed to the dense layers.\n      deterministic: Whether the dropout layers should be deterministic.\n      intermediate_dropout_rate: Dropout rate used after the intermediate layers.\n      dtype: computation data type for the dense layer.\n      weight_dtype: weight data type for the dense layer.\n      use_bias: whether to add bias in all feedforward layers.\n      use_pre_norm: whether to add pre layer norm in mlp layers.\n      quant: Optional quantization config, no quantization if None.\n      out_sharding: Named sharding of outputs\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.in_features = in_features\n    self.intermediate_dim = intermediate_dim\n    self.activations = activations\n    self.kernel_init = kernel_init\n    self.intermediate_dropout_rate = intermediate_dropout_rate\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.use_bias = use_bias\n    self.use_pre_norm = use_pre_norm\n    self.quant = quant\n    self.model_mode = model_mode\n\n    if self.use_pre_norm:\n      self.mlp_layer_norm = self.get_norm_layer(num_features=in_features)(\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          epsilon=config.normalization_layer_epsilon,\n          rngs=rngs,\n      )\n    else:\n      self.mlp_layer_norm = None\n\n    if self.model_mode == MODEL_MODE_PREFILL:\n      self.intermediate_logical = (\"activation_batch\", \"prefill_activation_length\", \"activation_mlp\")\n    elif config.expert_shard_attention_option == EP_AS_CONTEXT and self.model_mode == MODEL_MODE_TRAIN:\n      self.intermediate_logical = (\"activation_batch_no_exp\", \"activation_length\", \"activation_mlp\")\n    else:\n      self.intermediate_logical = (\"activation_batch\", \"activation_length_no_exp\", \"activation_mlp\")\n\n    if config.fused_mlp:\n      self.wi = DenseGeneral(\n          in_features_shape=in_features,\n          out_features_shape=(len(self.activations), self.intermediate_dim),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"num_activations\", \"mlp\"),\n          quant=self.quant,\n          use_bias=self.use_bias,\n          shard_mode=self.config.shard_mode,\n          matmul_precision=self.config.matmul_precision,\n          rngs=rngs,\n      )\n    else:\n      for idx in range(len(self.activations)):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = DenseGeneral(\n            in_features_shape=in_features,\n            out_features_shape=self.intermediate_dim,\n            dtype=self.dtype,\n            weight_dtype=self.weight_dtype,\n            kernel_init=self.kernel_init,\n            kernel_axes=(\"embed\", \"mlp\"),\n            quant=self.quant,\n            use_bias=self.use_bias,\n            shard_mode=self.config.shard_mode,\n            matmul_precision=self.config.matmul_precision,\n            rngs=rngs,\n        )\n        setattr(self, dense_name, module)\n    self.dropout = Dropout(rate=self.intermediate_dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n    self.wo = DenseGeneral(\n        in_features_shape=self.intermediate_dim,\n        out_features_shape=in_features,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"mlp\", \"embed\"),\n        quant=self.quant,\n        use_bias=self.use_bias,\n        shard_mode=self.config.shard_mode,\n        matmul_precision=self.config.matmul_precision,\n        rngs=rngs,\n    )\n\n    self._maybe_shard_with_logical = functools.partial(\n        maybe_shard_with_logical,\n        mesh=mesh,\n        shard_mode=config.shard_mode,\n    )\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer.\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(normalizations.RMSNorm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      from MaxText.layers import gpt3  # pylint: disable=import-outside-toplevel\n\n      return functools.partial(\n          gpt3.Gpt3LayerNorm, num_features=num_features, reductions_in_fp32=False, use_bias=self.use_bias\n      )\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def __call__(\n      self,\n      inputs,\n      decode: bool = False,\n      deterministic: bool = False,\n      intermediate_sharding: NamedSharding | None = None,\n      out_sharding: NamedSharding | None = None,\n  ):\n    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n    cfg = self.config\n\n    if self.mlp_layer_norm is not None:\n      inputs = self.mlp_layer_norm(inputs)\n\n    # Iterate over specified MLP input activation functions.\n    # e.g. ('relu',) or ('gelu', 'linear') for gated-gelu.\n    activations = []\n    if cfg.fused_mlp:\n      x = self.wi(inputs, out_sharding=intermediate_sharding)\n      x = checkpoint_name(x, \"mlpwi\")\n      for idx, act_fn in enumerate(self.activations):\n        y = _convert_to_activation_function(act_fn)(x[:, :, idx, ...])\n        activations.append(y)\n    else:\n      for idx, act_fn in enumerate(self.activations):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = getattr(self, dense_name)\n        x = module(inputs, out_sharding=intermediate_sharding)\n        x = checkpoint_name(x, \"mlp\" + dense_name)\n        if cfg.activations_in_float32:\n          x = x.astype(jnp.float32)\n        x = _convert_to_activation_function(act_fn)(x)\n        activations.append(x)\n\n    # Take elementwise product of above intermediate activations.\n    x = functools.reduce(operator.mul, activations).astype(self.dtype)\n    # Apply dropout and final dense output projection.\n    x = self.dropout(x, deterministic=deterministic)  # Broadcast along length.\n    x = self._maybe_shard_with_logical(x, self.intermediate_logical)\n    output = self.wo(x, out_sharding=out_sharding)\n\n    output = checkpoint_name(output, \"mlpwo\")\n    return output",
        "analysis": {
            "module_type": "mlp_block",
            "purpose": "Implements a Transformer's Feed-Forward Network (FFN) block, also known as an MLP block.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Optional pre-layer normalization.",
                "First linear transformation (wi) with activation(s).",
                "Element-wise product of intermediate activations (if multiple).",
                "Dropout.",
                "Second linear transformation (wo)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "DenseGeneral",
                "Dropout",
                "functools.partial",
                "operator.mul",
                "jax.numpy as jnp",
                "flax.nnx",
                "MaxText.layers.normalizations",
                "MaxText.sharding.maybe_shard_with_logical"
            ],
            "parameters": {
                "config": "Config object containing model parameters.",
                "mesh": "Mesh object for distributed computing.",
                "in_features": "Number of input features.",
                "intermediate_dim": "Shared dimension of hidden layers.",
                "activations": "Type of activations for each layer (e.g., 'relu', 'gelu').",
                "kernel_init": "Kernel initializer for dense layers.",
                "intermediate_dropout_rate": "Dropout rate for the intermediate layer.",
                "dtype": "Computation data type.",
                "weight_dtype": "Weight data type.",
                "use_bias": "Whether to use bias in dense layers.",
                "use_pre_norm": "Whether to use pre-layer normalization.",
                "quant": "Optional quantization configuration.",
                "model_mode": "Specifies the operational mode (e.g., 'train', 'prefill')."
            },
            "notes": [
                "Supports fused MLP operations for efficiency.",
                "Handles multiple activation functions, potentially for gated activations.",
                "Includes logic for sharding and logical partitioning based on model configuration and mode."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MlpBlock module with specified configuration and parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration and module parameters.",
                        "Initializes optional pre-layer normalization layer.",
                        "Determines intermediate logical sharding based on model mode and config.",
                        "Initializes DenseGeneral layers for 'wi' (potentially fused or per activation).",
                        "Initializes Dropout layer.",
                        "Initializes DenseGeneral layer for 'wo'.",
                        "Sets up partial function for sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "Dropout",
                        "functools.partial",
                        "MaxText.sharding.maybe_shard_with_logical",
                        "MaxText.layers.normalizations"
                    ],
                    "notes": [
                        "The initialization of 'wi' depends on `config.fused_mlp`.",
                        "Handles different normalization layers based on `config.decoder_block`."
                    ]
                },
                "get_norm_layer": {
                    "purpose": "Returns a partial function for a normalization layer based on the decoder block type.",
                    "input": {
                        "shape": "num_features: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks `self.config.decoder_block`.",
                        "Returns `functools.partial(normalizations.RMSNorm, ...)` for most cases.",
                        "Returns `functools.partial(gpt3.Gpt3LayerNorm, ...)` for GPT3.",
                        "Raises ValueError for unknown decoder block types."
                    ],
                    "output": {
                        "shape": "Callable",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "functools.partial",
                        "MaxText.layers.normalizations",
                        "MaxText.common_types.DecoderBlockType"
                    ],
                    "notes": [
                        "This method is used internally by `__init__` if `use_pre_norm` is True."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the Transformer MLP block to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies pre-layer normalization if enabled.",
                        "Performs the first linear transformation(s) ('wi') and applies activation functions.",
                        "Combines intermediate activations via element-wise multiplication.",
                        "Applies dropout.",
                        "Applies logical sharding.",
                        "Performs the second linear transformation ('wo').",
                        "Applies checkpointing for gradient computation."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "checkpoint_name",
                        "_convert_to_activation_function",
                        "operator.mul",
                        "functools.reduce",
                        "jax.numpy as jnp"
                    ],
                    "notes": [
                        "Handles fused MLP and non-fused MLP paths.",
                        "Supports `deterministic` flag for dropout during inference.",
                        "Allows specifying `intermediate_sharding` and `out_sharding` for distributed execution."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#mlp_block",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def mlp_block(\n    *,\n    config: Config,\n    mesh: Mesh,\n    in_features: int,\n    intermediate_dim: int = 2048,\n    activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    intermediate_dropout_rate: float = 0.1,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    use_bias: bool = False,\n    use_pre_norm: bool = False,\n    quant: None | Quant = None,\n    model_mode: None | str = None,\n    name: None | str = None,\n):\n  \"\"\"Creates a MlpBlock Linen module using nnx.bridge.to_linen.\"\"\"\n  module = nnx_wrappers.to_linen(\n      MlpBlock,\n      config=config,\n      mesh=mesh,\n      in_features=in_features,\n      intermediate_dim=intermediate_dim,\n      activations=activations,\n      kernel_init=kernel_init,\n      intermediate_dropout_rate=intermediate_dropout_rate,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      use_bias=use_bias,\n      use_pre_norm=use_pre_norm,\n      quant=quant,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "functionality": "Creates an MlpBlock Linen module using nnx.bridge.to_linen.",
            "usage": "Instantiate this function with a Config object, a Mesh object, and the number of input features. Optional arguments allow customization of intermediate dimensions, activation functions, dropout rates, initialization, data types, bias usage, pre-normalization, quantization, model mode, and module name. It returns a Linen module for an MLP block."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama2.py#LlamaDecoderLayer",
        "file_path": "src/MaxText/layers/llama2.py",
        "code_block": "class LlamaDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      model_mode: str,\n      mesh: Mesh,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n  ):\n\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        shard_mode=config.shard_mode,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        shard_mode=config.shard_mode,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        mesh=mesh,\n        quant=self.quant,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n\n    self._maybe_shard_with_logical = functools.partial(\n        maybe_shard_with_logical,\n        mesh=self.mesh,\n        shard_mode=config.shard_mode,\n    )\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n\n    inputs = self._maybe_shard_with_logical(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_sharding = NamedSharding(self.mesh, nn.logical_to_mesh_axes(self.activation_axis_names))\n    lnx = self.pre_self_attention_layer_norm(inputs, out_sharding=lnx_sharding)\n    lnx = self._maybe_shard_with_logical(lnx, self.activation_axis_names)\n\n    # Self-attention block\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n        out_sharding=lnx_sharding,\n    )\n\n    attention_lnx = self._maybe_shard_with_logical(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs, out_sharding=lnx_sharding)\n    hidden_states = self._maybe_shard_with_logical(hidden_states, self.activation_axis_names)\n\n    # MLP block.\n    mlp_intermediate_sharding = NamedSharding(\n        self.mesh,\n        nn.logical_to_mesh_axes((\"activation_batch\", \"activation_length_no_exp\", \"activation_mlp\")),\n    )\n    mlp_lnx = self.mlp(\n        hidden_states,\n        deterministic=deterministic,\n        intermediate_sharding=mlp_intermediate_sharding,\n        out_sharding=lnx_sharding,\n    )\n    mlp_lnx = self._maybe_shard_with_logical(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = self._maybe_shard_with_logical(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "llama_decoder_layer",
            "purpose": "Represents a single decoder layer in the Llama transformer model, incorporating self-attention and a feed-forward network.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies pre-self-attention layer normalization.",
                "Performs self-attention.",
                "Adds the output of self-attention to the input (residual connection).",
                "Applies post-self-attention layer normalization.",
                "Passes the result through a Multi-Layer Perceptron (MLP).",
                "Adds the output of the MLP to the intermediate result (residual connection).",
                "Applies dropout.",
                "Optionally records internal metrics if configured.",
                "Returns the final output, potentially with an auxiliary value if scan_layers is enabled."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "functools.partial",
                "checkpoint_name",
                "NamedSharding",
                "nn.logical_to_mesh_axes",
                "maybe_shard_with_logical",
                "max_utils.get_batch_seq_len_for_mode",
                "quantizations.configure_kv_quant"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions, attention parameters, and other settings.",
                "model_mode": "String indicating the current model mode (e.g., 'prefill' or 'decode').",
                "mesh": "JAX Mesh object for distributed computation.",
                "rngs": "JAX Rngs object for random number generation.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The `activation_axis_names` are determined based on the `model_mode`.",
                "Handles sharding and logical partitioning of tensors using `_maybe_shard_with_logical`.",
                "Supports checkpointing of intermediate activations.",
                "The `Attention` module is configured with various parameters derived from the `config`.",
                "The `__call__` method accepts optional arguments for state management during generation (slot, page_state, previous_chunk)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the LlamaDecoderLayer with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, and quantization settings.",
                        "Determines activation axis names based on model mode.",
                        "Calculates dummy input shape for attention module initialization.",
                        "Initializes RMSNorm for pre-self-attention.",
                        "Initializes the Attention module.",
                        "Initializes RMSNorm for post-self-attention.",
                        "Initializes the MlpBlock.",
                        "Initializes the Dropout layer.",
                        "Partially applies `maybe_shard_with_logical` for convenience."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "Quant",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "functools.partial",
                        "max_utils.get_batch_seq_len_for_mode",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "The `model_mode` influences the shape of activation tensors.",
                        "Various sub-modules like Attention and MLP are initialized with parameters from the `config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str, slot: Optional[int], page_state: Optional[page_manager.PageState], previous_chunk: Optional[Any]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies `_maybe_shard_with_logical` to inputs.",
                        "Applies checkpointing to inputs.",
                        "Applies pre-self-attention layer normalization.",
                        "Applies `_maybe_shard_with_logical` to normalized inputs.",
                        "Performs self-attention using the `self_attention` module.",
                        "Applies `_maybe_shard_with_logical` to attention output.",
                        "Adds attention output to original inputs (residual connection).",
                        "Applies post-self-attention layer normalization.",
                        "Applies `_maybe_shard_with_logical` to normalized hidden states.",
                        "Applies MLP block using the `mlp` module.",
                        "Applies `_maybe_shard_with_logical` to MLP output.",
                        "Adds MLP output to intermediate inputs (residual connection).",
                        "Applies dropout.",
                        "Applies `_maybe_shard_with_logical` to the final layer output.",
                        "Optionally records internal metrics.",
                        "Returns the layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None) if cfg.scan_layers is True",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self._maybe_shard_with_logical",
                        "checkpoint_name",
                        "self.pre_self_attention_layer_norm",
                        "self.self_attention",
                        "self.post_self_attention_layer_norm",
                        "self.mlp",
                        "self.dropout",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size",
                        "NamedSharding",
                        "nn.logical_to_mesh_axes"
                    ],
                    "notes": [
                        "The `decoder_segment_ids` and `decoder_positions` are used by the attention mechanism.",
                        "`deterministic` controls dropout behavior.",
                        "`model_mode` affects sharding and attention logic.",
                        "`slot`, `page_state`, and `previous_chunk` are used for managing KV cache during generation.",
                        "Conditional metric recording is based on `cfg.record_internal_nn_metrics`.",
                        "The return value depends on `cfg.scan_layers`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4UnfoldConvolution",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4UnfoldConvolution(nnx.Module):\n  \"\"\"implementation of Llama4UnfoldConvolution for Llama4 Multi modal model.\n\n  This module extracts patches from input images and projects them to hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.vit_unfold_linear = linears.DenseGeneral(\n        in_features_shape=(\n            self.config.num_channels_for_vit * self.config.patch_size_for_vit * self.config.patch_size_for_vit\n        ),\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=rngs,\n    )\n\n  def __call__(self, inputs: Array) -> Array:\n    batch_size, num_channels, img, _ = inputs.shape\n    num_patches = (img // self.config.patch_size_for_vit) ** 2\n\n    patches = lax.conv_general_dilated_patches(\n        inputs,\n        filter_shape=[self.config.patch_size_for_vit, self.config.patch_size_for_vit],\n        window_strides=[self.config.patch_size_for_vit, self.config.patch_size_for_vit],\n        padding=\"VALID\",\n        dimension_numbers=(\"NCHW\", \"HWIO\", \"NCHW\"),\n        precision=lax.Precision(self.config.matmul_precision),\n        preferred_element_type=self.config.dtype_mm,\n    )\n\n    patches = patches.reshape(\n        batch_size, num_channels * self.config.patch_size_for_vit * self.config.patch_size_for_vit, num_patches\n    )\n    patches = patches.transpose(0, 2, 1)\n\n    hidden_states = self.vit_unfold_linear(patches)\n\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_unfold_convolution",
            "purpose": "Extracts patches from input images and projects them to a hidden dimension using a convolutional approach.",
            "input": {
                "shape": "[batch_size, num_channels, height, width]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract patches using `lax.conv_general_dilated_patches`.",
                "Reshape the patches.",
                "Transpose the patches.",
                "Project the patches to the hidden dimension using a `DenseGeneral` layer."
            ],
            "output": {
                "shape": "[batch_size, num_patches, hidden_size_for_vit]"
            },
            "dependencies": [
                "jax.lax",
                "flax.nnx",
                "flax.linen",
                "MaxText.layers.linears.DenseGeneral"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like patch size, number of channels, hidden size, and dtype."
            },
            "notes": [
                "The input shape is assumed to be NCHW (Batch, Channels, Height, Width).",
                "The output shape is derived from the input batch size, the number of patches, and the hidden size specified in the config."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#pixel_shuffle",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def pixel_shuffle(input_tensor: Array, shuffle_ratio: float) -> Array:\n  \"\"\"Apply pixel shuffle operation to the input tensor.\"\"\"\n  batch_size, num_patches, channels = input_tensor.shape\n  patch_size = int(math.sqrt(num_patches))\n\n  # Reshape to [batch_size, patch_size, patch_size, channels]\n  input_tensor = input_tensor.reshape(batch_size, patch_size, patch_size, -1)\n  batch_size, height, width, channels = input_tensor.shape\n\n  # Reshape to [batch_size, height, width * shuffle_ratio, channels / shuffle_ratio]\n  reshaped_tensor = input_tensor.reshape(batch_size, height, int(width * shuffle_ratio), int(channels / shuffle_ratio))\n\n  # Transpose to [batch_size, width * shuffle_ratio, height, channels / shuffle_ratio]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape to [batch_size, height * shuffle_ratio, width * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.reshape(\n      batch_size, int(height * shuffle_ratio), int(width * shuffle_ratio), int(channels / (shuffle_ratio**2))\n  )\n\n  # Transpose to [batch_size, width * shuffle_ratio, height * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape back to [batch_size, num_patches, channels]\n  output_tensor = reshaped_tensor.reshape(batch_size, -1, reshaped_tensor.shape[-1])\n  return output_tensor",
        "analysis": {
            "functionality": "Applies a pixel shuffle operation to an input tensor, rearranging elements to increase spatial resolution while decreasing channel depth.",
            "usage": "This function takes a JAX Array `input_tensor` and a `shuffle_ratio` as input. The `input_tensor` is expected to have a shape like [batch_size, num_patches, channels], where `num_patches` is a perfect square. The `shuffle_ratio` determines how many times the spatial dimensions are increased. The function returns a modified JAX Array with an adjusted shape, typically [batch_size, new_num_patches, new_channels]."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP(nnx.Module):\n  \"\"\"MLP block for Llama4EncoderLayer.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.vit_encoder_layer_mlp_fc1 = linears.DenseGeneral(\n        in_features_shape=self.config.hidden_size_for_vit,\n        out_features_shape=self.config.intermediate_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.vit_encoder_layer_mlp_fc2 = linears.DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, hidden_states: Array) -> Array:\n    hidden_states = self.vit_encoder_layer_mlp_fc1(hidden_states)\n    hidden_states = nnx.gelu(hidden_states, approximate=False)\n    hidden_states = self.vit_encoder_layer_mlp_fc2(hidden_states)\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_mlp",
            "purpose": "Implements a Multi-Layer Perceptron (MLP) block specifically for the Llama4 vision encoder layer.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies the first DenseGeneral layer (fc1).",
                "Applies GELU activation.",
                "Applies the second DenseGeneral layer (fc2)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "flax.linen.nnx.Module",
                "flax.linen.nnx.Rngs",
                "MaxText.layers.linears.DenseGeneral",
                "jax.numpy as jnp"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like hidden_size_for_vit and intermediate_size_for_vit."
            },
            "notes": [
                "The MLP block consists of two linear transformations with a GELU activation in between.",
                "It is designed to be used within the Llama4EncoderLayer for vision processing."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Llama4VisionMLP module with configuration and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the config and rngs.",
                        "Initializes the first DenseGeneral layer (fc1) with specified input and output dimensions.",
                        "Initializes the second DenseGeneral layer (fc2) with specified input and output dimensions."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.nnx.Module",
                        "flax.linen.nnx.Rngs",
                        "MaxText.layers.linears.DenseGeneral"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the MLP block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies the first DenseGeneral layer (fc1).",
                        "Applies GELU activation.",
                        "Applies the second DenseGeneral layer (fc2)."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "flax.linen.nnx.Module",
                        "MaxText.layers.linears.DenseGeneral",
                        "jax.numpy as jnp"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP2",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP2(nnx.Module):\n  \"\"\"MLP block for Llama4VisionPixelShuffleMLP.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.vit_pixel_shuffle_mlp_fc1 = linears.DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.projector_input_dim_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.vit_pixel_shuffle_mlp_fc2 = linears.DenseGeneral(\n        in_features_shape=self.config.projector_input_dim_for_vit,\n        out_features_shape=self.config.projector_output_dim_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.dropout = linears.Dropout(rate=self.config.projector_dropout_for_vit, rngs=self.rngs)\n\n  def __call__(self, hidden_states: Array, deterministic: bool = False) -> Array:\n    hidden_states = self.vit_pixel_shuffle_mlp_fc1(hidden_states)\n    hidden_states = nnx.gelu(hidden_states, approximate=False)\n    hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n    hidden_states = self.vit_pixel_shuffle_mlp_fc2(hidden_states)\n    hidden_states = nnx.gelu(hidden_states, approximate=False)\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_mlp2",
            "purpose": "Implements a two-layer Multi-Layer Perceptron (MLP) block with GELU activations and dropout, specifically designed for the Llama4VisionPixelShuffleMLP.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies the first DenseGeneral layer (fc1).",
                "Applies GELU activation.",
                "Applies Dropout.",
                "Applies the second DenseGeneral layer (fc2).",
                "Applies GELU activation."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, output_dim]"
            },
            "dependencies": [
                "flax.linen as nn",
                "flax.nnx as nnx",
                "MaxText.layers.linears.DenseGeneral",
                "MaxText.layers.linears.Dropout"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like intermediate_size_for_vit, projector_input_dim_for_vit, projector_output_dim_for_vit, dtype_mm, matmul_precision, and projector_dropout_for_vit."
            },
            "notes": [
                "The input and output dimensions are determined by the config parameters.",
                "Uses nnx.Rngs for random number generation during initialization."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Llama4VisionMLP2 module with DenseGeneral layers and Dropout.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the config and rngs.",
                        "Initializes the first DenseGeneral layer (vit_pixel_shuffle_mlp_fc1).",
                        "Initializes the second DenseGeneral layer (vit_pixel_shuffle_mlp_fc2).",
                        "Initializes the Dropout layer."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx as nnx",
                        "MaxText.layers.linears.DenseGeneral",
                        "MaxText.layers.linears.Dropout"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the MLP block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies vit_pixel_shuffle_mlp_fc1.",
                        "Applies GELU activation.",
                        "Applies dropout.",
                        "Applies vit_pixel_shuffle_mlp_fc2.",
                        "Applies GELU activation."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "nnx.gelu"
                    ],
                    "notes": [
                        "The 'deterministic' flag controls whether dropout is applied."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionPixelShuffleMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionPixelShuffleMLP(nnx.Module):\n  \"\"\"Implementation of Llama4VisionPixelShuffleMLP for Llama4 Multi modal model.\n\n  This module applies pixel shuffle operation and MLP to encoded patches.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.rngs = rngs\n    self.pixel_shuffle_ratio = self.config.pixel_shuffle_ratio_for_vit\n    self.pixel_shuffle_mlp = Llama4VisionMLP2(config=config, rngs=self.rngs)\n\n  def __call__(self, encoded_patches: Array, deterministic: bool = False) -> Array:\n    # Apply pixel shuffle operation\n    encoded_patches = pixel_shuffle(encoded_patches, self.pixel_shuffle_ratio)\n\n    # Apply MLP transformation\n    result = self.pixel_shuffle_mlp(encoded_patches, deterministic=deterministic)\n\n    return result",
        "analysis": {
            "module_type": "llama4_vision_pixel_shuffle_mlp",
            "purpose": "Applies a pixel shuffle operation followed by an MLP transformation to encoded image patches.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply pixel_shuffle operation.",
                "Apply Llama4VisionMLP2 transformation."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, output_dim]"
            },
            "dependencies": [
                "Llama4VisionMLP2",
                "pixel_shuffle"
            ],
            "parameters": {
                "pixel_shuffle_ratio_for_vit": "Ratio for the pixel shuffle operation."
            },
            "notes": [
                "The output dimension of the MLP depends on the 'projector_output_dim_for_vit' in the config."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4MultiModalProjector",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4MultiModalProjector(nnx.Module):\n  \"\"\"Implementation of Llama4MultiModalProjector for Llama4 Multi modal model.\n\n  This module projects vision features to text hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.vit_multi_modal_projector = linears.DenseGeneral(\n        in_features_shape=self.config.vision_output_dim_for_vit,\n        out_features_shape=self.config.base_emb_dim,\n        dtype=self.config.dtype_mm,\n        use_bias=False,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, image_features: Array) -> Array:\n    \"\"\"Project image features to text hidden dimension.\n\n    Args:\n      image_features: Input tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]\n\n    Returns:\n      Tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]\n    \"\"\"\n    b, t, c, d = image_features.shape\n\n    # Reshape image_features to [b * t, c, d] and project to text hidden dimension\n    image_features = image_features.reshape(b * t, c, d)\n    hidden_states = self.vit_multi_modal_projector(image_features)\n    _, c, d = hidden_states.shape\n    hidden_states = hidden_states.reshape(b, t, c, d)\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_multi_modal_projector",
            "purpose": "Projects vision features to the text hidden dimension for a Llama4 multimodal model.",
            "input": {
                "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Reshape input image features.",
                "Project reshaped features using a DenseGeneral layer.",
                "Reshape output to match the desired text hidden dimension format."
            ],
            "output": {
                "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]"
            },
            "dependencies": [
                "nnx.Module",
                "Config",
                "Mesh",
                "nnx.Rngs",
                "linears.DenseGeneral",
                "Array"
            ],
            "parameters": {
                "config.vision_output_dim_for_vit": "The input dimension for the vision features.",
                "config.base_emb_dim": "The output dimension, which is the text hidden dimension.",
                "config.dtype_mm": "Data type for the projection layer.",
                "config.matmul_precision": "Precision for matrix multiplication."
            },
            "notes": [
                "The module uses a DenseGeneral layer for linear projection.",
                "The input shape description in the docstring seems to imply a specific intermediate shape after pixel shuffling, which is then projected.",
                "The output shape description in the docstring also reflects this structure."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Llama4MultiModalProjector module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, and rngs.",
                        "Initialize a DenseGeneral layer for projection."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "linears.DenseGeneral"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Projects input image features to the text hidden dimension.",
                    "input": {
                        "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get dimensions from input shape.",
                        "Reshape input tensor to [batch_size * num_patches, (pixel_shuffle_ratio**2), vision_output_dim].",
                        "Apply the DenseGeneral projection layer.",
                        "Reshape the output tensor to [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]"
                    },
                    "dependencies": [
                        "Array",
                        "linears.DenseGeneral"
                    ],
                    "notes": [
                        "The reshaping operations are crucial for applying the DenseGeneral layer correctly across the batch and patch dimensions.",
                        "The output shape is adjusted to match the expected format for text processing."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#llama4multimodalprojector_as_linen",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def llama4multimodalprojector_as_linen(config: Config, mesh: Mesh):\n  return nnx_wrappers.to_linen(\n      Llama4MultiModalProjector,\n      config=config,\n      mesh=mesh,\n      name=\"Llama4MultiModalProjector_0\",\n      abstract_init=False,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "Converts an NNX Llama4MultiModalProjector module to a Linen module.",
            "usage": "Call this function with a Config object and a Mesh object to get a Linen-compatible Llama4MultiModalProjector module. This module is used for projecting vision features to the text hidden dimension."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_nope_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_nope_layer(layer_id: int, nope_layer_interval: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` should use RoPE or not (NoPE).\n\n  Args:\n    layer_id: The index of the layer.\n    nope_layer_interval: The interval at which layers should use NoPE.\n\n  Returns:\n    True if the layer should use NoPE, False otherwise.\n  \"\"\"\n  return nope_layer_interval is not None and nope_layer_interval > 0 and (layer_id + 1) % nope_layer_interval == 0",
        "analysis": {
            "functionality": "Determines if a layer should use Rotary Positional Embeddings (RoPE) or not (NoPE) based on a layer ID and a specified interval.",
            "usage": "Call this function with the layer's index (`layer_id`) and the interval (`nope_layer_interval`) at which NoPE should be applied. It returns `True` if the layer should use NoPE, and `False` otherwise. The function checks if `nope_layer_interval` is valid (not None and greater than 0) and if the layer's index (plus one) is a multiple of the interval."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_moe_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_moe_layer(layer_id: int, interleave_moe_layer_step: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` is MoE layer.\n\n  This function implements a striding pattern. For example:\n  - If moe_layer_stride is 1, all layers are MoE layers.\n  - If moe_layer_stride is 2, layers with index 1, 3, 5, ... are MoE layers.\n\n  Args:\n    layer_id: The 0-based index of the layer being checked.\n    interleave_moe_layer_step: The interval or stride for placing MoE layers.\n\n  Returns:\n    True if the layer is MoE layer, False otherwise.\n  \"\"\"\n  return (\n      interleave_moe_layer_step is not None\n      and interleave_moe_layer_step > 0\n      and (layer_id + 1) % interleave_moe_layer_step == 0\n  )",
        "analysis": {
            "functionality": "Determines if a given layer is a Mixture-of-Experts (MoE) layer based on a stride.",
            "usage": "Call this function with the layer's index (`layer_id`) and the MoE layer stride (`interleave_moe_layer_step`). It returns `True` if the layer should be an MoE layer, and `False` otherwise. For example, if `interleave_moe_layer_step` is 2, layers with indices 1, 3, 5, etc., will be identified as MoE layers."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4DecoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4DecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer for Llama4.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      is_nope_layer: bool = False,\n      is_moe_layer: bool = False,\n  ):\n    \"\"\"Initializes the Llama4 decoder layer.\n\n    Args:\n      config: The main model configuration object.\n      mesh: The device mesh used for sharding parameters and activations.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: An `nnx.Rngs` object to provide random numbers.\n      quant: An optional configuration for quantization. Defaults to None.\n      is_nope_layer: If True, this layer will be configured as No Position Embeddings layer. Defaults to False.\n      is_moe_layer: If True, this layer will use a MoE block. Defaults to False as Dense.\n    \"\"\"\n\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.rngs = rngs\n    self.is_nope_layer = is_nope_layer\n    self.is_moe_layer = is_moe_layer\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    # Instead of scaling the query values in the checkpoint conversion (`llama_or_mistral_ckpt`)\n    # we'll do it dynamically in the forward pass of Attention\n    query_pre_attn_scalar = config.head_dim**-0.5\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        is_nope_layer=self.is_nope_layer,\n        use_qk_norm=config.use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        temperature_tuning=config.temperature_tuning,\n        temperature_tuning_scale=0.1,\n        temperature_tuning_floor_scale=8192.0,\n        # note: chunk_attn_window_size is set in the config\n        attention_type=AttentionType.GLOBAL if self.is_nope_layer else AttentionType.CHUNK,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    if self.is_moe_layer:\n      # NOTE: the name Llama4MoEBlock_0 is to ensure reverse compatibility with\n      # existing checkpoints for MoE block.\n      self.Llama4MoEBlock_0 = RoutedAndSharedMoE(\n          config=config,\n          mesh=self.mesh,\n          kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n          kernel_axes=(\"embed\", None),\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          quant=self.quant,\n          rngs=self.rngs,\n      )\n    else:\n      self.mlp = MlpBlock(\n          mesh=self.mesh,\n          in_features=config.emb_dim,\n          intermediate_dim=config.mlp_dim,\n          activations=config.mlp_activations,\n          intermediate_dropout_rate=config.dropout_rate,\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          config=config,\n          quant=self.quant,\n          model_mode=model_mode,\n          rngs=self.rngs,\n      )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n    if model_mode == MODEL_MODE_PREFILL:\n      self.activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  @property\n  def moe_block(self):\n    return self.Llama4MoEBlock_0\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n    assert cfg.num_experts >= 1, \"Expected the Llama4 config to have `num_experts > 1`.\"\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    # Self-attention block\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    if self.is_moe_layer:\n      mlp_lnx = self.moe_block(hidden_states)\n    else:\n      mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "Represents a single Transformer decoder layer for the Llama4 model.",
            "usage": "This class is initialized with a configuration object, a device mesh, the model mode (train, prefill, or autoregressive), and random number generators. It can optionally be configured for No Position Embeddings (is_nope_layer) or Mixture of Experts (is_moe_layer). The `__call__` method takes input tensors, segment IDs, positions, and other parameters to perform the forward pass through the layer, including self-attention and a feed-forward network (either a standard MLP or a MoE block). The output is the processed hidden states."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4ScannableBlock",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4ScannableBlock(nnx.Module):\n  \"\"\"A repeatable block given nope_layer_interval and interleave_moe_layer_step.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n      nope_layer_interval: int = 1,\n      interleave_moe_layer_step: int = 1,\n  ):\n    \"\"\"Initializes the scannable block.\n\n    Args:\n      config: The main model configuration object.\n      mesh: The device mesh used for sharding parameters and activations.\n      model_mode: One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.\n      rngs: An `nnx.Rngs` object to provide random numbers for initialization.\n      quant: An optional configuration for quantization. Defaults to None.\n      nope_layer_interval: Specifies the interval for inserting a NoPE layer.\n      interleave_moe_layer_step: Specifies the interval for inserting a MoE layer.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n    self.nope_layer_interval = nope_layer_interval\n    self.interleave_moe_layer_step = interleave_moe_layer_step\n\n    for layer_id in range(self.config.inhomogeneous_layer_cycle_interval):\n      nope_layer = determine_is_nope_layer(layer_id, self.nope_layer_interval)\n      moe_layer = determine_is_moe_layer(layer_id, self.interleave_moe_layer_step)\n      layer_name = f\"layers_{layer_id}\"\n      layer = Llama4DecoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          model_mode=self.model_mode,\n          rngs=self.rngs,\n          quant=self.quant,\n          is_nope_layer=nope_layer,\n          is_moe_layer=moe_layer,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n\n    cfg = self.config\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      y = getattr(self, f\"layers_{layer_id}\")(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "llama4_scannable_block",
            "purpose": "A repeatable block of Llama4 decoder layers, configurable with NoPE and MoE layer intervals.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through a configurable number of layers (inhomogeneous_layer_cycle_interval).",
                "For each layer, determine if it should be a NoPE (No Position Embeddings) layer or an MoE (Mixture of Experts) layer based on provided intervals.",
                "Instantiate a Llama4DecoderLayer with the determined configuration.",
                "Apply each Llama4DecoderLayer sequentially to the input tensor.",
                "Optionally, extract the first element of the output if cfg.scan_layers is True.",
                "Return the final processed tensor and potentially None if cfg.scan_layers is True."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None) if cfg.scan_layers is True"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "nnx.Module",
                "Llama4DecoderLayer",
                "determine_is_nope_layer",
                "determine_is_moe_layer",
                "nnx.Rngs",
                "Quant"
            ],
            "parameters": {
                "config": "The main model configuration object.",
                "mesh": "The device mesh used for sharding parameters and activations.",
                "model_mode": "One of MODEL_MODE_TRAIN, MODEL_MODE_PREFILL, or MODEL_MODE_AUTOREGRESSIVE.",
                "rngs": "An `nnx.Rngs` object to provide random numbers for initialization.",
                "quant": "An optional configuration for quantization.",
                "nope_layer_interval": "Specifies the interval for inserting a NoPE layer.",
                "interleave_moe_layer_step": "Specifies the interval for inserting a MoE layer."
            },
            "notes": [
                "The number of layers in the block is determined by `config.inhomogeneous_layer_cycle_interval`.",
                "The `determine_is_nope_layer` and `determine_is_moe_layer` functions are used to conditionally configure decoder layers.",
                "The `__call__` method applies a sequence of these configured decoder layers.",
                "If `config.scan_layers` is True, the output is a tuple `(output_tensor, None)`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the scannable block by creating and configuring Llama4DecoderLayer instances.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Iterate from 0 to `config.inhomogeneous_layer_cycle_interval - 1`.",
                        "For each `layer_id`, call `determine_is_nope_layer` and `determine_is_moe_layer`.",
                        "Instantiate `Llama4DecoderLayer` with appropriate parameters.",
                        "Set the instantiated layer as an attribute of the module using `setattr`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "Quant",
                        "Llama4DecoderLayer",
                        "determine_is_nope_layer",
                        "determine_is_moe_layer"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass through the sequence of Llama4DecoderLayers.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: N/A, decoder_positions: N/A, deterministic: bool, model_mode: str, slot: Optional[int], page_state: Optional[page_manager.PageState], previous_chunk: Optional[Any]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint to inputs.",
                        "Apply checkpoint name to inputs.",
                        "Initialize output `y` with inputs.",
                        "Iterate through the layers created in `__init__`.",
                        "Call each layer sequentially, passing inputs and other arguments.",
                        "If `cfg.scan_layers` is True, update `y` to be the first element of the layer's output.",
                        "Return the final `y` and potentially `None` if `cfg.scan_layers` is True."
                    ],
                    "output": {
                        "shape": "y: [batch_size, sequence_length, hidden_dim] or (y: [batch_size, sequence_length, hidden_dim], None) if cfg.scan_layers is True"
                    },
                    "dependencies": [
                        "getattr",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "The `slot`, `page_state`, and `previous_chunk` arguments are passed directly to the decoder layers.",
                        "The behavior of the output depends on the `cfg.scan_layers` flag."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoderLayer(nnx.Module):\n  \"\"\"Transformer encoder layer for Llama4 vision model.\"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.hidden_states_shape = (\n        self.config.per_device_batch_size,\n        (self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1,\n        self.config.hidden_size_for_vit,\n    )\n\n    self.input_layer_norm = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.self_attention_vision = Attention(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=(self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=self.hidden_states_shape,\n        inputs_kv_shape=self.hidden_states_shape,\n        float32_qk_product=self.config.float32_qk_product,\n        float32_logits=self.config.float32_logits,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        mesh=self.mesh,\n        dropout_rate=0,\n        name=\"self_attention_vision\",\n        attention_type=AttentionType.FULL,\n        is_nope_layer=False,\n        use_bias_in_projections=True,\n        is_vision=True,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / math.sqrt(self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit),\n        # The vision encoder processes an image in a single forward pass to produce\n        # embeddings. It doesn't have the concept of \"prefill\" and \"autoregressive\"\n        # steps that a text decoder has. Therefore, it doesn't need a KV cache for\n        # its self-attention mechanism.\n        model_mode=MODEL_MODE_TRAIN,\n        rngs=self.rngs,\n    )\n    self.post_attention_layer_norm = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit, epsilon=self.config.normalization_layer_epsilon, rngs=self.rngs\n    )\n    self.Llama4VisionMLP_0 = Llama4VisionMLP(config=self.config, rngs=self.rngs)\n\n  def __call__(\n      self,\n      hidden_states: Array,\n      deterministic: bool = False,\n  ):\n    residual = hidden_states\n    hidden_states = self.input_layer_norm(hidden_states)\n    hidden_states = self.self_attention_vision(\n        inputs_q=hidden_states,\n        inputs_kv=hidden_states,\n        deterministic=deterministic,\n    )\n    hidden_states = residual + hidden_states\n    residual = hidden_states\n    hidden_states = self.post_attention_layer_norm(hidden_states)\n    hidden_states = self.Llama4VisionMLP_0(hidden_states)\n    hidden_states = residual + hidden_states\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_encoder_layer",
            "purpose": "Represents a single transformer encoder layer for the Llama4 vision model.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply input layer normalization.",
                "Perform self-attention on the input.",
                "Add the self-attention output to the original input (residual connection).",
                "Apply post-attention layer normalization.",
                "Pass the result through an MLP block.",
                "Add the MLP output to the result after the first residual connection."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "nnx.LayerNorm",
                "Attention",
                "Llama4VisionMLP",
                "Config",
                "Mesh",
                "nnx.Rngs"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like hidden size, normalization epsilon, etc.",
                "mesh": "JAX device mesh for distributed computation.",
                "rngs": "Random number generators for parameter initialization and dropout."
            },
            "notes": [
                "The `hidden_states_shape` is calculated based on image size, patch size, and batch size from the config.",
                "The `Attention` module is configured for vision-specific use with `is_vision=True` and `attention_type=AttentionType.FULL`.",
                "The `Attention` module is configured without a KV cache as it processes images in a single forward pass.",
                "The `model_mode` for the `Attention` module is set to `MODEL_MODE_TRAIN`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Llama4VisionEncoderLayer with configuration and necessary sub-modules.",
                    "input": {
                        "shape": "config: Config, mesh: Mesh, rngs: nnx.Rngs",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, and rngs.",
                        "Calculate and store the expected hidden states shape.",
                        "Initialize input LayerNorm.",
                        "Initialize the self-attention module for vision.",
                        "Initialize post-attention LayerNorm.",
                        "Initialize the Llama4VisionMLP module."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "nnx.LayerNorm",
                        "Attention",
                        "Llama4VisionMLP"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the encoder layer.",
                    "input": {
                        "shape": "hidden_states: Array, deterministic: bool = False",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store input `hidden_states` as `residual`.",
                        "Apply `input_layer_norm` to `hidden_states`.",
                        "Pass `hidden_states` through `self_attention_vision`.",
                        "Add `residual` to the output of self-attention.",
                        "Store the result as `residual` for the next step.",
                        "Apply `post_attention_layer_norm` to the result.",
                        "Pass the normalized result through `Llama4VisionMLP_0`.",
                        "Add the second `residual` to the MLP output.",
                        "Return the final output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "nnx.LayerNorm",
                        "Attention",
                        "Llama4VisionMLP"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoder",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoder(nnx.Module):\n  \"\"\"Transformer encoder consisting of multiple Llama4VisionEncoderLayer layers.\n\n  This encoder is based on the PyTorch reference implementation and uses multiple\n  encoder layers to process vision input.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"layers_{lyr}\"\n      layer = Llama4VisionEncoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          rngs=self.rngs,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(self, hidden_states: Array, deterministic: bool = False):\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"layers_{lyr}\"\n      layer = getattr(self, layer_name)\n      hidden_states = layer(hidden_states, deterministic=deterministic)\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_encoder",
            "purpose": "Encodes vision input using a stack of Llama4VisionEncoderLayer layers.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through the configured number of vision encoder layers.",
                "For each layer, apply the Llama4VisionEncoderLayer transformation to the hidden states.",
                "Pass the output of one layer as the input to the next."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Llama4VisionEncoderLayer",
                "Config",
                "Mesh",
                "nnx.Module",
                "nnx.Rngs"
            ],
            "parameters": {
                "num_hidden_layers_for_vit": "The number of Llama4VisionEncoderLayer layers to stack in the encoder."
            },
            "notes": [
                "The module initializes each Llama4VisionEncoderLayer dynamically.",
                "The `deterministic` flag controls dropout behavior within the encoder layers."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Llama4VisionEncoder with a specified configuration and device mesh.",
                    "input": {
                        "shape": "config: Config, mesh: Mesh, rngs: nnx.Rngs",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the provided config, mesh, and rngs.",
                        "Iterates from 0 up to (but not including) `config.num_hidden_layers_for_vit`.",
                        "For each iteration, creates an instance of `Llama4VisionEncoderLayer`.",
                        "Assigns each created layer to an attribute named `layers_i` where `i` is the iteration number."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "Llama4VisionEncoderLayer"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the vision encoder.",
                    "input": {
                        "shape": "hidden_states: Array, deterministic: bool = False",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through the initialized encoder layers (e.g., `layers_0`, `layers_1`, ...).",
                        "Applies each layer sequentially to the `hidden_states`.",
                        "The output of one layer becomes the input for the next.",
                        "Passes the `deterministic` flag to each layer."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "Llama4VisionEncoderLayer"
                    ],
                    "notes": [
                        "The `deterministic` flag is propagated to the underlying encoder layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionModel",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionModel(nnx.Module):\n  \"\"\"Llama4 vision model for processing image inputs.\n\n  This model extracts patches from input image tiles and processes them\n  through Llama4VisionEncoder and other vision-specific layers.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs = None):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.scale = self.config.hidden_size_for_vit**-0.5\n    self.num_patches = (self.config.tile_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1\n    self.initializer = nnx.initializers.normal(self.scale)\n\n    self.class_embedding = nnx.Param(\n        self.initializer(self.rngs.params(), (self.config.hidden_size_for_vit,), self.config.dtype_mm)\n    )\n    self.positional_embedding_vlm = nnx.Param(\n        self.initializer(self.rngs.params(), (self.num_patches, self.config.hidden_size_for_vit), self.config.dtype_mm)\n    )\n    self.layernorm_pre = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit,\n        epsilon=self.config.normalization_layer_epsilon,\n        dtype=self.config.dtype_mm,\n        rngs=self.rngs,\n    )\n    self.layernorm_post = nnx.LayerNorm(\n        num_features=self.config.hidden_size_for_vit,\n        epsilon=self.config.normalization_layer_epsilon,\n        dtype=self.config.dtype_mm,\n        rngs=self.rngs,\n    )\n\n    self.Llama4UnfoldConvolution_0 = Llama4UnfoldConvolution(config=self.config, rngs=self.rngs)\n    self.Llama4VisionEncoder_0 = Llama4VisionEncoder(config=self.config, mesh=self.mesh, rngs=self.rngs)\n    self.Llama4VisionPixelShuffleMLP_0 = Llama4VisionPixelShuffleMLP(config=self.config, rngs=self.rngs)\n\n  def __call__(\n      self,\n      pixel_values: Array,\n      output_attentions: None | bool = None,\n      output_hidden_states: None | bool = None,\n      return_dict: None | bool = None,\n      deterministic: None | bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the Llama4 vision model.\n\n    Args:\n      inputs: Input tensor of shape:\n              [batch_size * num_images, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]\n      deterministic: Whether to use deterministic mode (disables dropout)\n\n    Returns:\n      Final hidden states from the vision encoder of shape:\n      [batch_size * num_images, num_tiles, num_patches, vision_output_dim_for_vit]\n    \"\"\"\n    # Reshape pixel values to combine batch and num_tiles dimensions\n    b, t, c, h, w = pixel_values.shape\n    pixel_values = jnp.reshape(pixel_values, [b * t, c, h, w])\n\n    hidden_states = self.Llama4UnfoldConvolution_0(pixel_values)\n\n    # Add class embedding to the beginning of the sequence\n    class_embedding_expanded = jnp.expand_dims(jnp.expand_dims(self.class_embedding, axis=0), axis=0)\n    class_embedding = jnp.broadcast_to(\n        class_embedding_expanded, (hidden_states.shape[0], 1, self.config.hidden_size_for_vit)\n    )\n    hidden_states = jnp.concatenate([hidden_states, class_embedding], axis=1)\n\n    # Add positional embedding\n    hidden_states += self.positional_embedding_vlm\n\n    # Transformation layers\n    hidden_states = self.layernorm_pre(hidden_states)\n    hidden_states = self.Llama4VisionEncoder_0(hidden_states)\n    hidden_states = self.layernorm_post(hidden_states)\n    hidden_states = hidden_states[:, :-1, :]\n\n    hidden_states = self.Llama4VisionPixelShuffleMLP_0(hidden_states)\n\n    # Reshape hidden states\n    _, patch_num, patch_dim = hidden_states.shape\n    hidden_states = jnp.reshape(hidden_states, [b, t, patch_num, patch_dim])\n\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_model",
            "purpose": "Processes image inputs by extracting patches, encoding them through a vision encoder, and applying pixel shuffle and MLP transformations.",
            "input": {
                "shape": "[batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Reshape input pixel values.",
                "Extract patches using Llama4UnfoldConvolution.",
                "Add class embedding.",
                "Add positional embedding.",
                "Apply pre-layer normalization.",
                "Process through Llama4VisionEncoder.",
                "Apply post-layer normalization.",
                "Remove the last token (class token).",
                "Apply Llama4VisionPixelShuffleMLP.",
                "Reshape output hidden states."
            ],
            "output": {
                "shape": "[batch_size, num_tiles, num_patches, vision_output_dim_for_vit]"
            },
            "dependencies": [
                "Llama4UnfoldConvolution",
                "Llama4VisionEncoder",
                "Llama4VisionPixelShuffleMLP",
                "nnx.LayerNorm",
                "nnx.Param"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like hidden size, tile size, patch size, etc.",
                "mesh": "JAX device mesh for sharding.",
                "rngs": "RNGs for parameter initialization."
            },
            "notes": [
                "The input shape is described in the docstring as [batch_size * num_images, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit], but the code reshapes it to [batch_size * num_tiles, ...], implying batch_size * num_images is treated as the new batch dimension.",
                "The output shape is described in the docstring as [batch_size * num_images, num_tiles, num_patches, vision_output_dim_for_vit], but the code reshapes it to [batch_size, num_tiles, num_patches, vision_output_dim_for_vit], where batch_size is the original batch_size from the input shape."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Llama4VisionModel with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, and rngs.",
                        "Initialize scale, number of patches, and initializer.",
                        "Create class_embedding, positional_embedding_vlm, layernorm_pre, and layernorm_post parameters/layers.",
                        "Instantiate Llama4UnfoldConvolution, Llama4VisionEncoder, and Llama4VisionPixelShuffleMLP."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "nnx.Param",
                        "nnx.LayerNorm",
                        "nnx.initializers.normal",
                        "Llama4UnfoldConvolution",
                        "Llama4VisionEncoder",
                        "Llama4VisionPixelShuffleMLP"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the Llama4 vision model.",
                    "input": {
                        "shape": "[batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape pixel_values.",
                        "Pass through Llama4UnfoldConvolution.",
                        "Expand and broadcast class_embedding.",
                        "Concatenate class_embedding with hidden_states.",
                        "Add positional_embedding_vlm.",
                        "Apply layernorm_pre.",
                        "Pass through Llama4VisionEncoder.",
                        "Apply layernorm_post.",
                        "Slice hidden_states to remove the class token.",
                        "Pass through Llama4VisionPixelShuffleMLP.",
                        "Reshape hidden_states to the final output format."
                    ],
                    "output": {
                        "shape": "[batch_size, num_tiles, num_patches, vision_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "jnp.expand_dims",
                        "jnp.broadcast_to",
                        "jnp.concatenate",
                        "Llama4UnfoldConvolution",
                        "Llama4VisionEncoder",
                        "Llama4VisionPixelShuffleMLP"
                    ],
                    "notes": [
                        "The `output_attentions`, `output_hidden_states`, and `return_dict` arguments are not used in the current implementation.",
                        "The `deterministic` argument controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#llama4visionmodel_as_linen",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def llama4visionmodel_as_linen(config: Config, mesh: Mesh) -> nn.Module:\n  return nnx_wrappers.to_linen(\n      Llama4VisionModel,\n      config=config,\n      mesh=mesh,\n      name=\"Llama4VisionModel_0\",\n      abstract_init=False,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "Converts an NNX module (Llama4VisionModel) into a Flax Linen module.",
            "usage": "This function takes a configuration object and a mesh as input and returns a Flax Linen module that wraps the Llama4VisionModel. It's used to integrate NNX models into a Flax Linen-based architecture."
        }
    },
    {
        "block_name": "src/MaxText/layers/mistral.py#MistralDecoderLayer",
        "file_path": "src/MaxText/layers/mistral.py",
        "code_block": "class MistralDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      model_mode: str,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n      quant: None | Quant = None,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.mlp = MlpBlock(\n        mesh=self.mesh,\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        quant=self.quant,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=self.rngs)\n\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      page_state: None | int = None,\n      slot: None | int = None,\n      previous_chunk=None,\n  ):\n\n    cfg = self.config\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # MLP block.\n    mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mistral_decoder_layer",
            "purpose": "Represents a single decoder layer in the Mistral Transformer model, responsible for self-attention and feed-forward processing.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies pre-self-attention layer normalization.",
                "Performs self-attention.",
                "Adds the output of self-attention to the original input (residual connection).",
                "Applies post-self-attention layer normalization.",
                "Processes the result through a Multi-Layer Perceptron (MLP).",
                "Adds the output of the MLP to the intermediate result (another residual connection).",
                "Applies dropout.",
                "Optionally records internal metrics if configured.",
                "Returns the final output of the layer."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "nnx.Rngs",
                "Quant",
                "RMSNorm",
                "Attention",
                "MlpBlock",
                "Dropout",
                "max_utils.get_batch_seq_len_for_mode",
                "nn.with_logical_constraint",
                "checkpoint_name",
                "jnp.mean",
                "jnp.std",
                "jnp.sum",
                "jnp.size"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions, attention parameters, and other settings.",
                "model_mode": "String indicating the current operating mode of the model (e.g., 'training', 'inference').",
                "mesh": "JAX Mesh object for distributed computation.",
                "rngs": "JAX nnx.Rngs object for random number generation.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The layer uses residual connections and layer normalization (RMSNorm).",
                "It incorporates self-attention and a feed-forward network (MLP).",
                "Supports optional quantization.",
                "Includes logic for recording internal metrics and conditional output based on `cfg.scan_layers`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MistralDecoderLayer with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, quantization settings, and RNGs.",
                        "Determines dummy input shapes based on config and model mode.",
                        "Initializes pre-self-attention RMSNorm.",
                        "Initializes the Attention module.",
                        "Initializes post-self-attention RMSNorm.",
                        "Initializes the MlpBlock.",
                        "Initializes the Dropout layer.",
                        "Sets activation axis names."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "Quant",
                        "RMSNorm",
                        "Attention",
                        "MlpBlock",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "The `__init__` method sets up all sub-modules required for a single decoder layer."
                    ]
                },
                "__call__": {
                    "purpose": "Performs a forward pass through the decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dimension], decoder_segment_ids: N/A, decoder_positions: N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints and checkpointing to inputs.",
                        "Applies pre-self-attention layer normalization.",
                        "Performs self-attention using the normalized inputs.",
                        "Applies logical constraints to attention output.",
                        "Adds attention output to original inputs (residual connection).",
                        "Applies post-self-attention layer normalization.",
                        "Applies logical constraints to normalized intermediate states.",
                        "Processes through the MLP block.",
                        "Applies logical constraints to MLP output.",
                        "Adds MLP output to intermediate inputs (second residual connection).",
                        "Applies dropout.",
                        "Applies logical constraints to the final layer output.",
                        "Optionally records internal metrics.",
                        "Returns the layer output, potentially with an additional None if `cfg.scan_layers` is True."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dimension] or ([batch_size, sequence_length, embedding_dimension], None)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self.pre_self_attention_layer_norm",
                        "self.self_attention",
                        "self.post_self_attention_layer_norm",
                        "self.mlp",
                        "self.dropout",
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size"
                    ],
                    "notes": [
                        "The `__call__` method implements the core logic of the decoder layer.",
                        "It handles various inputs for attention mechanisms and conditional metric recording.",
                        "The return value depends on the `cfg.scan_layers` configuration."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/mixtral.py#MixtralDecoderLayer",
        "file_path": "src/MaxText/layers/mixtral.py",
        "code_block": "class MixtralDecoderLayer(nnx.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  @nn.compact\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant = None,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        prefill_cache_axis_order=tuple(map(int, config.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, config.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, config.compute_axis_order.split(\",\"))),\n        reshape_q=config.reshape_q,\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        model_mode=model_mode,\n        rngs=self.rngs,\n    )\n\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=self.rngs,\n    )\n\n    self.MoeBlock_0 = moe.RoutedMoE(\n        config=config,\n        num_experts=config.num_experts,\n        num_experts_per_tok=config.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=config.mlp_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n\n    self.dropout = Dropout(rate=config.dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    load_balance_loss = None\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx, load_balance_loss = self.MoeBlock_0(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = self.dropout(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if self.config.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mixtral_decoder_layer",
            "purpose": "Represents a single decoder layer in the Mixtral architecture, incorporating self-attention and a Mixture-of-Experts (MoE) block.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies pre-self-attention layer normalization.",
                "Performs self-attention.",
                "Adds the output of self-attention to the original inputs (residual connection).",
                "Applies post-self-attention layer normalization.",
                "Passes the result through a Mixture-of-Experts (MoE) block.",
                "Adds the output of the MoE block to the intermediate result (residual connection).",
                "Applies dropout.",
                "Optionally records internal metrics.",
                "Returns the final output of the layer."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "RMSNorm",
                "Attention",
                "moe.RoutedMoE",
                "Dropout",
                "nnx_wrappers",
                "initializers",
                "max_utils",
                "jax.numpy as jnp",
                "jax.ad_checkpoint.checkpoint_name",
                "flax.linen as nn",
                "flax.nnx as nnx"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions, attention parameters, MoE parameters, etc.",
                "mesh": "JAX Mesh object for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'train', 'predict').",
                "quant": "Optional quantization configuration.",
                "rngs": "JAX Rngs object for random number generation."
            },
            "notes": [
                "The `__init__` method initializes various sub-modules like RMSNorm, Attention, and RoutedMoE based on the provided configuration.",
                "The `__call__` method defines the forward pass of the decoder layer.",
                "Includes support for checkpointing and logical constraints.",
                "Handles optional load balancing loss from the MoE block.",
                "Can record internal neural network metrics if enabled in the config.",
                "Supports `scan_layers` for potential optimization, returning an additional None if enabled."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MixtralDecoderLayer with configuration, mesh, and other necessary parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, model_mode, quant, and rngs.",
                        "Determines dummy input shapes based on config and model_mode.",
                        "Initializes pre-self-attention RMSNorm.",
                        "Initializes Attention module.",
                        "Initializes post-self-attention RMSNorm.",
                        "Initializes RoutedMoE block.",
                        "Initializes Dropout layer.",
                        "Sets activation axis names."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "RMSNorm",
                        "Attention",
                        "moe.RoutedMoE",
                        "Dropout",
                        "max_utils.get_batch_seq_len_for_mode",
                        "quantizations.configure_kv_quant",
                        "initializers.nd_dense_init"
                    ],
                    "notes": [
                        "Uses `@nn.compact` decorator for Flax's neural network module definition.",
                        "Initializes sub-modules with parameters derived from the `config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the Mixtral decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length], deterministic: bool, model_mode: str, previous_chunk: Optional, page_state: Optional, slot: Optional",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies logical constraints and checkpointing to inputs.",
                        "Applies pre-self-attention layer normalization.",
                        "Performs self-attention.",
                        "Applies residual connection after self-attention.",
                        "Applies post-self-attention layer normalization.",
                        "Passes through the MoE block, obtaining MLP output and load balance loss.",
                        "Applies residual connection after MoE block.",
                        "Applies dropout.",
                        "Sows MoE load balance loss if present.",
                        "Sows internal metrics if enabled.",
                        "Returns the layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim] or ([batch_size, sequence_length, hidden_dim], None) if scan_layers is True",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "self.pre_self_attention_layer_norm",
                        "self.self_attention",
                        "self.post_self_attention_layer_norm",
                        "self.MoeBlock_0",
                        "self.dropout",
                        "jnp.mean",
                        "jnp.std",
                        "jnp.sum",
                        "jnp.size",
                        "self.sow"
                    ],
                    "notes": [
                        "The `load_balance_loss` is returned from the MoE block and can be used for training.",
                        "Internal metrics like mean, standard deviation, and fraction of zeros are recorded if `config.record_internal_nn_metrics` is true.",
                        "The `scan_layers` configuration affects the return type."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinenPure",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinenPure(nn.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n  # pylint: enable=attribute-defined-outside-init\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    kwargs[\"model_mode\"] = model_mode\n    return nn.Module.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    kwargs[\"model_mode\"] = model_mode\n    return nn.Module.apply(module, *args, **kwargs)\n\n  def setup(self):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n    self.shared_embedding = embed_as_linen(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        name=\"token_embedder\",\n        config=cfg,\n        mesh=self.mesh,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n    self.decoder = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      self.mtp_block = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n\n  def logits_from_hidden_states(self, hidden_states, deterministic, model_mode):\n    \"\"\"\n    Compute logits from hidden states (wrapping decoder._apply_output_head).\n    This function is only used for vocabulary tiling.\n    \"\"\"\n    # pylint: disable=protected-access\n    logits = self.decoder._apply_output_head(\n        shared_embedding=self.shared_embedding,\n        y=hidden_states,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    return logits\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      encoder_image_masks: None | jnp.ndarray = None,\n      enable_dropout=True,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method=None,\n  ):\n    \"\"\"Applies Transformer decoder-branch on encoded-input and target.\n\n    Args:\n      true_length: (Optional) Prompt length before padding\n      slot: (Optional) An integer representing the decode batch index selected\n        for this request.\n    \"\"\"\n\n    if decoder_segment_ids is not None and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.shared_embedding,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n        image_masks=encoder_image_masks,\n    )\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    if self.is_initializing() and self.config.mtp_num_layers > 0:\n      if decoder_target_tokens is None:\n        dummy_shape = decoder_input_tokens.shape\n        decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.shared_embedding,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n          model_mode=model_mode,\n      )\n\n    return logits",
        "analysis": {
            "module_type": "transformer_linen_pure",
            "purpose": "An autoregressive transformer model implemented using Flax Linen.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim] for decoder_input_tokens and decoder_positions, potentially others for multimodal inputs.",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Initialization of shared embedding layer.",
                "Initialization of optional vision encoder.",
                "Initialization of the decoder.",
                "Conditional initialization of Multi-Token Prediction (MTP) block if enabled.",
                "Forward pass through the decoder to get logits and hidden states.",
                "Conditional processing of MTP block for auxiliary loss calculation during training.",
                "Return logits."
            ],
            "output": {
                "shape": "Logits tensor, shape depends on vocabulary size and batch size."
            },
            "dependencies": [
                "Config",
                "Mesh",
                "Quant",
                "MODEL_MODE_TRAIN",
                "MODEL_MODE_AUTOREGRESSIVE",
                "nn.Module",
                "jnp",
                "embed_as_linen",
                "VisionEncoder",
                "Decoder",
                "MultiTokenPredictionBlock",
                "page_manager.PageState"
            ],
            "parameters": {
                "config": "Configuration object for the transformer model.",
                "mesh": "JAX Mesh for distributed computation.",
                "quant": "Quantization configuration.",
                "model_mode": "Operational mode of the model (e.g., train, prefill, decode)."
            },
            "notes": [
                "The `model_mode` attribute can be different from the `model_mode` passed to `__call__`.",
                "Supports multimodal inputs via `encoder_images` and `encoder_image_masks`.",
                "Includes support for Multi-Token Prediction (MTP) for auxiliary loss calculation.",
                "Handles dummy target tensors during initialization if MTP is enabled.",
                "Raises a ValueError if `decoder_segment_ids` is provided during autoregressive decoding."
            ],
            "methods": {
                "init": {
                    "purpose": "Initializes the model, potentially cloning it with a specified model mode.",
                    "input": {
                        "shape": "Variable, depends on the underlying Flax initialization.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Clones the module with the specified `model_mode`.",
                        "Calls the parent `nn.Module.init` method."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nn.Module.init"
                    ],
                    "notes": []
                },
                "apply": {
                    "purpose": "Applies the model, potentially cloning it with a specified model mode.",
                    "input": {
                        "shape": "Variable, depends on the underlying Flax application.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Clones the module with the specified `model_mode`.",
                        "Calls the parent `nn.Module.apply` method."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nn.Module.apply"
                    ],
                    "notes": []
                },
                "setup": {
                    "purpose": "Initializes the shared embedding layer, optional vision encoder, decoder, and MTP block.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes `shared_embedding` using `embed_as_linen`.",
                        "Initializes `vision_encoder` if `cfg.use_multimodal` is True.",
                        "Initializes `decoder`.",
                        "If `cfg.mtp_num_layers` > 0, initializes `mtp_block` using a blueprint from the decoder layers."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "embed_as_linen",
                        "VisionEncoder",
                        "Decoder",
                        "MultiTokenPredictionBlock"
                    ],
                    "notes": []
                },
                "logits_from_hidden_states": {
                    "purpose": "Computes logits from hidden states, used for vocabulary tiling.",
                    "input": {
                        "shape": "Requires `hidden_states` tensor.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.decoder._apply_output_head` to compute logits."
                    ],
                    "output": {
                        "shape": "Logits tensor."
                    },
                    "dependencies": [
                        "self.decoder._apply_output_head"
                    ],
                    "notes": [
                        "This method is specifically for vocabulary tiling."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the transformer model.",
                    "input": {
                        "shape": "Requires `decoder_input_tokens` and `decoder_positions`. Accepts optional multimodal inputs and MTP-related targets.",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Validates `decoder_segment_ids` for autoregressive mode.",
                        "Processes multimodal inputs if available, generating `image_embeddings` and `bidirectional_mask`.",
                        "Performs the forward pass through the `decoder` to obtain `logits` and `hidden_state`.",
                        "If initializing and MTP is enabled, creates dummy target tensors.",
                        "If MTP is enabled, calls `self.mtp_block` to compute auxiliary losses.",
                        "Returns the computed `logits`."
                    ],
                    "output": {
                        "shape": "Logits tensor."
                    },
                    "dependencies": [
                        "self.decoder",
                        "self.vision_encoder",
                        "self.mtp_block",
                        "jnp",
                        "multimodal_utils",
                        "MODEL_MODE_AUTOREGRESSIVE",
                        "DECODING_ACTIVE_SEQUENCE_INDICATOR"
                    ],
                    "notes": [
                        "Handles both standard transformer operations and multimodal inputs.",
                        "Integrates Multi-Token Prediction (MTP) during training.",
                        "Includes specific checks for autoregressive decoding segment IDs.",
                        "The `enable_dropout` argument controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#transformer_as_linen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "def transformer_as_linen(\n    config: Config,\n    mesh: Mesh,\n    quant: Quant,\n    model_mode: str = MODEL_MODE_TRAIN,\n    *,\n    name: str | None = None,\n) -> nnx_wrappers.ToLinen | TransformerLinenPure:\n  \"\"\"Constructs a Transformer model as a Linen or NNX module.\n\n  This function returns an autoregressive Transformer model as either a Linen module\n  or an NNX-wrapped module, depending on the `config.enable_nnx` flag. The returned module\n  is suitable for training, evaluation, or decoding.\n\n  If `config.enable_nnx` is True, returns a `TransformerLinen` that wraps the NNX-style\n  Transformer for integration with NNX-specific APIs and workflows.\n  Otherwise, returns a pure Flax Linen implementation (`TransformerLinenPure`).\n\n  Args:\n    config (Config): The configuration object specifying model hyperparameters and options.\n    mesh (Mesh): The JAX sharding mesh for device partitioning.\n    quant (Quant): The quantization module or configuration to use.\n    model_mode (str, optional): The operational mode for the model, e.g.\n      training, prefill, or autoregressive. Defaults to `MODEL_MODE_TRAIN`.\n    name (str, optional): Optional module name for Linen/NNX construction.\n\n  Returns:\n    nnx_wrappers.ToLinen | TransformerLinenPure:\n      A constructed Transformer model compatible with the specified framework (Linen or NNX).\n  \"\"\"\n  if config.enable_nnx:\n    return TransformerLinen(\n        Transformer,\n        args=(),\n        kwargs=nn.FrozenDict(\n            {\n                \"mesh\": mesh,\n                \"config\": config,\n                \"quant\": quant,\n                \"model_mode\": model_mode,\n            }\n        ),\n        metadata_fn=initializers.variable_to_logically_partitioned,\n        name=name,\n    )\n  else:\n    return TransformerLinenPure(config, mesh, quant, model_mode=model_mode, name=name)",
        "analysis": {
            "functionality": "Constructs a Transformer model, returning either a Flax Linen module or an NNX-wrapped module based on a configuration flag.",
            "usage": "Call this function with a Config object, a JAX Mesh, and a Quantization configuration. It returns a Transformer model instance suitable for training, evaluation, or decoding, depending on the `config.enable_nnx` setting."
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinen(nnx_wrappers.ToLinen):\n  \"\"\"Transformer model as a linen module.\"\"\"\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    kwargs[\"model_mode\"] = model_mode\n    return nnx_wrappers.ToLinen.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    kwargs[\"model_mode\"] = model_mode\n    return nnx_wrappers.ToLinen.apply(module, *args, **kwargs)",
        "analysis": {
            "module_type": "transformer_linen_wrapper",
            "purpose": "Wraps a Transformer model to be compatible with Flax Linen, providing init and apply methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Copies the module's keyword arguments, updating 'model_mode'.",
                "Clones the module with the updated keyword arguments.",
                "Calls the parent class's init or apply method with the cloned module and provided arguments."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.ToLinen",
                "MODEL_MODE_TRAIN"
            ],
            "parameters": {
                "model_mode": "The operational mode of the model (e.g., training, inference)."
            },
            "notes": [
                "This class acts as an adapter for NNX Transformer models to be used within a Flax Linen framework.",
                "It ensures that the `model_mode` parameter is correctly handled during initialization and application."
            ],
            "methods": {
                "init": {
                    "purpose": "Initializes the wrapped Transformer model.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Copies and updates `self.kwargs` with the provided `model_mode`.",
                        "Clones the module with the updated arguments.",
                        "Calls `nnx_wrappers.ToLinen.init`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToLinen.init"
                    ],
                    "notes": []
                },
                "apply": {
                    "purpose": "Applies the wrapped Transformer model to input data.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Copies and updates `self.kwargs` with the provided `model_mode`.",
                        "Clones the module with the updated arguments.",
                        "Calls `nnx_wrappers.ToLinen.apply`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToLinen.apply"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#Transformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class Transformer(nnx.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  def __init__(self, config: Config, mesh: Mesh, quant: Quant, *, model_mode: str = MODEL_MODE_TRAIN, rngs: nnx.Rngs):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.model_mode = model_mode\n\n    cfg = self.config\n    mesh = self.mesh\n    self.token_embedder = Embed(\n        mesh=self.mesh,\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        config=cfg,\n        rngs=rngs,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n\n    decoder_linen = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    self.decoder = nnx_wrappers.ToNNX(decoder_linen, rngs=rngs)\n    self.hidden_states = None\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config=cfg, model_mode=model_mode)\n    dummy_decoder_input_tokens = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n    dummy_decoder_positions = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n\n    self.decoder.lazy_init(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=dummy_decoder_input_tokens,\n        decoder_positions=dummy_decoder_positions,\n    )\n\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      mtp_block_linen = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n      self.mtp_block = nnx_wrappers.ToNNX(mtp_block_linen, rngs=rngs)\n\n      self.mtp_block.lazy_init(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=jnp.ones((1, 1, self.config.emb_dim), dtype=self.config.dtype),\n          input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          target_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          target_mask=jnp.ones((1, 1), dtype=jnp.int32),\n          position_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          decoder_segment_ids=jnp.ones((1, 1), dtype=jnp.int32),\n          deterministic=True,\n      )\n\n  def no_op(self, *args, **kwargs):\n    \"\"\"A no-op method to allow the model to be used in a lazy context.\"\"\"\n    return\n\n  def init_cache(self, cache_size: int, batch_size: int, dtype=jnp.float32):\n    \"\"\"Initializes the KV cache for the Transformer.\n\n    Args:\n      cache_size: The maximum size of the KV cache.\n      batch_size: The batch size for which the cache is initialized.\n      dtype: Data type for the cache. Defaults to `jnp.float32`.\n\n    Returns:\n      True if the cache is successfully initialized.\n    \"\"\"\n    return True\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      cache=None,\n      encoder_images: jax.Array | None = None,\n      encoder_image_masks: jax.Array | None = None,\n      enable_dropout=True,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      true_length: int | None = None,\n      slot: int | None = None,\n      page_state: page_manager.PageState | None = None,\n      decoder_target_tokens: jax.Array | None = None,\n      decoder_target_mask: jax.Array | None = None,\n  ):\n    \"\"\"Applies the Zero-1 FSDP wrapped Transformer model.\n\n    This method handles the all-gather operation for model weights before\n    applying the underlying Transformer model, and then releases them.\n\n    Args:\n      decoder_input_tokens: Input tokens for the decoder.\n      decoder_positions: Positional encodings for the decoder inputs.\n      decoder_segment_ids: Segment IDs for the decoder inputs (optional).\n      encoder_images: Encoder images for multimodal models (optional).\n      enable_dropout: Whether to enable dropout. Defaults to True.\n      previous_chunk: Previous chunk for incremental decoding (optional).\n      true_length: True length of the prompt before padding (optional).\n      slot: An integer representing the decode batch index selected for this request (optional).\n      page_state: Page state for paged attention (optional).\n      partition_spec: Partition specification for FSDP all-gather.\n      decoder_target_tokens: Target tokens for the decoder (optional, used in MTP).\n      decoder_target_mask: Target mask for the decoder (optional, used in MTP).\n      nnx_method: Method to call on the NNX module (optional).\n\n    Returns:\n      Logits from the Transformer model.\n    \"\"\"\n    if decoder_segment_ids is not None and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n        image_masks=encoder_image_masks,\n    )\n\n    # Materialize hidden state when vocab tiling is enabled\n    if self.config.num_vocab_tiling > 1:\n      self.hidden_states = hidden_state\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    # if self.is_initializing() and self.config.mtp_num_layers > 0:\n    #   if decoder_target_tokens is None:\n    #     dummy_shape = decoder_input_tokens.shape\n    #     decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n          model_mode=model_mode,\n      )\n\n    return logits",
        "analysis": {
            "module_type": "transformer",
            "purpose": "An autoregressive transformer model that processes input tokens and generates output logits.",
            "input": {
                "shape": "[batch_size, sequence_length] for tokens and positions, potentially others for multimodal inputs.",
                "dtype": "int32 for tokens, float32 or other specified dtype for embeddings/activations."
            },
            "processing_steps": [
                "Token embedding lookup.",
                "Optional vision encoding for multimodal inputs.",
                "Decoder layer processing (self-attention, feed-forward networks).",
                "Optional Multi-Token Prediction (MTP) block processing.",
                "Logit computation."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, vocab_size] for logits."
            },
            "dependencies": [
                "Config",
                "Mesh",
                "Quant",
                "nnx.Rngs",
                "Embed",
                "VisionEncoder",
                "Decoder",
                "nnx_wrappers.ToNNX",
                "MultiTokenPredictionBlock",
                "max_utils.get_batch_seq_len_for_mode",
                "jnp"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like vocab size, embedding dimension, etc.",
                "mesh": "JAX sharding mesh for distributed computation.",
                "quant": "Quantization configuration.",
                "model_mode": "Operational mode of the model (e.g., training, inference).",
                "rngs": "Random number generators for parameter initialization."
            },
            "notes": [
                "The model supports multimodal inputs if `config.use_multimodal` is True.",
                "It includes an optional Multi-Token Prediction (MTP) block for auxiliary loss during training.",
                "The `__call__` method handles various optional arguments for advanced features like caching, paged attention, and incremental decoding.",
                "The `hidden_states` attribute is materialized when vocab tiling is enabled."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Transformer model, including token embedder, optional vision encoder, and decoder.",
                    "input": {
                        "shape": "N/A (constructor)",
                        "dtype": "N/A (constructor)"
                    },
                    "processing_steps": [
                        "Initialize token embedder.",
                        "Initialize optional vision encoder.",
                        "Initialize decoder.",
                        "Lazy initialization of the decoder with dummy inputs.",
                        "Conditional initialization of the Multi-Token Prediction (MTP) block."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Embed",
                        "VisionEncoder",
                        "Decoder",
                        "nnx_wrappers.ToNNX",
                        "MultiTokenPredictionBlock",
                        "max_utils.get_batch_seq_len_for_mode"
                    ],
                    "notes": [
                        "Sets up the MTP block if `config.mtp_num_layers` is greater than 0.",
                        "Uses lazy initialization for the decoder and MTP block."
                    ]
                },
                "no_op": {
                    "purpose": "A no-operation method, likely for compatibility in lazy contexts.",
                    "input": {
                        "shape": "Any",
                        "dtype": "Any"
                    },
                    "processing_steps": [
                        "Return without performing any operations."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "init_cache": {
                    "purpose": "Initializes the KV cache for the Transformer model.",
                    "input": {
                        "shape": "cache_size: int, batch_size: int",
                        "dtype": "dtype: jnp.dtype"
                    },
                    "processing_steps": [
                        "Return True, indicating successful initialization (implementation is a placeholder)."
                    ],
                    "output": {
                        "shape": "bool"
                    },
                    "dependencies": [
                        "jnp"
                    ],
                    "notes": [
                        "This method currently returns True without actual cache initialization logic."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the Transformer model to input tokens and returns logits.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, seq_len], decoder_positions: [batch_size, seq_len], ... (various optional arguments)",
                        "dtype": "int32 for tokens, float32 or other specified dtype for embeddings/activations."
                    },
                    "processing_steps": [
                        "Handle potential ValueError for segment IDs during autoregressive decoding.",
                        "Compute image embeddings if multimodal.",
                        "Determine bidirectional mask for multimodal inputs.",
                        "Apply the decoder to process inputs and generate logits and hidden states.",
                        "Store hidden states if vocab tiling is enabled.",
                        "Apply the MTP block if enabled, using target tokens and mask.",
                        "Return the computed logits."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size] for logits."
                    },
                    "dependencies": [
                        "self.vision_encoder",
                        "self.decoder",
                        "self.token_embedder",
                        "self.mtp_block",
                        "multimodal_utils",
                        "DecoderBlockType",
                        "jnp"
                    ],
                    "notes": [
                        "Supports multimodal inputs via `encoder_images`.",
                        "Handles different `model_mode` values.",
                        "Includes logic for MTP loss calculation if `config.mtp_num_layers > 0`.",
                        "The `cache`, `previous_chunk`, `true_length`, `slot`, and `page_state` arguments are for advanced inference scenarios."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/models.py#ZeroOneTransformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class ZeroOneTransformer(nn.Module):\n  \"\"\"\n  A wrapper for the base Transformer model designed to implement the Zero-1\n  FSDP optimization.\n\n  The goal of this optimization is to reduce communication overhead. In the standard\n  FSDP implementation, an all-gather operation on the model weights is performed twice\n  for each gradient accumulation microbatch (once for the forward pass, once for the backward pass).\n  This class changes that behavior. When enabled, it performs the all-gather operation\n  only *once* per full gradient accumulation step. It gathers the full weights into\n  memory, runs all the microbatch forward and backward passes, and then releases the\n  full weights. This trades higher peak memory usage for significantly reduced\n  network communication, which can improve training speed if sufficient memory is\n  available.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n\n  def setup(self):\n    \"\"\"Sets up the underlying Transformer model.\n\n    This method initializes the `self.model` attribute by calling the\n    `transformer_as_linen` factory function.\n    \"\"\"\n    self.model = transformer_as_linen(self.config, self.mesh, self.quant, self.model_mode)\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      encoder_image_masks: None | jnp.ndarray = None,\n      enable_dropout=True,\n      model_mode=MODEL_MODE_TRAIN,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      partition_spec=None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method: str | None = None,\n  ):\n    \"\"\"Applies the Zero-1 FSDP wrapped Transformer model.\n\n    This method handles the all-gather operation for model weights before\n    applying the underlying Transformer model, and then releases them.\n\n    Args:\n      decoder_input_tokens: Input tokens for the decoder.\n      decoder_positions: Positional encodings for the decoder inputs.\n      decoder_segment_ids: Segment IDs for the decoder inputs (optional).\n      encoder_images: Encoder images for multimodal models (optional).\n      enable_dropout: Whether to enable dropout. Defaults to True.\n      previous_chunk: Previous chunk for incremental decoding (optional).\n      true_length: True length of the prompt before padding (optional).\n      slot: An integer representing the decode batch index selected for this\n        request (optional).\n      page_state: Page state for paged attention (optional).\n      partition_spec: Partition specification for FSDP all-gather.\n      decoder_target_tokens: Target tokens for the decoder (optional, used in\n        MTP).\n      decoder_target_mask: Target mask for the decoder (optional, used in MTP).\n      nnx_method: Method to call on the NNX module (optional).\n\n    Returns:\n      Logits from the Transformer model.\n    \"\"\"\n    if self.is_initializing():\n      return self.model(\n          decoder_input_tokens=decoder_input_tokens,\n          decoder_positions=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          encoder_images=encoder_images,\n          encoder_image_masks=encoder_image_masks,\n          enable_dropout=enable_dropout,\n          model_mode=model_mode,\n          previous_chunk=previous_chunk,\n          true_length=true_length,\n          slot=slot,\n          page_state=page_state,\n      )\n    all_model_weights = all_gather_over_fsdp(\n        self.model.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n    )\n\n    return self.model.apply(\n        all_model_weights,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        encoder_images=encoder_images,\n        encoder_image_masks=encoder_image_masks,\n        enable_dropout=enable_dropout,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n        true_length=true_length,\n        slot=slot,\n        page_state=page_state,\n        mutable=False,\n        decoder_target_tokens=decoder_target_tokens,\n        decoder_target_mask=decoder_target_mask,\n        nnx_method=nnx_method,\n    )",
        "analysis": {
            "module_type": "zero_one_transformer",
            "purpose": "A wrapper for a base Transformer model that implements the Zero-1 FSDP optimization to reduce communication overhead by performing all-gather operations only once per gradient accumulation step.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim] for decoder_input_tokens and decoder_positions, other inputs are optional.",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Check if the model is initializing. If so, call the underlying model directly.",
                "Perform an all-gather operation on the model's variables using `all_gather_over_fsdp`.",
                "Apply the underlying model using the gathered weights and input arguments."
            ],
            "output": {
                "shape": "Logits from the Transformer model, typically [batch_size, sequence_length, vocab_size]."
            },
            "dependencies": [
                "nn.Module",
                "jnp.ndarray",
                "Mesh",
                "Quant",
                "Config",
                "MODEL_MODE_TRAIN",
                "transformer_as_linen",
                "all_gather_over_fsdp",
                "page_manager.PageState"
            ],
            "parameters": {
                "config": "Configuration object for the Transformer model.",
                "mesh": "JAX sharding Mesh for device partitioning.",
                "quant": "Quantization configuration.",
                "model_mode": "Operational mode of the model (e.g., training, prefill)."
            },
            "notes": [
                "This class optimizes communication by gathering all model weights once per gradient accumulation step, trading higher peak memory for reduced network traffic.",
                "The `setup` method initializes the underlying `self.model`.",
                "The `__call__` method handles the Zero-1 FSDP logic."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the underlying Transformer model by calling `transformer_as_linen`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `transformer_as_linen` with configuration, mesh, quantization, and model mode.",
                        "Assign the result to `self.model`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "transformer_as_linen"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Applies the Zero-1 FSDP wrapped Transformer model, handling weight gathering and model application.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]. Other arguments are optional.",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Check if `self.is_initializing()`. If true, call `self.model` directly.",
                        "Gather all model weights using `all_gather_over_fsdp`.",
                        "Apply the `self.model` with the gathered weights and input arguments."
                    ],
                    "output": {
                        "shape": "Logits from the Transformer model."
                    },
                    "dependencies": [
                        "all_gather_over_fsdp",
                        "jnp.ndarray",
                        "page_manager.PageState"
                    ],
                    "notes": [
                        "Handles multimodal inputs if `encoder_images` is provided.",
                        "Supports various optional arguments for decoding and specific model modes."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations(\n    inputs: jax.Array,\n    sort_indices: jax.Array,\n    use_custom_vjp: bool,\n) -> jax.Array:\n  \"\"\"Sort activations by `sort_indices`.\n\n  If `use_custom_vjp=True`, then we use a custom backward pass that\n  reverses the sort order. Specifically, this unsort operation is simply a sort\n  with `jnp.argsort(sort_indices)` as the sort indices. This is only needed in\n  the case where the compiler generates a less efficient backward pass op.\n\n  Note that `use_custom_vjp=True` assumes that `sort_indices` is a permutation\n  of `jnp.arange(inputs.shape[0])`.\n\n  Args:\n    inputs: `(tokens, ...)`-shaped array of input activations to sort.\n    sort_indices: `(tokens,)`-shaped array containing the sort order.\n    use_custom_vjp: Whether to use the explicit backward pass.\n\n  Returns:\n    `(tokens, ...)`-shaped array of input activations sorted by `sort_indices`.\n  \"\"\"\n  assert inputs.shape[0] == sort_indices.shape[0]\n\n  with jax.named_scope(\"sort_activations\"):\n    if use_custom_vjp:\n      return _sort_activations_custom(inputs, sort_indices)\n    return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "Sorts input activations based on provided sort indices, with an option for a custom backward pass.",
            "usage": "Call the function with input activations, sort indices, and a boolean flag to enable a custom backward pass. The input activations should have a shape where the first dimension matches the first dimension of the sort indices. The function returns the sorted activations."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom(inputs: jax.Array, sort_indices: jax.Array) -> jax.Array:\n  \"\"\"Sort functions with custom vjp.\"\"\"\n  return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "Sorts input activations based on provided sort indices.",
            "usage": "Takes an input JAX array and an array of sort indices. It returns the input array with its first dimension reordered according to the sort indices. This function is designed to work with JAX's automatic differentiation, potentially using a custom backward pass for efficiency."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_fwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_fwd(inputs: jax.Array, sort_indices: jax.Array) -> tuple[jax.Array, jax.Array]:\n  \"\"\"Forward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  return _sort_activations_custom(inputs, sort_indices), sort_indices",
        "analysis": {
            "functionality": "Defines the forward pass for a custom VJP (Vector-Jacobian Product) of a sorting operation.",
            "usage": "This function is intended for internal use within JAX's automatic differentiation system. It takes input activations and sorting indices, applies the custom sorting logic via `_sort_activations_custom`, and returns the sorted activations along with the original sorting indices, which are needed for the backward pass."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_bwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_bwd(residuals: jax.Array, grads: jax.Array) -> tuple[jax.Array, None]:\n  \"\"\"Backward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  sort_indices = residuals\n  return _sort_activations_custom(grads, jnp.argsort(sort_indices)), None",
        "analysis": {
            "functionality": "Implements the backward pass for a custom gradient calculation of an activation sorting operation.",
            "usage": "This function is intended to be used as part of a custom_vjp definition in JAX. It takes the residuals from the forward pass (which are the sort indices) and the gradients with respect to the output of the forward pass. It then applies the inverse sorting operation to the gradients using the provided sort indices to compute the gradients for the inputs of the original sorting function."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#random_routing",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def random_routing(rng_key, gate_logits, num_experts_per_tok):\n  \"\"\"Performs random routing of tokens to experts.\n\n  Args:\n    rng_key: A JAX PRNGKey for randomness.\n    gate_logits: A JAX array of shape (batch_size, sequence_length, num_experts)\n      representing the logits for each expert.\n    num_experts_per_tok: The number of experts to select for each token.\n\n  Returns:\n    A tuple containing:\n      - top_k_indices: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the indices of the selected experts for each\n                       token.\n      - top_k_weights: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the weights for the selected experts.\n  \"\"\"\n  bs, seq_len, num_experts = gate_logits.shape\n  indices = jnp.arange(num_experts).repeat(bs * seq_len)\n  selected_num = bs * seq_len * num_experts_per_tok\n  top_k_indices = jax.random.choice(rng_key, indices, shape=(selected_num,)).reshape(bs, seq_len, num_experts_per_tok)\n  top_k_weights = jnp.take_along_axis(gate_logits, top_k_indices, axis=-1)\n  return top_k_weights, top_k_indices",
        "analysis": {
            "functionality": "Performs random routing of tokens to experts by selecting a specified number of experts for each token based on random choices.",
            "usage": "Takes a JAX PRNGKey for randomness, gate logits (batch_size, sequence_length, num_experts), and the number of experts to select per token. Returns the weights and indices of the randomly selected experts for each token."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#GateLogit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class GateLogit(nnx.Module):\n  \"\"\"A layer used to compute gate logits, allowing to return the pre bias values for DeepSeek routing.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Union[Iterable[int], int],\n      out_features_shape: Union[Iterable[int], int],\n      model_name: str,\n      rngs: nnx.Rngs,\n      axis: Union[Iterable[int], int] = -1,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: Tuple[Optional[str], ...] = (),\n      use_bias: bool = False,\n      score_func: str = \"\",\n      quant: Optional[quantizations.AqtQuantization] = None,\n      matmul_precision: str = \"default\",\n  ):\n    \"\"\"Initializes the GateLogit module.\n\n    Attributes:\n      in_features_shape: The shape of the input features.\n      out_features_shape: The shape of the output features, typically the number of experts.\n      model_name: The name of the model.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      axis: The axis or axes over transformation is applied.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      use_bias: Whether to add learnable bias in gate logit scores. When enabled,\n        this bias aids expert load balancing (like in DeepSeek V3), and is not\n        part of the loss calculation.\n      score_func: Scoring function for output normalization before applying bias.\n      quant: The quantization configuration. If None, no quantization is applied.\n      matmul_precision: The precision level for the matrix multiplication.\n    \"\"\"\n    self.in_features_shape = linears.canonicalize_tuple(in_features_shape)\n    self.out_features_shape = linears.canonicalize_tuple(out_features_shape)\n    self.model_name = model_name\n    self.axis = linears.canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.use_bias = use_bias\n    self.score_func = score_func\n    self.quant = quant\n    self.matmul_precision = matmul_precision\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: jax.Array, _initializing: bool = False) -> Tuple[jax.Array, Optional[jax.Array]]:\n\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = linears.normalize_axes(self.axis, inputs.ndim)\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n    kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(norm_axis)))\n    output = linears._compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n    pre_bias_logits = None\n\n    if self.score_func:\n      output = linears._convert_to_activation_function(self.score_func)(output)\n      if self.model_name.startswith(\"deepseek3\"):\n        pre_bias_logits = output\n\n    if self.use_bias:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output, pre_bias_logits",
        "analysis": {
            "module_type": "gate_logit",
            "purpose": "A layer used to compute gate logits for routing tokens to experts, with an option to return pre-bias values for specific routing mechanisms like DeepSeek.",
            "input": {
                "shape": "[batch_size, ..., in_features_shape]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Canonicalize input and output feature shapes and axis.",
                "Initialize kernel weights and optionally bias.",
                "If quantization is enabled, set up the quantized dot general function.",
                "Perform a matrix multiplication (dot_general) between inputs and kernel weights.",
                "Optionally apply a score function (activation) to the output.",
                "If use_bias is enabled, add the bias to the output.",
                "Return the computed output and pre-bias logits if applicable."
            ],
            "output": {
                "shape": "[batch_size, ..., out_features_shape]"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.nnx",
                "flax.linen",
                "numpy",
                "typing",
                "ctypes",
                "MaxText.layers.linears",
                "MaxText.layers.quantizations",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features.",
                "out_features_shape": "The shape of the output features, typically the number of experts.",
                "model_name": "The name of the model, used for conditional logic (e.g., DeepSeek routing).",
                "axis": "The axis or axes over which the transformation is applied.",
                "weight_dtype": "The data type of the kernel weights.",
                "dtype": "The data type for the computation.",
                "kernel_init": "The initializer function for the kernel weight matrix.",
                "kernel_axes": "A tuple of logical axis names for partitioning the kernel.",
                "use_bias": "Whether to add learnable bias.",
                "score_func": "Scoring function for output normalization before applying bias.",
                "quant": "The quantization configuration.",
                "matmul_precision": "The precision level for the matrix multiplication."
            },
            "notes": [
                "The `__call__` method handles the forward pass.",
                "It supports optional quantization and bias.",
                "It can return pre-bias logits for specific routing strategies like DeepSeek.",
                "The `_initializing` flag is used internally during module initialization."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the GateLogit module with configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Canonicalize shapes and axes.",
                        "Initialize kernel parameters using `kernel_init`.",
                        "Initialize bias parameters if `use_bias` is True.",
                        "Set up quantization if `quant` is provided."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "quant_dot_general": {
                    "purpose": "Provides access to the quantized dot general function if quantization is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieve the quantized dot general function from the module's attributes."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the GateLogit module.",
                    "input": {
                        "shape": "[batch_size, ..., in_features_shape]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Convert inputs to the module's dtype.",
                        "Normalize axes for dot general operation.",
                        "Get kernel weights (potentially from quantized module if in serve mode).",
                        "Perform dot general operation using `linears._compute_dot_general_nnx`.",
                        "Optionally apply score function and update pre_bias_logits.",
                        "Optionally add bias.",
                        "Return the output and pre_bias_logits."
                    ],
                    "output": {
                        "shape": "Tuple[jax.Array, Optional[jax.Array]] where the first element is the output logits and the second is pre_bias_logits."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "MaxText.layers.linears",
                        "MaxText.layers.quantizations"
                    ],
                    "notes": [
                        "The `_initializing` flag is used to control behavior during module initialization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedMoE(nnx.Module):\n  \"\"\"Implements a routed MoE block.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      num_experts: int,\n      num_experts_per_tok: int,\n      mesh: jax.sharding.Mesh,\n      kernel_init: attentions.NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      intermediate_dim: int = 2048,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"Initializes the RoutedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      num_experts: Number of experts.\n      num_experts_per_tok: Number of experts for each token.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      intermediate_dim: Intermediate dimension of MoE.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.num_experts = num_experts\n    self.num_experts_per_tok = num_experts_per_tok\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.intermediate_dim = intermediate_dim\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n\n    if self.config.fsdp_shard_on_exp:\n      # special sharding for dsv3\n      self.wi_kernel_axes = (\"embed_no_exp\", None, \"mlp\")\n      self.wo_kernel_axes = (\"embed_no_exp\", \"mlp\", None)\n    else:\n      self.wi_kernel_axes = (\"exp\", \"embed_no_exp\", \"mlp\")\n      self.wo_kernel_axes = (\"exp\", \"mlp\", \"embed_no_exp\")\n\n    self.gate = GateLogit(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.num_experts,\n        model_name=self.config.model_name,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        kernel_init=self.kernel_init,\n        kernel_axes=self.kernel_axes,\n        use_bias=self.config.routed_bias,\n        score_func=self.config.routed_score_func,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # pylint: disable=protected-access\n    self.activation_fn = linears._convert_to_activation_function(self.config.mlp_activations[0])\n\n    kernel_in_axis = np.arange(1)\n    kernel_out_axis = np.arange(1, 2)\n\n    if quantizations.in_serve_mode(self.quant):\n      # During aqt convert state we delete kernel weight from params to save\n      # memory. Instead they are retrieved from the tensors stored in the 'aqt'\n      # collection.\n      self.wi_0 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wi_1 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wo = jnp.zeros((num_experts, intermediate_dim, self.config.emb_dim))\n    else:\n      self.wi_0 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wi_1 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wo = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (self.num_experts, self.intermediate_dim, self.config.emb_dim),\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wo_kernel_axes,\n      )\n\n    if self.config.mlp_bias:\n      wi_bias_axes = (\"exp\", \"activation_mlp\")\n      wo_bias_axes = (\"exp\", \"activation_embed\")\n      wi_bias_shape = (self.num_experts, self.intermediate_dim)\n      wo_bias_shape = (self.num_experts, self.config.emb_dim)\n      self.wi_0_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wi_1_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wo_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wo_bias_shape, self.weight_dtype),\n          sharding=wo_bias_axes,\n      )\n    else:\n      self.wi_0_bias = None\n      self.wi_1_bias = None\n      self.wo_bias = None\n\n  def get_expert_parallelism_size(self):\n    return self.mesh.shape[\"expert\"]\n\n  def get_tensor_parallelism_size(self):\n    return self.mesh.shape[\"tensor\"]\n\n  def get_tensor_transpose_parallelism_size(self):\n    return self.mesh.shape[\"tensor_transpose\"]\n\n  def get_context_autoregressive_parallelism_size(self):\n    return self.mesh.shape[\"context_autoregressive\"]\n\n  def get_topk(self, gate_logits, pre_bias_logits, rngs=None):\n    \"\"\"get topk.\"\"\"\n    # shape of top_k_weights & top_k_indices:\n    # (batch, sequence, num_experts_per_tok).\n    if self.config.use_random_routing:\n      if rngs is None:\n        raise ValueError(\"The random key cannot be None for random routing.\")\n      # Reuse the 'dropout' RNG stream to ensure random routing\n      rng = rngs.dropout()\n      top_k_weights, top_k_indices = random_routing(rng, gate_logits, self.num_experts_per_tok)\n      return top_k_weights, top_k_indices\n\n    if self.config.model_name.startswith(\"deepseek3\"):\n      top_k_weights, top_k_indices = self.deepseek_routing(gate_logits, pre_bias_logits)\n    else:\n      top_k_weights, top_k_indices = jax.lax.top_k(gate_logits, self.num_experts_per_tok)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.DEEPSEEK:\n      top_k_weights = self.deepseek_scale_weights(top_k_weights)\n    elif self.config.decoder_block != ctypes.DecoderBlockType.LLAMA4:\n      top_k_weights = jax.nn.softmax(top_k_weights.astype(jnp.float32), axis=-1).astype(self.dtype)\n\n    # This is the Qwen3-specific normalization of router weights.\n    if self.config.norm_topk_prob:\n      top_k_weights /= top_k_weights.sum(axis=-1, keepdims=True)\n\n    return top_k_weights, top_k_indices\n\n  def deepseek_scale_weights(self, weights):\n    \"\"\"Scales weights according to DeepSeek's v3 reference implementation.\"\"\"\n    # https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L592-L594.\n    if self.config.routed_score_func == \"sigmoid\":\n      weights /= weights.sum(-1, keepdims=True)\n    weights *= self.config.routed_scaling_factor\n    return weights\n\n  def expert_group_mask(self, gate_logits: jax.Array) -> jax.Array:\n    \"\"\"Returns a mask that selects only the top-k groups of experts.\n\n    Groups of experts are selected based on the sum of the top-2 expert scores\n    for each group.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n\n    Returns:\n      Array of shape `(batch, seq, num_experts)` that is 1 for experts in the\n      top-k groups and 0 elsewhere.\n    \"\"\"\n    # Find top groups based on each group's top-2 expert scores, where\n    # `scores_grouped.shape =\n    # (batch * seq, n_routing_groups, experts_per_group)`.\n    scores_grouped = jnp.reshape(\n        gate_logits,\n        gate_logits.shape[:-1] + (self.config.n_routing_groups, -1),\n    )\n    top2_in_group_vals, _ = jax.lax.top_k(scores_grouped, k=2)\n    group_scores = jnp.sum(jnp.astype(top2_in_group_vals, jnp.float32), axis=-1)\n    _, group_idx = jax.lax.top_k(group_scores, k=self.config.topk_routing_group)\n\n    # Mask selected groups so that only those experts are considered.\n    group_mask = jax.nn.one_hot(group_idx, num_classes=self.config.n_routing_groups, dtype=jnp.float32)\n    group_mask = jnp.sum(group_mask, axis=-2)\n\n    # Apply masks and get top-k indices.\n    score_mask_expanded = jnp.broadcast_to(\n        group_mask[..., None],\n        group_mask.shape + (self.num_experts // self.config.n_routing_groups,),\n    )\n    return jnp.reshape(\n        score_mask_expanded,\n        score_mask_expanded.shape[:-2] + (self.num_experts,),\n    )\n\n  def deepseek_routing(self, gate_logits: jax.Array, pre_bias_logits: jax.Array) -> tuple[jax.Array, jax.Array]:\n    \"\"\"DeepSeek routing logit.\n\n    If the configuration does not specify routing groups (`n_routing_groups` is\n    -1), we use a standard top-k routing mechanism. Otherwise, we force all\n    selected experts to be from the a subset of the highest rated expert groups.\n\n    The selection process uses post_bias logits, while the return weights use\n    pre_bias logits.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n      pre_bias_logits: Array of shape `(batch, seq,num_experts)`.\n\n    Returns:\n      - top_k_weights: `(batch, seq, num_experts_per_tok)` array of weight values for\n        each selected expert.\n      - top_k_indices: `(batch, seq, num_experts_per_tok)` array of indices\n        identifying the selected experts for each token.\n    \"\"\"\n    expert_mask = 1 if self.config.n_routing_groups == -1 else self.expert_group_mask(gate_logits)\n    _, top_k_indices = jax.lax.top_k(\n        jnp.where(expert_mask > 0, gate_logits, -jnp.inf),\n        k=self.num_experts_per_tok,\n    )\n    top_k_weights = jnp.take_along_axis(pre_bias_logits, top_k_indices, axis=-1)\n    return top_k_weights, top_k_indices\n\n  def apply_ffn_activation(self, layer_w0, layer_w1):\n    \"\"\"Applies FFN activation function.\"\"\"\n    with jax.named_scope(\"ffn_act\"):\n      if self.config.decoder_block == ctypes.DecoderBlockType.GPT_OSS:\n        layer_w0 = jnp.clip(layer_w0, a_min=None, a_max=self.config.mlp_activations_limit)\n        layer_w1 = jnp.clip(layer_w1, a_min=-self.config.mlp_activations_limit, a_max=self.config.mlp_activations_limit)\n        layer_act = self.activation_fn(layer_w0 * 1.702)\n        glu = jnp.multiply(layer_w0, layer_act)\n        intermediate_layer = jnp.multiply(glu, (layer_w1 + 1))\n      else:\n        layer_act = self.activation_fn(layer_w0)\n        intermediate_layer = jnp.multiply(layer_act, layer_w1)\n      return intermediate_layer.astype(self.dtype)\n\n  def permute(self, inputs, gate_logits, pre_bias_logits, use_custom_sort_vjp=True, rngs=None, roll_to_expert_id=None):\n    \"\"\"Permute tokens to group by expert to fit gmm call.\"\"\"\n    # reshape inputs (batch, sequence, emb) to (batch * sequence, emb)\n    inputs_shape = inputs.shape\n    bsz_times_seq_len = inputs_shape[0] * inputs_shape[1]\n    inputs_2d = jnp.reshape(inputs, (bsz_times_seq_len, inputs_shape[2]))\n    weights, selected_experts = self.get_topk(gate_logits, pre_bias_logits, rngs)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n      # weights will be of shape (batch_size, seq_len, num_experts_per_tok)\n      router_scores = jax.nn.sigmoid(weights.astype(jnp.float32))  # weights are top_k_weights here\n      # Squeeze router_scores to (batch_size * seq_len, num_experts_per_tok)\n      inputs_2d = inputs_2d * router_scores.reshape(bsz_times_seq_len, -1)\n\n    flatten_selected_experts = jnp.ravel(selected_experts)\n    if roll_to_expert_id is not None:\n      flatten_selected_experts = (flatten_selected_experts - roll_to_expert_id) % self.num_experts\n    sorted_selected_experts = jnp.argsort(flatten_selected_experts)\n    # sort inputs for number of selected experts\n    replicated_inputs_2d = jnp.repeat(inputs_2d, self.num_experts_per_tok, axis=0)\n    sorted_inputs = _sort_activations(replicated_inputs_2d, sorted_selected_experts, use_custom_sort_vjp).astype(\n        self.dtype\n    )\n    group_size = jnp.bincount(flatten_selected_experts, length=self.num_experts)\n    # Return the experts for each sorted input.\n    expert_indices = jnp.arange(self.num_experts)\n    sorted_experts = jnp.repeat(\n        expert_indices,\n        repeats=group_size,\n        total_repeat_length=flatten_selected_experts.shape[0],\n    )\n    return (\n        sorted_inputs,\n        sorted_selected_experts,\n        weights,\n        group_size,\n        sorted_experts,\n    )\n\n  def unpermute(\n      self,\n      intermediate,\n      sorted_selected_experts,\n      weights,\n      batch_size,\n      sequence_length,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Unpermute tokens to original order and combine weights.\"\"\"\n\n    unsort_intermediate = _sort_activations(\n        intermediate,\n        jnp.argsort(sorted_selected_experts),\n        use_custom_sort_vjp,\n    )\n    reshaped_weights = jnp.reshape(weights, (-1, self.num_experts_per_tok))\n    reshaped_intermediate = jnp.reshape(\n        unsort_intermediate,\n        (reshaped_weights.shape[0], self.num_experts_per_tok, -1),\n    )\n    with jax.named_scope(\"weight_sum\"):\n      matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n      if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n        # For Llama4, combine using weights of 1 for selected experts\n        reshaped_weights = jnp.ones_like(reshaped_weights)\n      if self.config.float32_weight_sum:\n        reshaped_intermediate = reshaped_intermediate.astype(jnp.float32)\n        reshaped_weights = reshaped_weights.astype(jnp.float32)\n      output = jnp.einsum(\n          \"BKE,BK -> BE\",\n          reshaped_intermediate,\n          reshaped_weights,\n          precision=matmul_precision,\n      )\n    return output.reshape(batch_size, sequence_length, -1).astype(self.dtype)\n\n  @staticmethod\n  def local_permute(\n      inputs,\n      global_group_sizes,\n      local_expert_size,\n      shard_index,\n      is_offset=False,\n      global_sorted_experts=None,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Permutes tokens locally within an expert shard.\n\n    This function prepares the input tokens for processing by the experts\n    located\n    on the current shard. It groups the tokens by their assigned local expert\n    index (0 to local_expert_size - 1).\n\n    Args:\n      inputs: The input data (tokens) assigned to the experts on this shard.\n        Shape `[tokens, emb_dim]`.\n      global_group_sizes: The count of tokens assignments for each global expert\n        across all the batch shards. Shape `[num_batch_shards, num_experts].\n      local_expert_size: The number of experts handled by the current shard.\n      shard_index: The index of the current expert shard (0 to\n        num_expert_parallelism - 1).\n      is_offset: If True, assumes `inputs` are pre-sorted by global expert ID\n        and selects the slice relevant to this shard's assigned experts. If\n        False, assumes that `inputs` corresponding to the shard's experts start\n        from the beginning of the tensor but need to be permuted by expert ID.\n      global_sorted_experts: Global expert IDs for the `inputs` used when\n        `is_offset` is True. Shape `[total_tokens_for_this_shard]`.\n\n    Returns:\n      A tuple containing:\n        sorted_inputs: Input data permuted local expert ID.\n        sorted_indices: Indices used to permute the inputs.\n        local_group_size: Number of tokens assigned to each local expert on this\n          shard.\n        sorted_experts_ids: expert ID corresponding to each token of the permuted\n        inputs.\n    \"\"\"\n\n    # Slice the count of local expert IDs in each batch shard.\n    # all_shard_local_sizes.shape: [expert_shard, local_expert_size]\n    all_shard_local_sizes = jax.lax.dynamic_slice_in_dim(\n        global_group_sizes,\n        shard_index * local_expert_size,\n        local_expert_size,\n        axis=1,\n    )\n    local_sizes = all_shard_local_sizes.reshape(-1)\n\n    # Total count of the local expert IDs is the sum of the counts across all\n    # batch shards, since all batch shards will send their contributions to the\n    # current expert shard.\n    local_group_size = jnp.sum(all_shard_local_sizes, axis=0)\n\n    # In this case, the data that needs to be processed by the local shard\n    # does not start from row 0 but actually starts at\n    # (jnp.concatenate((jnp.array([0]),\n    #  jnp.cumsum(local_group_sizes[:-1]))[shard_id]).\n    # This happens if batches (`inputs`) are replicated across expert shards and\n    # pre-sorted by global Expert ID (via permute()).\n    if is_offset:\n      divided_assignments = jnp.floor_divide(global_sorted_experts, local_expert_size)\n      expert_indices = jnp.where(\n          divided_assignments == shard_index,\n          jnp.mod(global_sorted_experts, local_expert_size),\n          local_expert_size,\n      )\n\n    # In this case the `input` data has been received from the batch shards and\n    # needs to be reorganized in order of local Expert IDs.\n    else:\n      base_indices = jnp.mod(jnp.arange(local_sizes.shape[0]), local_expert_size)\n      expert_indices = jnp.repeat(base_indices, local_sizes, total_repeat_length=inputs.shape[0])\n\n    sorted_indices = jnp.argsort(expert_indices)\n    sorted_inputs = _sort_activations(inputs, sorted_indices, use_custom_sort_vjp)\n    sorted_experts_ids = expert_indices[sorted_indices]\n    return (\n        sorted_inputs,\n        sorted_indices,\n        local_group_size,\n        sorted_experts_ids,\n    )\n\n  @staticmethod\n  def get_all_to_all_params(\n      all_shards_group_sizes,\n      shard_id,\n      num_expert_parallelism,\n      is_batch_sharded=True,\n  ):\n    \"\"\"Generates input offsets, send sizes, output offsets, and receive sizes used for ragged_all_to_all.\"\"\"\n\n    class TransformStrategy(enum.Enum):\n      INPUT_OFFSET = enum.auto()\n      SEND_SIZE = enum.auto()\n      OUTPUT_OFFSET = enum.auto()\n      RECV_SIZE = enum.auto()\n\n    def transform_array(input_array, shard_id, strategy, is_batch_sharded):\n      \"\"\"Transforms the input array based on the specified strategy.\"\"\"\n      # Prepares it for the usage with `ragged_all_to_all` API. The\n      # transformation determines how data is sent and received between shards.\n      if is_batch_sharded:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # Index of input array for the send\n          local_array = input_array[shard_id]\n          return jnp.concatenate((jnp.array([0]), jnp.cumsum(local_array)[:-1]))\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # Size of input array for the send\n          return input_array[shard_id]\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # Received index in the target output\n          zero_row = jnp.zeros((1,) + input_array.shape[1:], dtype=input_array.dtype)\n          array_with_zeros = jnp.concatenate((zero_row, input_array), axis=0)\n          cumulated_array = jnp.cumsum(array_with_zeros, axis=0, dtype=input_array.dtype)\n          return cumulated_array[shard_id]\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array[:, shard_id]\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n      # If the batch is unsharded then we send the same data slice to all other\n      # shards. We also assume each shard will have the local processed inputs\n      # sorted to start from index 0. Finally, len(input_array.shape) == 1 since\n      # there is only one batch shard.\n      else:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # The data on each shard always starts at 0.\n          return jnp.zeros(num_expert_parallelism, dtype=input_array.dtype)\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # The send amount is always the amount of data the current expert\n          # shard needs to process.\n          return jnp.repeat(input_array[shard_id], num_expert_parallelism)\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # The offset in each shard will just be the start of the group which\n          # that shard is responsible for.\n          output_offset = jnp.concatenate((jnp.array([0]), jnp.cumsum(input_array[:-1])))[shard_id]\n          return jnp.repeat(output_offset, num_expert_parallelism)\n        # The amount that each shard receives from all other shards is\n        # equivalent to the group sizes (aka input_array).\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n    input_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.INPUT_OFFSET,\n        is_batch_sharded,\n    )\n    send_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.SEND_SIZE,\n        is_batch_sharded,\n    )\n    output_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.OUTPUT_OFFSET,\n        is_batch_sharded,\n    )\n    recv_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.RECV_SIZE,\n        is_batch_sharded,\n    )\n    return input_offsets, send_sizes, output_offsets, recv_sizes\n\n  def transform_bias(self, experts_index, *biases):\n    \"\"\"Selects bias values for a variable number of bias tensors based on chosen experts.\"\"\"\n    return tuple(bias[experts_index] for bias in biases)\n\n  def sparse_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ):\n    \"\"\"Perform sparse matrix multiplication of inputs and Experts.\"\"\"\n\n    def gmm(inputs, kernel, tiling, group_sizes, expert_assignments):\n      pad_length = self.config.tile_batch_seq\n      hs_shape = inputs.shape\n      # pad length is the 1st dimension of tiling size in gmm call\n      if inputs.shape[0] != expert_assignments.shape[0]:\n        raise ValueError(\"The number of input tokens must match the number of expert\" \" assignments!\")\n      padding_amount = 0\n      if hs_shape[0] % pad_length:\n        padding_amount = pad_length - hs_shape[0] % pad_length\n        inputs = jax.lax.pad(inputs, jnp.array(0.0, dtype=inputs.dtype), [(0, padding_amount, 0), (0, 0, 0)])\n\n      inputs = inputs.astype(self.dtype)\n      kernel = kernel.astype(self.dtype)\n\n      lhs_quantize_dtype, rhs_quantize_dtype = None, None\n      if self.quant is not None:\n        quant_dg = self.quant.quant_dg\n        lhs_quantize_dtype = quant_dg.fwd.dg_quantizer.lhs.numerics.get_dtype()\n        rhs_quantize_dtype = quant_dg.fwd.dg_quantizer.rhs.numerics.get_dtype()\n      m, k, n = inputs.shape[0], inputs.shape[1], kernel.shape[2]\n      tiling = (\n          min(tiling[0], m),\n          min(tiling[1], k),\n          min(tiling[2], n),\n      )\n      if self.config.use_tokamax_gmm:\n        output = tokamax_api.ragged_dot(\n            lhs=inputs,\n            rhs=kernel,\n            group_sizes=group_sizes,\n            precision=jax.lax.Precision.DEFAULT,\n            preferred_element_type=self.dtype,\n            implementation=\"mosaic\",\n        )\n      else:\n        if self.config.megablox:\n          output = mblx.gmm(\n              lhs=inputs,\n              rhs=kernel,\n              group_sizes=group_sizes,\n              preferred_element_type=self.dtype,\n              tiling=tiling,\n              lhs_quantize_dtype=lhs_quantize_dtype,\n              rhs_quantize_dtype=rhs_quantize_dtype,\n              use_qwix_quantization=self.config.use_qwix_quantization,\n          )\n        else:\n          rhs_inputs = kernel\n          if isinstance(kernel, aqt.QTensor):\n            if kernel.bias or kernel.sparsity_mask or len(kernel.scale) > 1:\n              raise ValueError(\"Unsupported usecase for ragged_dot with quantized kernel.\")\n            rhs_inputs = kernel.qvalue\n          with set_xla_metadata(ragged_dot_tiling=\",\".join([str(t) for t in tiling])):\n            output = jax.lax.ragged_dot(\n                lhs=inputs,\n                rhs=rhs_inputs,\n                group_sizes=group_sizes,\n                preferred_element_type=self.dtype,\n            )\n          if isinstance(kernel, aqt.QTensor):\n            # Multiply outputs by the kernely scale\n            scales = jnp.take(kernel.scale[0].squeeze(), indices=expert_assignments, axis=0)\n            if padding_amount > 0:\n              scales = jax.lax.pad(\n                  scales,\n                  jnp.array(0.0, dtype=scales.dtype),\n                  [(0, padding_amount, 0), (0, 0, 0)],\n              )\n            output *= scales\n      if padding_amount > 0:\n        output = output[: hs_shape[0]]\n      return output\n\n    # Currently, we support data, tensor, and expert parallelism with Megablox.\n    # We all gather the input activations over tensor parallelism to follow\n    # https://parsa.epfl.ch/course-info/cs723/papers/Megatron.pdf.\n\n    # Check if the batch should be sharded by expert and whether the batch_size\n    # supports this. For example, for interleaved inference, prefill always has\n    # batch_size=1 while decode can have batch_size > 1.\n    try:\n      is_batch_sharded_by_expert = (\n          \"expert\"\n          in tuple(\n              filter(\n                  lambda tup: tup[0] == \"activation_batch\",\n                  self.config.logical_axis_rules,\n              )\n          )[\n              0\n          ][1]\n      )\n    except:  # pylint: disable=bare-except\n      is_batch_sharded_by_expert = False\n    if is_batch_sharded_by_expert and inputs.shape[0] > 1:\n      batch_logical_axis = \"activation_batch\"\n    else:\n      batch_logical_axis = \"activation_batch_no_exp\"\n\n    if self.get_tensor_transpose_parallelism_size() > 1:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n    else:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n\n    gate_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      pre_bias_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    else:\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits_pspec = None\n\n    # w0, w1, wo needs to be un sharded on fsdp / fsdp_transpose axis, so use\n    # mlp_no_fsdp axis\n    if self.config.fsdp_shard_on_exp:\n      # special sharding for dsv3 to remove overhead between gmm/AG\n      w0_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", \"mlp_no_fsdp\", None))\n    else:\n      w0_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n    if isinstance(w0_kernel, aqt.QTensor):\n      w0_pspec = aqt.partition_spec(w0_pspec, (1,), w0_kernel.dtype, use_bias=False)\n    if isinstance(w1_kernel, aqt.QTensor):\n      w1_pspec = aqt.partition_spec(w1_pspec, (1,), w1_kernel.dtype, use_bias=False)\n    if isinstance(wo_kernel, aqt.QTensor):\n      wo_pspec = aqt.partition_spec(wo_pspec, (1,), wo_kernel.dtype, use_bias=False)\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            input_partition_pspec,\n            gate_logits_pspec,\n            pre_bias_logits_pspec,\n            w0_pspec,\n            w1_pspec,\n            wo_pspec,\n            w0_bias_pspec,\n            w1_bias_pspec,\n            wo_bias_pspec,\n            None,\n        ),\n        out_specs=(nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))),\n        check_vma=False,\n    )\n    def wrapper(x, logits, pre_bias_logits, w0, w1, wo, w0_bias, w1_bias, wo_bias, rngs):\n      batch_size, sequence_length, _ = x.shape\n      expert_axis_name = \"expert\"\n      expert_shard_id = jax.lax.axis_index(expert_axis_name)\n      num_expert_parallelism = self.get_expert_parallelism_size()\n      if self.config.use_ring_of_experts:\n        # The ring-of-experts strategy first duplicates the inputs to all\n        # expert shards, and then routes within each shard.\n\n        # Duplicate inputs to all expert shards.\n        x, logits, pre_bias_logits = tuple(\n            jax.lax.all_gather(z, axis_name=expert_axis_name, tiled=True) for z in (x, logits, pre_bias_logits)\n        )\n\n        # \"Route\" tokens within each shard.\n        num_experts_per_shard = self.config.num_experts // num_expert_parallelism\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x,\n            logits,\n            pre_bias_logits,\n            self.config.use_custom_sort_vjp,\n            roll_to_expert_id=num_experts_per_shard * expert_shard_id,\n        )\n\n        # Filter down to the group sizes that apply to only the experts in the\n        # current shard.\n        group_sizes = group_sizes[:num_experts_per_shard]\n        mask = jnp.arange(x.shape[0]) < jnp.sum(group_sizes)\n        x = jnp.where(mask[:, None], x, 0)\n      else:\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x, logits, pre_bias_logits, self.config.use_custom_sort_vjp, rngs\n        )\n\n        if num_expert_parallelism > 1:\n          batch_axis = \"expert\" if is_batch_sharded_by_expert else \"data\"\n          # get group sizes for all shards\n          local_expert_size = self.config.num_experts // num_expert_parallelism\n          reshaped_group_sizes = jnp.sum(group_sizes.reshape(-1, local_expert_size), axis=1)\n          global_group_sizes = group_sizes\n          if is_batch_sharded_by_expert:\n            all_shards_group_sizes = jax.lax.all_gather(reshaped_group_sizes, axis_name=batch_axis)\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                all_shards_group_sizes,\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n\n            # TODO(ranran): For better performance, we could update output buffer to a smaller\n            # size to replace self.get_expert_parallelism_size() for efficiency,\n            # Or we could apply capacity_factor for excessive experts.\n            # Note: Reducing buffer increase the risk of token dropping under unbalanced distribution.\n\n            # In the worst case, all of the global input data is assigned to each expert in the current shard.\n            # This would result in num_expert_shards * input_size * experts_per_shard assignments. However, if\n            # experts_per_shard > num_experts_per_tok we cannot assign more than num_experts_per_tok to all of the inputs.\n            max_local_experts_per_tok = min(local_expert_size, self.config.num_experts_per_tok)\n            buffer_size = int(\n                num_expert_parallelism\n                * self.config.per_device_batch_size\n                * self.config.max_target_length\n                * max_local_experts_per_tok\n            )\n            output_shape = jnp.zeros((buffer_size, self.config.emb_dim), dtype=x.dtype)\n\n            x = jax.lax.ragged_all_to_all(\n                x,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n            global_group_sizes = jax.lax.all_gather(group_sizes, axis_name=expert_axis_name)\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes,\n                local_expert_size,\n                shard_index=expert_shard_id,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n          else:\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes[None, :],\n                local_expert_size,\n                shard_index=expert_shard_id,\n                is_offset=True,\n                global_sorted_experts=selected_experts,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n\n      if self.config.mlp_bias:\n        w0_bias, w1_bias, wo_bias = self.transform_bias(selected_experts, w0_bias, w1_bias, wo_bias)\n\n      gmm_fn = functools.partial(\n          gmm,\n          group_sizes=group_sizes,\n          expert_assignments=selected_experts,\n      )\n      wi_tile_size = (\n          self.config.tile_batch_seq,\n          self.config.tile_embed_dim,\n          self.config.tile_mlp_dim,\n      )\n      wo_tile_size = (\n          self.config.tile_batch_seq,\n          self.config.tile_mlp_dim,\n          self.config.tile_embed_dim,\n      )\n      layer_w0 = gmm_fn(x, w0, tiling=wi_tile_size)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w0 = jax.lax.psum(layer_w0, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w0 = layer_w0 + w0_bias\n      layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n\n      layer_w1 = gmm_fn(x, w1, tiling=wi_tile_size)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w1 = jax.lax.psum(layer_w1, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w1 = layer_w1 + w1_bias\n      layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      intermediate_layer = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      intermediate_output = gmm_fn(intermediate_layer, wo, tiling=wo_tile_size)\n      if self.get_tensor_parallelism_size() > 1:\n        intermediate_output = jax.lax.psum_scatter(intermediate_output, \"tensor\", scatter_dimension=1, tiled=True)\n      if self.config.mlp_bias:\n        intermediate_output = intermediate_output + wo_bias\n      intermediate_output = adc.checkpoint_name(intermediate_output, \"mlpwo\")\n\n      if self.config.use_ring_of_experts:\n        # Set the outputs of tokens which were not processed to 0.\n        mask = jnp.arange(intermediate_output.shape[0]) < jnp.sum(group_sizes)\n        intermediate_output = jnp.where(mask[:, None], intermediate_output, 0)\n\n        # Unsort and deduplicate the outputs locally.\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n        # Sum up the partial outputs across the expert shards.\n        output = jnp.reshape(output, (-1, sequence_length, self.config.emb_dim))\n        output = jax.lax.psum_scatter(output, expert_axis_name, scatter_dimension=0, tiled=True)\n\n      else:\n        if num_expert_parallelism > 1:\n          original_inputs_first_dim = batch_size * sequence_length * self.config.num_experts_per_tok\n          if sorted_selected_experts.shape[0] != original_inputs_first_dim:\n            raise ValueError(\"original_inputs_first_dim does not match the original tensor\" \" shape!\")\n          output_shape = jnp.zeros(\n              (\n                  original_inputs_first_dim,\n                  self.config.emb_dim // self.get_tensor_parallelism_size(),\n              ),\n              dtype=intermediate_output.dtype,\n          )\n          if is_batch_sharded_by_expert:\n            # locally unpermute back to the original order\n            local_output = _sort_activations(\n                intermediate_output,\n                jnp.argsort(local_sorted_indices),  # pylint: disable=undefined-variable\n                self.config.use_custom_sort_vjp,\n            )\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                jnp.transpose(all_shards_group_sizes),  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                local_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n          else:\n            # If bach is replicated across EP shards then each shard should send\n            # 0..local_shard_size data to the other shards and receive the\n            # local_shard data from all of the other shards using\n            # ragged_all_to_all.\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                reshaped_group_sizes,  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n                is_batch_sharded=False,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                intermediate_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n      return output, None\n\n    if self.config.moe_fsdp_use_two_stage_all_gather:\n      # Unshard on fsdp axis\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n\n      # Unshard on fsdp_transpose axis\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp\", \"embed_tensor_transpose\"))\n\n      # Make sure XLA does not optimize by combining above All-Gather to unshard\n      # on FSDP axis and the subsequent unshard on fsdp_transpose axis\n      w0_kernel = jax.lax.optimization_barrier(w0_kernel)\n      w1_kernel = jax.lax.optimization_barrier(w1_kernel)\n      wo_kernel = jax.lax.optimization_barrier(wo_kernel)\n\n      # Unshard on both fsdp and fsdp_transpose transpose\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n\n    return wrapper(\n        inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias, self.rngs\n    )\n\n  def reshape_and_update_weights(self, weights, indices):\n    \"\"\"reshape and update weights.\"\"\"\n    # input of weights and indices: (batch_size, seq_len, num_experts_per_tok)\n    # output of updated weights: (batch_size, seq_len, num_experts)\n    update_weights = jnp.zeros((weights.shape[0], weights.shape[1], self.num_experts), dtype=self.dtype)\n    index_update = (\n        jnp.arange(weights.shape[0])[:, None, None],\n        jnp.arange(weights.shape[1])[:, None],\n        indices,\n    )\n    update_weights = update_weights.at[index_update].set(weights)\n    return update_weights\n\n  def get_context_partition_and_sub_seq(self, seq_len):\n    cp = self.get_context_autoregressive_parallelism_size()\n    if seq_len % cp != 0:\n      cp = 1\n    sub_seq = seq_len // cp\n    return cp, sub_seq\n\n  def generate_masks_subgroup(self, top_k_indices, softmax_probs):\n    \"\"\"Subgroup mask generation for inference only.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    # Break sequence into subsequences (groups) of tokens, and route only within\n    # each group.\n    top_k_indices = jnp.reshape(top_k_indices, (batch_size, cp, sub_seq, top_k_indices.shape[2]))\n\n    tokens_per_batch = sub_seq * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, cp, sub_seq * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=2)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=3)\n\n    # reshape & update weights\n    softmax_probs = jnp.reshape(\n        softmax_probs,\n        ((batch_size, cp, sub_seq, self.num_experts)),\n    )\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=3) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    # ici_context_parallelism\n    dispatch_mask = jnp.reshape(\n        dispatch_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n    combine_mask = jnp.reshape(\n        combine_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n\n    return dispatch_mask, combine_mask\n\n  def generate_masks(self, top_k_indices, softmax_probs):\n    \"\"\"Generate masks.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n\n    tokens_per_batch = seq_len * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, seq_len * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=1)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, seq_len, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=2)\n\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, seq_len, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=2) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    return dispatch_mask, combine_mask\n\n  # See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details.\n  def load_balance_loss(self, top_k_indices, logits) -> jax.Array:\n    \"\"\"Compute the load balance loss.\"\"\"\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    summed_expert_mask = jnp.sum(expert_mask, axis=2)\n    # Get fraction of tokens dispatched to each expert\n    density = jnp.mean(summed_expert_mask, axis=1)\n    # get fraction of probability allocated to each expert\n    density_prob = jnp.mean(logits, axis=1)\n    loss = jnp.mean(density * density_prob) * (self.num_experts**2) * self.config.load_balance_loss_weight\n    return loss\n\n  def get_einsum(\n      self,\n      rhs_mesh_axes: Tuple[Optional[str], ...] = (),\n      einsum_name: str | None = None,\n  ):\n    \"\"\"Get the Einstein summation.\"\"\"\n\n    # the check is to prevent aqteinsum as einsum op for dispatch and combine\n    # einsums in ase when capacity_factor > 0\n    # this is necessary to load pre-quantized weights in case of inference\n    if self.config.model_call_mode == \"inference\" and einsum_name in (\n        DISPATCH,\n        COMBINE,\n    ):\n      return jnp.einsum\n\n    if self.quant:\n\n      def aqt_einsum(*args, **kwargs):  # pylint: disable=unused-argument\n        # simply skip kwargs, since aqt einsum doesn't support any kwargs\n        # like precision\n        is_aqt = not isinstance(self.quant, quantizations.Fp8Quantization)\n        kw = {\"mesh_axes\": rhs_mesh_axes} if is_aqt else {\"dtype\": self.dtype}\n        return self.quant.einsum(**kw)(*args)  # pytype: disable=attribute-error\n\n      einsum_op = aqt_einsum\n    else:\n      einsum_op = jnp.einsum\n    return einsum_op\n\n  def maybe_all_gather_kernel_weight_in_expert_parallelism(\n      self, kernel: jax.Array, kernel_axes: Tuple[Optional[str], ...]\n  ):\n    \"\"\"All-gather kernel weight in expert parallelism if needed.\"\"\"\n    if self.get_expert_parallelism_size() > 1:\n      # This will trigger all-gather using weight_dtype\n      # relax it unless really necessary in expert parallelism only\n      # Otherwise compiler will handle communication automatically\n      # esp. with int8 quantization, kernel will be all-gathered in int8 instead\n      # of weight_dtype\n      kernel = nn.with_logical_constraint(kernel, kernel_axes)\n    return kernel\n\n  def dense_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[jax.Array, Optional[jax.Array]]:\n    \"\"\"Dense matrix multiplication.\"\"\"\n    # gate_logits: batch, length, expert\n    gate_logits = nn.with_logical_constraint(gate_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits = nn.with_logical_constraint(pre_bias_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    top_k_weights, top_k_indices = self.get_topk(gate_logits, pre_bias_logits, self.rngs)\n    is_llama4_decoder_layer = self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4\n    if is_llama4_decoder_layer:\n      router_scores = jax.nn.sigmoid(top_k_weights.astype(jnp.float32)).astype(self.dtype)\n      inputs = inputs * router_scores\n    else:\n      weights = self.reshape_and_update_weights(top_k_weights, top_k_indices)\n    matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n\n    if self.config.model_call_mode != \"inference\":\n      softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n      loss = self.load_balance_loss(top_k_indices, softmax_probs)\n    else:\n      loss = None\n    batch_size = inputs.shape[0]\n    seq_len = inputs.shape[1]\n\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    if self.config.capacity_factor > 0:\n      # token dropping if needed\n      if self.config.model_call_mode != \"inference\":\n        # TODO(b/425930949): remove this pylint by refactoring the logic here.\n        dispatch_mask, combine_mask = self.generate_masks(\n            top_k_indices, weights  # pylint: disable=undefined-variable,possibly-used-before-assignment\n        )\n        mask_axes = (\"activation_batch\", \"activation_norm_length\", None, None)\n        dispatch_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_embed\",\n        )\n        mlp_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_mlp\",\n        )\n        dispatch_eimsum = \"BSM,BSEC -> EBCM\"\n        mlp_up_einsum = \"EBCM,EMH -> EBCH\"\n        mlp_down_einsum = \"EBCH,EHM -> EBCM\"\n        output_einsum = \"EBCM,BSEC -> BSM\"\n      else:\n        # TODO(b/425930507): Try replacing `softmax_probs` with padded weights\n        # and verify with decode acc tests.\n        softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n        dispatch_mask, combine_mask = self.generate_masks_subgroup(top_k_indices, softmax_probs)\n        if self.get_context_autoregressive_parallelism_size() > 0 and cp == 1:\n          mask_axes = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        else:\n          mask_axes = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        dispatch_eimsum = \"BNSM,BNSEC -> EBNCM\"\n        mlp_up_einsum = \"EBNCM,EMH -> EBNCH\"\n        mlp_down_einsum = \"EBNCH,EHM -> EBNCM\"\n        output_einsum = \"EBNCM,BNSEC -> BNSM\"\n\n        inputs = jnp.reshape(inputs, (batch_size, cp, sub_seq, inputs.shape[2]))\n        inputs = nn.with_logical_constraint(inputs, input_axis)\n\n      dispatch_mask = nn.with_logical_constraint(dispatch_mask, mask_axes)\n      combine_mask = nn.with_logical_constraint(combine_mask, mask_axes)\n\n      with jax.named_scope(\"dispatch\"):\n        # only cp during prefill\n        dispatch = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=DISPATCH)(\n            dispatch_eimsum, inputs, dispatch_mask, precision=matmul_precision\n        )\n        if cp > 1:\n          dispatch = nn.with_logical_constraint(\n              dispatch,\n              (\n                  None,\n                  \"activation_batch_no_exp\",\n                  \"activation_norm_length\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        dispatch = nn.with_logical_constraint(\n            dispatch,\n            dispatch_axis,\n        )\n      with jax.named_scope(\"wi_0\"):\n        w0_kernel_axes = (\"exp\", None, \"mlp\")\n        w0_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w0_kernel, w0_kernel_axes)\n        layer_w0 = self.get_einsum(rhs_mesh_axes=w0_kernel_axes)(\n            mlp_up_einsum, dispatch, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w0_bias = w0_bias[:, None, None, :]\n          layer_w0 = layer_w0 + w0_bias\n\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = nn.with_logical_constraint(\n            layer_w0,\n            mlp_axis,\n        )\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        w1_kernel_axes = (\"exp\", None, \"mlp\")\n        w1_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w1_kernel, w1_kernel_axes)\n        layer_w1 = self.get_einsum(rhs_mesh_axes=w1_kernel_axes)(\n            mlp_up_einsum, dispatch, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w1_bias = w1_bias[:, None, None, :]\n          layer_w1 = layer_w1 + w1_bias\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = nn.with_logical_constraint(\n            layer_w1,\n            mlp_axis,\n        )\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n      with jax.named_scope(\"wo\"):\n        wo_kernel_axes = (\"exp\", \"mlp\", None)\n        wo_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(wo_kernel, wo_kernel_axes)\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=wo_kernel_axes)(\n            mlp_down_einsum,\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          wo_bias = wo_bias[:, None, None, :]\n          intermediate_layer = intermediate_layer + wo_bias\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        if self.config.model_call_mode != \"inference\":\n          intermediate_layer = nn.with_logical_constraint(\n              intermediate_layer,\n              (\n                  \"activation_exp\",\n                  \"activation_batch_no_exp\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"combine\"):\n        # Matmul & element wise operation\n        output = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=COMBINE)(\n            output_einsum,\n            intermediate_layer,\n            combine_mask,\n            precision=matmul_precision,\n        )\n        if output.ndim == 4:\n          output = jnp.reshape(\n              output,\n              (\n                  output.shape[0],\n                  output.shape[1] * output.shape[2],\n                  output.shape[3],\n              ),\n          )\n      return output, loss\n    else:\n      inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n      with jax.named_scope(\"wi_0\"):\n        layer_w0 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w0 = layer_w0 + w0_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        layer_w1 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w1 = layer_w1 + w1_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      with jax.named_scope(\"wo\"):\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=self.wo_kernel_axes)(\n            \"BSEH,EHM -> BSEM\",\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          intermediate_layer = intermediate_layer + wo_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"weight_sum\"):\n        if is_llama4_decoder_layer:\n          weights = self.reshape_and_update_weights(jnp.ones_like(top_k_weights), top_k_indices)\n        if self.config.float32_weight_sum:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n          weights = weights.astype(jnp.float32)\n        # cast to f32 for sum up in einsum op\n        output = jnp.einsum(\n            \"BSEM,BSE -> BSM\",\n            intermediate_layer,\n            weights,\n            precision=matmul_precision,\n        ).astype(self.dtype)\n      return output, None\n\n  def retrieve_quantized_weight(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[aqt.QTensor, aqt.QTensor, aqt.QTensor]:\n    \"\"\"Retrieve quantized weights.\"\"\"\n    # This is called only during tracing. This is to invoke creation of\n    # quantized tensor inside AqtEinsum.  After jit, this will become no-op and\n    # will not affect performance.\n    _ = self.dense_matmul(\n        inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n    )\n\n    w0_kernel = self.variables[\"aqt\"][\"AqtEinsum_0\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    w1_kernel = self.variables[\"aqt\"][\"AqtEinsum_1\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    wo_kernel = self.variables[\"aqt\"][\"AqtEinsum_2\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n\n    w0_kernel = max_utils.unbox_logicallypartioned(w0_kernel)\n    w1_kernel = max_utils.unbox_logicallypartioned(w1_kernel)\n    wo_kernel = max_utils.unbox_logicallypartioned(wo_kernel)\n    return w0_kernel, w1_kernel, wo_kernel\n\n  def __call__(self, inputs: jax.Array) -> tuple[jax.Array, Optional[jax.Array]]:\n    cfg = self.config\n    inputs = inputs.astype(cfg.dtype)\n    gate_logits, pre_bias_logits = self.gate(inputs)\n\n    w0_kernel = jnp.asarray(self.wi_0[...], self.dtype)\n    w1_kernel = jnp.asarray(self.wi_1[...], self.dtype)\n    wo_kernel = jnp.asarray(self.wo[...], self.dtype)\n\n    if cfg.mlp_bias:\n      w0_bias = jnp.asarray(self.wi_0_bias[...], self.dtype)\n      w1_bias = jnp.asarray(self.wi_1_bias[...], self.dtype)\n      wo_bias = jnp.asarray(self.wo_bias[...], self.dtype)\n    else:\n      w0_bias, w1_bias, wo_bias = None, None, None\n\n    if cfg.sparse_matmul:\n      if quantizations.in_serve_mode(self.quant):\n        w0_kernel, w1_kernel, wo_kernel = self.retrieve_quantized_weight(\n            inputs,\n            gate_logits,\n            pre_bias_logits,\n            w0_kernel,\n            w1_kernel,\n            wo_kernel,\n            w0_bias,\n            w1_bias,\n            wo_bias,\n        )\n      return self.sparse_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )\n    else:\n      return self.dense_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )",
        "analysis": {
            "module_type": "routed_moe",
            "purpose": "Implements a routed Mixture-of-Experts (MoE) block, which dynamically selects a subset of experts to process input tokens.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Compute gate logits for each token to determine expert assignments.",
                "Select top-k experts for each token based on gate logits.",
                "Permute tokens to group them by their assigned experts.",
                "Perform sparse matrix multiplication (GMM) with selected experts.",
                "Apply FFN activation.",
                "Unpermute tokens to their original order.",
                "Combine outputs from selected experts.",
                "Calculate load balancing loss (if not in inference mode)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "GateLogit",
                "jax.lax.top_k",
                "jax.lax.ragged_dot",
                "jax.lax.ragged_all_to_all",
                "functools.partial",
                "jax.shard_map",
                "nnx.Param",
                "nnx.Module",
                "ctypes.Config",
                "quantizations.AqtQuantization",
                "attentions.NdInitializer",
                "linears._convert_to_activation_function",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "Configuration object containing various settings for the MoE block, including dimensions, activation functions, and parallelism strategies.",
                "num_experts": "The total number of experts available.",
                "num_experts_per_tok": "The number of experts to select for each input token.",
                "mesh": "The device mesh for distributed computation.",
                "kernel_init": "Initializer for the expert weight matrices.",
                "kernel_axes": "Logical axis names for partitioning the kernel weights.",
                "intermediate_dim": "The hidden dimension size within each expert's feed-forward network.",
                "weight_dtype": "Data type for the expert weight matrices.",
                "dtype": "Data type for computations.",
                "quant": "Quantization configuration, if any."
            },
            "notes": [
                "Supports various parallelism strategies including data, tensor, and expert parallelism.",
                "Includes logic for token dropping and capacity management.",
                "Handles different routing strategies, including random routing and DeepSeek-specific routing.",
                "Supports quantization through `aqt.QTensor` and `quantizations.AqtQuantization`.",
                "Includes options for sparse and dense matrix multiplication.",
                "The `__call__` method orchestrates the forward pass, selecting between sparse and dense matmul based on configuration."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RoutedMoE module with configuration, expert settings, and parallelism parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration and parallelism parameters.",
                        "Initialize the `GateLogit` module for expert routing.",
                        "Initialize expert weight matrices (`wi_0`, `wi_1`, `wo`) and biases if enabled.",
                        "Determine kernel axes based on `fsdp_shard_on_exp` configuration."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "GateLogit",
                        "nnx.Param",
                        "default_bias_init",
                        "quantizations.in_serve_mode"
                    ],
                    "notes": [
                        "Weights are initialized using `kernel_init` and sharded according to `kernel_axes`.",
                        "Biases are initialized if `config.mlp_bias` is True."
                    ]
                },
                "get_expert_parallelism_size": {
                    "purpose": "Returns the size of the expert parallelism dimension from the mesh.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Access the 'expert' dimension from the `self.mesh.shape`."
                    ],
                    "output": {
                        "shape": "Scalar integer",
                        "dtype": "int32"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "get_tensor_parallelism_size": {
                    "purpose": "Returns the size of the tensor parallelism dimension from the mesh.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Access the 'tensor' dimension from the `self.mesh.shape`."
                    ],
                    "output": {
                        "shape": "Scalar integer",
                        "dtype": "int32"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "get_tensor_transpose_parallelism_size": {
                    "purpose": "Returns the size of the tensor transpose parallelism dimension from the mesh.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Access the 'tensor_transpose' dimension from the `self.mesh.shape`."
                    ],
                    "output": {
                        "shape": "Scalar integer",
                        "dtype": "int32"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "get_context_autoregressive_parallelism_size": {
                    "purpose": "Returns the size of the context autoregressive parallelism dimension from the mesh.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Access the 'context_autoregressive' dimension from the `self.mesh.shape`."
                    ],
                    "output": {
                        "shape": "Scalar integer",
                        "dtype": "int32"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "get_topk": {
                    "purpose": "Selects the top-k experts for each token based on gate logits.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_experts]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If `use_random_routing` is enabled, perform random expert selection.",
                        "Otherwise, use `deepseek_routing` or `jax.lax.top_k` to find top experts.",
                        "Apply softmax to weights unless it's a specific decoder block type (DEEPSEEK, LLAMA4).",
                        "Optionally normalize top-k probabilities.",
                        "Return top-k weights and indices."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "random_routing",
                        "deepseek_routing",
                        "jax.lax.top_k",
                        "jax.nn.softmax"
                    ],
                    "notes": [
                        "Handles different routing strategies based on configuration."
                    ]
                },
                "deepseek_scale_weights": {
                    "purpose": "Scales weights according to DeepSeek's v3 reference implementation.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Optionally normalize weights if `routed_score_func` is 'sigmoid'.",
                        "Multiply weights by `config.routed_scaling_factor`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "expert_group_mask": {
                    "purpose": "Generates a mask to select top-k groups of experts based on their scores.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_experts]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape gate logits to group experts.",
                        "Find top-2 expert scores within each group.",
                        "Calculate group scores by summing top-2 scores.",
                        "Identify top-k routing groups.",
                        "Create a one-hot mask for selected groups.",
                        "Expand and reshape the mask to match the number of experts."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_experts]",
                        "dtype": "float32"
                    },
                    "dependencies": [
                        "jax.lax.top_k",
                        "jax.nn.one_hot"
                    ],
                    "notes": []
                },
                "deepseek_routing": {
                    "purpose": "Implements DeepSeek's routing logic, considering expert groups.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_experts]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine an expert mask: either all experts (if `n_routing_groups` is -1) or based on `expert_group_mask`.",
                        "Apply the mask to gate logits (setting masked logits to -infinity).",
                        "Find top-k indices using `jax.lax.top_k` on the masked logits.",
                        "Gather pre-bias logits using the top-k indices."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "expert_group_mask",
                        "jax.lax.top_k",
                        "jnp.where",
                        "jnp.take_along_axis"
                    ],
                    "notes": [
                        "Uses post-bias logits for selection and pre-bias logits for return weights."
                    ]
                },
                "apply_ffn_activation": {
                    "purpose": "Applies the configured FFN activation function to intermediate layer outputs.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_experts, intermediate_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply activation function to `layer_w0`.",
                        "Perform element-wise multiplication between activated `layer_w0` and `layer_w1` (with adjustments for GPT_OSS).",
                        "Return the result cast to `self.dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, num_experts, intermediate_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.named_scope",
                        "jnp.clip",
                        "jnp.multiply"
                    ],
                    "notes": [
                        "Handles specific activation logic for `GPT_OSS` decoder blocks."
                    ]
                },
                "permute": {
                    "purpose": "Reshapes and sorts input tokens to group them by expert assignments for efficient processing.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape inputs to 2D (batch*sequence, hidden_dim).",
                        "Get top-k expert weights and indices using `get_topk`.",
                        "Optionally apply router scores for Llama4.",
                        "Flatten selected expert indices.",
                        "Optionally roll expert IDs.",
                        "Sort selected expert indices.",
                        "Repeat inputs and sort them according to the sorted expert indices.",
                        "Calculate group sizes (number of tokens per expert).",
                        "Determine sorted expert IDs for each token."
                    ],
                    "output": {
                        "shape": "Tuple containing:",
                        "sorted_inputs": "[total_tokens_processed, hidden_dim]",
                        "sorted_selected_experts": "[total_tokens_processed]",
                        "weights": "[batch_size, sequence_length, num_experts_per_tok]",
                        "group_size": "[num_experts]",
                        "sorted_experts": "[total_tokens_processed]"
                    },
                    "dependencies": [
                        "get_topk",
                        "_sort_activations",
                        "jnp.reshape",
                        "jnp.repeat",
                        "jnp.argsort",
                        "jnp.bincount",
                        "jnp.arange"
                    ],
                    "notes": [
                        "This step prepares data for the GMM (General Matrix Multiply) operation."
                    ]
                },
                "unpermute": {
                    "purpose": "Reverses the permutation performed by `permute`, combining expert outputs back into the original token order.",
                    "input": {
                        "shape": "intermediate: [processed_tokens, intermediate_dim], sorted_selected_experts: [processed_tokens], weights: [batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Unsort the intermediate expert outputs using the inverse of `sorted_selected_experts`.",
                        "Reshape weights and intermediate outputs.",
                        "Combine intermediate outputs with expert weights using `jnp.einsum`.",
                        "Reshape the final output to the original batch and sequence dimensions."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "_sort_activations",
                        "jnp.reshape",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "Handles Llama4 specific combination logic where weights are treated as 1."
                    ]
                },
                "local_permute": {
                    "purpose": "Permutes tokens locally within an expert shard for distributed processing.",
                    "input": {
                        "shape": "inputs: [tokens, emb_dim], global_group_sizes: [num_batch_shards, num_experts], local_expert_size: scalar, shard_index: scalar",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Slice local group sizes from global group sizes.",
                        "Calculate local group size by summing across batch shards.",
                        "Determine expert indices based on `is_offset` flag and global expert IDs.",
                        "Sort inputs based on expert indices.",
                        "Determine sorted expert IDs."
                    ],
                    "output": {
                        "shape": "Tuple containing:",
                        "sorted_inputs": "[tokens, emb_dim]",
                        "sorted_indices": "[tokens]",
                        "local_group_size": "[local_expert_size]",
                        "sorted_experts_ids": "[tokens]"
                    },
                    "dependencies": [
                        "_sort_activations",
                        "jax.lax.dynamic_slice_in_dim",
                        "jnp.sum",
                        "jnp.mod",
                        "jnp.repeat",
                        "jnp.argsort"
                    ],
                    "notes": [
                        "This is a static method, intended for use in distributed expert parallelism."
                    ]
                },
                "get_all_to_all_params": {
                    "purpose": "Generates parameters for `ragged_all_to_all` communication.",
                    "input": {
                        "shape": "all_shards_group_sizes: [num_batch_shards, num_experts], shard_id: scalar, num_expert_parallelism: scalar",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines an inner `transform_array` function to calculate offsets and sizes based on `TransformStrategy`.",
                        "Applies `transform_array` for input offsets, send sizes, output offsets, and receive sizes.",
                        "Handles logic for both batch-sharded and unsharded scenarios."
                    ],
                    "output": {
                        "shape": "Tuple of four arrays: input_offsets, send_sizes, output_offsets, recv_sizes.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "enum.Enum",
                        "jnp.concatenate",
                        "jnp.cumsum",
                        "jnp.zeros",
                        "jnp.repeat"
                    ],
                    "notes": [
                        "This is a static method.",
                        "Parameters are tailored for `ragged_all_to_all` communication in distributed settings."
                    ]
                },
                "transform_bias": {
                    "purpose": "Selects bias values for chosen experts from multiple bias tensors.",
                    "input": {
                        "shape": "experts_index: [num_tokens], *biases: variable number of bias tensors",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through each bias tensor.",
                        "Selects elements from each bias tensor using `experts_index`."
                    ],
                    "output": {
                        "shape": "Tuple of bias tensors, each indexed by `experts_index`.",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "sparse_matmul": {
                    "purpose": "Performs sparse matrix multiplication for the MoE layers using GMM (General Matrix Multiply).",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], gate_logits: [batch_size, sequence_length, num_experts], pre_bias_logits: [batch_size, sequence_length, num_experts], w0_kernel, w1_kernel, wo_kernel: expert kernels, w0_bias, w1_bias, wo_bias: expert biases",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines an inner `gmm` function for the core matrix multiplication.",
                        "Handles parallelism and communication using `jax.shard_map` and `ragged_all_to_all`.",
                        "Permutes inputs and selects experts.",
                        "Performs GMM for `wi_0`, `wi_1`, and `wo` kernels.",
                        "Applies FFN activation.",
                        "Combines expert outputs using `unpermute`.",
                        "Calculates load balance loss if not in inference mode."
                    ],
                    "output": {
                        "shape": "Tuple: (output: [batch_size, sequence_length, hidden_dim], loss: Optional[jax.Array])",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "gmm",
                        "jax.shard_map",
                        "jax.lax.ragged_all_to_all",
                        "permute",
                        "unpermute",
                        "apply_ffn_activation",
                        "load_balance_loss",
                        "get_all_to_all_params",
                        "local_permute",
                        "transform_bias",
                        "maybe_all_gather_kernel_weight_in_expert_parallelism"
                    ],
                    "notes": [
                        "This method is complex and handles various distributed strategies like ring-of-experts and expert parallelism.",
                        "It uses `jax.lax.ragged_dot` or Megablox's `mblx.gmm` for the core computation."
                    ]
                },
                "retrieve_quantized_weight": {
                    "purpose": "Retrieves quantized weights from the 'aqt' collection, typically during tracing for inference.",
                    "input": {
                        "shape": "Same as `dense_matmul` or `sparse_matmul`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `dense_matmul` to trigger the creation of quantized tensors.",
                        "Accesses the quantized weights from `self.variables['aqt']`.",
                        "Unboxes logically partitioned weights."
                    ],
                    "output": {
                        "shape": "Tuple of three `aqt.QTensor` objects for `w0_kernel`, `w1_kernel`, and `wo_kernel`.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "dense_matmul",
                        "max_utils.unbox_logicallypartioned",
                        "aqt.QTensor"
                    ],
                    "notes": [
                        "This method is primarily for inference mode and tracing."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the RoutedMoE block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Cast inputs to the configured dtype.",
                        "Compute gate logits and pre-bias logits using the `gate` module.",
                        "Extract expert kernels and biases.",
                        "If `sparse_matmul` is enabled:",
                        "  If in serve mode with quantization, retrieve quantized weights.",
                        "  Call `sparse_matmul`.",
                        "Else (dense matmul):",
                        "  Call `dense_matmul`.",
                        "Return the output and the load balance loss (if applicable)."
                    ],
                    "output": {
                        "shape": "Tuple: (output: [batch_size, sequence_length, hidden_dim], loss: Optional[jax.Array])",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "gate",
                        "sparse_matmul",
                        "dense_matmul",
                        "retrieve_quantized_weight",
                        "quantizations.in_serve_mode"
                    ],
                    "notes": [
                        "The choice between sparse and dense matmul is determined by `config.sparse_matmul`."
                    ]
                },
                "get_context_partition_and_sub_seq": {
                    "purpose": "Calculates context partition size and subsequence length based on sequence length and context autoregressive parallelism.",
                    "input": {
                        "shape": "seq_len: scalar",
                        "dtype": "int32"
                    },
                    "processing_steps": [
                        "Get context autoregressive parallelism size from mesh.",
                        "If sequence length is not divisible by parallelism size, set parallelism size to 1.",
                        "Calculate subsequence length."
                    ],
                    "output": {
                        "shape": "Tuple: (cp: scalar, sub_seq: scalar)",
                        "dtype": "int32"
                    },
                    "dependencies": [
                        "get_context_autoregressive_parallelism_size"
                    ],
                    "notes": []
                },
                "generate_masks_subgroup": {
                    "purpose": "Generates dispatch and combine masks for subgroup processing, primarily for inference.",
                    "input": {
                        "shape": "top_k_indices: [batch_size, sequence_length, num_experts_per_tok], softmax_probs: [batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape indices and probabilities to include context partition.",
                        "Calculate expert capacity per batch.",
                        "Generate expert mask and calculate token counts per expert.",
                        "Truncate mask based on expert capacity.",
                        "Combine masks and calculate token positions within capacity.",
                        "Generate one-hot encoding for token positions.",
                        "Create dispatch and combine masks.",
                        "Reshape masks for subgroup processing."
                    ],
                    "output": {
                        "shape": "Tuple: (dispatch_mask: [batch_size, cp, sub_seq, num_experts, expert_capacity_per_batch], combine_mask: [batch_size, cp, sub_seq, num_experts, expert_capacity_per_batch])",
                        "dtype": "bool"
                    },
                    "dependencies": [
                        "get_context_partition_and_sub_seq",
                        "jax.nn.one_hot",
                        "jnp.reshape",
                        "jnp.cumsum",
                        "jnp.less_equal",
                        "jnp.sum",
                        "math.ceil"
                    ],
                    "notes": [
                        "Handles token dropping based on expert capacity."
                    ]
                },
                "generate_masks": {
                    "purpose": "Generates dispatch and combine masks for token dropping.",
                    "input": {
                        "shape": "top_k_indices: [batch_size, sequence_length, num_experts_per_tok], softmax_probs: [batch_size, sequence_length, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate expert capacity per batch.",
                        "Generate expert mask and calculate token counts per expert.",
                        "Truncate mask based on expert capacity.",
                        "Combine masks and calculate token positions within capacity.",
                        "Generate one-hot encoding for token positions.",
                        "Create dispatch and combine masks."
                    ],
                    "output": {
                        "shape": "Tuple: (dispatch_mask: [batch_size, seq_len, num_experts, expert_capacity_per_batch], combine_mask: [batch_size, seq_len, num_experts, expert_capacity_per_batch])",
                        "dtype": "bool"
                    },
                    "dependencies": [
                        "jax.nn.one_hot",
                        "jnp.reshape",
                        "jnp.cumsum",
                        "jnp.less_equal",
                        "jnp.sum",
                        "math.ceil"
                    ],
                    "notes": [
                        "Handles token dropping based on expert capacity."
                    ]
                },
                "load_balance_loss": {
                    "purpose": "Computes the load balance loss for MoE, encouraging even token distribution among experts.",
                    "input": {
                        "shape": "top_k_indices: [batch_size, sequence_length, num_experts_per_tok], logits: [batch_size, sequence_length, num_experts]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a one-hot expert mask from `top_k_indices`.",
                        "Sum the mask along the expert dimension to get tokens per expert.",
                        "Calculate density (fraction of tokens dispatched to each expert).",
                        "Calculate density probability (fraction of probability allocated to each expert).",
                        "Compute the loss using density, density probability, and `load_balance_loss_weight`."
                    ],
                    "output": {
                        "shape": "Scalar jax.Array",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.nn.one_hot",
                        "jnp.sum",
                        "jnp.mean"
                    ],
                    "notes": [
                        "Based on the Switch Transformer paper."
                    ]
                },
                "get_einsum": {
                    "purpose": "Returns the appropriate Einstein summation function, potentially a quantized version.",
                    "input": {
                        "shape": "rhs_mesh_axes: Tuple[Optional[str], ...], einsum_name: Optional[str]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If in inference mode and `einsum_name` is 'dispatch' or 'combine', return `jnp.einsum`.",
                        "If quantization is enabled, return a custom `aqt_einsum` function.",
                        "Otherwise, return `jnp.einsum`."
                    ],
                    "output": {
                        "shape": "Callable function",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.einsum",
                        "quantizations.Fp8Quantization",
                        "aqt.QTensor"
                    ],
                    "notes": [
                        "Handles specialized einsum operations for quantization and inference."
                    ]
                },
                "maybe_all_gather_kernel_weight_in_expert_parallelism": {
                    "purpose": "Applies `with_logical_constraint` for all-gathering kernel weights in expert parallelism if needed.",
                    "input": {
                        "shape": "kernel: jax.Array, kernel_axes: Tuple[Optional[str], ...]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if expert parallelism size is greater than 1.",
                        "If so, apply `nn.with_logical_constraint` to the kernel."
                    ],
                    "output": {
                        "shape": "jax.Array",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "get_expert_parallelism_size",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "Aims to relax communication unless strictly necessary."
                    ]
                },
                "dense_matmul": {
                    "purpose": "Performs dense matrix multiplication for the MoE layers.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], gate_logits: [batch_size, sequence_length, num_experts], pre_bias_logits: [batch_size, sequence_length, num_experts], w0_kernel, w1_kernel, wo_kernel: expert kernels, w0_bias, w1_bias, wo_bias: expert biases",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraints to gate logits and pre-bias logits.",
                        "Get top-k weights and indices.",
                        "Optionally apply router scores for Llama4.",
                        "Calculate load balance loss if not in inference mode.",
                        "If `capacity_factor > 0` (token dropping enabled):",
                        "  Generate dispatch and combine masks.",
                        "  Perform dispatch and combine operations using `get_einsum`.",
                        "  Perform GMM for `wi_0`, `wi_1`, and `wo` kernels.",
                        "  Apply FFN activation.",
                        "Else (no token dropping):",
                        "  Perform dense matrix multiplications for `wi_0`, `wi_1`, and `wo`.",
                        "  Apply FFN activation.",
                        "Return the output and the loss."
                    ],
                    "output": {
                        "shape": "Tuple: (output: [batch_size, sequence_length, hidden_dim], loss: Optional[jax.Array])",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "get_topk",
                        "reshape_and_update_weights",
                        "load_balance_loss",
                        "generate_masks",
                        "generate_masks_subgroup",
                        "get_einsum",
                        "apply_ffn_activation",
                        "nn.with_logical_constraint",
                        "jax.lax.Precision"
                    ],
                    "notes": [
                        "Handles token dropping logic based on `capacity_factor` and inference mode."
                    ]
                },
                "reshape_and_update_weights": {
                    "purpose": "Reshapes weights to include all experts and updates them with selected expert weights.",
                    "input": {
                        "shape": "weights: [batch_size, seq_len, num_experts_per_tok], indices: [batch_size, seq_len, num_experts_per_tok]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a zero-initialized weight tensor with shape (batch_size, seq_len, num_experts).",
                        "Use `update_weights.at[index_update].set(weights)` to place the selected weights into the correct positions."
                    ],
                    "output": {
                        "shape": "[batch_size, seq_len, num_experts]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.zeros",
                        "jnp.arange"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedAndSharedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedAndSharedMoE(nnx.Module):\n  \"\"\"Implements a block which combines shared and routed experts.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      mesh: jax.sharding.Mesh,\n      kernel_init: NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"nitializes the RoutedAndSharedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n    # NOTE: the name MoeBlock_0 is to ensure reverse compatibility with\n    # existing checkpoints for routed experts.\n    self.MoeBlock_0 = RoutedMoE(\n        config=self.config,\n        num_experts=self.config.num_experts,\n        num_experts_per_tok=self.config.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=self.config.moe_mlp_dim,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n    self.shared_experts = linears.MlpBlock(\n        mesh=self.mesh,\n        in_features=self.config.emb_dim,\n        intermediate_dim=self.config.shared_experts * self.config.moe_mlp_dim,\n        activations=self.config.mlp_activations,\n        intermediate_dropout_rate=self.config.dropout_rate,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        config=self.config,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n\n  @property\n  def routed_moe(self):\n    return self.MoeBlock_0\n\n  def __call__(self, inputs: jax.Array) -> jax.Array:\n    routed_experts, _ = self.routed_moe(inputs)\n    shared_experts = self.shared_experts(inputs)\n    return routed_experts + shared_experts",
        "analysis": {
            "module_type": "routed_and_shared_moe",
            "purpose": "Combines a Mixture-of-Experts (MoE) layer with shared expert layers.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a RoutedMoE module.",
                "Initializes a shared MlpBlock module.",
                "Processes input through RoutedMoE.",
                "Processes input through shared MlpBlock.",
                "Adds the outputs of RoutedMoE and shared MlpBlock."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "RoutedMoE",
                "linears.MlpBlock",
                "ctypes.Config",
                "jax.sharding.Mesh",
                "nnx.Rngs"
            ],
            "parameters": {
                "config": "Configuration object containing settings for MoE and shared experts.",
                "mesh": "Device mesh for distributed computation.",
                "kernel_init": "Initializer for kernel weights.",
                "kernel_axes": "Logical axis names for kernel partitioning.",
                "rngs": "Random number generators for parameter initialization.",
                "weight_dtype": "Data type for kernel weights.",
                "dtype": "Data type for computation.",
                "quant": "Quantization configuration."
            },
            "notes": [
                "The RoutedMoE module is named 'MoeBlock_0' for backward compatibility.",
                "The shared experts are implemented using a standard MlpBlock."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RoutedAndSharedMoE module with configuration and hardware details.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, initializers, and data types.",
                        "Initializes the internal RoutedMoE module.",
                        "Initializes the internal shared MlpBlock module."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RoutedMoE",
                        "linears.MlpBlock"
                    ],
                    "notes": [
                        "Sets up both the routed MoE experts and the shared MLP layers."
                    ]
                },
                "routed_moe": {
                    "purpose": "Provides access to the internal RoutedMoE module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the self.MoeBlock_0 instance."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This is a property getter."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass, combining routed and shared expert outputs.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Passes inputs through the routed MoE layer.",
                        "Passes inputs through the shared experts layer.",
                        "Adds the outputs from both layers."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "RoutedMoE.__call__",
                        "linears.MlpBlock.__call__"
                    ],
                    "notes": [
                        "The final output is the sum of the routed expert outputs and the shared expert outputs."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_gate_logit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_gate_logit(\n    inputs_shape: tuple[int, ...],\n    out_features_shape: Union[Iterable[int], int],\n    model_name: str,\n    axis: Union[Iterable[int], int] = -1,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: Tuple[Optional[str], ...] = (),\n    use_bias: bool = False,\n    score_func: str = \"\",\n    quant: Optional[quantizations.AqtQuantization] = None,\n    matmul_precision: str = \"default\",\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a GateLogit Linen module.\"\"\"\n\n  axis = linears.canonicalize_tuple(axis)\n  in_features_shape = tuple(inputs_shape[ax] for ax in linears.normalize_axes(axis, len(inputs_shape)))\n\n  module = nnx_wrappers.to_linen(\n      GateLogit,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      model_name=model_name,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      use_bias=use_bias,\n      score_func=score_func,\n      quant=quant,\n      matmul_precision=matmul_precision,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "functionality": "This function acts as a factory to create a Flax Linen module for computing gate logits, specifically designed for use with the `GateLogit` class.",
            "usage": "Call `get_gate_logit` with parameters defining the input shape, output features shape, model name, and optional configurations like axis, data types, initializers, bias usage, scoring function, quantization, and matrix multiplication precision. It returns a Flax Linen module instance that can be used within a larger neural network architecture."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_moe(\n    config: ctypes.Config,\n    num_experts: int,\n    num_experts_per_tok: int,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    intermediate_dim: int = 2048,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedMoE,\n      config=config,\n      num_experts=num_experts,\n      num_experts_per_tok=num_experts_per_tok,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      intermediate_dim=intermediate_dim,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "functionality": "Creates a Flax Linen module for a RoutedMoE (Mixture of Experts) layer.",
            "usage": "Instantiate this function with configuration details like number of experts, expert-per-token ratio, device mesh, kernel initializers, and data types to obtain a Flax Linen module that can be used within a neural network. The module handles the routing of tokens to experts and the subsequent processing through feed-forward networks."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_and_shared_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_and_shared_moe(\n    config: ctypes.Config,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedAndSharedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedAndSharedMoE,\n      config=config,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "functionality": "Creates a Flax Linen module for a RoutedAndSharedMoE layer.",
            "usage": "Instantiate this function with configuration, mesh, kernel initialization, kernel axes, and optional parameters like weight dtype, computation dtype, and quantization. It returns a Flax Linen module that can be used within a larger neural network architecture."
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#roll_and_mask",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def roll_and_mask(x: jnp.ndarray, shift: int = -1) -> jnp.ndarray:\n  \"\"\"\n  Performs a leftward roll on the sequence axis (axis=1) and masks the\n  newly created invalid positions at the end of the sequence.\n  Assumes input `x` has a batch dimension at axis 0 and sequence at axis 1.\n\n  Args:\n    x: The input array of shape [batch, seq_len, ...].\n    shift: The number of positions to shift left.\n\n  Returns:\n    The rolled array of the same shape as x.\n  \"\"\"\n  # If shift is 0, it's a no-op. Return the original array.\n  if shift == 0:\n    return x\n\n  # to set the last `abs(shift)` elements of the sequence to zero.\n  return jnp.roll(x, shift, axis=1).at[:, shift:, ...].set(0)",
        "analysis": {
            "functionality": "Performs a leftward roll on a JAX array along the sequence axis and masks the newly introduced invalid positions at the end of the sequence by setting them to zero.",
            "usage": "Call the `roll_and_mask` function with a JAX numpy array `x` and an optional integer `shift` (defaulting to -1). The input array `x` is expected to have a batch dimension at axis 0 and a sequence dimension at axis 1. The function returns a modified array of the same shape as `x`, where elements have been shifted left and the trailing elements are zeroed out."
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionLayer",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionLayer(nn.Module):\n  \"\"\"\n  Implements Multi-Token Prediction (MTP) step:\n      1. Normalization of previous hidden state and target token embedding.\n      2. Concatenation and Projection of normalized features.\n      3. Processing through a Transformer Decoder Layer.\n\n      Equation Representation (Conceptual):\n          norm_h = RMSNorm(h_prev)\n          norm_e = RMSNorm(e_target)\n          h_proj = W_p(concat(norm_h, norm_e))\n          h_next = TransformerLayer(h_proj, pos_ids, segment_ids, ...)\n\n      It takes the previous hidden state and target embedding as input and outputs the\n      processed hidden state from its internal transformer block.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  layer_number: int\n  transformer_layer_module: Type[DecoderLayer] = DecoderLayer\n\n  @nn.compact\n  def __call__(\n      self,\n      prev_hidden_state: jnp.ndarray,\n      target_token_embedding: jnp.ndarray,\n      position_ids: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> jnp.ndarray:\n    \"\"\"\n    Applies the MTP combination, projection, and internal transformer processing.\n\n    Args:\n        prev_hidden_state: Hidden state from the previous step/layer.\n                           Shape: [batch, seq_len, hidden_size]\n        target_token_embedding: Embedding of the target token. In the context of MTP,\n                                this often refers to a token at a position relative\n                                to the current step, where the offset is determined\n                                by the layer number `k` (i.e., token t+k).\n                                Shape: [batch, seq_len, embed_dim]\n        position_ids: Original position IDs for the sequence.\n                      Shape: [batch, seq_len]\n        decoder_segment_ids: Original segment IDs for the sequence (for attention mask).\n                             Shape: [batch, seq_len]\n        deterministic: If true, disable dropout.\n        model_mode: The current operational mode (train, eval, decode).\n\n    Returns:\n        next_hidden_state: The hidden state produced by this MTP step's internal transformer.\n                           Shape: [batch, seq_len, hidden_size]\n    \"\"\"\n    cfg = self.config\n    mesh = self.mesh\n    k = self.layer_number\n\n    # --- 1. Normalize Hidden State and Embedding ---\n    embedding_norm_layer = rms_norm(\n        num_features=target_token_embedding.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_embedding_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n    embedding_norm = embedding_norm_layer(target_token_embedding)\n\n    hidden_state_norm_layer = rms_norm(\n        num_features=prev_hidden_state.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_hidden_state_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n\n    hidden_state_norm = hidden_state_norm_layer(prev_hidden_state)\n\n    # --- 2. Concatenate Normalized Representations ---\n    # Shape: [B, S, 2*H]\n    concatenated_features = jnp.concatenate([embedding_norm, hidden_state_norm], axis=-1)\n\n    # --- 3. Project Concatenated Features ---\n    # Projects from 2*H back down to H\n    projection_layer = dense_general(\n        inputs_shape=concatenated_features.shape,\n        out_features_shape=cfg.base_emb_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        use_bias=False,\n        kernel_axes=(\"concat_embed\", \"embed\"),\n        name=f\"mtp_{k}_projection\",\n    )\n    # Shape: [B, S, H]\n    projected_features = projection_layer(concatenated_features)\n\n    # --- 4. Pass through MTP Transformer Block ---\n    output = self.transformer_layer_module(\n        config=cfg, mesh=mesh, model_mode=model_mode, name=f\"mtp_{k}_transformer_layer\"\n    )(\n        inputs=projected_features,\n        decoder_segment_ids=decoder_segment_ids,\n        decoder_positions=position_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    if isinstance(output, tuple):\n      # Handles the scan=True case, where the output is a tuple.\n      next_hidden_state = output[0]\n    else:\n      # Handles the scan=False case, where the output is a single tensor.\n      next_hidden_state = output\n\n    # Shape: [B, S, H]\n    # --- Return Processed Hidden State ---\n    return next_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_layer",
            "purpose": "Implements a Multi-Token Prediction (MTP) step by normalizing hidden states and target embeddings, concatenating, projecting, and processing through a Transformer Decoder Layer.",
            "input": {
                "shape": "[batch, seq_len, hidden_size] for prev_hidden_state, [batch, seq_len, embed_dim] for target_token_embedding, [batch, seq_len] for position_ids and decoder_segment_ids",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Normalize previous hidden state using RMSNorm.",
                "Normalize target token embedding using RMSNorm.",
                "Concatenate normalized hidden state and embedding.",
                "Project concatenated features to the hidden dimension using a dense layer.",
                "Pass projected features through an internal Transformer Decoder Layer."
            ],
            "output": {
                "shape": "[batch, seq_len, hidden_size]"
            },
            "dependencies": [
                "rms_norm",
                "dense_general",
                "DecoderLayer"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters like dtype, normalization epsilon, and base embedding dimension.",
                "mesh": "JAX Mesh object for distributed computation.",
                "layer_number": "The current layer number, used for naming and potentially for specific layer logic.",
                "transformer_layer_module": "The specific DecoderLayer class to be used internally."
            },
            "notes": [
                "The layer number `k` is used to uniquely name internal modules.",
                "Handles potential tuple output from the internal transformer layer (e.g., when scan=True).",
                "Assumes input shapes are consistent with the configuration."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the MTP combination, projection, and internal transformer processing.",
                    "input": {
                        "shape": "prev_hidden_state: [batch, seq_len, hidden_size], target_token_embedding: [batch, seq_len, embed_dim], position_ids: [batch, seq_len], decoder_segment_ids: [batch, seq_len] or None",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply RMSNorm to target_token_embedding.",
                        "Apply RMSNorm to prev_hidden_state.",
                        "Concatenate the normalized embedding and hidden state along the last axis.",
                        "Apply a dense_general projection layer to reduce dimensionality.",
                        "Pass the projected features through the internal transformer_layer_module.",
                        "Extract the hidden state from the transformer layer's output (handling tuple output).",
                        "Return the processed hidden state."
                    ],
                    "output": {
                        "shape": "[batch, seq_len, hidden_size]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "jnp.concatenate",
                        "dense_general",
                        "DecoderLayer"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior.",
                        "The `model_mode` parameter specifies the operational mode (train, eval, decode).",
                        "The `target_token_embedding` is conceptually related to a token at position t+k."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionBlock",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionBlock(nn.Module):\n  \"\"\"Orchestrates the MTP process by running a sequence of MTP layers.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  transformer_layer_module: Type[DecoderLayer]\n  decoder: Decoder\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding,\n      main_hidden_state,\n      input_ids,\n      target_ids,\n      target_mask,\n      position_ids,\n      decoder_segment_ids,\n      model_mode,\n      deterministic,\n  ):\n    cfg = self.config\n    # The initial hidden state for the MTP chain is the raw output from the main model.\n    mtp_hidden_state = main_hidden_state\n\n    # These variables are updated sequentially in each loop iteration,\n    # moving the prediction window one token to the right each time.\n    rolled_input_ids = input_ids\n    rolled_target_ids = target_ids\n    rolled_target_mask = target_mask\n    rolled_position_id = position_ids\n\n    # Range chosen to align with the naming convention of the paper\n    for k in range(1, cfg.mtp_num_layers + 1):\n      # Sequentially roll all tensors to prepare data for predicting the k-th future token.\n      rolled_input_ids = roll_and_mask(rolled_input_ids)\n      rolled_target_ids = roll_and_mask(rolled_target_ids)\n      rolled_target_mask = roll_and_mask(rolled_target_mask)\n      rolled_position_id = roll_and_mask(rolled_position_id)\n\n      # Embed the k-th future input tokens using the shared embedding module\n      target_token_embedding = self.decoder._apply_embedding(\n          shared_embedding, rolled_input_ids, rolled_position_id, deterministic, self.decoder.model_mode\n      )\n\n      # Instantiate and apply the MTP layer for this step\n      mtp_layer = MultiTokenPredictionLayer(\n          config=cfg,\n          mesh=self.mesh,\n          layer_number=k,\n          name=f\"mtp_layer_{k}\",\n          transformer_layer_module=self.transformer_layer_module,\n      )\n\n      next_mtp_hidden_state = mtp_layer(\n          mtp_hidden_state,\n          target_token_embedding,\n          position_ids,\n          decoder_segment_ids,\n          deterministic,\n          self.decoder.model_mode,\n      )\n\n      # Project to logits using the shared embedding transpose\n      mtp_logits = self.decoder._apply_output_head(shared_embedding, next_mtp_hidden_state, deterministic, model_mode)\n\n      # Calculate cross-entropy loss for this specific layer's prediction\n      mtp_xent, _ = max_utils.cross_entropy_with_logits(\n          mtp_logits, jax.nn.one_hot(rolled_target_ids, cfg.vocab_size), 0.0\n      )\n      mtp_xent_masked = mtp_xent * rolled_target_mask\n\n      # This logic doesn't run during model initialization to avoid unwated population of the mutable collections.\n      if not self.is_initializing():\n        # For evaluation, save the top prediction and a valid token mask.\n        # This is only active for the target layer during an eval run.\n        if cfg.mtp_eval_target_module == k and self.is_mutable_collection(\"mtp_acceptance\"):\n          mtp_top_1_pred = jnp.argmax(mtp_logits, axis=-1)\n          self.sow(\"mtp_acceptance\", \"mtp_preds\", mtp_top_1_pred)\n          self.sow(\"mtp_acceptance\", \"mtp_mask\", rolled_target_mask)\n\n        # For training, save the loss components for this MTP head.\n        # This is only active during a training run.\n        if self.is_mutable_collection(\"mtp_losses\"):\n          self.sow(\"mtp_losses\", \"losses\", jnp.sum(mtp_xent_masked))\n          self.sow(\"mtp_losses\", \"weights\", jnp.sum(rolled_target_mask))\n\n      # The output of this layer is the input for the next, maintaining the causal chain.\n      mtp_hidden_state = next_mtp_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_block",
            "purpose": "Orchestrates the Multi-Token Prediction (MTP) process by running a sequence of MTP layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize MTP hidden state with main_hidden_state.",
                "Initialize rolled input, target, mask, and position IDs.",
                "Iterate through MTP layers (from 1 to cfg.mtp_num_layers):",
                "  Roll input_ids, target_ids, target_mask, and position_ids.",
                "  Embed the rolled input tokens.",
                "  Instantiate and apply a MultiTokenPredictionLayer.",
                "  Project the output to logits using the shared embedding transpose.",
                "  Calculate cross-entropy loss and mask it.",
                "  If in evaluation mode and the target layer, sow MTP predictions and mask.",
                "  If in training mode, sow MTP losses and weights.",
                "  Update MTP hidden state for the next iteration."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "DecoderLayer",
                "Decoder",
                "MultiTokenPredictionLayer",
                "roll_and_mask",
                "max_utils.cross_entropy_with_logits"
            ],
            "parameters": {
                "config": "Configuration object containing MTP parameters like mtp_num_layers, vocab_size, etc.",
                "mesh": "Mesh object for distributed computation.",
                "transformer_layer_module": "The type of transformer layer module to use.",
                "decoder": "The decoder object, used for embedding and output head."
            },
            "notes": [
                "The initial hidden state for the MTP chain is the raw output from the main model.",
                "Tensors are sequentially rolled in each loop iteration to move the prediction window.",
                "Loss calculation and saving of predictions/losses are conditional based on model mode (training/evaluation) and specific layer configurations.",
                "The output of one MTP layer becomes the input for the next, maintaining a causal chain."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_loss",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_loss(intermediate_outputs, config):\n  \"\"\"Calculates the Multi Token Prediction loss from intermediate outputs.\"\"\"\n  losses_path = (\"mtp_losses\", \"mtp_block\", \"losses\")\n  weights_path = (\"mtp_losses\", \"mtp_block\", \"weights\")\n\n  mtp_losses = maxtext_utils.get_nested_value(intermediate_outputs, losses_path, default=())\n  mtp_weights = maxtext_utils.get_nested_value(intermediate_outputs, weights_path, default=())\n\n  if not mtp_losses:  # MTP heads did not run\n    return 0.0\n\n  sum_of_all_mtp_losses = jnp.sum(jnp.array(mtp_losses))\n  sum_of_all_mtp_weights = jnp.sum(jnp.array(mtp_weights))\n\n  avg_mtp_loss = sum_of_all_mtp_losses / (sum_of_all_mtp_weights + EPS)\n  scaled_mtp_loss = avg_mtp_loss * config.mtp_loss_scaling_factor\n  return scaled_mtp_loss",
        "analysis": {
            "functionality": "Calculates the Multi Token Prediction (MTP) loss by aggregating losses and weights from intermediate outputs.",
            "usage": "This function takes `intermediate_outputs` (a dictionary containing nested MTP losses and weights) and a `config` object (which includes `mtp_loss_scaling_factor`) as input. It returns a single float value representing the scaled MTP loss. If no MTP losses are found, it returns 0.0."
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_acceptance_rate",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_acceptance_rate(intermediate_outputs, config):\n  \"\"\"Calculates the MTP acceptance rate from intermediate outputs.\"\"\"\n\n  sown_data = maxtext_utils.get_nested_value(intermediate_outputs, (\"mtp_acceptance\", \"mtp_block\"), {})\n  mtp_preds = maxtext_utils.get_nested_value(sown_data, (\"mtp_preds\",), [None])[0]\n  valid_mask = maxtext_utils.get_nested_value(sown_data, (\"mtp_mask\",), [None])[0]\n\n  # These values are only \"sown\" (saved) during an evaluation run and only for the specific\n  # MTP layer specified by `config.mtp_eval_target_module`. This check handles cases\n  # where the required data is absent (e.g., during a training step) and prevents errors.\n  if mtp_preds is None or valid_mask is None:\n    return 0.0\n\n  # Get the main model's greedy predictions from the logits.\n  main_model_preds = jnp.argmax(intermediate_outputs[\"logits\"], axis=-1)\n\n  # Roll the main model's predictions to align them in time with the MTP head's target.\n  rolled_main_preds = main_model_preds\n  for _ in range(config.mtp_eval_target_module):\n    rolled_main_preds = roll_and_mask(rolled_main_preds)\n\n  # Compare the aligned predictions. The `valid_mask` ensures that the comparison\n  # only happens on valid tokens, ignoring the placeholder values introduced at the\n  # end of the sequence by the `roll_and_mask` operation.\n  correct_predictions = jnp.sum((mtp_preds == rolled_main_preds) * valid_mask)\n  total_valid_tokens = jnp.sum(valid_mask)\n\n  # Return acceptance rate as a percentage\n  return (correct_predictions / (total_valid_tokens + EPS)) * 100",
        "analysis": {
            "functionality": "Calculates the Multi-Token Prediction (MTP) acceptance rate by comparing the MTP head's predictions with the main model's greedy predictions, after aligning them in time.",
            "usage": "Takes `intermediate_outputs` (a dictionary containing model outputs like 'logits' and sown MTP predictions/masks) and a `config` object as input. It returns a float representing the MTP acceptance rate (percentage of correct predictions) or 0.0 if the necessary MTP data is not found in `intermediate_outputs`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#is_vanilla_variable",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def is_vanilla_variable(vs: variablelib.VariableState) -> bool:\n  \"\"\"A variables state is vanilla if its metadata is essentially blank.\n\n  Returns False only if it has non-empty hooks or any non-built-in attribute.\n  \"\"\"\n  for key, value in vs.get_metadata().items():\n    if key.endswith(\"_hooks\"):\n      if value != ():\n        return False\n    else:\n      return False\n  return True",
        "analysis": {
            "functionality": "Checks if a variable's metadata is essentially blank, indicating a 'vanilla' state.",
            "usage": "Accepts a `variablelib.VariableState` object and returns `True` if the variable has no non-empty hooks and no non-built-in attributes in its metadata, `False` otherwise."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_var(vs: variablelib.VariableState) -> meta.AxisMetadata:\n  metadata = vs.get_metadata()\n  if \"linen_meta_type\" in metadata:\n    linen_type = metadata[\"linen_meta_type\"]\n    if hasattr(linen_type, \"from_nnx_metadata\"):\n      return linen_type.from_nnx_metadata({\"value\": vs.value, **metadata})\n    return linen_type(vs.value, **metadata)\n  if is_vanilla_variable(vs):\n    return vs.value\n  return nnx.bridge.NNXMeta(vs.type, vs.value, metadata)",
        "analysis": {
            "functionality": "Converts a VariableState object to a Linen AxisMetadata object.",
            "usage": "Pass a flax.nnx.variablelib.VariableState object to this function. It returns a flax.core.meta.AxisMetadata object, which is used for representing metadata in Flax Linen modules. The conversion logic depends on whether the input VariableState has 'linen_meta_type' metadata, if it's a vanilla variable, or if it needs to be wrapped as an NNXMeta object."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#get_col_name",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def get_col_name(keypath: tp.Sequence[Any]) -> str:\n  \"\"\"Given the keypath of a Flax variable type, return its Linen collection name.\"\"\"\n  # Infer variable type from the leaf's path, which contains its Linen collection name\n  assert isinstance(keypath[0], jax.tree_util.DictKey)\n  return str(keypath[0].key)",
        "analysis": {
            "functionality": "Extracts the Linen collection name from a Flax variable's keypath.",
            "usage": "Pass a sequence representing the keypath of a Flax variable to this function. It returns the collection name as a string. For example, `get_col_name([jax.tree_util.DictKey('params'), ...])` would return 'params'."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_nnx_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_nnx_var(col: str, x: meta.AxisMetadata | Any) -> variablelib.Variable:\n  \"\"\"Convert a Linen variable to an NNX variable.\"\"\"\n  vtype = variablelib.variable_type_from_name(col, allow_register=True)\n  if isinstance(x, nnx.bridge.NNXMeta):\n    assert vtype == x.var_type, f\"Type stored in NNXMeta {x.var_type} != type inferred from collection name {vtype}\"\n    return x.to_nnx_variable()\n  if isinstance(x, meta.AxisMetadata):\n    x_metadata = vars(x)\n    if hasattr(x, \"to_nnx_metadata\"):\n      x_metadata = x.to_nnx_metadata()\n    assert hasattr(x, \"value\")\n    return vtype(**x_metadata, linen_meta_type=type(x))\n  return vtype(x)",
        "analysis": {
            "functionality": "Converts a Flax Linen variable to an NNX (Neural Network eXperimental) variable.",
            "usage": "Call this function with a collection name (string) and the Linen variable (which can be of type meta.AxisMetadata or any other type). It returns an NNX variable. If the input is an NNXMeta object, it's converted directly. If it's AxisMetadata, it's converted using its value and metadata. Otherwise, it's converted using the provided value."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_recursive_merge",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _recursive_merge(dict1, dict2):\n  \"\"\"Recursively merge two dicts.\"\"\"\n  flat_map = nnx.traversals.flatten_mapping(dict1)\n  flat_map |= nnx.traversals.flatten_mapping(dict2)\n  return nnx.traversals.unflatten_mapping(flat_map)",
        "analysis": {
            "module_type": "recursive_merge",
            "purpose": "Recursively merges two dictionaries by flattening them, combining their key-value pairs, and then unflattening the result.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Flatten dict1 using `nnx.traversals.flatten_mapping`.",
                "Flatten dict2 using `nnx.traversals.flatten_mapping`.",
                "Merge the flattened dictionaries using the `|=` operator.",
                "Unflatten the merged dictionary using `nnx.traversals.unflatten_mapping`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.traversals.flatten_mapping",
                "nnx.traversals.unflatten_mapping"
            ],
            "parameters": {},
            "notes": [
                "This function is used to merge nested dictionary structures, preserving the nested nature of the data.",
                "The `|=` operator for dictionaries performs an in-place update, but since `flatten_mapping` returns new dictionaries, it effectively merges the contents of both flattened dictionaries into `flat_map`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_vars_to_nnx_attrs",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_vars_to_nnx_attrs(variables: tp.Mapping[str, Any]) -> dict[str, Any]:\n  \"\"\"Convert a dict of Linen-style variables to NNX variables.\"\"\"\n  nnx_vars = jax.tree_util.tree_map_with_path(\n      lambda kp, x: to_nnx_var(get_col_name(kp), x),\n      variables,\n      is_leaf=lambda x: not isinstance(x, dict),\n  )\n\n  flat_paths: dict[tuple, tp.Any] = {}\n\n  for col_name, col_variables in nnx_vars.items():  # pylint: disable=unused-variable\n    for path, variable in nnx.traversals.flatten_mapping(col_variables).items():\n      if path in flat_paths:\n        raise ValueError(\n            f\"Found duplicate variable path {path} with variables \"\n            f\"{flat_paths[path]} and {variable}. \"\n            \"This is not allowed in NNX.\"\n        )\n      flat_paths[path] = variable\n\n  nnx_vars = nnx.traversals.unflatten_mapping(flat_paths)\n  return nnx_vars",
        "analysis": {
            "functionality": "Converts a dictionary of Flax/Linen-style variables into Flax/NNX-style variables.",
            "usage": "Pass a dictionary containing Flax/Linen variables to the function. It returns a dictionary structured for Flax/NNX."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#nnx_attrs_to_linen_vars",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def nnx_attrs_to_linen_vars(nnx_attrs: dict) -> dict:\n  \"\"\"Convert a dict of NNX variables (or variable states) to Linen-style variables.\"\"\"\n  linen_structured = {}\n  for kp, v in nnx.traversals.flatten_mapping(nnx_attrs).items():\n    if isinstance(v, variablelib.Variable):\n      col_name = variablelib.variable_name_from_type(type(v))\n      v = to_linen_var(v.to_state())\n    elif isinstance(v, variablelib.VariableState):\n      col_name = variablelib.variable_name_from_type(v.type)\n      v = to_linen_var(v)\n    else:\n      raise ValueError(f\"Cannot infer collection name from value: {v}\")\n    linen_structured[(col_name, *kp)] = v\n  variables = nnx.traversals.unflatten_mapping(linen_structured)\n  return variables",
        "analysis": {
            "functionality": "Converts a dictionary of NNX variables (or their states) into a dictionary of Linen-style variables.",
            "usage": "Pass a dictionary containing NNX variables or VariableStates to the function. It returns a dictionary structured for use with Flax Linen modules."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_set_initializing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _set_initializing(module: Module, initializing: bool):\n  for _, value in graph.iter_graph(module):\n    if isinstance(value, Pytree):\n      value._object__state._initializing = initializing",
        "analysis": {
            "module_type": "set_initializing",
            "purpose": "Sets the 'initializing' state for all Pytree objects within a given module.",
            "input": {
                "shape": "[N/A]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterates through the graph of the input module using `graph.iter_graph`.",
                "Checks if each encountered value is an instance of `Pytree`.",
                "If it is a `Pytree`, it accesses its internal state (`_object__state`) and sets the `_initializing` attribute to the provided boolean value."
            ],
            "output": {
                "shape": "[N/A]"
            },
            "dependencies": [
                "flax.nnx.graph",
                "flax.nnx.Pytree",
                "flax.nnx.Module"
            ],
            "parameters": {
                "module": "The NNX Module whose Pytree objects will have their initializing state set.",
                "initializing": "A boolean value to set the initializing state to (True or False)."
            },
            "notes": [
                "This function is likely used internally by NNX for managing module initialization states.",
                "It directly accesses protected members (`_object__state`, `_initializing`), which might indicate it's a low-level utility."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#lazy_init",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def lazy_init(fn: Module | tp.Callable[..., tp.Any], *args, **kwargs):\n  \"\"\"To run through an arbitrary nnx.Module method and initialize all its needed state.\n\n  Here used to trigger initialization of all `LinenToNNX` module variables.\"\"\"\n  if isinstance(fn, Module):\n    module = fn\n    assert callable(fn)\n  else:\n    if not (hasattr(fn, \"__self__\") and isinstance(fn.__self__, Module)):\n      raise ValueError(f\"{fn = } needs to be a method of an NNX Module.\")\n    module = fn.__self__\n  _set_initializing(module, True)\n  try:\n    _ = fn(*args, **kwargs)\n  finally:\n    _set_initializing(module, False)\n  return fn",
        "analysis": {
            "module_type": "lazy_initializer",
            "purpose": "Initializes all state variables of an NNX Module by running a given method.",
            "input": {
                "shape": "[N/A]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the input 'fn' is an NNX Module instance or a method of one.",
                "Set the 'initializing' flag for the module to True.",
                "Execute the provided function 'fn' with its arguments.",
                "Set the 'initializing' flag for the module to False in a finally block.",
                "Return the original function 'fn'."
            ],
            "output": {
                "shape": "[N/A]"
            },
            "dependencies": [
                "flax.nnx.module.Module",
                "flax.nnx.bridge.module._set_initializing"
            ],
            "parameters": {},
            "notes": [
                "This function is particularly useful for triggering the initialization of `LinenToNNX` module variables.",
                "It ensures that state is properly set up before the module is used.",
                "Raises a ValueError if 'fn' is not a method of an NNX Module when it's not a Module itself."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#current_linen_module",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def current_linen_module() -> linen.Module | None:\n  \"\"\"Get the current Linen module from the Linen context.\"\"\"\n  if linen.module._context.module_stack:  # pylint: disable=W0212\n    return linen.module._context.module_stack[-1]  # pylint: disable=W0212\n  return None",
        "analysis": {
            "functionality": "Retrieves the most recently added Linen module from the current context's module stack.",
            "usage": "Call `current_linen_module()` to get the current Linen module instance. Returns `None` if no module is currently in context."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToNNX",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToNNX(Module):\n  \"\"\"A wrapper to turn any Linen module into an NNX module.\n\n  The result NNX module can be used standalone with all NNX APIs, or as a submodule of\n  another NNX module.\n\n  Since Linen module initialization requires a sample input, you need to call `lazy_init`\n  with an argument to initialize the variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> linen_module = nn.Dense(features=64)\n    >>> x = jax.numpy.ones((1, 32))\n    >>> # Like Linen init(), initialize with a sample input\n    >>> model = nnx.bridge.ToNNX(linen_module, rngs=nnx.Rngs(0)).lazy_init(x)\n    >>> # Like Linen apply(), but using NNX's direct call method\n    >>> y = model(x)\n    >>> model.kernel.shape\n    (32, 64)\n\n  Args:\n    module: The Linen Module instance.\n    rngs: The `nnx.Rngs` instance being passed to any NNX module.\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  def __init__(\n      self,\n      module: linen.Module,\n      rngs: Rngs | jax.Array | None = None,\n  ):\n    self.to_nnx__module = module\n\n    self.to_nnx__rngs: Rngs | None\n    if isinstance(rngs, jax.Array):\n      self.to_nnx__rngs = Rngs(params=rngs)\n    elif isinstance(rngs, nnx.Rngs):\n      self.to_nnx__rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else nnx.clone(rngs)  # type: ignore\n    else:\n      self.to_nnx__rngs = rngs\n\n  def lazy_init(self, *args, **kwargs):\n    \"\"\"A shortcut of calling `nnx.bridge.lazy_init()` upon this module.\"\"\"\n    return lazy_init(self, *args, **kwargs)\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    maybe_method = getattr(type(self.to_nnx__module), name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def __call__(\n      self,\n      *args: Any,\n      rngs: Rngs | jax.Array | None = None,\n      method: tp.Callable[..., Any] | str | None = None,\n      mutable: tp.Any = None,\n      **kwargs: Any,\n  ) -> Any:\n    # Shape-based lazy init of the flax variables\n    if rngs is None:\n      rngs = self.to_nnx__rngs\n    if isinstance(rngs, nnx.Rngs):\n      _rngs = {name: stream() for name, stream in rngs.items()}\n    elif isinstance(rngs, jax.Array):\n      _rngs = {\"params\": rngs}\n    else:\n      _rngs = {}\n    # rename default to params\n    if \"params\" not in _rngs and \"default\" in _rngs:\n      _rngs[\"params\"] = _rngs.pop(\"default\")\n    if self._object__state.initializing:\n      out, updates = self.to_nnx__module.init_with_output(_rngs, *args, method=method, **kwargs)\n    else:\n      nnx_attrs = {\n          k: v\n          for k, v in vars(self).items()\n          if not k.startswith(\"to_nnx__\") and not k.startswith(\"_pytree__\") and not k.startswith(\"_object__\")\n      }\n      variables = nnx_attrs_to_linen_vars(nnx_attrs)\n\n      # Get `mutable` from top level bridge.Module context if any\n      if mutable is not None:\n        pass\n      elif (m := bdg_module.current_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      elif (m := current_linen_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      else:\n        mutable = False\n\n      out = self.to_nnx__module.apply(variables, *args, rngs=_rngs, method=method, mutable=mutable, **kwargs)\n\n      # Split out the updates if `mutable` is passed into the Flax module\n      if mutable is not False:\n        out, updates = out\n      else:\n        updates = None\n\n    # Split out the updates if `mutable` is passed into the Flax module\n    if updates:\n      nnx_attrs = linen_vars_to_nnx_attrs(updates)\n      # nnx.update(self, nnx_attrs)\n      # TODO(cgarciae): ideally we just do an update but currently dictionaries don't allow\n      # insertion of new keys, we need to enable this in NNX to simplify the code below\n      # to the simple nnx.update(self, nnx_attrs) above.\n      for attr_name, value in nnx_attrs.items():\n        if hasattr(self, attr_name) and isinstance(value, dict):\n          original_value = getattr(self, attr_name)\n          new_values = _recursive_merge(original_value, value)\n          setattr(self, attr_name, nnx.data(new_values))\n        else:\n          setattr(self, attr_name, nnx.data(value))\n\n    return out",
        "analysis": {
            "module_type": "to_nnx_wrapper",
            "purpose": "Wraps a Flax Linen module to make it compatible with NNX APIs, allowing it to be used as a standalone NNX module or a submodule within another NNX module.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim] (for sample input during initialization)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the wrapper with a Linen module instance and optional RNGs.",
                "Provides a `lazy_init` method to initialize variables using a sample input, similar to Linen's `init`.",
                "Intercepts attribute access to delegate calls to the wrapped Linen module's methods.",
                "Handles the `__call__` method to manage initialization (`init_with_output`) and application (`apply`) of the wrapped Linen module, converting between NNX and Linen variable formats.",
                "Updates NNX module state with changes from Linen's apply method when mutable is enabled."
            ],
            "output": {
                "shape": "N/A (returns the result of the wrapped Linen module's operations)"
            },
            "dependencies": [
                "flax.linen",
                "flax.nnx",
                "flax.nnx.Module",
                "flax.nnx.Rngs",
                "functools.partial",
                "typing.Any",
                "jax"
            ],
            "parameters": {
                "module": "The Flax Linen Module instance to be wrapped.",
                "rngs": "An optional `nnx.Rngs` instance or JAX array for random number generation."
            },
            "notes": [
                "Initialization requires calling `lazy_init` with a sample input because Linen modules need this for variable initialization.",
                "The wrapper attempts to automatically infer and manage RNG streams.",
                "Handles the conversion of NNX variables to Linen variables for `apply` and Linen variables back to NNX for state updates."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the ToNNX wrapper with a Linen module and optional RNGs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the provided Linen module instance.",
                        "Processes and stores the provided RNGs, converting JAX arrays to `nnx.Rngs` if necessary."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Rngs",
                        "jax.Array"
                    ],
                    "notes": []
                },
                "lazy_init": {
                    "purpose": "A convenience method to call `nnx.bridge.lazy_init` on the wrapped module.",
                    "input": {
                        "shape": "Arguments expected by the wrapped Linen module's initialization.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the external `lazy_init` function with `self` and provided arguments."
                    ],
                    "output": {
                        "shape": "The initialized NNX module (self)."
                    },
                    "dependencies": [
                        "flax.nnx.bridge.lazy_init"
                    ],
                    "notes": [
                        "This method is crucial for initializing the state of the wrapped Linen module within the NNX framework."
                    ]
                },
                "__getattr__": {
                    "purpose": "Intercepts attribute access to delegate calls to the wrapped Linen module's methods or attributes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the attribute exists on the wrapper itself (e.g., inherited from `Module`).",
                        "If not found, it attempts to get the attribute from the wrapped Linen module's class.",
                        "If the attribute is a callable method, it returns a partial function that calls the wrapper's `__call__` method with the Linen method specified.",
                        "Otherwise, it returns the attribute from the wrapper's superclass."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "This enables accessing and calling methods of the original Linen module as if they were methods of the NNX wrapper."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the wrapped Linen module, handling initialization and application, and managing variable conversions.",
                    "input": {
                        "shape": "Arguments expected by the wrapped Linen module's `__call__` or specified `method`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines the appropriate RNG streams to use.",
                        "If the module is in an initializing state (`self._object__state.initializing` is True):",
                        "  - Calls `self.to_nnx__module.init_with_output` to initialize variables.",
                        "If the module is not initializing:",
                        "  - Extracts current NNX attributes.",
                        "  - Converts NNX attributes to Linen-style variables.",
                        "  - Determines the `mutable` state, potentially from context.",
                        "  - Calls `self.to_nnx__module.apply` with the converted variables.",
                        "  - If `mutable` is enabled and updates are returned, converts Linen updates back to NNX attributes and updates the wrapper's state.",
                        "Returns the output of the Linen module's operation."
                    ],
                    "output": {
                        "shape": "The output of the wrapped Linen module's `init_with_output` or `apply` method."
                    },
                    "dependencies": [
                        "flax.nnx.Rngs",
                        "flax.nnx.bridge.bdg_module",
                        "flax.nnx.bridge.current_linen_module",
                        "flax.nnx.variablelib",
                        "flax.linen.Module",
                        "jax.Array"
                    ],
                    "notes": [
                        "This method is the core of the bridging functionality, managing the lifecycle and state synchronization between Linen and NNX.",
                        "It handles the conversion of RNG dictionaries and variable formats.",
                        "It supports dynamic determination of the `mutable` state from the NNX context."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_rngs_dict",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_rngs_dict(linen_module: linen.Module, add_default: bool = False):\n  \"\"\"Given a module, split out one of its every active RNG key collections.\"\"\"\n  assert linen_module.scope is not None, \"linen_rngs_dict() must be called inside a Linen module.\"\n  rngs: dict[str, tp.Any] = {name: linen_module.make_rng(name) for name in linen_module.scope.rngs.keys()}\n  if add_default and \"default\" not in rngs:\n    rngs[\"default\"] = 0\n  return rngs",
        "analysis": {
            "functionality": "Retrieves active RNG key collections from a Linen module's scope.",
            "usage": "Call this function with a Linen module instance. It returns a dictionary where keys are RNG collection names and values are the corresponding RNG keys. An optional boolean `add_default` can be set to `True` to ensure a 'default' RNG key is present, initializing it to 0 if not already found."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_get_module_method",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _get_module_method(module, method: tp.Callable[..., Any] | str | None):\n  \"\"\"Get a callable method from the module, or raise TypeError.\"\"\"\n  if method is None:\n    method = \"__call__\"\n\n  if isinstance(method, str):\n    attribute_name = method\n    method = getattr(type(module), attribute_name)\n    if not callable(method):\n      class_name = type(module).__name__\n      raise TypeError(f\"'{class_name}.{attribute_name}' must be a callable, got {type(method)}.\")\n  if not callable(method):\n    class_name = type(module).__name__\n    raise TypeError(f\"'{method}' must be a callable, got {type(method)}.\")\n\n  return method",
        "analysis": {
            "functionality": "Retrieves a callable method from a given module, handling string names or direct callables, and raises a TypeError if the method is not callable.",
            "usage": "Call `_get_module_method(module, method)` where `module` is an object and `method` is either a string representing the method name, a callable, or None (defaults to '__call__'). It returns the callable method. Raises TypeError if the specified method is not found or is not callable."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_fix_for_qwix_quantization",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _fix_for_qwix_quantization(module: Module):\n  \"\"\"Process the nnx module to make it compatible with QWIX quantization.\n\n  Normally Qwix only works with pure Linen modules or pure NNX modules. When\n  NNX modules are called inside Linen modules, Qwix will have issues to\n    * detect the correct module path when a Jax op (e.g. einsum) is called.\n    * detect the input types (whether it's a weight) of the Jax op.\n\n  This function will fix those issues.\n\n  Args:\n    module: The NNX module to be processed.\n  \"\"\"\n\n  # Wrap the __call__ function of the nnx modules to make sure the linen module\n  # path is updated correctly.\n  def wrap(call_fn, name: str):\n    def wrapped(*args, **kwargs):\n      if not linen.module._context.module_stack:  # pylint: disable=W0212\n        return call_fn(*args, **kwargs)\n      nn_module = linen.module._context.module_stack[-1]  # pylint: disable=W0212\n      old_path = nn_module.path\n      # We modify the path of the current nn module in place. This is a little\n      # bit hacky but should be good as a temporary solution.\n      nn_module.scope.path += (name,)\n      try:\n        return call_fn(*args, **kwargs)\n      finally:\n        nn_module.scope.path = old_path\n\n    return wrapped\n\n  for path, node in nnx.iter_graph(module):\n    # Only enable it on non-root nnx modules.\n    if path and isinstance(node, nnx.Module):\n      node.__class__ = type(\n          node.__class__.__name__,\n          (node.__class__,),\n          {\n              \"__call__\": wrap(node.__class__.__call__, str(path[-1])),\n          },\n      )\n\n  # Set the correct weight names. We call QtProvider.process_model_inputs here\n  # to avoid using Qwix internal APIs.\n  qwix.QtProvider.process_model_inputs(None, module, None, None)",
        "analysis": {
            "functionality": "Processes an NNX module to ensure compatibility with QWIX quantization, specifically addressing issues that arise when NNX modules are used within Linen modules.",
            "usage": "This function takes an NNX module as input and modifies it in place to resolve path detection and input type identification problems that QWIX encounters in mixed NNX-Linen environments. It wraps the __call__ method of non-root NNX modules and calls `qwix.QtProvider.process_model_inputs` to set correct weight names. It is intended for internal use when integrating NNX modules with QWIX quantization."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToLinen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToLinen(linen.Module):\n  \"\"\"A wrapper to turn any NNX module into a Linen module.\n\n  The result Linen module can be used standalone with all Linen APIs, or as a\n  submodule of\n  another Linen module.\n\n  Since NNX modules are stateful and owns the state, we only create it once\n  during init\n  time, and will track its state and static data as separate variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> model = nnx.bridge.ToLinen(nnx.Linear, args=(32, 64))\n    >>> x = jax.numpy.ones((1, 32))\n    >>> y, variables = model.init_with_output(jax.random.key(0), x)\n    >>> y.shape\n    (1, 64)\n    >>> variables['params']['kernel'].shape\n    (32, 64)\n    >>> # The static GraphDef of the underlying NNX module\n    >>> variables.keys()\n    dict_keys(['params'])\n\n  Args:\n    nnx_class: The NNX Module class (not instance!).\n    args: The arguments that normally would be passed in to create the NNX\n      module.\n    kwargs: The keyword arguments that normally would be passed in to create the\n      NNX module.\n    skip_rng: True if this NNX module doesn't need `rngs` arg during\n      initialization (not common).\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  nnx_class: tp.Callable[..., Module]\n  args: tp.Sequence = ()\n  kwargs: tp.Mapping[str, tp.Any] = FrozenDict({})\n  skip_rng: bool = False\n  metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var\n\n  @linen.compact\n  def __call__(self, *args, nnx_method: tp.Callable[..., Any] | str | None = None, **kwargs):\n    def _module_kwargs():\n      maybe_add_default = not self.is_initializing()\n      module_kwargs = dict(self.kwargs)\n      if not self.skip_rng:\n        module_kwargs[\"rngs\"] = nnx.Rngs(**linen_rngs_dict(self, add_default=maybe_add_default))\n      return module_kwargs\n\n    # init codepath\n    if self.is_initializing():\n      module = self.nnx_class(*self.args, **_module_kwargs())\n      # TODO: add lazy_init here in case there's an `ToNNX` submodule under `module`.\n      # update linen variables before call module to save initial state\n      self._update_variables(module)\n      _fix_for_qwix_quantization(module)\n      method_fn = _get_module_method(module, nnx_method)\n      out = method_fn(module, *args, **kwargs)\n      return out\n\n    # create the nnx module\n    module = self.nnx_class(*self.args, **_module_kwargs())\n\n    # update nnx module from linen variables\n    def maybe_unbox(x):\n      if isinstance(x, meta.AxisMetadata):\n        return x.unbox()\n      return x\n\n    states = jtu.tree_map(\n        maybe_unbox,\n        list(core.unfreeze(self.variables).values()),  # type: ignore[wrong-arg-types]\n        is_leaf=lambda x: isinstance(x, meta.AxisMetadata),\n    )\n    if not states:\n      states = ({},)\n\n    new_state = nnx.merge_state(*states)\n    new_state_flat = nnx.traversals.flatten_mapping(new_state)\n    current_state_flat = nnx.traversals.flatten_mapping(nnx.state(module))\n    unknown_state_flat = {path: v for path, v in new_state_flat.items() if path not in current_state_flat}\n\n    if unknown_state_flat:\n      paths_str = \"\"\n      for path, _ in unknown_state_flat.items():\n        paths_str += f\"\\n  - {'/'.join(map(str, path))}\"\n\n      warnings.warn(f\"Found unknown module paths in incoming state:{paths_str}\")\n\n    nnx.update(module, new_state)\n\n    _fix_for_qwix_quantization(module)\n    method_fn = _get_module_method(module, nnx_method)\n    out = method_fn(module, *args, **kwargs)\n    self._update_variables(module)\n    return out\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    if name in self.kwargs:\n      return self.kwargs[name]\n    maybe_method = getattr(self.nnx_class, name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, nnx_method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def _update_variables(self, module):\n    \"\"\"Store the NNX module's graph def and state inside Linen module variables.\"\"\"\n    state = nnx.state(module, nnx.Not(nnx.RngState))\n\n    collection_flat_state: dict[str, list[tuple[tuple[str, ...], tp.Any]]] = {}\n\n    # group state by collection\n    for path, leaf in nnx.to_flat_state(state):\n      type_ = leaf.type if isinstance(leaf, nnx.VariableState) else type(leaf)\n      collection = variablelib.variable_name_from_type(type_, allow_register=True)\n      if collection not in collection_flat_state:\n        collection_flat_state[collection] = []\n      collection_flat_state[collection].append((path, leaf))\n\n    # update linen variables\n    for collection, flat_state in collection_flat_state.items():\n      if self.is_mutable_collection(collection):\n\n        def _to_linen_var(x):\n          if isinstance(x, nnx.VariableState):\n            if self.metadata_fn is not None:\n              return self.metadata_fn(x)  # pylint: disable=too-many-function-args\n            else:\n              return x.value\n          return x\n\n        collection_state = nnx.traversals.unflatten_mapping(flat_state)\n        collection_state = jax.tree.map(\n            _to_linen_var,\n            collection_state,\n            is_leaf=lambda x: isinstance(x, nnx.VariableState),\n        )\n        for k, v in collection_state.items():\n          self.put_variable(collection, k, v)",
        "analysis": {
            "module_type": "to_linen_wrapper",
            "purpose": "Wraps an NNX module to make it usable as a Flax Linen module.",
            "input": {
                "shape": "[batch_size, ...], optional nnx_method",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Checks if the module is initializing.",
                "If initializing, creates the NNX module and updates Linen variables.",
                "If not initializing, creates the NNX module, updates it from Linen variables, and then calls the specified method.",
                "Updates Linen variables with the state of the NNX module.",
                "Returns the output of the NNX module's method."
            ],
            "output": {
                "shape": "Output shape depends on the wrapped NNX module's method."
            },
            "dependencies": [
                "flax.linen",
                "flax.nnx",
                "flax.core",
                "flax.nnx.variablelib",
                "functools.partial",
                "typing",
                "warnings",
                "jax"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class to be wrapped.",
                "args": "Positional arguments for the NNX module constructor.",
                "kwargs": "Keyword arguments for the NNX module constructor.",
                "skip_rng": "If True, the NNX module does not require rngs during initialization.",
                "metadata_fn": "A function to convert NNX VariableState to Linen variables."
            },
            "notes": [
                "This module allows seamless integration of NNX modules within the Flax Linen framework.",
                "It handles state management, ensuring that NNX module states are tracked as Linen variables.",
                "Supports calling specific methods of the wrapped NNX module via the `nnx_method` argument."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the wrapped NNX module, handling initialization and state updates.",
                    "input": {
                        "shape": "Input tensor shape and optional `nnx_method` (string or callable).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines the arguments for the NNX module constructor, including RNGs if not initializing.",
                        "If initializing, creates the NNX module, updates Linen variables, applies QWIX quantization fixes, and calls the specified method.",
                        "If not initializing, creates the NNX module, updates it from Linen variables, applies QWIX quantization fixes, calls the specified method, and then updates Linen variables.",
                        "Returns the output of the NNX module's method."
                    ],
                    "output": {
                        "shape": "Output shape depends on the wrapped NNX module's method."
                    },
                    "dependencies": [
                        "linen_rngs_dict",
                        "_get_module_method",
                        "_fix_for_qwix_quantization",
                        "nnx.update",
                        "nnx.state",
                        "nnx.merge_state",
                        "nnx.traversals.flatten_mapping",
                        "nnx.traversals.unflatten_mapping",
                        "jax.tree_util.tree_map",
                        "warnings.warn"
                    ],
                    "notes": [
                        "Handles both initialization and forward pass logic.",
                        "Manages the transfer of state between Linen variables and the NNX module.",
                        "Includes logic to warn about unknown paths in the incoming state."
                    ]
                },
                "__getattr__": {
                    "purpose": "Provides attribute access to the wrapped NNX module's attributes and methods.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the attribute exists on the Linen module itself.",
                        "Checks if the attribute is in the `kwargs` of the wrapper.",
                        "If the attribute is a callable method on the NNX class, it returns a partial function that calls the `__call__` method with `nnx_method` set.",
                        "Otherwise, it falls back to the default attribute access."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "Enables accessing methods of the underlying NNX module as if they were methods of the `ToLinen` wrapper."
                    ]
                },
                "_update_variables": {
                    "purpose": "Updates the Linen module's variables with the state from the NNX module.",
                    "input": {
                        "shape": "An NNX module instance.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extracts the state from the NNX module, excluding RNG states.",
                        "Groups the state by collection type.",
                        "Converts NNX VariableStates to Linen-compatible variables using `metadata_fn`.",
                        "Updates the Linen module's variables using `put_variable` for mutable collections."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.state",
                        "nnx.Not",
                        "nnx.to_flat_state",
                        "variablelib.variable_name_from_type",
                        "nnx.traversals.unflatten_mapping",
                        "jax.tree.map",
                        "variablelib.VariableState",
                        "meta.AxisMetadata"
                    ],
                    "notes": [
                        "This method is crucial for synchronizing state between the NNX module and its Linen wrapper."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_Missing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class _Missing:\n  ...",
        "analysis": {
            "module_type": "_missing",
            "purpose": "A placeholder class to represent a missing or undefined value.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "This class is intended to be used as a sentinel value, likely to indicate that a parameter or attribute has not been set."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen(\n    nnx_class: tp.Callable[..., Module],\n    *args,\n    metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    name: str | None = None,\n    skip_rng: bool = False,\n    abstract_init: bool = True,\n    **kwargs,\n):\n  \"\"\"Shortcut of `nnx.bridge.ToLinen` if user is not changing any of its default fields.\"\"\"\n  return ToLinen(\n      nnx_class,\n      args=args,\n      kwargs=FrozenDict(kwargs),\n      metadata_fn=metadata_fn,\n      skip_rng=skip_rng,\n      name=name,\n  )",
        "analysis": {
            "functionality": "Provides a convenient shortcut for creating a `ToLinen` instance, simplifying the process when default fields of `ToLinen` are not being modified.",
            "usage": "Call `to_linen` with an NNX Module class and its initialization arguments. It returns a `ToLinen` instance, which acts as a Linen Module wrapper for the specified NNX Module."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_class",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_class(\n    base_nnx_class: type[M],\n    base_metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    base_skip_rng: bool = False,\n    **partial_kwargs: tp.Any,\n) -> type[ToLinen]:\n  \"\"\"A dynamically created Linen Module that wraps a specific NNX Module.\n\n  This class is not meant to be used directly. Instead, it is created and\n  returned by the `to_linen_class` function. It acts as a \"partially applied\"\n  version of the `ToLinen` wrapper, where the NNX module to be wrapped and\n  its default arguments are pre-configured.\n\n  When you instantiate this class, it behaves like a standard Linen module.\n  The arguments you provide during instantiation can override the defaults\n  that were set when this class was created by `to_linen_class`.\n\n  For example:\n    >>> from flax import linen as nn, nnx\n    >>> from MaxText.layers import linears\n    >>> # Create a specialized Linen wrapper for linears.DenseGeneral\n    >>> LinenDenseGeneral = to_linen_class(linears.DenseGeneral)\n    >>> # Now, LinenDenseGeneral can be used like a regular Linen module\n    >>> class MyModel(nn.Module):\n    ...   def setup(self):\n    ...     # Instantiate the wrapped linears.DenseGeneral with its arguments\n    ...     self.dense = LinenDenseGeneral(\n    ...         in_features_shape=10, out_features_shape=5\n    ...     )\n    ...   def __call__(self, x):\n    ...     return self.dense(x)\n\n  Attributes:\n    (The attributes are dynamically set by the `ToLinen` parent class based\n      on the arguments provided during instantiation.)\n  \"\"\"\n\n  def __init__(\n      self,\n      args=None,\n      kwargs=None,\n      nnx_class=None,\n      skip_rng=None,\n      metadata_fn=None,\n      name=_MISSING,\n      parent=_MISSING,\n      **other_kwargs,\n  ):\n    linen_kwargs = {}\n    if not isinstance(parent, _Missing):\n      linen_kwargs[\"parent\"] = parent\n    if not isinstance(name, _Missing):\n      linen_kwargs[\"name\"] = name\n    ToLinen.__init__(\n        self,\n        nnx_class=nnx_class or base_nnx_class,\n        args=args or (),\n        metadata_fn=metadata_fn or base_metadata_fn,\n        skip_rng=skip_rng or base_skip_rng,\n        kwargs=FrozenDict({**partial_kwargs, **(kwargs or {}), **other_kwargs}),\n        **linen_kwargs,\n    )\n\n  class ToLinenPartial(ToLinen):\n    \"\"\"A dynamically created Linen Module that wraps a specific NNX Module.\"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n      super().__init_subclass__(**kwargs)\n      cls.__init__ = __init__\n\n  ToLinenPartial.__init__ = __init__\n\n  return ToLinenPartial",
        "analysis": {
            "functionality": "Dynamically creates a Flax Linen Module that wraps a given JAX NNX Module, allowing it to be used within a Linen-based model.",
            "usage": "Call `to_linen_class` with an NNX Module class (e.g., `nnx.Linear`). This returns a new class that can be instantiated like a Linen Module. Arguments passed during instantiation of the returned class will be used to configure the underlying NNX Module. Example: `LinenDense = to_linen_class(nnx.Dense)` then `self.dense = LinenDense(features=64)` within a `nn.Module` setup."
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#RMSNorm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "class RMSNorm(nnx.Module):\n  \"\"\"RMS normalization.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      shard_mode: ShardMode = ShardMode.AUTO,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.ones,\n      parameter_memory_host_offload: bool = False,\n      scale_offset: float = 0.0,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.num_features = num_features\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.shard_mode = shard_mode\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n    self.scale_offset = scale_offset\n    self.scale = nnx.Param(\n        scale_init(rngs.params(), (num_features,), weight_dtype),\n        sharding=kernel_axes,\n    )\n\n  def __call__(self, x: jnp.ndarray, out_sharding: NamedSharding | None = None) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    x = jnp.asarray(x, jnp.float32)\n    mean2 = jnp.mean(lax.square(x), axis=-1, keepdims=True)\n    y = jnp.asarray(x * lax.rsqrt(mean2 + self.epsilon), self.dtype)\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"normalizations.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n    # out_sharding must be None in auto shard mode\n    if self.shard_mode != ShardMode.EXPLICIT:\n      out_sharding = None\n\n    scale = jnp.asarray(scale, self.dtype)\n    effective_scale = scale + self.scale_offset  # Apply offset\n    # broadcast 2nd input then element-wise mul\n    return jnp.einsum(\"i...k,...k->i...k\", y, effective_scale, out_sharding=out_sharding)",
        "analysis": {
            "module_type": "rms_norm",
            "purpose": "Applies Root Mean Square Layer Normalization to an input tensor.",
            "input": {
                "shape": "[batch_size, ..., num_features]",
                "dtype": "float32"
            },
            "processing_steps": [
                "Calculate the mean of the squared input along the last axis.",
                "Compute the square root of the mean plus epsilon.",
                "Divide the input by the computed square root.",
                "Scale the normalized input by a learnable parameter.",
                "Apply an optional offset to the scale.",
                "Perform element-wise multiplication of the normalized input and the effective scale."
            ],
            "output": {
                "shape": "[batch_size, ..., num_features]"
            },
            "dependencies": [
                "jax.lax.square",
                "jax.lax.rsqrt",
                "jnp.mean",
                "jnp.asarray",
                "jnp.einsum"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor.",
                "epsilon": "A small value added to the variance to prevent division by zero.",
                "dtype": "The data type of the output tensor.",
                "weight_dtype": "The data type of the learnable scale parameter.",
                "shard_mode": "Specifies how the module should be sharded.",
                "kernel_axes": "Axes for sharding the kernel.",
                "scale_init": "Initializer for the learnable scale parameter.",
                "parameter_memory_host_offload": "If True, moves the scale parameter to the device.",
                "scale_offset": "An offset added to the scale parameter."
            },
            "notes": [
                "The `scale` parameter is a learnable `nnx.Param`.",
                "The `__call__` method handles the core RMS normalization logic.",
                "If `parameter_memory_host_offload` is True, the scale parameter is moved to the device.",
                "If `shard_mode` is not `ShardMode.EXPLICIT`, `out_sharding` is set to None."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RMSNorm module with specified parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store initialization parameters.",
                        "Initialize the learnable scale parameter using `scale_init` and `weight_dtype`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.Rngs"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Applies RMS normalization to the input tensor.",
                    "input": {
                        "shape": "[batch_size, ..., num_features]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Convert input `x` to float32.",
                        "Calculate `mean2` (mean of squared `x`).",
                        "Compute normalized `y` using `x`, `mean2`, and `epsilon`.",
                        "Retrieve the `scale` parameter value.",
                        "Optionally move `scale` to device if `parameter_memory_host_offload` is True.",
                        "Set `out_sharding` to None if `shard_mode` is not `EXPLICIT`.",
                        "Convert `scale` to the module's `dtype`.",
                        "Calculate `effective_scale` by adding `scale_offset`.",
                        "Apply scaling using `jnp.einsum`."
                    ],
                    "output": {
                        "shape": "[batch_size, ..., num_features]"
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "jnp.mean",
                        "lax.square",
                        "lax.rsqrt",
                        "jax.device_put",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "The input `x` is cast to `jnp.float32`.",
                        "The output `y` is cast to the module's `dtype`.",
                        "The `out_sharding` parameter is used for explicit sharding, but is ignored if `shard_mode` is not `EXPLICIT`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#Qwen3NextRMSNorm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def Qwen3NextRMSNorm(num_features: int, eps: float, dtype: DType, weight_dtype: DType, *, rngs: nnx.Rngs):\n  \"\"\"\n  Used for input and post attention layernorms\n  in Qwen3NextDecoderLayer.\n\n  This normalization layer is specific to Qwen3-Next. Key characteristics:\n  1.  The learnable scale parameter `scale` is initialized to ZEROS.\n  2.  The scale is applied as `(1.0 + self.scale)`, making the initial scale effectively 1.0.\n      This matches the PyTorch implementation of Qwen3NextRMSNorm.\n  \"\"\"\n  return nnx.data(\n      RMSNorm(\n          num_features=num_features,\n          epsilon=eps,\n          dtype=dtype,\n          weight_dtype=weight_dtype,\n          scale_init=linen_initializers.zeros,\n          scale_offset=1.0,\n          rngs=rngs,\n      )\n  )",
        "analysis": {
            "functionality": "Creates a specific RMS normalization layer tailored for Qwen3-Next models.",
            "usage": "Instantiate this function with the number of features, epsilon, data types, and random number generators. It returns an RMSNorm module with a learnable scale initialized to zeros and an offset of 1.0, effectively starting with a scale of 1.0, matching PyTorch's Qwen3NextRMSNorm implementation. This is used for input and post-attention layernorms in Qwen3NextDecoderLayer."
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#Qwen3NextRMSNormGated",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "class Qwen3NextRMSNormGated(nnx.Module):\n  \"\"\"\n  This applies RMS Normalization and then a gated activation function (SiLU).\n  This is used within the Qwen3NextGatedDeltaNet.\n\n  The normalization is performed by an internal `RMSNorm` instance (`self.rms_norm`),\n  which has its own learnable `scale` parameter, initialized to ONES.\n\n  Attributes:\n    num_features: The number of features in the input.\n    eps: A small epsilon value to prevent division by zero in RMSNorm.\n    dtype: The datatype of the computation.\n    weight_dtype: The datatype of the internal RMSNorm scale.\n  \"\"\"\n\n  def __init__(self, num_features: int, eps: float, dtype: DType, weight_dtype: DType, *, rngs: nnx.Rngs):\n    self.num_features = num_features\n    self.eps = eps\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.rms_norm = nnx.data(\n        RMSNorm(\n            num_features=num_features,\n            epsilon=eps,\n            dtype=dtype,\n            weight_dtype=weight_dtype,\n            scale_init=nnx.initializers.ones,\n            rngs=rngs,\n        )\n    )\n\n  def __call__(self, hidden_states: Array, gate: Array) -> Array:\n    \"\"\"\n    Applies RMSNorm and then a SiLU gate.\n\n    Args:\n      hidden_states: The input array to be normalized (o). Shape: (..., F)\n      gate: The gating array for the activation (z). Shape: (..., F)\n            where F is num_features.\n\n    Returns:\n      The normalized and gated output array. Shape: (..., F)\n    \"\"\"\n    normalized_states = self.rms_norm(hidden_states)\n\n    # Gated Activation using SiLU (Sigmoid-weighted Linear Unit)\n    gated_states = normalized_states * jax.nn.silu(gate.astype(jnp.float32))\n\n    return gated_states.astype(self.dtype)",
        "analysis": {
            "functionality": "Applies RMS Normalization followed by a gated activation function (SiLU).",
            "usage": "Instantiate with `num_features`, `eps`, `dtype`, `weight_dtype`, and `rngs`. Call with `hidden_states` and `gate` arrays. Returns the normalized and gated output array."
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#rms_norm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def rms_norm(\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    shard_mode: ShardMode = ShardMode.AUTO,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.ones,\n    name: None | str = None,\n    parameter_memory_host_offload: bool = False,\n):\n  \"\"\"Creates a RMSNorm module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      RMSNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      shard_mode=shard_mode,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "functionality": "This function acts as a factory to create a Flax Linen RMSNorm module, adapting the NNX RMSNorm class for use within the Linen API.",
            "usage": "Call this function with the desired number of features and optional parameters like epsilon, data types, sharding mode, and initialization functions to obtain a configurable RMSNorm module compatible with Flax Linen. The module can then be used to normalize input tensors."
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#l2norm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def l2norm(x: Array, dim: int = -1, eps: float = 1e-6) -> Array:\n  \"\"\"L2 normalization function. Normalizes a vector to have a length of 1.\n\n  Args:\n    x: Input array.\n    dim: The axis or axes along which to normalize. Defaults to the last axis.\n    eps: Small epsilon to prevent division by zero.\n\n  Returns:\n    L2 normalized array with the same shape as x.\n  \"\"\"\n\n  inv_norm = jax.lax.rsqrt((x * x).sum(axis=dim, keepdims=True) + jnp.array(eps, dtype=x.dtype))\n  return x * inv_norm",
        "analysis": {
            "functionality": "Performs L2 normalization on an input array.",
            "usage": "Call the `l2norm` function with an input array `x`, an optional dimension `dim` to normalize along (defaults to the last axis), and an optional small epsilon `eps` to prevent division by zero. It returns an array of the same shape as `x` with L2 normalized values."
        }
    },
    {
        "block_name": "src/MaxText/layers/pipeline.py#Pipeline",
        "file_path": "src/MaxText/layers/pipeline.py",
        "code_block": "class Pipeline(nn.Module):\n  \"\"\"Module that implements pipelining across stages.\n\n  This module will loop over microbatches and execute the main body with a vmap for both the inputs and weights.\n  This will produce a pipeline pattern if the stage dimension is sharded.\n\n  Supports circular pipelines, and multiple layers per stage are used when a module that executes multiple layers\n  is passed as the layers input.\n\n  Attributes:\n    config: Importantly contains num_pipeline_microbatches, num_pipeline_repeats.\n    layers: A module instance that each stage can execute. It can either be a single layer such as a\n      LlamaDecoderLayer instance or scanned/looped set of decoder layers to execute multiple layers per stage.\n    mesh:  The device mesh of the system.\n    remat_policy: Remat policy to use for the loop iterations\n  \"\"\"\n\n  config: Config\n  layers: nn.Module  # The name of this property (layers) is reflected in the state pytree and thus also checkpoints.\n  mesh: Mesh\n  remat_policy: Any = None\n\n  def setup(self):  # pylint: disable=missing-function-docstring\n    self.num_stages = self.config.ici_pipeline_parallelism * self.config.dcn_pipeline_parallelism\n    self.forwarding_delay = 2 if self.config.pipeline_delay_activation_forwarding else 1\n    self.pipeline_microbatch_size = self.config.micro_batch_size_to_train_on // self.config.num_pipeline_microbatches\n    microbatches_per_stage = self.config.num_pipeline_microbatches // self.num_stages\n    self.microbatches_per_stage = microbatches_per_stage\n    self.use_circ_storage = self.need_circ_storage()\n\n    if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      self.batch_axis_name = \"activation_batch_no_exp\"\n      self.seq_len_axis_name = \"activation_length\"\n    else:\n      self.batch_axis_name = \"activation_batch\"\n      self.seq_len_axis_name = \"activation_length_no_exp\"\n\n  def need_circ_storage(self):\n    return (\n        self.config.num_pipeline_repeats > 1\n        and self.config.num_pipeline_microbatches > self.num_stages * self.forwarding_delay\n    )\n\n  def iterations_to_complete_first_microbatch_one_repeat(self):\n    # Return the number of iterations it takes for microbatch 0 to finish a repeat\n    return self.forwarding_delay * (self.num_stages - 1)\n\n  def iterations_to_complete_first_microbatch(self):\n    # Return the number of iterations it takes for microbatch 0 to finish the last stage of the last repeat\n    return (\n        self.config.num_pipeline_microbatches * (self.config.num_pipeline_repeats - 1)\n        + self.iterations_to_complete_first_microbatch_one_repeat()\n    )\n\n  def init_states(self, inputs):\n    \"\"\"Initialize components of state: state_io, shift, circular_storage and circular_storage_mover\n    Assumes input has already been reshaped into microbatches: [num_micro_batches, micro_batch_size, sequence, embed]\n\n    Returns a dictionary with properties\n      shift: zeros shape [num_stages, micro_size, sequence, embed]\n      prev_outputs: same shape as shift, only used when pipeline_delay_activation_forwarding is set to true, else None\n      state_io: reshaped inputs [num_stages, microbatches/stages, micro_size, sequence, embed]\n      circ_storage: zeros [num_stages, microbatches, micro_size, sequence, embed] when needed, else None\n      circ_storage_mover: zeros[num_stages, micro_size, sequence, embed] when needed, else None\n      loop_iteration: scalar set initially to 0.\n    \"\"\"\n\n    # Shift is used to rotate the output of each pipeline into the input of the next\n    # shift has shape [num_stages, micro_size, sequence, embed]\n    shift = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n\n    shift = nn.with_logical_constraint(\n        shift,\n        (\"activation_stage\", self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # Prev outputs has the same shape of the output (and shift)\n    if self.config.pipeline_delay_activation_forwarding:\n      prev_outputs = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n      prev_outputs = nn.with_logical_constraint(\n          prev_outputs,\n          (\"activation_stage\", self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n          rules=self.config.logical_axis_rules,\n          mesh=self.mesh,\n      )\n    else:\n      prev_outputs = None\n\n    # state_io (state input output) at first holds all of the input batches, but also will hold the outputs\n    #   as the pipeline runs/finishes\n    # state_io has shape [num_stages, microbatches/stages, micro_size, sequence, embed]\n    state_io = jnp.reshape(inputs, (self.num_stages, self.microbatches_per_stage) + inputs.shape[1:])\n    # We shard the pipeline_microbatch_size axis by data/fsdp, not num_microbatches since those are looped over.\n    state_io = nn.with_logical_constraint(\n        state_io,\n        (\"activation_stage\", None, self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # circ_storage is used to hold the final pipeline stage outputs before it is used for the next repeat. It is only\n    # needed when num_microbatches > num_stages, else instead the final stage will immediately pass to the first without\n    # additional storage.\n    # circ_storage has shape [num_stages, microbatches, micro_size, sequence, embed].\n    # Note that this shape is a factor of num_stages larger than necessary - each stage holds the global batch, but only\n    # stage 0 holds the real activations (since it will use them), the rest hold dummy ones. This amount of storage\n    # [global_batch, sequence, embed] is fine as long as there is some amount of additional sharding axes, e.g. FSDP,\n    # TP, DP (e.g. there are many devices that shard stage 0)\n    # We may look into alternatives using less storage if this becomes an issue (ideas in b/347603101).\n    if self.use_circ_storage:\n      circ_storage = jnp.zeros((self.num_stages,) + inputs.shape, dtype=inputs.dtype)\n    else:\n      circ_storage = None\n\n    # circ_storage_mover is used to push the microbatches from the pipeline into circ_storage with one buffer iteration\n    # of delay circ_storage_mover shape is same as shift: [num_stages, micro_size, sequence, embed]\n    if self.use_circ_storage:\n      circ_storage_mover = shift\n    else:\n      circ_storage_mover = None\n\n    init_loop_state = {\n        \"state_io\": state_io,\n        \"shift\": shift,\n        \"circ_storage\": circ_storage,\n        \"circ_storage_mover\": circ_storage_mover,\n        \"loop_iteration\": 0,\n        \"prev_outputs\": prev_outputs,\n    }\n    return init_loop_state\n\n  def get_iteration_inputs(self, loop_iteration, state_io, circ_storage, shift):\n    \"\"\"\n    Construct stages_in: the global array that is operated on for this iteration, shape same as\n    shift=[stages, micro_size, sequence, embed]\n    This is almost a rotated version of the last outputs, except for the first stage which must grab a new batch from\n    state_io or an old one from circ_storage\n    \"\"\"\n\n    # Setup potential input from state_io, which has a rotating microbatch index (size of microbatches_per_stage)\n    state_io_batch_idx = loop_iteration % self.microbatches_per_stage\n    state_io_slice = state_io[:, state_io_batch_idx]\n\n    if self.use_circ_storage:\n      # Setup potential input from circ_storage, which also has a rotating index for microbatch,\n      # size of num_microbatches\n      circ_storage_batch_idx = loop_iteration % self.config.num_pipeline_microbatches\n      circular_stage_in = circ_storage[:, circ_storage_batch_idx]\n    else:\n      # The last stage immediately flows into the first stage, use this rotated shift instead of circular storage\n      circular_stage_in = shift\n\n    # For early loop iterations we grab a new input for stage 0 from the state_io. Once each microbatch has left\n    # state_io we instead grab from the last stage's output (possibly buffered when num_microbatches > num_stages, e.g.\n    # from circ_storage).\n    first_stage_in = jnp.where(loop_iteration < self.config.num_pipeline_microbatches, state_io_slice, circular_stage_in)\n\n    # Note that first_stage_in may correspond to bubble computation during the last few iterations.\n    # However, these bubble computation results remain in the shift buffer (do not make it back to state_io) and are\n    # thus discarded / not returned.\n    # The final returned output is stored in the state_io, which has the appropriate total size of num_microbatches. The\n    # state_io will not contain bubble results at the end of the last iteration.\n\n    def select_state_or_input(first_stage_in, shift):\n      # Selects input for stage 0, shift for other stages\n      return jnp.where(jax.lax.broadcasted_iota(\"int32\", shift.shape, 0) == 0, first_stage_in, shift)\n\n    # Selects input (from stream_io) for stage 0, other stages get from shift (the rotated previous output)\n    stages_in = select_state_or_input(first_stage_in, shift)\n    stages_in = nn.with_logical_constraint(\n        stages_in,\n        (\"activation_stage\", self.batch_axis_name, self.seq_len_axis_name, \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n    return stages_in\n\n  def shard_dim_by_stages(self, x, dim: int):\n    # Shards a dimension by stages. Currently, the sharding of other dimensions are left up the compiler, alternatively\n    # we may want to copy over the sharding from the other input axes.\n    dims_mapping = [jax.sharding.PartitionSpec.UNCONSTRAINED] * x.ndim\n    dims_mapping[dim] = \"stage\"\n    dims_mapping = tuple(dims_mapping)\n    sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(*dims_mapping))\n    return jax.lax.with_sharding_constraint(x, sharding)\n\n  def get_microbatch_and_repeat_ids(self, loop_iteration):\n    \"\"\"Gets the microbatch_ids and repeat_ids for all stages on this loop_iteration. Works for both circular and\n    non-circular\"\"\"\n    # Stage 0 has processed one microbatch every loop_iter, but Stage 1 is 1 behind due to bubble, etc for other stages\n    microbatches_processed = jnp.maximum(loop_iteration - self.forwarding_delay * jnp.arange(self.num_stages), 0)\n    microbatch_ids = microbatches_processed % self.config.num_pipeline_microbatches\n    repeat_ids = microbatches_processed // self.config.num_pipeline_microbatches\n    return microbatch_ids, repeat_ids\n\n  def vmap_parallel_gather(self, weights, repeat_ids, repeat_dim_in_weights, stages_dim_in_weights):\n    \"\"\"Use vmap to implement a sharded parallel gather.\n    Parallel gather means each stage has its own weights, and gets one slice from it.\n    Args:\n      weights: Per-stage data to be gathered from.\n      repeat_ids: Integer tensor of shape [num_stages], the repeats of the stages.\n      repeat_dim_in_weights: The dimension in weights where repeat_ids are applied. The output will not\n        have this dimension.\n      stages_dim_in_weights: The dimension in weights that represents parallel stages.\n    Returns:\n      The per-stage gathered values. The shape is weights.shape but with repeat_dim_in_weights\n        removed.\n    \"\"\"\n\n    def _gather_one(x, repeat_id):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, repeat_id, 1, repeat_dim_in_weights), repeat_dim_in_weights)\n\n    gathered_weights_stage_dim = 0\n    repeat_ids = self.shard_dim_by_stages(repeat_ids, 0)\n    weights = self.shard_dim_by_stages(weights, stages_dim_in_weights)\n    stage_weights = jax.vmap(_gather_one, in_axes=(stages_dim_in_weights, 0), out_axes=gathered_weights_stage_dim)(\n        weights, repeat_ids\n    )\n    stage_weights = self.shard_dim_by_stages(stage_weights, gathered_weights_stage_dim)\n    return stage_weights\n\n  def vmap_gather(self, xs, ids, ids_dim):\n    \"\"\"Use vmap to implement a stage-wise sharded gather.\n\n    The stages share the same input, but they have different offsets.\n\n    Args:\n      xs: Data shared by all stages, to be gathered from.\n      ids: Integer tensor of shape [num_stages], the offsets of the stages.\n      ids_dim: The dimension in xs where ids are applied. In the output, this\n        dimension will be [num_stages], since each stage gets one slice.\n\n    Returns:\n      The per-stage gathered values. The shape is xs.shape but with ids_dim size\n        replaced with [num_stages].\n    \"\"\"\n\n    def _gather_one(x, i):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, i, 1, ids_dim), ids_dim)\n\n    ids = self.shard_dim_by_stages(ids, 0)\n    outs = jax.vmap(_gather_one, in_axes=(None, 0), out_axes=ids_dim)(xs, ids)\n    return self.shard_dim_by_stages(outs, 0)\n\n  def get_new_loop_state(self, output, loop_state):\n    \"\"\"\n    Update the various buffers given the output of the most recent iteration\n    * state_io: rotates left/up by 1 (the whole created in the last slot is filled with the most recent pipeline output)\n       * Pushing inputs up from top of state_io into first stage of shift\n       * Pulling outputs up from last stage of shift into bottom of state_io\n    * shift: rotate output (or prev_outputs if using delay) right/down by 1 - we imagine the pipeline moves to\n               right/down\n    * circ_storage: pushes circ_storage_mover (the output of the previous iteration) into rotating index of circ_storage\n    * circ_storage_mover: assigned to rotated output and pushed into circ_storage on the next iteration\n    * prev_outputs: is set to the current output\n    \"\"\"\n\n    old_state_io = loop_state[\"state_io\"]\n    old_circ_storage = loop_state[\"circ_storage\"]\n    old_circ_storage_mover = loop_state[\"circ_storage_mover\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n    old_prev_outputs = loop_state[\"prev_outputs\"]\n\n    def _rotate_right(arr):\n      # Use lax.slice to avoid generating a gather.\n      last = jax.lax.slice_in_dim(arr, self.num_stages - 1, self.num_stages, axis=0)\n      except_last = jax.lax.slice_in_dim(arr, 0, self.num_stages - 1, axis=0)\n      return jnp.concatenate([last, except_last], axis=0)\n\n    def _shift_right(arr):\n      padding = [[1, 0]] + [[0, 0]] * (arr.ndim - 1)\n      # Use lax.slice to guarantee the gradient is a pad.\n      return jax.lax.slice(jnp.pad(arr, padding), [0] * arr.ndim, arr.shape)\n\n    # Shift either rotates or shifts depending on if the last stage immediately must send to first or not\n    # For non-circular pipelines, the last stage does not need to send to first\n    # For circular pipelines with #micro = #stages, last stage immediately sends to first\n    # For circular pipelines with #micro > stages (circ_storage), last stage sends to circ storage\n    def _update_shift(output_in):\n      if self.config.num_pipeline_repeats == 1 or self.use_circ_storage:\n        return _shift_right(output_in)  # last stage does not have to send to first immediately\n      else:\n        return _rotate_right(output_in)  # last stage must immediately send to first\n\n    if self.config.pipeline_delay_activation_forwarding:\n      new_shift = _update_shift(old_prev_outputs)\n      new_prev_outputs = output\n    else:\n      new_shift = _update_shift(output)\n      new_prev_outputs = None\n\n    if self.use_circ_storage:\n      # Insert the circ_storage_mover into new_circ_storage at a microbatch-rotating index.\n      # circ_storage_mover still points to the output of PREVIOUS iteration, which should aid in allowing overlapped\n      # compute/async transfers\n      def _rotate_right_and_update(circ_storage_mover_in, circ_storage_in):\n        rotated = _rotate_right(circ_storage_mover_in)\n        rotated = jnp.expand_dims(rotated, 1)\n        # We rotate the pushing index into circ storage, and ensure that microbatch 0 lands in index 0\n        offset = (\n            loop_iteration - self.iterations_to_complete_first_microbatch_one_repeat() - 1\n        ) % self.config.num_pipeline_microbatches  # Note extra -1 b/c grabbing from the\n        # previous output - using circ_storage_mover before it is updated\n        return jax.lax.dynamic_update_slice_in_dim(circ_storage_in, rotated, offset, axis=1)\n\n      new_circ_storage = _rotate_right_and_update(old_circ_storage_mover, old_circ_storage)\n      new_circ_storage_mover = output\n    else:\n      new_circ_storage = None\n      new_circ_storage_mover = None\n\n    # Rotate stream_io left/up by 1 on rotating micro/stage index (stream_buf_idx), replacing the last/bottom with the\n    # last stage output\n    stream_buf_idx = loop_iteration % self.microbatches_per_stage\n    stream_slice = old_state_io[:, stream_buf_idx]\n\n    def _update_state_io(state_in, stream_slice, output):\n      # Shift the current slice to the left, then fill the last stage with the final output.\n      padding = [[0, 1]] + [[0, 0]] * (stream_slice.ndim - 1)\n      stream_slice = jax.lax.slice_in_dim(jnp.pad(stream_slice, padding), 1, stream_slice.shape[0] + 1, axis=0)\n      stream_slice = jnp.where(\n          jax.lax.broadcasted_iota(\"int32\", stream_slice.shape, 0) == self.num_stages - 1, output, stream_slice\n      )\n      stream_slice = jnp.expand_dims(stream_slice, 1)\n      return jax.lax.dynamic_update_slice_in_dim(state_in, stream_slice, stream_buf_idx, axis=1)\n\n    new_state = _update_state_io(old_state_io, stream_slice, output)\n\n    new_loop_state = {\n        \"state_io\": new_state,\n        \"shift\": new_shift,\n        \"circ_storage\": new_circ_storage,\n        \"circ_storage_mover\": new_circ_storage_mover,\n        \"loop_iteration\": loop_iteration + 1,\n        \"prev_outputs\": new_prev_outputs,\n    }\n    return new_loop_state\n\n  def permute_output_micro_per_stage_dim(self, output):\n    # The first real output (microbatch 0) takes a certain amount of loop iterations to finish and be pushed to\n    # state_io - it will land on a different index of state_io depending on the number of iterations.\n    microbatch_0_idx = self.iterations_to_complete_first_microbatch() % self.microbatches_per_stage\n    permutation = (\n        np.arange(self.microbatches_per_stage) + microbatch_0_idx\n    ) % self.microbatches_per_stage  # permute so the value in land_idx is moved into idx 0, and (land_idx + 1) appear\n    # in idx 1, etc\n    output = output[:, permutation]\n    return output\n\n  def get_current_stage_weights(self, pipeline_weights, loop_iteration):\n    \"\"\"\n    Gets the current weights used for one iteration. Outputs a pytree whose arrays have leading dimension of stages, e.g.\n    {'mlp': 'wo': [stages, mlp, embed]}. Stage 0 will use the 0th index of this pytree, Stage 1 the 1st index, etc.\n    For non-circular pipelines, this simply returns all weights - every weight is used in every iteraiton. However\n    for circular pipelines each stage grabs only the weights corresponding to the current repeat.\n    \"\"\"\n    if self.config.num_pipeline_repeats > 1:\n      return self.get_current_repeat_from_stages(pipeline_weights, loop_iteration)\n    else:\n      return pipeline_weights\n\n  def get_current_repeat_from_stages(self, weights, loop_iteration):\n    \"\"\"get current repeat from stages\"\"\"\n    _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    def gather_weights_for_stages_in(weights):\n      return jax.tree.map(\n          functools.partial(\n              self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n          ),\n          weights,\n      )\n\n    circular_metadata_params = {\n        nn.PARTITION_NAME: \"circular_repeats\",\n        \"sub_weight_split_dims_mapping\": (None,),\n        \"is_initializing\": self.is_initializing(),\n        \"x_times\": self.config.num_pipeline_repeats,\n        \"optimizer_dims_mapping\": None,\n    }\n    weights = meta.remove_axis(\n        weights, 0, circular_metadata_params\n    )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one circular\n    # entry per stage.\n    weights = gather_weights_for_stages_in(weights)\n    return weights\n\n  def get_vmap_func_for_init(self):\n    \"\"\"This vmap func is used to initialize the weights only on init.\"\"\"\n\n    def func_to_vmap(body_instance, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      return body_instance(stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0, \"_overwrite_with_gradient\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def get_main_vmap_func_for_iterations(self):\n    \"\"\"\n    Returns main stage function vmapped by number of stages.\n    This becomes a vmap over a single layer instance if body_instance is a single layer,\n    else a set of layers if body_instance is a set of layers.\n    \"\"\"\n\n    def func_to_vmap(\n        body_instance, weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode\n    ):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      weights = meta.remove_axis(\n          weights,\n          0,\n          {\n              nn.PARTITION_NAME: \"layers\",\n              \"sub_weight_split_dims_mapping\": (None,),\n              \"is_initializing\": self.is_initializing(),\n              \"x_times\": self.num_stages,\n          },\n      )\n      return body_instance.apply(weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def run_one_iteration(\n      self, loop_state, pipeline_weights, positions, segment_ids, deterministic, model_mode, decoder_layer_instance\n  ):\n    \"\"\"Run one loop iteration - gets weights and inputs for each stage, run the stages in parallel,\n    and update the loop state.\"\"\"\n    state_io = loop_state[\"state_io\"]\n    shift = loop_state[\"shift\"]\n    circ_storage = loop_state[\"circ_storage\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n\n    microbatch_ids, _ = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    stages_inputs = self.get_iteration_inputs(loop_iteration, state_io, circ_storage, shift)\n    # We checkpoint stages_inputs since we are grabbing only one slice of the state_io, don't need to save the entire\n    # buffer.\n    stages_inputs = jax.ad_checkpoint.checkpoint_name(stages_inputs, \"iteration_input\")\n    stages_positions = self.vmap_gather(positions, microbatch_ids, 0) if positions is not None else None\n    stages_segment_ids = self.vmap_gather(segment_ids, microbatch_ids, 0) if segment_ids is not None else None\n\n    vmap_func = self.get_main_vmap_func_for_iterations()\n\n    if self.config.num_pipeline_repeats > 1:\n      _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n      def prepare_vars_for_main_vmap(weights):\n        def gather_weights_for_stages_in(weights):\n          return jax.tree.map(\n              functools.partial(\n                  self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n              ),\n              weights,\n          )\n\n        circular_metadata_params = {\n            nn.PARTITION_NAME: \"circular_repeats\",\n            \"sub_weight_split_dims_mapping\": (None,),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.config.num_pipeline_repeats,\n            \"optimizer_dims_mapping\": None,\n        }\n        weights = meta.remove_axis(\n            weights, 0, circular_metadata_params\n        )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one\n        # circular entry per stage.\n        weights = gather_weights_for_stages_in(weights)\n        return weights\n\n      vmap_func = nn.map_variables(\n          vmap_func,\n          mapped_collections=[\"params\", \"_overwrite_with_gradient\", \"non_trainable\", \"summaries\", \"intermediates\"],\n          mutable=True,\n          trans_in_fn=prepare_vars_for_main_vmap,\n      )\n\n    stage_weights = self.get_current_stage_weights(pipeline_weights, loop_iteration)\n    stages_output = vmap_func(\n        decoder_layer_instance,\n        stage_weights,\n        stages_inputs,\n        stages_segment_ids,\n        stages_positions,\n        deterministic,\n        model_mode,\n    )\n    if self.config.scan_layers:\n      stages_output = stages_output[0]\n\n    new_state = self.get_new_loop_state(stages_output, loop_state)\n    return new_state\n\n  def get_pipeline_remat_policy(self):\n    \"\"\"Returns the pipeline remat policy for this pipeline.\"\"\"\n    # We ensure that the decoder layer inputs are saved, although we leave it to a custom\n    # policy if they should be saved to device or offloaded.\n    if self.config.remat_policy == \"custom\":\n      return self.remat_policy\n\n    save_input_policy = jax.checkpoint_policies.save_only_these_names(\"iteration_input\", \"decoder_layer_input\")\n    if self.remat_policy is not None:\n      remat_policy = jax.checkpoint_policies.save_from_both_policies(self.remat_policy, save_input_policy)\n    else:\n      remat_policy = save_input_policy\n    return remat_policy\n\n  def get_weight_sharding(self, *init_args):\n    \"\"\"get weight sharding function for this pipeline.\"\"\"\n    # Returns a partition spec of all weights. Requires passing in arguments to init.\n    key = jax.random.PRNGKey(0)\n    keys = {\"params\": key, \"dropout\": key, \"aqt\": key}\n    weights = self.init(keys, *init_args)\n\n    def get_partition_spec(pytree):\n      def _is_leaf(x):\n        return isinstance(x, nn.spmd.LogicallyPartitioned)\n\n      def get_partition_spec_leaf(leaf):\n        return leaf.get_partition_spec()\n\n      partition_spec_tree = jax.tree.map(get_partition_spec_leaf, pytree, is_leaf=_is_leaf)\n      return partition_spec_tree\n\n    partition_spec_with_extra_layer = get_partition_spec(weights)\n    partition_spec = {\"params\": partition_spec_with_extra_layer[\"params\"][\"layers\"]}\n    return partition_spec\n\n  def get_physical_spec_no_fsdp(self, full_logical):\n    \"\"\"\n    Get physical spec without fsdp.\n\n    TODO: Remove the expert sharding on attention weights as well, since those act like fsdp.\n\n    Args:\n      full_logical: original logical partition specs of all weights\n\n    Returns:\n      Modified physical spec with \"fsdp\" and \"fsdp_transpose\" removed\n    \"\"\"\n\n    def remove_fsdp_sharding(sharding_tree):\n      def _remove_fsdp_from_partition_spec(named_sharding):\n        if isinstance(named_sharding, jax.sharding.NamedSharding):\n          new_spec = []\n          for axis in named_sharding.spec:\n            if axis is None:\n              new_spec.append(None)\n            elif isinstance(axis, str):\n              if axis not in (\"fsdp\", \"fsdp_transpose\"):\n                new_spec.append(axis)\n              else:\n                new_spec.append(None)\n            elif isinstance(axis, (list, tuple)):\n              new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n              new_spec.append(tuple(new_axis))\n            else:\n              raise ValueError(f\"Unsupported axis type: {type(axis)}\")\n          return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n        return named_sharding\n\n      return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n    physical = nn.logical_to_mesh_sharding(full_logical, mesh=self.mesh, rules=self.config.logical_axis_rules)\n    physical_no_fsdp = remove_fsdp_sharding(physical)\n    return physical_no_fsdp\n\n  def all_gather_over_fsdp(self, sharding_info):\n    physical_constraint_no_fsdp = self.get_physical_spec_no_fsdp(sharding_info)\n    return jax.lax.with_sharding_constraint(self.layers.variables, physical_constraint_no_fsdp)\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      segment_ids: jnp.ndarray,\n      positions: jnp.ndarray,\n      deterministic: bool,\n      model_mode=MODEL_MODE_TRAIN,\n      partition_spec=None,  # Pytree of sharding specifications of the weights (aka self.layers.variables)\n  ) -> jnp.ndarray:\n    \"\"\"The main method that maps the series of decoder layer inputs to final layer outputs.\n    Has the same signature of a single decoder layer, and expects the same shapes, e.g. the inputs should have shape\n    [global_batch], and internally this will be reshapped into microbatches.\n    \"\"\"\n    # Reshape inputs of [global_batch, ...] to [microbatches, pipeline_microbatch_sizes, ...]\n    inputs = inputs.reshape(\n        (\n            self.config.num_pipeline_microbatches,\n            self.pipeline_microbatch_size,\n            self.config.max_target_length,\n            self.config.emb_dim,\n        )\n    )\n    example_inputs = jax.lax.broadcast(inputs[0], [self.num_stages])  # dummy inputs fed to initialize the module\n    # weights.\n    ag_sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(None, None))\n    if positions is not None:\n      # AG positions\n      positions = jax.lax.with_sharding_constraint(positions, ag_sharding)\n\n      positions = positions.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_position = jax.lax.broadcast(positions[0], [self.num_stages])\n      position_idx = 0\n    else:\n      example_position = None\n      position_idx = None\n    if segment_ids is not None:\n      segment_ids = jax.lax.with_sharding_constraint(segment_ids, ag_sharding)\n      segment_ids = segment_ids.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_segmentation = jax.lax.broadcast(segment_ids[0], [self.num_stages])\n      segment_idx = 0\n    else:\n      example_segmentation = None\n      segment_idx = None\n\n    loop_state = self.init_states(inputs)\n\n    # Each microbatch should go through each stage (with repeats) - so there is num_micro * (num_stages * repeats)\n    # compute to perform\n    # Each iteration is vmapped by num_stages, so the number of iterations should be\n    # num_micro * num_stages * repeats / num_stages = num_micro * repeats\n    # However due to the pipeline bubble some iterations process less than num_stages microbatches. It takes\n    # num_micro * repeat iterations for the last microbatch to start the final repeat, then an additional\n    # num_stages - 1 to finish the final repeat.\n    # Thus the total iterations is num_micro * repeat + num_stages - 1, & we may consider the num_stages - 1 as bubble.\n    # The bubble doubles when we use forwarding delay.\n    bubble_iterations = self.forwarding_delay * (self.num_stages - 1)\n    real_iterations = self.config.num_pipeline_microbatches * self.config.num_pipeline_repeats\n    total_iterations = real_iterations + bubble_iterations\n\n    if self.is_initializing():\n      vmap_func = self.get_vmap_func_for_init()\n\n      if self.config.num_pipeline_repeats > 1:\n        # To shard the weights on initialization for the circular pipeline we create weights of\n        # shape [num_repeat, num_stages, ...] (e.g. [num_repeat, num_stages, embed, mlp]) and shard the num_stages axis.\n        # We wrap the main stage vmap with a num_repeat vmap to generate this axis only for parameter initialization.\n        vmap_func = nn.vmap(\n            vmap_func,\n            in_axes=(0, segment_idx, position_idx, None, None),\n            variable_axes={\n                \"params\": 0,\n                \"_overwrite_with_gradient\": 0,\n                \"non_trainable\": 0,\n                \"hyper_params\": 0,\n            },\n            split_rngs={\"params\": True, \"dropout\": self.config.enable_dropout},\n            metadata_params={\n                nn.PARTITION_NAME: \"circular_repeats\",\n                \"sub_weight_split_dims_mapping\": (None,),\n                \"is_initializing\": True,\n                \"x_times\": self.config.num_pipeline_repeats,\n                \"optimizer_dims_mapping\": None,\n            },\n        )\n\n        example_inputs = jax.lax.broadcast(example_inputs, [self.config.num_pipeline_repeats])\n        example_segmentation = (\n            jax.lax.broadcast(example_segmentation, [self.config.num_pipeline_repeats])\n            if example_segmentation is not None\n            else None\n        )\n        example_position = (\n            jax.lax.broadcast(example_position, [self.config.num_pipeline_repeats])\n            if example_position is not None\n            else None\n        )\n      # We only need to run one set of stages to initialize the variables, instead of looping over all microbatches for\n      # the full total_iterations.\n      stage_outputs = vmap_func(\n          self.layers, example_inputs, example_segmentation, example_position, deterministic, model_mode\n      )\n      if self.config.scan_layers:\n        stage_outputs = stage_outputs[0]\n\n      # We return something of the correct shape (global_batch, sequence, embed) by reshaping a single stages output\n      # which has shape [pipeline_microbatch_size, sequence, embed]\n      if self.config.num_pipeline_repeats > 1:\n        stage_outputs = stage_outputs[0]  # Remove extra dimension created for the circular vmap\n      broadcasted_stage_outpus = jax.lax.broadcast(\n          stage_outputs[0], [self.config.micro_batch_size_to_train_on // self.pipeline_microbatch_size]\n      )\n      return jnp.reshape(\n          broadcasted_stage_outpus,\n          [self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim],\n      )\n\n    if self.config.pipeline_fsdp_ag_once:\n      all_pipeline_weights = all_gather_over_fsdp(\n          self.layers.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n      )\n    else:\n      all_pipeline_weights = self.layers.variables\n\n    def run_iteration_scannable(model, loop_state, xs):\n      # flax transforms like nn.scan and nn.remat can only be applied to nn.module classes or nn.module instances, so we\n      # explicitly wrap the run_one_iteration in this method - the 1st argument model (`self`) is a nn.module instance.\n      return (\n          model.run_one_iteration(\n              loop_state, all_pipeline_weights, positions, segment_ids, deterministic, model_mode, model.layers\n          ),\n          None,\n      )\n\n    if self.config.set_remat_policy_on_pipeline_iterations:\n      run_iteration_scannable = nn.remat(\n          run_iteration_scannable,\n          prevent_cse=not self.config.scan_pipeline_iterations,  # prevent_cse not used with scan\n          policy=self.get_pipeline_remat_policy(),\n      )\n\n    # The scan cannot be used on init since it broadcasts the weights, which aren't yet initialized.\n    if self.config.scan_pipeline_iterations:\n      variable_carry = []\n      variable_broadcast = [\n          \"params\",\n          \"_overwrite_with_gradient\",\n      ]  # All loop iterations need the weights for the full pipeline.\n      if self.is_mutable_collection(\"non_trainable\"):\n        variable_carry.append(\"non_trainable\")\n      else:\n        variable_broadcast.append(\"non_trainable\")\n      run_all_iterations_scanned = nn.scan(\n          run_iteration_scannable,\n          variable_axes={\n              \"summaries\": 0,\n              \"aux_loss\": 0,\n              \"intermediates\": 0,\n              \"hyper_params\": 0,\n          },\n          variable_broadcast=variable_broadcast,\n          variable_carry=variable_carry,\n          # Dropout/aqt keys will be split for each iteration.\n          split_rngs={\"random\": True},\n          length=total_iterations,\n      )\n      loop_state, _ = run_all_iterations_scanned(self, loop_state, None)\n    else:\n      for _ in range(total_iterations):\n        loop_state, _ = run_iteration_scannable(self, loop_state, None)\n\n    # The final output is located in the input/output array, however the output microbatches may be permuted relative to\n    # the input\n    final_output = self.permute_output_micro_per_stage_dim(loop_state[\"state_io\"])\n\n    # reshape outputs to match input shape of total batch instead of microbatches [batch, sequence, embed]\n    final_output = jnp.reshape(\n        final_output, (self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim)\n    )\n\n    return final_output",
        "analysis": {
            "module_type": "pipeline",
            "purpose": "Implements pipelining across multiple stages for parallel processing of microbatches.",
            "input": {
                "shape": "[global_batch, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Reshape input to microbatches.",
                "Initialize pipeline states.",
                "Iterate through total_iterations (real_iterations + bubble_iterations).",
                "In each iteration: get stage inputs, gather segment_ids and positions, get current stage weights.",
                "Run stages in parallel using a vmapped function.",
                "Update pipeline loop state.",
                "Permute output microbatches.",
                "Reshape final output to match original batch shape."
            ],
            "output": {
                "shape": "[global_batch, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.linen",
                "flax.core.meta",
                "functools",
                "numpy",
                "MaxText.common_types.Config",
                "MaxText.sharding.all_gather_over_fsdp"
            ],
            "parameters": {
                "config": "Configuration object containing pipeline parameters like num_pipeline_microbatches, num_pipeline_repeats, etc.",
                "layers": "A nn.Module instance representing the layers to be executed by each stage.",
                "mesh": "The device mesh of the system for distributed computation.",
                "remat_policy": "Rematerialization policy to use for loop iterations."
            },
            "notes": [
                "Supports circular pipelines and multiple layers per stage.",
                "Handles pipeline bubbles and forwarding delays.",
                "Includes logic for weight sharding and initialization.",
                "Can optionally use `nn.scan` for iteration, or a standard for loop.",
                "Supports checkpointing of iteration inputs."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes pipeline-specific attributes based on the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate number of stages.",
                        "Determine forwarding delay.",
                        "Calculate pipeline microbatch size.",
                        "Calculate microbatches per stage.",
                        "Determine if circular storage is needed.",
                        "Set batch and sequence length axis names based on expert shard attention option."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "need_circ_storage": {
                    "purpose": "Determines if circular storage is required based on pipeline configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if num_pipeline_repeats > 1 and num_pipeline_microbatches is sufficiently large compared to stages and delay."
                    ],
                    "output": {
                        "shape": "bool"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "iterations_to_complete_first_microbatch_one_repeat": {
                    "purpose": "Calculates the number of iterations needed for the first microbatch to complete one repeat.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate iterations based on forwarding delay and number of stages."
                    ],
                    "output": {
                        "shape": "int"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "iterations_to_complete_first_microbatch": {
                    "purpose": "Calculates the total number of iterations for the first microbatch to finish the last repeat.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate iterations based on total microbatches, repeats, and iterations for one repeat."
                    ],
                    "output": {
                        "shape": "int"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "init_states": {
                    "purpose": "Initializes the state variables required for the pipeline loop.",
                    "input": {
                        "shape": "[num_micro_batches, micro_batch_size, sequence, embed]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize 'shift' buffer.",
                        "Initialize 'prev_outputs' buffer if pipeline_delay_activation_forwarding is true.",
                        "Reshape and initialize 'state_io' buffer.",
                        "Initialize 'circ_storage' if needed.",
                        "Initialize 'circ_storage_mover' if needed.",
                        "Initialize 'loop_iteration' to 0."
                    ],
                    "output": {
                        "shape": "dict"
                    },
                    "dependencies": [
                        "jnp.zeros",
                        "nn.with_logical_constraint",
                        "jnp.reshape"
                    ],
                    "notes": [
                        "Assumes input has already been reshaped into microbatches.",
                        "Applies logical constraints to buffers."
                    ]
                },
                "get_iteration_inputs": {
                    "purpose": "Constructs the input array for all stages in a given iteration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine the microbatch index for state_io.",
                        "Slice state_io for the current microbatch.",
                        "Determine the microbatch index for circ_storage if used.",
                        "Slice circ_storage or use shift for circular input.",
                        "Use jnp.where to select between state_io slice and circular input for the first stage.",
                        "Use jnp.where to select between first_stage_in and shift for all stages.",
                        "Apply logical constraint to the resulting stages_in."
                    ],
                    "output": {
                        "shape": "[num_stages, micro_size, sequence, embed]"
                    },
                    "dependencies": [
                        "jnp.where",
                        "jax.lax.broadcasted_iota",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "Handles the 'bubble' computation during later iterations."
                    ]
                },
                "shard_dim_by_stages": {
                    "purpose": "Applies sharding constraint to a specified dimension by stages.",
                    "input": {
                        "shape": "Any tensor `x`",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define partition spec with 'stage' on the specified dimension.",
                        "Create NamedSharding object.",
                        "Apply sharding constraint using jax.lax.with_sharding_constraint."
                    ],
                    "output": {
                        "shape": "Same as input `x`"
                    },
                    "dependencies": [
                        "jax.sharding.PartitionSpec",
                        "jax.sharding.NamedSharding",
                        "jax.lax.with_sharding_constraint"
                    ],
                    "notes": []
                },
                "get_microbatch_and_repeat_ids": {
                    "purpose": "Calculates the microbatch and repeat IDs for all stages in the current loop iteration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate microbatches processed by each stage, considering forwarding delay.",
                        "Compute microbatch IDs using modulo operation.",
                        "Compute repeat IDs using integer division."
                    ],
                    "output": {
                        "shape": "tuple(microbatch_ids, repeat_ids)"
                    },
                    "dependencies": [
                        "jnp.maximum",
                        "jnp.arange"
                    ],
                    "notes": [
                        "Works for both circular and non-circular pipelines."
                    ]
                },
                "vmap_parallel_gather": {
                    "purpose": "Gathers weights for each stage in a sharded parallel manner.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define a helper function `_gather_one` to slice a single element.",
                        "Shard repeat_ids and weights by stages.",
                        "Use jax.vmap to apply `_gather_one` across stages and repeat IDs.",
                        "Shard the gathered weights by stages."
                    ],
                    "output": {
                        "shape": "Gathered weights with repeat dimension removed."
                    },
                    "dependencies": [
                        "jnp.squeeze",
                        "jax.lax.dynamic_slice_in_dim",
                        "jax.vmap",
                        "self.shard_dim_by_stages"
                    ],
                    "notes": [
                        "Each stage gets one slice from its own weights."
                    ]
                },
                "vmap_gather": {
                    "purpose": "Gathers data for each stage using vmap, where stages share the same input but have different offsets.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define a helper function `_gather_one` to slice a single element.",
                        "Shard the input IDs by stages.",
                        "Use jax.vmap to apply `_gather_one` across stages.",
                        "Shard the output by stages."
                    ],
                    "output": {
                        "shape": "Per-stage gathered values with ids_dim replaced by [num_stages]."
                    },
                    "dependencies": [
                        "jnp.squeeze",
                        "jax.lax.dynamic_slice_in_dim",
                        "jax.vmap",
                        "self.shard_dim_by_stages"
                    ],
                    "notes": []
                },
                "get_new_loop_state": {
                    "purpose": "Updates the pipeline's loop state based on the output of the current iteration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define helper functions `_rotate_right` and `_shift_right` for buffer manipulation.",
                        "Define `_update_shift` to handle shift buffer updates based on pipeline type.",
                        "Update 'shift' and 'prev_outputs' based on pipeline delay activation.",
                        "Update 'circ_storage' and 'circ_storage_mover' if circular storage is used.",
                        "Update 'state_io' by rotating and inserting the current output.",
                        "Increment 'loop_iteration'."
                    ],
                    "output": {
                        "shape": "dict (updated loop state)"
                    },
                    "dependencies": [
                        "jax.lax.slice_in_dim",
                        "jnp.concatenate",
                        "jnp.pad",
                        "jax.lax.slice",
                        "jnp.expand_dims",
                        "jax.lax.dynamic_update_slice_in_dim",
                        "jax.lax.broadcasted_iota",
                        "jnp.where"
                    ],
                    "notes": [
                        "Manages the rotation and shifting of data between pipeline stages."
                    ]
                },
                "permute_output_micro_per_stage_dim": {
                    "purpose": "Permutes the output microbatches to align them correctly after processing.",
                    "input": {
                        "shape": "[num_stages, microbatches/stages, micro_size, sequence, embed]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the index where microbatch 0 lands in state_io.",
                        "Create a permutation array based on this index.",
                        "Apply the permutation to the output."
                    ],
                    "output": {
                        "shape": "Permuted output tensor."
                    },
                    "dependencies": [
                        "self.iterations_to_complete_first_microbatch",
                        "np.arange",
                        "jnp.where"
                    ],
                    "notes": [
                        "Ensures that the output microbatches are in the correct order relative to the input."
                    ]
                },
                "get_current_stage_weights": {
                    "purpose": "Retrieves the weights relevant for the current iteration, especially for circular pipelines.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if num_pipeline_repeats > 1.",
                        "If so, call `get_current_repeat_from_stages`.",
                        "Otherwise, return all weights."
                    ],
                    "output": {
                        "shape": "Pytree of weights for the current stage/repeat."
                    },
                    "dependencies": [
                        "self.get_current_repeat_from_stages"
                    ],
                    "notes": []
                },
                "get_current_repeat_from_stages": {
                    "purpose": "Gathers the specific weights for the current repeat in a circular pipeline.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get repeat IDs for the current loop iteration.",
                        "Define a helper function `gather_weights_for_stages_in` using `vmap_parallel_gather`.",
                        "Remove the circular metadata axis from weights.",
                        "Apply `gather_weights_for_stages_in` to the weights."
                    ],
                    "output": {
                        "shape": "Pytree of weights, with a leading dimension for stages."
                    },
                    "dependencies": [
                        "self.get_microbatch_and_repeat_ids",
                        "self.vmap_parallel_gather",
                        "jax.tree.map",
                        "functools.partial",
                        "meta.remove_axis",
                        "nn.PARTITION_NAME"
                    ],
                    "notes": []
                },
                "get_vmap_func_for_init": {
                    "purpose": "Returns a vmapped function specifically for initializing model weights.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define `func_to_vmap` which calls the body instance.",
                        "Create a vmapped function using `nn.vmap` with appropriate axes and variable collections."
                    ],
                    "output": {
                        "shape": "A vmapped function."
                    },
                    "dependencies": [
                        "nn.vmap"
                    ],
                    "notes": [
                        "Used only during model initialization."
                    ]
                },
                "get_main_vmap_func_for_iterations": {
                    "purpose": "Returns the main vmapped function used for running pipeline iterations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define `func_to_vmap` which applies the body instance with weights.",
                        "Remove the 'layers' axis from weights.",
                        "Create a vmapped function using `nn.vmap` with appropriate axes and variable collections."
                    ],
                    "output": {
                        "shape": "A vmapped function."
                    },
                    "dependencies": [
                        "nn.vmap",
                        "meta.remove_axis",
                        "nn.PARTITION_NAME"
                    ],
                    "notes": []
                },
                "run_one_iteration": {
                    "purpose": "Executes a single iteration of the pipeline, including weight fetching, stage execution, and state updates.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract state components from loop_state.",
                        "Get microbatch and repeat IDs.",
                        "Determine stage inputs using `get_iteration_inputs`.",
                        "Checkpoint stage inputs.",
                        "Gather stage positions and segment IDs using `vmap_gather`.",
                        "Get the main vmapped function for iterations.",
                        "If num_pipeline_repeats > 1, modify the vmap function to handle circular weights.",
                        "Get current stage weights using `get_current_stage_weights`.",
                        "Execute the vmapped function with stage weights and inputs.",
                        "Handle `scan_layers` if applicable.",
                        "Update the loop state using `get_new_loop_state`."
                    ],
                    "output": {
                        "shape": "dict (updated loop state)"
                    },
                    "dependencies": [
                        "self.get_microbatch_and_repeat_ids",
                        "self.get_iteration_inputs",
                        "jax.ad_checkpoint.checkpoint_name",
                        "self.vmap_gather",
                        "self.get_main_vmap_func_for_iterations",
                        "nn.map_variables",
                        "self.get_current_stage_weights",
                        "self.get_new_loop_state"
                    ],
                    "notes": []
                },
                "get_pipeline_remat_policy": {
                    "purpose": "Returns the rematerialization policy for the pipeline iterations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If remat_policy is 'custom', return it directly.",
                        "Otherwise, create a policy that saves 'iteration_input' and 'decoder_layer_input'.",
                        "Combine with user-provided remat_policy if it exists."
                    ],
                    "output": {
                        "shape": "Rematerialization policy object."
                    },
                    "dependencies": [
                        "jax.checkpoint_policies.save_only_these_names",
                        "jax.checkpoint_policies.save_from_both_policies"
                    ],
                    "notes": []
                },
                "get_weight_sharding": {
                    "purpose": "Determines and returns the partition spec for the pipeline's weights.",
                    "input": {
                        "shape": "Initialization arguments for the model.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize the model to get weight structure.",
                        "Define a helper function `get_partition_spec` to extract partition specs from logically partitioned leaves.",
                        "Map `get_partition_spec` over the initialized weights.",
                        "Extract the partition spec for the 'layers' collection."
                    ],
                    "output": {
                        "shape": "Partition spec for the pipeline's weights."
                    },
                    "dependencies": [
                        "jax.random.PRNGKey",
                        "nn.spmd.LogicallyPartitioned",
                        "jax.tree.map"
                    ],
                    "notes": [
                        "Requires passing initialization arguments to the method."
                    ]
                },
                "get_physical_spec_no_fsdp": {
                    "purpose": "Converts logical partition specs to physical specs, removing FSDP sharding.",
                    "input": {
                        "shape": "Logical partition specs of all weights.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Convert logical sharding to physical sharding using mesh and rules.",
                        "Define a helper function `remove_fsdp_sharding` to iterate through partition specs and remove 'fsdp'/'fsdp_transpose' axes.",
                        "Apply `remove_fsdp_sharding` to the physical sharding."
                    ],
                    "output": {
                        "shape": "Physical sharding specs without FSDP."
                    },
                    "dependencies": [
                        "nn.logical_to_mesh_sharding",
                        "jax.sharding.NamedSharding",
                        "jax.sharding.PartitionSpec"
                    ],
                    "notes": [
                        "TODO: Remove expert sharding on attention weights as well."
                    ]
                },
                "all_gather_over_fsdp": {
                    "purpose": "Applies an all-gather constraint over FSDP-sharded weights.",
                    "input": {
                        "shape": "Sharding information (logical partition specs).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get physical spec without FSDP using `get_physical_spec_no_fsdp`.",
                        "Apply sharding constraint using `jax.lax.with_sharding_constraint`."
                    ],
                    "output": {
                        "shape": "Weights with all-gather constraint applied."
                    },
                    "dependencies": [
                        "self.get_physical_spec_no_fsdp",
                        "jax.lax.with_sharding_constraint"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "The main forward pass method that orchestrates the pipeline execution.",
                    "input": {
                        "shape": "[global_batch, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshape inputs into microbatches.",
                        "Prepare example inputs for initialization.",
                        "Handle sharding and reshaping of positions and segment_ids.",
                        "Initialize pipeline states using `init_states`.",
                        "Calculate total iterations required.",
                        "If initializing:",
                        "  Get initialization vmap function.",
                        "  If circular pipeline, wrap vmap for repeats.",
                        "  Run vmap function to initialize weights.",
                        "  Return reshaped initialized weights.",
                        "If not initializing:",
                        "  Handle FSDP all-gather if configured.",
                        "  Define `run_iteration_scannable` wrapper for `run_one_iteration`.",
                        "  Apply rematerialization policy if configured.",
                        "  If `scan_pipeline_iterations` is true, use `nn.scan` to run all iterations.",
                        "  Otherwise, loop through iterations and call `run_iteration_scannable`.",
                        "Permute the final output microbatches.",
                        "Reshape the final output to match the original batch shape."
                    ],
                    "output": {
                        "shape": "[global_batch, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "jax.lax.broadcast",
                        "jax.sharding.NamedSharding",
                        "jax.lax.with_sharding_constraint",
                        "self.init_states",
                        "self.is_initializing",
                        "self.get_vmap_func_for_init",
                        "nn.vmap",
                        "self.all_gather_over_fsdp",
                        "nn.remat",
                        "nn.scan",
                        "self.permute_output_micro_per_stage_dim"
                    ],
                    "notes": [
                        "Handles both model initialization and forward pass.",
                        "Supports various pipeline configurations including circularity, repeats, and delays.",
                        "Integrates with JAX's checkpointing and scanning mechanisms."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Quantization:\n  \"\"\"Base class for quantization configurations\"\"\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Placeholder for dot_general implementation in subclasses.\"\"\"\n\n  def einsum(self, dtype: DType = jnp.float32):\n    \"\"\"Placeholder for einsum implementation in subclasses.\"\"\"",
        "analysis": {
            "functionality": "Defines a base class for quantization configurations, providing placeholder methods for dot_general and einsum operations.",
            "usage": "This class serves as a blueprint for specific quantization configurations. Subclasses are expected to implement the `dot_general_cls` and `einsum` methods to define how quantization is applied to dot general and einsum operations, respectively. The `dot_general_cls` method can optionally take `mesh_axes` as an argument, while `einsum` can take a `dtype` argument."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_tiling_fn",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _tiling_fn(lhs, rhs, dimension_numbers, tile_size):\n  \"\"\"apply tiling function\"\"\"\n  del lhs, rhs\n\n  (lhs_ca, rhs_ca), _ = dimension_numbers\n  ret = tiled_dot_general.Cfg(\n      lhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n      rhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n  )\n\n  for lhs_idx, rhs_idx in zip(lhs_ca, rhs_ca):\n    ret.lhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=lhs_idx, tile_size=tile_size, tile_count=None))\n    ret.rhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=rhs_idx, tile_size=tile_size, tile_count=None))\n\n  return ret",
        "analysis": {
            "functionality": "Configures tiling parameters for a dot general operation.",
            "usage": "This function takes the left-hand side (lhs) and right-hand side (rhs) tensors, dimension numbers specifying contraction and remaining axes, and a tile size. It returns a configuration object (`tiled_dot_general.Cfg`) that defines how the contraction axes should be tiled. The `lhs` and `rhs` arguments are not directly used in the computation but are present for API consistency. The function iterates through the contraction axes defined in `dimension_numbers` and creates `tiled_dot_general.AxisTiling` objects for each, specifying the axis and the `tile_size`. These are then added to the `ret.lhs.contraction_axes` and `ret.rhs.contraction_axes` of the returned configuration."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_rhs_axis_metadata_wrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _rhs_axis_metadata_wrapper(\n    x: jnp.ndarray,\n    tile_map,\n    no_sharding_axis: Sequence[int],\n    mesh_axes: Tuple[str, ...],\n    is_tiled: bool,\n    replicate_scale: bool = False,\n):\n  \"\"\"right-hand-side axis metadata wrapper\"\"\"\n  if replicate_scale:\n    # Temporarily using the shape to identify the scale.\n    # TODO: remove the replication once the 2d sharding quantization\n    # works as expected.\n    if len(x.shape) == 1:\n      return nn.with_logical_partitioning((lambda: x), tuple(None for _ in mesh_axes))()\n\n  mesh_axes = list(mesh_axes)\n  if is_tiled:\n    # tile_map is a mapping between original rank and a list of new, tiled rank.\n    if len(mesh_axes) < len(tile_map):\n      mesh_axes = [None] * (len(tile_map) - len(mesh_axes)) + mesh_axes\n    new_mesh_axes = [None] * len(x.shape)\n    for orig_rank, new_rank in tile_map.items():\n      assert new_rank\n      assert len(new_rank) <= 2\n      new_mesh_axes[new_rank[-1]] = mesh_axes[orig_rank]\n    mesh_axes = new_mesh_axes\n\n  if mesh_axes is not None and len(mesh_axes) > 0:\n    for no_shard_idx in no_sharding_axis:\n      if no_shard_idx < len(mesh_axes):\n        mesh_axes[no_shard_idx] = None\n\n  return nn.with_logical_partitioning((lambda: x), mesh_axes)()",
        "analysis": {
            "module_type": "rhs_axis_metadata_wrapper",
            "purpose": "Wraps a function with logical partitioning based on mesh axes and tiling information.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Handle replicate_scale logic for 1D shapes.",
                "Adjust mesh_axes based on tiling information if is_tiled is True.",
                "Set specific mesh_axes to None for axes that should not be sharded.",
                "Apply logical partitioning using nn.with_logical_partitioning."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.numpy as jnp",
                "flax.linen as nn"
            ],
            "parameters": {
                "x": "Input JAX numpy array.",
                "tile_map": "A mapping from original rank to new tiled rank.",
                "no_sharding_axis": "A sequence of axis indices that should not be sharded.",
                "mesh_axes": "A tuple of strings representing the mesh axes.",
                "is_tiled": "A boolean indicating if the input is tiled.",
                "replicate_scale": "A boolean to control scale replication (defaults to False)."
            },
            "notes": [
                "The `replicate_scale` logic is marked as temporary and may be removed.",
                "Assumes `tile_map` contains valid mappings and `new_rank` has a length of at most 2."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#AqtQuantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class AqtQuantization:\n  \"\"\"Configures AQT quantization github.com/google/aqt.\"\"\"\n\n  quant_dg: aqt_config.DotGeneral\n  quant_mode: aqt_flax.QuantMode = aqt_flax.QuantMode.TRAIN\n  replicate_scale: bool = False\n\n  def _get_mixed_precision_cfg(self):\n    \"\"\"get configuration for mixed precision\"\"\"\n    quant_dg = None\n    is_tiled = False\n    tiling_fn = None\n    # pylint: disable=protected-access\n    module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    tile_size = -1\n    for layer_name_re, layer_quant_dg in self.quant_dg.items():\n      if re.fullmatch(layer_name_re, module_path):\n        quant_dg, tile_size = layer_quant_dg\n    if quant_dg is None:\n      quant_dg, tile_size = self.quant_dg[DEFAULT]\n    if tile_size != -1:\n      is_tiled = True\n      tiling_fn = functools.partial(_tiling_fn, tile_size=tile_size)\n    return quant_dg, is_tiled, tiling_fn\n\n  def _get_rhs_axis_metadata_wrapper(\n      self, mesh_axes: Tuple[str, ...] = (), is_tiled: bool = False, replicate_scale: bool = False\n  ):\n    if self.quant_mode == aqt_flax.QuantMode.CONVERT:\n      return None\n    return functools.partial(\n        _rhs_axis_metadata_wrapper, mesh_axes=mesh_axes, is_tiled=is_tiled, replicate_scale=replicate_scale\n    )\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    # module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    # print(f\"quant_dg: {quant_dg}, is_tiled: {is_tiled}, module_path: {module_path}\")\n    aqt_dg_cls = functools.partial(\n        aqt_flax.AqtDotGeneral,\n        quant_dg,\n        rhs_quant_mode=self.quant_mode,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n        rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n        use_legacy_freezer=False,\n        tiling_fn=tiling_fn,\n    )\n    return aqt_dg_cls\n\n  def einsum(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns einsum configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    aqt_einsum = functools.partial(\n        aqt_flax.AqtEinsum(\n            cfg=quant_dg,\n            rhs_quant_mode=self.quant_mode,\n            lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n            rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n            rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n            use_legacy_freezer=False,\n            tiling_fn=tiling_fn,\n        )\n    )\n    return aqt_einsum",
        "analysis": {
            "module_type": "aqt_quantization",
            "purpose": "Configures AQT (Approximate Quantization) for neural network layers, supporting both standard and mixed-precision quantization schemes.",
            "input": {
                "shape": "N/A (This is a configuration class, not a direct processing module)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes with a quantization configuration (aqt_config.DotGeneral).",
                "Determines quantization parameters based on the current module path for mixed precision.",
                "Configures AqtDotGeneral or AqtEinsum based on the quantization settings."
            ],
            "output": {
                "shape": "N/A (Returns configuration objects, not direct tensor outputs)",
                "dtype": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "aqt.jax.v2.flax.aqt_flax",
                "functools.partial",
                "re",
                "typing.Tuple"
            ],
            "parameters": {
                "quant_dg": "The AQT dot general configuration, which can be a single config or a dictionary for mixed precision.",
                "quant_mode": "The quantization mode (e.g., TRAIN, CONVERT, SERVE).",
                "replicate_scale": "Boolean flag to control scale replication."
            },
            "notes": [
                "Supports mixed precision quantization where different layers can have different quantization configurations.",
                "Handles tiling for optimized computations.",
                "The `dot_general_cls` and `einsum` methods return partial functions that can be used to instantiate AQT operations."
            ],
            "methods": {
                "_get_mixed_precision_cfg": {
                    "purpose": "Retrieves quantization configuration for mixed precision based on the current module path.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through layer-specific configurations in `self.quant_dg`.",
                        "Matches the current module path against regular expressions.",
                        "Falls back to a default configuration if no specific match is found.",
                        "Determines if tiling is required and prepares the tiling function."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "re",
                        "functools.partial",
                        "nn.module._context"
                    ],
                    "notes": [
                        "Assumes `self.quant_dg` is a dictionary for mixed precision.",
                        "Uses `DEFAULT` key for fallback configuration."
                    ]
                },
                "_get_rhs_axis_metadata_wrapper": {
                    "purpose": "Creates a partial function for wrapping RHS axis metadata, used for sharding and tiling.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the quantization mode is `CONVERT`; returns None if it is.",
                        "Returns a partial application of `_rhs_axis_metadata_wrapper` with relevant parameters."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "functools.partial",
                        "aqt_flax.QuantMode",
                        "_rhs_axis_metadata_wrapper"
                    ],
                    "notes": [
                        "Handles `replicate_scale` and `is_tiled` parameters."
                    ]
                },
                "dot_general_cls": {
                    "purpose": "Returns a partial function that instantiates AqtDotGeneral with the configured quantization parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines the quantization configuration (`quant_dg`, `is_tiled`, `tiling_fn`) based on whether `self.quant_dg` is a dictionary.",
                        "Obtains the RHS axis metadata wrapper.",
                        "Creates and returns a `functools.partial` object for `aqt_flax.AqtDotGeneral`."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "functools.partial",
                        "aqt_flax.AqtDotGeneral",
                        "aqt_flax.FreezerMode",
                        "typing.Tuple"
                    ],
                    "notes": [
                        "Can handle mixed precision configurations."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a partial function that instantiates AqtEinsum with the configured quantization parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines the quantization configuration (`quant_dg`, `is_tiled`, `tiling_fn`) based on whether `self.quant_dg` is a dictionary.",
                        "Obtains the RHS axis metadata wrapper.",
                        "Creates and returns a `functools.partial` object for `aqt_flax.AqtEinsum`."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "functools.partial",
                        "aqt_flax.AqtEinsum",
                        "aqt_flax.FreezerMode",
                        "typing.Tuple"
                    ],
                    "notes": [
                        "Can handle mixed precision configurations."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Quantization(Quantization):\n  \"\"\"Configures Fp8 quantization for NVIDIA GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.Fp8DirectDotGeneralOp\n\n  def einsum(self, dtype: DType = jnp.float32):\n    return _Fp8EinsumWrapper(dtype=dtype)",
        "analysis": {
            "module_type": "fp8_quantization_config",
            "purpose": "Configures FP8 quantization specifically for NVIDIA GPUs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Sets the `quant_mode` attribute to 'train'.",
                "Defines the `dot_general_cls` method to return `nn.Fp8DirectDotGeneralOp`.",
                "Defines the `einsum` method to return an instance of `_Fp8EinsumWrapper`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization (base class)",
                "nn.Fp8DirectDotGeneralOp",
                "_Fp8EinsumWrapper",
                "jnp.float32"
            ],
            "parameters": {
                "quant_mode": "Specifies the quantization mode, defaulting to 'train'."
            },
            "notes": [
                "This class is designed to configure FP8 quantization for NVIDIA GPUs.",
                "It inherits from a base `Quantization` class.",
                "The `einsum` method uses a wrapper class `_Fp8EinsumWrapper` to handle computation dtype."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the class for FP8 direct dot general operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns `nn.Fp8DirectDotGeneralOp`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nn.Fp8DirectDotGeneralOp"
                    ],
                    "notes": []
                },
                "einsum": {
                    "purpose": "Returns a wrapper for FP8 einsum operations, configured with a specified dtype.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiates and returns `_Fp8EinsumWrapper` with the provided `dtype`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_Fp8EinsumWrapper",
                        "DType",
                        "jnp.float32"
                    ],
                    "notes": [
                        "The `dtype` parameter defaults to `jnp.float32`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_Fp8EinsumWrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class _Fp8EinsumWrapper(nn.Module):\n  \"\"\"Wrapper for nn.Fp8Einsum to handle computation dtype.\"\"\"\n\n  dtype: DType\n\n  @nn.compact\n  def __call__(self, eqn, lhs, rhs, **kwargs):\n    # nn.Fp8Einsum determines compute dtype from rhs.\n    # We cast rhs to the desired computation dtype.\n    # nn.Fp8Einsum will then cast lhs to the same dtype.\n    rhs = rhs.astype(self.dtype)\n    return nn.Fp8Einsum(name=\"fp8_einsum\")(eqn, lhs, rhs, **kwargs)",
        "analysis": {
            "functionality": "Wraps nn.Fp8Einsum to manage the computation data type.",
            "usage": "Instantiate with a desired `dtype`. Call the instance with an equation string (`eqn`), a left-hand side tensor (`lhs`), and a right-hand side tensor (`rhs`). The `rhs` tensor is cast to the specified `dtype` before being passed to `nn.Fp8Einsum`, which then also casts `lhs` to the same `dtype`."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Einsum",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Einsum(nn.Module):\n  \"\"\"An fp8 einsum op.\n\n  Attributes:\n    amax_history_length: size of the amax history.\n    e4m3_dtype: e4m3 variants, e.g., e4m3fn, e4m3fnuz.\n    e5m2_dtype: e5m2 variants, e.g., e5m2, e5m2fnuz.\n    dtype: computation dtype.\n  \"\"\"\n\n  amax_history_length: int = 1024\n  e4m3_dtype: DType = jnp.float8_e4m3fn\n  e5m2_dtype: DType = jnp.float8_e5m2\n  dtype: DType = jnp.float32\n\n  def setup(self) -> None:\n    \"\"\"init with input_amax_history, kernel_amax_history, output_grad_amax_history,\n    input_scale, kernel_scale, output_grad_scale\"\"\"\n    scale_args = (\n        flax_initializers.ones_init(),\n        jax.random.PRNGKey(0),\n        (1,),\n        jnp.float32,\n    )\n    amax_history_args = (\n        flax_initializers.zeros_init(),\n        jax.random.PRNGKey(0),\n        (self.amax_history_length,),\n        jnp.float32,\n    )\n\n    OVERWRITE_WITH_GRADIENT = \"_overwrite_with_gradient\"\n    self.input_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"input_amax_history\", *amax_history_args)\n    self.kernel_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_amax_history\", *amax_history_args)\n    self.output_grad_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_amax_history\", *amax_history_args)\n\n    self.input_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"input_scale\", *scale_args)\n    self.kernel_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_scale\", *scale_args)\n    self.output_grad_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_scale\", *scale_args)\n\n  def __call__(self, eqn, *args, **kwargs):\n    assert len(args) == 2\n    x = args[0]\n    k = args[1]\n\n    comp_dtype = self.dtype\n    k = jnp.asarray(k, comp_dtype)\n    x = jnp.asarray(x, comp_dtype)\n\n    x_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, x, self.input_scale.value, self.input_amax_history.value)\n    k_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, k, self.kernel_scale.value, self.kernel_amax_history.value)\n\n    y_qdq = jnp.einsum(eqn, x_qdq, k_qdq, _dot_general=fp8_ops.dot_general_with_precision)\n\n    y = fp8_ops.out_qdq(\n        comp_dtype,\n        self.e5m2_dtype,\n        y_qdq,\n        self.output_grad_scale.value,\n        self.output_grad_amax_history.value,\n    )\n    return y",
        "analysis": {
            "module_type": "fp8_einsum",
            "purpose": "Performs an einsum operation using FP8 (8-bit floating-point) quantization for optimized computation.",
            "input": {
                "shape": "[batch_size, ...], [batch_size, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Convert input tensors to the computation dtype.",
                "Quantize input tensors using `fp8_ops.in_qdq`.",
                "Perform einsum operation using `jnp.einsum` with FP8 dot general.",
                "Dequantize the result using `fp8_ops.out_qdq`."
            ],
            "output": {
                "shape": "Output shape determined by the einsum equation and input shapes.",
                "dtype": "N/A"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.linen.fp8_ops",
                "flax.linen.initializers"
            ],
            "parameters": {
                "amax_history_length": "Size of the activation maximum history for quantization.",
                "e4m3_dtype": "The FP8 dtype variant for input quantization (e.g., e4m3fn).",
                "e5m2_dtype": "The FP8 dtype variant for output dequantization (e.g., e5m2fnuz).",
                "dtype": "The computation dtype for intermediate calculations."
            },
            "notes": [
                "This module manages internal state for quantization scales and amax history.",
                "It uses `flax.linen.variable` to store these states."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the module's state variables for quantization scales and amax history.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define initializers for scale (ones) and amax history (zeros).",
                        "Create and initialize state variables for input, kernel, and output gradient scales and amax histories."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax_initializers",
                        "jax.random",
                        "jnp"
                    ],
                    "notes": [
                        "Uses `_overwrite_with_gradient` as the variable collection."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the FP8 quantized einsum operation.",
                    "input": {
                        "shape": "Two tensors (x, k) for the einsum operation.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert that exactly two input tensors are provided.",
                        "Convert input tensors to the module's computation dtype.",
                        "Quantize the input tensors `x` and `k` using `fp8_ops.in_qdq`.",
                        "Perform the einsum operation using `jnp.einsum` with `fp8_ops.dot_general_with_precision`.",
                        "Dequantize the result using `fp8_ops.out_qdq`."
                    ],
                    "output": {
                        "shape": "The shape of the output tensor depends on the einsum equation and input shapes.",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "fp8_ops.in_qdq",
                        "jnp.einsum",
                        "fp8_ops.dot_general_with_precision",
                        "fp8_ops.out_qdq"
                    ],
                    "notes": [
                        "The `eqn` argument specifies the einsum equation string.",
                        "The `_dot_general` argument in `jnp.einsum` is overridden to use FP8 precision."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Quantization(Quantization):\n  \"\"\"Configures NANOO Fp8 quantization for AMD MI300/MI325 GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.NANOOFp8DotGeneralOp",
        "analysis": {
            "functionality": "Configures NANOO Fp8 quantization specifically for AMD MI300/MI325 GPUs.",
            "usage": "Instantiate this class to configure NANOO Fp8 quantization. The `dot_general_cls` method returns the appropriate dot general operation class (`nn.NANOOFp8DotGeneralOp`) for this quantization type."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_int8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_int8_quant_config(config):\n  drhs_bits = None\n  drhs_accumulator_dtype = None\n  drhs_local_aqt = None\n  if config.quantization_local_shard_count != 0:\n    drhs_bits = 8\n    drhs_accumulator_dtype = jnp.int32\n    drhs_local_aqt = aqt_config.LocalAqt(contraction_axis_shard_count=config.quantization_local_shard_count)\n  return aqt_config.config_v3(\n      fwd_bits=8,\n      dlhs_bits=8,\n      drhs_bits=drhs_bits,\n      rng_type=\"jax.uniform\",\n      dlhs_local_aqt=None,\n      drhs_local_aqt=drhs_local_aqt,\n      fwd_accumulator_dtype=jnp.int32,\n      dlhs_accumulator_dtype=jnp.int32,\n      drhs_accumulator_dtype=drhs_accumulator_dtype,\n  )",
        "analysis": {
            "functionality": "Generates an AQT (Adaptive Quantization Toolkit) configuration for int8 quantization.",
            "usage": "Call this function with a configuration object that has a `quantization_local_shard_count` attribute. It returns an `aqt_config.config_v3` object suitable for int8 quantization, potentially including local AQT settings if `quantization_local_shard_count` is non-zero."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#ConstantBoundConfig",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class ConstantBoundConfig:\n  fwd_lhs_bound: float | None = None\n  fwd_rhs_bound: float | None = None\n  dlhs_lhs_bound: float | None = None\n  dlhs_rhs_bound: float | None = None\n  drhs_lhs_bound: float | None = None\n  drhs_rhs_bound: float | None = None",
        "analysis": {
            "module_type": "constant_bound_config",
            "purpose": "Configuration for setting constant bounds for quantization calibration.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "fwd_lhs_bound": "Optional float for the forward pass left-hand-side bound.",
                "fwd_rhs_bound": "Optional float for the forward pass right-hand-side bound.",
                "dlhs_lhs_bound": "Optional float for the dlhs (down-left-hand-side) left-hand-side bound.",
                "dlhs_rhs_bound": "Optional float for the dlhs right-hand-side bound.",
                "drhs_lhs_bound": "Optional float for the drhs (down-right-hand-side) left-hand-side bound.",
                "drhs_rhs_bound": "Optional float for the drhs right-hand-side bound."
            },
            "notes": [
                "This is a dataclass used to hold configuration values.",
                "All parameters are optional and default to None."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_const_scale_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_const_scale_config(\n    aqt_dg: aqt_config.DotGeneral,\n    cst_bound_config: ConstantBoundConfig,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a constant scale config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    cst_bound_config: The constant bound config.\n\n  Returns:\n    The AQT dot general config with constant scale config.\n  \"\"\"\n  if cst_bound_config.fwd_lhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_lhs_bound\n    )\n  if cst_bound_config.fwd_rhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_rhs_bound\n    )\n  if cst_bound_config.dlhs_lhs_bound:\n    aqt_dg.dlhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_lhs_bound\n    )\n\n  if cst_bound_config.dlhs_rhs_bound is not None:\n    aqt_dg.dlhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_rhs_bound\n    )\n\n  if cst_bound_config.drhs_lhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_lhs_bound\n    )\n\n  if cst_bound_config.drhs_rhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_rhs_bound\n    )\n\n  return aqt_dg",
        "analysis": {
            "functionality": "Builds a configuration for AQT (Adaptive Quantization Toolkit) dot general operations, specifically setting constant scale bounds for quantization calibration.",
            "usage": "This function takes an existing AQT dot general configuration (`aqt_dg`) and a `ConstantBoundConfig` object. It modifies the `aqt_dg` by setting the calibration bounds for various parts of the dot general operation (forward, dlhs, drhs, for both lhs and rhs) if the corresponding bounds are provided in `cst_bound_config`. It returns the modified `aqt_dg` object."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#PerTensorScales",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class PerTensorScales:\n  fwd_lhs: bool = False\n  fwd_rhs: bool = False\n  dlhs_lhs: bool = False\n  dlhs_rhs: bool = False\n  drhs_lhs: bool = False\n  drhs_rhs: bool = False",
        "analysis": {
            "module_type": "per_tensor_scales",
            "purpose": "A dataclass to configure whether per-tensor scaling should be used for different parts of a dot general operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "fwd_lhs": "Boolean indicating if per-tensor scaling should be used for the forward pass's left-hand side operand.",
                "fwd_rhs": "Boolean indicating if per-tensor scaling should be used for the forward pass's right-hand side operand.",
                "dlhs_lhs": "Boolean indicating if per-tensor scaling should be used for the backward pass's left-hand side derivative's left-hand side operand.",
                "dlhs_rhs": "Boolean indicating if per-tensor scaling should be used for the backward pass's left-hand side derivative's right-hand side operand.",
                "drhs_lhs": "Boolean indicating if per-tensor scaling should be used for the backward pass's right-hand side derivative's left-hand side operand.",
                "drhs_rhs": "Boolean indicating if per-tensor scaling should be used for the backward pass's right-hand side derivative's right-hand side operand."
            },
            "notes": [
                "This dataclass is used to configure quantization settings, specifically for controlling the scope of scale calibration in AQT (Approximate Quantized Training) operations.",
                "Each boolean attribute corresponds to a specific operand in a dot general operation (forward and backward passes) and determines if a single scale factor is used for the entire tensor or if scales are computed per-tensor."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_per_tensor_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_per_tensor_config(\n    aqt_dg: aqt_config.DotGeneral,\n    per_tensor_scales: PerTensorScales,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a per tensor config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    per_tensor_scales: The per tensor scales config.\n\n  Returns:\n    The AQT dot general config with per tensor config.\n  \"\"\"\n  if per_tensor_scales.fwd_lhs:\n    aqt_dg.fwd.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.fwd_rhs:\n    aqt_dg.fwd.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_lhs:\n    aqt_dg.dlhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_rhs:\n    aqt_dg.dlhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_lhs:\n    aqt_dg.drhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_rhs:\n    aqt_dg.drhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  return aqt_dg",
        "analysis": {
            "functionality": "Modifies an AQT dot general configuration to use per-tensor quantization scales.",
            "usage": "This function takes an existing AQT dot general configuration (`aqt_dg`) and a `PerTensorScales` object. It updates the `calib_shared_axes` attribute of the quantizers within the `aqt_dg` configuration to 'per_tensor' based on the boolean flags set in `per_tensor_scales`. It returns the modified `aqt_dg` configuration."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_default_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_default_config(config):\n  \"\"\"Get aqt for 8-bit floating point quantization configuration.\"\"\"\n  aqt_dg = aqt_config.config_v4(\n      fwd_bits=\"e4m3\",\n      dlhs_bits=\"e5m2\",\n      drhs_bits=\"e5m2\",\n      use_dummy_static_bound=False,\n      fwd_accumulator_dtype=jnp.bfloat16,\n      dlhs_accumulator_dtype=jnp.bfloat16,\n      drhs_accumulator_dtype=jnp.bfloat16,\n      dlhs_use_fwd_quant=False,\n      drhs_use_fwd_quant=False,\n  )\n  constant_bound_config = None\n\n  if len(config.constant_bound_config) == 6:\n    fwd_lhs_bound, fwd_rhs_bound, dlhs_lhs_bound, dlhs_rhs_bound, drhs_lhs_bound, drhs_rhs_bound = (\n        config.constant_bound_config\n    )\n    constant_bound_config = ConstantBoundConfig(\n        fwd_lhs_bound=fwd_lhs_bound,\n        fwd_rhs_bound=fwd_rhs_bound,\n        dlhs_lhs_bound=dlhs_lhs_bound,\n        dlhs_rhs_bound=dlhs_rhs_bound,\n        drhs_lhs_bound=drhs_lhs_bound,\n        drhs_rhs_bound=drhs_rhs_bound,\n    )\n    aqt_dg = _build_const_scale_config(aqt_dg, constant_bound_config)\n\n  aqt_config.set_stochastic_rounding(\n      aqt_dg,\n      vjp_lhs_stochastic_rounding=False,\n      vjp_rhs_stochastic_rounding=False,\n      implementation=\"jax.uniform\",\n  )\n\n  per_tensor_scales = PerTensorScales(\n      fwd_lhs=True,\n      fwd_rhs=True,\n      dlhs_lhs=True,\n      dlhs_rhs=True,\n      drhs_lhs=True,\n      drhs_rhs=True,\n  )\n  return _build_per_tensor_config(aqt_dg, per_tensor_scales)",
        "analysis": {
            "functionality": "Generates a configuration for 8-bit floating-point quantization (FP8) using the AQT library.",
            "usage": "This function takes a configuration object as input and returns an AQT configuration suitable for FP8 quantization. It handles both dynamic scaling and optional static scaling based on `config.constant_bound_config`. The output is an AQT dot general configuration object."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_quant_config(config):\n  \"\"\"get aqt for 8-bit floating point quantization configuration\"\"\"\n  cfg = aqt_config.config_v4(fwd_bits=\"e4m3\", dlhs_bits=None, drhs_bits=None, fwd_accumulator_dtype=jnp.bfloat16)\n  return cfg",
        "analysis": {
            "functionality": "Retrieves the AQT (Advanced Quantization Toolkit) configuration for 8-bit floating-point quantization.",
            "usage": "Call this function with a configuration object. It returns an AQT configuration object suitable for 8-bit floating-point quantization, specifically using 'e4m3' format for forward passes and bfloat16 for accumulators. The 'config' parameter is expected to be a configuration object that might contain other settings, though only 'fwd_bits' and 'fwd_accumulator_dtype' are directly used in this function."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_dot_general_make",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _dot_general_make(quant_cfg):\n  \"\"\"Create quantization configs for input matrices to a matmul\"\"\"\n  lhs_bits = quant_cfg[_A_BITS]\n  lhs_scale = quant_cfg[_A_SCALE]\n  rhs_bits = quant_cfg[_W_BITS]\n  rhs_scale = quant_cfg[_W_SCALE]\n  aqt_dg = aqt_config.dot_general_make(lhs_bits=lhs_bits, rhs_bits=rhs_bits)\n  if lhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=lhs_scale)\n  if rhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=rhs_scale)\n  return aqt_dg",
        "analysis": {
            "functionality": "Creates quantization configurations for the input matrices of a matrix multiplication operation.",
            "usage": "This function takes a `quant_cfg` dictionary as input, which is expected to contain keys like '_A_BITS', '_A_SCALE', '_W_BITS', and '_W_SCALE'. It uses these values to configure quantization settings for the left-hand side (lhs) and right-hand side (rhs) of a dot general operation, specifically applying AbsMaxCalibration if the scale is less than 1.0. It returns a configured `aqt_config.DotGeneral` object."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_default_mp_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_default_mp_config(default=None):\n  default_config = {_W_BITS: None, _A_BITS: None, _W_SCALE: 1.0, _A_SCALE: 1.0, _TILE_SIZE: -1}\n  if default:\n    default_config.update(default)\n  return default_config",
        "analysis": {
            "functionality": "This function retrieves a default configuration dictionary for mixed-precision quantization, allowing for optional overrides.",
            "usage": "Call `_get_default_mp_config()` to get a default configuration. Optionally, pass a dictionary to the `default` argument to override specific keys in the default configuration. The function returns a dictionary containing keys like `_W_BITS`, `_A_BITS`, `_W_SCALE`, `_A_SCALE`, and `_TILE_SIZE`."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_mixed_precision_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_mixed_precision_quant_config(mixed_precision_config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  ret_config = {}\n  default_mp_config = _get_default_mp_config(default=mixed_precision_config.get(DEFAULT, None))\n  for layer_name_re, layer_quantization_config in mixed_precision_config.items():\n    # Make a copy of default_mp_config to avoid updating original dict\n    quant_config = default_mp_config.copy()\n    # print(f\"Mixed precision config: processing\n    # {layer_name_re} - {layer_quantization_config}, default config - {quant_config}\")\n    if layer_name_re != DEFAULT:\n      for k in quant_config:\n        quant_config[k] = layer_quantization_config.get(k, default_mp_config[k])\n    ret_config[layer_name_re] = [_dot_general_make(quant_config), quant_config[\"tile_size\"]]\n  return ret_config",
        "analysis": {
            "functionality": "This function processes a dictionary of mixed-precision quantization configurations, applying default settings and merging them with layer-specific overrides.",
            "usage": "It takes a `mixed_precision_config` dictionary as input, where keys are regular expressions matching layer names and values are dictionaries of quantization parameters (e.g., 'w_bits', 'a_bits', 'w_scale', 'a_scale', 'tile_size'). It returns a dictionary where each key is a layer name regex, and the value is a list containing a quantization configuration object and a tile size. The `DEFAULT` key in the input dictionary specifies default parameters that are applied to all layers unless overridden."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_quant_config(config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  if not config.quantization or config.quantization == \"\":\n    return None\n  if config.quantization == \"int8\":\n    return _get_int8_quant_config(config)\n  if config.quantization == \"intmp\":\n    assert config.quant_cfg_path, \"Must specify quant_cfg for mixed precision quantization\"\n    with open(config.quant_cfg_path, \"rt\", encoding=\"utf8\") as config_file:\n      mixed_precision_config = json.load(config_file)\n    return _get_mixed_precision_quant_config(mixed_precision_config)\n  if config.quantization == \"fp8\":\n    return \"fp8\"\n  if config.quantization == \"nanoo_fp8\":\n    return \"nanoo_fp8\"\n  if config.quantization == \"aqt_fp8\":\n    return _get_aqt_fp8_quant_config(config)\n  if config.quantization == \"aqt_fp8_full\":\n    return _get_aqt_fp8_default_config(config)\n\n  raise ValueError(f\"Invalid value configured for quantization {config.quantization}.\")",
        "analysis": {
            "functionality": "This function determines and returns the appropriate quantization configuration based on the provided configuration object.",
            "usage": "Call this function with a configuration object that has a 'quantization' attribute. It returns a quantization configuration object or None if quantization is not enabled. It handles various quantization types like 'int8', 'intmp', 'fp8', 'nanoo_fp8', 'aqt_fp8', and 'aqt_fp8_full'."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_convert_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_convert_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.CONVERT)",
        "analysis": {
            "functionality": "Checks if a given quantization object is in 'convert' mode.",
            "usage": "Call this function with a quantization object. It returns True if the object exists and its 'quant_mode' attribute is equal to aqt_flax.QuantMode.CONVERT, otherwise False."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_serve_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_serve_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.SERVE)",
        "analysis": {
            "functionality": "Checks if a given quantization object is in serve mode.",
            "usage": "Call this function with a quantization object. It returns True if the object's quant_mode is equal to aqt_flax.QuantMode.SERVE, and False otherwise. The 'quant' object is expected to have a 'quant_mode' attribute."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quant_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quant_mode(quant_mode_str: str = \"train\"):\n  \"\"\"Set quant mode.\"\"\"\n  if quant_mode_str == \"train\":\n    return aqt_flax.QuantMode.TRAIN\n  elif quant_mode_str == \"serve\":\n    return aqt_flax.QuantMode.SERVE\n  elif quant_mode_str == \"convert\":\n    return aqt_flax.QuantMode.CONVERT\n  else:\n    raise ValueError(f\"Invalid quantization mode {quant_mode_str}.\")\n  return None",
        "analysis": {
            "functionality": "This function maps a string representation of a quantization mode to its corresponding enum value from `aqt_flax.QuantMode`.",
            "usage": "Call `get_quant_mode` with a string like 'train', 'serve', or 'convert' to get the respective `QuantMode` enum. Defaults to 'train' if no argument is provided. Raises a ValueError for invalid input strings."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_quantization(config: Config, quant_mode_str: str = \"train\"):\n  \"\"\"Configure quantization based on user config and quant mode.\"\"\"\n  if config.use_qwix_quantization:\n    return None\n  quant_cfg = _get_quant_config(config)\n  if quant_cfg:\n    if quant_cfg == \"fp8\":\n      return Fp8Quantization()\n    elif quant_cfg == \"nanoo_fp8\":\n      return NANOOFp8Quantization()\n    quant_mode = get_quant_mode(quant_mode_str)\n    replicate_scale = config.replicate_quant_scale if config.replicate_quant_scale else False\n    return AqtQuantization(quant_dg=quant_cfg, quant_mode=quant_mode, replicate_scale=replicate_scale)\n  return None",
        "analysis": {
            "functionality": "Configures and returns a quantization object based on the provided configuration and quantization mode.",
            "usage": "Call this function with a Config object and an optional quantization mode string. It returns a quantization object (e.g., Fp8Quantization, NANOOFp8Quantization, AqtQuantization) or None if quantization is disabled or not applicable."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#match_aqt_and_unquantized_param",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def match_aqt_and_unquantized_param(aqt_params, params):\n  \"\"\"match aqt and unquantized params\"\"\"\n  aqt_param_flat, aqt_tree_def = jax.tree_util.tree_flatten_with_path(\n      aqt_params, is_leaf=lambda x: isinstance(x, aqt_tensor.QTensor)\n  )\n  param_tree_flat, _ = jax.tree_util.tree_flatten_with_path(params)\n  aqt_paths = []\n  # Original path of quantized AQT param path.\n  param_paths = []\n\n  for aqt_k, _ in aqt_param_flat:\n    index = None\n    for index, (k, _) in enumerate(param_tree_flat):\n      path_depth = len(k)\n      # every quantized parameter has AQT.. as the leaf node\n      # AqtDotGeneral and AqtEinsum replace leaf node.\n      # Therefore, leaf node should be ignored for path matching\n      # Note: Aqt only operates on kernels so don't pop bias parameters.\n      # Ref: https://github.com/AI-Hypercomputer/maxtext/compare/main...quantize_r1\n      if k[: path_depth - 1] == aqt_k[: path_depth - 1] and k[-1].key != \"bias\":\n        aqt_paths.append(aqt_k)\n        param_paths.append(k)\n        break\n    assert index is not None\n    # since the parameter is already added, we can delete it.\n    param_tree_flat.pop(index)\n  return jax.tree_util.tree_unflatten(aqt_tree_def, param_paths)",
        "analysis": {
            "functionality": "This function matches parameters from a quantized AQT (Approximate Quantization) model with their corresponding unquantized parameters.",
            "usage": "It takes two arguments: `aqt_params` (the quantized parameters) and `params` (the unquantized parameters). It returns a flattened list of the unquantized parameter paths that correspond to the quantized AQT parameters, excluding bias parameters. This is useful for aligning quantized and unquantized model weights."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_key_paths",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_key_paths(aqt_vars, params):\n  \"\"\"Generate a list of paths which have aqt state\"\"\"\n  aqt_to_unquantized_key_path = match_aqt_and_unquantized_param(aqt_vars, params)\n  aqt_key_paths, _ = jax.tree_util.tree_flatten(aqt_to_unquantized_key_path, is_leaf=lambda x: isinstance(x, tuple))\n  return list(aqt_key_paths)",
        "analysis": {
            "functionality": "Generates a list of key paths within a nested structure that correspond to AQT (Approximate Quantization) states, by matching them against unquantized parameters.",
            "usage": "Call this function with `aqt_vars` (containing AQT states) and `params` (the full parameter tree). It returns a list of strings, where each string is a path to an AQT-quantized parameter in the `params` structure."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#remove_quantized_params",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def remove_quantized_params(params, aqt_vars):\n  \"\"\"Remove param values with aqt tensors to Null to optimize memory.\"\"\"\n  quantized_param_paths = _get_aqt_key_paths(aqt_vars, params)\n  tree_flat, tree_struct = tree_flatten_with_path(params)\n  for i, (k, v) in enumerate(tree_flat):\n    if k in quantized_param_paths:\n      v = {}\n    tree_flat[i] = v\n  return tree_unflatten(tree_struct, tree_flat)",
        "analysis": {
            "functionality": "Removes parameters associated with AQT (Approximate Quantized Training) tensors from a given parameter tree, setting their values to empty dictionaries to optimize memory usage.",
            "usage": "Call this function with the model's parameters (`params`) and the AQT variables (`aqt_vars`). It returns a modified parameter tree with quantized parameters removed. This is useful for reducing memory footprint, especially during inference or when certain quantized states are no longer needed."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_kv_quant",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_kv_quant(config):\n  return None if not config.quantize_kvcache else KVQuant(config)",
        "analysis": {
            "functionality": "Configures Key-Value (KV) caching quantization based on a provided configuration.",
            "usage": "Call this function with a configuration object. If `config.quantize_kvcache` is True, it returns an instance of `KVQuant` initialized with the config; otherwise, it returns None."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NvidaFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NvidaFp8Provider(qwix.QtProvider):\n  \"\"\"Wraps nn.Fp8DirectDotGeneralOp with Qwix's provider interface.\"\"\"\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.Fp8DirectDotGeneralOp(name=op_id)(*args, **kwargs)\n\n  def einsum(self, *args, **kwargs):\n    rule, op_id = self._get_current_rule_and_op_id(\"einsum\")\n    if rule is None:\n      return jnp.einsum(*args, **kwargs)\n    return nn.Fp8Einsum(name=op_id)(*args, **kwargs)",
        "analysis": {
            "functionality": "Provides a wrapper for NVIDIA FP8 operations, specifically for dot_general and einsum, integrating with the Qwix quantization framework.",
            "usage": "This class is intended to be used within the Qwix framework. It intercepts calls to 'dot_general' and 'einsum'. If a quantization rule is active, it uses NVIDIA's FP8 operations (nn.Fp8DirectDotGeneralOp or nn.Fp8Einsum); otherwise, it falls back to standard JAX operations (jax.lax.dot_general or jnp.einsum). The specific FP8 operation used is determined by an 'op_id' obtained from the Qwix framework."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Provider(qwix.QtProvider):\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.NANOOFp8DotGeneralOp(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "nanoo_fp8_provider",
            "purpose": "Provides a wrapper for the NANOOFp8DotGeneralOp for use with the qwix quantization framework.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Retrieves the current quantization rule and operation ID using _get_current_rule_and_op_id.",
                "If no rule is found, it calls the standard jax.lax.dot_general.",
                "If a rule is found, it instantiates and calls nn.NANOOFp8DotGeneralOp with the operation ID."
            ],
            "output": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax.dot_general",
                "flax.linen.NANOOFp8DotGeneralOp"
            ],
            "parameters": {},
            "notes": [
                "This class is designed to integrate custom FP8 dot general operations (NANOOFp8DotGeneralOp) into a system that uses qwix for quantization management.",
                "The `dot_general` method acts as a conditional dispatcher based on whether a specific quantization rule is active."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quantization_rule",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quantization_rule(config: Config):\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.int8,\n          act_qtype=jnp.int8,\n          bwd_qtype=jnp.int8,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_full\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e5m2,\n          weight_calibration_method=config.quantization_calibration_method,\n          act_calibration_method=config.quantization_calibration_method,\n          bwd_calibration_method=config.quantization_calibration_method,\n          op_names=(\"dot_general\", \"gmm\"),\n      )\n    case \"fp8_gpu\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_nanoo\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"\":\n      return None",
        "analysis": {
            "functionality": "This function determines and returns a quantization rule based on the provided configuration.",
            "usage": "Call this function with a 'Config' object. The 'config.quantization' attribute dictates which quantization rule is returned. It supports 'int8', 'fp8', 'fp8_full', 'fp8_gpu', 'fp8_nanoo', and an empty string for no quantization. It returns a 'qwix.QtRule' object or None."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_qt_provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_qt_provider(config):\n  \"\"\"Get quantization rules based on the config.\"\"\"\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_full\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_gpu\":\n      return NvidaFp8Provider([get_quantization_rule(config)])\n    case \"fp8_nanoo\":\n      return NANOOFp8Provider([get_quantization_rule(config)])\n  return None",
        "analysis": {
            "functionality": "This function retrieves a quantization provider based on the configuration's quantization type.",
            "usage": "Call `get_qt_provider` with a configuration object. It returns a `qwix.QtProvider` or a specialized provider like `NvidaFp8Provider` or `NANOOFp8Provider` based on `config.quantization`. Returns `None` if the quantization type is not recognized."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#maybe_quantize_model",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def maybe_quantize_model(model, config):\n  \"\"\"Quantize the model if quantization is enabled.\"\"\"\n  if config.use_qwix_quantization:\n    quantization_provider = get_qt_provider(config)\n    if quantization_provider:\n      model = qwix.quantize_model(model, quantization_provider)\n  return model",
        "analysis": {
            "functionality": "Conditionally quantizes a given model using the Qwix library if quantization is enabled in the configuration.",
            "usage": "Pass a model object and a configuration object to this function. If `config.use_qwix_quantization` is true and a quantization provider can be obtained based on the config, the model will be quantized; otherwise, the original model is returned. The function returns the (potentially quantized) model."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#jax_chunk_gated_delta_rule",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "def jax_chunk_gated_delta_rule(\n    query: Array,\n    key: Array,\n    value: Array,\n    g: Array,\n    beta: Array,\n    chunk_size: int = 64,\n    initial_state: None | Array = None,\n    use_qk_norm_in_gdn: bool = False,\n) -> tuple[Array, None | Array]:\n  \"\"\"\n  A JAX implementation of the chunked Gated Delta Rule, a parallel scan algorithm.\n  This function implements the core recurrent logic of the Gated Delta Network in\n  a hardware-efficient way by splitting the sequence into chunks and using\n  jax.lax.scan for the recurrent part.\n\n  Tensor Shape Abbreviations:\n    B: batch_size, S: sequence_length, H: num_heads,\n    D_k: key/query_head_dim, D_v: value_head_dim,\n    N: num_chunks, C: chunk_size\n\n  Args:\n    query: Query tensor. Shape (B, S, H, D_k)\n    key: Key tensor. Shape (B, S, H, D_k)\n    value: Value tensor. Shape (B, S, H, D_v)\n    g: Log decay tensor. Shape (B, S, H)\n    beta: Gate tensor. Shape (B, S, H)\n    chunk_size: The size of each chunk for processing.\n    initial_state: Optional initial state for the recurrence. Shape (B, H, D_k, D_v)\n    use_qk_norm_in_gdn: Whether to apply L2 normalization to query and key.\n\n  Returns:\n    Output tensor. Shape (B, S, H, D_v)\n    Final recurrent state. Shape (B, H, D_k, D_v) or None\n  \"\"\"\n\n  # =========================================================================\n  # STAGE 1: PREPARATION & PADDING\n  # =========================================================================\n  initial_dtype = query.dtype\n  if use_qk_norm_in_gdn:\n    query = l2norm(query, dim=-1, eps=1e-6)\n    key = l2norm(key, dim=-1, eps=1e-6)\n\n  # Transpose (B, S, H, D) -> (B, H, S, D)\n  query = jnp.transpose(query, (0, 2, 1, 3)).astype(jnp.float32)\n  key = jnp.transpose(key, (0, 2, 1, 3)).astype(jnp.float32)\n  value = jnp.transpose(value, (0, 2, 1, 3)).astype(jnp.float32)\n  # Transpose (B, S, H) -> (B, H, S)\n  beta = jnp.transpose(beta, (0, 2, 1)).astype(jnp.float32)\n  g = jnp.transpose(g, (0, 2, 1)).astype(jnp.float32)\n\n  batch_size, num_heads, sequence_length, k_head_dim = key.shape\n  v_head_dim = value.shape[-1]\n  pad_size = (chunk_size - sequence_length % chunk_size) % chunk_size\n\n  # Padding to make sequence_length divisible by chunk_size\n  if pad_size > 0:\n    query = jnp.pad(query, ((0, 0), (0, 0), (0, pad_size), (0, 0)))  # (B, H, S_padded, D_k)\n    key = jnp.pad(key, ((0, 0), (0, 0), (0, pad_size), (0, 0)))  # (B, H, S_padded, D_k)\n    value = jnp.pad(value, ((0, 0), (0, 0), (0, pad_size), (0, 0)))  # (B, H, S_padded, D_v)\n    beta = jnp.pad(beta, ((0, 0), (0, 0), (0, pad_size)))  # (B, H, S_padded)\n    g = jnp.pad(g, ((0, 0), (0, 0), (0, pad_size)))  # (B, H, S_padded)\n\n  total_sequence_length = sequence_length + pad_size\n  # query shape: (B, H, S_padded, D_k)\n  scale = jax.lax.rsqrt(jnp.array(query.shape[-1]).astype(jnp.float32))\n  query = query * scale\n\n  v_beta = value * jnp.expand_dims(beta, -1)  # (B, H, S_padded, D_v)\n  k_beta = key * jnp.expand_dims(beta, -1)  # (B, H, S_padded, D_k)\n\n  # Reshape to chunks\n  num_chunks = total_sequence_length // chunk_size\n  # query_c shape: (B, H, N, C, D_k)\n  query_c = query.reshape(batch_size, num_heads, num_chunks, chunk_size, k_head_dim)\n  key_c = key.reshape(batch_size, num_heads, num_chunks, chunk_size, k_head_dim)\n  k_beta_c = k_beta.reshape(batch_size, num_heads, num_chunks, chunk_size, k_head_dim)\n  v_beta_c = v_beta.reshape(batch_size, num_heads, num_chunks, chunk_size, v_head_dim)\n  g_c = g.reshape(batch_size, num_heads, num_chunks, chunk_size)  # (B, H, N, C)\n\n  mask = jnp.triu(jnp.ones((chunk_size, chunk_size), dtype=bool), k=0)  # (C, C)\n\n  # =========================================================================\n  # STAGE 2: INTRA-CHUNK CALCULATION (PARALLEL)\n  # =========================================================================\n  # g_cumsum shape: (B, H, N, C)\n  g_cumsum = jnp.cumsum(g_c, axis=-1)\n  # g_diff shape: (B, H, N, C, C)\n  g_diff = jnp.expand_dims(g_cumsum, -1) - jnp.expand_dims(g_cumsum, -2)\n\n  # Apply tril to zero out the upper triangle of g_diff. This is crucial because\n  # the upper triangle contains large positive values that would cause exp() to overflow.\n  g_diff_tril = jnp.tril(g_diff)\n\n  # Exponentiate the lower triangular g_diff. Since these values are non-positive,\n  # exp() will not overflow and will produce values between 0 and 1.\n  g_diff_exp = jnp.exp(g_diff_tril).astype(jnp.float32)\n\n  # The result g_diff_exp is already lower triangular and serves as the decay_mask.\n  # decay_mask shape: (B, H, N, C, C)\n  decay_mask = g_diff_exp\n\n  # --- Precompute within-chunk attention ---\n  # NOTE: Precision set to HIGHEST for numerical accuracy.\n  prec = jax.lax.Precision.HIGHEST\n  # attn shape: (B, H, N, C, C)\n  attn = -jnp.matmul(k_beta_c, jnp.swapaxes(key_c, -1, -2), precision=prec) * decay_mask\n  attn = jnp.where(mask, 0.0, attn)\n\n  # Iterative refinement of the intra-chunk attention.\n  # This loop is equivalent to inverting (I - A) where A is the lower triangular part of attn.\n  def inner_attn_body(i, attn_val):\n    # indices: (C,)\n    indices = jnp.arange(chunk_size)\n    # col_mask: (C,)\n    col_mask = indices < i\n    # row: (B, H, N, C)\n    row = attn_val[..., i, :] * col_mask\n    # sub_mask: (C, C)\n    sub_mask = jnp.expand_dims(indices < i, -1) & (indices < i)\n    # sub: (B, H, N, C, C)\n    sub = attn_val * sub_mask\n    # row_exp: (B, H, N, C, 1)\n    row_exp = jnp.expand_dims(row, -1)\n    # term: (B, H, N, C, C)\n    term = row_exp * sub\n    # summed: (B, H, N, C)\n    summed = jnp.sum(term, axis=-2)\n    # update_val: (B, H, N, C)\n    update_val = row + summed\n    # original_row: (B, H, N, C)\n    original_row = attn_val[..., i, :]\n    # new_row: (B, H, N, C)\n    new_row = jnp.where(col_mask, update_val, original_row)\n    return attn_val.at[..., i, :].set(new_row)\n\n  attn = jax.lax.fori_loop(1, chunk_size, inner_attn_body, attn)\n\n  attn = attn + jnp.eye(chunk_size, dtype=attn.dtype)  # (B, H, N, C, C)\n  # value_intra shape: (B, H, N, C, D_v)\n  value_intra = jnp.matmul(attn, v_beta_c, precision=prec)\n  # k_cumdecay shape: (B, H, N, C, D_k)\n  k_cumdecay = jnp.matmul(attn, (k_beta_c * jnp.expand_dims(jnp.exp(g_cumsum), -1)), precision=prec)\n  # --- End Precompute ---\n\n  output_final_state = initial_state is not None\n  if initial_state is None:\n    # last_recurrent_state shape: (B, H, D_k, D_v)\n    last_recurrent_state = jnp.zeros((batch_size, num_heads, k_head_dim, v_head_dim), dtype=value_intra.dtype)\n  else:\n    last_recurrent_state = initial_state.astype(value_intra.dtype)\n\n  # mask_inter shape: (C, C)\n  mask_inter = jnp.triu(jnp.ones((chunk_size, chunk_size), dtype=bool), k=1)\n\n  # Transpose for scan: (B, H, N, C, D) -> (N, B, H, C, D)\n  query_scan = jnp.transpose(query_c, (2, 0, 1, 3, 4))\n  key_scan = jnp.transpose(key_c, (2, 0, 1, 3, 4))\n  value_scan = jnp.transpose(value_intra, (2, 0, 1, 3, 4))\n  k_cumdecay_scan = jnp.transpose(k_cumdecay, (2, 0, 1, 3, 4))\n  # Transpose for scan: (B, H, N, C) -> (N, B, H, C)\n  g_scan = jnp.transpose(g_cumsum, (2, 0, 1, 3))\n  decay_mask_scan = jnp.transpose(decay_mask, (2, 0, 1, 3, 4))\n\n  xs = (query_scan, key_scan, value_scan, k_cumdecay_scan, g_scan, decay_mask_scan)\n\n  # =========================================================================\n  # STAGE 3: INTER-CHUNK RECURRENCE (SEQUENTIAL VIA SCAN)\n  # =========================================================================\n  def scan_body(prev_state, x):\n    q_i, k_i, v_i, k_cumdecay_i, g_i, decay_mask_i = x\n    # prev_state shape: (B, H, D_k, D_v)\n    last_recurrent_state = prev_state\n    prec = jax.lax.Precision.HIGHEST\n\n    # Intra-chunk attention for the current chunk\n    # attn_i shape: (B, H, C, C)\n    attn_i = jnp.matmul(q_i, jnp.swapaxes(k_i, -1, -2), precision=prec) * decay_mask_i\n    attn_i = jnp.where(mask_inter, 0.0, attn_i)\n\n    # Interaction with the recurrent state\n    # v_prime shape: (B, H, C, D_v)\n    v_prime = jnp.matmul(k_cumdecay_i, last_recurrent_state, precision=prec)\n    # v_new shape: (B, H, C, D_v)\n    v_new = v_i - v_prime\n\n    # g_i is cumulative sum, so exp(g_i) is the decay factor\n    g_i_exp = jnp.exp(g_i)\n    # attn_inter shape: (B, H, C, D_v)\n    attn_inter = jnp.matmul(q_i * jnp.expand_dims(g_i_exp, -1), last_recurrent_state, precision=prec)\n\n    # core_attn_out_i shape: (B, H, C, D_v)\n    core_attn_out_i = attn_inter + jnp.matmul(attn_i, v_new, precision=prec)\n\n    # Update the recurrent state\n    # g_i_last_exp shape: (B, H, 1, 1)\n    g_i_last_exp = jnp.exp(g_i[..., -1, None, None])\n    # new_last_recurrent_state shape: (B, H, D_k, D_v)\n    new_last_recurrent_state = last_recurrent_state * g_i_last_exp\n\n    # g_diff_exp shape: (B, H, C, 1)\n    g_diff_exp = jnp.expand_dims(jnp.exp(jnp.expand_dims(g_i[..., -1], -1) - g_i), -1)\n    # k_i_g_diff shape: (B, H, C, D_k)\n    k_i_g_diff = k_i * g_diff_exp\n\n    # Update term shape: (B, H, D_k, D_v)\n    update_term = jnp.matmul(jnp.swapaxes(k_i_g_diff, -1, -2), v_new, precision=prec)\n    new_last_recurrent_state = new_last_recurrent_state + update_term\n\n    return new_last_recurrent_state, core_attn_out_i\n\n  # final_state shape: (B, H, D_k, D_v)\n  # core_attn_out_stacked shape: (N, B, H, C, D_v)\n  final_state, core_attn_out_stacked = jax.lax.scan(scan_body, last_recurrent_state, xs)\n\n  # =========================================================================\n  # STAGE 4: FINALIZATION\n  # =========================================================================\n  # core_attn_out shape: (B, H, N, C, D_v)\n  core_attn_out = jnp.transpose(core_attn_out_stacked, (1, 2, 0, 3, 4))\n\n  # core_attn_out shape: (B, H, S_padded, D_v)\n  core_attn_out = core_attn_out.reshape(batch_size, num_heads, -1, v_head_dim)\n  # Trim padding: (B, H, S, D_v)\n  core_attn_out = core_attn_out[:, :, :sequence_length, :]\n\n  # Transpose back to (B, S, H, D_v)\n  core_attn_out = jnp.transpose(core_attn_out, (0, 2, 1, 3)).astype(initial_dtype)\n\n  return core_attn_out, final_state if output_final_state else None",
        "analysis": {
            "module_type": "chunk_gated_delta_rule",
            "purpose": "Implements a chunked Gated Delta Rule using JAX's parallel scan for efficient recurrent computation.",
            "input": {
                "shape": "query: (B, S, H, D_k), key: (B, S, H, D_k), value: (B, S, H, D_v), g: (B, S, H), beta: (B, S, H), initial_state: (B, H, D_k, D_v) or None",
                "dtype": "float32 or compatible"
            },
            "processing_steps": [
                "Optional L2 normalization of query and key.",
                "Transpose input tensors to (B, H, S, D) format.",
                "Pad sequences to be divisible by chunk_size.",
                "Calculate scaling factor and apply to query.",
                "Apply beta gating to value and key.",
                "Reshape tensors into chunks.",
                "Precompute intra-chunk attention using decay masks derived from 'g'.",
                "Iteratively refine intra-chunk attention.",
                "Initialize or use provided initial recurrent state.",
                "Transpose chunked tensors for jax.lax.scan.",
                "Perform inter-chunk recurrence using jax.lax.scan.",
                "Transpose and reshape output to original sequence format.",
                "Trim padding from the output.",
                "Transpose output back to (B, S, H, D_v) format."
            ],
            "output": {
                "shape": "Output tensor: (B, S, H, D_v), Final recurrent state: (B, H, D_k, D_v) or None"
            },
            "dependencies": [
                "jax",
                "jax.numpy as jnp",
                "jax.lax",
                "l2norm"
            ],
            "parameters": {
                "chunk_size": "The size of each chunk for processing.",
                "initial_state": "Optional initial state for the recurrence.",
                "use_qk_norm_in_gdn": "Whether to apply L2 normalization to query and key."
            },
            "notes": [
                "The function implements a parallel scan algorithm for hardware efficiency.",
                "It handles sequence padding to ensure divisibility by chunk_size.",
                "Numerical precision is set to HIGHEST for critical matrix multiplications.",
                "The intra-chunk attention refinement is an iterative process.",
                "The final recurrent state is returned only if initial_state was provided."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextGatedDeltaNet",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextGatedDeltaNet(nnx.Module):\n  \"\"\"\n  This module implements the full end-to-end logic of a Gated Delta Network layer.\n\n  End-to-End Equations Implemented:\n  Let `x` be the input `hidden_states`.\n\n  Step A: Input Projections\n  1. (q_raw, k_raw, v_raw, z) = Linear_qkvz(x)\n  2. (b, a) = Linear_ba(x)\n\n  Step B: 1D Convolution\n  1. qkv_conv = silu(Conv1D(concatenate(q_raw, k_raw, v_raw)))\n  2. (q, k, v) = split(qkv_conv)\n\n  Step C: Gated Delta Rule (Recurrent Core)\n  1. Gates: \u03b2=sigmoid(b), g = -exp(A_log) * softplus(a + dt_bias)\n  2. Core Calculation: core_attn_out = jax_chunk_gated_delta_rule(q, k, v, g, \u03b2)\n\n  Step D: Final Output Stage\n  1. y = RMSNorm(core_attn_out) * silu(z)\n  2. output = Linear_out(y)\n\n  Attributes:\n    config: MaxText configuration object.\n    dtype: The datatype of the computation.\n  \"\"\"\n\n  def __init__(self, config: Config, dtype: DType = jnp.float32, *, rngs: nnx.Rngs):\n    self.config = config\n    self.dtype = dtype\n    cfg = self.config\n\n    in_features = cfg.emb_dim\n    self.num_v_heads = cfg.gdn_num_value_heads\n    self.num_k_heads = cfg.gdn_num_key_heads\n    self.head_k_dim = cfg.gdn_key_head_dim\n    self.head_v_dim = cfg.gdn_value_head_dim\n    self.key_dim = self.head_k_dim * self.num_k_heads\n    self.value_dim = self.head_v_dim * self.num_v_heads\n    conv_dim = self.key_dim * 2 + self.value_dim\n    conv_kernel_size = cfg.gdn_conv_kernel_dim\n\n    # Submodule instantiations\n    self.in_proj_qkvz = linears.DenseGeneral(\n        in_features_shape=in_features,\n        out_features_shape=(self.key_dim * 2 + self.value_dim * 2),\n        dtype=cfg.dtype,\n        kernel_axes=(\"embed\", \"mlp\"),\n        matmul_precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n    self.in_proj_ba = linears.DenseGeneral(\n        in_features_shape=in_features,\n        out_features_shape=(self.num_v_heads * 2),\n        dtype=cfg.dtype,\n        kernel_axes=(\"embed\", \"mlp\"),\n        matmul_precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n\n    self.conv1d = nnx.Conv(\n        in_features=conv_dim,\n        out_features=conv_dim,\n        kernel_size=(conv_kernel_size,),\n        feature_group_count=conv_dim,  # Depthwise\n        padding=\"CAUSAL\",\n        use_bias=False,\n        dtype=cfg.dtype,\n        precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n\n    # Initialize A_log to match torch.log(torch.uniform(0, 16))\n    def a_log_init(key, shape, dtype=jnp.float32):\n      # Sample from Uniform(epsilon, 16) to avoid log(0)\n      a_vals = jax.random.uniform(key, shape=shape, dtype=dtype, minval=1e-9, maxval=16.0)\n      return jnp.log(a_vals)\n\n    self.A_log = nnx.Param(a_log_init(rngs.params(), (self.num_v_heads,)))\n    self.dt_bias = nnx.Param(nnx.initializers.ones(rngs.params(), (self.num_v_heads,)))\n\n    self.norm = Qwen3NextRMSNormGated(\n        num_features=self.head_v_dim,  # Normalize over the head dimension (D_v)\n        eps=cfg.normalization_layer_epsilon,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        rngs=rngs,\n    )\n    self.out_proj = linears.DenseGeneral(\n        in_features_shape=self.value_dim,\n        out_features_shape=(in_features,),\n        dtype=cfg.dtype,\n        kernel_axes=(\"mlp\", \"embed\"),\n        matmul_precision=cfg.matmul_precision,\n        rngs=rngs,\n    )\n\n  def __call__(self, hidden_states: Array) -> Array:\n    cfg = self.config\n\n    # =========================================================================\n    # STEP A: Input Projections\n    # =========================================================================\n    # hidden_states shape: (B, S, E)\n    # qkvz shape: (B, S, 2*key_dim + 2*value_dim)\n    qkvz = self.in_proj_qkvz(hidden_states)\n    # ba shape: (B, S, 2*H_v)\n    ba = self.in_proj_ba(hidden_states)\n\n    # q shape: (B, S, key_dim), k shape: (B, S, key_dim), v shape: (B, S, value_dim), z shape: (B, S, value_dim)\n    q, k, v, z = jnp.split(qkvz, [self.key_dim, 2 * self.key_dim, 2 * self.key_dim + self.value_dim], axis=-1)\n    # b shape: (B, S, H_v), a shape: (B, S, H_v)\n    b, a = jnp.split(ba, [self.num_v_heads], axis=-1)\n\n    # =========================================================================\n    # STEP B: 1D Convolution\n    # =========================================================================\n    # qkv shape: (B, S, conv_dim)\n    qkv = jnp.concatenate([q, k, v], axis=-1)\n\n    # TODO(parambole): Implement caching logic for conv_state and recurrent_state\n\n    # Input to conv_layer should be (B, S, C)\n    # qkv_conv shape: (B, S, conv_dim)\n    qkv_conv = jax.nn.silu(self.conv1d(qkv).astype(jnp.float32)).astype(cfg.dtype)\n    # q_conv shape: (B, S, key_dim), k_conv shape: (B, S, key_dim), v_conv shape: (B, S, value_dim)\n    q_conv, k_conv, v_conv = jnp.split(qkv_conv, [self.key_dim, 2 * self.key_dim], axis=-1)\n\n    # Reshape for multi-head processing\n    batch, seq_len, _ = hidden_states.shape\n    # query shape: (B, S, H_k, D_k)\n    query = q_conv.reshape(batch, seq_len, self.num_k_heads, self.head_k_dim)\n    # key shape: (B, S, H_k, D_k)\n    key = k_conv.reshape(batch, seq_len, self.num_k_heads, self.head_k_dim)\n    # value shape: (B, S, H_v, D_v)\n    value = v_conv.reshape(batch, seq_len, self.num_v_heads, self.head_v_dim)\n\n    # =========================================================================\n    # STEP C: Gated Delta Rule Recurrence\n    # =========================================================================\n    A_log = self.A_log.value\n    dt_bias = self.dt_bias.value\n    # beta shape: (B, S, H_v)\n    beta = jax.nn.sigmoid(b)\n    # g shape: (B, S, H_v)\n    g = -jnp.exp(A_log.astype(jnp.float32)) * jax.nn.softplus(a.astype(jnp.float32) + dt_bias.astype(jnp.float32))\n    g = g.astype(cfg.dtype)\n\n    if self.num_v_heads > self.num_k_heads and self.num_v_heads % self.num_k_heads == 0:\n      repeats = self.num_v_heads // self.num_k_heads\n      # query shape after repeat: (B, S, H_v, D_k)\n      query = jnp.repeat(query, repeats, axis=2)\n      # key shape after repeat: (B, S, H_v, D_k)\n      key = jnp.repeat(key, repeats, axis=2)\n    elif self.num_k_heads > self.num_v_heads and self.num_k_heads % self.num_v_heads == 0:\n      # This case might occur if key/query heads are more than value heads.\n      pass  # No repeating needed for query/key in this case\n\n    # TODO(parambole): Pass and update cache state for jax_chunk_gated_delta_rule\n    # core_attn_out shape: (B, S, H_v, D_v)\n    core_attn_out, _ = jax_chunk_gated_delta_rule(\n        query, key, value, g, beta, chunk_size=cfg.gdn_chunk_size, use_qk_norm_in_gdn=cfg.use_qk_norm_in_gdn\n    )\n\n    # =========================================================================\n    # STEP D: Final Output Stage\n    # =========================================================================\n    # The normalization and gating is applied per-head on the value dimension.\n    # We first reshape the `z` tensor to match the multi-head structure of `core_attn_out`.\n    # z shape from (B, S, value_dim) -> (B, S, H_v, D_v)\n    z_reshaped = z.reshape(batch, seq_len, self.num_v_heads, self.head_v_dim)\n\n    # Apply the norm and gate. Output shape: (B, S, H_v, D_v)\n    gated_output_reshaped = self.norm(core_attn_out, z_reshaped)\n\n    # Reshape back to a single feature dimension for the final projection.\n    # Shape from (B, S, H_v, D_v) -> (B, S, value_dim)\n    gated_output = gated_output_reshaped.reshape(batch, seq_len, -1)\n\n    # Final output shape: (B, S, E)\n    output = self.out_proj(gated_output)\n\n    return output",
        "analysis": {
            "functionality": "Implements a Gated Delta Network layer, a type of recurrent neural network layer used in transformer architectures.",
            "usage": "Instantiate the `Qwen3NextGatedDeltaNet` class with a configuration object and RNGs. Call the instance with the input hidden states (a JAX array). The module processes the input through several stages including input projections, 1D convolution, a gated delta rule recurrence, and a final output stage, returning the processed hidden states."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextFullAttention",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextFullAttention(nnx.Module):\n  \"\"\"Qwen3-Next Full Attention Layer.\n\n  This module implements the full self-attention mechanism as used in\n  Qwen3-Next models for layers that do not use the Gated Delta Network.\n  It wraps the main `attentions.Attention` class, which handles the core attention operation,\n  including the query, key, value, and output projections.\n\n  Qwen3 Next Attention differs from standard attention by the following features:\n    - Query and Gate splitting from a single q projection.\n    - Application of a sigmoid gate to the attention output.\n    - Usage of `Qwen3NextRMSNorm` for query and key normalization.\n    - Usage of `Qwen3NextRotaryEmbedding` for partial rotary position embeddings.\n      - Partial ROPE is applied to the first 25% of head dimensions\n\n  Attributes:\n    config: MaxText configuration object.\n    mesh: The device mesh for sharding.\n    model_mode: The operational mode (e.g., 'train', 'prefill').\n    layer_idx: The index of the current layer.\n    quant: Optional quantization configuration.\n    attention: An instance of `attentions.Attention` which contains the\n      learnable parameters for query, key, value, and output projections\n      (e.g., `attention.query`, `attention.key`, etc.), and performs\n      the attention calculation.\n  \"\"\"\n\n  def __init__(\n      self, config: Config, mesh: Mesh, model_mode: str, layer_idx: int, quant: None | Quant = None, *, rngs: nnx.Rngs\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.layer_idx = layer_idx\n    self.quant = quant\n    cfg = self.config\n\n    scaling_factor = self.config.head_dim**-0.5\n\n    inputs_q_shape = (cfg.per_device_batch_size, cfg.max_target_length, cfg.emb_dim)\n    inputs_kv_shape = (cfg.per_device_batch_size, cfg.max_target_length, cfg.emb_dim)\n    self.attention = attentions.Attention(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=inputs_q_shape,\n        inputs_kv_shape=inputs_kv_shape,\n        out_axis_names=(BATCH, LENGTH_NO_EXP, EMBED),\n        mesh=self.mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_qk_norm=cfg.use_qk_norm,\n        query_pre_attn_scalar=scaling_factor,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n  ):\n    attention_output = self.attention(\n        inputs_q=inputs,\n        inputs_kv=inputs,\n        inputs_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    return attention_output",
        "analysis": {
            "module_type": "qwen3_next_full_attention",
            "purpose": "Implements the full self-attention mechanism for Qwen3-Next models that do not use the Gated Delta Network.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize attention parameters (query, key, value, output projections).",
                "Perform attention calculation using the `attentions.Attention` class.",
                "Apply scaling factor.",
                "Handle query and key normalization using `Qwen3NextRMSNorm`.",
                "Apply partial rotary position embeddings.",
                "Apply sigmoid gate to attention output."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "attentions.Attention",
                "Qwen3NextRMSNorm",
                "Qwen3NextRotaryEmbedding",
                "quantizations.configure_kv_quant"
            ],
            "parameters": {
                "config": "MaxText configuration object containing model dimensions, attention parameters, etc.",
                "mesh": "Device mesh for sharding.",
                "model_mode": "Operational mode (e.g., 'train', 'prefill').",
                "layer_idx": "Index of the current layer.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This module wraps the core `attentions.Attention` class.",
                "It incorporates specific Qwen3-Next features like query/gate splitting, sigmoid gating, and partial ROPE.",
                "The `__call__` method passes inputs directly to the underlying `attentions.Attention` module."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Qwen3NextFullAttention module with configuration and attention parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration, mesh, model mode, layer index, and quantization settings.",
                        "Calculate scaling factor.",
                        "Define input shapes for attention module.",
                        "Instantiate the `attentions.Attention` module with all relevant parameters."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "attentions.Attention",
                        "quantizations.configure_kv_quant"
                    ],
                    "notes": [
                        "Sets up the core attention mechanism as a submodule."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the attention layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim] for inputs, and optional decoder_segment_ids and decoder_positions.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the underlying `self.attention` module with the provided inputs and arguments."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "attentions.Attention"
                    ],
                    "notes": [
                        "The `inputs_q` and `inputs_kv` are both set to the `inputs` argument, indicating self-attention.",
                        "Passes through `decoder_segment_ids`, `decoder_positions`, `deterministic`, and `model_mode` to the attention module."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextSparseMoeBlock",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextSparseMoeBlock(nnx.Module):\n  \"\"\"\n  This module encapsulates the unique MoE structure of Qwen3-Next, which includes:\n  1. A set of routed experts, where each token is sent to a subset of experts.\n  2. A single shared expert, which all tokens pass through.\n  3. A learnable gate that determines the contribution of the shared expert.\n\n  Attributes:\n    config: The model configuration object.\n    mesh: The device mesh for sharding.\n    quant: Optional quantization configuration.\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, quant: None | Quant = None, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    cfg = self.config\n\n    # 1. Instantiate and apply the routed experts block.\n    self.routed_experts = moe.RoutedMoE(\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.moe_mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n        rngs=rngs,\n    )\n\n    # 2. Instantiate and apply the shared expert.\n    self.shared_expert = linears.MlpBlock(\n        config=cfg,\n        mesh=mesh,\n        in_features=cfg.emb_dim,\n        intermediate_dim=cfg.moe_mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n        model_mode=config.model_call_mode,\n        rngs=rngs,\n    )\n\n    # 3. Instantiate and apply the gate for the shared expert.\n    self.shared_expert_gate = linears.DenseGeneral(\n        in_features_shape=cfg.emb_dim,\n        out_features_shape=1,\n        use_bias=False,  # Qwen3-Next shared_expert_gate does not have a bias\n        dtype=cfg.dtype,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", \"vocab\"),\n        rngs=rngs,\n    )\n\n  def __call__(self, hidden_states: Array, deterministic: bool) -> tuple[Array, Array | None]:\n    \"\"\"\n    Applies the sparse MoE block to the input hidden states.\n\n    Args:\n      hidden_states: The input array from the previous layer. Shape: (batch, seq, embed_dim)\n      deterministic: If True, disables dropout.\n\n    Returns:\n      A tuple containing:\n        - The output array of the MoE block.\n        - The load balancing loss from the routed experts, if applicable during training.\n    \"\"\"\n    # 1. Apply the routed experts block.\n    routed_output, load_balance_loss = self.routed_experts(hidden_states)\n\n    # 2. Apply the shared expert.\n    shared_expert_output = self.shared_expert(hidden_states, deterministic=deterministic)\n\n    # 3. Apply the gate for the shared expert.\n    shared_gate_output = self.shared_expert_gate(hidden_states)\n\n    # 4. Combine the outputs.\n    final_output = routed_output + jax.nn.sigmoid(shared_gate_output) * shared_expert_output\n\n    return final_output, load_balance_loss",
        "analysis": {
            "module_type": "qwen3_next_sparse_moe_block",
            "purpose": "This module implements a Mixture-of-Experts (MoE) block specific to the Qwen3-Next architecture, combining routed experts with a shared expert.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply routed experts block.",
                "Apply shared expert.",
                "Apply gate for shared expert.",
                "Combine outputs of routed experts and gated shared expert."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "dependencies": [
                "moe.RoutedMoE",
                "linears.MlpBlock",
                "linears.DenseGeneral",
                "max_initializers.nd_dense_init"
            ],
            "parameters": {
                "config": "Model configuration object containing details like number of experts, intermediate dimensions, etc.",
                "mesh": "Device mesh for sharding operations.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The module consists of three main components: routed experts, a shared expert, and a gate for the shared expert.",
                "The output is a combination of the routed experts' output and the shared expert's output, weighted by the gate.",
                "Returns a tuple containing the final output and a load balancing loss from the routed experts."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Qwen3NextSparseMoeBlock with configuration, device mesh, and optional quantization settings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store config, mesh, and quant.",
                        "Instantiate RoutedMoE.",
                        "Instantiate MlpBlock for shared expert.",
                        "Instantiate DenseGeneral for shared expert gate."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "moe.RoutedMoE",
                        "linears.MlpBlock",
                        "linears.DenseGeneral",
                        "max_initializers.nd_dense_init"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Applies the sparse MoE block to the input hidden states.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply the routed experts block to get routed_output and load_balance_loss.",
                        "Apply the shared expert to hidden_states to get shared_expert_output.",
                        "Apply the shared expert gate to hidden_states to get shared_gate_output.",
                        "Combine routed_output with the gated shared_expert_output."
                    ],
                    "output": {
                        "shape": "tuple[Array, Array | None]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.nn.sigmoid"
                    ],
                    "notes": [
                        "The `deterministic` flag controls dropout behavior.",
                        "The second element of the returned tuple is the load balancing loss, which is None if not applicable."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextScannableBlock",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextScannableBlock(nnx.Module):\n  \"\"\"A scannable block of Qwen3-Next decoder layers.\n\n  This module contains a fixed number of heterogeneous decoder layers that form\n  a repeating pattern, as defined by `config.inhomogeneous_layer_cycle_interval`. It is\n  intended to be the body of an `nn.scan` transformation to construct the full\n  decoder stack efficiently.\n\n  Attributes:\n    config: The model configuration object.\n    mesh: The device mesh for sharding.\n    model_mode: The operational mode (e.g., 'train', 'prefill').\n    quant: Optional quantization configuration.\n  \"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, model_mode: str, quant: None | Quant = None, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.quant = quant\n    self.rngs = rngs\n    cfg = self.config\n\n    # Instantiate each layer within the block in __init__\n    for i in range(cfg.inhomogeneous_layer_cycle_interval):\n      layer_rngs = self.rngs.fork()  # Fork RNGs for each layer\n      layer_name = f\"layer_{i}\"\n      layer = Qwen3NextDecoderLayer(\n          config=self.config,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=self.model_mode,\n          layer_idx=i,\n          rngs=layer_rngs,\n      )\n      setattr(self, layer_name, layer)\n\n  def __call__(\n      self,\n      carry: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ) -> tuple[Array, None]:\n    \"\"\"Applies the block of decoder layers to the input carry.\n\n    Args:\n      carry: The input tensor from the previous scan iteration.\n      # ... other arguments are broadcasted to each iteration.\n\n    Returns:\n      A tuple containing the output of the block (the new carry) and an empty\n      value for the scan's `y` collection.\n    \"\"\"\n    cfg = self.config\n    x = carry\n\n    # Loop over the number of sub-layers that make up one repeating pattern.\n    for i in range(cfg.inhomogeneous_layer_cycle_interval):\n      layer = getattr(self, f\"layer_{i}\")\n      x = layer(\n          x,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk,\n          page_state,\n          slot,\n      )\n\n    # The output of the block is the carry for the next scan iteration.\n    return x, None",
        "analysis": {
            "module_type": "qwen3_next_scannable_block",
            "purpose": "A scannable block of Qwen3-Next decoder layers designed for efficient construction of the decoder stack using `nn.scan`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Instantiate `Qwen3NextDecoderLayer` for each layer in the `inhomogeneous_layer_cycle_interval`.",
                "Iterate through the instantiated decoder layers.",
                "Apply each decoder layer to the input 'carry'.",
                "Return the output of the last decoder layer as the new 'carry'."
            ],
            "output": {
                "shape": "tuple[Array, None]"
            },
            "dependencies": [
                "Qwen3NextDecoderLayer",
                "nnx.Module",
                "nnx.Rngs",
                "jax.numpy as jnp",
                "flax.nnx as nnx",
                "Config",
                "Mesh",
                "Quant",
                "page_manager.PageState"
            ],
            "parameters": {
                "config": "The model configuration object.",
                "mesh": "The device mesh for sharding.",
                "model_mode": "The operational mode (e.g., 'train', 'prefill').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The number of decoder layers within this block is determined by `config.inhomogeneous_layer_cycle_interval`.",
                "This module is intended to be used as the body of an `nn.scan` transformation.",
                "RNGs are forked for each instantiated layer."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Qwen3NextScannableBlock by instantiating a fixed number of Qwen3NextDecoderLayers.",
                    "input": {
                        "shape": "config: Config, mesh: Mesh, model_mode: str, quant: None | Quant, rngs: nnx.Rngs",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration, mesh, model_mode, and quantization settings.",
                        "Iterate from 0 up to `config.inhomogeneous_layer_cycle_interval`.",
                        "Fork RNGs for each layer.",
                        "Instantiate a `Qwen3NextDecoderLayer` with relevant parameters.",
                        "Set the instantiated layer as an attribute of the module using `setattr`."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "Qwen3NextDecoderLayer",
                        "nnx.Module",
                        "nnx.Rngs",
                        "Config",
                        "Mesh",
                        "Quant"
                    ],
                    "notes": [
                        "Each layer is dynamically named as 'layer_i'."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the block of decoder layers to the input carry, simulating a scan operation.",
                    "input": {
                        "shape": "carry: jnp.ndarray, decoder_segment_ids: None | jnp.ndarray, decoder_positions: None | jnp.ndarray, deterministic: bool, model_mode: str, previous_chunk=None, page_state: None | page_manager.PageState = None, slot: None | int = None",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assign the input 'carry' to a local variable 'x'.",
                        "Iterate through the instantiated decoder layers (from layer_0 to layer_n-1).",
                        "Apply the current decoder layer to 'x' and update 'x' with the output.",
                        "Return the final 'x' and None (as the 'y' output for scan)."
                    ],
                    "output": {
                        "shape": "tuple[Array, None]"
                    },
                    "dependencies": [
                        "getattr",
                        "Qwen3NextDecoderLayer.__call__"
                    ],
                    "notes": [
                        "This method is designed to be the body of a `jax.lax.scan` transformation.",
                        "Arguments like `decoder_segment_ids`, `decoder_positions`, `deterministic`, `model_mode`, `previous_chunk`, `page_state`, and `slot` are broadcasted to each layer's call."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3NextDecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3NextDecoderLayer(nnx.Module):\n  \"\"\"\n  This layer is a hybrid, capable of functioning as either:\n  1. A standard attention + MoE layer.\n  2. A linear attention + MoE layer.\n\n  NOTE: This implementation assumes every layer contains a MoE block, which is true for\n  models like Qwen3-Next-80B-A3B where `decoder_sparse_step=1`. For models that\n  interleave dense and sparse MLP layers, conditional logic would be needed here.\n\n  Attributes:\n    config: The model configuration object.\n    mesh: The device mesh for sharding.\n    model_mode: The operational mode (e.g., 'train', 'prefill').\n    layer_idx: The index of the current layer in the transformer stack.\n    quant: Optional quantization configuration.\n  \"\"\"\n\n  def __init__(\n      self, config: Config, mesh: Mesh, model_mode: str, layer_idx: int, quant: None | Quant = None, *, rngs: nnx.Rngs\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.layer_idx = layer_idx\n    self.quant = quant\n    cfg = self.config\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    # First LayerNorm, applied before the attention block.\n    self.input_layernorm = Qwen3NextRMSNorm(\n        num_features=cfg.emb_dim,\n        eps=cfg.normalization_layer_epsilon,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        rngs=rngs,\n    )\n\n    # Determine the type of attention mechanism for the current layer.\n    is_full_attention_layer = (self.layer_idx + 1) % cfg.inhomogeneous_layer_cycle_interval == 0\n\n    # Conditionally instantiate either the Linear Attention or Full Attention block.\n    if is_full_attention_layer:\n      self.attention = Qwen3NextFullAttention(\n          config=cfg,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=model_mode,\n          layer_idx=self.layer_idx,\n          rngs=rngs,\n      )\n    else:\n      self.attention = Qwen3NextGatedDeltaNet(config=cfg, dtype=cfg.dtype, rngs=rngs)\n\n    # Second LayerNorm, applied before the MoE block.\n    self.post_attention_layernorm = Qwen3NextRMSNorm(\n        num_features=cfg.emb_dim,\n        eps=cfg.normalization_layer_epsilon,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        rngs=rngs,\n    )\n\n    # Instantiate our `Qwen3NextSparseMoeBlock`.\n    self.mlp = Qwen3NextSparseMoeBlock(config=cfg, mesh=self.mesh, quant=self.quant, rngs=rngs)\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    residual = inputs\n\n    # First LayerNorm, applied before the attention block.\n    hidden_states = self.input_layernorm(inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # Conditionally apply either the Linear Attention or Full Attention block.\n    if isinstance(self.attention, Qwen3NextFullAttention):\n      attention_output = cast(Qwen3NextFullAttention, self.attention)(\n          hidden_states,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n    elif isinstance(self.attention, Qwen3NextGatedDeltaNet):\n      attention_output = cast(Qwen3NextGatedDeltaNet, self.attention)(hidden_states)\n    else:\n      raise TypeError(f\"Unexpected type for self.attention: {type(self.attention)}\")\n\n    # First residual connection after attention\n    hidden_states = residual + attention_output\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # Prepare for the MoE block by capturing the new residual\n    residual = hidden_states\n\n    # Second LayerNorm, applied before the MoE block.\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n\n    # Instantiate and call our `Qwen3NextSparseMoeBlock`.\n    mlp_output, load_balance_loss = self.mlp(hidden_states, deterministic=deterministic)\n\n    # We sow the load balancing loss so it can be collected and added to the total loss\n    # during training.\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    # Final residual connection (after the MoE block)\n    layer_output = residual + mlp_output\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        self.activation_axis_names,\n    )\n\n    return layer_output",
        "analysis": {
            "module_type": "qwen3_next_decoder_layer",
            "purpose": "Represents a single decoder layer in the Qwen3-Next architecture, which can either be a standard attention + MoE layer or a linear attention + MoE layer.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply input LayerNorm.",
                "Conditionally apply either Qwen3NextFullAttention or Qwen3NextGatedDeltaNet.",
                "Add residual connection after attention.",
                "Apply post-attention LayerNorm.",
                "Apply Qwen3NextSparseMoeBlock.",
                "Add residual connection after MoE block."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "Qwen3NextRMSNorm",
                "Qwen3NextFullAttention",
                "Qwen3NextGatedDeltaNet",
                "Qwen3NextSparseMoeBlock",
                "jax.numpy",
                "flax.linen"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters.",
                "mesh": "Device mesh for distributed computation.",
                "model_mode": "Operational mode of the model (e.g., 'train', 'prefill').",
                "layer_idx": "Index of the current layer.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The type of attention used (full or linear) is determined by the layer index and `inhomogeneous_layer_cycle_interval` in the config.",
                "Assumes every layer has an MoE block, which might not hold for all Qwen3 variants.",
                "Applies logical constraints to activation tensors."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Qwen3NextDecoderLayer with configuration, device mesh, operational mode, layer index, and optional quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration, mesh, model mode, layer index, and quantization settings.",
                        "Initialize `input_layernorm`.",
                        "Determine attention type based on `layer_idx` and `config.inhomogeneous_layer_cycle_interval`.",
                        "Conditionally instantiate `Qwen3NextFullAttention` or `Qwen3NextGatedDeltaNet`.",
                        "Initialize `post_attention_layernorm`.",
                        "Instantiate `Qwen3NextSparseMoeBlock`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Qwen3NextRMSNorm",
                        "Qwen3NextFullAttention",
                        "Qwen3NextGatedDeltaNet",
                        "Qwen3NextSparseMoeBlock",
                        "flax.nnx"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying normalization, attention, and MoE blocks with residual connections.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store input as residual.",
                        "Apply `input_layernorm`.",
                        "Apply logical constraint.",
                        "Conditionally apply attention module (`Qwen3NextFullAttention` or `Qwen3NextGatedDeltaNet`).",
                        "Add attention output to residual.",
                        "Apply logical constraint.",
                        "Capture new residual for MoE block.",
                        "Apply `post_attention_layernorm`.",
                        "Apply logical constraint.",
                        "Apply `Qwen3NextSparseMoeBlock`.",
                        "Sow MoE load balancing loss if available.",
                        "Add MoE output to residual.",
                        "Apply logical constraint.",
                        "Return final layer output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "cast",
                        "nn.with_logical_constraint",
                        "Qwen3NextFullAttention",
                        "Qwen3NextGatedDeltaNet",
                        "Qwen3NextSparseMoeBlock"
                    ],
                    "notes": [
                        "Handles different attention types based on the instantiated `self.attention` object.",
                        "Collects and sows the load balancing loss from the MoE block."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#AttentionWithNorm",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class AttentionWithNorm(nnx.Module):\n  \"\"\"Base class with shared common components: self-attention block with normalization.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n\n    batch_size, seq_len = max_utils.get_batch_seq_len_for_mode(config, model_mode)\n    dummy_inputs_shape = (batch_size, seq_len, config.emb_dim)\n    self.activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    # Corresponds to Qwen3's `input_layernorm`\n    self.pre_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n    # Self-attention block\n    query_pre_attn_scalar = config.head_dim**-0.5  # Qwen3 specific scaling\n    self.self_attention = Attention(\n        config=config,\n        num_query_heads=config.num_query_heads,\n        num_kv_heads=config.num_kv_heads,\n        head_dim=config.head_dim,\n        max_target_length=config.max_target_length,\n        max_prefill_predict_length=config.max_prefill_predict_length,\n        attention_kernel=config.attention,\n        inputs_q_shape=dummy_inputs_shape,\n        inputs_kv_shape=dummy_inputs_shape,\n        mesh=mesh,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        dropout_rate=config.dropout_rate,\n        float32_qk_product=config.float32_qk_product,\n        float32_logits=config.float32_logits,\n        quant=quant,\n        kv_quant=quantizations.configure_kv_quant(config),\n        use_ragged_attention=config.use_ragged_attention,\n        ragged_block_size=config.ragged_block_size,\n        use_qk_norm=config.use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n    # Post Attention LayerNorm (corresponds to Qwen3's `post_attention_layernorm`)\n    self.post_self_attention_layer_norm = RMSNorm(\n        num_features=config.emb_dim,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        kernel_axes=(\"norm\",),\n        epsilon=config.normalization_layer_epsilon,\n        rngs=rngs,\n    )\n\n  def apply_attention_with_norm(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n  ):\n    \"\"\"Applies self-attention with pre and post-layer normalization.\"\"\"\n    inputs = nn.with_logical_constraint(inputs, self.activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # Pre attention norm\n    lnx = self.pre_self_attention_layer_norm(inputs)\n    lnx = nn.with_logical_constraint(lnx, self.activation_axis_names)\n    # Self attention\n    attention_lnx = self.self_attention(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    attention_lnx = nn.with_logical_constraint(attention_lnx, self.activation_axis_names)\n    # Residual connection after attention\n    intermediate_inputs = inputs + attention_lnx\n    # Post attention norm\n    hidden_states = self.post_self_attention_layer_norm(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, self.activation_axis_names)\n    return hidden_states, intermediate_inputs",
        "analysis": {
            "module_type": "attention_with_norm",
            "purpose": "Base class for decoder layers, providing common self-attention and normalization components.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply pre-attention layer normalization.",
                "Perform self-attention calculation.",
                "Add attention output to the original input (residual connection).",
                "Apply post-attention layer normalization."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Config",
                "Mesh",
                "RMSNorm",
                "Attention",
                "max_utils",
                "nnx",
                "jax"
            ],
            "parameters": {
                "config": "Configuration object containing model parameters.",
                "mesh": "Device mesh for distributed computation.",
                "model_mode": "String indicating the operational mode (e.g., 'train', 'inference').",
                "quant": "Optional quantization configuration.",
                "rngs": "JAX random number generators."
            },
            "notes": [
                "This class is intended to be a base class and is not directly instantiated.",
                "It defines the common structure for self-attention with pre- and post-layer normalization.",
                "The `apply_attention_with_norm` method encapsulates the core logic."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the AttentionWithNorm module with configuration and common components.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration, mesh, and quantization settings.",
                        "Determine dummy input shapes based on model mode and config.",
                        "Instantiate `pre_self_attention_layer_norm` (RMSNorm).",
                        "Instantiate `self_attention` (Attention module).",
                        "Instantiate `post_self_attention_layer_norm` (RMSNorm)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "RMSNorm",
                        "Attention",
                        "max_utils",
                        "nnx"
                    ],
                    "notes": [
                        "The `query_pre_attn_scalar` is a Qwen3-specific scaling factor.",
                        "Dummy input shapes are used for initializing the Attention module."
                    ]
                },
                "apply_attention_with_norm": {
                    "purpose": "Applies self-attention with pre and post-layer normalization to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply logical constraint to inputs.",
                        "Checkpoint the input for gradient computation.",
                        "Apply pre-attention layer normalization.",
                        "Apply logical constraint to normalized input.",
                        "Perform self-attention calculation.",
                        "Apply logical constraint to attention output.",
                        "Add attention output to the original input (residual connection).",
                        "Apply post-attention layer normalization.",
                        "Apply logical constraint to the final hidden states."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "pre_self_attention_layer_norm",
                        "self_attention",
                        "post_self_attention_layer_norm"
                    ],
                    "notes": [
                        "Returns both the final hidden states and the intermediate result after the first residual connection."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3DecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3DecoderLayer(AttentionWithNorm):\n  \"\"\"Qwen3 Transformer decoder layer (dense).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    super().__init__(config, mesh, model_mode, quant, rngs)\n    self.mlp = MlpBlock(\n        in_features=config.emb_dim,\n        intermediate_dim=config.mlp_dim,\n        activations=config.mlp_activations,\n        intermediate_dropout_rate=config.dropout_rate,\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        config=config,\n        mesh=mesh,\n        quant=quant,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    hidden_states, intermediate_inputs = self.apply_attention_with_norm(\n        inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode\n    )\n\n    mlp_lnx = self.mlp(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n\n    layer_output = intermediate_inputs + mlp_lnx\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "qwen3_decoder_layer",
            "purpose": "Represents a single dense decoder layer in the Qwen3 Transformer architecture.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Apply pre-attention normalization.",
                "Perform self-attention calculation.",
                "Add attention output to the input (residual connection).",
                "Apply post-attention normalization.",
                "Pass the normalized output through an MLP block.",
                "Add MLP output to the intermediate result (residual connection)."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "AttentionWithNorm",
                "MlpBlock",
                "RMSNorm",
                "Attention"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions and parameters.",
                "mesh": "Device mesh for distributed computation.",
                "model_mode": "Operational mode (e.g., 'train', 'prefill').",
                "quant": "Quantization configuration, if any.",
                "rngs": "Random number generators for parameter initialization."
            },
            "notes": [
                "This layer inherits from AttentionWithNorm, which handles the attention mechanism and associated normalization layers.",
                "It specifically adds an MLP block after the attention mechanism.",
                "The output shape is the same as the input shape, indicating a transformation within the same dimensionality.",
                "It supports conditional output based on `config.scan_layers`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3MoeDecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3MoeDecoderLayer(AttentionWithNorm):\n  \"\"\"Qwen3 Transformer decoder layer (MoE).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    super().__init__(config, mesh, model_mode, quant, rngs)\n    self.moe_block = RoutedMoE(\n        config=config,\n        num_experts=config.num_experts,\n        num_experts_per_tok=config.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=config.moe_mlp_dim,  # same as config.mlp_dim\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=quant,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    hidden_states, intermediate_inputs = self.apply_attention_with_norm(\n        inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode\n    )\n\n    mlp_lnx, load_balance_loss = self.moe_block(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    layer_output = intermediate_inputs + mlp_lnx\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "qwen3_moe_decoder_layer",
            "purpose": "Represents a decoder layer in the Qwen3 Transformer model that utilizes a Mixture-of-Experts (MoE) approach for the feed-forward network.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Applies pre-attention normalization.",
                "Performs self-attention calculation.",
                "Applies residual connection after attention.",
                "Applies post-attention normalization.",
                "Processes input through a Mixture-of-Experts (MoE) block.",
                "Applies residual connection after MoE block."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "AttentionWithNorm",
                "RoutedMoE",
                "jnp",
                "nnx",
                "Config",
                "Mesh",
                "Quant"
            ],
            "parameters": {
                "config": "Configuration object for the model, containing parameters like hidden dimensions, number of experts, etc.",
                "mesh": "Device mesh for distributed computation.",
                "model_mode": "Operational mode of the model (e.g., 'train', 'inference').",
                "quant": "Quantization configuration, if any.",
                "rngs": "Random number generators for parameter initialization and dropout."
            },
            "notes": [
                "This layer inherits from `AttentionWithNorm`, which handles the attention mechanism and normalization layers.",
                "The MoE block is implemented using `RoutedMoE`.",
                "It sows a 'moe_lb_loss' (load balancing loss) if it's not None, which is useful during training.",
                "The output shape is the same as the input shape, indicating a residual connection.",
                "The `scan_layers` configuration parameter determines if the output is a tuple (for scan) or a single tensor."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Qwen3MoeDecoderLayer with configuration and components.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the parent class constructor (`AttentionWithNorm`).",
                        "Initializes the `moe_block` using `RoutedMoE` with parameters from the config."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "AttentionWithNorm.__init__",
                        "RoutedMoE"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, including attention and MoE processing.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Applies attention and normalization using `apply_attention_with_norm`.",
                        "Passes the hidden states through the `moe_block`.",
                        "Applies logical constraints to the MoE output.",
                        "Sows the load balancing loss if available.",
                        "Combines the attention output and MoE output via a residual connection.",
                        "Applies logical constraints to the final layer output.",
                        "Returns the layer output, potentially with a None value if `scan_layers` is True."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim] or tuple([batch_size, sequence_length, hidden_dim], None)"
                    },
                    "dependencies": [
                        "apply_attention_with_norm",
                        "RoutedMoE.__call__",
                        "nn.with_logical_constraint",
                        "self.sow"
                    ],
                    "notes": [
                        "The `decoder_segment_ids`, `decoder_positions`, `previous_chunk`, `page_state`, and `slot` arguments are passed through but not directly used in the core logic shown in this block; they are handled by `apply_attention_with_norm` or are for potential future use.",
                        "The `deterministic` flag controls dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleDecoderLayer(nnx.Module):\n  \"\"\"Decoder layer consisting of a single [embed, embed] weight matrix.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ) -> None:\n\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.rngs = rngs\n    self.quant = quant\n\n    init_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), sharding=(\"embed\", \"mlp\"), mesh=self.mesh)\n\n    self.weights = nnx.Param(\n        init_fn(self.rngs.params(), (self.config.emb_dim, self.config.emb_dim)),\n    )\n\n    activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    self.out_sharding = (\n        NamedSharding(self.mesh, nn.logical_to_mesh_axes(activation_axis_names))\n        if config.shard_mode == ShardMode.EXPLICIT\n        else None\n    )\n\n  def __call__(\n      self, inputs: jnp.ndarray, positions, segmentation, deterministic, model_mode, previous_chunk=None, page_state=None\n  ):\n    if self.config.scan_layers:\n      return jnp.dot(inputs, self.weights.astype(inputs.dtype), out_sharding=self.out_sharding), None\n    return jnp.dot(inputs, self.weights.astype(inputs.dtype), out_sharding=self.out_sharding)",
        "analysis": {
            "module_type": "simple_decoder_layer",
            "purpose": "Implements a simplified decoder layer with a single weight matrix for linear transformation.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "float32 or float16"
            },
            "processing_steps": [
                "Performs a matrix multiplication (dot product) between the input tensor and the layer's weights.",
                "Optionally applies sharding to the output based on configuration."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "The same as the input dtype."
            },
            "dependencies": [
                "jax.numpy",
                "jax.sharding.Mesh",
                "jax.sharding.NamedSharding",
                "flax.nnx",
                "MaxText.common_types.Config",
                "MaxText.common_types.ShardMode",
                "MaxText.layers.quantizations",
                "MaxText.layers.nnx_wrappers"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions and sharding settings.",
                "mesh": "JAX Mesh object for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'train', 'predict').",
                "rngs": "JAX Rngs object for parameter initialization.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The layer uses a single weight matrix of shape [emb_dim, emb_dim].",
                "Initialization uses lecun_normal.",
                "Output sharding is applied if config.shard_mode is ShardMode.EXPLICIT.",
                "The `scan_layers` config option determines if an additional None is returned (for compatibility with scan operations)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the SimpleDecoderLayer with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, model mode, RNGs, and quantization settings.",
                        "Initializes a weight parameter using lecun_normal and applies partitioning based on the mesh.",
                        "Determines output sharding based on the configuration's shard_mode."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.with_partitioning",
                        "nnx.initializers.lecun_normal",
                        "NamedSharding",
                        "nn.logical_to_mesh_axes"
                    ],
                    "notes": [
                        "The weight matrix is initialized with dimensions (emb_dim, emb_dim)."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dimension]",
                        "dtype": "float32 or float16"
                    },
                    "processing_steps": [
                        "Performs a matrix multiplication of inputs with the layer's weights.",
                        "Casts weights to the input dtype before multiplication.",
                        "Applies output sharding if configured.",
                        "Conditionally returns None as a second element if config.scan_layers is True."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dimension] or ([batch_size, sequence_length, embedding_dimension], None)",
                        "dtype": "The same as the input dtype."
                    },
                    "dependencies": [
                        "jnp.dot"
                    ],
                    "notes": [
                        "The `positions`, `segmentation`, `deterministic`, `model_mode`, `previous_chunk`, and `page_state` arguments are accepted but not used in this simplified layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleMlpDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleMlpDecoderLayer(nnx.Module):\n  \"\"\"Decoder layer consisting of [embed,mlp] followed by an [mlp,embed] matmul.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      rngs: nnx.Rngs,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ) -> None:\n\n    self.config = config\n    self.mesh = mesh\n    self.model_mode = model_mode\n    self.rngs = rngs\n    self.quant = quant\n\n    init_ff1_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), sharding=(\"embed\", \"mlp\"), mesh=self.mesh)\n\n    self.ff_1 = nnx.Param(\n        init_ff1_fn(self.rngs.params(), (self.config.emb_dim, self.config.mlp_dim)),\n    )\n\n    init_ff2_fn = nnx.with_partitioning(nnx.initializers.lecun_normal(), sharding=(\"mlp\", \"embed\"), mesh=self.mesh)\n\n    self.ff_2 = nnx.Param(\n        init_ff2_fn(self.rngs.params(), (self.config.mlp_dim, self.config.emb_dim)),\n    )\n\n    activation_axes_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    self.activation_sharding = (\n        NamedSharding(mesh, nn.logical_to_mesh_axes(activation_axes_names))\n        if config.shard_mode == ShardMode.EXPLICIT\n        else None\n    )\n    mlp_axes_names = (\"activation_batch\", \"activation_norm_length\", \"activation_mlp\")\n    self.mlp_sharding = (\n        NamedSharding(mesh, nn.logical_to_mesh_axes(mlp_axes_names)) if config.shard_mode == ShardMode.EXPLICIT else None\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      positions,\n      segmentation,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=0,\n  ):\n    intermediate = jnp.dot(inputs, self.ff_1.astype(inputs.dtype), out_sharding=self.mlp_sharding)\n    output = jnp.dot(intermediate, self.ff_2.astype(inputs.dtype), out_sharding=self.activation_sharding)\n    if self.config.scan_layers:\n      return output, None\n    return output",
        "analysis": {
            "module_type": "simple_mlp_decoder_layer",
            "purpose": "Implements a decoder layer composed of two linear transformations, simulating a feed-forward network within a larger model.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "float32 (or compatible)"
            },
            "processing_steps": [
                "First linear transformation (inputs @ ff_1)",
                "Second linear transformation (intermediate @ ff_2)",
                "Conditional return based on config.scan_layers"
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embedding_dimension]",
                "dtype": "float32 (or compatible)"
            },
            "dependencies": [
                "jax.numpy",
                "jax.sharding.Mesh",
                "jax.sharding.NamedSharding",
                "flax.nnx",
                "flax.linen",
                "MaxText.common_types.Config",
                "MaxText.common_types.ShardMode",
                "MaxText.layers.quantizations",
                "MaxText.layers.nnx_wrappers"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions and sharding settings.",
                "mesh": "JAX Mesh object for distributed computation.",
                "model_mode": "String indicating the current model mode (e.g., 'train', 'eval').",
                "rngs": "JAX Rngs object for parameter initialization.",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "The layer consists of two feed-forward layers (ff_1 and ff_2) with dimensions (emb_dim, mlp_dim) and (mlp_dim, emb_dim) respectively.",
                "Uses lecun_normal initializer for weights.",
                "Supports explicit sharding based on config.shard_mode.",
                "The __call__ method accepts additional arguments (positions, segmentation, deterministic, etc.) which are not directly used in the current implementation but are part of the expected interface.",
                "If config.scan_layers is True, it returns a tuple (output, None)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the SimpleMlpDecoderLayer with configuration, mesh, and random number generators.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration, mesh, model_mode, rngs, and quant.",
                        "Initializes ff_1 parameter with lecun_normal initializer and specified sharding.",
                        "Initializes ff_2 parameter with lecun_normal initializer and specified sharding.",
                        "Sets up activation and mlp sharding based on config.shard_mode."
                    ],
                    "output": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "nnx.with_partitioning",
                        "nnx.initializers.lecun_normal",
                        "nnx.Param",
                        "NamedSharding",
                        "nn.logical_to_mesh_axes"
                    ],
                    "notes": [
                        "Parameters ff_1 and ff_2 are partitioned across the mesh.",
                        "Sharding for intermediate and final outputs is configured based on the shard_mode."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying two linear transformations.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embedding_dimension]",
                        "dtype": "float32 (or compatible)"
                    },
                    "processing_steps": [
                        "Applies the first linear transformation: inputs @ self.ff_1.",
                        "Applies the second linear transformation: intermediate @ self.ff_2.",
                        "Conditionally returns the output and None if config.scan_layers is True, otherwise returns only the output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dimension]",
                        "dtype": "float32 (or compatible)"
                    },
                    "dependencies": [
                        "jnp.dot"
                    ],
                    "notes": [
                        "The input dtype is cast to match the parameter dtype before the dot product.",
                        "The output sharding is applied based on the initialized activation_sharding.",
                        "The arguments 'positions', 'segmentation', 'deterministic', 'model_mode', 'previous_chunk', 'page_state', and 'slot' are accepted but not used in this specific layer's computation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "functionality": "Validates configuration settings for a decoding process, ensuring that the model is not attempting to load a full state and that the number of initial prefill streams is within valid bounds.",
            "usage": "This function is called internally during the setup phase of a decoding process. It takes a configuration object as input and raises an AssertionError if the configuration is invalid. It expects the `config` object to have attributes `load_full_state_path` and implicitly uses global constants `_INITIAL_PREFILL_STREAMS` and `_NUM_STREAMS`."
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#main",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "functionality": "This code block implements a main function that orchestrates interleaved inference for a language model. It handles prefilling initial prompts, generating subsequent tokens in an interleaved manner across multiple streams, and managing the lifecycle of these streams until generation is complete or a maximum length is reached.",
            "usage": "This function is intended to be the entry point for running an interleaved inference process. It takes a sequence of strings `argv` as input, which are typically command-line arguments used to configure the inference process via `pyconfig.initialize`. The function initializes the MaxEngine, loads parameters, tokenizes an input prompt, and then enters a loop to perform prefill and generation steps for multiple concurrent streams. The output is printed to the console, showing the input prompt and the generated output for each stream. It requires a configuration object that specifies parameters like `prompt`, `max_prefill_predict_length`, `per_device_batch_size`, `max_target_length`, and `autoregressive_decode_assert`."
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#reverse_transpose",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def reverse_transpose(transposed_array, transpose_axis_order):\n  return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)",
        "analysis": {
            "functionality": "Reverses the axis transposition applied during a previous transpose operation.",
            "usage": "Takes a transposed array and an axis order, and returns the array with axes moved back to their original positions. This is useful for undoing a specific type of transposition, likely for data processing or model compatibility."
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#transpose_tuple",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def transpose_tuple(items: tuple[Any, ...], axis_order: AxisIdxes) -> tuple[Any, ...]:\n  return tuple((items[i] for i in axis_order))",
        "analysis": {
            "functionality": "Transposes elements of a tuple based on a specified axis order.",
            "usage": "Takes a tuple `items` and an `axis_order` tuple of indices. It returns a new tuple where the elements from `items` are reordered according to `axis_order`. For example, if `items` is (a, b, c) and `axis_order` is (2, 0, 1), the function returns (c, a, b)."
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVQuant",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVQuant:\n  \"\"\"Class to configure quantization for KV cache.\"\"\"\n\n  axis_cfg = \"\"\n  dtype = None\n\n  def __init__(self, config: Config):\n    assert config.quantize_kvcache\n    self.axis_cfg = config.kv_quant_axis\n    self.dtype = self._get_dtype(config.kv_quant_dtype)\n\n  def _get_dtype(self, dtype_cfg: str):\n    if dtype_cfg == \"int4\":\n      return jnp.int4\n    if dtype_cfg == \"int8\":\n      return jnp.int8\n    if dtype_cfg == \"fp8\":\n      return jnp.float8_e4m3fn\n    raise ValueError(f\"Invalid kv_quant_dtype: {dtype_cfg}\")\n\n  def _get_max_axis(self, axis_names: AxisNames):\n    if self.axis_cfg == \"dkv\":\n      return axis_names.index(CACHE_KV)\n    if self.axis_cfg == \"heads_and_dkv\":\n      return (axis_names.index(CACHE_HEADS), axis_names.index(CACHE_KV))\n    raise ValueError(f\"Invalid KV quant axis cfg: {self.axis_cfg}\")\n\n  def quantize(self, kv: Array, axis_names: AxisNames):\n    \"\"\"Quantize key/values stored in kvcache.\"\"\"\n    assert self.axis_cfg, \"KV quant axis cannot be None\"\n    max_axis = self._get_max_axis(axis_names)\n    scale = jnp.max(jnp.abs(kv), axis=max_axis, keepdims=True)\n    if self.dtype == jnp.int8:\n      value = jnp.int8(jnp.rint(kv * (MAX_INT8 / scale)))\n      return value, scale\n    if self.dtype == jnp.int4:\n      value = jnp.int4(jnp.rint(kv * (MAX_INT4 / scale)))\n      return value, scale\n    if self.dtype == jnp.float8_e4m3fn:\n      value = jnp.float8_e4m3fn(kv * (E4M3_MAX / scale))\n      return value, scale\n    raise ValueError(f\"Invalid KV quant dtype:{self.dtype}.\")\n\n  def einsum_fn_with_rhs_qtensor(\n      self,\n      rhs_dequant_mode=None,\n      rhs_calibration_mode=None,\n      lhs_dequant_mode=None,\n      lhs_calibration_mode=None,\n  ):\n    \"\"\"einsum function where QTensor is the right-hand-side\"\"\"\n    # Assumes kv is already quantized.\n    einsum = jnp.einsum\n    if self.dtype != jnp.float8_e4m3fn:\n      num_bits = 4 if self.dtype == jnp.int4 else 8\n      kv_cfg = aqt_config.dot_general_make(\n          lhs_bits=None,\n          rhs_bits=num_bits,\n          bwd_bits=None,\n          use_fwd_quant=False,\n      )\n    else:\n      kv_cfg = aqt_config.config_fwd_fp8()\n\n    if rhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, rhs_dequant_mode=rhs_dequant_mode)\n    if rhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          rhs_calibration_mode=rhs_calibration_mode,\n      )\n    if lhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, lhs_dequant_mode=lhs_dequant_mode)\n    if lhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          lhs_calibration_mode=lhs_calibration_mode,\n      )\n    einsum = aqt_flax.AqtEinsum(\n        rhs_quant_mode=aqt_flax.QuantMode.TRAIN,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        cfg=kv_cfg,\n    )\n    return einsum\n\n  def einsum_fn_with_rhs_qtensor_and_dequant(self):\n    \"\"\"Get einstein summation for different dequant modes.\"\"\"\n    if self.dtype == jnp.float8_e4m3fn:\n      return self.einsum_fn_with_rhs_qtensor(\n          lhs_dequant_mode=aqt_config.DequantMode.THIS_INPUT,\n          lhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )\n    else:\n      return self.einsum_fn_with_rhs_qtensor(\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )",
        "analysis": {
            "module_type": "kv_quantization_config",
            "purpose": "Configures quantization parameters for the KV cache.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes with a configuration object.",
                "Determines the quantization data type (int4, int8, fp8).",
                "Determines the axis configuration for quantization (dkv or heads_and_dkv).",
                "Quantizes the KV cache tensor using the configured data type and axis.",
                "Provides methods to configure einsum operations for quantized tensors."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "aqt.jax.v2.config",
                "aqt.jax.v2.aqt_tensor",
                "aqt.jax.v2.flax.aqt_flax",
                "MaxText.common_types"
            ],
            "parameters": {
                "config": "Configuration object containing quantization settings like kv_quant_axis and kv_quant_dtype."
            },
            "notes": [
                "The `quantize` method takes a KV cache tensor and its axis names, and returns the quantized tensor and its scale.",
                "The `einsum_fn_with_rhs_qtensor` and `einsum_fn_with_rhs_qtensor_and_dequant` methods are used to create specialized einsum functions for quantized tensors."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVQuant object with configuration settings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Asserts that kvcache quantization is enabled in the config.",
                        "Stores the KV quantization axis configuration.",
                        "Determines and stores the quantization data type."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "_get_dtype": {
                    "purpose": "Maps a string configuration to a JAX numpy dtype for quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks the input dtype_cfg string ('int4', 'int8', 'fp8').",
                        "Returns the corresponding JAX numpy dtype.",
                        "Raises ValueError for invalid configurations."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp"
                    ],
                    "notes": []
                },
                "_get_max_axis": {
                    "purpose": "Determines the axis or axes to use for calculating the quantization scale based on the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks the `self.axis_cfg` ('dkv' or 'heads_and_dkv').",
                        "Finds the index of the relevant cache axis names (CACHE_KV, CACHE_HEADS).",
                        "Returns the axis index or a tuple of axis indices.",
                        "Raises ValueError for invalid configurations."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "AxisNames",
                        "CACHE_KV",
                        "CACHE_HEADS"
                    ],
                    "notes": []
                },
                "quantize": {
                    "purpose": "Quantizes a given key/value tensor for the KV cache.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                        "dtype": "float32 or similar"
                    },
                    "processing_steps": [
                        "Asserts that the KV quant axis configuration is set.",
                        "Determines the maximum axis for scaling.",
                        "Calculates the scale factor based on the maximum absolute value along the specified axis.",
                        "Performs quantization to the configured dtype (int8, int4, or float8_e4m3fn) using the calculated scale.",
                        "Returns the quantized value and the scale."
                    ],
                    "output": {
                        "shape": "Tuple of (quantized_kv, scale)"
                    },
                    "dependencies": [
                        "jnp",
                        "Array",
                        "AxisNames",
                        "MAX_INT8",
                        "MAX_INT4",
                        "E4M3_MAX"
                    ],
                    "notes": [
                        "The input `kv` is expected to be in a shape that aligns with the `axis_names` provided."
                    ]
                },
                "einsum_fn_with_rhs_qtensor": {
                    "purpose": "Creates an AqtEinsum function configured for training with a quantized right-hand-side tensor.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines the quantization configuration (`kv_cfg`) based on `self.dtype`.",
                        "Applies dequantization and calibration modes if provided as arguments.",
                        "Instantiates and returns an `aqt_flax.AqtEinsum` object."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp",
                        "aqt_config",
                        "aqt_flax"
                    ],
                    "notes": [
                        "Assumes the KV cache tensor is already quantized.",
                        "Configures the einsum for training mode (`QuantMode.TRAIN`)."
                    ]
                },
                "einsum_fn_with_rhs_qtensor_and_dequant": {
                    "purpose": "Returns an AqtEinsum function configured with specific dequantization modes for FP8 or other dtypes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks `self.dtype`.",
                        "If `float8_e4m3fn`, calls `einsum_fn_with_rhs_qtensor` with specific dequant/calibration modes for both LHS and RHS.",
                        "Otherwise, calls `einsum_fn_with_rhs_qtensor` with dequant/calibration modes for RHS only."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "aqt_config"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_heads: int,\n    value_heads: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the KVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n    cache_logical_axis_names: The logical axis names for the cache.\n    cache_scale_logical_axis_names: The logical axis names for the cache scale.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    key_axis_order: The axis order for the key.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `KVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      KVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      kv_quant=kv_quant,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      key_axis_order=key_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "kv_cache_initializer",
            "purpose": "Initializes and returns a Flax Linen module that wraps an NNX KVCache module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `KVCache` NNX module to a Linen module.",
                "Passes all initialization arguments to the `KVCache` constructor."
            ],
            "output": {
                "shape": "N/A (returns a Flax Linen module)"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "KVCache"
            ],
            "parameters": {
                "max_prefill_length": "The maximum length of the prefill sequence.",
                "max_target_length": "The maximum length of the target sequence.",
                "batch": "The batch size.",
                "key_seq_len": "The sequence length for keys.",
                "value_seq_len": "The sequence length for values.",
                "key_heads": "The number of heads for keys.",
                "value_heads": "The number of heads for values.",
                "key_head_size": "The size of each head for keys.",
                "value_head_size": "The size of each head for values.",
                "dtype": "The data type for the cache.",
                "kv_quant": "Optional KV quantization configuration.",
                "prefill_cache_logical_axis_names": "Logical axis names for the prefill cache.",
                "cache_logical_axis_names": "Logical axis names for the general cache.",
                "cache_scale_logical_axis_names": "Logical axis names for the cache scale.",
                "prefill_cache_axis_order": "Axis order for the prefill cache.",
                "ar_cache_axis_order": "Axis order for the autoregressive cache.",
                "key_axis_order": "Axis order for keys.",
                "use_chunked_prefill": "Whether to use chunked prefill.",
                "model_mode": "The current model mode (e.g., prefill, autoregressive).",
                "name": "Optional name for the Linen module."
            },
            "notes": [
                "This function acts as a factory or wrapper to create a Linen-compatible KVCache module from its NNX counterpart.",
                "It configures the KVCache with various parameters related to sequence lengths, batch size, heads, dimensions, and quantization."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVCache(nnx.Module):\n  \"\"\"Implementation of the KVCache.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len, key_heads,\n      # and value_heads from key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_heads: int,\n      value_heads: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in KVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the KVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      model_mode: The model mode.\n      use_chunked_prefill: Whether to use chunked prefill.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.max_prefill_length = max_prefill_length\n    self.max_target_length = max_target_length\n    self.batch = batch\n    self.key_seq_len = key_seq_len\n    self.value_seq_len = value_seq_len\n    self.key_heads = key_heads\n    self.value_heads = value_heads\n    self.key_head_size = key_head_size\n    self.value_head_size = value_head_size\n    self.dtype = dtype\n    self.kv_quant = kv_quant\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.key_axis_order = key_axis_order\n    self.model_mode = model_mode\n    self.use_chunked_prefill = use_chunked_prefill\n\n    if model_mode in (MODEL_MODE_PREFILL, MODEL_MODE_AUTOREGRESSIVE):\n      self._initialize_prefill_caches(model_mode)\n      self._initialize_ar_cache_vars(model_mode)\n\n  @property\n  def prefill_key_vars(self):\n    return (self.cached_prefill_key, self.cached_prefill_key_scale)\n\n  @property\n  def prefill_value_vars(self):\n    return (self.cached_prefill_value, self.cached_prefill_value_scale)\n\n  @property\n  def ar_key_vars(self):\n    return (self.cached_ar_key, self.cached_ar_key_scale)\n\n  @property\n  def ar_value_vars(self):\n    return (self.cached_ar_value, self.cached_ar_value_scale)\n\n  def _get_cached_kv_dtype(self):\n    return self.kv_quant.dtype if self.kv_quant else self.dtype\n\n  def _get_cache_scale_logical_shape(self, heads, cache_length):\n    assert self.kv_quant\n    if self.kv_quant.axis_cfg == \"dkv\":\n      return (self.batch, cache_length, heads, 1)\n    if self.kv_quant.axis_cfg == \"heads_and_dkv\":\n      return (self.batch, cache_length, 1, 1)\n    raise ValueError(f\"Invalid config for kv_quant_axis:{self.kv_quant.axis_cfg}\")\n\n  def _initialize_prefill_caches(self, model_mode):\n    \"\"\"Get a shaped abstraction of the state\"\"\"\n\n    cache_length = self.max_prefill_length\n    dtype = self._get_cached_kv_dtype()\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    self.cached_prefill_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_prefill_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n\n    self.cache_prefill_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      self.cached_prefill_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_prefill_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_prefill_key_scale = None\n      self.cached_prefill_value_scale = None\n\n  def _get_prefill_cache_vars(self):\n    return self.prefill_key_vars, self.prefill_value_vars, self.cache_prefill_segment_id\n\n  def _initialize_ar_cache_vars(self, model_mode):\n    \"\"\"get ar cache vars\"\"\"\n\n    dtype = self._get_cached_kv_dtype()\n    if self.max_target_length <= self.max_prefill_length:\n      raise ValueError(\n          f\"max_target_length: {self.max_target_length} should be greater than max_prefill_length:\"\n          f\" {self.max_prefill_length}!\"\n      )\n    cache_length = self.max_target_length - self.max_prefill_length\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    # TODO(b/339703100): investigate the issue why with_logical_partitioning doesn't enforce sharding\n    self.cached_ar_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_key.value = nn.with_logical_constraint(\n        self.cached_ar_key.value,\n        cache_axis_names,\n    )\n\n    self.cached_ar_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_value.value = nn.with_logical_constraint(\n        self.cached_ar_value.value,\n        cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n    self.cache_ar_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    self.cached_ar_lengths = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0],), dtype=jnp.int32),\n        sharding=(CACHE_BATCH,),\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      self.cached_ar_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_ar_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_ar_key_scale = None\n      self.cached_ar_value_scale = None\n\n    self.cache_ar_index = nnx.Cache(\n        jnp.zeros((1,), dtype=jnp.int32),\n        sharding=(),\n    )\n\n  def _get_ar_cache_vars(self):\n    return self.ar_key_vars, self.ar_value_vars, self.cache_ar_segment_id, self.cache_ar_index, self.cached_ar_lengths\n\n  def kv_cache_chunked_prefill(\n      self, key: Array, value: Array, decoder_segment_ids: Array, previous_chunk: None | Array = None\n  ):\n    \"\"\"Update the current kv cache into previous chunk and return needed length.\n\n    The previous chunk kv cache should be in the model's param.\n\n    Prefill cache need to be max prefill length to prevent different shape of kv cache.\n    Different shape of kv cache in previous chunk could produce different compiled graph.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n      previous_chunk:\n        In shape [b, s]. The tokens without padding in previous chunk.\n        Use to preserve the previous kv cache.\n\n    Returns:\n      key, value, decoder_segment_id.\n    \"\"\"\n\n    assert not self.kv_quant, \"Not support kv_quant now.\"\n    if decoder_segment_ids is not None:\n      _, segment_id_seq_len = decoder_segment_ids.shape\n      assert self.key_seq_len == segment_id_seq_len, f\"{self.key_seq_len=}, {segment_id_seq_len=} should match.\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n    assert self.key_seq_len == self.value_seq_len, f\"{self.key_seq_len=}, {self.value_seq_len=} should match.\"\n\n    next_pos = 0\n    if previous_chunk is not None:\n      # We only have 1 prompt in prefill mode.\n      next_pos = previous_chunk.shape[1]\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n    # TODO: Find a way to not enable the ar cache for prefill mode.\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    # For quantized kv cached. Could be get without transpose twice.\n    cached_key = self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order)\n    cached_value = self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order)\n    cached_key_value = jnp.transpose(cached_key, self.prefill_cache_axis_order)\n    cached_value_value = jnp.transpose(cached_value, self.prefill_cache_axis_order)\n\n    seq_axis = self.prefill_cache_logical_axis_names.index(CACHE_SEQUENCE)\n    cache_seq_axis = self.prefill_cache_axis_order.index(seq_axis)\n\n    assert next_pos + key_shaped_for_cache.shape[cache_seq_axis] <= self.max_prefill_length, (\n        f\"Previous kv cache[{next_pos}] + \"\n        f\"current kv cache[{key_shaped_for_cache.shape[cache_seq_axis]}] \"\n        f\"> max length[{self.max_prefill_length}]\"\n    )\n\n    # We don't zero out remain values. Use segment id to mask out.\n    cached_prefill_key_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_key_value, key_shaped_for_cache, next_pos, cache_seq_axis\n    )\n    cached_prefill_value_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_value_value, value_shaped_for_cache, next_pos, cache_seq_axis\n    )\n\n    if decoder_segment_ids is not None:\n      # Need zero out the remain values to prevent wrong mask in autoregressive.\n      previous_segment_id = cached_prefill_segment_id_var.value[:, :next_pos]\n      cached_prefill_segment_id_var.value = jnp.zeros_like(cached_prefill_segment_id_var.value, dtype=jnp.int32)\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, previous_segment_id, start_index=0, axis=1\n      )\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, decoder_segment_ids, next_pos, axis=1\n      )\n\n    # Return needed kv cache to reduce computation of attention.\n    needed_prefill_key_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_key_vars[0].value, start_index=0, slice_size=(next_pos + self.key_seq_len), axis=cache_seq_axis\n    )\n    needed_prefill_value_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_value_vars[0].value, start_index=0, slice_size=(next_pos + self.value_seq_len), axis=cache_seq_axis\n    )\n    needed_segment_id = None\n    if decoder_segment_ids is not None:\n      needed_segment_id = jax.lax.dynamic_slice_in_dim(\n          cached_prefill_segment_id_var.value, start_index=0, slice_size=(next_pos + segment_id_seq_len), axis=1\n      )\n\n    return (\n        jnp.transpose(needed_prefill_key_value, self.key_axis_order),\n        jnp.transpose(needed_prefill_value_value, self.key_axis_order),\n        needed_segment_id,\n    )\n\n  def kv_cache_prefill(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n  ):\n    \"\"\"In prefill mode, we zero out the existing cache, run the computation and\n    prepare the cache as necessary.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n\n    Returns:\n      key, value, decoder_segment_id.\n\n    \"\"\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    if self.kv_quant:\n      prefill_key_axis_names = transpose_tuple(self.cache_logical_axis_names, self.prefill_cache_axis_order)\n      key_shaped_for_cache, key_scale_shaped_for_cache = self.kv_quant.quantize(\n          key_shaped_for_cache, prefill_key_axis_names\n      )\n      value_shaped_for_cache, value_scale_shaped_for_cache = self.kv_quant.quantize(\n          value_shaped_for_cache, prefill_key_axis_names\n      )\n      cached_prefill_key_vars[1].value = key_scale_shaped_for_cache\n      cached_prefill_value_vars[1].value = value_scale_shaped_for_cache\n\n    cached_prefill_key_vars[0].value = key_shaped_for_cache\n    cached_prefill_value_vars[0].value = value_shaped_for_cache\n\n    if decoder_segment_ids is not None:\n      cached_prefill_segment_id_var.value = decoder_segment_ids\n    return key, value, decoder_segment_ids\n\n  def update_ar_key_value(\n      self,\n      one_token_key: Array,\n      one_token_value: Array,\n      key_caches: tuple[nnx.Cache, nnx.Cache | None],\n      value_caches: tuple[nnx.Cache, nnx.Cache | None],\n      one_hot_indices: Array,\n      lengths: Array,\n      use_ragged_attention: bool,\n  ) -> None:\n    \"\"\"Adds a single token's results to the ar kv cache\n\n    Args:\n        one_token_key (Array): Key of one token to add to the cache\n        one_token_value (Array): Value of one token to add to the cache\n        cached_ar_key (tuple[nnx.Cache, nnx.Cache|None],): Cached keys to add new token key to, possibly with scale\n        cached_ar_value (tuple[nnx.Cache, nnx.Cache|None],: Cached values to add new token value to, possible with scale\n        one_hot_indices (Array): Location of the new token within the cache\n\n    Returns:\n        tuple[Array, Array]: Updated caches for key and value with new token info added\n    \"\"\"\n\n    cached_key, cached_key_scale = key_caches\n    cached_value, cached_value_scale = value_caches\n\n    # In order to update the key, value caches with the current key and\n    # value, we reshape the one_token_key and one_token_value\n    one_token_key_shaped_for_cache = jnp.transpose(one_token_key, self.ar_cache_axis_order)\n    one_token_value_shaped_for_cache = jnp.transpose(one_token_value, self.ar_cache_axis_order)\n\n    ar_cache_axis_names = transpose_tuple(self.cache_logical_axis_names, self.ar_cache_axis_order)\n    if self.kv_quant:\n      one_token_key_shaped_for_cache, one_token_key_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_key_shaped_for_cache, ar_cache_axis_names\n      )\n      one_token_value_shaped_for_cache, one_token_value_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_value_shaped_for_cache, ar_cache_axis_names\n      )\n\n    ar_cache_update_idx = jnp.squeeze(one_hot_indices)\n    ar_cache_sequence_axis = ar_cache_update_axis = ar_cache_axis_names.index(CACHE_SEQUENCE)\n    ar_cache_batch_axis = ar_cache_axis_names.index(CACHE_BATCH)\n\n    if use_ragged_attention:\n      cache_locations = [slice(None)] * 4\n      new_token_locations = [slice(None)] * 4\n      new_token_locations[ar_cache_sequence_axis] = 0\n\n      def key_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_key_shaped_for_cache[tuple(new_token_locations)])\n\n      def value_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_value_shaped_for_cache[tuple(new_token_locations)])\n\n      cached_key.value = jax.lax.fori_loop(\n          0, one_token_key_shaped_for_cache.shape[0], key_body, cached_key.value, unroll=8\n      )\n      cached_value.value = jax.lax.fori_loop(\n          0, one_token_value_shaped_for_cache.shape[0], value_body, cached_value.value, unroll=8\n      )\n\n    else:\n      one_hot_indices = one_hot_indices.astype(int)\n\n      # Align batch size for cache with new token in decoding\n      if cached_key.value.shape[2] != one_token_key_shaped_for_cache.shape[2]:\n        cached_key.value = jnp.repeat(cached_key.value, one_token_key_shaped_for_cache.shape[2], axis=2)\n        cached_value.value = jnp.repeat(cached_value.value, one_token_value_shaped_for_cache.shape[2], axis=2)\n\n      cached_key.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key.value, one_token_key_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n      cached_value.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value.value, one_token_value_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n    cached_key.value = nn.with_logical_constraint(cached_key.value, ar_cache_axis_names)\n    cached_value.value = nn.with_logical_constraint(cached_value.value, ar_cache_axis_names)\n\n    if self.kv_quant:\n      ar_cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n      ar_cache_scale_update_axis = ar_cache_scale_axis_names.index(CACHE_SCALE_SEQUENCE)\n      assert cached_key_scale is not None, \"cached_key_scale_var cannot be None\"\n      assert cached_value_scale is not None, \"cached_value_scale_var cannot be None\"\n      cached_key_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key_scale.value, one_token_key_scale_shaped_for_cache, ar_cache_update_idx, ar_cache_scale_update_axis\n      )\n      cached_value_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value_scale.value,\n          one_token_value_scale_shaped_for_cache,\n          ar_cache_update_idx,\n          ar_cache_scale_update_axis,\n      )\n\n  def get_cached_values(self, cache_vars, target_dtype, cache_axis_order) -> jax.Array | KVTensor:\n    \"\"\"get cached values\"\"\"\n    cache_var, cache_scale_var = cache_vars\n    cache_value = cache_var.value\n    if cache_scale_var is not None:\n      scale_value = cache_scale_var.value\n      dtype = cache_value.dtype\n      if dtype == jnp.int8:\n        scale_value /= MAX_INT8\n      elif dtype == jnp.int4:\n        scale_value /= MAX_INT4\n      elif dtype == jnp.float8_e4m3fn:\n        scale_value /= E4M3_MAX\n\n      cache_value = KVTensor(qvalue=cache_value, scale=[scale_value], scale_t=None, dequant_dtype=target_dtype, bias=[])\n    cache_value_in_logical_shape = jax.tree.map(lambda x: reverse_transpose(x, cache_axis_order), cache_value)\n    return cache_value_in_logical_shape\n\n  def kv_cache_autoregressive(\n      self,\n      key: Array,\n      value: Array,\n      use_ragged_attention: bool = False,\n  ):\n    \"\"\"In autoregressive mode, we update the cache for this entry and\n       then return the full cache.\n\n    Args:\n      key: in shape [b, 1, n, d].\n      value: in shape [b, 1, n, d].\n      decoder_segment_ids: [b, 1] -- marking segment ids for tokens\n\n    Returns:\n      tuple of (key, value, segment_id) for both prefill and ar cache,\n    Raises:\n      ValueError: when key/value shape is not [batch, 1, num_heads, heads_dim].\n    \"\"\"\n    _, sequence, _, _ = value.shape\n    if sequence != 1:\n      raise ValueError(f\"Sequence length should be 1 during autoregression, got {sequence=}\")\n\n    cached_ar_key_vars, cached_ar_value_vars, cached_ar_segment_id_var, cache_ar_index_var, cache_ar_lengths_var = (\n        self._get_ar_cache_vars()\n    )\n\n    self.update_ar_key_value(\n        key,\n        value,\n        cached_ar_key_vars,\n        cached_ar_value_vars,\n        cache_ar_index_var.value,\n        cache_ar_lengths_var.value,\n        use_ragged_attention,\n    )\n    active_indicator = jnp.zeros((self.batch, 1), dtype=jnp.int32) + DECODING_ACTIVE_SEQUENCE_INDICATOR\n\n    # Align batch size for cached segment IDs with indicator in decoding\n    if cached_ar_segment_id_var.value.shape[0] != active_indicator.shape[0]:\n      cached_ar_segment_id_var.value = jnp.repeat(cached_ar_segment_id_var.value, active_indicator.shape[0], axis=0)\n\n    cached_ar_segment_id_var.value = jax.lax.dynamic_update_index_in_dim(\n        cached_ar_segment_id_var.value, active_indicator, jnp.squeeze(cache_ar_index_var.value), 1\n    )\n    cache_ar_index_var.value = jnp.mod(cache_ar_index_var.value + 1, self.max_target_length - self.max_prefill_length)\n    cache_ar_lengths_var.value = cache_ar_lengths_var.value.at[:].add(1)\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    cached_prefill = (\n        self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order),\n        self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order),\n        cached_prefill_segment_id_var.value,\n    )\n\n    cached_ar = (\n        self.get_cached_values(cached_ar_key_vars, key.dtype, self.ar_cache_axis_order),\n        self.get_cached_values(cached_ar_value_vars, value.dtype, self.ar_cache_axis_order),\n        cached_ar_segment_id_var.value,\n        cache_ar_lengths_var.value,\n    )\n    return cached_prefill, cached_ar\n\n  def __call__(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple:\n    \"\"\"KV cache takes the current state and updates the state accordingly.\n\n    The key and value have dimension [b, s, n_kv, d],\n    but we cache them with a reshape as defined in *_axis_order config as a TPU\n    fusion optimization. This also enables the \"scatter via one-hot\n    broadcast\" trick, which means we do a one-hot broadcast instead of a\n    scatter/gather operations, resulting in a 3-4x speedup in practice.\n\n    Args:\n      key: in shape [b, s, n_kv, d].\n      value: in shape [b, s, n_kv, d].\n      model_mode: model mode controlling model.\n\n    Returns:\n      two tuples of (k, v, decoder_segments) -- either can be Nones\n\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      if self.use_chunked_prefill:\n        return self.kv_cache_chunked_prefill(key, value, decoder_segment_ids, previous_chunk), None\n      else:\n        return self.kv_cache_prefill(key, value, decoder_segment_ids), None\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      return self.kv_cache_autoregressive(key, value, use_ragged_attention)\n    else:\n      raise ValueError(f\"Model Mode isn't supported! {model_mode=}\")",
        "analysis": {
            "module_type": "kv_cache",
            "purpose": "Manages the Key-Value (KV) cache for transformer models, storing and retrieving past key and value states to optimize inference.",
            "input": {
                "shape": "N/A (parameters are passed during initialization)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialization of prefill and autoregressive caches based on model mode.",
                "Updating the cache during prefill or autoregressive steps.",
                "Retrieving cached values for attention computations."
            ],
            "output": {
                "shape": "N/A (output depends on the specific method called, e.g., __call__ returns cached states)",
                "dtype": "N/A"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.nnx",
                "flax.linen"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum sequence length for the target generation.",
                "batch": "The batch size.",
                "key_seq_len": "The sequence length for keys.",
                "value_seq_len": "The sequence length for values.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "key_head_size": "The size of each key head.",
                "value_head_size": "The size of each value head.",
                "dtype": "The data type for the cache.",
                "kv_quant": "Optional KV quantization configuration.",
                "prefill_cache_logical_axis_names": "Logical axis names for the prefill cache.",
                "cache_logical_axis_names": "Logical axis names for the autoregressive cache.",
                "cache_scale_logical_axis_names": "Logical axis names for the cache scale.",
                "prefill_cache_axis_order": "Axis order for the prefill cache.",
                "ar_cache_axis_order": "Axis order for the autoregressive cache.",
                "key_axis_order": "Axis order for the key.",
                "use_chunked_prefill": "Flag to enable chunked prefill.",
                "model_mode": "The current operating mode of the model (e.g., prefill, autoregressive)."
            },
            "notes": [
                "The class supports both prefill and autoregressive modes.",
                "It handles optional KV quantization.",
                "Axis ordering and logical axis names are configurable for distributed training.",
                "The `__call__` method orchestrates the caching logic based on `model_mode`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVCache module with configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores initialization parameters.",
                        "Initializes prefill and autoregressive caches if model_mode is PREFILL or AUTOREGRESSIVE."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "prefill_key_vars": {
                    "purpose": "Returns the cached prefill key and its scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns a tuple containing the cached prefill key and its scale."
                    ],
                    "output": {
                        "shape": "tuple[nnx.Cache, nnx.Cache | None]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "prefill_value_vars": {
                    "purpose": "Returns the cached prefill value and its scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns a tuple containing the cached prefill value and its scale."
                    ],
                    "output": {
                        "shape": "tuple[nnx.Cache, nnx.Cache | None]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "ar_key_vars": {
                    "purpose": "Returns the autoregressive cached key and its scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns a tuple containing the autoregressive cached key and its scale."
                    ],
                    "output": {
                        "shape": "tuple[nnx.Cache, nnx.Cache | None]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "ar_value_vars": {
                    "purpose": "Returns the autoregressive cached value and its scale.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns a tuple containing the autoregressive cached value and its scale."
                    ],
                    "output": {
                        "shape": "tuple[nnx.Cache, nnx.Cache | None]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "_get_cached_kv_dtype": {
                    "purpose": "Determines the data type for the cached KV tensors, considering quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if KV quantization is enabled and returns its dtype, otherwise returns the module's dtype."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "_get_cache_scale_logical_shape": {
                    "purpose": "Calculates the logical shape for the cache scale based on quantization configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Asserts that KV quantization is enabled.",
                        "Determines the shape based on `kv_quant.axis_cfg` ('dkv' or 'heads_and_dkv')."
                    ],
                    "output": {
                        "shape": "tuple[int, int, int, int]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "_initialize_prefill_caches": {
                    "purpose": "Initializes the cache variables for the prefill phase.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determines cache length and dtype.",
                        "Sets appropriate logical axis names based on `model_mode`.",
                        "Calculates and transposes cache shapes for keys, values, and segment IDs.",
                        "Creates `nnx.Cache` instances for `cached_prefill_key`, `cached_prefill_value`, and `cache_prefill_segment_id`.",
                        "Initializes scale caches if `kv_quant` is enabled."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp.zeros",
                        "nnx.Cache",
                        "transpose_tuple"
                    ],
                    "notes": []
                },
                "_get_prefill_cache_vars": {
                    "purpose": "Retrieves the cache variables for the prefill phase.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns a tuple containing prefill key variables, prefill value variables, and prefill segment ID cache."
                    ],
                    "output": {
                        "shape": "tuple[tuple[nnx.Cache, nnx.Cache | None], tuple[nnx.Cache, nnx.Cache | None], nnx.Cache]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "_initialize_ar_cache_vars": {
                    "purpose": "Initializes the cache variables for the autoregressive (AR) phase.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates `max_target_length` against `max_prefill_length`.",
                        "Calculates AR cache length.",
                        "Determines logical axis names and transposes shapes for AR keys, values, segment IDs, and lengths.",
                        "Creates `nnx.Cache` instances for `cached_ar_key`, `cached_ar_value`, `cache_ar_segment_id`, `cached_ar_lengths`, and `cache_ar_index`.",
                        "Applies logical constraints to AR key and value caches.",
                        "Initializes AR scale caches if `kv_quant` is enabled."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp.zeros",
                        "nnx.Cache",
                        "nn.with_logical_constraint",
                        "transpose_tuple"
                    ],
                    "notes": []
                },
                "kv_cache_chunked_prefill": {
                    "purpose": "Updates the KV cache during chunked prefill and returns the needed cache length.",
                    "input": {
                        "shape": "[b, s, n, d] for key and value, [b, s] for decoder_segment_ids, optional [b, s] for previous_chunk",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Asserts that KV quantization is not used.",
                        "Calculates `next_pos` based on `previous_chunk`.",
                        "Transposes input key and value to cache format.",
                        "Retrieves existing cached values.",
                        "Updates the `cached_prefill_key` and `cached_prefill_value` using `dynamic_update_slice_in_dim`.",
                        "Updates `cached_prefill_segment_id` if `decoder_segment_ids` are provided.",
                        "Extracts and returns the necessary portion of the updated cache."
                    ],
                    "output": {
                        "shape": "tuple of (key, value, decoder_segment_id) for the needed cache portion",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "self.get_cached_values",
                        "jax.lax.dynamic_update_slice_in_dim",
                        "jax.lax.dynamic_slice_in_dim"
                    ],
                    "notes": [
                        "Assumes `kv_quant` is not used in this method.",
                        "The cache is updated in place."
                    ]
                },
                "kv_cache_prefill": {
                    "purpose": "Updates the KV cache during the prefill phase, zeroing out existing cache.",
                    "input": {
                        "shape": "[b, s, n, d] for key and value, [b, s] for decoder_segment_ids",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Asserts key and value dtypes match.",
                        "Retrieves prefill cache variables.",
                        "Transposes input key and value to cache format.",
                        "Quantizes key and value if `kv_quant` is enabled and updates scale caches.",
                        "Updates `cached_prefill_key` and `cached_prefill_value` with the transposed inputs.",
                        "Updates `cached_prefill_segment_id` if `decoder_segment_ids` are provided.",
                        "Returns the original key, value, and decoder_segment_ids."
                    ],
                    "output": {
                        "shape": "tuple[Array, Array, Array] (original key, value, decoder_segment_ids)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "self.kv_quant.quantize"
                    ],
                    "notes": [
                        "This method effectively overwrites the existing prefill cache."
                    ]
                },
                "update_ar_key_value": {
                    "purpose": "Adds a single token's key and value to the autoregressive (AR) KV cache.",
                    "input": {
                        "shape": "key and value: [b, 1, n, d], one_hot_indices: [b, 1], lengths: [b]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Transposes input key and value to AR cache format.",
                        "Quantizes key and value if `kv_quant` is enabled.",
                        "Determines the AR cache update index and sequence axis.",
                        "If `use_ragged_attention` is True, uses `jax.lax.fori_loop` to update the cache for each batch element.",
                        "If `use_ragged_attention` is False, uses `jax.lax.dynamic_update_index_in_dim` to update the cache.",
                        "Applies logical constraints to the updated AR key and value caches.",
                        "Updates AR scale caches if `kv_quant` is enabled."
                    ],
                    "output": {
                        "shape": "None (updates caches in-place)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "self.kv_quant.quantize",
                        "jax.lax.fori_loop",
                        "jax.lax.dynamic_update_index_in_dim",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "Handles both standard and ragged attention updates.",
                        "Reshapes/repeats cache if batch sizes mismatch during decoding."
                    ]
                },
                "get_cached_values": {
                    "purpose": "Retrieves and potentially dequantizes cached key/value tensors.",
                    "input": {
                        "shape": "cache_vars: tuple of (nnx.Cache, nnx.Cache | None), target_dtype: DType, cache_axis_order: AxisIdxes",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extracts cache value and scale.",
                        "If scale exists, applies dequantization based on the cache value's dtype.",
                        "Constructs a `KVTensor` if quantization was used.",
                        "Reverses the transpose operation to return the cache value in its logical shape."
                    ],
                    "output": {
                        "shape": "jax.Array or KVTensor",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "KVTensor",
                        "reverse_transpose"
                    ],
                    "notes": []
                },
                "kv_cache_autoregressive": {
                    "purpose": "Updates the AR KV cache and returns both prefill and AR cached states.",
                    "input": {
                        "shape": "key and value: [b, 1, n, d]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates that the sequence length of key/value is 1.",
                        "Retrieves AR cache variables.",
                        "Calls `update_ar_key_value` to add the current token's key/value to the cache.",
                        "Updates `cached_ar_segment_id` and `cache_ar_index`.",
                        "Increments `cached_ar_lengths`.",
                        "Retrieves and dequantizes prefill and AR cache states using `get_cached_values`.",
                        "Returns tuples containing the prefill and AR cached states."
                    ],
                    "output": {
                        "shape": "tuple[tuple[Array, Array, Array], tuple[Array, Array, Array, Array]]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self.update_ar_key_value",
                        "self.get_cached_values",
                        "jax.lax.dynamic_update_index_in_dim"
                    ],
                    "notes": [
                        "This method is called during the autoregressive generation phase."
                    ]
                },
                "__call__": {
                    "purpose": "Orchestrates the KV caching process based on the model mode.",
                    "input": {
                        "shape": "key and value: [b, s, n_kv, d], decoder_segment_ids: [b, s]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks the `model_mode`.",
                        "If `MODEL_MODE_PREFILL`:",
                        "  Calls `kv_cache_chunked_prefill` if `use_chunked_prefill` is True.",
                        "  Otherwise, calls `kv_cache_prefill`.",
                        "If `MODEL_MODE_AUTOREGRESSIVE`:",
                        "  Calls `kv_cache_autoregressive`.",
                        "Raises a ValueError for unsupported `model_mode`."
                    ],
                    "output": {
                        "shape": "tuple containing cached states, structure depends on model_mode",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "self.kv_cache_chunked_prefill",
                        "self.kv_cache_prefill",
                        "self.kv_cache_autoregressive"
                    ],
                    "notes": [
                        "This is the main entry point for using the KVCache module."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#mla_kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def mla_kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    key_heads: int = 1,\n    value_heads: int = 1,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the MlaKVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `MlaKVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MlaKVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      kv_quant=kv_quant,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "mla_kv_cache_as_linen",
            "purpose": "Initializes and returns a Flax Linen module that wraps an NNX MlaKVCache module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert an NNX module to a Linen module.",
                "Passes the `MlaKVCache` class and initialization arguments to `to_linen`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlaKVCache"
            ],
            "parameters": {
                "max_prefill_length": "The maximum prefill length.",
                "max_target_length": "The maximum target length.",
                "batch": "The batch size.",
                "key_seq_len": "The key sequence length.",
                "value_seq_len": "The value sequence length.",
                "key_head_size": "The key head size.",
                "value_head_size": "The value head size.",
                "dtype": "The data type.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "kv_quant": "The KVQuant configuration.",
                "prefill_cache_axis_order": "The axis order for the prefill cache.",
                "ar_cache_axis_order": "The axis order for the autoregressive cache.",
                "use_chunked_prefill": "Whether to use chunked prefill.",
                "model_mode": "The model mode.",
                "name": "The name of the Linen module."
            },
            "notes": [
                "This function acts as a factory to create a Linen-compatible KVCache module from an NNX KVCache module.",
                "It configures the KVCache with various parameters related to sequence lengths, batch size, head dimensions, and quantization."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#MlaKVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class MlaKVCache(KVCache):\n  \"\"\"Implementation of the KVCache for MLA.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len,\n      # key_head_size, value_head_size, key_heads, and value_heads from\n      # key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      key_heads: int = 1,\n      value_heads: int = 1,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in MlaKVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the MlaKVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill\n        cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache\n        scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      use_chunked_prefill: Whether to use chunked prefill.\n      model_mode: The model mode.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    super().__init__(\n        max_prefill_length=max_prefill_length,\n        max_target_length=max_target_length,\n        batch=batch,\n        key_seq_len=key_seq_len,\n        value_seq_len=value_seq_len,\n        key_heads=key_heads,\n        value_heads=value_heads,\n        key_head_size=key_head_size,\n        value_head_size=value_head_size,\n        dtype=dtype,\n        kv_quant=kv_quant,\n        prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n        cache_logical_axis_names=cache_logical_axis_names,\n        cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        key_axis_order=key_axis_order,\n        use_chunked_prefill=use_chunked_prefill,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def key_latent_add_head_dim(self, key_latent: Array):\n    b, l, hz = key_latent.shape\n    return key_latent.reshape(b, l, 1, hz)\n\n  def key_latent_remove_head_dim(self, key_latent: Array):\n    b, l, _, hz = key_latent.shape\n    return key_latent.reshape(b, l, hz)\n\n  def __call__(\n      self,\n      key_latent: Array,\n      key_rope: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple[\n      None | tuple[Array, Array, Array],\n      None | tuple[Array, Array, Array, Array],\n  ]:\n    assert model_mode != MODEL_MODE_TRAIN, \"incorrectly updating kvcache in train mode.\"\n    assert self.kv_quant is None, \"kvcache quantization not supported with mla.\"\n    key_latent = self.key_latent_add_head_dim(key_latent)\n    prefill_cache, ar_cache = super().__call__(key_latent, key_rope, decoder_segment_ids, model_mode)\n    if prefill_cache:\n      key_latent, key_rope, decoder_segments_ids = prefill_cache\n      prefill_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n      )\n    if ar_cache:\n      key_latent, key_rope, decoder_segments_ids, lengths = ar_cache\n      ar_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n          lengths,\n      )\n    return prefill_cache, ar_cache",
        "analysis": {
            "functionality": "Implements a Key-Value (KV) cache specifically for the MLA (Multi-Layer Attention) architecture, inheriting from a base KVCache class.",
            "usage": "Instantiate `MlaKVCache` with parameters like `max_prefill_length`, `max_target_length`, `batch`, `key_seq_len`, `value_seq_len`, `key_head_size`, `value_head_size`, `dtype`, and optional parameters for heads, quantization, and axis configurations. The `__call__` method is used to update and retrieve the KV cache during model inference, taking `key_latent`, `key_rope`, `decoder_segment_ids`, and `model_mode` as input. It handles reshaping of key dimensions and asserts that quantization is not supported for MLA KV caches."
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InputData",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InputData:\n  \"\"\"Container for input data and metadata.\n\n  Attributes:\n      id: Unique identifier for this input\n      tokens: JAX array containing the tokenized input\n      true_length: Actual length of the input before padding\n  \"\"\"\n\n  id: str\n  tokens: jax.Array | np.ndarray\n  true_length: int",
        "analysis": {
            "module_type": "input_data_container",
            "purpose": "A container for input data and associated metadata.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.Array",
                "np.ndarray"
            ],
            "parameters": {
                "id": "Unique identifier for this input.",
                "tokens": "JAX array containing the tokenized input.",
                "true_length": "Actual length of the input before padding."
            },
            "notes": [
                "This is a dataclass used to structure input data for processing.",
                "It holds the input's ID, its tokenized representation, and its original length."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#CompletionOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class CompletionOutput:\n  \"\"\"Container for model generation output.\n\n  Attributes:\n      index: The index of the output in the request.\n      token_ids: The token IDs of the prompt and generated output text.\n      logprobs: The log probabilities of the prompt and generated output tokens.\n      prompt_length: The number of prompt tokens.\n  \"\"\"\n\n  index: str\n  token_ids: np.ndarray\n  logprobs: np.ndarray\n  prompt_length: int",
        "analysis": {
            "module_type": "completion_output",
            "purpose": "A container to hold the output of a model generation task.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy"
            ],
            "parameters": {
                "index": "The index of the output in the request.",
                "token_ids": "The token IDs of the prompt and generated output text.",
                "logprobs": "The log probabilities of the prompt and generated output tokens.",
                "prompt_length": "The number of prompt tokens."
            },
            "notes": [
                "This is a dataclass used for structuring the results of text generation.",
                "Attributes are type-hinted but not explicitly initialized in the provided snippet."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#TokenOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class TokenOutput:\n  \"\"\"Container for individual token generation result.\"\"\"\n\n  token: np.ndarray\n  log_prob: np.ndarray",
        "analysis": {
            "module_type": "token_output",
            "purpose": "A container for storing the generated token and its associated log probability.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy"
            ],
            "parameters": {},
            "notes": [
                "This is a dataclass used to structure the output of token generation, holding both the token ID and its log probability."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#DetokenizationTask",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class DetokenizationTask:\n  \"\"\"Container for detokenization work to be done on background thread.\"\"\"\n\n  task_type: str  # \"prefill\" or \"decode\"\n  # For prefill tasks\n  result_tokens: Any = None\n  log_prob: Any = None\n  prompt_logp: Any = None\n  prompt_ids: list = None\n  slots: list = None\n  # For decode tasks\n  tokens_buffer: list = None\n  logprob_buffer: list = None",
        "analysis": {
            "module_type": "detokenization_task",
            "purpose": "A container for detokenization work to be done on a background thread.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Stores task type ('prefill' or 'decode').",
                "Stores results for prefill tasks (result_tokens, log_prob, prompt_logp, prompt_ids, slots).",
                "Stores buffers for decode tasks (tokens_buffer, logprob_buffer)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "task_type": "Specifies whether the task is for 'prefill' or 'decode' operations.",
                "result_tokens": "Tokens generated during prefill.",
                "log_prob": "Log probabilities associated with prefill tokens.",
                "prompt_logp": "Log probabilities of the prompt tokens.",
                "prompt_ids": "List of prompt identifiers for the prefill task.",
                "slots": "List of decode slots associated with the prefill task.",
                "tokens_buffer": "Buffer holding tokens generated during a decode step.",
                "logprob_buffer": "Buffer holding log probabilities for tokens generated during a decode step."
            },
            "notes": [
                "This class is a data structure to hold information related to detokenization tasks.",
                "It differentiates between 'prefill' and 'decode' tasks and stores relevant data for each."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#SafeThread",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class SafeThread(threading.Thread):\n  \"\"\"Thread class with exception handling to prevent silent failures.\"\"\"\n\n  def run(self):\n    try:\n      super().run()\n    except Exception as _:  # pylint: disable=broad-exception-caught\n      traceback.print_exc()\n      # Kill the process if a thread encounters an error\n      os.kill(os.getpid(), signal.SIGKILL)",
        "analysis": {
            "module_type": "safe_thread",
            "purpose": "A thread class that enhances standard threading by adding exception handling to prevent silent failures.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Execute the thread's target function using super().run().",
                "Catch any Exception that occurs during execution.",
                "Print the traceback of the caught exception.",
                "Terminate the entire process using os.kill() with SIGKILL signal."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "threading.Thread",
                "traceback",
                "os",
                "signal"
            ],
            "parameters": {},
            "notes": [
                "This class is designed to ensure that any unhandled exception within a thread leads to the termination of the entire program, making debugging easier by preventing silent thread failures.",
                "The `pylint: disable=broad-exception-caught` comment suggests that catching all exceptions is a deliberate design choice for this specific error handling mechanism."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillType",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillType(Enum):\n  \"\"\"Enumeration of supported prefill processing methods.\"\"\"\n\n  DEFAULT = \"default\"\n  BATCH = \"batch\"",
        "analysis": {
            "module_type": "prefill_type_enum",
            "purpose": "Enumerates the supported methods for prefill processing.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines enum members for different prefill processing strategies."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "The 'DEFAULT' option likely refers to processing one sequence at a time.",
                "The 'BATCH' option likely refers to packing multiple sequences for more efficient processing."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillResult",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillResult:\n  \"\"\"Result from prefill processing operation.\"\"\"\n\n  result_tokens: \"jetstream.engine_api.ResultTokens\"\n  slot: int\n  prompt_logp: None | jax.Array",
        "analysis": {
            "module_type": "prefill_result",
            "purpose": "Stores the result of a prefill processing operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jetstream.engine_api.ResultTokens"
            ],
            "parameters": {},
            "notes": [
                "This is a dataclass used to hold the output of a prefill operation, including the generated tokens, the slot index, and the prompt log probability."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillHelper",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillHelper:\n  \"\"\"Abstraction layer for different prefill processing strategies.\n\n  Provides a unified interface for both default (single-sequence) and batch\n  (packed multi-sequence) prefill processing methods.\n  \"\"\"\n\n  def __init__(\n      self,\n      prefill_type: PrefillType,\n      engine: MaxEngine,\n      prefill_lengths: list[int],\n      batch_prefill_max_batch_size: int = 16,\n      rng=None,\n  ):\n    \"\"\"Initialize the PrefillHelper.\n\n    Args:\n        type: The type of prefill processor to use (\"default\" or \"batch\")\n        engine: The MaxEngine instance to use for prefill operations\n        prefill_lengths: list of prompt lengths to support\n        batch_prefill_max_batch_size: Maximum number of prompts in one packed\n            sequence for batch prefill\n    \"\"\"\n    self._type = prefill_type\n    self.engine = engine\n    self.prefill_lengths = sorted(prefill_lengths)\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    if prefill_type == PrefillType.DEFAULT:\n      self._processor = PrefillProcessor(engine)\n    elif prefill_type == PrefillType.BATCH:\n      self._batch_processor = BatchedPrefillProcessor(\n          engine=engine,\n          max_batch_size=batch_prefill_max_batch_size,\n          auto_layout_supported=False,\n      )\n      # Keep fallback processor for edge cases\n      self._processor = PrefillProcessor(engine)\n    else:\n      raise ValueError(f\"Invalid prefill type: {prefill_type}\")\n\n  @functools.partial(jax.jit, static_argnums=(0), donate_argnames=(\"decode_state\",))\n  def _jitted_single_prefill(\n      self, params, tokens, slot, true_length, decode_state, rng\n  ) -> tuple[jax.Array, jax.Array, DecodeState, jax.Array] | tuple[jax.Array, jax.Array, DecodeState]:\n    \"\"\"Prefill a single input.\"\"\"\n    # pylint: disable=protected-access\n    first_token, decode_state = self._processor._process(\n        params,\n        tokens,\n        slot,\n        true_length,\n        decode_state,\n        rng,\n        return_prompt_logp=True,\n    )\n\n    return (\n        first_token,\n        decode_state,\n        decode_state[\"prompt_logp\"],\n    )\n\n  def process(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      decode_slot: int,\n      input_id: int,\n      input_tokens_padded: jax.Array,\n      input_true_length: int,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Process an input through the appropriate prefill processor.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decode state\n        decode_slot: The decode slot index to use for this input\n        input_id: Unique identifier for this input\n        input_tokens_padded: Padded token array for the input\n        input_true_length: Actual length of the input before padding\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    padded_length = len(input_tokens_padded)\n    # Use default processor if configured or if input is already at max length\n    if self._type == PrefillType.DEFAULT or padded_length == self.max_prefill_length:\n      first_token, decode_state, prompt_logp = self._jitted_single_prefill(\n          model_params,\n          input_tokens_padded,\n          decode_slot,\n          input_true_length,\n          decode_state,\n          self.rng,\n      )\n      prefill_done(\n          [PrefillResult(first_token, decode_slot, prompt_logp)],\n          [input_id],\n          decode_state,\n      )\n    # Use batch processor for inputs that can benefit from prefill packing\n    elif self._type == PrefillType.BATCH:\n      self._batch_processor.process(\n          model_params,\n          decode_state,\n          decode_slot,\n          input_id,\n          input_tokens_padded[:input_true_length],\n          padded_length,\n          self.max_prefill_length,\n          prefill_done,\n          return_prompt_logp=True,\n      )\n\n  def finalize(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Finalize prefill operations, flushing any pending inputs.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decoder state\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    if self._type == PrefillType.DEFAULT:\n      # No finalization needed for default processor\n      pass\n    elif self._type == PrefillType.BATCH:\n      # Flush any remaining inputs in the batch processor\n      self._batch_processor.flush(model_params, decode_state, prefill_done, return_prompt_logp=True)",
        "analysis": {
            "module_type": "prefill_helper",
            "purpose": "Manages different prefill processing strategies, supporting both single-sequence and batch (packed multi-sequence) methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialization with prefill type, engine, and lengths.",
                "Selection of appropriate prefill processor (default or batch).",
                "Jitting of the single prefill operation.",
                "Processing of input tokens using the selected processor.",
                "Finalization to flush any pending batch prefill operations."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "PrefillType",
                "MaxEngine",
                "PrefillProcessor",
                "BatchedPrefillProcessor",
                "jax",
                "functools"
            ],
            "parameters": {
                "prefill_type": "The type of prefill processor to use ('default' or 'batch').",
                "engine": "The MaxEngine instance to use for prefill operations.",
                "prefill_lengths": "List of prompt lengths to support.",
                "batch_prefill_max_batch_size": "Maximum number of prompts in one packed sequence for batch prefill."
            },
            "notes": [
                "The class provides a unified interface for prefill operations.",
                "It handles the logic for choosing between default and batch prefill based on configuration and input characteristics.",
                "The `_jitted_single_prefill` method is JIT-compiled for performance."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PrefillHelper with the specified prefill strategy and engine.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores prefill type, engine, and prefill lengths.",
                        "Determines the maximum prefill length.",
                        "Initializes the appropriate prefill processor (_processor or _batch_processor) based on prefill_type."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillType",
                        "MaxEngine",
                        "PrefillProcessor",
                        "BatchedPrefillProcessor",
                        "jax.random.PRNGKey"
                    ],
                    "notes": [
                        "If prefill_type is BATCH, both _batch_processor and a fallback _processor are initialized.",
                        "Raises ValueError for invalid prefill_type."
                    ]
                },
                "_jitted_single_prefill": {
                    "purpose": "JIT-compiled function to perform prefill on a single input sequence.",
                    "input": {
                        "shape": "[batch_size, sequence_length], slot_index, true_length, decode_state, rng",
                        "dtype": "jax.Array, int, int, Any, jax.random.PRNGKey"
                    },
                    "processing_steps": [
                        "Calls the internal _process method of the selected prefill processor.",
                        "Returns the first generated token, updated decode state, and prompt log probabilities."
                    ],
                    "output": {
                        "shape": "tuple[jax.Array, DecodeState, jax.Array] or tuple[jax.Array, DecodeState]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "PrefillProcessor._process",
                        "jax.jit",
                        "functools.partial"
                    ],
                    "notes": [
                        "Decorated with jax.jit for compilation.",
                        "Uses `donate_argnames` for efficient memory management of decode_state."
                    ]
                },
                "process": {
                    "purpose": "Processes an input through the appropriate prefill processor based on the helper's configuration.",
                    "input": {
                        "shape": "model_params, decode_state, decode_slot, input_id, input_tokens_padded, input_true_length, prefill_done",
                        "dtype": "Params, DecodeState, int, int, jax.Array, int, Callable"
                    },
                    "processing_steps": [
                        "Determines if default prefill or batch prefill should be used.",
                        "If default or input is at max length, calls `_jitted_single_prefill`.",
                        "If batch prefill is enabled and applicable, calls `_batch_processor.process`.",
                        "Invokes the `prefill_done` callback with results."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "None"
                    },
                    "dependencies": [
                        "_jitted_single_prefill",
                        "PrefillProcessor",
                        "BatchedPrefillProcessor.process",
                        "PrefillType",
                        "PrefillResult"
                    ],
                    "notes": [
                        "Handles padding and potential truncation of input tokens.",
                        "The `prefill_done` callback is crucial for passing results to the next stage."
                    ]
                },
                "finalize": {
                    "purpose": "Finalizes any pending prefill operations, particularly for batch prefill.",
                    "input": {
                        "shape": "model_params, decode_state, prefill_done",
                        "dtype": "Params, DecodeState, Callable"
                    },
                    "processing_steps": [
                        "If the prefill type is BATCH, calls the `flush` method of the `_batch_processor`.",
                        "If the prefill type is DEFAULT, does nothing."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "None"
                    },
                    "dependencies": [
                        "PrefillType",
                        "BatchedPrefillProcessor.flush"
                    ],
                    "notes": [
                        "Ensures that all batched prefill requests are processed before the inference run concludes."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InferenceWorker",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InferenceWorker:\n  \"\"\"\n  InferenceWorker runs continuous batching over\n  a queue of inputs.\n\n    Continuous batching workflow:\n    1. Process inputs one at a time from queue\n    2. Prefill input and insert into KV cache\n    3. Continue prefilling until enough samples for batch decode\n    4. Decode until at least one sequence completes\n    5. Refill newly available decode slots with prefill\n    6. Repeat until all sequences complete\n\n    Prefill Packing:\n        When enable_batch_prefill is True, the prefill processor\n        will pack multiple inputs into a single sequence before\n        doing the prefill.\n\n        There are multiple buckets for packed sequences, where each bucket\n        contains inputs with the same padded length. Only inputs with the same\n        padded length can be packed together.\n\n        It is important to sort the inputs by padded length so that the\n        buckets fill up quickly.\n\n        When a decode slot frees up, the prefill processor will add the\n        sequence to a bucket. If the bucket becomes full, the packed sequence\n        will be prefilled.\n\n        E.g.\n        Bucket for length 64: [...seq1, ...seq2, ...seq3, ...seq4]\n        Bucket for length 128: [...seq1, ...seq2]\n        Bucket for length 256: [...seq1]\n\n  \"\"\"\n\n  def __init__(\n      self,\n      config: MaxTextConfig,\n      params: Params | None,\n      min_decode_steps: int,\n      enable_batch_prefill: bool,\n      devices: list[Any],\n      tokenizer: Any,\n      eos_ids: list[int],\n      prefill_lengths: list[int],\n      max_decode_length: int,\n      batch_prefill_max_batch_size: int,\n      is_pw_reshard: bool = True,\n      rng: jax.random.PRNGKey = None,\n      mesh: Mesh = None,\n      debug: bool = False,\n  ):\n    \"\"\"\n    Args:\n        config: MaxText configuration\n        params: Model parameters, if None, the params will be loaded from the config\n        min_decode_steps: Minimum number of decode steps to run at once\n        enable_batch_prefill: Whether to enable batch prefill\n        devices: JAX devices to use for this worker\n        tokenizer: Tokenizer to use\n        eos_ids: End-of-sequence token IDs\n        prefill_lengths: list of supported prefill lengths\n        max_decode_length: Maximum tokens to generate per sequence\n        batch_prefill_max_batch_size: Maximum batch size for batch prefill\n        run_as_a_thread: Whether to run in a separate thread\n        rng: Random number generator key\n        mesh: JAX mesh for distributed computation\n        is_pw_reshard: Whether to use Pathways for resharding\n    \"\"\"\n    # Configurations\n    self.config = config\n    self.params = params\n    self.devices = devices\n    self.is_pw_reshard = is_pw_reshard\n    self.enable_batch_prefill = enable_batch_prefill\n    self.prefill_type = PrefillType.BATCH if enable_batch_prefill else PrefillType.DEFAULT\n    self.prefill_lengths = prefill_lengths\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.max_decode_length = max_decode_length\n    self.eos_ids = eos_ids\n    self.tokenizer = tokenizer\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.min_decode_steps = min_decode_steps\n    self.mesh = mesh\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n\n    # Inference state (initialized later)\n    self.running = False\n    self.detokenization_queue = queue.Queue()\n    self.empty_decode_slots = set()\n    self.slot_to_id: dict[int, None | int] = {}\n    self.completed_sequences: set = set()\n\n    self.decode_state: DecodeState = None\n    self.completion_tokens_by_id: dict[Hashable, list[TokenOutput]] = {}\n    self.prompt_logprobs_by_id: dict[Hashable, list[np.ndarray]] = {}\n    self.true_lengths: dict[Hashable, int] = {}\n\n    # Model components (initialized later)\n    self.engine = None\n    self.decode_batch_size = None\n    self.prefill_helper = None\n    self.generate_fn = None\n\n    start_time = time.time()\n    # Initialize MaxEngine(s)\n    self.params, self.engine = self._init_engine(self.params)\n    self.tokenizer = self._init_tokenizer()\n    self.decode_batch_size = self.engine.max_concurrent_decodes\n    # Initialize prefill helper\n    self.prefill_helper = PrefillHelper(\n        self.prefill_type,\n        self.engine,\n        self.prefill_lengths,\n        self.batch_prefill_max_batch_size,\n        rng=self.rng,\n    )\n    # Initialize decode state\n    start_time_decode_state = time.time()\n    self.generate_fn = self.engine.generate\n    self.decode_state = self.engine.init_decode_state(self.rng)\n\n    if self.debug:\n      max_logging.log(f\"time taken to initialize decode_state: {time.time() - start_time_decode_state} seconds\")\n    max_logging.log(f\"Initialized Inference worker in {time.time() - start_time} seconds\")\n\n  def _init_engine(self, params):\n    \"\"\"Initialize the MaxEngine.\n\n    Args:\n        params: Model parameters\n\n    Returns:\n        tuple of (params, engine)\n    \"\"\"\n    start_time = time.time()\n    engine = MaxEngine(self.config, self.devices)\n    params = engine.load_params(params=params, rng=self.rng)\n    max_logging.log(f\"Time taken to initialize engine: {time.time() - start_time} seconds\")\n    return params, engine\n\n  def _init_tokenizer(self):\n    \"\"\"Initialize the tokenizer.\n\n    Returns:\n        Initialized tokenizer\n    \"\"\"\n    if self.eos_ids is None and self.tokenizer is None:\n      tokenizer_params = self.engine.get_tokenizer()\n      self.tokenizer = self.engine.build_tokenizer(tokenizer_params)\n    if self.eos_ids is None:\n      self.eos_ids = [self.tokenizer.eos_id]\n    return self.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n  ):\n    \"\"\"Update the model parameters\"\"\"\n    self.params = params\n\n  def reset_state(self):\n    \"\"\"Reset all worker state for a new inference run.\n\n    This allows reusing the same InferenceWorker instance across multiple\n    batch_inference calls without recreating the expensive engine components.\n    \"\"\"\n    max_logging.log(\"Resetting InferenceWorker state\")\n\n    # Reset inference state\n    self.running = False\n    self.completion_tokens_by_id = defaultdict(list)\n    self.prompt_logprobs_by_id = defaultdict(list)\n    self.empty_decode_slots = set()\n    for i in range(self.decode_batch_size):\n      self.empty_decode_slots.add(i)\n    self.slot_to_id = {}\n    self.true_lengths = {}\n    self.detokenization_queue = queue.Queue()\n    self.completed_sequences = set()\n\n    max_logging.log(\"InferenceWorker state reset complete\")\n\n  def run_inference(self, data: list[InputData], rng=None):\n    \"\"\"Start the inference process.\n\n    Args:\n        data: list of InputData objects containing input sequences\n        rng: Random number generator key. If None, the previous key will be used.\n    \"\"\"\n\n    # Reset state for new inference run\n    self.reset_state()\n\n    # Reset rng\n    if rng is not None:\n      self.rng = rng\n\n    # Set up state for this inference run\n    self.true_lengths = {input.id: input.true_length for input in data}\n    self.running = True\n\n    max_logging.log(\"Continuous batching started\")\n\n    self._run_continuous_batching(data)\n\n    return self._build_final_outputs(data)\n\n  def _run_continuous_batching(\n      self,\n      data: list[InputData],\n  ):\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects containing input sequences\n    \"\"\"\n\n    # Start detokenization thread\n    detokenization_thread = SafeThread(\n        target=self.background_detokenization,\n        name=\"detokenization\",\n    )\n    detokenization_thread.start()\n\n    # Process each input\n    for row in data:\n      # 1. Wait for an empty slot\n      while not self.empty_decode_slots:\n        self.decode()\n      # 2. Get an available slot\n      slot = self.empty_decode_slots.pop()\n      # 3. Prefill and insert kv cache\n      self.prefill_helper.process(\n          model_params=self.params,\n          decode_state=self.decode_state,\n          decode_slot=slot,\n          input_id=row.id,\n          input_tokens_padded=row.tokens,\n          input_true_length=row.true_length,\n          prefill_done=self.prefill_done,\n      )\n\n    # 4. Flush any pending inputs in batch prefill mode\n    self.prefill_helper.finalize(self.params, self.decode_state, self.prefill_done)\n\n    # 5. Continue decoding until all sequences are complete\n    while not all(value is None for value in self.slot_to_id.values()):\n      self.decode()\n\n    # Wait for detokenization to complete\n    self.running = False\n    max_logging.log(\"Inference worker: joining detokenization thread\")\n    start_time = time.time()\n\n    with jax.profiler.TraceAnnotation(\"Flushing detokenization thread\"):\n      detokenization_thread.join()\n\n    max_logging.log(f\"Inference worker: detokenization thread joined in {time.time() - start_time} seconds\")\n\n  def _build_final_outputs(self, input_data: list[InputData]) -> list[CompletionOutput]:\n    \"\"\"Build the final list of CompletionOutput.\"\"\"\n\n    with jax.profiler.TraceAnnotation(\"offline_engine.batch_inference.return_final_output\"):\n      completion_outputs = []\n      for row in input_data:\n        input_id = row.id\n        prompt_length = row.true_length\n        prompt_tokens = row.tokens[: row.true_length].squeeze()\n        completion_tokens = np.array(\n            [token_output.token for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        logprobs = np.array(\n            [token_output.log_prob.flatten() for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        prompt_logprobs = np.array(self.prompt_logprobs_by_id[input_id]).flatten()\n        completion_outputs.append(\n            CompletionOutput(\n                index=str(input_id),\n                prompt_length=prompt_length,\n                token_ids=np.concatenate(\n                    (\n                        prompt_tokens,\n                        completion_tokens,\n                    )\n                ),\n                logprobs=np.concatenate(\n                    (\n                        prompt_logprobs,\n                        logprobs,\n                    )\n                ),\n            )\n        )\n    return completion_outputs\n\n  def prefill_done(self, prefill_result: list[PrefillResult], prompt_ids: list[int], decode_state: DecodeState):\n    \"\"\"Callback function called when prefill completes.\n    This function queues the prefill data for background processing.\n\n    Args:\n        prefill_result: list of (token, slot) tuples\n        prompt_ids: list of prompt IDs\n        decode_state: Updated decode state\n    \"\"\"\n    # Update decode state\n    self.decode_state = decode_state\n    # Process each prefill result\n    slots = []\n    result_tokens_list = []\n    prompt_logp_list = []\n    for i, result in enumerate(prefill_result):\n      input_id = prompt_ids[i]\n      slot = result.slot\n      self.slot_to_id[slot] = input_id\n      slots.append(slot)\n      result_tokens_list.append(result.result_tokens)\n      prompt_logp_list.append(result.prompt_logp)\n\n    # Queue detokenization task\n    task = DetokenizationTask(\n        task_type=\"prefill\",\n        result_tokens=result_tokens_list,\n        prompt_logp=prompt_logp_list,\n        prompt_ids=prompt_ids,\n        slots=slots,\n    )\n    self.detokenization_queue.put_nowait(task)\n\n  def decode(self):\n    \"\"\"Run decode steps on current decoder state.\n\n    Performs `self.min_decode_steps` decode operations\n    and queues results for background processing.\n    \"\"\"\n\n    for i in range(self.min_decode_steps):\n      # Generate next tokens\n      self.decode_state, result_tokens, log_prob = self._jitted_generate_fn(self.params, self.decode_state, self.rng)\n      if i == self.min_decode_steps - 1:\n        # Block on the last token\n        jax.block_until_ready(result_tokens)\n\n      # Queue detokenization task\n      task = DetokenizationTask(\n          task_type=\"decode\",\n          tokens_buffer=result_tokens,\n          logprob_buffer=log_prob,\n      )\n\n      self.detokenization_queue.put_nowait(task)\n\n  @functools.partial(jax.jit, static_argnums=(0,), donate_argnums=(2,))\n  def _jitted_generate_fn(self, params, decode_state, rng):\n    decode_state, result_tokens = self.engine.generate(params, decode_state, rng=rng)\n    return decode_state, result_tokens.data[:, 0], result_tokens.log_prob\n\n  def background_detokenization(self):\n    \"\"\"Background thread that handles all GPU-to-CPU transfers and token emission.\n\n    This thread processes DetokenizationTask objects from the queue,\n    performs the numpy conversions, emits tokens, and manages decode slots.\n    \"\"\"\n    max_logging.log(\"Inference worker: starting detokenization thread\")\n\n    while True:\n      try:\n        task = self.detokenization_queue.get(timeout=0.1)\n      except queue.Empty:\n        if not self.running and self.detokenization_queue.empty():\n          break\n        continue\n\n      start_time = time.time()\n      newly_empty = []\n\n      if task.task_type == \"prefill\":\n\n        # Process prefill results - convert to numpy and emit\n        with jax.profiler.TraceAnnotation(\"convert_to_numpy_and_emit_prefill\"):\n          for i, result_tokens in enumerate(task.result_tokens):\n            prompt_id = task.prompt_ids[i]\n            slot = task.slots[i]\n\n            prompt_logp = task.prompt_logp[i]\n            true_length = self.true_lengths[prompt_id]\n\n            # Convert to numpy\n            first_token = np.array(result_tokens.data[:, 0])\n            log_prob = np.array(result_tokens.log_prob)\n            prompt_logp_np = np.array(prompt_logp)[:, :true_length]\n\n            # Emit token directly\n            should_terminate = self.emit_token(prompt_id, int(first_token), log_prob, prompt_logp=prompt_logp_np)\n            if should_terminate:\n              newly_empty.append(slot)\n\n      elif task.task_type == \"decode\":\n\n        # Check if there are any active sequences before expensive numpy conversion\n        active_slots = []\n        for slot, id_ in self.slot_to_id.items():\n          if id_ is not None and id_ not in self.completed_sequences:\n            active_slots.append((slot, id_))\n\n        # Skip processing entirely if no active sequences\n        if not active_slots:\n          continue\n\n        # Process single decode step - convert to numpy and emit\n        with jax.profiler.TraceAnnotation(\"convert_to_numpy_and_emit_decode_step\"):\n          result_tokens_step = np.array(task.tokens_buffer)  # Single step tokens\n          log_prob_step = np.array(task.logprob_buffer)  # Single step logprobs\n\n          for slot, id_ in active_slots:\n            log_prob_at_slot = log_prob_step[slot]\n            result_tokens_at_slot = result_tokens_step[slot]\n            should_terminate = self.emit_token(id_, int(result_tokens_at_slot), log_prob_at_slot)\n            if should_terminate:\n              newly_empty.append(slot)\n      # Update decode slots\n      for slot in newly_empty:\n        self.slot_to_id[slot] = None\n        self.empty_decode_slots.add(slot)\n\n      if self.debug:\n        max_logging.log(f\"Inference worker: detokenization in {time.time() - start_time} seconds\")\n\n  def emit_token(\n      self,\n      prompt_id,\n      result_token: int,\n      log_prob: float,\n      prompt_logp: None | np.ndarray = None,\n  ):\n    \"\"\"Adds the token to the results for the specified prompt ID and\n    determines if generation should terminate.\n\n    Args:\n        prompt_id: ID of the prompt\n        result_token: Token to emit\n        log_prob: Log probability of the token\n        prompt_logp: Log probabilities for the prompt tokens\n\n    Returns:\n        True if this token signals the end of generation, False otherwise\n    \"\"\"\n    # Skip if sequence already completed\n    if prompt_id in self.completed_sequences:\n      return True\n\n    index = len(self.completion_tokens_by_id[prompt_id])\n    if prompt_logp is not None:\n      self.prompt_logprobs_by_id[prompt_id] = [prompt_logp]\n\n    self.completion_tokens_by_id[prompt_id].append(TokenOutput(np.array(result_token), np.array(log_prob)))\n\n    # Check if this token completes the sequence\n    should_terminate = (result_token in self.eos_ids) or (index + 1 == self.max_decode_length)\n    if should_terminate:\n      self.completed_sequences.add(prompt_id)\n\n    return should_terminate",
        "analysis": {
            "module_type": "inference_worker",
            "purpose": "Manages continuous batching for inference, processing inputs, prefilling, decoding, and detokenizing.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize MaxEngine and tokenizer.",
                "Initialize PrefillHelper.",
                "Initialize DecodeState.",
                "Start a background detokenization thread.",
                "Process each input: wait for slot, prefill, and insert into KV cache.",
                "Finalize prefill operations (especially for batch prefill).",
                "Continue decoding until all sequences are complete.",
                "Join the detokenization thread.",
                "Build and return final completion outputs."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "MaxTextConfig",
                "Params",
                "MaxEngine",
                "PrefillHelper",
                "DecodeState",
                "InputData",
                "CompletionOutput",
                "TokenOutput",
                "DetokenizationTask",
                "SafeThread",
                "PrefillType",
                "PrefillResult",
                "jax",
                "numpy",
                "queue",
                "threading",
                "time",
                "functools",
                "dataclasses",
                "typing",
                "collections.abc",
                "collections",
                "jax.sharding.Mesh",
                "jax.experimental.mesh_utils",
                "max_logging"
            ],
            "parameters": {
                "config": "MaxText configuration object.",
                "params": "Model parameters; if None, loaded from config.",
                "min_decode_steps": "Minimum number of decode steps to run at once.",
                "enable_batch_prefill": "Whether to enable batch prefill for packing multiple inputs.",
                "devices": "List of JAX devices to use.",
                "tokenizer": "Tokenizer object.",
                "eos_ids": "List of end-of-sequence token IDs.",
                "prefill_lengths": "List of supported prefill lengths.",
                "max_decode_length": "Maximum tokens to generate per sequence.",
                "batch_prefill_max_batch_size": "Maximum batch size for batch prefill.",
                "is_pw_reshard": "Whether to use Pathways for resharding (default: True).",
                "rng": "JAX random number generator key.",
                "mesh": "JAX mesh for distributed computation.",
                "debug": "Boolean flag for enabling debug logging."
            },
            "notes": [
                "Implements continuous batching for efficient inference.",
                "Supports prefill packing when `enable_batch_prefill` is True.",
                "Uses a background thread for detokenization to avoid blocking the main inference loop.",
                "Manages inference state including decode slots, completed sequences, and output buffers."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the InferenceWorker with configuration, model parameters, and inference settings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set configuration attributes.",
                        "Initialize MaxEngine and load parameters.",
                        "Initialize tokenizer.",
                        "Initialize PrefillHelper.",
                        "Initialize DecodeState.",
                        "Log initialization time."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxEngine",
                        "PrefillHelper",
                        "jax",
                        "time",
                        "max_logging"
                    ],
                    "notes": [
                        "Initializes various internal states and components required for inference."
                    ]
                },
                "_init_engine": {
                    "purpose": "Initializes the MaxEngine and loads model parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a MaxEngine instance.",
                        "Load model parameters using the engine.",
                        "Log initialization time."
                    ],
                    "output": {
                        "shape": "tuple[Params, MaxEngine]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "MaxEngine",
                        "jax",
                        "time",
                        "max_logging"
                    ],
                    "notes": []
                },
                "_init_tokenizer": {
                    "purpose": "Initializes the tokenizer, loading it from the engine if necessary.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if tokenizer and eos_ids are already set.",
                        "If not, get tokenizer parameters from the engine and build the tokenizer.",
                        "Set eos_ids if they are not provided.",
                        "Return the initialized tokenizer."
                    ],
                    "output": {
                        "shape": "Any (tokenizer object)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "MaxEngine"
                    ],
                    "notes": []
                },
                "update_params": {
                    "purpose": "Updates the model parameters used by the inference worker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assign the new parameters to `self.params`."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "reset_state": {
                    "purpose": "Resets all internal state variables for a new inference run.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `self.running` to False.",
                        "Clear and reinitialize output buffers (`completion_tokens_by_id`, `prompt_logprobs_by_id`, `true_lengths`).",
                        "Reset decode slots (`empty_decode_slots`, `slot_to_id`).",
                        "Clear completed sequences set.",
                        "Reinitialize the detokenization queue.",
                        "Log state reset completion."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "defaultdict",
                        "queue",
                        "max_logging"
                    ],
                    "notes": [
                        "Allows reusing the same InferenceWorker instance without recreating expensive engine components."
                    ]
                },
                "run_inference": {
                    "purpose": "Starts the inference process for a given batch of input data.",
                    "input": {
                        "shape": "data: list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reset the worker's state.",
                        "Update the RNG key if provided.",
                        "Set up `self.true_lengths` and `self.running` flag.",
                        "Log the start of continuous batching.",
                        "Call `_run_continuous_batching` to perform the inference.",
                        "Call `_build_final_outputs` to format the results.",
                        "Return the final outputs."
                    ],
                    "output": {
                        "shape": "list[CompletionOutput]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "InputData",
                        "CompletionOutput",
                        "max_logging"
                    ],
                    "notes": []
                },
                "_run_continuous_batching": {
                    "purpose": "Executes the core continuous batching inference loop.",
                    "input": {
                        "shape": "data: list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Start the background detokenization thread.",
                        "Iterate through each input in the data:",
                        "  Wait for an empty decode slot.",
                        "  Get an available slot.",
                        "  Prefill the input using `self.prefill_helper.process`.",
                        "Flush any pending inputs in batch prefill mode using `self.prefill_helper.finalize`.",
                        "Continue decoding until all sequences are complete by repeatedly calling `self.decode()`.",
                        "Set `self.running` to False.",
                        "Join the detokenization thread.",
                        "Log detokenization thread join time."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "SafeThread",
                        "PrefillHelper",
                        "max_logging",
                        "jax.profiler.TraceAnnotation"
                    ],
                    "notes": [
                        "This method orchestrates the prefill and decode steps.",
                        "It relies on the `detokenization_queue` and `background_detokenization` method for asynchronous processing."
                    ]
                },
                "_build_final_outputs": {
                    "purpose": "Constructs the final list of `CompletionOutput` objects from the collected results.",
                    "input": {
                        "shape": "input_data: list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through each input in the original `input_data`.",
                        "Retrieve prompt length, prompt tokens, completion tokens, log probabilities, and prompt log probabilities.",
                        "Concatenate prompt and completion tokens and log probabilities.",
                        "Create a `CompletionOutput` object for each input.",
                        "Append the `CompletionOutput` to the results list.",
                        "Return the list of `CompletionOutput` objects."
                    ],
                    "output": {
                        "shape": "list[CompletionOutput]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "CompletionOutput",
                        "InputData",
                        "jax.profiler.TraceAnnotation",
                        "numpy"
                    ],
                    "notes": []
                },
                "prefill_done": {
                    "purpose": "Callback function executed after prefill operations complete.",
                    "input": {
                        "shape": "prefill_result: list[PrefillResult], prompt_ids: list[int], decode_state: DecodeState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Update `self.decode_state`.",
                        "Process each `PrefillResult`:",
                        "  Map slot to input ID.",
                        "  Collect results, prompt IDs, and slots.",
                        "Create a `DetokenizationTask` of type 'prefill'.",
                        "Put the task into the `self.detokenization_queue`."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "PrefillResult",
                        "DetokenizationTask",
                        "queue"
                    ],
                    "notes": [
                        "This method is designed to be called by the `PrefillHelper`."
                    ]
                },
                "decode": {
                    "purpose": "Performs a fixed number of decode steps on the current decoder state.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Loop `self.min_decode_steps` times:",
                        "  Call the jitted generate function (`_jitted_generate_fn`) to get the next tokens and log probabilities.",
                        "  If it's the last step, block until the results are ready.",
                        "  Create a `DetokenizationTask` of type 'decode'.",
                        "  Put the task into the `self.detokenization_queue`."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "DetokenizationTask",
                        "queue",
                        "jax"
                    ],
                    "notes": [
                        "Queues decode results for background processing by the detokenization thread."
                    ]
                },
                "_jitted_generate_fn": {
                    "purpose": "A jitted function that performs one step of generation (decoding).",
                    "input": {
                        "shape": "params: Params, decode_state: DecodeState, rng: jax.random.PRNGKey",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `self.engine.generate` to get the updated decode state and result tokens.",
                        "Extract the first token and its log probability from the result tokens.",
                        "Return the updated decode state, the first token, and its log probability."
                    ],
                    "output": {
                        "shape": "tuple[DecodeState, jax.Array, jax.Array]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "MaxEngine",
                        "jax"
                    ],
                    "notes": [
                        "Decorated with `jax.jit` for performance optimization.",
                        "`donate_argnums=(2,)` suggests that the `rng` argument might be mutated or consumed."
                    ]
                },
                "background_detokenization": {
                    "purpose": "Runs in a separate thread to process detokenization tasks from a queue.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Enter an infinite loop that breaks when `self.running` is False and the queue is empty.",
                        "Try to get a task from `self.detokenization_queue` with a timeout.",
                        "If the task type is 'prefill':",
                        "  Convert prefill results (tokens, log probabilities) to numpy arrays.",
                        "  Call `self.emit_token` for each prefilled token.",
                        "  If `emit_token` returns True, mark the slot for freeing.",
                        "If the task type is 'decode':",
                        "  Check for active sequences.",
                        "  Convert decode step results (tokens, log probabilities) to numpy arrays.",
                        "  Call `self.emit_token` for each active sequence's token.",
                        "  If `emit_token` returns True, mark the slot for freeing.",
                        "Update decode slots by freeing them and adding to `self.empty_decode_slots`.",
                        "Log detokenization processing time if debug is enabled."
                    ],
                    "output": {
                        "shape": "None",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "queue",
                        "numpy",
                        "jax.profiler.TraceAnnotation",
                        "max_logging"
                    ],
                    "notes": [
                        "Handles GPU-to-CPU transfers and token emission.",
                        "Manages the freeing of decode slots upon sequence completion."
                    ]
                },
                "emit_token": {
                    "purpose": "Appends a generated token to the results for a given prompt ID and checks for sequence termination.",
                    "input": {
                        "shape": "prompt_id: Any, result_token: int, log_prob: float, prompt_logp: None | np.ndarray = None",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If the prompt ID is already in `self.completed_sequences`, return True.",
                        "Append the `result_token` and `log_prob` to `self.completion_tokens_by_id[prompt_id]`.",
                        "If `prompt_logp` is provided, store it in `self.prompt_logprobs_by_id[prompt_id]`.",
                        "Determine if the sequence should terminate based on `result_token` being an EOS ID or reaching `self.max_decode_length`.",
                        "If termination is signaled, add the `prompt_id` to `self.completed_sequences`.",
                        "Return whether the sequence should terminate."
                    ],
                    "output": {
                        "shape": "bool",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "TokenOutput",
                        "numpy"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#OfflineEngine",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class OfflineEngine:\n  \"\"\"Class for handling offline inference on batches of inputs.\"\"\"\n\n  def __init__(\n      self,\n      config: Any,\n      params: None | Params = None,\n      enable_batch_prefill: bool = False,\n      min_decode_steps: int = 10,\n      tokenizer: Any = None,\n      eos_ids: list[int] | None = None,\n      prefill_lengths: list[int] | str = \"auto\",\n      batch_prefill_max_batch_size: int = 16,\n      mesh: Mesh = None,\n      rng: jax.random.PRNGKey = None,\n      debug: bool = False,\n  ):\n    \"\"\"Initialize the OfflineEngine.\n\n    Args:\n        config: The MaxText config object which will be used to\n          create MaxEngine instance(s).\n        params: Model parameters (loaded from engine if None)\n        enable_batch_prefill: Whether to use prefill packing.\n            config.scan_layers must be False if this is True\n        min_decode_steps: Number of decode steps to perform at a time,\n            before checking for completion.\n        eos_ids: list of EOS token IDs for checking sequence completion.\n          If None, the tokenizer's EOS token will be used.\n        tokenizer: Tokenizer instance for encoding/decoding text. If None,\n          will be created using the config if eos_ids is not provided.\n        prefill_lengths: list of expected prefill lengths, or \"auto\" to\n            automatically determine appropriate lengths from the engine\n            config. Input sequences will be padded to the nearest length\n            in this list.\n        batch_prefill_max_batch_size: Maximum number of inputs to pack\n          into a single prefill. This is only used when enable_batch_prefill\n          is True.\n        mesh: JAX Mesh object. Use this\n          argument if you want to use only some of the devices for OfflineEngine and\n          reserve the rest for other tasks. If None, OfflineEngine will create the mesh\n          automatically.\n        rng: Random number generator key. If None, a new key will be created.\n    \"\"\"\n    max_logging.log(\"Initializing OfflineEngine\")\n    # Configurations\n    self.config = config\n    self.params = params\n    self.min_decode_steps = min_decode_steps\n    self.enable_batch_prefill = enable_batch_prefill\n    self.mesh = mesh\n    self.tokenizer = tokenizer\n    self.eos_ids = eos_ids\n    self.prefill_lengths = prefill_lengths\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.max_prefill_length = self.config.max_prefill_predict_length\n    self.max_decode_length = self.config.max_target_length - self.max_prefill_length\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n    self._validate_config()\n\n    # Create prefill buckets: [0, 64], (64, 128], (128, 256], ..., [max_length//2, max_length]\n    if prefill_lengths == \"auto\":\n      self.prefill_lengths = [2**i for i in range(6, max(6, (self.max_prefill_length - 1).bit_length()) + 1)]\n    else:\n      self.prefill_lengths = sorted(prefill_lengths)\n\n    # Create meshes\n    if not self.mesh:\n      self.mesh = OfflineEngine.create_mesh(jax.devices(), self.config)\n\n    self.worker = InferenceWorker(\n        config=self.config,\n        params=self.params,\n        min_decode_steps=self.min_decode_steps,\n        enable_batch_prefill=self.enable_batch_prefill,\n        mesh=self.mesh,\n        devices=self.mesh.devices.flatten(),\n        tokenizer=self.tokenizer,\n        eos_ids=self.eos_ids,\n        prefill_lengths=self.prefill_lengths,\n        max_decode_length=self.max_decode_length,\n        batch_prefill_max_batch_size=self.batch_prefill_max_batch_size,\n        rng=self.rng,\n        debug=self.debug,\n    )\n\n    self.tokenizer = self.worker.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n  ):\n    \"\"\"Update model weights.\"\"\"\n    self.worker.update_params(params)\n\n  def batch_inference(\n      self,\n      data: list[InputData] | list[jax.Array] | list[np.ndarray],\n      desc: str = \"\",\n      rng=None,\n  ) -> list[CompletionOutput]:\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays.\n            If input is JAX or numpy array, it must not contain padding tokens.\n        desc: Description string for logging\n        rng: Random number generator key. If None, the previous key will be used.\n\n    Returns:\n        list of CompletionOutput objects, one for each input in data\n    \"\"\"\n    data = self.prepare_data(data)\n\n    return self.worker.run_inference(data, rng)\n\n  def prepare_data(self, data: list[InputData | jax.Array | np.ndarray]) -> list[InputData]:\n    \"\"\"Pad and if batch prefill is enabled, sort data by length.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays\n\n    Returns:\n        list of prepared InputData objects\n    \"\"\"\n    # Convert JAX arrays to numpy arrays\n    if isinstance(data[0], jax.Array):\n      data = [np.array(array) for array in data]\n\n    # Convert numpy arrays to InputData objects\n    if isinstance(data[0], np.ndarray):\n      max_logging.log(\n          \"When you provide JAX/numpy arrays to Offline Engine, \"\n          \"make sure that the arrays are not padded with padding tokens.\"\n      )\n      data = [InputData(id=i, tokens=array, true_length=len(array)) for i, array in enumerate(data)]\n\n    # Make sure all data id is unique\n    if len(data) != len({item.id for item in data}):\n      raise ValueError(\"All data ids must be unique\")\n\n    data = self.pad_data(data)\n\n    if self.enable_batch_prefill:\n      return sorted(data, key=lambda x: x.tokens.shape[0])\n\n    return data\n\n  def pad_data(self, data: list[InputData]) -> list[InputData]:\n    \"\"\"For each input, pad it to the next length in self.prefill_lengths\n    that is greater than or equal to its true length.\n\n    Args:\n        data: list of InputData objects\n\n    Returns:\n        list of padded InputData objects\n    \"\"\"\n    padded_data = []\n\n    for item in data:\n      # Find the smallest prefill length that can accommodate this input\n      target_length = None\n      for length in self.prefill_lengths:\n        if length >= item.true_length:\n          target_length = length\n          break\n\n      # If no suitable length found, use the maximum prefill length\n      if target_length is None:\n        target_length = self.max_prefill_length\n\n      # Pad or truncate as needed\n      if len(item.tokens) < target_length:\n        # Pad with zeros\n        padded_tokens = np.zeros(target_length, dtype=item.tokens.dtype)\n        padded_tokens[: item.true_length] = item.tokens[: item.true_length]\n      else:\n        # Input is too long, truncate to max_prefill_length\n        padded_tokens = item.tokens[:target_length]\n\n      # Create new InputData with padded tokens\n      padded_data.append(InputData(id=item.id, tokens=padded_tokens, true_length=item.true_length))\n\n    return padded_data\n\n  @staticmethod\n  def create_mesh(devices, config):\n    \"\"\"Create data parallelism meshes for each Inference worker.\"\"\"\n    ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), len(devices), \"ICI\")\n    devices_array = mesh_utils.create_device_mesh(\n        ici_parallelism,\n        devices,\n        contiguous_submeshes=False,\n        allow_split_physical_axes=config.allow_split_physical_axes or False,\n    )\n    mesh = Mesh(devices_array.reshape(ici_parallelism), config.mesh_axes)\n    return mesh\n\n  def _validate_config(self):\n    \"\"\"Validate configuration parameters and check for incompatible settings.\"\"\"\n    if not self.config.return_log_prob:\n      raise ValueError(\"return_log_prob must be True when using OfflineEngine\")\n    if self.enable_batch_prefill and self.config.scan_layers:\n      raise ValueError(\"scan_layers must be False if enable_batch_prefill is True\")\n\n    if self.max_decode_length <= 0:\n      raise ValueError(\"Make sure max_target_length - max_prefill_predict_length is greater than 0\")\n    if self.config.scan_layers:\n      max_logging.log(\n          \"WARNING: scan_layers=True will result in slow step time. \" \"It is recommended for debugging purposes only.\"\n      )",
        "analysis": {
            "module_type": "offline_engine",
            "purpose": "Handles offline inference on batches of input sequences.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Configuration validation",
                "Prefill length bucket creation",
                "Mesh creation (if not provided)",
                "InferenceWorker initialization",
                "Data preparation (padding, sorting)",
                "Batch inference execution",
                "Output construction"
            ],
            "output": {
                "shape": "list[CompletionOutput]",
                "dtype": "N/A"
            },
            "dependencies": [
                "InferenceWorker",
                "MaxEngine",
                "MaxTextConfig",
                "jax",
                "numpy",
                "Mesh",
                "mesh_utils",
                "max_utils",
                "max_logging"
            ],
            "parameters": {
                "config": "MaxText config object for creating MaxEngine instances.",
                "params": "Model parameters (loaded from engine if None).",
                "enable_batch_prefill": "Boolean flag to enable prefill packing.",
                "min_decode_steps": "Minimum number of decode steps to perform at a time.",
                "tokenizer": "Tokenizer instance for encoding/decoding text.",
                "eos_ids": "List of EOS token IDs for sequence completion.",
                "prefill_lengths": "List of expected prefill lengths or 'auto'.",
                "batch_prefill_max_batch_size": "Maximum number of inputs to pack into a single prefill.",
                "mesh": "JAX Mesh object for distributed computation.",
                "rng": "Random number generator key.",
                "debug": "Boolean flag for enabling debug logging."
            },
            "notes": [
                "Requires `config.return_log_prob` to be True.",
                "If `enable_batch_prefill` is True, `config.scan_layers` must be False.",
                "Input data can be `InputData` objects, JAX arrays, or NumPy arrays.",
                "If JAX or NumPy arrays are provided, they must not contain padding tokens."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the OfflineEngine with configuration and parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Log initialization",
                        "Store configuration parameters",
                        "Validate configuration",
                        "Determine prefill lengths",
                        "Create JAX mesh if not provided",
                        "Initialize InferenceWorker"
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker",
                        "OfflineEngine.create_mesh",
                        "OfflineEngine._validate_config"
                    ],
                    "notes": []
                },
                "update_params": {
                    "purpose": "Updates the model weights used by the InferenceWorker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `update_params` on the internal InferenceWorker."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker.update_params"
                    ],
                    "notes": []
                },
                "batch_inference": {
                    "purpose": "Runs inference on a batch of input data.",
                    "input": {
                        "shape": "list[InputData | jax.Array | np.ndarray]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Prepare input data (padding, sorting).",
                        "Call `run_inference` on the internal InferenceWorker.",
                        "Return the results."
                    ],
                    "output": {
                        "shape": "list[CompletionOutput]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "OfflineEngine.prepare_data",
                        "InferenceWorker.run_inference"
                    ],
                    "notes": [
                        "The `desc` argument is for logging purposes.",
                        "An optional `rng` key can be provided for reproducibility."
                    ]
                },
                "prepare_data": {
                    "purpose": "Prepares input data by converting, padding, and sorting it.",
                    "input": {
                        "shape": "list[InputData | jax.Array | np.ndarray]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Convert JAX arrays to NumPy arrays if necessary.",
                        "Convert NumPy arrays to `InputData` objects if necessary.",
                        "Validate that all input IDs are unique.",
                        "Pad the input data to appropriate lengths.",
                        "Sort data by length if `enable_batch_prefill` is True."
                    ],
                    "output": {
                        "shape": "list[InputData]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "InputData",
                        "OfflineEngine.pad_data",
                        "max_logging"
                    ],
                    "notes": [
                        "Warns if JAX/NumPy arrays are provided without padding tokens.",
                        "Padding is done based on `self.prefill_lengths`."
                    ]
                },
                "pad_data": {
                    "purpose": "Pads each input `InputData` object to the nearest length in `self.prefill_lengths`.",
                    "input": {
                        "shape": "list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through each `InputData` item.",
                        "Find the smallest prefill length greater than or equal to the item's true length.",
                        "Pad the item's tokens with zeros to the target length or truncate if necessary.",
                        "Create a new `InputData` object with the padded tokens."
                    ],
                    "output": {
                        "shape": "list[InputData]",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "InputData"
                    ],
                    "notes": [
                        "If no suitable prefill length is found, `self.max_prefill_length` is used.",
                        "Padding is done with zeros."
                    ]
                },
                "create_mesh": {
                    "purpose": "Creates a JAX Mesh object for distributed computation.",
                    "input": {
                        "shape": "list[jax.Device], MaxTextConfig",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine ICI parallelism based on config and available devices.",
                        "Create a device mesh using `mesh_utils.create_device_mesh`.",
                        "Reshape the device array and create a JAX `Mesh` object."
                    ],
                    "output": {
                        "shape": "Mesh",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "jax.devices",
                        "Mesh",
                        "mesh_utils",
                        "max_utils"
                    ],
                    "notes": [
                        "This is a static method."
                    ]
                },
                "_validate_config": {
                    "purpose": "Validates configuration parameters for compatibility.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `config.return_log_prob` is True.",
                        "Check if `config.scan_layers` is False when `enable_batch_prefill` is True.",
                        "Check if `max_decode_length` is positive.",
                        "Log a warning if `config.scan_layers` is True."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_logging"
                    ],
                    "notes": [
                        "Raises `ValueError` for incompatible settings.",
                        "Logs a warning for potentially slow `scan_layers` usage."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageState",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageState:\n  \"\"\"Represents the global state of memory pages managed by the `PageManager`.\n\n  This dataclass tracks the allocation status of each page across the entire system,\n  the mapping of pages to page groups (requests), and the current position within\n  each sequence's pages. State is managed globally, providing a single view\n  across all potential layers using this manager.\n\n  Attributes:\n    page_status: A `jnp.ndarray` of shape `[num_pages]`. Each element\n      indicates whether the corresponding page in the global pool is free (0)\n      or allocated (1).\n    page_map: A `jnp.ndarray` of shape `[max_page_groups, max_pages_per_group]`.\n      This array maps each page group to the indices (within the global pool)\n      of its allocated pages. Entries beyond `num_pages_used` for a group are invalid.\n    num_pages_used: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      tracks the number of pages currently allocated to each page group. This\n      determines the valid entries in `page_map` for each group.\n    sequence_lengths: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the current true length of each sequence (in tokens) associated\n      with a page group.\n    active_page: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the global index of the *currently active* page (the page where the\n      next token will be written) for each page group. Only valid if the\n      corresponding `has_active_page` is True.\n    has_active_page: A `jnp.ndarray` of shape `[max_page_groups]`. Boolean mask\n      indicating whether a page group currently represents an active sequence\n      and thus whether its `active_page` and `active_page_position` entries\n      are meaningful.\n    active_page_position: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the index (offset, 0 to tokens_per_page-1) of the next available\n      token *within the `active_page`* for each page group. Only valid if\n      `has_active_page` is True.\n  \"\"\"\n\n  page_status: PagesInt1d\n  page_map: GroupsPagesInt2d\n  num_pages_used: GroupsInt1d\n  sequence_lengths: GroupsInt1d\n  active_page: GroupsInt1d\n  has_active_page: GroupsBool1d\n  active_page_position: GroupsInt1d",
        "analysis": {
            "module_type": "page_state",
            "purpose": "Represents the global state of memory pages managed by the `PageManager`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jnp.ndarray",
                "PagesInt1d",
                "GroupsPagesInt2d",
                "GroupsInt1d",
                "GroupsBool1d",
                "flax.struct.dataclass"
            ],
            "parameters": {
                "page_status": "A jnp.ndarray of shape [num_pages] indicating page allocation status (0=free, 1=allocated).",
                "page_map": "A jnp.ndarray of shape [max_page_groups, max_pages_per_group] mapping page groups to global page indices.",
                "num_pages_used": "A jnp.ndarray of shape [max_page_groups] tracking pages allocated per group.",
                "sequence_lengths": "A jnp.ndarray of shape [max_page_groups] storing current sequence lengths.",
                "active_page": "A jnp.ndarray of shape [max_page_groups] storing the global index of the currently active page for each group.",
                "has_active_page": "A jnp.ndarray of shape [max_page_groups] boolean mask indicating if a group has an active page.",
                "active_page_position": "A jnp.ndarray of shape [max_page_groups] storing the offset within the active page for each group."
            },
            "notes": [
                "This is a dataclass used to hold the state for paged attention.",
                "It tracks the global status of all pages, how they are mapped to different page groups (requests), and the current progress within each sequence."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#initialize_page_state",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def initialize_page_state(\n    num_pages: int,\n    max_page_groups: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Creates and initializes a global `PageState` object.\n\n  All pages in the global pool are initially marked as free (status 0), except\n  for page 0 which is marked used as a workaround. No pages are assigned to any\n  page group. Sequence lengths and page usage counts are initialized to zero.\n  Active page tracking is also reset.\n\n  Args:\n    num_pages: The total number of available pages in the global pool.\n    max_page_groups: The maximum number of page groups (concurrent sequences/requests)\n      the system can track.\n    max_pages_per_group: The maximum number of pages that can be allocated to\n      a single page group (determines the size of the second dimension of `page_map`).\n\n  Returns:\n    An initialized `PageState` object with all values set to their defaults (zeros/False).\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  initial_page_status = jnp.zeros((num_pages,), dtype=jnp.int32)\n  initial_page_status = initial_page_status.at[0].set(1)  # Workaround page 0\n  return PageState(\n      page_status=initial_page_status,\n      page_map=jnp.zeros((max_page_groups, max_pages_per_group), dtype=jnp.int32),\n      num_pages_used=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      sequence_lengths=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      active_page=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      has_active_page=jnp.zeros((max_page_groups,), dtype=jnp.bool_),\n      active_page_position=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n  )",
        "analysis": {
            "module_type": "page_state_initializer",
            "purpose": "Initializes the global state for managing memory pages used in paged attention.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create an array `initial_page_status` of zeros with size `num_pages`.",
                "Set the first element of `initial_page_status` to 1 as a workaround.",
                "Instantiate a `PageState` object with initialized arrays for page status, page map, number of pages used, sequence lengths, active page, has active page, and active page position."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.numpy as jnp",
                "PageState (dataclass)"
            ],
            "parameters": {
                "num_pages": "The total number of available pages in the global pool.",
                "max_page_groups": "The maximum number of page groups (concurrent sequences/requests) the system can track.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "Page 0 is marked as used as a workaround.",
                "All other pages are initially marked as free (status 0).",
                "No pages are assigned to any page group initially.",
                "Sequence lengths, page usage counts, and active page tracking are initialized to zero/False."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_find_next_free_page_index",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _find_next_free_page_index(page_status: PagesInt1d) -> ScalarInt:\n  \"\"\"Finds the index of the next available free page in the global pool.\n\n  Searches the `page_status` array for the first occurrence of 0 (indicating\n  a free page), skipping index 0 due to potential issues.\n\n  Args:\n    page_status: A 1D `jnp.ndarray` representing the global status of pages\n      (0 for free, 1 for allocated). Should have shape [num_pages].\n\n  Returns:\n    A scalar `jnp.int32` array containing the index of the next free page\n    (the lowest index >= 1 where `page_status` is 0).\n    Returns -1 if no free pages (at index >= 1) are found.\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  search_status = page_status[1:]\n  overall_free_mask = search_status == 0\n\n  # argmax returns the index of the *first* True. If none are True, it returns 0.\n  next_free_relative = jnp.argmax(overall_free_mask)\n  # Add 1 to compensate for the slice [1:]\n  next_free_overall = next_free_relative + 1\n  # Check if a free page exists\n  has_free_overall = jnp.any(overall_free_mask)\n  # If a free page exists, return its index, otherwise return -1\n  return jnp.where(has_free_overall, next_free_overall, -1)",
        "analysis": {
            "functionality": "Finds the index of the next available free page in a global memory pool, excluding page 0.",
            "usage": "This function takes a 1D JAX numpy array `page_status` representing the allocation status of pages (0 for free, 1 for allocated). It returns a scalar integer representing the index of the first free page found starting from index 1. If no free pages are found (from index 1 onwards), it returns -1. The input `page_status` is expected to have a shape of [num_pages]."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_pages_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases all pages associated with a given page group.\n\n  This function iterates through the potential pages allocated to the specified\n  `page_group_id` (up to `max_pages_per_group`). For each page index actually\n  used by the group (determined by `num_pages_used`), it retrieves the global\n  page index from `page_map` and resets its status to 0 (free) in the global\n  `page_status` array. It also resets all state fields related to the\n  `page_group_id` (length, count, active status, etc.) to their initial values.\n\n  Args:\n    page_state: The current global `PageState`.\n    page_group_id: The index of the page group whose pages are to be released.\n    max_pages_per_group: The maximum number of pages a group can hold (used as\n      the loop bound).\n\n  Returns:\n    A new `PageState` object where the specified group's pages are marked as free\n    in `page_status`, and the group's specific state entries are reset.\n  \"\"\"\n  current_page_status = page_state.page_status\n  current_page_map = page_state.page_map\n  num_valid_pages = page_state.num_pages_used[page_group_id]\n\n  def release_page(i: int, status: PagesInt1d) -> PagesInt1d:\n    is_valid = i < num_valid_pages\n    page_idx = current_page_map[page_group_id, i]\n    # Only release if index 'i' points to a valid allocated page\n    should_release = jnp.logical_and(is_valid, page_idx > 0)\n\n    return jax.lax.cond(should_release, lambda s: s.at[page_idx].set(0), lambda s: s, status)\n\n  new_page_status = jax.lax.fori_loop(0, max_pages_per_group, release_page, current_page_status)\n\n  return page_state.replace(\n      page_status=new_page_status,\n      num_pages_used=page_state.num_pages_used.at[page_group_id].set(0),\n      sequence_lengths=page_state.sequence_lengths.at[page_group_id].set(0),\n      active_page=page_state.active_page.at[page_group_id].set(0),\n      has_active_page=page_state.has_active_page.at[page_group_id].set(False),\n      active_page_position=page_state.active_page_position.at[page_group_id].set(0),\n  )",
        "analysis": {
            "functionality": "Releases all pages associated with a specific page group and resets the group's state.",
            "usage": "Call `_release_pages_for_group` with the current `PageState`, the `page_group_id` to clear, and `max_pages_per_group`. It returns an updated `PageState` with the specified group's pages freed and its state reset."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_reserve_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _reserve_pages_for_group(\n    released_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Reserves pages for a specific group, assuming true_length > 0.\n\n  PRECONDITION: `true_length` must be > 0. This function assumes the caller\n  (e.g., `PageManager.update_prefill_pages`) has validated this.\n\n  Calculates the number of pages required for `true_length`. Checks if enough\n  free pages exist globally and if the group has capacity based on the state\n  provided in `released_state`. If resources are sufficient, it iteratively\n  finds free pages, marks them allocated, records them in the map, and updates\n  the group's state fields. If resources are insufficient, it returns the\n  `released_state` unchanged (effectively leaving the group empty).\n\n  Args:\n      released_state: The global `PageState` after pages for `page_group_id`\n          have already been released.\n      page_group_id: The index of the page group to allocate pages for.\n      true_length: The target sequence length for the prefill. MUST BE > 0.\n      tokens_per_page: The capacity of each page.\n      max_pages_per_group: The maximum number of pages the group can hold.\n\n  Returns:\n      A new `PageState` with pages allocated for the group and its state updated,\n      or the input `released_state` if allocation failed due to resource limits.\n  \"\"\"\n  num_pages_needed = (true_length + tokens_per_page - 1) // tokens_per_page\n  last_token_abs_idx = true_length - 1\n  last_page_position_idx = last_token_abs_idx % tokens_per_page\n  next_write_position = (last_page_position_idx + 1) % tokens_per_page\n\n  current_page_status = released_state.page_status\n  current_page_map = released_state.page_map\n  current_num_pages_used = released_state.num_pages_used\n\n  num_free_pages = jnp.sum(current_page_status == 0)\n  group_has_capacity = jax.lax.le(num_pages_needed, max_pages_per_group)\n  sufficient_free_pages = jax.lax.ge(num_free_pages, num_pages_needed)\n  has_enough_resources = jnp.logical_and(sufficient_free_pages, group_has_capacity)\n\n  def allocate_and_update_state(initial_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]) -> PageState:\n    \"\"\"Allocates pages iteratively if resources are sufficient.\"\"\"\n    initial_status, initial_map, initial_num_used = initial_state_tuple\n\n    def allocate_one_page(\n        page_idx_in_group: ScalarInt, loop_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]\n    ) -> tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]:\n      \"\"\"Allocates a single page within the fori_loop.\"\"\"\n      current_loop_status, current_loop_map, current_loop_num_used = loop_state_tuple\n      next_free_page_global = _find_next_free_page_index(current_loop_status)\n      page_allocated = jax.lax.ge(next_free_page_global, 0)\n\n      new_loop_status = jax.lax.cond(\n          page_allocated,\n          lambda s: s.at[next_free_page_global].set(1),\n          lambda s: s,\n          current_loop_status,\n      )\n      new_loop_map = jax.lax.cond(\n          page_allocated,\n          lambda m: m.at[page_group_id, page_idx_in_group].set(next_free_page_global),\n          lambda m: m,\n          current_loop_map,\n      )\n      new_loop_num_used = jax.lax.cond(\n          page_allocated,\n          lambda n: n.at[page_group_id].add(1),\n          lambda n: n,\n          current_loop_num_used,\n      )\n      return new_loop_status, new_loop_map, new_loop_num_used\n\n    final_page_status, final_page_map, final_num_pages_used = jax.lax.fori_loop(\n        0,\n        num_pages_needed,\n        allocate_one_page,\n        (initial_status, initial_map, initial_num_used),\n    )\n    active_page_global_index = final_page_map[page_group_id, num_pages_needed - 1]\n\n    return released_state.replace(\n        page_status=final_page_status,\n        page_map=final_page_map,\n        num_pages_used=final_num_pages_used,\n        sequence_lengths=released_state.sequence_lengths.at[page_group_id].set(true_length),\n        active_page=released_state.active_page.at[page_group_id].set(active_page_global_index),\n        has_active_page=released_state.has_active_page.at[page_group_id].set(True),\n        active_page_position=released_state.active_page_position.at[page_group_id].set(next_write_position),\n    )\n\n  # Conditionally perform allocation or return the released state\n  final_state = jax.lax.cond(\n      has_enough_resources,\n      allocate_and_update_state,\n      lambda _: released_state,\n      operand=(current_page_status, current_page_map, current_num_pages_used),\n  )\n  return final_state",
        "analysis": {
            "functionality": "Reserves pages for a specific group, assuming true_length > 0, by calculating needed pages, checking global and group capacity, and iteratively allocating free pages.",
            "usage": "Call with `released_state` (PageState), `page_group_id` (ScalarInt), `true_length` (ScalarInt), `tokens_per_page` (int), and `max_pages_per_group` (int). Returns an updated PageState with allocated pages or the original PageState if allocation fails due to resource limits. Precondition: `true_length` must be > 0."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_and_reserve_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_and_reserve_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases existing pages and reserves new pages for a group during prefill.\n\n  Assumes true_length > 0. Caller MUST validate inputs.\n  \"\"\"\n  released_state = _release_pages_for_group(page_state, page_group_id, max_pages_per_group)\n  final_state = _reserve_pages_for_group(released_state, page_group_id, true_length, tokens_per_page, max_pages_per_group)\n  return final_state",
        "analysis": {
            "functionality": "Releases existing pages and reserves new pages for a specified group during the prefill phase of attention.",
            "usage": "This function is called with the current PageState, the ID of the page group, the true length of the sequence, the number of tokens per page, and the maximum number of pages allowed per group. It returns an updated PageState reflecting the released and newly reserved pages. It assumes `true_length` is greater than 0 and that inputs have been validated by the caller."
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_update_decode_pages_global",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _update_decode_pages_global(\n    page_state: PageState,\n    tokens_per_page: ScalarInt,\n    max_pages_per_group: ScalarInt,\n) -> PageState:\n  \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n  This function performs the following steps for all page groups simultaneously:\n  1. Increments `sequence_lengths` for groups marked as `has_active_page`.\n  2. Calculates the new `active_page_position` based on the incremented length.\n  3. Determines which active groups now require a new page because their sequence\n     length has crossed a page boundary (`required_pages > num_pages_used`) and\n     they still have capacity (`required_pages <= max_pages_per_group`).\n  4. For each group identified in step 3, it attempts to find a free page globally and\n     allocate it, updating `page_status`, `page_map`, `num_pages_used`, and\n     `active_page` for that group.\n\n  Args:\n    page_state: The current global `PageState`.\n    tokens_per_page: The capacity of each page.\n    max_pages_per_group: The maximum number of pages allowed per group.\n\n  Returns:\n    A new `PageState` object reflecting the state after the decode step, potentially\n    with new pages allocated to groups that crossed page boundaries.\n  \"\"\"\n  max_page_groups = page_state.sequence_lengths.shape[0]\n\n  seq_len_increment = jnp.where(page_state.has_active_page, 1, 0)\n  new_sequence_lengths = page_state.sequence_lengths + seq_len_increment\n\n  new_active_page_position = jnp.where(\n      page_state.has_active_page,\n      (new_sequence_lengths - 1) % tokens_per_page,\n      page_state.active_page_position,\n  )\n\n  required_pages_per_group = (new_sequence_lengths + tokens_per_page - 1) // tokens_per_page\n  needs_new_page_mask = jnp.logical_and(page_state.has_active_page, required_pages_per_group > page_state.num_pages_used)\n  has_capacity_mask = required_pages_per_group <= max_pages_per_group\n  needs_allocation_mask = jnp.logical_and(needs_new_page_mask, has_capacity_mask)\n\n  def allocate_for_group_if_needed(group_idx: ScalarInt, current_state: PageState) -> PageState:\n    \"\"\"Inner function for fori_loop to conditionally allocate a page.\"\"\"\n    current_status = current_state.page_status\n    current_map = current_state.page_map\n    current_num_used = current_state.num_pages_used\n    current_active_page = current_state.active_page\n\n    needs_alloc = needs_allocation_mask[group_idx]\n    next_free_page_global = _find_next_free_page_index(current_status)\n    can_allocate = jnp.logical_and(needs_alloc, next_free_page_global >= 0)\n\n    new_status = jax.lax.cond(can_allocate, lambda s: s.at[next_free_page_global].set(1), lambda s: s, current_status)\n\n    page_map_index = current_num_used[group_idx]\n    new_map = jax.lax.cond(\n        can_allocate, lambda m: m.at[group_idx, page_map_index].set(next_free_page_global), lambda m: m, current_map\n    )\n    new_num_used = jax.lax.cond(can_allocate, lambda n: n.at[group_idx].add(1), lambda n: n, current_num_used)\n    new_active_page = jax.lax.cond(\n        can_allocate, lambda a: a.at[group_idx].set(next_free_page_global), lambda a: a, current_active_page\n    )\n\n    # Reconstruct state for loop carry/return\n    return current_state.replace(\n        page_status=new_status,\n        page_map=new_map,\n        num_pages_used=new_num_used,\n        active_page=new_active_page,\n    )\n\n  # Initialize loop state with pre-calculated lengths and positions\n  initial_loop_state = page_state.replace(\n      sequence_lengths=new_sequence_lengths,\n      active_page_position=new_active_page_position,\n  )\n\n  # Apply conditional allocation across all groups\n  final_state = jax.lax.fori_loop(0, max_page_groups, allocate_for_group_if_needed, initial_loop_state)\n  return final_state",
        "analysis": {
            "module_type": "update_decode_pages_global",
            "purpose": "Updates the global page state for one step of autoregressive decoding across all page groups.",
            "input": {
                "shape": "PageState object, ScalarInt (tokens_per_page), ScalarInt (max_pages_per_group)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Increment sequence lengths for active page groups.",
                "Calculate new active page positions.",
                "Determine which groups require new pages based on sequence length and page capacity.",
                "Conditionally allocate new pages globally for groups that need them.",
                "Update page status, page map, number of pages used, and active page for allocated groups.",
                "Return the updated PageState."
            ],
            "output": {
                "shape": "PageState object"
            },
            "dependencies": [
                "PageState",
                "jnp.where",
                "jnp.logical_and",
                "jax.lax.cond",
                "jax.lax.fori_loop",
                "_find_next_free_page_index"
            ],
            "parameters": {
                "tokens_per_page": "The capacity of each page in tokens.",
                "max_pages_per_group": "The maximum number of pages allowed per page group."
            },
            "notes": [
                "This function is designed to be JIT-compiled.",
                "It operates on all page groups simultaneously using JAX's functional programming constructs.",
                "Page allocation is conditional on the availability of free pages and the group's capacity."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageManager",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageManager:\n  \"\"\"Manages the global allocation and release of pages for paged attention.\n\n  This class provides an interface for reserving pages during prefill and\n  decoding, and for releasing pages when a sequence (page group) is complete.\n  It encapsulates the logic for tracking page allocation globally and managing\n  the `PageState`. It uses the concept of page groups, where each group typically\n  corresponds to a single request or sequence being processed.\n\n  Example:\n    ```python\n    # Initialize a PageManager from configuration\n    config = YourConfig(...) # Set pagedattn_num_pages, etc.\n    page_manager = PageManager(config)\n\n    # Get initial page state (all pages free, except potentially page 0)\n    state = page_manager.get_initial_page_state()\n\n    # Update pages for prefill of a sequence in group 0 with length 16\n    state = page_manager.update_prefill_pages(\n        page_state=state,\n        page_group_id=0,\n        true_length=16\n    )\n\n    # Update pages for a single decode step (increments lengths, allocates if needed)\n    state = page_manager.update_decode_pages(state)\n\n    # Release pages associated with group 0 when the sequence is finished\n    state = page_manager.release_pages(\n        page_state=state,\n        page_group_id=0\n    )\n    ```\n  \"\"\"\n\n  def __init__(self, config: Config):\n    \"\"\"Initializes the `PageManager` from a configuration object.\n\n    Args:\n      config: A `Config` object containing the necessary parameters:\n        * `max_target_length`: The maximum sequence length supported.\n        * `pagedattn_num_pages`: The total number of pages available globally.\n        * `pagedattn_tokens_per_page`: The number of tokens each page can hold.\n        * `global_batch_size_to_load`: Used to determine the maximum number of concurrent\n          page groups (`max_page_groups`) the system can manage.\n        * `pagedattn_max_pages_per_group`: The maximum number of pages that can be\n          allocated to a single page group.\n\n    Raises:\n      ValueError: If the configuration parameters are invalid (e.g., non-positive\n        values, insufficient pages per group for max length).\n    \"\"\"\n    self.num_pages: int = config.pagedattn_num_pages\n    self.tokens_per_page: int = config.pagedattn_tokens_per_page\n    self.max_target_length: int = config.max_target_length\n    self.max_page_groups: int = config.global_batch_size_to_load\n    self.max_pages_per_group: int = config.pagedattn_max_pages_per_group\n    self._validate_init_params()\n\n  def _validate_init_params(self) -> None:\n    \"\"\"Validates initialization parameters for logical consistency.\"\"\"\n    if self.max_pages_per_group <= 0:\n      raise ValueError(\"`pagedattn_max_pages_per_group` must be positive.\")\n    min_required = (self.max_target_length + self.tokens_per_page - 1) // self.tokens_per_page\n    if self.max_pages_per_group < min_required:\n      raise ValueError(\n          f\"`pagedattn_max_pages_per_group` ({self.max_pages_per_group}) is insufficient for `max_target_length` \"\n          f\"({self.max_target_length}). Needs {min_required}.\"\n      )\n    # Check > 1 due to potential page 0 workaround\n    if self.num_pages <= 1:\n      raise ValueError(\"`pagedattn_num_pages` must be greater than 1.\")\n    if self.tokens_per_page <= 0:\n      raise ValueError(\"`pagedattn_tokens_per_page` must be positive.\")\n    if self.max_page_groups <= 0:\n      raise ValueError(\"`pagedattn_max_page_groups` must be positive.\")\n\n  def update_prefill_pages(self, page_state: PageState, page_group_id: int, true_length: int) -> PageState:\n    \"\"\"Reserves pages for a specific page group during prefill (global state).\n\n    This method first releases any pages currently allocated to the given\n    `page_group_id`. It then attempts to allocate the necessary number of pages\n    from the global pool to accommodate a sequence of `true_length`. If successful,\n    it updates the `PageState` to reflect the new allocation and marks the group\n    as active. If there are not enough free pages globally or the group exceeds\n    its `max_pages_per_group` limit, the group's state remains cleared (as after\n    the initial release). Input validation ensures `page_group_id` and `true_length`\n    are within valid ranges.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to allocate pages for. Must\n        be between 0 and `max_page_groups - 1`.\n      true_length: The sequence length to allocate pages for. Must be between 0\n        and `max_target_length`.\n\n    Returns:\n      The updated `PageState`. If allocation fails due to resource limits, the\n      returned state will have the specified `page_group_id` cleared.\n\n    Raises:\n      ValueError: If `page_group_id` or `true_length` are outside their valid ranges.\n\n    Example:\n      ```python\n      # Reserve pages for a 16-token sequence in group 0\n      state = page_manager.update_prefill_pages(\n          page_state=state,\n          page_group_id=0,\n          true_length=16\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    if true_length <= 0 or true_length > self.max_target_length:\n      raise ValueError(f\"PageManager: true_length ({true_length}) out of range (0, {self.max_target_length}]\")\n\n    return _release_and_reserve_for_group(\n        page_state, page_group_id, true_length, self.tokens_per_page, self.max_pages_per_group\n    )\n\n  def update_decode_pages(self, page_state: PageState) -> PageState:\n    \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n    This method advances the state for all active page groups. It increments\n    their sequence lengths by one and updates their position within the current\n    active page. If this increment causes a sequence to cross a page boundary\n    (i.e., it needs more pages than currently allocated), this method attempts\n    to allocate a new page from the global pool, provided the group has not\n    reached its `max_pages_per_group` limit and free pages are available.\n\n    Args:\n      page_state: The current global `PageState`.\n\n    Returns:\n      The updated `PageState` reflecting the state after the decode step.\n      Sequence lengths and active positions are updated for all active groups.\n      Groups that required and successfully obtained a new page will have their\n      `num_pages_used`, `page_map`, and `active_page` updated.\n\n    Example:\n      ```python\n      # Advance state for all active sequences by one decode step\n      state = page_manager.update_decode_pages(state)\n      ```\n    \"\"\"\n    return _update_decode_pages_global(page_state, self.tokens_per_page, self.max_pages_per_group)\n\n  def release_pages(self, page_state: PageState, page_group_id: int) -> PageState:\n    \"\"\"Releases all pages associated with a given page group (global state).\n\n    This method identifies all pages currently allocated to the specified\n    `page_group_id` using the `page_map` and `num_pages_used`. It marks these\n    pages as free (status 0) in the global `page_status` array. It also resets\n    all state information specific to the `page_group_id` (sequence length,\n    page count, active status, etc.) to their initial zero/False values.\n    Input validation ensures the `page_group_id` is within the valid range.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to release. Must be\n        between 0 and `max_page_groups - 1`.\n\n    Returns:\n      The updated `PageState` after releasing the pages and resetting the group's\n      state.\n\n    Raises:\n      ValueError: If `page_group_id` is outside its valid range.\n\n    Example:\n      ```python\n      # Release all pages currently held by group 0\n      state = page_manager.release_pages(\n          page_state=state,\n          page_group_id=0\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    return _release_pages_for_group(page_state, page_group_id, self.max_pages_per_group)\n\n  def get_initial_page_state(self) -> PageState:\n    \"\"\"Creates and returns an initial global `PageState`.\n\n    This is a convenience method that calls `initialize_page_state` with\n    the parameters (`num_pages`, `max_page_groups`, `max_pages_per_group`)\n    stored during the `PageManager` initialization.\n\n    Returns:\n      An initialized `PageState` object where all pages are free (except possibly 0)\n      and no groups are active.\n\n    Example:\n      ```python\n      # Get a fresh, empty page state\n      initial_state = page_manager.get_initial_page_state()\n      ```\n    \"\"\"\n    return initialize_page_state(\n        num_pages=self.num_pages,\n        max_page_groups=self.max_page_groups,\n        max_pages_per_group=self.max_pages_per_group,\n    )",
        "analysis": {
            "module_type": "page_manager",
            "purpose": "Manages the global allocation and release of memory pages for paged attention, tracking page usage across multiple sequences (page groups).",
            "input": {
                "shape": "N/A (operates on PageState and configuration)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialization with configuration parameters.",
                "Validation of initialization parameters.",
                "Updating page allocations for prefill.",
                "Updating page allocations for decoding steps.",
                "Releasing all allocated pages for a specific page group.",
                "Providing an initial, empty page state."
            ],
            "output": {
                "shape": "N/A (returns updated PageState objects)"
            },
            "dependencies": [
                "Config",
                "PageState",
                "_release_and_reserve_for_group",
                "_update_decode_pages_global",
                "_release_pages_for_group",
                "initialize_page_state"
            ],
            "parameters": {
                "max_target_length": "The maximum sequence length supported.",
                "pagedattn_num_pages": "The total number of pages available globally.",
                "pagedattn_tokens_per_page": "The number of tokens each page can hold.",
                "global_batch_size_to_load": "Used to determine the maximum number of concurrent page groups.",
                "pagedattn_max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "Uses a PageState object to maintain global memory page status.",
                "Supports prefill (initial allocation) and decode (incremental allocation) operations.",
                "Handles page release when a sequence is complete.",
                "Includes validation to ensure configuration parameters are logical and sufficient.",
                "Page 0 is treated as a special case and is not available for general allocation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PageManager with configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration values to instance attributes.",
                        "Calls `_validate_init_params` to check parameter consistency."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "_validate_init_params"
                    ],
                    "notes": []
                },
                "_validate_init_params": {
                    "purpose": "Validates the initialization parameters of the PageManager for logical consistency.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if `max_pages_per_group` is positive.",
                        "Calculates minimum required pages for `max_target_length` and compares with `max_pages_per_group`.",
                        "Checks if `num_pages` is greater than 1.",
                        "Checks if `tokens_per_page` is positive.",
                        "Checks if `max_page_groups` is positive.",
                        "Raises `ValueError` if any validation fails."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Raises `ValueError` for invalid configurations."
                    ]
                },
                "update_prefill_pages": {
                    "purpose": "Reserves pages for a specific page group during the prefill phase.",
                    "input": {
                        "shape": "page_state: PageState, page_group_id: int, true_length: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates `page_group_id` and `true_length` against configured limits.",
                        "Calls `_release_and_reserve_for_group` to first release existing pages for the group and then reserve new ones.",
                        "Updates the `PageState` to reflect the new page allocations."
                    ],
                    "output": {
                        "shape": "PageState: The updated global page state."
                    },
                    "dependencies": [
                        "_release_and_reserve_for_group"
                    ],
                    "notes": [
                        "If allocation fails due to resource limits, the group's state is cleared.",
                        "Assumes `true_length` is positive."
                    ]
                },
                "update_decode_pages": {
                    "purpose": "Advances the state for all active page groups by one decoding step, allocating new pages if necessary.",
                    "input": {
                        "shape": "page_state: PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `_update_decode_pages_global` to handle the global update.",
                        "Increments sequence lengths for active groups.",
                        "Updates active page positions.",
                        "Allocates new pages if a sequence crosses a page boundary and resources are available."
                    ],
                    "output": {
                        "shape": "PageState: The updated global page state after one decode step."
                    },
                    "dependencies": [
                        "_update_decode_pages_global"
                    ],
                    "notes": []
                },
                "release_pages": {
                    "purpose": "Releases all pages associated with a given page group.",
                    "input": {
                        "shape": "page_state: PageState, page_group_id: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validates `page_group_id` against configured limits.",
                        "Calls `_release_pages_for_group` to free the pages and reset the group's state.",
                        "Updates the `PageState` to reflect the released pages and reset group information."
                    ],
                    "output": {
                        "shape": "PageState: The updated global page state after releasing pages."
                    },
                    "dependencies": [
                        "_release_pages_for_group"
                    ],
                    "notes": []
                },
                "get_initial_page_state": {
                    "purpose": "Creates and returns an initial global `PageState` object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `initialize_page_state` with the manager's configuration parameters.",
                        "Returns the newly created `PageState`."
                    ],
                    "output": {
                        "shape": "PageState: An initialized page state where all pages are free (except potentially page 0) and no groups are active."
                    },
                    "dependencies": [
                        "initialize_page_state"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#paged_attention_op_as_linen",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "def paged_attention_op_as_linen(\n    *,\n    mesh: Mesh,\n    num_pages: int,\n    tokens_per_page: int,\n    max_pages_per_slot: int,\n    max_pages_per_prefill: int,\n    pages_per_compute_block: int,\n    num_kv_heads: int,\n    kv_head_dim_size: int,\n    dtype: DType = jnp.float32,\n    attn_logits_soft_cap: float | None = None,\n    query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n    kv_pages_axis_names: AxisNames = (\n        \"paged_kv_heads\",\n        \"num_pages\",\n        \"tokens_per_page\",\n        \"paged_kv_head_dim_size\",\n    ),\n):\n  \"\"\"A factory function to create a PagedAttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `PagedAttentionOp`\n  within a Linen model. It wraps the `PagedAttentionOp` module using\n  `nnx.bridge.to_linen`, making it compatible with the Linen API. This is\n  useful for gradual migration of a codebase from Linen to NNX.\n\n  Args:\n    mesh: The device mesh for sharding.\n    num_pages: The total number of pages in the KV cache.\n    tokens_per_page: The number of tokens each page can hold.\n    max_pages_per_slot: The maximum number of pages a single sequence can use.\n    max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n    pages_per_compute_block: The number of pages processed in one kernel block.\n    num_kv_heads: The number of key/value heads.\n    kv_head_dim_size: The dimension of each key/value head.\n    dtype: The data type for computations.\n    attn_logits_soft_cap: The soft cap for attention logits.\n    query_axis_names: The logical axis names for the query tensor.\n    kv_pages_axis_names: The logical axis names for the KV cache pages.\n\n  Returns:\n    A Linen module that wraps the NNX `PagedAttentionOp` module.\n  \"\"\"\n\n  return nnx.bridge.to_linen(\n      PagedAttentionOp,\n      mesh=mesh,\n      num_pages=num_pages,\n      tokens_per_page=tokens_per_page,\n      max_pages_per_slot=max_pages_per_slot,\n      max_pages_per_prefill=max_pages_per_prefill,\n      pages_per_compute_block=pages_per_compute_block,\n      num_kv_heads=num_kv_heads,\n      kv_head_dim_size=kv_head_dim_size,\n      dtype=dtype,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      query_axis_names=query_axis_names,\n      kv_pages_axis_names=kv_pages_axis_names,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "functionality": "Creates a Flax Linen module that wraps an NNX-based PagedAttentionOp.",
            "usage": "This function acts as a factory to instantiate a PagedAttentionOp as a Linen module, facilitating gradual migration from Linen to NNX. It takes various configuration parameters related to the attention mechanism and KV cache management, and returns a Linen module compatible with Flax models."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#PagedAttentionOp",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "class PagedAttentionOp(nnx.Module):\n  \"\"\"An NNX module for paged attention.\n\n  This module implements the paged attention mechanism, which is an efficient\n  method for handling attention in autoregressive models with long sequences.\n  It divides the KV cache into fixed-size \"pages\" to manage memory dynamically.\n  \"\"\"\n\n  def __init__(\n      self,\n      mesh: Mesh,\n      num_pages: int,\n      tokens_per_page: int,\n      max_pages_per_slot: int,\n      max_pages_per_prefill: int,\n      pages_per_compute_block: int,\n      num_kv_heads: int,\n      kv_head_dim_size: int,\n      dtype: DType = jnp.float32,\n      attn_logits_soft_cap: float | None = None,\n      query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n      kv_pages_axis_names: AxisNames = (\n          \"paged_kv_heads\",\n          \"num_pages\",\n          \"tokens_per_page\",\n          \"paged_kv_head_dim_size\",\n      ),\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the PagedAttentionOp module.\n\n    Args:\n      mesh: The device mesh for sharding.\n      num_pages: The total number of pages in the KV cache.\n      tokens_per_page: The number of tokens each page can hold.\n      max_pages_per_slot: The maximum number of pages a single sequence can use.\n      max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n      pages_per_compute_block: The number of pages processed in one kernel block.\n      num_kv_heads: The number of key/value heads.\n      kv_head_dim_size: The dimension of each key/value head.\n      dtype: The data type for computations.\n      attn_logits_soft_cap: The soft cap for attention logits.\n      query_axis_names: The logical axis names for the query tensor.\n      kv_pages_axis_names: The logical axis names for the KV cache pages.\n      rngs: The random number generators for initialization (required by NNX).\n    \"\"\"\n\n    self.mesh = mesh\n    self.num_pages = num_pages\n    self.tokens_per_page = tokens_per_page\n    self.max_pages_per_slot = max_pages_per_slot\n    self.max_pages_per_prefill = max_pages_per_prefill\n    self.pages_per_compute_block = pages_per_compute_block\n    self.num_kv_heads = num_kv_heads\n    self.kv_head_dim_size = kv_head_dim_size\n    self.dtype = dtype\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.query_axis_names = query_axis_names\n    self.kv_pages_axis_names = kv_pages_axis_names\n\n    self.kv_pages_shape = (\n        self.num_kv_heads,\n        self.num_pages,\n        self.tokens_per_page,\n        self.kv_head_dim_size,\n    )\n\n    self.key_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n    self.value_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n\n  def _maybe_materialize_cache(self, cache: nnx.Cache) -> nnx.Cache:\n    \"\"\"Materializes the cache if it's currently a ShapeDtypeStruct.\"\"\"\n    if isinstance(cache.value, jax.ShapeDtypeStruct):\n      # This is needed because the Linen bridge lazily creates this state. We\n      # need to ensure the cache state is accessible at runtime.\n      # TODO: Delete this function when the to_linen bridge is no longer needed.\n      return nnx.Cache(\n          jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n          sharding=cache.sharding,\n      )\n    return cache\n\n  def get_kv_pages(self):\n    \"\"\"Retrieves the key and value page caches.\n\n    This method ensures the KV cache pages are materialized (if they are abstract\n    ShapeDtypeStructs, a temporary state during Linen bridge initialization) and\n    applies the necessary sharding constraints.\n\n    Returns:\n      A tuple containing the key pages and value pages caches (`nnx.Cache`).\n    \"\"\"\n\n    # TODO: Remove once to_linen bridge is no longer needed\n    self.key_pages = self._maybe_materialize_cache(self.key_pages)\n    self.value_pages = self._maybe_materialize_cache(self.value_pages)\n\n    self.key_pages.value = nn.with_logical_constraint(self.key_pages.value, self.kv_pages_axis_names)\n    self.value_pages.value = nn.with_logical_constraint(self.value_pages.value, self.kv_pages_axis_names)\n    return self.key_pages, self.value_pages\n\n  def pad_qkv(self, *qkv):\n    \"\"\"Pad input to kv_head_dim_size\"\"\"\n\n    def pad_to_kv_head_dim_size(x):\n      if x.shape[-1] != self.kv_head_dim_size:\n        return jnp.pad(\n            x,\n            ((0, 0), (0, 0), (0, 0), (0, self.kv_head_dim_size - x.shape[-1])),\n            mode=\"constant\",\n            constant_values=0.0,\n        )\n      else:\n        return x\n\n    # Align Q, K, V to the same head dim. This is required by the kernel.\n    return tuple(pad_to_kv_head_dim_size(x) for x in qkv)\n\n  def paged_dot_product_attention_with_max_and_sum(self, query, key, value):\n    \"\"\"paged dot product attention with max & sum\"\"\"\n    b, t, n, d = query.shape\n    _, s, n_kv, _ = key.shape\n    query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n\n    attn_weights = jnp.einsum(\"btkgd,bskd->bkgts\", query, key)\n\n    causal_mask = jnp.triu(jnp.ones((t, s)), k=1)\n    causal_mask = jnp.reshape(causal_mask, (1, 1, 1, t, s))\n    masked_weights = jnp.where(causal_mask, jnp.full_like(attn_weights, -1e10), attn_weights)\n\n    local_max = jnp.max(masked_weights, axis=-1, keepdims=True)\n    local_exps = jnp.exp(masked_weights - local_max)\n    local_sums = jnp.sum(local_exps, axis=-1, keepdims=True)\n\n    attn = jnp.einsum(\"bkgts,bskd->btkgd\", local_exps, value)\n    attn = jnp.reshape(attn, (b, t, n, d))\n\n    local_max = jnp.moveaxis(local_max, -2, 1)\n    local_max = jnp.reshape(local_max, (b, t, n, 1))\n\n    local_sums = jnp.moveaxis(local_sums, -2, 1)\n    local_sums = jnp.reshape(local_sums, (b, t, n, 1))\n\n    return attn, local_max, local_sums\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_prefill(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in prefill only. The assumption\n    is the batch_size is only 1\n    \"\"\"\n    assert query.shape[0] == 1  # ensure the batch size is 0\n    # shape of key_pages_cache.value is [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.array([0, page_state.sequence_lengths[0]])  # [0, prefill_true_length]\n    num_seqs = jnp.array([1])\n    query = query[0]  # [batch_size, max_num_tokens, num_kv_heads, head_dim] to [max_num_tokens, num_kv_heads, head_dim]\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,\n        kv_lens=jnp.array([query.shape[0]]),  # max_prefill_length\n        cu_q_lens=c_q_l,  # the accumulative real lengths of requests, starting from 0\n        page_indices=page_state.page_map,\n        num_seqs=num_seqs,\n        # TODO(rupliu) debug: repeated response when enabled below\n        # num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(result, axis=0)  # [batch_size, seq_len, n_kv_head, head_dim] and batch_size is 1 for now\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in decode only.\"\"\"\n    batch_size = query.shape[0]\n    query = jnp.squeeze(query, axis=1)  # [batch_size, seq_len, n_kv_head, head_dim] to [batch_size, n_kv_head, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.arange(batch_size + 1)  # one token per sequence\n    num_seqs = jnp.array([batch_size])  # real number of requests, set it to batch_size\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,  # [max_batched_num_tokens, num_kv_heads, head_dim]\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        kv_lens=page_state.sequence_lengths,  # [max_num_seqs]\n        cu_q_lens=c_q_l,  # [max_num_seqs+1]\n        page_indices=page_state.page_map,  # [max_num_seqs, pages_per_seq]\n        num_seqs=num_seqs,\n        num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(\n        result, axis=1\n    )  # [batch_size, n_kv_head, head_dim] to [batch_size, seq_len, n_kv_head, head_dim]\n\n  # v1 kernel has around 20% performance gain than v2 kernel in decode only task\n  def paged_attention_v1_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply Paged Attention v1 in decode only.\"\"\"\n    kv_pages_pspec = nn.logical_to_mesh_axes((\"paged_kv_heads\", None, None, None))\n    q_pspec = nn.logical_to_mesh_axes((None, None, \"paged_kv_heads\", None))\n\n    @functools.partial(\n        jax.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            q_pspec,\n            kv_pages_pspec,\n            kv_pages_pspec,\n            P(None),\n            P(None, None),\n            None,\n        ),\n        out_specs=q_pspec,\n        check_vma=False,\n    )\n    def wrap_paged_attention(q, k_pages, v_pages, lengths, page_indices, pages_per_compute_block):\n      q = jnp.squeeze(q, axis=1)\n      result = paged_attention_kernel.paged_attention(\n          q=q,  # [batch_size, num_heads, head_dim]\n          k_pages=k_pages,\n          v_pages=v_pages,\n          lengths=lengths,\n          page_indices=page_indices,\n          pages_per_compute_block=pages_per_compute_block,\n      )\n      return jnp.expand_dims(result, axis=1)  # [batch_size, n_kv_head, head_dim] to [batch_size, 1, n_kv_head, head_dim]\n\n    return wrap_paged_attention(\n        query,\n        key_pages_cache.value,\n        value_pages_cache.value,\n        page_state.sequence_lengths,\n        page_state.page_map,\n        self.pages_per_compute_block,\n    )\n\n  def __call__(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    \"\"\"Applies the paged attention mechanism.\n\n    This is the main entry point for the module. It takes query, key, and value\n    tensors and performs paged attention based on the current model mode\n    (prefill or autoregressive).\n\n    Args:\n      query: The query tensor.\n      key: The key tensor for the current step.\n      value: The value tensor for the current step.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The current operational mode, either 'prefill' or\n        'autoregressive'.\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n      slot: The batch slot index for the current request.\n      page_state: The current state of the page manager.\n\n    Returns:\n        A tuple (output, exponentials_max, exponentials_sum) containing:\n        - The attention output tensor.\n        - The max of the exponentials (for prefill mode with dot-product attention).\n        - The sum of the exponentials (for prefill mode with dot-product attention).\n        The latter two are None for autoregressive mode, as this is handled\n        internally by the paged attention kernel.\n    \"\"\"\n\n    key_pages_cache, value_pages_cache = self.get_kv_pages()\n    query, key, value = self.pad_qkv(query, key, value)\n\n    # update kv pages and call page attention kernel\n    if model_mode == MODEL_MODE_PREFILL:\n      self.update_prefill_step_pages(key_pages_cache, value_pages_cache, key, value, slot, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_prefill(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return self.paged_dot_product_attention_with_max_and_sum(query, key, value)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and page_state is not None:\n      self.update_decode_step_pages(key_pages_cache, value_pages_cache, key, value, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_decode(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return (\n          self.paged_attention_v1_decode(query, key_pages_cache, value_pages_cache, page_state),\n          None,\n          None,\n      )\n    else:\n      raise NotImplementedError(model_mode)\n\n  def update_prefill_step_pages(\n      self,\n      key_pages_cache: nnx.Cache,  # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n      value_pages_cache: nnx.Cache,\n      key: Array,\n      value: Array,\n      slot: int,\n      page_state: page_manager.PageState,\n  ) -> None:\n    \"\"\"Update pages for prefill step.\"\"\"\n    assert (\n        key.shape == value.shape\n    ), f\"prefill_step key/value should have the same shape, but getting {key.shape=} and {value.shape=} instead\"\n    batch_size, seq_len, n_kv_head, head_dim = key.shape\n    assert seq_len % self.tokens_per_page == 0, f\"seq_length {seq_len} and  tokens_per_page {self.tokens_per_page}\"\n    assert key_pages_cache.value.shape == value_pages_cache.value.shape, (\n        f\"prefill_step key/value_pages_cache should have the same shape, but \"\n        f\"getting {key_pages_cache.shape=} and {value_pages_cache.shape=} instead\"\n    )\n\n    v_n_kv, _, v_p, v_d = key_pages_cache.value.shape\n    assert v_n_kv == n_kv_head, f\"{v_n_kv=} {n_kv_head=}\"\n    assert v_p == self.tokens_per_page, f\"{v_p=} {self.tokens_per_page=}\"\n    assert v_d == head_dim, f\"{v_d=} {head_dim=}\"\n    assert page_state.page_map.shape == (\n        page_state.num_pages_used.shape[0],\n        self.max_pages_per_slot,\n    )\n\n    # Handle both init (b>1) and runtime (b=1) cases\n    if batch_size == 1:\n      key = jnp.squeeze(key)  # [batch_size, seq_len, n_kv_head, head_dim] to [seq_len, n_kv_head, head_dim]\n      value = jnp.squeeze(value)\n    else:\n      key = key[0]\n      value = value[0]\n\n    key = jnp.transpose(key, axes=(1, 0, 2))\n    value = jnp.transpose(value, axes=(1, 0, 2))\n\n    key = jnp.reshape(\n        key,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n    value = jnp.reshape(\n        value,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n\n    key_pages_cache.value = nn.with_logical_constraint(key, self.kv_pages_axis_names)\n    value_pages_cache.value = nn.with_logical_constraint(value, self.kv_pages_axis_names)\n\n  def update_decode_step_pages(self, key_pages_cache, value_pages_cache, key, value, page_state):\n    \"\"\"Update decode-step pages\"\"\"\n    key_pages = key_pages_cache.value\n    value_pages = value_pages_cache.value\n\n    batch_size, _, kv_heads, head_dim = key.shape\n    kv_heads, _, _, head_dim = key_pages.shape\n\n    new_key = key.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_key = jnp.transpose(new_key, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n    new_value = value.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_value = jnp.transpose(new_value, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n\n    broadcast_pages = jnp.tile(page_state.active_page, (kv_heads, 1))  # [n_kv_heads, batch_size]\n    broadcast_pos = jnp.tile(page_state.active_page_position, (kv_heads, 1))  # [n_kv_heads, batch_size]\n\n    kv_indices = jnp.arange(kv_heads)[:, None]  # [n_kv_heads, 1]\n    kv_indices = jnp.tile(kv_indices, (1, batch_size))  # [n_kv_heads, batch_size]\n\n    # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    key_pages_updated = key_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_key)\n    value_pages_updated = value_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_value)\n\n    key_pages_cache.value = key_pages_updated\n    value_pages_cache.value = value_pages_updated\n    return key_pages_cache, value_pages_cache",
        "analysis": {
            "module_type": "paged_attention_op",
            "purpose": "Implements the paged attention mechanism for efficient handling of attention in autoregressive models with long sequences by dynamically managing KV cache memory using fixed-size pages.",
            "input": {
                "shape": "query: [batch_size, sequence_length, num_heads, head_dim], key: [batch_size, sequence_length, num_kv_heads, head_dim], value: [batch_size, sequence_length, num_kv_heads, head_dim], decoder_segment_ids: [batch_size, sequence_length], model_mode: 'prefill' or 'autoregressive', previous_chunk: (optional), slot: (optional), page_state: (optional)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Retrieve and materialize KV page caches.",
                "Pad query, key, and value tensors to kv_head_dim_size.",
                "Conditionally update KV pages based on model mode (prefill or autoregressive).",
                "Apply paged attention using either v1 or v2 kernels based on `_use_kernel_v2` flag.",
                "Return attention output, and optionally max/sum of exponentials for prefill mode."
            ],
            "output": {
                "shape": "A tuple containing: (attention_output, exponentials_max, exponentials_sum). attention_output: [batch_size, sequence_length, num_heads, head_dim]. exponentials_max and exponentials_sum are None for autoregressive mode."
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "flax.linen",
                "flax.nnx",
                "MaxText.inference.page_manager",
                "MaxText.inference.paged_attention_kernel_v2",
                "MaxText.common_types",
                "functools",
                "jax.sharding"
            ],
            "parameters": {
                "mesh": "The device mesh for sharding.",
                "num_pages": "The total number of pages in the KV cache.",
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_slot": "The maximum number of pages a single sequence can use.",
                "max_pages_per_prefill": "The maximum number of pages for a prefill sequence.",
                "pages_per_compute_block": "The number of pages processed in one kernel block.",
                "num_kv_heads": "The number of key/value heads.",
                "kv_head_dim_size": "The dimension of each key/value head.",
                "dtype": "The data type for computations.",
                "attn_logits_soft_cap": "The soft cap for attention logits.",
                "query_axis_names": "The logical axis names for the query tensor.",
                "kv_pages_axis_names": "The logical axis names for the KV cache pages."
            },
            "notes": [
                "The module uses NNX's `nnx.Cache` for managing KV pages.",
                "It supports two versions of paged attention kernels: v1 and v2, controlled by `_use_kernel_v2`.",
                "The `__call__` method handles different `model_mode` ('prefill' or 'autoregressive').",
                "Helper methods `_maybe_materialize_cache`, `get_kv_pages`, `pad_qkv`, `paged_dot_product_attention_with_max_and_sum`, `paged_attention_v2_prefill`, `paged_attention_v2_decode`, `paged_attention_v1_decode`, `update_prefill_step_pages`, and `update_decode_step_pages` are used internally."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PagedAttentionOp module with configuration parameters for paged attention.",
                    "input": {
                        "shape": "N/A (constructor arguments)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns initialization parameters to instance attributes.",
                        "Defines the shape for KV pages.",
                        "Initializes `key_pages` and `value_pages` as `nnx.Cache` objects."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "flax.nnx",
                        "jax.sharding.Mesh",
                        "MaxText.common_types.DType",
                        "MaxText.common_types.AxisNames"
                    ],
                    "notes": [
                        "Requires `rngs` argument for NNX initialization, even if not directly used.",
                        "The `kv_pages_axis_names` parameter defines the sharding strategy for KV caches."
                    ]
                },
                "_maybe_materialize_cache": {
                    "purpose": "Ensures that the cache value is a concrete array and not a `ShapeDtypeStruct`.",
                    "input": {
                        "shape": "cache: nnx.Cache",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the cache value is a `jax.ShapeDtypeStruct`.",
                        "If it is, creates and returns a new `nnx.Cache` with a zero-initialized array of the correct shape and dtype.",
                        "Otherwise, returns the original cache."
                    ],
                    "output": {
                        "shape": "nnx.Cache"
                    },
                    "dependencies": [
                        "jax",
                        "flax.nnx"
                    ],
                    "notes": [
                        "This method is a workaround for lazy state creation in the Linen bridge and is marked for removal."
                    ]
                },
                "get_kv_pages": {
                    "purpose": "Retrieves the key and value page caches, ensuring they are materialized and have correct sharding constraints applied.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `_maybe_materialize_cache` for both `key_pages` and `value_pages`.",
                        "Applies logical constraints to the cache values using `nn.with_logical_constraint`.",
                        "Returns the key and value page caches."
                    ],
                    "output": {
                        "shape": "tuple(nnx.Cache, nnx.Cache)"
                    },
                    "dependencies": [
                        "flax.linen",
                        "flax.nnx"
                    ],
                    "notes": [
                        "This method is intended to be called before using the KV caches in attention computations."
                    ]
                },
                "pad_qkv": {
                    "purpose": "Pads the input query, key, and value tensors to match the `kv_head_dim_size`.",
                    "input": {
                        "shape": "*qkv: Array (tuple of query, key, value tensors)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through the input tensors (q, k, v).",
                        "For each tensor, checks if its last dimension (head dimension) matches `self.kv_head_dim_size`.",
                        "If not, pads the tensor along the last dimension with zeros.",
                        "Returns a tuple of the (potentially padded) q, k, v tensors."
                    ],
                    "output": {
                        "shape": "tuple(Array, Array, Array)"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This padding is required by the attention kernels."
                    ]
                },
                "paged_dot_product_attention_with_max_and_sum": {
                    "purpose": "Computes paged dot-product attention, including intermediate max and sum of exponentials.",
                    "input": {
                        "shape": "query: Array, key: Array, value: Array",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshapes query tensor.",
                        "Computes attention weights using einsum.",
                        "Creates and applies a causal mask.",
                        "Calculates local max and exponential values.",
                        "Computes the sum of exponentials.",
                        "Computes the attention output by applying local exponentials to value.",
                        "Reshapes and rearranges intermediate results (local_max, local_sums).",
                        "Returns attention output, local max, and local sums."
                    ],
                    "output": {
                        "shape": "tuple(Array, Array, Array)"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This appears to be a reference implementation or a fallback for prefill mode when kernel v2 is not used."
                    ]
                },
                "paged_attention_v2_prefill": {
                    "purpose": "Applies paged attention in prefill mode using the v2 kernel, assuming a batch size of 1.",
                    "input": {
                        "shape": "query: Array, key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, page_state: page_manager.PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Asserts batch size is 1.",
                        "Permutes dimensions of key and value pages caches.",
                        "Constructs cumulative query lengths (`cu_q_lens`).",
                        "Squeezes query tensor.",
                        "Calls `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Expands the result to include batch dimension."
                    ],
                    "output": {
                        "shape": "Array: Attention output tensor."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "flax.nnx",
                        "MaxText.inference.page_manager",
                        "MaxText.inference.paged_attention_kernel_v2"
                    ],
                    "notes": [
                        "Assumes `query.shape[0] == 1`.",
                        "TODO: Add sharding when SPMD is fully supported."
                    ]
                },
                "paged_attention_v2_decode": {
                    "purpose": "Applies paged attention in decode mode using the v2 kernel.",
                    "input": {
                        "shape": "query: Array, key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, page_state: page_manager.PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Squeezes query tensor.",
                        "Permutes dimensions of key and value pages caches.",
                        "Constructs cumulative query lengths (`cu_q_lens`).",
                        "Calls `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Expands the result to include a sequence length dimension of 1."
                    ],
                    "output": {
                        "shape": "Array: Attention output tensor."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "flax.nnx",
                        "MaxText.inference.page_manager",
                        "MaxText.inference.paged_attention_kernel_v2"
                    ],
                    "notes": [
                        "TODO: Add sharding when SPMD is fully supported."
                    ]
                },
                "paged_attention_v1_decode": {
                    "purpose": "Applies paged attention in decode mode using the v1 kernel.",
                    "input": {
                        "shape": "query: Array, key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, page_state: page_manager.PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Defines sharding specifications (`kv_pages_pspec`, `q_pspec`).",
                        "Defines a `wrap_paged_attention` function decorated with `jax.shard_map`.",
                        "Inside `wrap_paged_attention`: squeezes query, calls `paged_attention_kernel.paged_attention`, and expands result.",
                        "Calls `wrap_paged_attention` with appropriate arguments."
                    ],
                    "output": {
                        "shape": "Array: Attention output tensor."
                    },
                    "dependencies": [
                        "functools",
                        "jax",
                        "jax.numpy",
                        "flax.linen",
                        "flax.nnx",
                        "MaxText.inference.page_manager",
                        "MaxText.inference.paged_attention_kernel"
                    ],
                    "notes": [
                        "This method uses `jax.shard_map` for distributed execution.",
                        "The v1 kernel is noted to have a performance gain in decode-only tasks."
                    ]
                },
                "__call__": {
                    "purpose": "Main entry point to apply the paged attention mechanism based on the model mode.",
                    "input": {
                        "shape": "query: Array, key: Array, value: Array, decoder_segment_ids: Array, model_mode: str, previous_chunk: (optional), slot: (optional), page_state: (optional)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieves and materializes KV page caches.",
                        "Pads QKV tensors.",
                        "Conditional logic based on `model_mode`:",
                        "  - If 'prefill': updates prefill pages and calls either `paged_attention_v2_prefill` or `paged_dot_product_attention_with_max_and_sum`.",
                        "  - If 'autoregressive' and `page_state` is provided: updates decode pages and calls either `paged_attention_v2_decode` or `paged_attention_v1_decode`.",
                        "Raises `NotImplementedError` for unknown `model_mode`."
                    ],
                    "output": {
                        "shape": "tuple(Array, None, None) for autoregressive mode, or (Array, Array, Array) for prefill mode."
                    },
                    "dependencies": [
                        "MaxText.common_types.MODEL_MODE_PREFILL",
                        "MaxText.common_types.MODEL_MODE_AUTOREGRESSIVE"
                    ],
                    "notes": [
                        "The `decoder_segment_ids`, `previous_chunk`, and `slot` arguments are not explicitly used within the `__call__` method's logic shown, but are part of its signature."
                    ]
                },
                "update_prefill_step_pages": {
                    "purpose": "Updates the key and value pages cache for a single prefill step.",
                    "input": {
                        "shape": "key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, key: Array, value: Array, slot: int, page_state: page_manager.PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Performs assertions on input shapes.",
                        "Handles batch size 1 by squeezing, otherwise takes the first element.",
                        "Transposes and reshapes key and value tensors to match the page cache structure.",
                        "Updates `key_pages_cache.value` and `value_pages_cache.value` with the reshaped key and value data, applying logical constraints."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "flax.linen",
                        "flax.nnx",
                        "MaxText.inference.page_manager"
                    ],
                    "notes": [
                        "Assumes `seq_len` is divisible by `tokens_per_page`.",
                        "Handles both initialization (batch_size > 1) and runtime (batch_size == 1) cases for prefill."
                    ]
                },
                "update_decode_step_pages": {
                    "purpose": "Updates the key and value pages cache for a single decode step.",
                    "input": {
                        "shape": "key_pages_cache: nnx.Cache, value_pages_cache: nnx.Cache, key: Array, value: Array, page_state: page_manager.PageState",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshapes and transposes new key and value tensors.",
                        "Tiles `page_state.active_page` and `page_state.active_page_position`.",
                        "Creates KV indices.",
                        "Uses `key_pages.at[...].set(...)` and `value_pages.at[...].set(...)` to update the KV pages cache with the new key and value data at the correct page and position.",
                        "Updates `key_pages_cache.value` and `value_pages_cache.value`."
                    ],
                    "output": {
                        "shape": "tuple(nnx.Cache, nnx.Cache)"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "flax.nnx",
                        "MaxText.inference.page_manager"
                    ],
                    "notes": [
                        "This method directly modifies the `key_pages_cache` and `value_pages_cache`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#MultiPageAsyncCopyDescriptor",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "class MultiPageAsyncCopyDescriptor:\n  \"\"\"Descriptor for async copy of multiple K/V pages from HBM.\"\"\"\n\n  def __init__(\n      self,\n      pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads_per_blk, head_dim]\n      vmem_buf,  # [num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n      sem,\n      page_indices_ref,  # i32[max_num_seqs, pages_per_seq]\n      offset,  # [seq_idx, kv_pages_start]\n  ):\n    self._vmem_buf = vmem_buf\n    seq_id, kv_pages_start = offset\n    self._async_copies = [\n        pltpu.make_async_copy(\n            pages_hbm_ref.at[page_indices_ref[seq_id, kv_pages_start + i]],\n            vmem_buf.at[i],\n            sem,\n        )\n        for i in range(vmem_buf.shape[0])\n    ]\n\n  def start(self):\n    \"\"\"Starts the async copies.\"\"\"\n    for async_copy in self._async_copies:\n      async_copy.start()\n\n  def wait(self):\n    for async_copy in self._async_copies:\n      async_copy.wait()\n    return self._vmem_buf",
        "analysis": {
            "module_type": "multi_page_async_copy_descriptor",
            "purpose": "Manages asynchronous copying of multiple key/value pages from HBM to VMEM.",
            "input": {
                "shape": "N/A (parameters are passed during initialization)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes with references to HBM pages, VMEM buffer, semaphore, page indices, and an offset.",
                "Creates a list of `pltpu.make_async_copy` operations for each page to be copied.",
                "The `start` method initiates all asynchronous copy operations.",
                "The `wait` method blocks until all copy operations are complete and returns the VMEM buffer."
            ],
            "output": {
                "shape": "The shape of the `vmem_buf` provided during initialization."
            },
            "dependencies": [
                "pltpu.make_async_copy",
                "pltpu.at"
            ],
            "parameters": {
                "pages_hbm_ref": "Reference to the source key/value pages in HBM.",
                "vmem_buf": "The destination buffer in VMEM.",
                "sem": "Semaphore to synchronize asynchronous operations.",
                "page_indices_ref": "Indices mapping sequences to pages.",
                "offset": "Specifies the sequence ID and starting page index for the copy operation."
            },
            "notes": [
                "This class is designed to handle the asynchronous transfer of data for paged attention mechanisms.",
                "It leverages TPU-specific asynchronous copy operations (`pltpu.make_async_copy`)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the descriptor with source and destination buffers, semaphore, and indexing information.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the `vmem_buf`.",
                        "Extracts `seq_id` and `kv_pages_start` from the `offset`.",
                        "Iterates through the number of pages to copy (determined by `vmem_buf.shape[0]`).",
                        "For each page, creates an `async_copy` object using `pltpu.make_async_copy`, specifying the source from `pages_hbm_ref` and destination in `vmem_buf`, along with the semaphore."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "pltpu.make_async_copy",
                        "pltpu.at"
                    ],
                    "notes": [
                        "The indexing `pages_hbm_ref.at[page_indices_ref[seq_id, kv_pages_start + i]]` is crucial for selecting the correct source page.",
                        "The destination `vmem_buf.at[i]` implies that the `vmem_buf` is structured to receive pages sequentially."
                    ]
                },
                "start": {
                    "purpose": "Initiates all the asynchronous copy operations that were set up during initialization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through the `_async_copies` list.",
                        "Calls the `start()` method on each `async_copy` object."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "This method triggers the data transfer without waiting for completion."
                    ]
                },
                "wait": {
                    "purpose": "Waits for all initiated asynchronous copy operations to complete and returns the destination buffer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterates through the `_async_copies` list.",
                        "Calls the `wait()` method on each `async_copy` object.",
                        "Returns the `_vmem_buf` after all waits are finished."
                    ],
                    "output": {
                        "shape": "The shape of the `_vmem_buf`."
                    },
                    "dependencies": [],
                    "notes": [
                        "This method ensures that the data has been successfully transferred to the VMEM buffer before proceeding."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ref_ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ref_ragged_paged_attention(\n    queries: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1],\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n):\n  \"\"\"Ref ragged paged attention.\"\"\"\n  _, _, num_kv_heads, head_dim = k_pages.shape\n  num_q_heads = queries.shape[1]\n  assert num_q_heads % num_kv_heads == 0\n  num_query_per_kv = num_q_heads // num_kv_heads\n  outputs = []\n  for i in range(num_seqs[0]):\n    q_start = cu_q_lens[i]\n    q_end = cu_q_lens[i + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens[i]\n    indices = page_indices[i]\n    q = queries[q_start:q_end]\n    k = k_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    v = v_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    k = jnp.repeat(k, num_query_per_kv, axis=1)\n    v = jnp.repeat(v, num_query_per_kv, axis=1)\n    attn = jnp.einsum(\"qhd,khd->hqk\", q, k, preferred_element_type=jnp.float32)\n    attn *= sm_scale\n    q_span = (kv_len - q_len) + jax.lax.broadcasted_iota(jnp.int32, attn.shape, 1)\n    kv_span = jax.lax.broadcasted_iota(jnp.int32, attn.shape, 2)\n    attn += jnp.where(q_span < kv_span, mask_value, 0.0)\n    attn = jax.nn.softmax(attn, axis=-1).astype(v.dtype)\n    out = jnp.einsum(\"hqk,khd->qhd\", attn, v).astype(queries.dtype)\n    outputs.append(out)\n\n  return jnp.concatenate(outputs, axis=0)",
        "analysis": {
            "functionality": "Performs ragged paged attention, a key operation in transformer models for efficient sequence processing, especially during mixed prefill and decoding phases.",
            "usage": "This function takes query, key, and value tensors, along with sequence length and page index information, to compute attention scores and produce output tensors. It is designed for optimized performance on TPUs and handles variable sequence lengths by leveraging paged memory for key and value caches."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#validate_inputs_on_runtime",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def validate_inputs_on_runtime(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"validate inputs on runtime\"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  max_num_batched_tokens = q.shape[0]\n  page_size = k_pages.shape[1]\n  max_num_seqs, pages_per_seq = page_indices.shape\n  if num_seqs[0] > max_num_seqs:\n    raise ValueError(f\"{num_seqs[0]=} must be less or equal to {max_num_seqs=}\")\n  max_kv_len = jnp.max(kv_lens)\n  min_pages_per_seq = ceil_div(max_kv_len, page_size)\n  if pages_per_seq < min_pages_per_seq:\n    raise ValueError(\n        f\"{pages_per_seq=} must be greater or equal to\" f\" {min_pages_per_seq=} given {max_kv_len=} and {page_size=}.\"\n    )\n  if cu_q_lens[num_seqs[0]] > max_num_batched_tokens:\n    raise ValueError(f\"Total q tokens {cu_q_lens[num_seqs[0]]} must be less or equal to\" f\" {max_num_batched_tokens=}.\")\n  for i in range(num_seqs[0]):\n    q_len = cu_q_lens[i + 1] - cu_q_lens[i]\n    kv_len = kv_lens[i]\n    if q_len > kv_len:\n      raise ValueError(f\"{q_len=} must be less or equal to {kv_len=} at sequence {i}.\")",
        "analysis": {
            "module_type": "input_validation",
            "purpose": "Validates the shapes and dimensions of input tensors for a paged attention mechanism at runtime.",
            "input": {
                "shape": "q: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "jax.Array for q, k_pages, v_pages; jax.Array (int32) for kv_lens, page_indices, cu_q_lens, num_seqs"
            },
            "processing_steps": [
                "Calls `check_inputs_shapes` to perform initial shape validation.",
                "Extracts dimensions like `max_num_batched_tokens`, `page_size`, `max_num_seqs`, and `pages_per_seq` from input shapes.",
                "Checks if `num_seqs` exceeds `max_num_seqs`.",
                "Calculates the minimum required `pages_per_seq` based on `max_kv_len` and `page_size`.",
                "Checks if the actual `pages_per_seq` is less than the minimum required.",
                "Validates that the total number of query tokens (`cu_q_lens[num_seqs[0]]`) does not exceed `max_num_batched_tokens`.",
                "Iterates through each sequence to check if the query length (`q_len`) is less than or equal to the key-value length (`kv_len`)."
            ],
            "output": {
                "shape": "N/A (raises ValueError on invalid input)"
            },
            "dependencies": [
                "jax.Array",
                "jax.numpy as jnp",
                "ceil_div",
                "check_inputs_shapes"
            ],
            "parameters": {},
            "notes": [
                "This function is intended to be called at runtime to catch input errors before computation.",
                "It assumes that `check_inputs_shapes` has already been called or performs similar checks.",
                "Raises `ValueError` with descriptive messages if any validation fails."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#check_inputs_shapes",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def check_inputs_shapes(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"check shapes of inputs\"\"\"\n  _, num_q_heads, head_dim = q.shape\n  _, _, num_kv_heads, head_dim_k = k_pages.shape\n  max_num_seqs, _ = page_indices.shape\n  if num_seqs.shape != (1,):\n    raise ValueError(f\"{num_seqs.shape=} must be (1,)\")\n  if k_pages.shape != v_pages.shape:\n    raise ValueError(f\"{k_pages.shape=} and {v_pages.shape=} must have the same shape.\")\n  if head_dim_k != head_dim:\n    raise ValueError(f\"Q head_dim {head_dim} must be the same as that of K/V {head_dim_k}.\")\n  if kv_lens.shape != (max_num_seqs,):\n    raise ValueError(\n        f\"Expected {kv_lens.shape=} to be ({max_num_seqs},) where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if cu_q_lens.shape != (max_num_seqs + 1,):\n    raise ValueError(\n        f\"Expected {cu_q_lens.shape=} to be ({max_num_seqs + 1},)  where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if kv_lens.dtype != jnp.int32 or page_indices.dtype != jnp.int32 or cu_q_lens.dtype != jnp.int32:\n    raise ValueError(\n        \"The dtype of `kv_lens`, `page_indices`, and `cu_q_lens` must be\"\n        f\" int32. Got {kv_lens.dtype=}, {page_indices.dtype=},\"\n        f\" {cu_q_lens.dtype=}.\"\n    )\n  if num_q_heads % num_kv_heads != 0:\n    raise ValueError(f\"{num_q_heads=} must be divisible by {num_kv_heads=}\")",
        "analysis": {
            "functionality": "Checks the shapes and data types of input arrays for a ragged paged attention mechanism.",
            "usage": "This function is intended to be called with JAX arrays representing queries, key-value pages, sequence lengths, page indices, cumulative query lengths, and the number of sequences. It validates that these inputs conform to expected shapes and dtypes to prevent runtime errors in subsequent operations. It raises a ValueError if any input does not meet the specified criteria."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention_kernel",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention_kernel(\n    # Prefetch\n    kv_lens_ref,  # [max_num_seqs]\n    page_indices_ref,  # [max_num_seqs, pages_per_seq]\n    cu_q_lens_ref,  # [max_num_seqs + 1]\n    seq_buf_idx_ref,\n    # TODO(jevinjiang): if OOM in SMEM, consider pack to other scalar refs.\n    num_seqs_ref,\n    # Input\n    q_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    k_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    # Output\n    o_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    # Scratch\n    k_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    v_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    sems,  # [2, 2]\n    l_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    m_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    *,\n    sm_scale: float,\n    mask_value: float,\n):\n  \"\"\"ragged paged-attention kernel\"\"\"\n  num_q_per_blk, num_q_heads_per_blk, head_dim = q_ref.shape\n  num_seqs = num_seqs_ref[0]\n  _, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, _ = k_bufs.shape\n  num_kv_per_blk = num_kv_pages_per_blk * page_size\n  num_q_heads_per_kv_head = num_q_heads_per_blk // num_kv_heads_per_blk\n  heads_blk_idx, q_blk_idx = (\n      pl.program_id(0),\n      pl.program_id(1),\n  )\n  num_heads_blks = pl.num_programs(0)\n  init_seq_idx = seq_buf_idx_ref[0]\n  init_buf_idx = seq_buf_idx_ref[1]\n  q_len_start = q_blk_idx * num_q_per_blk\n  q_len_end = q_len_start + num_q_per_blk\n\n  def create_kv_async_copy_descriptors(heads_blk_idx, seq_idx, kv_blk_idx, buf_idx):\n    offset = (seq_idx, kv_blk_idx * num_kv_pages_per_blk)\n    heads_start = heads_blk_idx * num_kv_heads_per_blk\n    async_copy_k = MultiPageAsyncCopyDescriptor(\n        k_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        k_bufs.at[buf_idx],\n        sems.at[buf_idx, 0],\n        page_indices_ref,\n        offset,\n    )\n    async_copy_v = MultiPageAsyncCopyDescriptor(\n        v_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        v_bufs.at[buf_idx],\n        sems.at[buf_idx, 1],\n        page_indices_ref,\n        offset,\n    )\n    return async_copy_k, async_copy_v\n\n  # TODO(jevinjiang): Add these to Mosaic:\n  # 1. Support arbitrary strided load/store for any dtype.\n  # 2. Support arbitrary strided load/store for any last dimension.\n  def strided_load_kv(ref, start, step):\n    if ref.dtype == jnp.float32:\n      return ref[start::step, :]\n    packing = get_dtype_packing(ref.dtype)\n    assert ref.dtype == jnp.bfloat16\n    assert step % packing == 0\n    b_start = start // packing\n    b_offset = start % packing\n    b_step = step // packing\n    b_ref = ref.bitcast(jnp.int32)\n    b = b_ref[b_start::b_step, :]\n    bw = 32 // packing\n    b = jnp.right_shift(b, bw * b_offset)\n    b = jnp.left_shift(b, bw * (packing - 1))\n    return pltpu.bitcast(b, jnp.float32).astype(jnp.bfloat16)\n\n  def fold_on_2nd_minor(vec):\n    assert vec.dtype in (jnp.bfloat16, jnp.float32)\n    assert len(vec.shape) >= 2\n    last_dim = vec.shape[-1]\n    packing = get_dtype_packing(vec.dtype)\n    if vec.shape[-2] % packing != 0:\n      vec = vec.astype(jnp.float32)\n    return vec.reshape(-1, last_dim)\n\n  @pl.when(heads_blk_idx + q_blk_idx == 0)\n  def prefetch_first_kv_blk():\n    async_copy_k, async_copy_v = create_kv_async_copy_descriptors(heads_blk_idx, init_seq_idx, 0, init_buf_idx)\n    async_copy_k.start()\n    async_copy_v.start()\n\n  def is_cur_q_blk_needed(q_states):\n    done, cur_seq_idx, _ = q_states\n    return jnp.logical_and(done == 0, cur_seq_idx < num_seqs)\n\n  def compute_with_cur_q_blk(q_states):\n    done, cur_seq_idx, cur_buf_idx = q_states\n    q_start = cu_q_lens_ref[cur_seq_idx]\n    q_end = cu_q_lens_ref[cur_seq_idx + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens_ref[cur_seq_idx]\n\n    def get_next_prefetch_ids(heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx):\n      next_kv_blk_idx = kv_blk_idx + 1\n      is_last_kv_blk = next_kv_blk_idx * num_kv_per_blk >= kv_len\n      next_kv_blk_idx = lax.select(\n          is_last_kv_blk,\n          0,\n          next_kv_blk_idx,\n      )\n      is_cur_seq_end_in_cur_q_blk = q_end <= q_len_end\n      next_seq_idx = lax.select(\n          is_last_kv_blk,\n          lax.select(is_cur_seq_end_in_cur_q_blk, cur_seq_idx + 1, cur_seq_idx),\n          cur_seq_idx,\n      )\n      is_last_seq = next_seq_idx == num_seqs\n      next_seq_idx = lax.select(\n          is_last_seq,\n          0,\n          next_seq_idx,\n      )\n      next_heads_blk_idx = lax.select(\n          is_last_seq,\n          heads_blk_idx + 1,\n          heads_blk_idx,\n      )\n      next_buf_idx = lax.select(cur_buf_idx == 0, 1, 0)\n      return next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n\n    def flash_attention(\n        q,  # [num_q_per_blk * num_q_heads_per_kv_head, head_dim]\n        k,  # [num_kv_per_blk, head_dim]\n        v,  # [num_kv_per_blk, head_dim]\n        head_l_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_m_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_o_ref,  # [num_q_per_blk, num_q_heads_per_kv_head, head_dim]\n        *,\n        kv_blk_idx,\n    ):\n      assert q.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          head_dim,\n      )\n      assert k.shape == (\n          num_kv_per_blk,\n          head_dim,\n      ), f\"{k.shape=}, {(num_kv_per_blk, head_dim)=} {k.dtype=}\"\n      assert v.shape == (num_kv_per_blk, head_dim)\n      assert head_m_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_l_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_o_ref.shape == (\n          num_q_per_blk,\n          num_q_heads_per_kv_head,\n          head_dim,\n      )\n      kv_len_start = kv_blk_idx * num_kv_per_blk\n\n      def masked_store(ref, val, start, end, group=1):\n        iota = lax.broadcasted_iota(jnp.int32, ref.shape, 0) // group\n        mask = jnp.logical_and(iota >= start, iota < end)\n        pl.store(ref, tuple(slice(None) for _ in ref.shape), val, mask=mask)\n\n      qk = jnp.einsum(\"nd,md->nm\", q, k, preferred_element_type=jnp.float32) * sm_scale\n      store_start = jnp.maximum(q_start - q_len_start, 0)\n      store_end = jnp.minimum(q_end - q_len_start, num_q_per_blk)\n\n      @pl.when(kv_blk_idx == 0)\n      def init_scratch_ref():\n        masked_store(\n            head_m_ref,\n            jnp.full_like(head_m_ref, -jnp.inf),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_l_ref,\n            jnp.zeros_like(head_l_ref),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_o_ref,\n            jnp.zeros_like(head_o_ref),\n            store_start,\n            store_end,\n        )\n\n      row_ids = (\n          (kv_len - q_len)\n          + q_len_start\n          - q_start\n          + jax.lax.broadcasted_iota(\n              jnp.int32,\n              (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n              0,\n          )\n          // num_q_heads_per_kv_head\n      )\n      col_ids = kv_len_start + jax.lax.broadcasted_iota(\n          jnp.int32,\n          (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n          1,\n      )\n      causal_mask = row_ids < col_ids\n      qk += jnp.where(causal_mask, mask_value, 0.0)\n      m_curr = jnp.max(qk, axis=1, keepdims=True)\n      s_curr = jnp.exp(qk - m_curr)\n      qkv = jnp.dot(s_curr, v, preferred_element_type=jnp.float32)\n      lm_store_shape = head_m_ref.shape\n      m_curr = jnp.broadcast_to(m_curr, lm_store_shape)\n      l_curr = jnp.broadcast_to(s_curr.sum(axis=1, keepdims=True), lm_store_shape)\n      m_prev = head_m_ref[...]\n      l_prev = head_l_ref[...]\n      m_next = jnp.maximum(m_prev, m_curr)\n      masked_store(head_m_ref, m_next, store_start, store_end, num_q_heads_per_kv_head)\n      alpha = jnp.exp(m_prev - m_next)\n      beta = jnp.exp(m_curr - m_next)\n      l_alpha = alpha * l_prev\n      l_next = l_alpha + beta * l_curr\n      l_next_safe = jnp.where(l_next == 0.0, 1.0, l_next)\n      masked_store(\n          head_l_ref,\n          l_next_safe,\n          store_start,\n          store_end,\n          num_q_heads_per_kv_head,\n      )\n\n      def broadcast_to_shape(arr, shape):\n        if arr.shape == shape:\n          return arr\n        assert len(arr.shape) == len(shape)\n        assert arr.shape[0] == shape[0]\n        assert shape[1] % arr.shape[1] == 0\n        # no-op concatenation.\n        return jnp.concatenate([arr for _ in range(shape[1] // arr.shape[1])], axis=1)\n\n      o_curr = head_o_ref[...].reshape(-1, head_dim)\n      l_alpha = broadcast_to_shape(l_alpha, qkv.shape)\n      beta = broadcast_to_shape(beta, qkv.shape)\n      l_next_safe = broadcast_to_shape(l_next_safe, qkv.shape)\n      out = lax.div(\n          l_alpha * o_curr + beta * qkv,\n          l_next_safe,\n      ).astype(head_o_ref.dtype)\n      masked_store(\n          head_o_ref,\n          out.reshape(head_o_ref.shape),\n          store_start,\n          store_end,\n      )\n\n    def is_valid_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, _ = kv_states\n      return kv_blk_idx * num_kv_per_blk < kv_len\n\n    def compute_with_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, cur_buf_idx = kv_states\n      next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx = get_next_prefetch_ids(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n\n      @pl.when(next_heads_blk_idx < num_heads_blks)\n      def prefetch_next_kv_blk():\n        # TODO(jevinjiang): reuse the same buffer if it is already prefetched!\n        # TODO(jevinjiang): only fetch effective dynamic size to hold kv_len and\n        # DMA to fixed size buffer!\n        next_async_copy_k, next_async_copy_v = create_kv_async_copy_descriptors(\n            next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n        )\n        next_async_copy_k.start()\n        next_async_copy_v.start()\n\n      cur_async_copy_k, cur_async_copy_v = create_kv_async_copy_descriptors(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n      kv_to_load_shape = (\n          num_kv_pages_per_blk * page_size * num_kv_heads_per_blk,\n          head_dim,\n      )\n      k_ref = cur_async_copy_k.wait().reshape(kv_to_load_shape)\n      v_ref = cur_async_copy_v.wait().reshape(kv_to_load_shape)\n      for kv_head_idx in range(num_kv_heads_per_blk):\n        q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n        # TODO(jevinjiang): extra handling for packed type that can start at\n        # unaligned position!\n        q = fold_on_2nd_minor(q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :])\n        k = strided_load_kv(k_ref, kv_head_idx, num_kv_heads_per_blk)\n        v = strided_load_kv(v_ref, kv_head_idx, num_kv_heads_per_blk)\n        flash_attention(\n            q,\n            k,\n            v,\n            l_ref.at[kv_head_idx],\n            m_ref.at[kv_head_idx],\n            o_ref.at[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :],\n            kv_blk_idx=kv_blk_idx,\n        )\n      return kv_blk_idx + 1, next_buf_idx\n\n    _, next_buf_idx = lax.while_loop(\n        is_valid_kv_blk_in_cur_seq,\n        compute_with_kv_blk_in_cur_seq,\n        (0, cur_buf_idx),  # (kv_blk_idx, buf_idx)\n    )\n    next_seq_idx = lax.select(q_end <= q_len_end, cur_seq_idx + 1, cur_seq_idx)\n    done = lax.select(q_end < q_len_end, done, 1)\n    return done, next_seq_idx, next_buf_idx\n\n  _, seq_idx, buf_idx = lax.while_loop(\n      is_cur_q_blk_needed,\n      compute_with_cur_q_blk,\n      (0, init_seq_idx, init_buf_idx),  # (done, seq_idx, buf_idx)\n  )\n  # Reset seq_idx for next kv_heads_blk if run out of seqs!\n  seq_buf_idx_ref[0] = lax.select(seq_idx < num_seqs, seq_idx, 0)\n  seq_buf_idx_ref[1] = buf_idx",
        "analysis": {
            "functionality": "This function implements a highly optimized kernel for ragged paged attention, designed for TPUs. It efficiently handles attention calculations for sequences of varying lengths by loading key and value data in pages from HBM to VMEM, performing attention, and writing results back. It supports mixed prefill and decoding scenarios.",
            "usage": "This kernel is intended to be called within a larger attention mechanism. It takes query, key, and value data, along with sequence length information and page indices, as input. It requires pre-allocated scratch space for intermediate computations and asynchronous data transfers. The output is the computed attention result. Key parameters include `sm_scale` for softmax scaling and `mask_value` for causal masking."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ceil_div",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ceil_div(a, b):\n  assert b != 0\n  return (a + b - 1) // b",
        "analysis": {
            "functionality": "Calculates the ceiling division of two integers.",
            "usage": "Call the function `ceil_div(a, b)` with two integer arguments `a` and `b`. It returns the smallest integer greater than or equal to `a / b`. An assertion ensures that `b` is not zero."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_dtype_packing",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_dtype_packing(dtype):\n  if dtype == jnp.float32:\n    return 1\n  if dtype == jnp.bfloat16:\n    return 2\n  if dtype == jnp.int8:\n    return 4\n  if dtype == jnp.int4:\n    return 8\n  raise ValueError(f\"Not implemented: unsupported {dtype=}\")",
        "analysis": {
            "module_type": "dtype_packing_utility",
            "purpose": "Determines an integer packing factor based on the input data type.",
            "input": {
                "shape": "N/A",
                "dtype": "A JAX dtype (e.g., jnp.float32, jnp.bfloat16)."
            },
            "processing_steps": [
                "Checks if the input dtype is jnp.float32 and returns 1.",
                "Checks if the input dtype is jnp.bfloat16 and returns 2.",
                "Checks if the input dtype is jnp.int8 and returns 4.",
                "Checks if the input dtype is jnp.int4 and returns 8.",
                "Raises a ValueError if the dtype is not supported."
            ],
            "output": {
                "shape": "N/A",
                "dtype": "int32"
            },
            "dependencies": [
                "jax.numpy as jnp"
            ],
            "parameters": {},
            "notes": [
                "This function is likely used to determine how many elements of a certain dtype can fit into a larger integer type (e.g., int32) for optimized memory access or computation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_min_heads_per_blk",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_min_heads_per_blk(num_q_heads, num_kv_heads, q_dtype, kv_dtype):\n  \"\"\"get min heads per block\"\"\"\n  q_packing = get_dtype_packing(q_dtype)\n  kv_packing = get_dtype_packing(kv_dtype)\n\n  def can_be_xla_fully_tiled(x, packing):\n    if x % packing != 0:\n      return False\n    x //= packing\n    return x in (1, 2, 4, 8) or x % 8 == 0\n\n  # TODO(jevinjiang): support unaligned number of heads!\n  if not can_be_xla_fully_tiled(num_kv_heads, kv_packing):\n    raise ValueError(f\"Not implemented: {num_kv_heads=} can not be XLA fully tiled.\")\n  assert num_q_heads % num_kv_heads == 0\n  ratio = num_q_heads // num_kv_heads\n  # TODO(jevinjiang): we can choose smaller tiling for packed type if large\n  # second minor tiling is not on.\n  max_kv_tiling = 8 * kv_packing\n  min_kv_heads = max_kv_tiling if num_kv_heads % max_kv_tiling == 0 else num_kv_heads\n  min_q_heads = min_kv_heads * ratio\n  if can_be_xla_fully_tiled(min_q_heads, q_packing):\n    return min_q_heads, min_kv_heads\n  return num_q_heads, num_kv_heads",
        "analysis": {
            "module_type": "get_min_heads_per_blk",
            "purpose": "Calculates the minimum number of query and key-value heads per block for XLA fully tiled execution, considering data type packing.",
            "input": {
                "shape": "[N/A]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine data type packing for query and key-value heads.",
                "Define a helper function `can_be_xla_fully_tiled` to check if a number of heads can be XLA fully tiled based on packing.",
                "Validate if the number of key-value heads can be XLA fully tiled.",
                "Calculate the ratio between query and key-value heads.",
                "Determine a maximum tiling size for key-value heads.",
                "Calculate the minimum key-value heads based on the maximum tiling size.",
                "Calculate the minimum query heads based on the minimum key-value heads and the ratio.",
                "Check if the calculated minimum query heads can be XLA fully tiled.",
                "Return the minimum query and key-value heads if they can be fully tiled, otherwise return the original numbers."
            ],
            "output": {
                "shape": "[2,]",
                "dtype": "N/A"
            },
            "dependencies": [
                "get_dtype_packing"
            ],
            "parameters": {
                "num_q_heads": "The total number of query heads.",
                "num_kv_heads": "The total number of key-value heads.",
                "q_dtype": "The data type of the query heads.",
                "kv_dtype": "The data type of the key-value heads."
            },
            "notes": [
                "Includes a TODO comment indicating that support for unaligned number of heads is not yet implemented.",
                "Includes a TODO comment about potentially choosing smaller tiling for packed types.",
                "Raises a ValueError if `num_kv_heads` cannot be XLA fully tiled.",
                "Asserts that `num_q_heads` is divisible by `num_kv_heads`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    # TODO(jevinjiang): create a write_to_kv_cache kernel!\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1]\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n    num_kv_pages_per_block: int = 16,\n    num_queries_per_block: int = 128,\n    vmem_limit_bytes: int | None = None,\n):\n  \"\"\"Ragged paged attention that supports mixed prefill and decode.\n\n  Args:\n    q: concatenated all sequences' queries.\n    k_pages: paged K cache. Normally in HBM.\n    v_pages: paged V cache. Normally in HBM.\n    kv_lens: padded kv lengths. Only the first num_seqs values are valid.\n    page_indices: the first index indicates which page to use in the kv cache\n      for each sequence. Only the first num_seqs values are valid.\n    cu_q_lens: the cumulative sum of the effective query lengths. Similar to\n      kv_lens, only the first num_seqs+1 values are valid.\n    num_seqs: the dynamic number of sequences.\n    sm_scale: the softmax scale which will be applied to the Q@K^T.\n    mask_value: mask value for causal mask.\n    num_kv_pages_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    num_queries_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    vmem_limit_bytes: the vmem limit for the pallas kernel.\n\n  Returns:\n    The output of the attention.\n  \"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  _, num_q_heads, head_dim = q.shape\n  _, page_size, num_kv_heads, _ = k_pages.shape\n  num_q_per_blk = num_queries_per_block\n  num_kv_pages_per_blk = num_kv_pages_per_block\n  num_q_heads_per_kv_head = num_q_heads // num_kv_heads\n  num_q_blks = ceil_div(cu_q_lens[num_seqs[0]], num_q_per_blk)\n  num_q_heads_per_blk, num_kv_heads_per_blk = get_min_heads_per_blk(num_q_heads, num_kv_heads, q.dtype, k_pages.dtype)\n  assert num_q_heads_per_blk % num_q_heads_per_kv_head == 0\n  num_heads_blks = num_q_heads // num_q_heads_per_blk\n  grid = (num_heads_blks, num_q_blks)\n\n  def q_index_map(heads_blk_idx, q_blk_idx, *_):\n    return (q_blk_idx, heads_blk_idx, 0)\n\n  q_block_spec = pl.BlockSpec(\n      (num_q_per_blk, num_q_heads_per_blk, head_dim),\n      q_index_map,\n  )\n  in_specs = [\n      q_block_spec,\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n  ]\n  out_specs = q_block_spec\n  lm_scratch = pltpu.VMEM(\n      # TODO(jevinjiang): use 128 instead of 1 is due to Mosaic does not support\n      # unaligned slicing!\n      (num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128),\n      jnp.float32,\n  )\n  double_buf_scratch = pltpu.VMEM(\n      (\n          2,  # For double buffering during DMA copies.\n          num_kv_pages_per_blk,\n          page_size,\n          num_kv_heads_per_blk,\n          head_dim,\n      ),\n      k_pages.dtype,\n  )\n  scratch_shapes = [\n      double_buf_scratch,  # k_bufs\n      double_buf_scratch,  # v_bufs\n      pltpu.SemaphoreType.DMA((2, 2)),  # [double_buffers, k_sem/v_sem]\n      lm_scratch,  # l_ref\n      lm_scratch,  # m_ref\n  ]\n  scalar_prefetches = (\n      kv_lens,\n      page_indices,\n      cu_q_lens,\n      jnp.array((0, 0), jnp.int32),  # seq_idx, buf_idx\n      num_seqs,\n  )\n  kernel = pl.pallas_call(\n      functools.partial(\n          ragged_paged_attention_kernel,\n          sm_scale=sm_scale,\n          mask_value=mask_value,\n      ),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n          num_scalar_prefetch=len(scalar_prefetches),\n          in_specs=in_specs,\n          out_specs=out_specs,\n          grid=grid,\n          scratch_shapes=scratch_shapes,\n      ),\n      compiler_params=pltpu.CompilerParams(\n          dimension_semantics=(\n              \"arbitrary\",\n              \"arbitrary\",\n          ),\n          vmem_limit_bytes=vmem_limit_bytes,\n      ),\n      out_shape=jax.ShapeDtypeStruct(shape=q.shape, dtype=jnp.float32),\n      name=\"ragged_paged_attention_kernel\",\n  )\n  # TODO(jevinjiang): Use f32 acc scratch for output! So we only need\n  # to transfer output with desired dtype back to HBM.\n  return kernel(*scalar_prefetches, q, k_pages, v_pages).astype(q.dtype)",
        "analysis": {
            "module_type": "ragged_paged_attention",
            "purpose": "Implements a highly optimized ragged paged attention mechanism for TPUs, supporting mixed prefill and decoding scenarios.",
            "input": {
                "shape": "[max_num_batched_tokens, num_q_heads, head_dim], [total_num_pages, page_size, num_kv_heads, head_dim], [total_num_pages, page_size, num_kv_heads, head_dim], [max_num_seqs], [max_num_seqs, pages_per_seq], [max_num_seqs + 1], [1]",
                "dtype": "jax.Array (float32/bfloat16 for q, k, v), int32 for lengths and indices"
            },
            "processing_steps": [
                "Input shape validation.",
                "Calculation of grid dimensions for parallel processing.",
                "Definition of block specifications for input and output tensors.",
                "Allocation of scratch memory for intermediate computations (k_bufs, v_bufs, l_ref, m_ref).",
                "Configuration of scalar prefetch arguments.",
                "Compilation of the Pallas kernel using `pl.pallas_call`.",
                "Execution of the compiled kernel with input data and prefetch arguments.",
                "Casting the output to the input query's data type."
            ],
            "output": {
                "shape": "[max_num_batched_tokens, num_q_heads, head_dim]"
            },
            "dependencies": [
                "jax",
                "jax.lax",
                "jax.experimental.pallas",
                "jax.experimental.pallas.tpu",
                "jax.numpy",
                "functools",
                "ceil_div",
                "get_dtype_packing",
                "get_min_heads_per_blk",
                "check_inputs_shapes",
                "ragged_paged_attention_kernel"
            ],
            "parameters": {
                "sm_scale": "Scaling factor applied to the attention scores (Q@K^T).",
                "mask_value": "Value used for masking in the attention mechanism.",
                "num_kv_pages_per_block": "Number of KV pages processed in one Pallas kernel block.",
                "num_queries_per_block": "Number of queries processed in one Pallas kernel block.",
                "vmem_limit_bytes": "Optional limit for virtual memory usage by the Pallas kernel."
            },
            "notes": [
                "The function is decorated with `functools.partial(jax.jit, ...)` making it JIT-compilable and allowing static arguments.",
                "The kernel is designed for TPU execution and leverages Pallas for low-level optimization.",
                "It handles variable sequence lengths (raggedness) and paged KV caches.",
                "Includes TODO comments for potential future optimizations and features."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "functionality": "Validates configuration settings for a decoding process, ensuring that the model is not attempting to load a full state and that the number of initial prefill streams is within valid bounds.",
            "usage": "This function is called internally during the setup phase of a decoding process. It takes a configuration object as input and raises an AssertionError if the configuration is invalid. It expects the configuration object to have attributes like `load_full_state_path` and relies on global constants `_INITIAL_PREFILL_STREAMS` and `_NUM_STREAMS`."
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#main",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "functionality": "This code block implements a main function that orchestrates interleaved inference for a language model. It handles prefilling initial prompts, generating subsequent tokens in an interleaved manner across multiple streams (simulating concurrent requests), and managing the lifecycle of these streams until they complete or reach a maximum length.",
            "usage": "This function is intended to be the entry point for running an inference job. It takes command-line arguments (`argv`) to configure the inference process, including model parameters, prompt text, and generation constraints. It initializes the MaxEngine, loads model parameters, tokenizes the input prompt, and then enters a loop to perform prefill and generation steps for multiple concurrent streams. The final generated text for each stream is decoded and printed. It requires a configuration object (presumably from `pyconfig.initialize`) and relies on the `MaxEngine` class for core inference operations."
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#latency_bound_comms",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def latency_bound_comms(comm: float, latency=1e-6):\n  return max(comm, latency)",
        "analysis": {
            "functionality": "Ensures that the communication volume is at least a minimum latency value.",
            "usage": "This function takes a communication volume (comm) and an optional latency value. It returns the maximum of the two, effectively bounding the communication time by the latency if the communication volume is smaller."
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#calculate_matmul_resources",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def calculate_matmul_resources(\n    activations_shape: tuple[int, ...],\n    weights_shape: tuple[int, ...],\n    ici_bandwidth: float,\n    peak_flops: float,\n    sD: int = 1,\n    sK: int = 1,\n    sW: int = 1,\n    sF: int = 1,\n    sE: int = 1,\n    activation_size_bytes: int = 2,\n    weight_size_bytes: int = 2,\n    ici_latency: float = 1e-6,\n    all_gather_axes: Sequence[str] = tuple(),\n    debug=True,\n) -> dict[str, float]:\n  \"\"\"\n  Calculates estimated FLOPs, communication volume, and memory for a distributed matrix multiplication.\n\n  The multiplication is A @ W.\n  A (activations) has shape (M, K).\n  W (weights) has shape (G, K, F).\n\n  Sharding strategy assumed:\n  - Data Parallelism: `sD` shards the M dim of A.\n  - Embedding Parallelism: `sK` shards on the embedding dim of A.\n  - Tensor Parallelism for W dim: `sK` shards the W dimension of W.\n  - Tensor Parallelism for F dim: `sF` shards the second weight dim of W.\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n                     G is the number of groups if this is a GMM (e.g in MoE layer).\n      sD: Number of data parallel shards (sD). Must be >= 1.\n      sK: Sharding factor for the activation embedding dimension.\n      sW: Sharding factor for the first weight dimension.\n      sF: Sharding factor for the second weight dimension.\n      sE: Sharding factor to split up expert weights.\n      activation_size_bytes: Size of a single element in bytes for the activations.\n      weight_size_bytes: Size of a single element in bytes for the weights.\n      ici_latency: The latency overhead of communicating between TPUs.\n      all_gather_axes: Optional additional output axes that need to be all-gathered (e.g. \"M\", \"F\").\n      debug: Whether to print intermediate resource calculations.\n\n  Returns:\n      A dictionary with keys:\n          \"t_flops\": Estimated FLOPs latency.\n          \"t_comms\": Estimated communication latency.\n          \"memory\": Estimated memory footprint per device for storing\n                                   local shards of activations, weights, and output (bytes).\n  \"\"\"\n\n  M, K_act = activations_shape[0], activations_shape[-1]\n  # Intermediate activation shape\n  I = np.prod(np.array(activations_shape[1:-1]))\n  if len(weights_shape) == 3:\n    G, K_w, F = weights_shape\n  elif len(weights_shape) == 2:\n    K_w, F = weights_shape\n    G = 1\n  else:\n    raise ValueError(f\"weights_shape={weights_shape} is not supported!.\")\n\n  def _gather_dim_to_shard():\n    # # Used to map all-gather arguments to the respective shardings.\n    return {\"D\": sD, \"K\": sK, \"W\": sW, \"F\": sF, \"E\": sE}\n\n  gather_dim_to_shard = _gather_dim_to_shard()\n\n  def _validate_shardings_and_shapes():\n    if not (sD >= 1 and sK >= 1 and sW >= 1 and sF >= 1 and sE >= 1):\n      raise ValueError(\"All sharding amounts must be >= 1.\")\n    if sK > 1 and sF > 1:\n      raise ValueError(\"Cannot have both sK & sF > 1!\")\n    if K_act != K_w:\n      raise ValueError(f\"K dimension of activations ({K_act}) must match K dimension of weights ({K_w})\")\n    if sK > 1 and sW > 1 and sK != sW:\n      raise ValueError(\"Sharding amounts between embedding dim and first weight matricx dim are different!.\")\n    # Warnings for non-divisibility. Calculations proceed with float division,\n    # implying an average or approximation if not perfectly divisible.\n    if M % sD != 0:\n      print(\n          f\"Warning: Activations M dimension ({M}) is not perfectly divisible by sharding amount {sD}.\",\n          \"Results are approximate.\",\n      )\n    if K_act % sK != 0:\n      print(\n          f\"Warning: Common K dimension ({K_act}) is not perfectly divisible by sharding amount {sK}.\",\n          \"Results are approximate.\",\n      )\n    if K_w % sW != 0:\n      print(\n          f\"Warning: Common W dimension ({K_w}) is not perfectly divisible by sharding amount {sW}. Results are approximate.\"\n      )\n    if F % sF != 0:\n      print(\n          f\"Warning: Weights F dimension ({F}) is not perfectly divisible by sharding amount {sF}. Results are approximate.\"\n      )\n    if G % sE != 0:\n      print(\n          f\"Warning: Experts G dimension ({G}) is not perfectly divisible by sharding amount {sE}. Results are approximate.\"\n      )\n\n  _validate_shardings_and_shapes()\n  K = K_act\n\n  # Implied all-gather flags\n  is_fsdp_act = sK > 1 and sW == 1\n  is_fsdp_weight = sK == 1 and sW > 1\n\n  # Local device dimensions\n  local_M_dim = M // sD\n  local_K_dim = K // sK\n  local_W_dim = K // sW\n  local_G_dim = G // sE\n  local_F_dim = F // sF\n\n  # 1. Total FLOPs\n  # For A(M,K) @ W(K,F), FLOPs = 2 * M * K * F\n  total_flops = 2.0 * np.prod(activations_shape) * G * F / (sF * sE * sD * sK * sW)\n  if debug:\n    print(f\"Total GFlops = {total_flops/1e9}\")\n  if is_fsdp_act:\n    total_flops *= sK\n    if debug:\n      print(f\"Total GFlops after activation all-gather = {total_flops/1e9}\")\n  elif is_fsdp_weight:\n    total_flops *= sW\n    if debug:\n      print(f\"Total GFlops after weights all-gather = {total_flops/1e9}\")\n  t_flops = total_flops / peak_flops\n\n  # 2. Memory per device\n  # A_local: (M/sD, K/sK)\n  # W_local: (G/gE, K/sK, N/sF)\n  # Out_local: (M/sD, N/sF) (buffer for local output)\n  mem_activations_bytes = local_M_dim * I * local_K_dim * activation_size_bytes\n  mem_weights_bytes = local_G_dim * local_W_dim * local_F_dim * weight_size_bytes\n  if debug:\n    print(f\"Activation memory (GB): {mem_activations_bytes/1e9}\")\n    print(f\"Weights memory (GB): {mem_weights_bytes/1e9}\")\n  # All-gather\n  if is_fsdp_act:\n    mem_activations_bytes *= sK\n    if debug:\n      print(f\"Activation memory (GB) after all-gather: {mem_activations_bytes/1e9}\")\n  elif is_fsdp_weight:\n    mem_weights_bytes *= sW\n    if debug:\n      print(f\"Weight memory (GB) after all-gather: {mem_weights_bytes/1e9}\")\n\n  local_output_bytes = local_M_dim * I * local_G_dim * local_F_dim * max(activation_size_bytes, weight_size_bytes)\n  if debug:\n    print(f\"Output memory (GB): {local_output_bytes/1e9}\")\n\n  gathered_output_bytes = local_output_bytes * np.prod([gather_dim_to_shard[axis] for axis in all_gather_axes])\n  if debug:\n    print(f\"Output memory (GB) after additional axes gathers: {gathered_output_bytes/1e9}\")\n  memory_per_TPU_bytes = mem_activations_bytes + mem_weights_bytes + gathered_output_bytes\n  if debug:\n    print(f\"Total memory (GB): {memory_per_TPU_bytes/1e9}\")\n\n  # 3. Communication Volume per TPU\n  t_comms = 0.0\n\n  # For FSDP-style comms, all-gather the tensor.\n  if is_fsdp_act:\n    communication_volume_per_TPU_bytes = np.prod(np.array(activations_shape)) / sK * activation_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for activation all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif is_fsdp_weight:\n    communication_volume_per_TPU_bytes = np.prod(np.array(weights_shape)) / sW * weight_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sW - 1)\n    if debug:\n      print(f\"Per-TPU comms for weights all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif sK > 1 and sW > 1:\n    # Perform reduce-scatter on the output.\n    t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for all-reduce (GB): {local_output_bytes/1e9}\")\n\n  # All-to-all on the output during expert parallelism (assuming equal loads. i.e. 1/4 * comms(all-gather))\n  if sE > 1:\n    t_comms += latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sE - 1) / 4\n    if debug:\n      print(f\"Per-TPU comms for all-to-all (GB): {local_output_bytes/1e9}\")\n\n  for axis in all_gather_axes:\n    current_output_bytes = local_output_bytes\n    current_sharding = gather_dim_to_shard[axis]\n    t_comms += latency_bound_comms(current_output_bytes / ici_bandwidth, ici_latency) * (current_sharding - 1)\n    if debug:\n      print(f\"Per-TPU comms for axis {axis} all-gather (GB): {current_output_bytes/1e9}\")\n    current_output_bytes *= current_sharding\n\n  return {\n      \"t_flops\": t_flops,\n      \"t_comms\": t_comms,\n      \"memory_per_TPU_bytes\": memory_per_TPU_bytes,\n  }",
        "analysis": {
            "functionality": "Calculates estimated FLOPs, communication volume, and memory footprint for a distributed matrix multiplication (A @ W) based on various sharding strategies.",
            "usage": "Call this function with the shapes of activations and weights, inter-chip communication bandwidth and latency, peak FLOPs, and sharding parameters (sD, sK, sW, sF, sE). It returns a dictionary containing estimated FLOPs latency ('t_flops'), communication latency ('t_comms'), and memory footprint per device ('memory_per_TPU_bytes')."
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#plot_sharding_scheme_comparison",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def plot_sharding_scheme_comparison(\n    calc_resource_func,\n    activations_shape,\n    weights_shape,\n    sharding_schemes: list[dict],\n):\n  \"\"\"\n  Generates plots comparing different sharding schemes:\n  1. Communication latency vs. FLOPs latency\n  2. Communication latency / memory per device\n  3. Memory & Communication Latency\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n      sharding_schemes: A list of dictionaries. Each dictionary must contain:\n          - \"label\": A string label for the scheme (e.g., \"DP=8\").\n          - \"shard_settings\": A dictionary with sharding parameters used for calc_resource_func().\n          E.g:\n          [\n              {\n                  \"label\": \"DP=8\", # Pure Data Parallelism\n                  \"shard_settings\": {\n                      \"sD\": 8,\n                      \"all_gather_axes\": (\"D\",)\n                  }\n              },\n          ]\n      element_size_bytes: Size of a single element in bytes.\n  \"\"\"\n  results = []\n  valid_schemes_labels = []\n\n  print(\"Calculating resources for sharding schemes...\")\n  for scheme in sharding_schemes:\n    label = scheme.get(\"label\", \"Unknown Scheme\")\n    shard_settings = scheme.get(\"shard_settings\")\n\n    print(f\"\\n--- Scheme: {label} ---\")\n    try:\n      # Clear previous warnings for divisibility for cleaner output per iteration\n      with warnings.catch_warnings(record=True) as caught_warnings:\n        del caught_warnings\n        warnings.simplefilter(\"always\")  # Catch all warnings\n\n        # Call the resource calculation function\n        res = calc_resource_func(activations_shape, weights_shape, **shard_settings)\n        print(\"Workload stats:\\n\")\n        pprint.PrettyPrinter(indent=4).pprint(res)\n\n      results.append(res)\n      valid_schemes_labels.append(label)\n    except ValueError as e:\n      print(f\"Error calculating resources for scheme '{label}': {e}. Skipping.\")\n    except (TypeError, KeyError, ZeroDivisionError, AttributeError) as e:\n      print(f\"An unexpected error occurred for scheme '{label}': {e}. Skipping.\")\n\n  if not results:\n    print(\"No valid data points generated. Cannot create plots.\")\n    return\n\n  # Extract data for plotting\n  t_flops_list = np.array([r[\"t_flops\"] for r in results])\n  t_comms_list = np.array([r[\"t_comms\"] for r in results])\n  mem_list = np.array([r[\"memory_per_TPU_bytes\"] for r in results]) / (1024**3)  # GB\n  title_suffix_context = f\": A{activations_shape} @ W{weights_shape}\"\n  num_schemes = len(valid_schemes_labels)  # Number of successfully processed schemes\n  colors = plt.cm.viridis(np.linspace(0, 1, num_schemes)) if num_schemes > 0 else []\n\n  # Calculate FLOPs/Communication ratio\n  flops_per_comm_ratio = np.zeros(num_schemes)\n  has_infinite_ratio = [False] * num_schemes\n  for i in range(num_schemes):\n    if t_comms_list[i] > 1e-9:  # Threshold to avoid near-zero division issues\n      flops_per_comm_ratio[i] = t_flops_list[i] / t_comms_list[i]\n    elif t_flops_list[i] > 1e-9:  # Positive FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = np.inf\n      has_infinite_ratio[i] = True\n    else:  # Zero FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = 0\n\n  finite_ratios = flops_per_comm_ratio[np.isfinite(flops_per_comm_ratio)]\n  placeholder_for_inf = 0\n  if finite_ratios.size > 0:\n    placeholder_for_inf = np.max(finite_ratios) * 1.5 if np.max(finite_ratios) > 0 else 1000\n  elif np.any(has_infinite_ratio):\n    placeholder_for_inf = 1000\n\n  plot_ratios = np.array(\n      [placeholder_for_inf if r_inf else r_val for r_val, r_inf in zip(flops_per_comm_ratio, has_infinite_ratio)]\n  )\n  plot_ratios = np.nan_to_num(plot_ratios, nan=0.0, posinf=placeholder_for_inf, neginf=-placeholder_for_inf)\n\n  # --- Create Plots ---\n  categorical_x = np.arange(num_schemes)  # For categorical x-axis\n\n  # Plot 1: FLOPs & Communication (Grouped Bar Plot)\n  grouped_bar_width_fc = 0.35\n  fig_flops_comm_grouped, ax_flops_comm_grouped = plt.subplots(figsize=(max(10, num_schemes * 1.7), 7))\n\n  rects_flops = ax_flops_comm_grouped.bar(\n      categorical_x - grouped_bar_width_fc / 2,\n      t_flops_list,\n      grouped_bar_width_fc,\n      label=\"T_flops\",\n      color=\"mediumseagreen\",\n  )\n  rects_comms_grouped = ax_flops_comm_grouped.bar(\n      categorical_x + grouped_bar_width_fc / 2, t_comms_list, grouped_bar_width_fc, label=\"T_comms\", color=\"deepskyblue\"\n  )\n\n  ax_flops_comm_grouped.set_xlabel(\"Sharding Scheme\")\n  ax_flops_comm_grouped.set_ylabel(\"Seconds\")\n  ax_flops_comm_grouped.set_title(f\"T_flops & T_comms by Sharding Scheme{title_suffix_context}\", fontsize=14)\n  ax_flops_comm_grouped.set_xticks(categorical_x)\n  ax_flops_comm_grouped.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  if num_schemes > 0:\n    ax_flops_comm_grouped.legend(fontsize=10)\n\n  ax_flops_comm_grouped.bar_label(rects_flops, padding=3, fmt=\"%.2e\", fontsize=9)\n  ax_flops_comm_grouped.bar_label(rects_comms_grouped, padding=3, fmt=\"%.2e\", fontsize=9)\n\n  ax_flops_comm_grouped.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  max_y_val_fc = 0\n  if t_flops_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_flops_list))\n  if t_comms_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_comms_list))\n  print(f\"max_y_val_fc = {max_y_val_fc}\")\n  ax_flops_comm_grouped.set_ylim(0, max_y_val_fc * 1.15)\n\n  fig_flops_comm_grouped.tight_layout()\n  plt.show()\n\n  # Plot 2: FLOPs/Communication Ratio\n  fig_ratio, ax_ratio = plt.subplots(figsize=(max(10, num_schemes * 1.1), 7))\n\n  bars_ratio = ax_ratio.bar(categorical_x, plot_ratios, width=0.6, color=colors, alpha=0.9)\n\n  ax_ratio.set_xlabel(\"Sharding Scheme\")\n  ax_ratio.set_ylabel(\"T_flops / T_comms\")\n  ax_ratio.set_title(f\"Roofline (T_flops vs. T_comms) for {title_suffix_context}\", fontsize=14)\n  ax_ratio.set_xticks(categorical_x)\n  ax_ratio.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_ratio.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  for i, bar in enumerate(bars_ratio):\n    yval = bar.get_height()\n    label_text = f\"{yval:.2f}\"\n    if has_infinite_ratio[i] and yval == placeholder_for_inf:\n      label_text = f\"> {np.max(finite_ratios):.2f}\\n(Effectively Inf)\" if finite_ratios.size > 0 else \"Very High\"\n    ax_ratio.text(bar.get_x() + bar.get_width() / 2.0, yval, label_text, va=\"bottom\", ha=\"center\", fontsize=9)\n\n  if plot_ratios.size > 0:\n    max_ratio_plot_val = np.max(plot_ratios)\n    ax_ratio.set_ylim(0, max_ratio_plot_val * 1.15)\n\n  fig_ratio.tight_layout()\n  plt.show()\n\n  # Plot 3: Memory vs. Communication (Bars positioned by Communication Volume)\n  fig_mem, ax_mem = plt.subplots(figsize=(max(10, num_schemes * 1.3), 7))  # Slightly wider for labels\n  bar_width_mem = 0.6\n\n  ax_mem.bar(\n      categorical_x,\n      mem_list,\n      width=bar_width_mem,\n      color=colors,\n      alpha=0.85,\n      edgecolor=[np.array(c[:3]) * 0.6 for c in colors],\n  )\n\n  ax_mem.set_xlabel(\"Sharding Scheme\")\n  ax_mem.set_ylabel(\"Memory per TPU (GB)\")\n  ax_mem.set_title(f\"Memory & Comm. by Sharding Scheme{title_suffix_context}\", fontsize=14)  # Updated title\n  ax_mem.set_xticks(categorical_x)\n  ax_mem.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_mem.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  # Add custom labels in scientific notation\n  for i in range(num_schemes):\n    mem_val = mem_list[i]\n    comm_val = t_comms_list[i]  # This is assumed to be in MB\n\n    # Format the label string as requested\n    # Using \\n for a new line to make it more readable on the plot\n    label_text = f\"mem: {mem_val:.2e} GB\\nt_comms: {comm_val:.2e} sec\"\n\n    ax_mem.text(\n        categorical_x[i],  # x-position: center of the bar\n        mem_val,  # y-position: top of the bar\n        label_text,\n        ha=\"center\",  # Horizontal alignment\n        va=\"bottom\",  # Vertical alignment (anchor at bottom of text, so text is above y)\n        fontsize=8,\n        rotation=0,\n        bbox={\"facecolor\": \"white\", \"alpha\": 0.6, \"pad\": 2, \"boxstyle\": \"round,pad=0.3\"},  # Added bbox\n    )\n\n  if mem_list.size > 0:\n    max_mem_val = np.max(mem_list)\n    # Adjust y-limit to accommodate multi-line labels; factor might need tuning\n    ax_mem.set_ylim(0, max_mem_val * 1.35)  # Increased padding for multi-line labels\n  else:\n    ax_mem.set_ylim(0, 1)\n\n  fig_mem.tight_layout()\n  plt.show()",
        "analysis": {
            "module_type": "sharding_scheme_plotter",
            "purpose": "Generates and displays plots comparing different sharding schemes based on calculated resource metrics.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterates through provided sharding schemes.",
                "Calls a resource calculation function for each scheme.",
                "Collects results (FLOPs latency, communication latency, memory per device).",
                "Calculates FLOPs/Communication ratio.",
                "Generates three plots: FLOPs vs. Communication, FLOPs/Communication ratio, and Memory vs. Communication.",
                "Displays the generated plots."
            ],
            "output": {
                "shape": "N/A (displays plots)",
                "dtype": "N/A"
            },
            "dependencies": [
                "numpy",
                "matplotlib.pyplot",
                "pprint",
                "warnings"
            ],
            "parameters": {
                "calc_resource_func": "A function that takes activation shape, weights shape, and sharding settings, and returns a dictionary of resource metrics.",
                "activations_shape": "Tuple representing the shape of the activations tensor.",
                "weights_shape": "Tuple representing the shape of the weights tensor.",
                "sharding_schemes": "A list of dictionaries, where each dictionary contains a 'label' and 'shard_settings' for a specific sharding configuration."
            },
            "notes": [
                "Handles potential errors during resource calculation for individual schemes.",
                "Adjusts plot limits and labels for readability.",
                "Plots are displayed using plt.show()."
            ],
            "methods": {
                "plot_sharding_scheme_comparison": {
                    "purpose": "The main function that orchestrates the calculation of resources for various sharding schemes and generates comparative plots.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes empty lists for results and valid scheme labels.",
                        "Loops through each `scheme` in `sharding_schemes`.",
                        "Extracts `label` and `shard_settings` from the current `scheme`.",
                        "Calls `calc_resource_func` with `activations_shape`, `weights_shape`, and unpacked `shard_settings`.",
                        "Appends the result to `results` and the label to `valid_schemes_labels` if successful.",
                        "Catches and prints errors for invalid or problematic schemes.",
                        "Checks if any valid results were obtained; returns if not.",
                        "Extracts `t_flops`, `t_comms`, and `memory_per_TPU_bytes` into numpy arrays.",
                        "Converts memory to GB.",
                        "Constructs a title suffix based on input shapes.",
                        "Determines colors for plotting based on the number of schemes.",
                        "Calculates the FLOPs/Communication ratio, handling potential division by zero or infinity.",
                        "Prepares data for plotting, including handling infinite ratios.",
                        "Creates three distinct plots using matplotlib:",
                        "  1. Grouped bar plot of T_flops and T_comms.",
                        "  2. Bar plot of FLOPs/Communication ratio.",
                        "  3. Bar plot of Memory per TPU (GB) with communication latency as text labels.",
                        "Sets appropriate labels, titles, and ticks for each plot.",
                        "Adds data labels to bars where applicable.",
                        "Adjusts y-axis limits for better visualization.",
                        "Uses `fig.tight_layout()` for better spacing.",
                        "Displays each plot using `plt.show()`."
                    ],
                    "output": {
                        "shape": "N/A (displays plots)",
                        "dtype": "N/A"
                    },
                    "dependencies": [
                        "numpy",
                        "matplotlib.pyplot",
                        "pprint",
                        "warnings"
                    ],
                    "notes": [
                        "The `calc_resource_func` is expected to return a dictionary containing at least 't_flops', 't_comms', and 'memory_per_TPU_bytes'.",
                        "Error handling is included for `ValueError`, `TypeError`, `KeyError`, `ZeroDivisionError`, and `AttributeError` during resource calculation.",
                        "The function assumes that `t_comms` is in seconds and `memory_per_TPU_bytes` is in bytes, converting the latter to GB for plotting.",
                        "Special handling is implemented for cases where the FLOPs/Communication ratio is infinite or zero.",
                        "Plot aesthetics like colors, bar widths, and label formatting are customized."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/test_sharding_utils.py#ShardingTests",
        "file_path": "src/MaxText/inference/scripts/test_sharding_utils.py",
        "code_block": "class ShardingTests(unittest.TestCase):\n  \"\"\"Test suite for sharding resource calculation utilities.\"\"\"\n\n  def test_no_sharding(self):\n    \"\"\"Tests the basic case with no sharding.\"\"\"\n    sD, sK, sW, sF, sE = 1, 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    # Total FLOPs = 2 * M * K * F\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF(self):\n    \"\"\"Tests sharding on the F dimension of weights (sF > 1).\"\"\"\n    sF = 4\n    sD, sK, sW, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_data_parallelism_sD(self):\n    \"\"\"Tests sharding on the M dimension of activations (sD).\"\"\"\n    sD = 4\n    sK, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs:\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_activation_sharding_sK(self):\n    \"\"\"Tests FSDP-style sharding on the K dimension of activations (sK).\n\n    In this scenario, the weights are not sharded (sW=1).\n    \"\"\"\n    sK = 4\n    sD, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (M * K / sK) * activation_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_weight_sharding_sW(self):\n    \"\"\"Tests FSDP-style sharding on the W dimension of weights (sW).\n\n    In this scenario, the activations are not sharded (sK=1).\n    \"\"\"\n    sW = 4\n    sD, sK, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (K * F / sW) * weight_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sW - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_tensor_parallel_sK_sW(self):\n    \"\"\"Tests tensor parallelism where both sK and sW are used.\n\n    This test assumes sK == sW and a reduce-scatter operation for partial\n    results.\n    \"\"\"\n    sK = 2\n    sW = 2  # Must be equal to sK for this path\n    sD, sF, sE = 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF_with_all_gather_F(self):\n    \"\"\"Tests sF sharding with a subsequent all-gather on the F dimension.\"\"\"\n    sF = 4  # Shard the output feature dimension\n    sD, sK, sW, sE = 1, 1, 1, 1  # Isolate sF effect\n    all_gather_axes = [\"F\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_for_gather = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_for_gather / ici_bandwidth_val, ici_latency_val) * (sF - 1)\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU:\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Outputs\n    expected_mem_output_gathered = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output_gathered\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_expert_parallelism_sE(self):\n    \"\"\"Tests expert parallelism sharding on the G dimension (sE).\"\"\"\n    G_val = 8\n    weights_shape_3d = (G_val, K, F)\n    sE = 4\n    sD, sK, sW, sF = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_3d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F * G_val) / (peak_flops_val * sE)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sE - 1) / 4\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (G_val / sE) * K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_mixed_sharding_sD_sK_sW(self):\n    \"\"\"Tests a mix of data and tensor parallelism (reduce-scatter).\"\"\"\n    sD = 2\n    sK = 2\n    sW = 2  # sK == sW\n    sF, sE = 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * (M / sD) * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_additional_all_gather_axes_D(self):\n    \"\"\"Tests an additional all-gather on the 'D' dimension of the output.\"\"\"\n    sD = 2\n    sK, sW, sF, sE = 1, 1, 1, 1\n    all_gather_axes = [\"D\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_base = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_base / ici_bandwidth_val, ici_latency_val) * (sD - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE",
        "analysis": {
            "module_type": "test_suite",
            "purpose": "Provides unit tests for the `calculate_matmul_resources` function, covering various sharding strategies and their impact on FLOPs, communication costs, and memory usage.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes sharding parameters (sD, sK, sW, sF, sE).",
                "Calls `calculate_matmul_resources` with specific sharding configurations.",
                "Asserts the returned values (t_flops, t_comms, memory_per_TPU_bytes) against expected calculations.",
                "Tests different sharding scenarios including no sharding, output feature parallelism, data parallelism, FSDP, tensor parallelism, expert parallelism, and mixed sharding."
            ],
            "output": {
                "shape": "N/A (tests assert conditions, do not return values)",
                "dtype": "N/A"
            },
            "dependencies": [
                "unittest",
                "calculate_matmul_resources",
                "latency_bound_comms"
            ],
            "parameters": {
                "M, K, F": "Dimensions for matrix multiplication (activations_shape_2d=(M, K), weights_shape_2d=(K, F)).",
                "activations_shape_2d": "Shape of the input activation tensor.",
                "weights_shape_2d": "Shape of the weight tensor.",
                "ici_bandwidth_val": "Inter-chip interconnect bandwidth.",
                "peak_flops_val": "Peak floating-point operations per second of the hardware.",
                "activation_size_bytes_val": "Size of activation elements in bytes.",
                "weight_size_bytes_val": "Size of weight elements in bytes.",
                "ici_latency_val": "Inter-chip interconnect latency.",
                "TOLERANCE": "Floating-point comparison tolerance.",
                "sD, sK, sW, sF, sE": "Sharding factors for Data, Kernel (activation), Weight, Output Feature, and Expert dimensions respectively."
            },
            "notes": [
                "Each test method focuses on a specific sharding strategy or combination.",
                "The tests verify FLOPs, communication time, and memory usage per TPU.",
                "Assumptions are made about the underlying communication primitives (e.g., reduce-scatter, all-gather) based on the sharding strategy.",
                "The `debug` parameter is set to `False` in all test calls."
            ],
            "methods": {
                "test_no_sharding": {
                    "purpose": "Tests the resource calculation when no sharding is applied (all sharding factors are 1).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sD, sK, sW, sF, sE to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for no sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_output_feature_parallelism_sF": {
                    "purpose": "Tests resource calculation when sharding is applied to the output feature dimension (F) of the weights.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sF to 4 and other sharding factors to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for sF sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_data_parallelism_sD": {
                    "purpose": "Tests resource calculation when sharding is applied to the data (M) dimension of the activations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sD to 4 and other sharding factors to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for sD sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_fsdp_activation_sharding_sK": {
                    "purpose": "Tests Fully Sharded Data Parallelism (FSDP) style sharding on the K dimension of activations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sK to 4 and other sharding factors to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for FSDP activation sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "Assumes weights are not sharded (sW=1)."
                    ]
                },
                "test_fsdp_weight_sharding_sW": {
                    "purpose": "Tests Fully Sharded Data Parallelism (FSDP) style sharding on the W dimension of weights.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sW to 4 and other sharding factors to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for FSDP weight sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "Assumes activations are not sharded (sK=1)."
                    ]
                },
                "test_tensor_parallel_sK_sW": {
                    "purpose": "Tests tensor parallelism where both K and W dimensions are sharded, assuming sK equals sW.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sK and sW to 2, and other sharding factors to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for tensor parallelism."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "Assumes a reduce-scatter operation for partial results."
                    ]
                },
                "test_output_feature_parallelism_sF_with_all_gather_F": {
                    "purpose": "Tests sF sharding combined with an all-gather operation on the F dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sF to 4, other sharding factors to 1, and `all_gather_axes` to ['F'].",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values, considering the all-gather."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_expert_parallelism_sE": {
                    "purpose": "Tests expert parallelism sharding on the G dimension (represented by sE).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets weights_shape to 3D (G_val, K, F).",
                        "Sets sE to 4 and other sharding factors to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for expert parallelism."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_mixed_sharding_sD_sK_sW": {
                    "purpose": "Tests a combination of data parallelism (sD) and tensor parallelism (sK, sW).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sD=2, sK=2, sW=2, and sF, sE to 1.",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values for mixed sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "Assumes sK == sW for the tensor parallelism part."
                    ]
                },
                "test_additional_all_gather_axes_D": {
                    "purpose": "Tests sharding on the D dimension (data parallelism) with an additional all-gather on the output's D dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets sD to 2, other sharding factors to 1, and `all_gather_axes` to ['D'].",
                        "Calls `calculate_matmul_resources`.",
                        "Asserts FLOPs, comms, and memory against expected values, considering the all-gather on D."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#DecoderBlockType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class DecoderBlockType(enum.Enum):\n  \"\"\"Decoder block types.\"\"\"\n\n  DEFAULT = \"default\"\n  LLAMA2 = \"llama2\"\n  MISTRAL = \"mistral\"\n  MIXTRAL = \"mixtral\"\n  DEEPSEEK = \"deepseek\"\n  GEMMA = \"gemma\"\n  GEMMA2 = \"gemma2\"\n  GEMMA3 = \"gemma3\"\n  QWEN3 = \"qwen3\"\n  QWEN3_MOE = \"qwen3_moe\"\n  QWEN3_NEXT = \"qwen3_next\"\n  GPT3 = \"gpt3\"\n  GPT_OSS = \"gpt_oss\"\n  SIMPLE = \"simple\"\n  SIMPLE_MLP = \"simple_mlp\"\n  LLAMA4 = \"llama4\"",
        "analysis": {
            "functionality": "Defines an enumeration for different types of decoder blocks used in transformer models.",
            "usage": "This enum can be used to specify which type of decoder block architecture to instantiate or configure. For example, a configuration parameter might accept one of these enum values to select between 'llama2', 'mistral', 'gemma', etc."
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#AttentionType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class AttentionType(enum.Enum):\n  GLOBAL = \"global\"  # default, with causality\n  LOCAL_SLIDING = \"local_sliding\"\n  CHUNK = \"chunk\"\n  MLA = \"mla\"\n  FULL = \"full\"",
        "analysis": {
            "functionality": "Defines an enumeration for different types of attention mechanisms.",
            "usage": "This enum can be used to specify and differentiate between various attention strategies like GLOBAL, LOCAL_SLIDING, CHUNK, MLA, and FULL within a configuration or model definition."
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#ShardMode",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class ShardMode(enum.Enum):\n  AUTO = \"auto\"  # default\n  EXPLICIT = \"explicit\"",
        "analysis": {
            "functionality": "Defines an enumeration for shard modes, specifying automatic or explicit control.",
            "usage": "Used to indicate how data sharding should be handled. Can be set to ShardMode.AUTO for default behavior or ShardMode.EXPLICIT for manual control."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_input_data_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_input_data_sharding(config, mesh):\n  max_logging.log(\n      \"WARNING: Function maxtext_utils.get_input_data_sharding is deprecated. Please use sharding.get_input_data_sharding.\"\n  )\n  return sharding.get_input_data_sharding(config, mesh)",
        "analysis": {
            "module_type": "deprecated_function_wrapper",
            "purpose": "This function is a deprecated wrapper that forwards calls to the sharding.get_input_data_sharding function and logs a warning.",
            "input": {
                "shape": "[N/A]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Log a deprecation warning.",
                "Call sharding.get_input_data_sharding with the provided arguments."
            ],
            "output": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "dependencies": [
                "max_logging",
                "sharding"
            ],
            "parameters": {
                "config": "Configuration object containing sharding parameters.",
                "mesh": "Device mesh object for sharding."
            },
            "notes": [
                "This function is marked as deprecated and users are advised to use `sharding.get_input_data_sharding` directly."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#assert_params_sufficiently_sharded",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def assert_params_sufficiently_sharded(params, mesh, tolerance):\n  max_logging.log(\n      \"WARNING: Function maxtext_utils.assert_params_sufficiently_sharded is deprecated.\"\n      \"Please use sharding.assert_params_sufficiently_sharded.\"\n  )\n  return sharding.assert_params_sufficiently_sharded(params, mesh, tolerance)",
        "analysis": {
            "module_type": "deprecated_function_wrapper",
            "purpose": "This function is a deprecated wrapper that calls the sharding.assert_params_sufficiently_sharded function.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Logs a deprecation warning.",
                "Calls sharding.assert_params_sufficiently_sharded with the provided arguments."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "max_logging",
                "sharding"
            ],
            "parameters": {
                "params": "Parameters to check for sharding.",
                "mesh": "The device mesh.",
                "tolerance": "The tolerance for sharding checks."
            },
            "notes": [
                "This function is deprecated and will be removed in a future version.",
                "Users should migrate to using `sharding.assert_params_sufficiently_sharded` directly."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#add_data_to_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def add_data_to_sharding(mesh, path, aval, shardings):\n  max_logging.log(\n      \"WARNING: Function maxtext_utils.add_data_to_sharding is deprecated. Please use sharding.add_data_to_sharding.\"\n  )\n  return sharding.add_data_to_sharding(mesh, path, aval, shardings)",
        "analysis": {
            "module_type": "deprecated_add_data_to_sharding",
            "purpose": "Adds data to sharding, but is deprecated and forwards to the sharding module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Logs a deprecation warning.",
                "Calls the `sharding.add_data_to_sharding` function with the provided arguments."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "max_logging",
                "sharding"
            ],
            "parameters": {},
            "notes": [
                "This function is a deprecated wrapper and should not be used directly.",
                "It logs a warning and then calls the equivalent function in the `sharding` module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#maybe_update_params_sharding_with_opt",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def maybe_update_params_sharding_with_opt(config, state_mesh_shardings):\n  max_logging.log(\n      \"WARNING: Function maxtext_utils.maybe_update_params_sharding_with_opt is deprecated.\"\n      \"Please use sharding.maybe_update_params_sharding_with_opt.\"\n  )\n  return sharding.maybe_update_params_sharding_with_opt(config, state_mesh_shardings)",
        "analysis": {
            "functionality": "This function is a deprecated wrapper that logs a warning and then calls the equivalent function from the `sharding` module.",
            "usage": "Call this function with a `config` object and `state_mesh_shardings`. It will log a deprecation warning and return the result of calling `sharding.maybe_update_params_sharding_with_opt` with the same arguments."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#all_gather_over_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def all_gather_over_fsdp(variables, sharding_info, mesh, logical_axis_rules):\n  max_logging.log(\n      \"WARNING: Function maxtext_utils.all_gather_over_fsdp is deprecated. Please use sharding.all_gather_over_fsdp.\"\n  )\n  return sharding.all_gather_over_fsdp(variables, sharding_info, mesh, logical_axis_rules)",
        "analysis": {
            "functionality": "This function is a deprecated wrapper for `sharding.all_gather_over_fsdp`. It logs a warning and then calls the actual implementation.",
            "usage": "Call this function to perform an all-gather operation across FSDP (Fully Sharded Data Parallel) shards. It requires the variables to gather, sharding information, a mesh, and logical axis rules. Note that this function is deprecated and `sharding.all_gather_over_fsdp` should be used instead."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_train_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_train_with_signature(\n    train_step, data_sharding, state_mesh_shardings, model, config, params_shardings=None\n):\n  \"\"\"Get the shardings (both state and data) for `train_step`.\"\"\"\n  functional_train = functools.partial(train_step, model, config, state_mesh_shardings, params_shardings)\n  functional_train.__name__ = \"train_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = (state_mesh_shardings, None)  # State, metrics\n  static_argnums = ()  # We partial out the static argnums of model and config\n  donate_argnums = 0  # This is the index of the state - we allow the compiler to make use of this memory.\n  return functional_train, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "functionality": "This function prepares a callable `train_step` for use with JAX's `pmap` or `pjit` by partially applying model and configuration arguments and defining input/output shardings and argument donation.",
            "usage": "Call `get_functional_train_with_signature` with the `train_step` function, data sharding information, state sharding information, the model, and configuration. It returns a partially applied `train_step` function, input shardings, output shardings, static argument numbers, and argument numbers to donate. This is typically used to set up a JAX transformation for distributed training."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_eval_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_eval_with_signature(eval_step, data_sharding, state_mesh_shardings, model, config):\n  \"\"\"Get the shardings (both state and data) for `eval_step`.\"\"\"\n  functional_eval = functools.partial(eval_step, model, config)\n  functional_eval.__name__ = \"eval_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = None  # metrics\n  static_argnums = ()  # We partial out the static argnums of model, config\n  donate_argnums = ()  # state will be kept instead of being donated in eval_step\n  return functional_eval, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "eval_signature_utility",
            "purpose": "Prepares a functional version of an evaluation step with specific sharding configurations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a partial function for the eval_step, binding model and config.",
                "Sets the __name__ attribute of the partial function to 'eval_step'.",
                "Defines input shardings for state, batch, and RNG.",
                "Sets output shardings to None (indicating metrics).",
                "Defines static argument numbers as an empty tuple.",
                "Defines donate argument numbers as an empty tuple."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "eval_step": "The evaluation step function to be partially applied.",
                "data_sharding": "The sharding configuration for the input data.",
                "state_mesh_shardings": "The sharding configuration for the model's state.",
                "model": "The model object.",
                "config": "The configuration object."
            },
            "notes": [
                "This function is designed to be used in distributed training/evaluation setups.",
                "The `donate_argnums` being empty indicates that the state is intended to be kept and not donated.",
                "The `out_shardings` being None suggests that the output (metrics) is not explicitly sharded by this function."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#shard_reorder_causal_load_balanced",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def shard_reorder_causal_load_balanced(batch, cp_size, shard_mode):\n  \"\"\"Shard the output of the reordered sequence.\"\"\"\n  reordered = max_utils.reorder_causal_load_balanced(batch, cp_size)\n  for _, v in batch.items():\n    if isinstance(v, jax.Array):\n      reordered = sharding.maybe_shard_with_name(reordered, v.sharding, shard_mode)\n      break\n  return reordered",
        "analysis": {
            "module_type": "shard_reorder_causal_load_balanced",
            "purpose": "Shards the output of a reordered sequence based on the sharding of an input batch.",
            "input": {
                "shape": "[batch_size, sequence_length, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Reorder the sequence using max_utils.reorder_causal_load_balanced.",
                "Iterate through the input batch items to find a jax.Array.",
                "Apply sharding to the reordered sequence using sharding.maybe_shard_with_name, inheriting sharding from the found jax.Array.",
                "Break the loop once sharding is applied."
            ],
            "output": {
                "shape": "Same shape as the reordered sequence, but with applied sharding."
            },
            "dependencies": [
                "max_utils.reorder_causal_load_balanced",
                "sharding.maybe_shard_with_name",
                "jax.Array"
            ],
            "parameters": {
                "batch": "A dictionary representing the input batch, expected to contain at least one jax.Array.",
                "cp_size": "A parameter used by max_utils.reorder_causal_load_balanced, likely related to checkpointing or sharding size.",
                "shard_mode": "A parameter that dictates how sharding is applied, passed to sharding.maybe_shard_with_name."
            },
            "notes": [
                "This function assumes that the input 'batch' dictionary contains at least one jax.Array from which sharding information can be inferred.",
                "The loop breaks after the first jax.Array is found to avoid redundant sharding operations if multiple arrays exist with potentially different shardings."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_reorder_callable",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_reorder_callable(cp_size, shard_mode):\n  \"\"\"Creates a callable that can be used with map() to reorder batches.\"\"\"\n  return functools.partial(shard_reorder_causal_load_balanced, cp_size=cp_size, shard_mode=shard_mode)",
        "analysis": {
            "module_type": "reorder_callable_factory",
            "purpose": "Creates a callable function that reorders batches according to specified parameters.",
            "input": {
                "shape": "[batch_size, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Uses functools.partial to create a new function with pre-filled arguments.",
                "The new function wraps `shard_reorder_causal_load_balanced`."
            ],
            "output": {
                "shape": "A callable function.",
                "dtype": "function"
            },
            "dependencies": [
                "functools.partial",
                "shard_reorder_causal_load_balanced"
            ],
            "parameters": {
                "cp_size": "Checkpointing size parameter.",
                "shard_mode": "Sharding mode parameter."
            },
            "notes": [
                "The returned callable is intended to be used with map() for batch reordering."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_shaped_batch",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_shaped_batch(config):\n  \"\"\"Return the shape of the batch - this is what eval_shape would return for the\n  output of create_data_iterator, but eval_shape doesn't work, see b/306901078.\"\"\"\n  batch_shape = (config.global_batch_size_to_load, config.max_target_length)\n  shaped_batch = {}\n  shaped_batch[\"inputs\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  if config.use_multimodal:\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n    shaped_batch[\"images\"] = jax.ShapeDtypeStruct(image_shape, jnp.int32)\n    shaped_batch[\"image_masks\"] = jax.ShapeDtypeStruct(image_shape[:2], jnp.int32)\n  return shaped_batch",
        "analysis": {
            "module_type": "data_shape_utility",
            "purpose": "Returns a dictionary representing the expected shape and dtype of a data batch, mimicking the output of `eval_shape` for a data iterator.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define the base batch shape using `config.global_batch_size_to_load` and `config.max_target_length`.",
                "Initialize an empty dictionary `shaped_batch`.",
                "For each standard input/target field ('inputs', 'inputs_position', 'inputs_segmentation', 'targets', 'targets_position', 'targets_segmentation'), create a `jax.ShapeDtypeStruct` with the base batch shape and `jnp.int32` dtype, and add it to `shaped_batch`.",
                "If `config.use_multimodal` is True:",
                "  Get the dummy image shape using `multimodal_utils.get_dummy_image_shape_for_init`.",
                "  Add 'images' with the image shape and `jnp.int32` dtype to `shaped_batch`.",
                "  Add 'image_masks' with the first two dimensions of the image shape and `jnp.int32` dtype to `shaped_batch`.",
                "Return the `shaped_batch` dictionary."
            ],
            "output": {
                "shape": "A dictionary where keys are data field names (e.g., 'inputs', 'targets', 'images') and values are `jax.ShapeDtypeStruct` objects describing the shape and dtype of each field."
            },
            "dependencies": [
                "jax",
                "jax.numpy as jnp",
                "multimodal_utils"
            ],
            "parameters": {
                "config": "Configuration object containing parameters like `global_batch_size_to_load`, `max_target_length`, `use_multimodal`, `model_name`, and `micro_batch_size_to_train_on`."
            },
            "notes": [
                "This function is a workaround for issues with `eval_shape` on data iterators.",
                "The dtype for all fields is hardcoded to `jnp.int32`.",
                "The image shape calculation depends on `config.model_name` and `config.micro_batch_size_to_train_on` when `use_multimodal` is True."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#should_prevent_cse_in_remat",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def should_prevent_cse_in_remat(config):\n  \"\"\"Determines whether to prevent common subexpression elimination (CSE) in remat.\n\n  CSE should not be prevented when:\n  1. Layers are being scanned (scan_layers=True), OR\n  2. Gradient accumulation is enabled (gradient_accumulation_steps > 1) on GPU hardware\n\n  Args:\n    config: Configuration object with scan_layers, gradient_accumulation_steps, and hardware\n\n  Returns:\n    bool: True if CSE should be prevented, False otherwise\n  \"\"\"\n  if config.scan_layers:\n    return False\n\n  if config.gradient_accumulation_steps > 1 and config.hardware in (\"gpu\", \"gpu_multiprocess\"):\n    return False\n\n  return True",
        "analysis": {
            "module_type": "configuration_check",
            "purpose": "Determines whether to prevent common subexpression elimination (CSE) during checkpointing based on configuration settings.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.scan_layers` is True. If so, return False (do not prevent CSE).",
                "Check if `config.gradient_accumulation_steps` is greater than 1 AND `config.hardware` is 'gpu' or 'gpu_multiprocess'. If so, return False (do not prevent CSE).",
                "If neither of the above conditions is met, return True (prevent CSE)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "config object (assumed to have attributes: scan_layers, gradient_accumulation_steps, hardware)"
            ],
            "parameters": {
                "config": "Configuration object containing training and hardware settings."
            },
            "notes": [
                "CSE is prevented to avoid potential issues with specific optimization strategies like layer scanning or gradient accumulation on GPUs.",
                "The function returns `True` if CSE should be prevented, and `False` otherwise."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#load_compiled",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def load_compiled(config, partial_train, state):\n  \"\"\"# Loading a serialized compiled train step function.\"\"\"\n\n  # Currently partial_train and state  are needed to reconstruct\n  # input/output shapes to construct the in_trees and out_trees for load API\n  # Parker is working on a serializing these\n  def load_serialized_compiled(save_name):\n    with open(save_name, \"rb\") as f:\n      serialized_compiled = pickle.load(f)\n    return serialized_compiled\n\n  def get_train_input_output_trees(func, input_args, input_kwargs):\n    _, in_tree_recreated = jax.tree_util.tree_flatten((input_args, input_kwargs))\n    out_shaped = jax.eval_shape(func, *input_args, **input_kwargs)\n    _, out_tree_recreated = jax.tree_util.tree_flatten(out_shaped)\n    return in_tree_recreated, out_tree_recreated\n\n  serialized_compiled = load_serialized_compiled(config.compiled_trainstep_file)\n  shaped_batch = get_shaped_batch(config)\n  example_rng = jax.random.PRNGKey(0)\n  shaped_input_args = (state, shaped_batch, example_rng)\n  shaped_input_kwargs = {}\n  in_tree, out_tree = get_train_input_output_trees(partial_train, shaped_input_args, shaped_input_kwargs)\n  p_train_step = deserialize_and_load(serialized_compiled, in_tree, out_tree)\n  return p_train_step",
        "analysis": {
            "module_type": "compiled_train_step_loader",
            "purpose": "Loads a serialized compiled training step function and prepares it for execution.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a helper function `load_serialized_compiled` to read a pickled object from a file.",
                "Define a helper function `get_train_input_output_trees` to determine the input and output shapes of the training function using `jax.eval_shape`.",
                "Load the serialized compiled training step from the file specified in the config.",
                "Get a shaped batch of data using `get_shaped_batch`.",
                "Create an example PRNG key.",
                "Construct the shaped input arguments for the training function.",
                "Determine the input and output trees (shapes and structure) of the training function.",
                "Deserialize and load the compiled training step using `deserialize_and_load` with the determined input/output trees.",
                "Return the loaded and deserialized training step function."
            ],
            "output": {
                "shape": "N/A",
                "dtype": "Callable"
            },
            "dependencies": [
                "pickle",
                "jax",
                "jax.tree_util",
                "jax.random",
                "deserialize_and_load",
                "get_shaped_batch"
            ],
            "parameters": {
                "config.compiled_trainstep_file": "Path to the serialized compiled training step file."
            },
            "notes": [
                "The `partial_train` and `state` arguments are currently needed to reconstruct input/output shapes for the `load` API.",
                "The code assumes that `get_shaped_batch` is available and correctly returns a shaped batch.",
                "The `example_rng` is initialized with a fixed seed (0)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tokens_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tokens_training_per_device(config):\n  \"\"\"Calculate training Tokens per device\"\"\"\n  return config.max_target_length * config.per_device_batch_size * config.gradient_accumulation_steps",
        "analysis": {
            "functionality": "Calculates the total number of tokens processed per device during training.",
            "usage": "Pass a configuration object with 'max_target_length', 'per_device_batch_size', and 'gradient_accumulation_steps' attributes to this function to get the total tokens per device."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma2_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma2_tflops_training_per_device(config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops):\n  \"\"\"\n  Calculate training TFLOP for Gemma2 as in Gemma2 we combine [local_attention, global_attention] into one decoder\n  layer and we use sliding window attention in local_attention\n  \"\"\"\n  noncausal_attention_flops = (\n      # global attention\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n      +\n      # local attention\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  # multiply num_decoder_layers by 2 because we combine [local_attention, global_attention] into one decoder layer\n  learnable_weight_tflops = (\n      ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers * 2 + embedding_flops) * 3 / 10**12\n  )\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "module_type": "gemma2_tflops_calculation",
            "purpose": "Calculates the training TFLOPs (Tera Floating-point Operations) per device for the Gemma2 model.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate non-causal attention FLOPs (combining global and local attention).",
                "Calculate causal attention FLOPs by dividing non-causal attention FLOPs by 2.",
                "Convert attention FLOPs to TFLOPs and multiply by the number of decoder layers and 3 (for forward/backward pass).",
                "Calculate learnable weight TFLOPs by summing FFN, QKV, and projection FLOPs, multiplying by twice the number of decoder layers (due to combined attention), adding embedding FLOPs, and converting to TFLOPs.",
                "Return attention TFLOPs and learnable weight TFLOPs."
            ],
            "output": {
                "shape": "(attention_tflops, learnable_weight_tflops)"
            },
            "dependencies": [
                "config object (containing batch size, sequence length, number of heads, head dimension, number of decoder layers, sliding window size)",
                "total_ffn_flops",
                "qkv_flops",
                "projection_flops",
                "embedding_flops"
            ],
            "parameters": {
                "per_device_batch_size": "Batch size per device.",
                "max_target_length": "Maximum sequence length.",
                "num_query_heads": "Number of query heads in attention.",
                "head_dim": "Dimension of each attention head.",
                "num_decoder_layers": "Number of decoder layers in the model.",
                "sliding_window_size": "Size of the sliding window for local attention."
            },
            "notes": [
                "The calculation accounts for Gemma2's combined local and global attention within a single decoder layer.",
                "It specifically considers the sliding window attention mechanism used in local attention.",
                "The factor of 2 in `learnable_weight_tflops` accounts for the combination of local and global attention into one decoder layer.",
                "The final multiplication by 3 in TFLOP calculations accounts for the forward pass, backward pass, and optimizer updates."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mixed_attention_model_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mixed_attention_model_tflops_training_per_device(\n    config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length\n):\n  \"\"\"\n  Calculate training TFLOPs for models with a mixed attention pattern of local\n  and global attention layers, like Gemma3 and GPT-OSS.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n\n  num_global_layers = num_layers // attention_pattern_length\n  num_local_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention)\n  # Formula: 4 * batch_size * seq_len^2 * num_heads * head_dim\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single local attention layer (sliding window)\n  # Formula: 4 * batch_size * seq_len * window_size * num_heads * head_dim\n  local_attention_flops_per_layer = (\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n\n  # Total attention FLOPs = (num_global_layers * FLOPs_per_global) + (num_local_layers * FLOPs_per_local)\n  noncausal_attention_flops = (\n      num_global_layers * global_attention_flops_per_layer + num_local_layers * local_attention_flops_per_layer\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Convert to TFLOPs and multiply by 3 for fwd/bwd pass\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  # Learnable weights (FFN, QKV, Projections) are present in every layer.\n  learnable_weight_tflops = ((total_ffn_flops + qkv_flops + projection_flops) * num_layers + embedding_flops) * 3 / 10**12\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "module_type": "mixed_attention_model_tflops_calculator",
            "purpose": "Calculates the training TFLOPs for models with a mixed attention pattern, combining local and global attention layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine the number of global and local attention layers based on `attention_pattern_length`.",
                "Calculate FLOPs for a single global attention layer.",
                "Calculate FLOPs for a single local attention layer.",
                "Calculate total non-causal attention FLOPs.",
                "Calculate causal attention FLOPs.",
                "Convert attention FLOPs to TFLOPs and multiply by 3 for forward/backward pass.",
                "Calculate TFLOPs for learnable weights (FFN, QKV, Projections) and embedding layers.",
                "Return attention TFLOPs and learnable weight TFLOPs."
            ],
            "output": {
                "shape": "A tuple containing two float values: (attention_tflops, learnable_weight_tflops)."
            },
            "dependencies": [
                "config object (containing parameters like num_decoder_layers, per_device_batch_size, max_target_length, num_query_heads, head_dim, sliding_window_size)"
            ],
            "parameters": {
                "config": "Configuration object containing model and training parameters.",
                "total_ffn_flops": "Total FLOPs for Feed-Forward Network layers.",
                "qkv_flops": "FLOPs for Query, Key, Value projections.",
                "projection_flops": "FLOPs for attention output projection.",
                "embedding_flops": "FLOPs for token embeddings.",
                "attention_pattern_length": "An integer defining the pattern length for mixed attention (e.g., how often a global attention layer appears)."
            },
            "notes": [
                "Assumes a mixed attention pattern where some layers are global and others are local (sliding window).",
                "The calculation includes FLOPs for both attention mechanisms and learnable weights.",
                "The final TFLOPs are for training (forward and backward pass)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_calculate_chunked_attention_flops_per_layer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size):\n  \"\"\"Calculates the non-causal FLOPs for a single layer of chunked attention.\"\"\"\n  num_chunks = seq_len // chunk_size\n  rem_chunk_size = seq_len % chunk_size\n  # The complexity of chunked attention is the sum of squares of chunk lengths.\n  chunked_complexity = (num_chunks * chunk_size**2) + (rem_chunk_size**2)\n  # The formula for non-causal attention FLOPs is 4 * B * complexity * H * D,\n  # where B=batch_size, H=num_heads, D=head_dim.\n  return 4 * config.per_device_batch_size * chunked_complexity * config.num_query_heads * config.head_dim",
        "analysis": {
            "module_type": "chunked_attention_flops_calculator",
            "purpose": "Calculates the floating-point operations (FLOPs) for a single layer of non-causal chunked attention.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the number of full chunks and the size of the remaining chunk.",
                "Compute the chunked complexity by summing the squares of chunk lengths.",
                "Apply the FLOPs formula: 4 * batch_size * complexity * num_heads * head_dim."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "config object (containing per_device_batch_size, num_query_heads, head_dim)"
            ],
            "parameters": {
                "config": "Configuration object containing model dimensions and batch size.",
                "seq_len": "The total sequence length.",
                "chunk_size": "The size of each attention chunk."
            },
            "notes": [
                "This function specifically calculates FLOPs for non-causal attention.",
                "It assumes a configuration object with relevant dimensions is provided."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_attention_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_attention_tflops(config):\n  \"\"\"\n  Calculates attention-only training TFLOPs for Llama4's specific architecture,\n  which has an alternating pattern of global and chunked attention layers.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n  seq_len = config.max_target_length\n  chunk_size = config.chunk_attn_window_size\n\n  # Determine number of global vs. chunked layers based on the NoPE interval.\n  # A \"NoPE\" layer uses global attention.\n  num_global_layers = num_layers // config.nope_layer_interval\n  num_chunked_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention, non-causal)\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * seq_len**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single chunked attention layer (non-causal)\n  chunked_attention_flops_per_layer = _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size)\n\n  # Total non-causal attention FLOPs is the sum of all global and all chunked layers\n  noncausal_attention_flops = (num_global_layers * global_attention_flops_per_layer) + (\n      num_chunked_layers * chunked_attention_flops_per_layer\n  )\n\n  # Apply causal mask and convert to TFLOPs (multiply by 3 for fwd/bwd pass)\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  return attention_tflops",
        "analysis": {
            "module_type": "llama4_attention_tflops_calculator",
            "purpose": "Calculates the attention-only training TFLOPs for the Llama4 architecture, which features an alternating pattern of global and chunked attention layers.",
            "input": {
                "shape": "N/A (expects a config object)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine the number of global and chunked attention layers based on the `nope_layer_interval` configuration.",
                "Calculate FLOPs for a single global attention layer.",
                "Calculate FLOPs for a single chunked attention layer using the `_calculate_chunked_attention_flops_per_layer` helper function.",
                "Sum the FLOPs for all global and chunked layers to get total non-causal attention FLOPs.",
                "Apply the causal mask by dividing non-causal FLOPs by 2.",
                "Convert the result to TFLOPs by multiplying by 3 (for forward/backward pass) and dividing by 10^12."
            ],
            "output": {
                "shape": "N/A (returns a float representing TFLOPs)"
            },
            "dependencies": [
                "_calculate_chunked_attention_flops_per_layer"
            ],
            "parameters": {
                "num_decoder_layers": "Total number of decoder layers in the model.",
                "max_target_length": "The maximum sequence length for target tokens.",
                "chunk_attn_window_size": "The window size for chunked attention.",
                "nope_layer_interval": "The interval at which 'NoPE' (global attention) layers occur.",
                "per_device_batch_size": "The batch size per device.",
                "num_query_heads": "The number of attention query heads.",
                "head_dim": "The dimension of each attention head."
            },
            "notes": [
                "This function specifically calculates attention FLOPs and does not include FFN or embedding FLOPs.",
                "Assumes a non-causal attention calculation initially, then applies the causal mask.",
                "The final multiplication by 3 accounts for the forward and backward passes during training."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mla_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mla_tflops_per_device(config):\n  \"\"\"Calculate Multi-Head Latent Attention TFLOP\"\"\"\n  batch_len = config.per_device_batch_size * config.max_target_length\n  qk_head_dim_sum = config.qk_nope_head_dim + config.qk_rope_head_dim\n  # calculate mla query projection\n  if config.q_lora_rank == 0:\n    q_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * qk_head_dim_sum\n  else:\n    # calculate query down and up flops\n    q_flops = (\n        2\n        * batch_len\n        * (config.emb_dim * config.q_lora_rank + config.q_lora_rank * config.num_query_heads * qk_head_dim_sum)\n    )\n  # calculate mla kv projection with down and up flops\n  kv_flops = (\n      2\n      * batch_len\n      * (\n          config.emb_dim * (config.kv_lora_rank + config.qk_rope_head_dim)\n          + config.kv_lora_rank * config.num_query_heads * (config.qk_nope_head_dim + config.v_head_dim)\n      )\n  )\n  qkv_flops = q_flops + kv_flops\n\n  attention_flops = (\n      2 * batch_len * config.max_target_length * config.num_query_heads * (qk_head_dim_sum + config.v_head_dim)\n  )\n  projection_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * config.v_head_dim\n  return qkv_flops, attention_flops, projection_flops",
        "analysis": {
            "module_type": "multi_head_latent_attention_tflops_calculator",
            "purpose": "Calculates the Floating Point Operations (FLOPs) for Multi-Head Latent Attention (MLA) per device during training.",
            "input": {
                "shape": "N/A (expects a config object)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate batch length.",
                "Calculate sum of query-key head dimensions.",
                "Calculate query projection FLOPs (conditional on LoRA rank).",
                "Calculate key-value projection FLOPs (conditional on LoRA rank).",
                "Sum query and key-value projection FLOPs.",
                "Calculate attention FLOPs.",
                "Calculate projection FLOPs.",
                "Return calculated FLOPs."
            ],
            "output": {
                "shape": "Tuple of three float values representing QKV FLOPs, attention FLOPs, and projection FLOPs."
            },
            "dependencies": [
                "config object (assumed to have attributes like per_device_batch_size, max_target_length, qk_nope_head_dim, qk_rope_head_dim, q_lora_rank, emb_dim, num_query_heads, kv_lora_rank, v_head_dim)"
            ],
            "parameters": {
                "per_device_batch_size": "Batch size per device.",
                "max_target_length": "Maximum sequence length for the target.",
                "qk_nope_head_dim": "Dimension of query/key heads without RoPE.",
                "qk_rope_head_dim": "Dimension of query/key heads with RoPE.",
                "q_lora_rank": "Rank of the LoRA projection for queries (0 if not using LoRA).",
                "emb_dim": "Embedding dimension of the model.",
                "num_query_heads": "Number of query heads in the attention mechanism.",
                "kv_lora_rank": "Rank of the LoRA projection for key/value.",
                "v_head_dim": "Dimension of value heads."
            },
            "notes": [
                "This function calculates FLOPs for a single attention layer.",
                "It accounts for LoRA ranks in query and key-value projections.",
                "The FLOPs are calculated per device."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_ffn_mamtul_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_ffn_mamtul_tflops_per_device(config, mlp_dim):\n  \"\"\"Helper function to calculate matmul TFLOP in ffn based on MLP dimension.\n\n  Applies to:\n    - Dense FFN layers (mlp_dim = config.mlp_dim).\n    - MoE FFN layers (mlp_dim = config.moe_mlp_dim),\n      need to scale by shared_experts or num_experts_per_tok.\n  \"\"\"\n  ffn1_flops = (\n      2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim * len(config.mlp_activations)\n  )\n  ffn2_flops = 2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim\n  return ffn1_flops + ffn2_flops",
        "analysis": {
            "module_type": "flops_calculation_utils",
            "purpose": "Calculates the floating-point operations (FLOPs) for the feed-forward network (FFN) layers per device during training.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate FLOPs for the first FFN layer (ffn1_flops).",
                "Calculate FLOPs for the second FFN layer (ffn2_flops).",
                "Sum ffn1_flops and ffn2_flops to get the total FFN FLOPs."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "config object (contains per_device_batch_size, max_target_length, emb_dim, mlp_activations)"
            ],
            "parameters": {
                "config": "Configuration object containing model and training parameters.",
                "mlp_dim": "The dimension of the MLP layer, which can be config.mlp_dim for dense FFN or config.moe_mlp_dim for MoE FFN."
            },
            "notes": [
                "This function is a helper for calculating TFLOPs.",
                "It accounts for both dense and MoE FFN layers by taking mlp_dim as an argument.",
                "For MoE layers, the caller needs to scale the result by shared_experts or num_experts_per_tok."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_routed_and_shared_ffn_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_routed_and_shared_ffn_tflops_per_device(config):\n  \"\"\"Helper function to calculate DeepSeek-style ffn TFLOP\"\"\"\n  gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n  # Due to the mixed decoder layers, the flops is multiplied by num of layers for both dense and moe\n  num_dense_layers, num_moe_layers = get_dense_moe_layers(config)\n  dense_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * num_dense_layers\n  shared_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.shared_experts\n  routed_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.num_experts_per_tok\n  moe_ffn_flops = (gate_flops + shared_experts_flops + routed_experts_flops) * num_moe_layers\n  total_ffn_flops = dense_ffn_flops + moe_ffn_flops\n  return total_ffn_flops",
        "analysis": {
            "functionality": "Calculates the TFLOPs (Tera Floating Point Operations) for the Feed-Forward Network (FFN) layers in a DeepSeek-style model configuration.",
            "usage": "Call this function with a configuration object (`config`) that contains parameters like `per_device_batch_size`, `max_target_length`, `emb_dim`, `num_experts`, `mlp_dim`, `moe_mlp_dim`, `shared_experts`, and `num_experts_per_tok`. It returns the total FFN TFLOPs per device for training."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_dense_moe_layers",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_dense_moe_layers(config):\n  \"\"\"Helper function to calculate number of dense and moe layers\"\"\"\n  if config.decoder_block == DecoderBlockType.DEEPSEEK:\n    num_dense_layers = config.first_num_dense_layers\n    num_moe_layers = config.num_decoder_layers - config.first_num_dense_layers\n    return num_dense_layers, num_moe_layers\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    num_moe_layers = config.num_decoder_layers // config.interleave_moe_layer_step\n    num_dense_layers = config.num_decoder_layers - num_moe_layers\n  else:\n    raise ValueError(\"Currently we only support DeepSeek and Llama4 calculation.\")\n\n  return num_dense_layers, num_moe_layers",
        "analysis": {
            "functionality": "Calculates the number of dense and Mixture-of-Experts (MoE) layers based on the configuration.",
            "usage": "Call this function with a configuration object that specifies the decoder block type (e.g., DEEPSEEK, LLAMA4) and layer counts. It returns the count of dense layers and MoE layers."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma3_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma3_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Gemma3 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.image_size_for_vit  # Gemma3 default 896\n  embed_dim = config.emb_dim  # text embedding dim after projection\n  # Values below are hardcoded in Gemma3VisionEncoderLayer\n  patch_size = 14\n  hidden_dim = 1152\n  intermediate_dim = 4304\n  num_layers = 27\n  vision_exit_pooling_window = 4\n\n  # 1. Patch embedding (Conv2D)\n  num_patches_h = H // patch_size\n  num_patches_w = W // patch_size\n  seq_len = num_patches_h * num_patches_w  # 64*64=4096\n  patch_embed_flops = 2 * B * seq_len * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. gemma3.Encoder: num_layers * gemma3.Encoder1DBlock\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 4. VisionEmbedder\n  seq_len_after_pooling = (num_patches_h // vision_exit_pooling_window) * (num_patches_w // vision_exit_pooling_window)\n  vision_embedder_flops = 2 * B * seq_len_after_pooling * hidden_dim * embed_dim  # One linear projection\n\n  # Learnable weights summation\n  learnable_weight_flops = patch_embed_flops + encoder_flops + vision_embedder_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * vision_embedder_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "module_type": "gemma3_vision_layers_tflops_calculator",
            "purpose": "Estimates the TFLOPs for the Gemma3 vision encoder (ViT-style) per device.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract configuration values for batch size, image dimensions, embedding dimensions, and model architecture specifics (patch size, hidden dimensions, number of layers).",
                "Calculate FLOPs for patch embedding (Conv2D).",
                "Calculate FLOPs for the Gemma3 Encoder layers, including QKV projections, attention scores, attention output projection, and MLP layers.",
                "Calculate FLOPs for the VisionEmbedder's linear projection after pooling.",
                "Sum FLOPs for learnable weights (patch embedding, encoder, vision embedder).",
                "Adjust learnable weight FLOPs based on whether vision encoder parameters are frozen (only projector learnable) or fully trainable (multiply by 3 for fwd/bwd/optimizer).",
                "Convert total FLOPs to TFLOPs by dividing by 1e12.",
                "Return total TFLOPs, learnable weight TFLOPs, and attention TFLOPs."
            ],
            "output": {
                "shape": "(total_tflops, learnable_weight_tflops, attention_tflops)"
            },
            "dependencies": [
                "config object (containing per_device_batch_size, num_channels_for_vit, image_size_for_vit, emb_dim, freeze_vision_encoder_params)"
            ],
            "parameters": {
                "per_device_batch_size": "Batch size per device.",
                "num_channels_for_vit": "Number of input channels for the vision transformer.",
                "image_size_for_vit": "The size of the input image for the vision transformer.",
                "emb_dim": "The embedding dimension after projection for text.",
                "freeze_vision_encoder_params": "Boolean flag to indicate if vision encoder parameters are frozen."
            },
            "notes": [
                "This function calculates FLOPs for a Vision Transformer (ViT) style encoder used in Gemma3.",
                "It accounts for patch embedding, encoder layers (attention and MLP), and a final projection layer.",
                "The calculation includes forward pass, backward pass, and optimizer updates for learnable weights.",
                "Specific architectural details like patch size, hidden dimensions, and pooling window are hardcoded based on Gemma3VisionEncoderLayer."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Llama4 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.tile_size_for_vit\n  patch_size = config.patch_size_for_vit\n  hidden_dim = config.hidden_size_for_vit\n  intermediate_dim = config.intermediate_size_for_vit\n  num_layers = config.num_hidden_layers_for_vit\n  pixel_shuffle_fc1_out_dim = config.projector_input_dim_for_vit  # 4096\n  pixel_shuffle_fc2_out_dim = config.projector_output_dim_for_vit  # 4096\n  base_emb_dim = config.base_emb_dim\n  pixel_shuffle_ratio = config.pixel_shuffle_ratio_for_vit  # 0.5\n  num_patches = (H // patch_size) * (W // patch_size)  # 24*24 = 576\n  pixel_shuffle_tokens = num_patches * pixel_shuffle_ratio**2  # 144\n\n  # 1. Llama4UnfoldConvolution (flops by linear projection)\n  # lax.conv_general_dilated_patches extracts patches through reshaping/indexing without flops\n  # Each patch: C * patch_size * patch_size -> hidden_dim\n  patch_embed_flops = 2 * B * num_patches * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. Llama4VisionEncoder: num_layers * (qkv + att_projection + mlp)\n  seq_len = num_patches + 1  # +1 for class token, so 577\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)  # Q, K, V projections\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim  # Attention scores and weighted sum\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  vision_encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 3. Llama4VisionPixelShuffleMLP\n  # (B, 144, 5632) -> (B, 144, 4096) -> (B, 144, 4096)\n  pixel_shuffle_fc1_flops = 2 * B * pixel_shuffle_tokens * intermediate_dim * pixel_shuffle_fc1_out_dim\n  pixel_shuffle_fc2_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * pixel_shuffle_fc2_out_dim\n  pixel_shuffle_total_flops = pixel_shuffle_fc1_flops + pixel_shuffle_fc2_flops\n\n  # 4. Llama4MultiModalProjector: (B, 144, 5120) x (5120, base_emb_dim)\n  projector_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * base_emb_dim\n\n  # Learnable weights: all matmuls above\n  learnable_weight_flops = patch_embed_flops + vision_encoder_flops + pixel_shuffle_total_flops + projector_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * projector_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "module_type": "llama4_vision_layers_tflops_calculator",
            "purpose": "Estimates the TFLOPs for the Llama4 vision encoder, including forward, backward, and optimizer passes.",
            "input": {
                "shape": "N/A (takes a config object)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Extract configuration values for batch size, image dimensions, patch size, hidden dimensions, number of layers, etc.",
                "Calculate FLOPs for patch embedding (linear projection).",
                "Calculate FLOPs for the vision encoder layers (QKV, attention, projection, MLP).",
                "Calculate FLOPs for the Llama4VisionPixelShuffleMLP.",
                "Calculate FLOPs for the Llama4MultiModalProjector.",
                "Sum FLOPs for learnable weights, considering whether the vision encoder is frozen.",
                "Adjust learnable weight FLOPs by a factor of 3 for forward, backward, and optimizer passes.",
                "Convert total FLOPs to TFLOPs.",
                "Calculate total attention TFLOPs.",
                "Return total TFLOPs, learnable weight TFLOPs, and attention TFLOPs."
            ],
            "output": {
                "shape": "Tuple of three float values: (total_tflops, learnable_weight_tflops, attention_tflops)",
                "dtype": "float"
            },
            "dependencies": [
                "config object (containing various model and training parameters)"
            ],
            "parameters": {
                "per_device_batch_size": "Batch size per device.",
                "num_channels_for_vit": "Number of input channels for the vision transformer.",
                "tile_size_for_vit": "The spatial dimension (height/width) of the input image tiles.",
                "patch_size_for_vit": "The size of the patches extracted from the image.",
                "hidden_size_for_vit": "The hidden dimension size of the vision encoder layers.",
                "intermediate_size_for_vit": "The intermediate dimension size of the MLP in the vision encoder layers.",
                "num_hidden_layers_for_vit": "The number of layers in the vision encoder.",
                "projector_input_dim_for_vit": "The input dimension of the first linear layer in the pixel shuffle projector.",
                "projector_output_dim_for_vit": "The output dimension of the second linear layer in the pixel shuffle projector.",
                "base_emb_dim": "The base embedding dimension of the model.",
                "pixel_shuffle_ratio_for_vit": "Ratio used in pixel shuffle for downsampling.",
                "freeze_vision_encoder_params": "Boolean flag indicating if vision encoder parameters are frozen."
            },
            "notes": [
                "This function calculates TFLOPs for a Vision Transformer (ViT) style encoder used in Llama4.",
                "It accounts for patch embedding, encoder layers (QKV, attention, MLP), and multimodal projection.",
                "The calculation includes FLOPs for forward pass, backward pass, and optimizer updates.",
                "It differentiates between learnable weight FLOPs and attention FLOPs.",
                "Assumes specific architectural components like Llama4UnfoldConvolution, Llama4VisionEncoder, Llama4VisionPixelShuffleMLP, and Llama4MultiModalProjector."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_vision_encoder_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_vision_encoder_tflops(config):\n  \"\"\"Calculate vision encoder TFLOPs per prefill step per device.\"\"\"\n  if config.model_name.startswith(\"gemma3\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_gemma3_vision_layers_tflops_per_device(\n        config\n    )\n  elif config.model_name.startswith(\"llama4\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_llama4_vision_layers_tflops_per_device(\n        config\n    )\n  else:\n    max_logging.log(\n        f\"Vision encoder TFLOPs calculation not implemented for model {config.model_name}, counting as 0 for now.\"\n    )\n    mm_total_tflops = mm_learnable_weight_tflops = mm_attention_tflops = 0\n\n  return mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops",
        "analysis": {
            "functionality": "Calculates the TFLOPs (Tera Floating Point Operations) for the vision encoder component of a model per prefill step per device.",
            "usage": "This function takes a configuration object (`config`) as input. It determines the model name from `config.model_name` and calls a specific TFLOP calculation function (`calculate_gemma3_vision_layers_tflops_per_device` or `calculate_llama4_vision_layers_tflops_per_device`) based on whether the name starts with 'gemma3' or 'llama4'. If the model name does not match, it logs a message and returns zero TFLOPs. The function returns a tuple containing the total TFLOPs, TFLOPs from learnable weights, and TFLOPs from attention operations for the vision encoder."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tflops_training_per_device(config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  # MLP flops\n  if config.num_experts > 1:\n    # calculation based on dropless implementation\n    if config.decoder_block in (DecoderBlockType.DEEPSEEK, DecoderBlockType.LLAMA4):\n      total_ffn_flops = calculate_routed_and_shared_ffn_tflops_per_device(config)\n    else:\n      gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n      total_ffn_flops = (\n          gate_flops + calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * config.num_experts_per_tok\n      )\n  else:\n    total_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim)\n\n  # Attention flops\n  if config.attention_type == \"mla\":\n    qkv_flops, noncausal_attention_flops, projection_flops = calculate_mla_tflops_per_device(config)\n  else:\n    qkv_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * (config.num_query_heads + 2 * config.num_kv_heads)\n        * config.head_dim\n    )\n    noncausal_attention_flops = (\n        4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n    )\n    projection_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * config.num_query_heads\n        * config.head_dim\n    )\n\n  # Divide attention flops by 2 due to causal mask\n  # References:\n  # NVIDIA/Megatron-LM (2025 March): https://github.com/NVIDIA/Megatron-LM/blob/250b79415dcc4b660521273c87f15334c804eeae/megatron/training/training.py#L361-L362\n  # NVIDIA/NeMo (2025 April): https://github.com/NVIDIA/NeMo/blob/ba4d6d116463de512ff0cfc14641aa6cf4577a42/nemo/utils/flops_formulas.py#L259-L272\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Embedding flops\n  embedding_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.vocab_size\n\n  # Combine flops with number of decoder layers\n  if config.decoder_block == DecoderBlockType.GEMMA2:\n    attention_tflops, learnable_weight_tflops = calculate_gemma2_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops\n    )\n  elif config.decoder_block == DecoderBlockType.GEMMA3:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=6\n    )\n  elif config.decoder_block == DecoderBlockType.GPT_OSS:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=2\n    )\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    # Use the new helper to calculate attention TFLOPs correctly.\n    attention_tflops = calculate_llama4_attention_tflops(config)\n    # The learnable weight calculation remains the same as it correctly handles Llama4's MoE structure.\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n  elif config.decoder_block == DecoderBlockType.DEEPSEEK:\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n  else:\n    # multiply by 3 for both feed forward and back propagation flops\n    learnable_weight_tflops = (\n        ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  learnable_weight_tflops = learnable_weight_tflops * config.gradient_accumulation_steps\n  attention_tflops = attention_tflops * config.gradient_accumulation_steps\n\n  # DPO includes one additional forward pass per gradient accumulation step\n  if config.use_dpo:\n    reference_model_tflops = learnable_weight_tflops / 3  # additional forward pass\n    reference_model_attention_tflops = attention_tflops / 3\n    attention_tflops = attention_tflops + reference_model_attention_tflops\n  else:\n    reference_model_tflops = 0\n\n  total_tflops = learnable_weight_tflops + attention_tflops + reference_model_tflops\n\n  if config.use_multimodal:\n    # Add vision layers TFLOPs for multimodal models\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_vision_encoder_tflops(config)\n    if log:\n      print(\n          f\"{config.model_name} vision layers per train step:\\n\",\n          f\"Total TFLOPs: {mm_total_tflops:.2f} \\n\",\n          f\"split as {100 * mm_learnable_weight_tflops/mm_total_tflops:.2f}% learnable weight flops\",\n          f\"and {100 * mm_attention_tflops/mm_total_tflops:.2f}% attention flops;\\n\",\n          f\"learnable weight {mm_learnable_weight_tflops:.2f} TFLOPs, attention {mm_attention_tflops:.2f} TFLOPs\",\n      )\n    total_tflops += mm_total_tflops\n    learnable_weight_tflops += mm_learnable_weight_tflops\n    attention_tflops += mm_attention_tflops\n\n  if log:\n    print(\n        \"Per train step:\\n\",\n        f\"Total TFLOPs: {total_tflops:.2f} \\n\",\n        f\"split as {100 * learnable_weight_tflops/total_tflops:.2f}% learnable weight flops\",\n        f\"and {100 * attention_tflops/total_tflops:.2f}% attention flops\",\n    )\n  return total_tflops, learnable_weight_tflops, attention_tflops",
        "analysis": {
            "module_type": "training_flops_calculator",
            "purpose": "Calculates the total Floating Point Operations (FLOPs) for training a model per device, broken down into learnable weight and attention components.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim] (implicitly via config)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate MLP (Feed-Forward Network) FLOPs, considering MoE (Mixture of Experts) if applicable.",
                "Calculate Attention FLOPs, handling different attention types (e.g., standard, MLA).",
                "Adjust attention FLOPs for causal masking.",
                "Calculate Embedding FLOPs.",
                "Combine FLOPs based on the specific decoder block type (e.g., Gemma2, Llama4, DeepSeek).",
                "Scale FLOPs by the number of gradient accumulation steps.",
                "Incorporate additional FLOPs if Direct Preference Optimization (DPO) is used.",
                "Add vision encoder FLOPs for multimodal models.",
                "Optionally log the calculated FLOPs."
            ],
            "output": {
                "shape": "(total_tflops, learnable_weight_tflops, attention_tflops)",
                "dtype": "Tuple[float, float, float]"
            },
            "dependencies": [
                "config object (containing model dimensions, attention type, decoder block type, etc.)",
                "DecoderBlockType enum",
                "calculate_routed_and_shared_ffn_tflops_per_device",
                "calculate_ffn_mamtul_tflops_per_device",
                "calculate_mla_tflops_per_device",
                "calculate_gemma2_tflops_training_per_device",
                "calculate_mixed_attention_model_tflops_training_per_device",
                "calculate_llama4_attention_tflops",
                "calculate_vision_encoder_tflops"
            ],
            "parameters": {
                "config": "Configuration object containing model architecture details, batch size, sequence length, etc.",
                "log": "Boolean flag to control whether to print the FLOPs breakdown."
            },
            "notes": [
                "The calculation accounts for both forward and backward passes (multiplied by 3).",
                "Handles different attention mechanisms and MoE configurations.",
                "Includes specific logic for multimodal models by calling `calculate_vision_encoder_tflops`.",
                "The FLOPs are calculated per device and per training step."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_prefill_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_prefill_tflops_per_device(num_model_parameters, prefill_length, config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  learnable_weight_tflops = 2 * num_model_parameters * prefill_length / jax.device_count() / 1e12\n  noncausal_attention_flops = (\n      4\n      * config.num_query_heads\n      * config.num_decoder_layers\n      * config.head_dim\n      * prefill_length**2\n      / jax.device_count()\n      / 1e12\n  )\n  causal_attention_tflops = noncausal_attention_flops / 2  # due to causality in attention\n  total_tflops = learnable_weight_tflops + causal_attention_tflops\n\n  if log:\n    print(\n        \"Per prefill step per device: \\n\",\n        f\"\\tTotal TFLOPs: {total_tflops:.2f} \\n\",\n        f\"\\t\\tLearnable weight TFLOPs: {learnable_weight_tflops:.2f} \",\n        f\"({100 * learnable_weight_tflops/total_tflops:.2f})% of Total\\n\",\n        f\"\\t\\tCausal attention TFLOPs: {causal_attention_tflops:.2f} \",\n        f\"({100 * causal_attention_tflops/total_tflops:.2f})% of Total\",\n    )\n  return total_tflops, learnable_weight_tflops, causal_attention_tflops",
        "analysis": {
            "module_type": "prefill_tflops_calculator",
            "purpose": "Calculates the theoretical TFLOPs (Tera Floating Point Operations) per device for a prefill step in a language model.",
            "input": {
                "shape": "[N/A]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate learnable weight TFLOPs based on model parameters and prefill length.",
                "Calculate non-causal attention FLOPs.",
                "Adjust attention FLOPs to be causal.",
                "Sum learnable weight and causal attention TFLOPs to get total TFLOPs.",
                "Optionally print a breakdown of TFLOPs if log is True."
            ],
            "output": {
                "shape": "[total_tflops, learnable_weight_tflops, causal_attention_tflops]"
            },
            "dependencies": [
                "jax"
            ],
            "parameters": {
                "num_model_parameters": "The total number of trainable parameters in the model.",
                "prefill_length": "The length of the input sequence during the prefill phase.",
                "config": "A configuration object containing model architecture details like num_query_heads, num_decoder_layers, and head_dim.",
                "log": "A boolean flag to control whether to print the TFLOPs breakdown."
            },
            "notes": [
                "The calculation assumes a standard transformer architecture for attention FLOPs.",
                "TFLOPs are divided by jax.device_count() to get per-device values.",
                "The formula for causal attention FLOPs is half of the non-causal attention FLOPs.",
                "The output is a tuple containing total TFLOPs, learnable weight TFLOPs, and causal attention TFLOPs."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#apply_gradient_clipping",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def apply_gradient_clipping(raw_grads, state, clipping_threshold):\n  \"\"\"Applies gradient clipping to raw gradients, with special handing for FLAX fp8 stats.\n\n  Args:\n    raw_grads: A pytree of raw gradients.\n    state: The current optimizer state.\n    clipping_threshold: The gradient clipping threshold.\n\n  Returns:\n    A pytree of clipped gradients.\n  \"\"\"\n  gradient_clip_transformation = optax.clip_by_global_norm(clipping_threshold)\n  if OVERWRITE_WITH_GRADIENT in raw_grads:\n    # Scales + Amax History for Delayed Tensor Scaling SHOULD NOT be clipped or affect clipping\n    fp8_stats = raw_grads.pop(OVERWRITE_WITH_GRADIENT)\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n    grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n    raw_grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n  else:\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n\n  return grads",
        "analysis": {
            "module_type": "gradient_clipping_utility",
            "purpose": "Applies gradient clipping to raw gradients, with special handling for FLAX fp8 stats.",
            "input": {
                "shape": "raw_grads: pytree of gradients, state: optimizer state, clipping_threshold: float",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize gradient clipping transformation using optax.clip_by_global_norm.",
                "Check if OVERWRITE_WITH_GRADIENT key exists in raw_grads.",
                "If it exists, pop the associated fp8_stats, apply clipping to the remaining gradients, and re-insert fp8_stats.",
                "If it does not exist, apply clipping to all raw_grads.",
                "Return the clipped gradients."
            ],
            "output": {
                "shape": "A pytree of clipped gradients."
            },
            "dependencies": [
                "optax.clip_by_global_norm",
                "OVERWRITE_WITH_GRADIENT (constant)"
            ],
            "parameters": {
                "clipping_threshold": "The maximum global norm for gradient clipping."
            },
            "notes": [
                "The function specifically handles a key `_overwrite_with_gradient` which is assumed to contain FP8 statistics that should not be clipped.",
                "The `state` parameter is used by the `optax.clip_by_global_norm.update` method.",
                "The second return value of `gradient_clip_transformation.update` (which is typically the new optimizer state) is ignored."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_nested_value",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_nested_value(dictionary, nested_key, default=None):\n  \"\"\"\n  Retrieves a value from a nested key in a dictionary.\n\n  Args:\n      dictionary: The dictionary to search in.\n      nested_key: A tuple representing the nested key, e.g., ('level1', 'level2', 'key').\n      default: The value to return if the nested key is not found.\n\n  Returns:\n      The value associated with the nested key, or the default value if not found.\n  \"\"\"\n  current_level = dictionary\n\n  for key in nested_key:\n    if not isinstance(current_level, dict) or key not in current_level:\n      return default\n    current_level = current_level[key]\n  return current_level",
        "analysis": {
            "module_type": "dictionary_utils",
            "purpose": "Retrieves a value from a nested key within a dictionary, providing a default value if the key is not found.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize `current_level` with the input dictionary.",
                "Iterate through each `key` in the `nested_key` tuple.",
                "In each iteration, check if `current_level` is a dictionary and if the `key` exists within it.",
                "If not, return the `default` value.",
                "If the `key` exists, update `current_level` to the value associated with that `key`.",
                "After iterating through all keys, return the final `current_level`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "dictionary": "The dictionary to search within.",
                "nested_key": "A tuple of keys representing the path to the desired value.",
                "default": "The value to return if the nested key path does not exist in the dictionary."
            },
            "notes": [
                "The `nested_key` is expected to be a tuple, e.g., ('level1', 'level2', 'key').",
                "If the `nested_key` is empty or invalid, the function will return the `default` value."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_decode_state(apply_fn, params) -> train_state.TrainState:\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState(step=0, apply_fn=apply_fn, params=params, tx=None, opt_state={})  # type: ignore\n  return state",
        "analysis": {
            "module_type": "train_state_initializer",
            "purpose": "Initializes a Flax train state specifically for decoding purposes, with a null optimizer state.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a `train_state.TrainState` object.",
                "Initializes the `step` to 0.",
                "Assigns the provided `apply_fn` to `apply_fn`.",
                "Assigns the provided `params` to `params`.",
                "Sets the optimizer transformation (`tx`) to `None`.",
                "Initializes the optimizer state (`opt_state`) as an empty dictionary."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {
                "apply_fn": "The function to be applied by the model (e.g., model.apply).",
                "params": "The model parameters."
            },
            "notes": [
                "This function is intended for inference or decoding scenarios where an optimizer is not needed.",
                "The `type: ignore` comment suggests potential type checking issues that are being bypassed."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_training_state(apply_fn, params, tx):\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState.create(apply_fn=apply_fn, params=params, tx=tx)\n  return state",
        "analysis": {
            "module_type": "training_state_initializer",
            "purpose": "Initializes a training state object for use with an optimizer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a `train_state.TrainState` object using the provided `apply_fn`, `params`, and `tx`.",
                "The `TrainState.create` method is used, which typically initializes the optimizer state based on the `tx` (optimizer transformation)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {
                "apply_fn": "The function to be applied during training (e.g., the model's `apply` method).",
                "params": "The model parameters.",
                "tx": "The optax optimizer transformation (e.g., optax.adamw)."
            },
            "notes": [
                "This function is a utility to create a standard training state object.",
                "It specifically uses `TrainState.create`, which handles the initialization of the optimizer's internal state (like momentum buffers)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_initial_state(model, tx, config, is_training, key):\n  \"\"\"\n  We pass in \"static\" objects like model, tx, config as JAX compares them by\n  object hash, and instantiating them inside causes pjit top-level annotations\n  to fail to match as pytree prefixes if we re-instantiate.\n\n  Args: model, tx, config, is_training, key\n  \"\"\"\n  input_shape = (config.micro_batch_size_to_train_on, config.max_target_length)\n  image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n      config.model_name, batch_size=config.micro_batch_size_to_train_on\n  )\n  model_vars = model.init(\n      {\"params\": key, \"dropout\": key, \"aqt\": key},\n      np.ones(input_shape, dtype=jnp.int32),\n      np.ones(input_shape, dtype=jnp.int32),\n      encoder_images=np.ones(image_shape, dtype=jnp.int32) if config.use_multimodal else None,\n      # nnx_method=\"no_op\",\n  )\n  if is_training:\n    return init_training_state(model.apply, model_vars, tx)\n  return init_decode_state(model.apply, model_vars)",
        "analysis": {
            "functionality": "Initializes the model's state for either training or decoding.",
            "usage": "Call this function with the model, optimizer, configuration, a boolean indicating if training, and a JAX random key. It returns the initialized training or decoding state."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_abstract_param",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_abstract_param(model, config):\n  \"\"\"Get abstract model structure (name, shape) without materializing the weights to save memory\"\"\"\n  key = jax.random.PRNGKey(0)\n  input_shape = (config.micro_batch_size_to_train_on, config.max_target_length)\n  image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n      config.model_name, batch_size=config.micro_batch_size_to_train_on\n  )\n  abstract_vars = jax.eval_shape(\n      model.init,\n      {\"params\": key, \"dropout\": key, \"aqt\": key},\n      jnp.ones(input_shape, dtype=jnp.int32),\n      jnp.ones(input_shape, dtype=jnp.int32),\n      encoder_images=np.ones(image_shape, dtype=jnp.int32) if config.use_multimodal else None,\n  )\n  return abstract_vars",
        "analysis": {
            "functionality": "Calculates the abstract shape and dtype of model parameters without materializing weights.",
            "usage": "Call this function with the model and configuration objects. It returns an abstract representation of the model's variables, useful for memory-saving analysis or shape inference."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_decode_state(model, config, rng, mesh, checkpoint_manager):\n  \"\"\"Setup decode state by loading params from a checkpoint.\n  Args:\n    model: the flax model to initialize\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: Checkpoint manager\n\n  Returns:\n    state: state with decode params loaded from the checkpoint\n    state_mesh_annotations: the mesh annotations for the state\n  \"\"\"\n  if not config.load_parameters_path:\n    # generate random params\n    max_logging.log(\"No decode checkpoint specified - generating random weights.\")\n    state, state_mesh_annotations, _, _ = setup_initial_state(\n        model, None, None, config, rng, mesh, checkpoint_manager, False\n    )\n  else:\n    # Load params from checkpoint\n    max_logging.log(f\"Loading decode params from {config.load_parameters_path}\")\n    unboxed_abstract_state, state_mesh_annotations, _ = get_abstract_state(model, None, config, rng, mesh, False)\n    with nn_partitioning.axis_rules(config.logical_axis_rules):\n      params = checkpointing.load_params_from_path(\n          config.load_parameters_path,\n          unboxed_abstract_state.params,\n          config.checkpoint_storage_concurrent_gb,\n          config.checkpoint_storage_use_ocdbt,\n          config.checkpoint_storage_use_zarr3,\n      )\n    state = init_decode_state(None, params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n  return state, state_mesh_annotations",
        "analysis": {
            "module_type": "decode_state_setup",
            "purpose": "Sets up the decode state for a model, either by loading parameters from a checkpoint or generating random ones.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.load_parameters_path` is provided.",
                "If not, log a message and call `setup_initial_state` to generate random parameters.",
                "If yes, log a message indicating checkpoint loading.",
                "Call `get_abstract_state` to get abstract state and mesh annotations.",
                "Use `nn_partitioning.axis_rules` to define partitioning rules.",
                "Call `checkpointing.load_params_from_path` to load parameters from the specified path.",
                "Call `init_decode_state` to initialize the decode state with loaded parameters.",
                "Call `max_utils.unbox_logicallypartioned` to unbox the state.",
                "Return the state and its mesh annotations."
            ],
            "output": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "dependencies": [
                "setup_initial_state",
                "get_abstract_state",
                "nn_partitioning",
                "checkpointing.load_params_from_path",
                "init_decode_state",
                "max_utils.unbox_logicallypartioned",
                "max_logging"
            ],
            "parameters": {
                "load_parameters_path": "Path to the checkpoint from which to load parameters.",
                "logical_axis_rules": "Rules for logical axis partitioning.",
                "checkpoint_storage_concurrent_gb": "Concurrent GB for checkpoint storage.",
                "checkpoint_storage_use_ocdbt": "Boolean flag to use OCDBT for checkpoint storage.",
                "checkpoint_storage_use_zarr3": "Boolean flag to use Zarr3 for checkpoint storage."
            },
            "notes": [
                "This function handles the initialization of the model's state for decoding purposes.",
                "It provides a fallback to random initialization if no checkpoint is specified.",
                "The mesh annotations are returned alongside the state."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_training_state(model, data_iterator, tx, config, rng, mesh, checkpoint_manager):\n  is_training = True\n  return setup_initial_state(\n      model,\n      data_iterator,\n      tx,\n      config,\n      rng,\n      mesh,\n      checkpoint_manager,\n      is_training,\n  )",
        "analysis": {
            "module_type": "training_state_setup",
            "purpose": "Sets up the initial training state for a model, including parameters, optimizer, and data iterator.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Sets `is_training` flag to True.",
                "Calls `setup_initial_state` with all provided arguments and `is_training=True`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "setup_initial_state"
            ],
            "parameters": {},
            "notes": [
                "This function is a wrapper around `setup_initial_state` specifically for training scenarios."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_initial_state(\n    model,\n    data_iterator,\n    tx,\n    config,\n    rng,\n    mesh,\n    checkpoint_manager,\n    is_training=True,\n):\n  \"\"\"We initialize the model and optimizer state, and optionally load from a\n  checkpoint as necessary.\n\n  Args:\n    model: the flax model to initialize\n    tx: the optax.GradientTransformation\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: an Orbax checkpointing.CheckpointManager object\n    is_training: True to initialize training state, False for decode state\n\n  Returns:\n    state: the initialized train state\n    state_mesh_annotations: the mesh annotations for the train state\n  \"\"\"\n\n  unboxed_abstract_state, state_mesh_annotations, state_mesh_shardings = get_abstract_state(\n      model, tx, config, rng, mesh, is_training\n  )\n\n  # Initialization\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    restored, raw_params = checkpointing.load_state_if_possible(\n        checkpoint_manager,\n        data_iterator,\n        config.load_parameters_path,\n        config.load_full_state_path,\n        config.checkpoint_storage_concurrent_gb,\n        unboxed_abstract_state,\n        config.enable_single_replica_ckpt_restoring,\n        config.dataset_type,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n        enable_orbax_v1=config.enable_orbax_v1,\n        checkpoint_conversion_fn=config.checkpoint_conversion_fn,\n        source_checkpoint_layout=config.source_checkpoint_layout,\n        expansion_factor_real_data=config.expansion_factor_real_data,\n    )\n\n    if restored:\n      if isinstance(\n          checkpoint_manager,\n          (\n              emergency_checkpoint_manager.CheckpointManager,\n              emergency_replicator_checkpoint_manager.ReplicatorCheckpointManager,\n          ),\n      ):\n        state = restored\n      else:\n        # The update of data_iterator state happens in place, no need to assign explicitly\n        state = restored[\"items\"]\n    else:\n      init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training)\n      init_state_partial.__name__ = \"initialize_state\"\n      # pylint: disable=not-callable\n      state = jax.jit(\n          init_state_partial,\n          in_shardings=None,\n          out_shardings=state_mesh_shardings,\n      )(rng)\n      if raw_params:  # If we loaded a partial state, we need to merge it.\n        state = state.replace(params=raw_params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n\n  return state, state_mesh_annotations, state_mesh_shardings, data_iterator",
        "analysis": {
            "functionality": "Initializes the model and optimizer state, with options to load from a checkpoint.",
            "usage": "Call this function with the model, optimizer, configuration, random key, mesh, and checkpoint manager. It returns the initialized state, its mesh annotations, shardings, and the data iterator. It can be used for both training and decoding states."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_abstract_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_abstract_state(model, tx, config, rng, mesh, is_training=True):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n  init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training, rng)\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    abstract_state = jax.eval_shape(init_state_partial)\n\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n\n  state_mesh_shardings = nn.logical_to_mesh_sharding(state_logical_annotations, mesh, config.logical_axis_rules)\n  if is_training and config.shard_optimizer_over_data:\n    # Add data to sharding for optimizer state\n    state_mesh_shardings = state_mesh_shardings.replace(\n        opt_state=jax.tree.map_with_path(\n            functools.partial(sharding.add_data_to_sharding, mesh),\n            max_utils.unbox_logicallypartioned(abstract_state).opt_state,\n            state_mesh_shardings.opt_state,\n        )\n    )\n  if is_training and config.optimizer_memory_host_offload:\n    opt_state = jax.tree_util.tree_map(lambda x: x.with_memory_kind(kind=\"pinned_host\"), state_mesh_shardings.opt_state)\n    state_mesh_shardings = state_mesh_shardings.replace(opt_state=opt_state)\n  if is_training and config.parameter_memory_host_offload:\n    assert config.param_scan_axis == 0, \"You must set the scan axis 0 to enable parameter offloading.\"\n\n    def move(path, x):\n      max_logging.log(f\"max_utils.py: Moving {path} to host\")\n      return x.with_memory_kind(kind=\"pinned_host\")\n\n    params = jax.tree_util.tree_map_with_path(move, state_mesh_shardings.params)\n    state_mesh_shardings = state_mesh_shardings.replace(params=params)\n\n  abstract_sharded_state = jax.jit(init_state_partial, in_shardings=None, out_shardings=state_mesh_shardings).eval_shape()\n\n  unboxed_abstract_sharded_state = max_utils.unbox_logicallypartioned(abstract_sharded_state)\n  # Initialization\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return (\n      unboxed_abstract_sharded_state,\n      state_mesh_annotations,\n      state_mesh_shardings,\n  )",
        "analysis": {
            "module_type": "state_initialization_utils",
            "purpose": "Provides utility functions for initializing and abstracting model states, including handling sharding and memory optimizations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines `init_initial_state` to create an initial model state.",
                "Uses `jax.eval_shape` with `functools.partial` to get an abstract shape of the state.",
                "Determines logical annotations and mesh shardings for the state.",
                "Applies optional sharding, host offloading, and parameter memory optimizations based on configuration.",
                "Uses `jax.jit` with `eval_shape` to get the abstract sharded state.",
                "Converts logical annotations to mesh annotations.",
                "Returns the unboxed abstract state, mesh annotations, and mesh shardings."
            ],
            "output": {
                "shape": "N/A (returns multiple values: unboxed abstract state, mesh annotations, mesh shardings)"
            },
            "dependencies": [
                "functools",
                "flax.linen as nn",
                "flax.linen.partitioning as nn_partitioning",
                "jax",
                "jax.numpy as jnp",
                "MaxText.max_utils",
                "MaxText.sharding",
                "MaxText.checkpointing"
            ],
            "parameters": {
                "model": "The Flax model instance.",
                "tx": "The optax optimizer transformation.",
                "config": "Configuration object containing various settings like sharding rules, memory offloading, etc.",
                "rng": "JAX random number generator key.",
                "mesh": "The JAX device mesh.",
                "is_training": "Boolean flag to indicate if the state is for training or decoding."
            },
            "notes": [
                "This function is crucial for setting up the initial state of the model, especially in distributed training scenarios.",
                "It handles complex sharding and memory management configurations.",
                "The `init_initial_state` function is partially applied and then evaluated using `jax.eval_shape` to determine the state's structure without materializing it."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_prefill_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_prefill_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (\n        config.micro_batch_size_to_train_on,\n        config.max_prefill_predict_length,\n    )\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_PREFILL,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "module_type": "kv_cache_annotations",
            "purpose": "Calculates and returns the mesh annotations for the KV cache state of a model during prefill.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines an inner function `init_kv_cache` to initialize the KV cache.",
                "Determines the input and image shapes based on the configuration.",
                "Initializes the model variables using `model.init` in `MODEL_MODE_PREFILL`.",
                "Extracts the 'cache' from the initialized model variables.",
                "Uses `functools.partial` to create a partial function for `init_kv_cache`.",
                "Evaluates the shape of the partial function using `jax.eval_shape` to get an abstract state.",
                "Retrieves the logical annotations (partition spec) of the abstract state using `nn.get_partition_spec`.",
                "Converts the logical annotations to mesh annotations using `nn.logical_to_mesh` within the provided mesh and axis rules."
            ],
            "output": {
                "shape": "The shape of the KV cache annotations, which depends on the model architecture and configuration."
            },
            "dependencies": [
                "flax.linen as nn",
                "flax.linen.partitioning as nn_partitioning",
                "jax",
                "jax.numpy as jnp",
                "multimodal_utils",
                "MaxText.common_types.MODEL_MODE_PREFILL",
                "MaxText.inference.page_manager.PageState"
            ],
            "parameters": {
                "config.micro_batch_size_to_train_on": "The micro-batch size for training.",
                "config.max_prefill_predict_length": "The maximum length for prefill prediction.",
                "config.model_name": "The name of the model, used to determine dummy image shape.",
                "config.use_multimodal": "Boolean flag indicating if the model uses multimodal inputs.",
                "config.logical_axis_rules": "Defines the logical axis rules for partitioning."
            },
            "notes": [
                "This function is used to get the sharding annotations for the KV cache during the prefill phase of generation.",
                "It leverages JAX's `eval_shape` to determine the shape and sharding without actually initializing the model weights.",
                "The `page_state` parameter is optional and can be used for paged attention mechanisms."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (config.micro_batch_size_to_train_on, 1)\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(\n        config.model_name, batch_size=config.micro_batch_size_to_train_on\n    )\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_AUTOREGRESSIVE,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "module_type": "kv_cache_annotations",
            "purpose": "Gets a shaped abstraction of the KV cache state for model initialization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines an inner function `init_kv_cache` to initialize the KV cache.",
                "Calculates the input shape based on configuration.",
                "Initializes the model using `model.init` with dummy inputs and specific model mode.",
                "Extracts the 'cache' from the model's initialized variables.",
                "Uses `jax.eval_shape` to get the abstract shape of the KV cache.",
                "Applies axis rules for partitioning.",
                "Converts logical annotations to mesh annotations."
            ],
            "output": {
                "shape": "Describes the shape and sharding of the KV cache."
            },
            "dependencies": [
                "flax.linen",
                "flax.linen.partitioning",
                "jax",
                "jax.numpy",
                "functools",
                "multimodal_utils",
                "PageState"
            ],
            "parameters": {
                "config.micro_batch_size_to_train_on": "Specifies the micro-batch size for training.",
                "config.model_name": "The name of the model, used for multimodal image shape initialization.",
                "config.use_multimodal": "Boolean flag indicating if multimodal features are used.",
                "config.logical_axis_rules": "Defines the logical axis rules for partitioning."
            },
            "notes": [
                "The function is designed to work with JAX's `eval_shape` to determine the shape and sharding of the KV cache without materializing the actual weights.",
                "It handles both standard and multimodal model initializations.",
                "The `model_mode` is set to `MODEL_MODE_AUTOREGRESSIVE`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#save_quantized_checkpoint_if_configured",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def save_quantized_checkpoint_if_configured(config, params):\n  \"\"\"Save quantized checkpoint if configured\"\"\"\n  assert config.quantization, \"quantization must be configured\"\n  if config.save_quantized_params_path:\n    checkpointing.save_params_to_path(\n        checkpoint_dir=config.save_quantized_params_path,\n        params=params,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n    )\n  else:\n    max_logging.log(\"Skipping saving quantized checkpoint as save_quantized_params_path is null.\")",
        "analysis": {
            "module_type": "save_quantized_checkpoint",
            "purpose": "Saves quantized model parameters to a specified path if quantization is configured and a save path is provided.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that quantization is configured.",
                "Check if 'save_quantized_params_path' is set in the config.",
                "If the path is set, call 'checkpointing.save_params_to_path' with relevant configuration options.",
                "If the path is not set, log a message indicating that saving is skipped."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "checkpointing.save_params_to_path",
                "max_logging.log"
            ],
            "parameters": {
                "config": "Configuration object containing settings for quantization and saving paths.",
                "params": "The model parameters to be saved."
            },
            "notes": [
                "This function is conditional on `config.save_quantized_params_path` being non-null.",
                "It uses `config.checkpoint_storage_use_ocdbt` and `config.checkpoint_storage_use_zarr3` to configure the saving mechanism."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#add_config_to_summary_writer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def add_config_to_summary_writer(config, summary_writer):\n  \"\"\"Writes config params to tensorboard\"\"\"\n  if jax.process_index() == 0:\n    for key, value in config.get_keys().items():\n      max_utils.add_text_to_summary_writer(key, str(value), summary_writer)",
        "analysis": {
            "module_type": "config_logging",
            "purpose": "Writes configuration parameters to a TensorBoard summary writer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the current JAX process is the primary one (process_index == 0).",
                "Iterate through key-value pairs of configuration parameters obtained from config.get_keys().",
                "For each key-value pair, convert the value to a string.",
                "Use max_utils.add_text_to_summary_writer to write the key and stringified value to the summary writer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "max_utils"
            ],
            "parameters": {
                "config": "An object containing configuration parameters, expected to have a get_keys() method.",
                "summary_writer": "An object representing the TensorBoard summary writer."
            },
            "notes": [
                "This function is intended to be called only on the primary process (rank 0) to avoid redundant logging.",
                "The configuration values are converted to strings before being written."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_device_mesh",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_device_mesh(config, devices=None):\n  \"\"\"Creates a device mesh with each slice in its own data parallel group. If there is only one slice, uses two replicas\"\"\"\n  if devices is None:\n    devices = jax.devices()\n  if config.subslice_shape and config.enable_single_controller and config.num_slices == 1:\n    max_logging.log(f\"Trying to create a subslice with shape: {config.subslice_shape}\")\n    subslice_shape = tuple(int(x) for x in config.subslice_shape.split(\",\"))\n    device_coords = [device.coords for device in devices]\n    device_coords_np = np.array(device_coords)\n\n    # Find the minimum coordinates to start the subslice\n    min_coords = device_coords_np.min(axis=0)\n\n    subslice_devices = []\n    for device in devices:\n      coords = device.coords\n      if all(min_coords[i] <= coords[i] < min_coords[i] + subslice_shape[i] for i in range(len(subslice_shape))):\n        subslice_devices.append(device)\n    devices = subslice_devices\n\n  num_devices = len(devices)\n  num_slices = 1 if config.inference_benchmark_test else config.num_slices\n  num_devices_per_slice = num_devices // num_slices\n\n  multi_slice_env = num_slices > 1\n\n  # Find possible unspecified parallelisms\n  ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), num_devices_per_slice, \"ICI\")\n\n  allow_split_physical_axes = config.allow_split_physical_axes if config.allow_split_physical_axes else False\n\n  if multi_slice_env:\n    dcn_parallelism = max_utils.fill_unspecified_mesh_axes(config.dcn_parallelism.copy(), num_slices, \"DCN\")\n    if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n      mesh = max_utils.create_custom_device_mesh(ici_parallelism, dcn_parallelism, devices, config.custom_mesh)\n    else:\n      mesh = mesh_utils.create_hybrid_device_mesh(\n          ici_parallelism,\n          dcn_parallelism,\n          devices,\n          allow_split_physical_axes=allow_split_physical_axes,\n      )\n  else:\n    if allow_split_physical_axes:\n      if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n        mesh = mesh_utils.create_device_mesh(\n            [16, 16],\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=False,\n        )\n        mesh = max_utils.reshape_mesh_to_rings(mesh, config.custom_mesh)\n        mesh = np.reshape(mesh, ici_parallelism)\n      else:\n        mesh = mesh_utils.create_device_mesh(\n            ici_parallelism,\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=allow_split_physical_axes,\n        )\n    else:\n      mesh = mesh_utils.create_device_mesh(\n          ici_parallelism,\n          devices,\n      )\n      if config.optimize_mesh_for_tpu_v6e:\n        mesh = max_utils.optimize_mesh_for_tpu_v6e(mesh, devices)\n\n  max_logging.log(f\"Num_devices: {num_devices}, shape {mesh.shape}\")\n\n  return mesh",
        "analysis": {
            "module_type": "device_mesh_creation",
            "purpose": "Creates a JAX device mesh for distributed computation, handling multi-slice environments and optional sub-slicing.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine available devices.",
                "Optionally create a subslice of devices based on config.subslice_shape.",
                "Calculate the number of devices per slice.",
                "Determine inter-controller (ICI) and data-center (DCN) parallelism axes.",
                "Create the device mesh using either a hybrid approach (multi-slice) or a standard approach (single-slice).",
                "Optionally optimize the mesh for specific hardware (e.g., TPU v6e).",
                "Log the final mesh shape."
            ],
            "output": {
                "shape": "N/A (returns a JAX device mesh object)"
            },
            "dependencies": [
                "jax",
                "jax.numpy",
                "numpy",
                "max_utils",
                "max_logging",
                "mesh_utils"
            ],
            "parameters": {
                "config": "Configuration object containing parameters like num_slices, ici_parallelism, dcn_parallelism, subslice_shape, enable_single_controller, inference_benchmark_test, allow_split_physical_axes, custom_mesh, and optimize_mesh_for_tpu_v6e.",
                "devices": "Optional list of JAX devices to use. Defaults to all available JAX devices."
            },
            "notes": [
                "The function supports creating meshes for multi-slice environments (num_slices > 1) and single-slice environments.",
                "It can handle custom mesh configurations.",
                "The 'subslice_shape' parameter allows for creating a smaller mesh from a subset of devices when 'enable_single_controller' is true and 'num_slices' is 1.",
                "The 'allow_split_physical_axes' parameter influences how the mesh is created, especially in single-slice scenarios."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_learning_rate_schedule",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_learning_rate_schedule(config):\n  \"\"\"Creates a warmup and cosine decay learning rate schedule:\n  We take inspiration from Llama2's learning rate (LR) schedule, see https://arxiv.org/pdf/2307.09288.pdf section 2.2\n  Learning rate schedule has either two or three parts:\n  1) Linear warmup from 0 to [learning_rate] over steps 0 to [learning_rate_schedule_steps * warmup_steps_fraction]\n  2) Cosine from [learning_rate] to [learning_rate * cosine_learning_rate_final_fraction] until learning_rate_schedule_steps\n  3) Constant learning rate of 0 from learning_rate_schedule_steps to steps.\n  The zero learning rate section can be used to more accurately measure the fully trained model's performance.\n  \"\"\"\n\n  def make_cos_schedule(init_lr, final_lr, len_steps):\n    def schedule(step):\n      pct = (step) / len_steps\n      a = 0.5 * (jnp.cos(jnp.pi * pct) + 1)\n      lr = init_lr * a + final_lr * (1 - a)\n      return lr\n\n    return schedule\n\n  lr = config.learning_rate\n  cos_final_lr = lr * config.cosine_learning_rate_final_fraction\n\n  warmup_steps = int(config.learning_rate_schedule_steps * config.warmup_steps_fraction)\n  cos_steps = config.learning_rate_schedule_steps - warmup_steps\n  constant_zero_steps = config.steps - config.learning_rate_schedule_steps\n\n  warmup_schedule = optax.linear_schedule(init_value=0.0, end_value=lr, transition_steps=warmup_steps)\n  cos_schedule = make_cos_schedule(lr, cos_final_lr, cos_steps)\n  constant_schedule = optax.constant_schedule(0.0)\n\n  pieces = [warmup_schedule, cos_schedule]\n  boundaries = [\n      warmup_steps,\n      warmup_steps + cos_steps,\n  ]\n\n  if constant_zero_steps > 0:\n    pieces.append(constant_schedule)\n    boundaries.append(warmup_steps + cos_steps + constant_zero_steps)\n\n  return optax.join_schedules(pieces, boundaries)",
        "analysis": {
            "module_type": "learning_rate_schedule_creator",
            "purpose": "Creates a learning rate schedule with linear warmup, cosine decay, and an optional constant zero phase.",
            "input": {
                "shape": "N/A (expects a config object)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines an inner function `make_cos_schedule` for cosine decay.",
                "Calculates initial and final learning rates for the cosine decay phase.",
                "Determines the number of steps for warmup, cosine decay, and constant zero phases.",
                "Creates individual schedules for warmup (linear), cosine decay, and constant zero.",
                "Joins these schedules together with specified boundaries.",
                "Handles the case where there is no constant zero phase."
            ],
            "output": {
                "shape": "N/A (returns an Optax schedule object)"
            },
            "dependencies": [
                "optax.linear_schedule",
                "optax.constant_schedule",
                "optax.join_schedules",
                "jax.numpy as jnp"
            ],
            "parameters": {
                "config.learning_rate": "The peak learning rate.",
                "config.cosine_learning_rate_final_fraction": "The fraction of the peak learning rate to decay to in the cosine phase.",
                "config.learning_rate_schedule_steps": "The total number of steps for the warmup and cosine decay phases combined.",
                "config.warmup_steps_fraction": "The fraction of `learning_rate_schedule_steps` allocated to warmup.",
                "config.steps": "The total number of training steps."
            },
            "notes": [
                "The schedule is inspired by Llama2's learning rate schedule.",
                "The optional constant zero learning rate phase at the end can be used for performance measurement.",
                "The `make_cos_schedule` function implements a cosine decay from `init_lr` to `final_lr` over `len_steps`."
            ]
        }
    }
]