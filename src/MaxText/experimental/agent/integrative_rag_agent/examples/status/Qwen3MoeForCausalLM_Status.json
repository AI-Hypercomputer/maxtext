{
    "data": {
        "q": [],
        "processed_components": [
            "transformers/utils/quantization_config.py#TorchAoConfig",
            "transformers/integrations/tensor_parallel.py#_torch_distributed_available",
            "transformers/utils/import_utils.py#G2P_EN_IMPORT_ERROR",
            "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING",
            "transformers/masking_utils.py#_is_torch_xpu_available",
            "transformers/utils/import_utils.py#PHONEMIZER_IMPORT_ERROR",
            "transformers/loss/loss_utils.py#ForCausalLMLoss",
            "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters",
            "transformers/utils.py#is_torch_available",
            "transformers/modeling_flash_attention_utils.py#_get_unpad_data",
            "transformers/utils/import_utils.py#is_rjieba_available",
            "transformers/utils/quantization_config.py#AWQLinearVersion",
            "transformers/integrations/accelerate.py#expand_device_map",
            "transformers/modeling_utils.py#_is_quantized",
            "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor",
            "transformers/loss/loss_grounding_dino.py#GroundingDinoHungarianMatcher",
            "transformers/utils.py#is_flute_available",
            "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher",
            "transformers/utils/import_utils.py#is_cython_available",
            "transformers/utils.py#is_flash_attn_2_available",
            "transformers/modeling_flash_attention_utils.py#_flash_fn",
            "transformers/integrations/accelerate.py#accelerate_dispatch",
            "transformers/modeling_rope_utils.py#dynamic_rope_update",
            "transformers/modeling_rope_utils.py#_check_received_keys",
            "transformers/modeling_rope_utils.py#_validate_longrope_parameters",
            "transformers/utils/quantization_config.py#SpQRConfig",
            "transformers/utils/import_utils.py#is_auto_gptq_available",
            "transformers/modeling_utils.py#_find_identical",
            "transformers/cache_utils.py#DynamicLayer",
            "transformers/utils.py#is_aqlm_available",
            "transformers/quantizers/quantizer_mxfp4.py#Mxfp4HfQuantizer",
            "transformers/modeling_utils.py#XLA_DOWNCAST_BF16",
            "transformers/modeling_flash_attention_utils.py#_prepare_from_posids",
            "transformers/modeling_utils.py#no_init_weights",
            "transformers/integrations/tensor_parallel.py#repack_weights",
            "transformers/quantizers/base.py#_assign_original_dtype",
            "transformers/distributed.py#DistributedConfig",
            "transformers/utils.py#is_torch_xla_available",
            "transformers/configuration_utils.py#SpecificPreTrainedConfigType",
            "transformers/utils/quantization_config.py#QuantoConfig",
            "transformers/utils/quantization_config.py#BitsAndBytesConfig",
            "transformers/utils/import_utils.py#VersionComparison",
            "transformers/modeling_gguf_pytorch_utils.py#LlamaTensorProcessor",
            "transformers/quantizers/quantizer_quark.py#QuarkHfQuantizer",
            "transformers/utils/import_utils.py#is_pydantic_available",
            "transformers/modeling_rope_utils.py#RopeParameters",
            "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
            "transformers/utils/import_utils.py#is_levenshtein_available",
            "transformers/modeling_flash_attention_utils.py#_upad_input",
            "transformers/loss/loss_for_object_detection.py#ImageLoss",
            "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
            "transformers/utils/import_utils.py#PRETTY_MIDI_IMPORT_ERROR",
            "transformers/utils/import_utils.py#FTFY_IMPORT_ERROR",
            "transformers/utils.py#is_torch_mlu_available",
            "transformers/loss/loss_grounding_dino.py#GroundingDinoImageLoss",
            "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES",
            "transformers/utils/quantization_config.py#FineGrainedFP8Config",
            "transformers/utils.py#ADAPTER_SAFE_WEIGHTS_NAME",
            "transformers/utils/import_utils.py#is_g2p_en_available",
            "transformers/modeling_flash_attention_utils.py#fa_peft_integration_check",
            "transformers/modeling_utils.py#_infer_parameter_dtype",
            "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
            "transformers/utils/import_utils.py#split_package_version",
            "transformers/utils.py#is_hadamard_available",
            "transformers/utils/import_utils.py#PYTORCH_IMPORT_ERROR",
            "transformers/utils.py#is_quark_available",
            "transformers/modeling_gguf_pytorch_utils.py#MambaTensorProcessor",
            "transformers/utils.py#WEIGHTS_NAME",
            "transformers/utils.py#is_torch_greater_or_equal",
            "transformers/utils/import_utils.py#SACREMOSES_IMPORT_ERROR",
            "transformers/utils/quantization_config.py#EetqConfig",
            "transformers/utils/import_utils.py#is_torchaudio_available",
            "transformers/utils.py#is_fbgemm_gpu_available",
            "transformers/utils/import_utils.py#is_fastapi_available",
            "transformers/utils/import_utils.py#is_peft_available",
            "transformers/modeling_utils.py#VLMS",
            "transformers/loss/loss_d_fine.py#_set_aux_loss2",
            "transformers/modeling_gguf_pytorch_utils.py#GPT2TensorProcessor",
            "transformers/utils/import_utils.py#is_jinja_available",
            "transformers/quantizers/quantizer_bitnet.py#BitNetHfQuantizer",
            "transformers/utils/quantization_config.py#QuantizationMethod.QUARK",
            "transformers/modeling_gguf_pytorch_utils.py#NemotronTensorProcessor",
            "transformers/modeling_flash_attention_utils.py#_flash_attention_forward",
            "transformers/quantizers/quantizer_torchao.py#TorchAoHfQuantizer",
            "transformers/utils.py#is_triton_available",
            "transformers/utils/import_utils.py#is_faiss_available",
            "transformers/utils/import_utils.py#is_decord_available",
            "transformers/integrations/accelerate.py#compute_module_sizes",
            "transformers/quantizers/quantizers_utils.py#get_module_from_name",
            "transformers/utils/quantization_config.py#AutoRoundConfig.from_dict",
            "transformers/quantizers/base.py#SequentialLlama4TextExperts",
            "transformers/integrations/accelerate.py#init_on_device",
            "transformers/utils.py#is_torch_npu_available",
            "transformers/utils/import_utils.py#VISION_IMPORT_ERROR",
            "transformers/modeling_utils.py#ModuleUtilsMixin",
            "transformers/dynamic_module_utils.py#custom_object_save",
            "transformers/loss/loss_rt_detr.py#RTDetrForObjectDetectionLoss",
            "transformers/integrations/accelerate.py#find_tied_parameters",
            "transformers/modeling_utils.py#_add_variant",
            "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES",
            "transformers/integrations/fsdp.py#is_fsdp_enabled",
            "transformers/utils.py#download_url",
            "transformers/masking_utils.py#create_causal_mask",
            "transformers/loss/loss_rt_detr.py#_set_aux_loss",
            "transformers/integrations/hub_kernels.py#is_kernel",
            "transformers/utils.py#is_fp_quant_available",
            "transformers/modeling_gguf_pytorch_utils.py#read_field",
            "transformers/modeling_utils.py#SpecificPreTrainedModelType",
            "transformers/utils/generic.py#is_timm_config_dict",
            "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism",
            "transformers/utils/import_utils.py#CCL_IMPORT_ERROR",
            "transformers/modeling_utils.py#_get_tied_weight_keys",
            "transformers/utils/import_utils.py#CYTHON_IMPORT_ERROR",
            "transformers/modeling_utils.py#EmbeddingAccessMixin",
            "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
            "transformers/modeling_utils.py#is_local_dist_rank_0",
            "transformers/integrations/accelerate.py#init_empty_weights",
            "transformers/modeling_utils.py#TORCH_INIT_FUNCTIONS",
            "transformers/utils/import_utils.py#SPEECH_IMPORT_ERROR",
            "transformers/modeling_utils.py#unwrap_model",
            "transformers/utils/generic.py#OutputRecorder",
            "transformers/utils/quantization_config.py#GPTQConfig",
            "transformers/quantizers.py#HfQuantizer",
            "transformers/modeling_utils.py#ALL_ATTENTION_FUNCTIONS",
            "transformers/modeling_rope_utils.py#_compute_yarn_parameters",
            "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint",
            "transformers/modeling_gguf_pytorch_utils.py#T5TensorProcessor",
            "transformers/cache_utils.py#DynamicCache",
            "transformers/utils.py#is_torch_tensor",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRotaryEmbedding",
            "transformers/utils/import_utils.py#ESSENTIA_IMPORT_ERROR",
            "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal",
            "transformers/utils/import_utils.py#GGUF_MIN_VERSION",
            "transformers/utils/quantization_config.py#AWQLinearVersion.GEMM",
            "transformers/loss/loss_utils.py#ForQuestionAnsweringLoss",
            "transformers/loss/loss_for_object_detection.py#ForSegmentationLoss",
            "transformers/modeling_rope_utils.py#_validate_yarn_parameters",
            "transformers/utils/quantization_config.py#AwqBackendPackingMethod",
            "transformers/utils/quantization_config.py#AqlmConfig",
            "transformers/utils/import_utils.py#FAISS_IMPORT_ERROR",
            "transformers/quantizers/quantizer_quark.py#CHECKPOINT_KEYS",
            "transformers/utils/import_utils.py#is_ftfy_available",
            "transformers/utils.py#is_torch_xpu_available",
            "transformers/integrations.py#GGUF_CONFIG_MAPPING",
            "transformers/utils/import_utils.py#is_vision_available",
            "transformers/utils/import_utils.py#ACCELERATE_MIN_VERSION",
            "transformers/modeling_utils.py#load_shard_files_with_threadpool",
            "transformers/loss/loss_d_fine.py#DFineLoss",
            "transformers/integrations.py#is_deepspeed_zero3_enabled",
            "transformers/utils.py#SAFE_WEIGHTS_NAME",
            "transformers/utils.py#ADAPTER_WEIGHTS_NAME",
            "transformers/utils/import_utils.py#PYDANTIC_IMPORT_ERROR",
            "transformers/utils/hub.py#get_checkpoint_shard_files",
            "transformers/utils.py#is_kernels_available",
            "transformers/quantizers/auto.py#AutoHfQuantizer",
            "transformers/integrations.py#use_kernel_forward_from_hub",
            "transformers/modeling_utils.py#str_to_torch_dtype",
            "transformers/modeling_utils.py#load_state_dict",
            "transformers/__init__.py#__version__",
            "transformers/utils.py#requires_backends",
            "transformers/utils/import_utils.py#JINJA_IMPORT_ERROR",
            "transformers/utils/import_utils.py#is_natten_available",
            "transformers/utils.py#is_flash_attn_3_available",
            "transformers/utils.py#strtobool",
            "transformers/modeling_flash_attention_utils.py#_process_flash_attention_kwargs",
            "transformers/utils/import_utils.py#AV_IMPORT_ERROR",
            "transformers/utils.py#ContextManagers",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#eager_attention_forward",
            "transformers/utils/import_utils.py#SCIPY_IMPORT_ERROR",
            "transformers/utils/import_utils.py#is_accelerate_available",
            "transformers/utils/import_utils.py#RJIEBA_IMPORT_ERROR",
            "transformers/modeling_flash_attention_utils.py#_index_first_axis",
            "transformers/integrations.py#replace_with_spqr_linear",
            "transformers/quantizers/quantizer_vptq.py#VptqHfQuantizer",
            "transformers/distributed.py#DistributedConfig.from_dict",
            "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
            "transformers/loss/loss_for_object_detection.py#box_iou",
            "transformers/utils.py#is_auto_awq_available",
            "transformers/utils/quantization_config.py#QuarkConfig",
            "transformers/utils/quantization_config.py#QuantizationMethod.MXFP4",
            "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING",
            "transformers/utils/import_utils.py#is_torch_available",
            "transformers/quantizers/quantizer_awq.py#AwqQuantizer",
            "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
            "transformers/integrations/accelerate.py#get_balanced_memory",
            "transformers/utils/import_utils.py#LEVENSHTEIN_IMPORT_ERROR",
            "transformers/utils.py#is_auto_round_available",
            "transformers/utils/quantization_config.py#VptqConfig",
            "transformers/utils/quantization_config.py#CompressedTensorsConfig",
            "transformers/utils/import_utils.py#is_torchvision_available",
            "transformers/utils/import_utils.py#is_datasets_available",
            "transformers/masking_utils.py#find_packed_sequence_indices",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
            "transformers/utils/import_utils.py#FASTAPI_IMPORT_ERROR",
            "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
            "transformers/utils/import_utils.py#is_tokenizers_available",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#apply_rotary_pos_emb",
            "transformers/utils/import_utils.py#CV2_IMPORT_ERROR",
            "transformers/utils.py#is_torch_flex_attn_available",
            "transformers/modeling_rope_utils.py#standardize_rope_params",
            "transformers/utils/quantization_config.py#QuantizationMethod.FBGEMM_FP8",
            "transformers/utils/import_utils.py#is_torchcodec_available",
            "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
            "transformers/cache_utils.py#Cache",
            "transformers/modeling_flash_attention_utils.py#_unpad_input",
            "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
            "transformers/utils/import_utils.py#TORCHVISION_IMPORT_ERROR",
            "transformers/quantizers/quantizer_torchao.py#_linear_extra_repr",
            "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
            "transformers/utils/quantization_config.py#AutoRoundConfig",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func",
            "transformers/quantizers/base.py#HfQuantizer",
            "transformers/utils.py#is_hqq_available",
            "transformers/integrations.py#PeftAdapterMixin",
            "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
            "transformers/modeling_gguf_pytorch_utils.py#GGUF_SUPPORTED_ARCHITECTURES",
            "transformers/utils/import_utils.py#is_yt_dlp_available",
            "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
            "transformers/quantizers/quantizer_spqr.py#SpQRHfQuantizer",
            "transformers/utils/quantization_config.py#QuantizationMethod",
            "transformers/modeling_utils.py#load_shard_file",
            "transformers/utils/import_utils.py#is_sacremoses_available",
            "transformers/utils/import_utils.py#DECORD_IMPORT_ERROR",
            "transformers/masking_utils.py#create_sliding_window_causal_mask",
            "transformers/utils/import_utils.py#RICH_IMPORT_ERROR",
            "transformers/integrations/tensor_parallel.py#convert_local_tensor_to_dtensor",
            "transformers/utils.py#is_optimum_quanto_available",
            "transformers/utils.py#is_gptqmodel_available",
            "transformers/integrations/flash_attention.py#flash_attention_forward",
            "transformers/utils/import_utils.py#is_pyctcdecode_available",
            "transformers/utils/import_utils.py#DATASETS_IMPORT_ERROR",
            "transformers/integrations/flash_attention.py#_use_top_left_mask",
            "transformers/utils.py#is_vptq_available",
            "transformers/modeling_flash_attention_utils.py#_hf_api_to_flash_mapping",
            "transformers/integrations.py#replace_with_aqlm_linear",
            "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS",
            "transformers/utils/import_utils.py#PYTORCH_QUANTIZATION_IMPORT_ERROR",
            "transformers/modeling_utils.py#_find_disjoint",
            "transformers/utils/import_utils.py#is_nltk_available",
            "transformers/utils/import_utils.py#PYTESSERACT_IMPORT_ERROR",
            "transformers/utils.py#is_accelerate_available",
            "transformers/modeling_utils.py#_get_dtype",
            "transformers/utils/import_utils.py#is_uvicorn_available",
            "transformers/quantizers/quantizer_torchao.py#_quantization_type",
            "transformers/utils.py#auto_docstring",
            "transformers/utils/import_utils.py#is_uroman_available",
            "transformers/utils/import_utils.py#TORCHCODEC_IMPORT_ERROR",
            "transformers/quantizers/auto.py#AutoQuantizationConfig",
            "transformers/configuration_utils.py#get_configuration_file",
            "transformers/integrations/flash_attention.py#get_target_dtype",
            "transformers/generation.py#GenerationMixin",
            "transformers/utils.py#can_return_tuple",
            "transformers/utils/import_utils.py#is_detectron2_available",
            "transformers/utils/quantization_config.py#FPQuantConfig",
            "transformers/utils/import_utils.py#SKLEARN_IMPORT_ERROR",
            "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel",
            "transformers/modeling_utils.py#XLA_USE_BF16",
            "transformers/configuration_utils.py#PreTrainedConfig",
            "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
            "transformers/loss/loss_grounding_dino.py#GroundingDinoForObjectDetectionLoss",
            "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor",
            "transformers/modeling_utils.py#_find_missing_and_unexpected_keys",
            "transformers/masking_utils.py#sliding_window_overlay",
            "transformers/utils/import_utils.py#Backend",
            "transformers/utils/quantization_config.py#QuantizationMethod.GPTQ",
            "transformers/utils.py#is_torchao_available",
            "transformers/modeling_gguf_pytorch_utils.py#Qwen2MoeTensorProcessor",
            "transformers/modeling_utils.py#get_state_dict_dtype",
            "transformers/utils.py#TransformersKwargs",
            "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
            "transformers/utils/quantization_config.py#AWQLinearVersion.IPEX",
            "transformers/utils.py#is_auto_gptq_available",
            "transformers/quantizers/quantizer_auto_round.py#AutoRoundQuantizer",
            "transformers/processing_utils.py#Unpack",
            "transformers/utils.py#is_compressed_tensors_available",
            "transformers/utils/import_utils.py#TORCHAUDIO_IMPORT_ERROR",
            "transformers/utils/import_utils.py#is_pytesseract_available",
            "transformers/modeling_utils.py#_init_weights",
            "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
            "transformers/utils.py#PushToHubMixin",
            "transformers/utils/import_utils.py#is_sentencepiece_available",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#rotate_half",
            "transformers/modeling_rope_utils.py#_validate_default_rope_parameters",
            "transformers/utils.py#is_spqr_available",
            "transformers/integrations/accelerate.py#accelerate_disk_offload",
            "transformers/utils/quantization_config.py#GPTQConfig.from_dict",
            "transformers/quantizers/quantizer_bnb_8bit.py#Bnb8BitHfQuantizer",
            "transformers/utils/import_utils.py#SENTENCEPIECE_IMPORT_ERROR",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeAttention",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeForCausalLM",
            "transformers/integrations/accelerate.py#_get_device_map",
            "transformers/loss/loss_utils.py#fixed_cross_entropy",
            "transformers/image_transforms.py#_center_to_corners_format_numpy",
            "transformers/utils/import_utils.py#PEFT_IMPORT_ERROR",
            "transformers/utils/import_utils.py#is_pretty_midi_available",
            "transformers/utils/import_utils.py#LIBROSA_IMPORT_ERROR",
            "transformers/utils/import_utils.py#is_librosa_available",
            "transformers/loss/loss_for_object_detection.py#_set_aux_loss",
            "transformers/loss/loss_for_object_detection.py#box_area",
            "transformers/loss/loss_deformable_detr.py#DeformableDetrForSegmentationLoss",
            "transformers/models/auto/configuration_auto.py#AutoConfig.from_pretrained",
            "transformers/loss/loss_grounding_dino.py#sigmoid_focal_loss",
            "transformers/generation.py#CompileConfig",
            "transformers/modeling_flash_attention_utils.py#_flash_varlen_fn",
            "transformers/modeling_flash_attention_utils.py#flash_attn_supports_top_left_mask",
            "transformers/utils/import_utils.py#BS4_IMPORT_ERROR",
            "transformers/integrations/tensor_parallel.py#add_tensor_parallel_hooks_to_module",
            "transformers/utils/import_utils.py#PANDAS_IMPORT_ERROR",
            "transformers/utils/import_utils.py#DETECTRON2_IMPORT_ERROR",
            "transformers/utils/import_utils.py#BACKENDS_MAPPING",
            "transformers/pytorch_utils.py#id_tensor_storage",
            "transformers/utils/import_utils.py#is_cv2_available",
            "transformers/utils/import_utils.py#is_phonemizer_available",
            "transformers/modeling_flash_attention_utils.py#_is_packed_sequence",
            "transformers/modeling_utils.py#is_accelerator_device",
            "transformers/utils.py#WEIGHTS_INDEX_NAME",
            "transformers/utils.py#has_file",
            "transformers/loss/loss_utils.py#ForSequenceClassificationLoss",
            "transformers/modeling_gguf_pytorch_utils.py#TENSOR_PROCESSORS",
            "transformers/quantizers/quantizer_fbgemm_fp8.py#FbgemmFp8HfQuantizer",
            "transformers/utils/import_utils.py#PROTOBUF_IMPORT_ERROR",
            "transformers/utils/import_utils.py#TOKENIZERS_IMPORT_ERROR",
            "transformers/modeling_utils.py#set_quantized_state",
            "transformers/image_transforms.py#_center_to_corners_format_torch",
            "transformers/loss/loss_for_object_detection.py#_max_by_axis",
            "transformers/quantizers/quantizer_torchao.py#fuzzy_match_size",
            "transformers/utils/import_utils.py#is_essentia_available",
            "transformers/modeling_flash_attention_utils.py#_lazy_imports",
            "transformers/masking_utils.py#causal_mask_function",
            "transformers/utils/quantization_config.py#HiggsConfig",
            "transformers/quantizers/quantizer_quanto.py#QuantoHfQuantizer",
            "transformers/integrations/tensor_parallel.py#distribute_model",
            "transformers/modeling_utils.py#_get_resolved_checkpoint_files",
            "transformers/quantizers/auto.py#get_hf_quantizer",
            "transformers/utils.py#is_optimum_available",
            "transformers/quantizers/base.py#MODULES_TO_PATCH_FOR_QUANTIZATION",
            "transformers/utils/quantization_config.py#Mxfp4Config",
            "transformers/utils/import_utils.py#is_torch_greater_or_equal",
            "transformers/modeling_utils.py#_load_parameter_into_model",
            "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters",
            "transformers/utils/import_utils.py#UROMAN_IMPORT_ERROR",
            "transformers/masking_utils.py#_preprocess_mask_arguments",
            "transformers/utils/import_utils.py#is_pytorch_quantization_available",
            "transformers/quantizers/quantizer_aqlm.py#AqlmHfQuantizer",
            "transformers/quantizers/quantizer_eetq.py#EetqHfQuantizer",
            "transformers/modeling_utils.py#_find_mismatched_keys",
            "transformers/pytorch_utils.py#_torch_distributed_available",
            "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
            "transformers/utils/import_utils.py#is_timm_available",
            "transformers/modeling_flash_attention_utils.py#_unpad_fn",
            "transformers/integrations/peft.py#maybe_load_adapters",
            "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES.keys",
            "transformers/integrations/tensor_parallel.py#verify_tp_plan",
            "transformers/utils/import_utils.py#is_mistral_common_available",
            "transformers/utils.py#is_flash_attn_greater_or_equal_2_10",
            "transformers/utils.py#KernelConfig",
            "transformers/cache_utils.py#DynamicSlidingWindowLayer",
            "transformers/masking_utils.py#and_masks",
            "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
            "transformers/modeling_gguf_pytorch_utils.py#get_gguf_hf_weights_map",
            "transformers/quantizers/quantizer_compressed_tensors.py#CompressedTensorsHfQuantizer",
            "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss",
            "transformers/utils/quantization_config.py#BitNetQuantConfig",
            "transformers/utils.py#CONFIG_NAME",
            "transformers/quantizers/quantizer_gptq.py#GptqHfQuantizer",
            "transformers/utils/import_utils.py#TIMM_IMPORT_ERROR",
            "transformers/modeling_flash_attention_utils.py#_pad_fn",
            "transformers/utils/import_utils.py#PYCTCDECODE_IMPORT_ERROR",
            "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7",
            "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention",
            "transformers/utils/quantization_config.py#AwqConfig",
            "transformers/quantizers/quantizer_bnb_4bit.py#Bnb4BitHfQuantizer",
            "transformers/utils/import_utils.py#_is_package_available",
            "transformers/utils/import_utils.py#is_ccl_available",
            "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
            "transformers/masking_utils.py#sliding_window_causal_mask_function",
            "transformers/utils/quantization_config.py#VptqLayerConfig",
            "transformers/masking_utils.py#or_masks",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel",
            "transformers/modeling_outputs.py#MoeModelOutputWithPast",
            "transformers/modeling_flash_attention_utils.py#prepare_fa_kwargs_from_position_ids",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#repeat_kv",
            "transformers/quantizers/quantizer_higgs.py#HiggsHfQuantizer",
            "transformers/utils.py#PushToHubMixin.push_to_hub",
            "transformers/utils/import_utils.py#is_pandas_available",
            "transformers/utils.py#DUMMY_INPUTS",
            "transformers/utils.py#TensorType",
            "transformers/modeling_flash_attention_utils.py#_process_flash_kwargs_fn",
            "transformers/utils/import_utils.py#YT_DLP_IMPORT_ERROR",
            "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
            "transformers/utils/quantization_config.py#QuantizationConfigMixin",
            "transformers/modeling_flash_attention_utils.py#_lazy_define_process_function",
            "transformers/integrations.py#deepspeed_config",
            "transformers/modeling_gguf_pytorch_utils.py#BloomTensorProcessor",
            "transformers/modeling_utils.py#PreTrainedModel",
            "transformers/masking_utils.py#packed_sequence_mask_function",
            "transformers/utils/import_utils.py#NATTEN_IMPORT_ERROR",
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
            "transformers/utils.py#is_qutlass_available",
            "transformers/utils/quantization_config.py#AWQLinearVersion.EXLLAMA",
            "transformers/loss/loss_utils.py#ForMaskedLMLoss",
            "transformers/utils/quantization_config.py#HqqConfig",
            "transformers/utils/import_utils.py#is_protobuf_available",
            "transformers/utils/import_utils.py#is_scipy_available",
            "transformers/loss/loss_for_object_detection.py#dice_loss",
            "transformers/loss/loss_d_fine.py#translate_gt",
            "transformers/utils/import_utils.py#is_speech_available",
            "transformers/safetensors_conversion.py#auto_conversion",
            "transformers/integrations/accelerate.py#check_and_set_device_map",
            "transformers/modeling_utils.py#_is_ds_init_called",
            "transformers/utils/import_utils.py#is_sklearn_available",
            "transformers/loss/loss_d_fine.py#_set_aux_loss",
            "transformers/utils.py#is_torch_hpu_available",
            "transformers/loss/loss_utils.py#ForTokenClassification",
            "transformers/integrations.py#GGUF_TOKENIZER_MAPPING",
            "transformers/loss/loss_for_object_detection.py#NestedTensor",
            "transformers/modeling_utils.py#_torch_distributed_available",
            "transformers/utils.py#is_eetq_available",
            "transformers/loss/loss_utils.py#LOSS_MAPPING",
            "transformers/modeling_rope_utils.py#_validate_llama3_parameters",
            "transformers/loss/loss_for_object_detection.py#ForObjectDetectionLoss",
            "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
            "transformers/utils/import_utils.py#requires",
            "transformers/modeling_gguf_pytorch_utils.py#Lfm2TensorProcessor",
            "transformers/integrations.py#_gguf_parse_value",
            "transformers/modeling_utils.py#_end_ptr",
            "transformers/utils/import_utils.py#is_av_available",
            "transformers/utils/quantization_config.py#FbgemmFp8Config",
            "transformers/utils/import_utils.py#is_rich_available",
            "transformers/utils.py#is_remote_url",
            "transformers/utils/hub.py#DownloadKwargs",
            "transformers/quantizers/quantizer_fp_quant.py#FPQuantHfQuantizer",
            "transformers/modeling_utils.py#restore_default_dtype",
            "transformers/utils/import_utils.py#OPENAI_IMPORT_ERROR",
            "transformers/modeling_utils.py#set_zero3_state",
            "transformers/image_transforms.py#center_to_corners_format",
            "transformers/utils.py#cached_file",
            "transformers/loss/loss_deformable_detr.py#DeformableDetrHungarianMatcher",
            "transformers/modeling_utils.py#get_parameter_device",
            "transformers/loss/loss_d_fine.py#weighting_function",
            "transformers/utils/import_utils.py#is_bs4_available",
            "transformers/cache_utils.py#CacheLayerMixin",
            "transformers/utils.py#extract_commit_hash",
            "transformers/modeling_gguf_pytorch_utils.py#Gemma2TensorProcessor",
            "transformers/loss/loss_d_fine.py#DFineForObjectDetectionLoss",
            "transformers/utils/generic.py#check_model_inputs",
            "transformers/modeling_utils.py#_is_dtensor_available",
            "transformers/integrations.py#is_fsdp_enabled",
            "transformers/utils/quantization_config.py#QuantizationMethod.COMPRESSED_TENSORS",
            "transformers/integrations.py#prepare_for_hqq_linear",
            "transformers/utils/import_utils.py#ACCELERATE_IMPORT_ERROR",
            "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs",
            "transformers/loss/loss_for_object_detection.py#_upcast",
            "transformers/loss/loss_d_fine.py#bbox2distance",
            "transformers/utils/import_utils.py#MISTRAL_COMMON_IMPORT_ERROR",
            "transformers/modeling_utils.py#get_parameter_dtype",
            "transformers/modeling_rope_utils.py#rope_config_validation",
            "transformers/loss/loss_deformable_detr.py#DeformableDetrForObjectDetectionLoss",
            "transformers/modeling_utils.py#_load_state_dict_into_meta_model",
            "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
            "transformers/quantizers/quantizer_finegrained_fp8.py#FineGrainedFP8HfQuantizer",
            "transformers/quantizers/quantizer_hqq.py#HqqHfQuantizer",
            "transformers/utils.py#is_offline_mode",
            "transformers/modeling_flash_attention_utils.py#_pad_input",
            "transformers/utils/import_utils.py#NLTK_IMPORT_ERROR",
            "transformers/loss/loss_rt_detr.py#RTDetrLoss",
            "transformers/utils/import_utils.py#is_openai_available",
            "transformers/utils.py#ModelOutput",
            "transformers/generation.py#GenerationConfig.from_model_config",
            "transformers/utils/import_utils.py#UVICORN_IMPORT_ERROR",
            "transformers/utils/quantization_config.py#ExllamaVersion",
            "transformers/utils/import_utils.py#PACKAGE_DISTRIBUTION_MAPPING",
            "transformers/utils/import_utils.py#is_gguf_available",
            "transformers/utils.py#is_bitsandbytes_available"
        ],
        "processed_count": 930,
        "file_analysis_cache": {
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeForCausalLM": {
                "sorted_modules": {
                    "Qwen3MoeForCausalLM": "\n\n@auto_docstring\nclass Qwen3MoeForCausalLM(Qwen3MoePreTrainedModel, GenerationMixin):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = Qwen3MoeModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.router_aux_loss_coef = config.router_aux_loss_coef\n        self.num_experts = config.num_experts\n        self.num_experts_per_tok = config.num_experts_per_tok\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @can_return_tuple\n    @auto_docstring\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_router_logits: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        logits_to_keep: Union[int, torch.Tensor] = 0,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> MoeCausalLMOutputWithPast:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, Qwen3MoeForCausalLM\n\n        >>> model = Qwen3MoeForCausalLM.from_pretrained(\"Qwen/Qwen3-MoE-15B-A2B\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-MoE-15B-A2B\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n\n        output_router_logits = (\n            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n        )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs: MoeModelOutputWithPast = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_router_logits=output_router_logits,\n            cache_position=cache_position,\n            **kwargs,\n        )\n\n        hidden_states = outputs.last_hidden_state\n        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n        logits = self.lm_head(hidden_states[:, slice_indices, :])\n\n        loss = None\n        if labels is not None:\n            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n\n        aux_loss = None\n        if output_router_logits:\n            aux_loss = load_balancing_loss_func(\n                outputs.router_logits,\n                self.num_experts,\n                self.num_experts_per_tok,\n                attention_mask,\n            )\n            if labels is not None:\n                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n\n        return MoeCausalLMOutputWithPast(\n            loss=loss,\n            aux_loss=aux_loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            router_logits=outputs.router_logits,\n        )"
                },
                "component_dependencies": {
                    "Qwen3MoeForCausalLM": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/generation.py#GenerationMixin",
                        "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                        "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs",
                        "transformers/utils.py#auto_docstring",
                        "transformers/utils.py#can_return_tuple"
                    ]
                },
                "warning": null
            },
            "transformers/cache_utils.py#Cache": {
                "sorted_modules": {
                    "Cache": "\n\nclass Cache:\n    \"\"\"\n    A `Cache` is mostly a list of `CacheLayerMixin` objects, one per model layer. It serves as a container for\n    the Cache of each layer.\n\n    Args:\n        layers (`Optional`, *optional*):\n            A list of pre-created `CacheLayerMixin`. If omitted (`None`), then `layer_class_to_replicate` will\n            be used.\n        layer_class_to_replicate (`type[CacheLayerMixin]`, *optional*):\n            Only used if `layers` is omitted (`None`), in which case it will be used as the base class for each layer,\n            and the layers will be added lazily as soon as `update` is called with a `layer_idx` greater than the current\n            list of layers.\n        offloading (`bool`, *optional*, defaults to `False`):\n            Whether to perform offloading of the layers to `cpu`, to save GPU memory.\n        offload_only_non_sliding (`bool`, *optional*, defaults to `True`):\n            If `offloading` is `True`, this further decides if only the non-sliding layers will be offloaded (because\n            usually the sliding layers are small in size, so there is no need to offload them, and skipping it is faster).\n    \"\"\"\n\n    def __init__(\n        self,\n        layers: Optional[list[CacheLayerMixin]] = None,\n        layer_class_to_replicate: Optional[type[CacheLayerMixin]] = None,\n        offloading: bool = False,\n        offload_only_non_sliding: bool = True,\n    ):\n        if layers is not None and layer_class_to_replicate is not None:\n            raise ValueError(\n                \"You can construct a Cache either from a list `layers` of all the predefined `CacheLayer`, or from a \"\n                \"`layer_class_to_replicate`, in which case the Cache will append a new layer corresponding to \"\n                \"`layer_class_to_replicate` for each new call to `update` with an idx not already in the Cache.\"\n            )\n        if layers is None and layer_class_to_replicate is None:\n            raise ValueError(\n                \"You should provide exactly one of `layers` or `layer_class_to_replicate` to initialize a Cache.\"\n            )\n        self.layers = layers if layers is not None else []\n        self.layer_class_to_replicate = layer_class_to_replicate\n        self.offloading = offloading\n        if self.offloading:\n            self.only_non_sliding = offload_only_non_sliding\n            self.prefetch_stream = torch.Stream() if _is_torch_greater_or_equal_than_2_7 else torch.cuda.Stream()\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}(layers={self.layers})\"\n\n    def prefetch(self, layer_idx: int, only_non_sliding: bool = True):\n        \"\"\"\n        Prefetch a given layer on its device. If `only_non_sliding` is True, it will try to prefetch only the layers\n        which are non-sliding. If the `layer_idx` is outside the range, this will circle back to the first layers.\n        Note that we use a non-default stream for this, to avoid blocking.\n        \"\"\"\n        if only_non_sliding:\n            # Try to find next non-sliding, starting at `layer_idx`\n            try:\n                layer_idx = layer_idx + self.is_sliding[layer_idx:].index(False)\n            # In this case, we need to circle back to the beginning\n            except ValueError:\n                layer_idx = self.is_sliding.index(False)\n        else:\n            layer_idx = layer_idx if layer_idx < len(self.layers) else 0\n\n        # Prefetch\n        with self.prefetch_stream if _is_torch_greater_or_equal_than_2_7 else torch.cuda.stream(self.prefetch_stream):\n            self.layers[layer_idx].prefetch()\n\n    def offload(self, layer_idx: int, only_non_sliding: bool = True):\n        \"\"\"\n        Offload a given `layer_idx`. If `only_non_sliding` is True, it will offload `layer_idx` only if it is a\n        non-sliding layer. Note that we do it on the default stream, so that we ensure all earlier\n        computation in the layer's `update` methods are finished.\n        \"\"\"\n        if not (only_non_sliding and self.is_sliding[layer_idx]):\n            self.layers[layer_idx].offload()\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[dict[str, Any]] = None,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`dict[str, Any]`, *optional*):\n                Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n                cache to be created.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # In this case, the `layers` were not provided, and we must append as much as `layer_idx`\n        if self.layer_class_to_replicate is not None:\n            while len(self.layers) <= layer_idx:\n                self.layers.append(self.layer_class_to_replicate())\n\n        if self.offloading:\n            # Wait for the stream to finish if needed, and start prefetching the next layer\n            torch.cuda.default_stream(key_states.device).wait_stream(self.prefetch_stream)\n            self.prefetch(layer_idx + 1, self.only_non_sliding)\n\n        keys, values = self.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n\n        if self.offloading:\n            self.offload(layer_idx, self.only_non_sliding)\n\n        return keys, values\n\n    def early_initialization(\n        self, batch_size: int, num_heads: int, head_dim: int, dtype: torch.dtype, device: torch.device\n    ):\n        \"\"\"\n        Initialize all the layers in advance (it's otherwise lazily initialized on the first `update` call).\n        This is useful for our `export` recipes, as `export` needs everything in advance.\n        \"\"\"\n        # Note that the initialization needs all dimensions (except -2), as well as device and dtype, so we use\n        # this fake tensor approach. It has size 0 on the -2 dimension, so it does not allocate any data (it only\n        # creates an empty tensor with correct shape, dtype and device), which is very efficient and practical\n        fake_keys_tensor = torch.zeros((batch_size, num_heads, 0, head_dim), dtype=dtype, device=device)\n        # Init all layers\n        for layer in self.layers:\n            layer.lazy_initialization(fake_keys_tensor)\n\n    def get_seq_length(self, layer_idx: int = 0) -> int:\n        \"\"\"Returns the sequence length of the cache for the given layer.\"\"\"\n        if layer_idx >= len(self.layers):\n            return 0\n        return self.layers[layer_idx].get_seq_length()\n\n    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n        \"\"\"\n        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n        the given layer at `layer_idx`.\n        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns for each layer.\n        \"\"\"\n        # For DynamicCache, where the layers are created at runtime -> if it was not yet created, the size is\n        # simply the shape of `cache_position`\n        if layer_idx >= len(self.layers):\n            return cache_position.shape[0], 0\n        return self.layers[layer_idx].get_mask_sizes(cache_position)\n\n    def get_max_cache_shape(self, layer_idx: int = 0) -> int:\n        \"\"\"Returns maximum sequence length of the cache object. Dynamic caches do not have a maximum length.\"\"\"\n        # For DynamicCache, where the layers are created at runtime -> if it was not yet created, return -1\n        # as DynamicLayer does\n        if layer_idx >= len(self.layers):\n            return -1\n        return self.layers[layer_idx].get_max_cache_shape()\n\n    def reset(self):\n        \"\"\"Recursively reset all layers tensors\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].reset()\n\n    def reorder_cache(self, beam_idx: torch.LongTensor):\n        \"\"\"Reorder the cache for beam search\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].reorder_cache(beam_idx)\n\n    def crop(self, max_length: int):\n        \"\"\"Crop the cache to the given length\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].crop(max_length)\n\n    def batch_repeat_interleave(self, repeats: int):\n        \"\"\"Repeat and interleave the cache\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].batch_repeat_interleave(repeats)\n\n    def batch_select_indices(self, indices: torch.Tensor):\n        \"\"\"Select indices from the cache\"\"\"\n        for layer_idx in range(len(self.layers)):\n            self.layers[layer_idx].batch_select_indices(indices)\n\n    @property\n    def max_batch_size(self) -> int:\n        \"\"\"Return the maximum batch size of the cache\"\"\"\n        values = [layer.max_batch_size for layer in self.layers]\n        if len(set(values)) > 1:\n            raise ValueError(f\"Max batch size is not consistent across layers: {values}\")\n        return values[0]\n\n    @property\n    def max_cache_len(self) -> int:\n        \"\"\"Return the maximum cache length of the cache\"\"\"\n        values = [layer.max_cache_len for layer in self.layers]\n        return max(values)\n\n    @property\n    def is_compileable(self) -> bool:\n        \"\"\"Return whether the cache is compileable\"\"\"\n        # For DynamicCache dispatching the layers lazily (otherwise, all([]) is True)\n        if len(self.layers) == 0:\n            return False\n        return all(layer.is_compileable for layer in self.layers)\n\n    @property\n    def is_initialized(self) -> bool:\n        \"\"\"Return whether the cache data is initialized\"\"\"\n        return len(self.layers) > 0 and all(layer.is_initialized for layer in self.layers)\n\n    @property\n    def is_sliding(self) -> list[bool]:\n        \"\"\"Return whether the layers of the cache are sliding window\"\"\"\n        return [getattr(layer, \"is_sliding\", False) for layer in self.layers]\n\n    def __len__(self):\n        \"\"\"\n        This value corresponds to the number of layers in the model.\n        \"\"\"\n        # Note: for DynamicCache, layers are initialized lazily, so this will not be accurate before the first\n        # forward through all the layers\n        return len(self.layers)"
                },
                "component_dependencies": {
                    "Cache": [
                        "transformers/cache_utils.py#CacheLayerMixin",
                        "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast": {
                "sorted_modules": {
                    "MoeCausalLMOutputWithPast": "\n\n@dataclass\nclass MoeCausalLMOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for causal language model (or autoregressive) with mixture of experts outputs.\n\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss (for next-token prediction).\n\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n\n        aux_loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided):\n            aux_loss for the sparse modules.\n\n        router_logits (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_probs=True` and `config.add_router_probs=True` is passed or when `config.output_router_probs=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.\n\n            Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliary\n            loss for Mixture of Experts models.\n\n        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    aux_loss: Optional[torch.FloatTensor] = None\n    logits: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[Cache] = None\n    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n    router_logits: Optional[tuple[torch.FloatTensor]] = None"
                },
                "component_dependencies": {
                    "MoeCausalLMOutputWithPast": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/utils.py#ModelOutput"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_outputs.py#MoeModelOutputWithPast": {
                "sorted_modules": {
                    "MoeModelOutputWithPast": "\n\n@dataclass\nclass MoeModelOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for model's outputs, with potential hidden states and attentions.\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n            input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n        router_logits (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_probs=True` and `config.add_router_probs=True` is passed or when `config.output_router_probs=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.\n\n            Raw router logtis (post-softmax) that are computed by MoE routers, these terms are used to compute the auxiliary\n            loss for Mixture of Experts models.\n    \"\"\"\n\n    last_hidden_state: Optional[torch.FloatTensor] = None\n    past_key_values: Optional[Cache] = None\n    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n    router_logits: Optional[tuple[torch.FloatTensor]] = None"
                },
                "component_dependencies": {
                    "MoeModelOutputWithPast": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/utils.py#ModelOutput"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel": {
                "sorted_modules": {
                    "Qwen3MoeModel": "\n\n@auto_docstring\nclass Qwen3MoeModel(Qwen3MoePreTrainedModel):\n    def __init__(self, config: Qwen3MoeConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [Qwen3MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = Qwen3MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = Qwen3MoeRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @check_model_inputs()\n    @auto_docstring\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[TransformersKwargs],\n    ) -> MoeModelOutputWithPast:\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n\n        if use_cache and past_key_values is None:\n            past_key_values = DynamicCache(config=self.config)\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n        causal_mask = mask_function(\n            config=self.config,\n            input_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            cache_position=cache_position,\n            past_key_values=past_key_values,\n            position_ids=position_ids,\n        )\n\n        hidden_states = inputs_embeds\n        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n\n        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n            hidden_states = decoder_layer(\n                hidden_states,\n                attention_mask=causal_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                use_cache=use_cache,\n                cache_position=cache_position,\n                position_embeddings=position_embeddings,\n                **kwargs,\n            )\n\n        hidden_states = self.norm(hidden_states)\n\n        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n            last_hidden_state=hidden_states,\n            past_key_values=past_key_values,\n        )"
                },
                "component_dependencies": {
                    "Qwen3MoeModel": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/cache_utils.py#DynamicCache",
                        "transformers/masking_utils.py#create_causal_mask",
                        "transformers/masking_utils.py#create_sliding_window_causal_mask",
                        "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                        "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRotaryEmbedding",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs",
                        "transformers/utils.py#auto_docstring",
                        "transformers/utils/generic.py#check_model_inputs"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel": {
                "sorted_modules": {
                    "Qwen3MoePreTrainedModel": "\n\n@auto_docstring\nclass Qwen3MoePreTrainedModel(PreTrainedModel):\n    config: Qwen3MoeConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"Qwen3MoeDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn = True\n    _supports_sdpa = True\n    _supports_flex_attn = True\n    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n    _supports_attention_backend = True\n    _can_record_outputs = {\n        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n        \"hidden_states\": Qwen3MoeDecoderLayer,\n        \"attentions\": Qwen3MoeAttention,\n    }"
                },
                "component_dependencies": {
                    "Qwen3MoePreTrainedModel": [
                        "transformers/modeling_utils.py#PreTrainedModel",
                        "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeAttention",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer",
                        "transformers/utils.py#auto_docstring",
                        "transformers/utils/generic.py#OutputRecorder"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func": {
                "sorted_modules": {
                    "load_balancing_loss_func": "\n\ndef load_balancing_loss_func(\n    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n    num_experts: Optional[int] = None,\n    top_k=2,\n    attention_mask: Optional[torch.Tensor] = None,\n) -> Union[torch.Tensor, int]:\n    r\"\"\"\n    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n\n    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n    experts is too unbalanced.\n\n    Args:\n        gate_logits:\n            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n            shape [batch_size X sequence_length, num_experts].\n        num_experts:\n            Number of experts\n        top_k:\n            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n            parameter.\n        attention_mask (`torch.Tensor`, *optional*):\n            The attention_mask used in forward function\n            shape [batch_size X sequence_length] if not None.\n\n    Returns:\n        The auxiliary loss.\n    \"\"\"\n    if gate_logits is None or not isinstance(gate_logits, tuple):\n        return 0\n\n    if isinstance(gate_logits, tuple):\n        compute_device = gate_logits[0].device\n        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n\n    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n\n    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n\n    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n\n    if attention_mask is None:\n        # Compute the percentage of tokens routed to each experts\n        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n\n        # Compute the average probability of routing to these experts\n        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n    else:\n        batch_size, sequence_length = attention_mask.shape\n        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n\n        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n        expert_attention_mask = (\n            attention_mask[None, :, :, None, None]\n            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n            .reshape(-1, top_k, num_experts)\n            .to(compute_device)\n        )\n\n        # Compute the percentage of tokens routed to each experts\n        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n            expert_attention_mask, dim=0\n        )\n\n        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n        router_per_expert_attention_mask = (\n            attention_mask[None, :, :, None]\n            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n            .reshape(-1, num_experts)\n            .to(compute_device)\n        )\n\n        # Compute the average probability of routing to these experts\n        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n            router_per_expert_attention_mask, dim=0\n        )\n\n    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n    return overall_loss * num_experts"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/cache_utils.py#CacheLayerMixin": {
                "sorted_modules": {
                    "CacheLayerMixin": "\n\nclass CacheLayerMixin(ABC):\n    \"\"\"Base, abstract class for a single layer's cache.\"\"\"\n\n    is_compileable = False\n\n    def __init__(self):\n        self.keys: Optional[torch.Tensor] = None\n        self.values: Optional[torch.Tensor] = None\n        self.is_initialized = False\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}\"\n\n    @abstractmethod\n    def lazy_initialization(self, key_states: torch.Tensor): ...\n\n    @abstractmethod\n    def update(\n        self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]] = None\n    ) -> tuple[torch.Tensor, torch.Tensor]: ...\n\n    @abstractmethod\n    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]: ...\n\n    @abstractmethod\n    def get_seq_length(self) -> int: ...\n\n    @abstractmethod\n    def get_max_cache_shape(self) -> int: ...\n\n    def offload(self):\n        \"\"\"Offload this layer's data to CPU device.\"\"\"\n        if self.is_initialized:\n            self.keys = self.keys.to(\"cpu\", non_blocking=True)\n            self.values = self.values.to(\"cpu\", non_blocking=True)\n\n    def prefetch(self):\n        \"\"\"In case of layer offloading, this allows to move the data back to the layer's device ahead of time.\"\"\"\n        if self.is_initialized and self.keys.device != self.device:\n            self.keys = self.keys.to(self.device, non_blocking=True)\n            self.values = self.values.to(self.device, non_blocking=True)\n\n    def reset(self) -> None:\n        \"\"\"Resets the cache values while preserving the objects\"\"\"\n        if self.is_initialized:\n            self.keys.zero_()\n            self.values.zero_()\n        # This attribute is set on several Layers\n        if hasattr(self, \"cumulative_length\"):\n            self.cumulative_length = 0\n\n    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:\n        \"\"\"Reorders this layer's cache for beam search.\"\"\"\n        if self.get_seq_length() > 0:\n            self.keys = self.keys.index_select(0, beam_idx.to(self.keys.device))\n            self.values = self.values.index_select(0, beam_idx.to(self.values.device))"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7": {
                "sorted_modules": {
                    "_is_torch_greater_or_equal_than_2_7": "\n_is_torch_greater_or_equal_than_2_7 = is_torch_greater_or_equal(\"2.7\", accept_dev=True)"
                },
                "component_dependencies": {
                    "_is_torch_greater_or_equal_than_2_7": [
                        "transformers/utils.py#is_torch_greater_or_equal"
                    ]
                },
                "warning": null
            },
            "transformers/cache_utils.py#DynamicCache": {
                "sorted_modules": {
                    "DynamicCache": "\n\nclass DynamicCache(Cache):\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n    It stores the key and value states as a list of `CacheLayer`, one for each layer. The expected shape for each tensor\n    in the `CacheLayer`s is `[batch_size, num_heads, seq_len, head_dim]`.\n    If a config is passed, it will additionally check for sliding or hybrid cache structure, greatly reducing the\n    memory requirement of the cached tensors to `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n\n    See `Cache` for details on common methods that are implemented by all cache classes.\n\n    Args:\n        ddp_cache_data (`Iterable[tuple[torch.Tensor, torch.Tensor]]`, *optional*):\n            It was originally added for compatibility with `torch.distributed` (DDP). In a nutshell, it is\n            `map(gather_map, zip(*caches))`, i.e. each item in the iterable contains the key and value states\n            for a layer gathered across replicas by torch.distributed (shape=[global batch size, num_heads, seq_len, head_dim]).\n            Note: it needs to be the 1st arg as well to work correctly\n        config (`PreTrainedConfig`, *optional*):\n            The config of the model for which this Cache will be used. If passed, it will be used to check for sliding\n            or hybrid layer structure, greatly reducing the memory requirement of the cached tensors to\n            `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n        offloading (`bool`, *optional*, defaults to `False`):\n            Whether to perform offloading of the layers to `cpu`, to save GPU memory.\n        offload_only_non_sliding (`bool`, *optional*, defaults to `False`):\n            If `offloading` is `True`, this further decides if only the non-sliding layers will be offloaded (because\n            usually the sliding layers are small in size, so there is no need to offload them, and skipping it is faster).\n\n    Example:\n\n    ```python\n    >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n\n    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n\n    >>> inputs = tokenizer(text=\"My name is Qwen2\", return_tensors=\"pt\")\n\n    >>> # Prepare a cache class and pass it to model's forward\n    >>> past_key_values = DynamicCache(config=model.config)\n    >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n    >>> outputs.past_key_values # access cache filled with key/values from generation\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], ...]]] = None,\n        config: Optional[PreTrainedConfig] = None,\n        offloading: bool = False,\n        offload_only_non_sliding: bool = False,\n    ):\n        layers = []\n        # If a config is passed, use it to infer the layer types and initialize accordingly\n        if config is not None:\n            decoder_config = config.get_text_config(decoder=True)\n            sliding_window = getattr(decoder_config, \"sliding_window\", None) or getattr(\n                decoder_config, \"attention_chunk_size\", None\n            )\n            layer_types = getattr(decoder_config, \"layer_types\", None)\n            if layer_types is None:\n                layer_types = [\n                    \"sliding_attention\" if sliding_window is not None else \"full_attention\"\n                    for _ in range(decoder_config.num_hidden_layers)\n                ]\n            # Some models have shared layers thus no cache is needed for them (e.g. Gemma3n)\n            if hasattr(decoder_config, \"num_kv_shared_layers\"):\n                layer_types = layer_types[: -decoder_config.num_kv_shared_layers]\n\n            for layer_type in layer_types:\n                # From a cache point of view, both sliding and chunked are the same in how they should behave and how many\n                # states they should return - only the mask changes to make them different at the end!\n                if layer_type in (\"sliding_attention\", \"chunked_attention\"):\n                    layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                else:\n                    layers.append(DynamicLayer())\n\n        # In this case, use the passed data to already fill in the Cache\n        if ddp_cache_data is not None:\n            # Init all the layers with the data\n            for layer_idx, kv_and_optional_sliding in enumerate(ddp_cache_data):\n                # If the config was not passed above, initialize a new cache layer for each entry of the ddp_data\n                if config is None:\n                    # kv_and_optional_sliding contains at least two elements: the key and value states. It can also\n                    # contain a third element, which is an optional sliding window tensor.\n                    sliding_window_tensor = kv_and_optional_sliding[2] if len(kv_and_optional_sliding) == 3 else None\n                    # If there is a sliding window tensor, use it to initialize the layer\n                    if sliding_window_tensor is not None:\n                        # Since the same layer is dispatched across replicas, sliding_window is the same for all\n                        sliding_window = sliding_window_tensor[0].item()\n                        layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                    else:\n                        layers.append(DynamicLayer())\n                # Update the layer with the data\n                _, _ = layers[layer_idx].update(kv_and_optional_sliding[0], kv_and_optional_sliding[1])\n\n        # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n        if len(layers) == 0:\n            super().__init__(\n                layer_class_to_replicate=DynamicLayer,\n                offloading=offloading,\n                offload_only_non_sliding=offload_only_non_sliding,\n            )\n        else:\n            super().__init__(layers=layers, offloading=offloading, offload_only_non_sliding=offload_only_non_sliding)\n\n    def __iter__(self):\n        for layer in self.layers:\n            yield layer.keys, layer.values, getattr(layer, \"_sliding_window_tensor\", None)"
                },
                "component_dependencies": {
                    "DynamicCache": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/cache_utils.py#DynamicLayer",
                        "transformers/cache_utils.py#DynamicSlidingWindowLayer",
                        "transformers/configuration_utils.py#PreTrainedConfig"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#create_causal_mask": {
                "sorted_modules": {
                    "create_causal_mask": "\n\ndef create_causal_mask(\n    config: PreTrainedConfig,\n    input_embeds: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    cache_position: torch.Tensor,\n    past_key_values: Optional[Cache],\n    position_ids: Optional[torch.Tensor] = None,\n    or_mask_function: Optional[Callable] = None,\n    and_mask_function: Optional[Callable] = None,\n) -> Optional[Union[torch.Tensor, BlockMask]]:\n    \"\"\"\n    Create a standard causal mask based on the attention implementation used (stored in the config). If `past_key_values`\n    has an hybrid cache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n    to what is needed in the `modeling_xxx.py` files).\n\n    Args:\n        config (`PreTrainedConfig`):\n            The model config.\n        input_embeds (`torch.Tensor`):\n            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n            batch size, query length and dtype.\n        attention_mask (`torch.Tensor`, optional):\n            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n            It can also be an already prepared 4D mask, in which case it is returned as-is.\n        cache_position (`torch.Tensor`):\n            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n        past_key_values (`Cache`, optional):\n            The past key values, if we use a cache.\n        position_ids (`torch.Tensor`, optional)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n        or_mask_function (`Callable`, optional):\n            An optional mask function to combine with the causal mask function (by doing the union of both). This is\n            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n        and_mask_function (`Callable`, optional):\n            An optional mask function to combine with the causal mask function (by doing the intersection of both). This is\n            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n    \"\"\"\n    # If we have an hybrid cache structure, here we want to create the mask for the full layers\n    if hasattr(past_key_values, \"is_sliding\") and False in past_key_values.is_sliding:\n        layer_idx = past_key_values.is_sliding.index(False)\n    else:\n        layer_idx = 0\n\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n    )\n    if early_exit:\n        return attention_mask\n\n    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n    mask_factory_function = causal_mask_function\n    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n\n    # Do not allow skip if we are compiling (this is to match BC)\n    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n    if _is_torch_xpu_available:\n        # Do not allow skip if we are compiling for decoding, but for prefill, we still allow skip to optimization the perf of 1st token generation\n        allow_is_causal_skip = not (getattr(past_key_values, \"is_compileable\", False) and cache_position.shape[0] == 1)\n    else:\n        allow_is_causal_skip = not getattr(past_key_values, \"is_compileable\", False)\n\n    # Allow slight deviations from causal mask\n    # Note that it is very important to apply this before any other deviations of the mask (such as packed sequence mask,\n    # padding mask, etc) as the resulting mask may otherwise not be correct!\n    if or_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n        allow_is_causal_skip = False\n    if and_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n        allow_is_causal_skip = False\n\n    # If we detected packing format\n    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n        allow_is_causal_skip = False\n\n    # We now create the mask\n    causal_mask = mask_interface(\n        batch_size=batch_size,\n        cache_position=cache_position,\n        kv_length=kv_length,\n        kv_offset=kv_offset,\n        mask_function=mask_factory_function,\n        attention_mask=attention_mask,\n        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n        dtype=dtype,  # Additional kwarg for eager\n        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n    )\n    return causal_mask"
                },
                "component_dependencies": {
                    "create_causal_mask": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                        "transformers/masking_utils.py#_is_torch_xpu_available",
                        "transformers/masking_utils.py#_preprocess_mask_arguments",
                        "transformers/masking_utils.py#and_masks",
                        "transformers/masking_utils.py#causal_mask_function",
                        "transformers/masking_utils.py#or_masks",
                        "transformers/masking_utils.py#packed_sequence_mask_function"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#create_sliding_window_causal_mask": {
                "sorted_modules": {
                    "create_sliding_window_causal_mask": "\n\ndef create_sliding_window_causal_mask(\n    config: PreTrainedConfig,\n    input_embeds: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    cache_position: torch.Tensor,\n    past_key_values: Optional[Cache],\n    position_ids: Optional[torch.Tensor] = None,\n    or_mask_function: Optional[Callable] = None,\n    and_mask_function: Optional[Callable] = None,\n) -> Optional[Union[torch.Tensor, BlockMask]]:\n    \"\"\"\n    Create a sliding window causal mask based on the attention implementation used (stored in the config). This type\n    of attention pattern was mostly democratized by Mistral. If `past_key_values` has an hybrid cache structure, this\n    function will return the mask corresponding to one of the \"sliding_attention\" layers (to align to what is needed in the\n    `modeling_xxx.py` files).\n\n    Args:\n        config (`PreTrainedConfig`):\n            The model config.\n        input_embeds (`torch.Tensor`):\n            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n            batch size, query length and dtype.\n        attention_mask (`torch.Tensor`, optional):\n            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n            It can also be an already prepared 4D mask, in which case it is returned as-is.\n        cache_position (`torch.Tensor`):\n            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n        past_key_values (`Cache`, optional):\n            The past key values, if we use a cache.\n        position_ids (`torch.Tensor`, optional)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n        or_mask_function (`Callable`, optional):\n            An optional mask function to combine with the sliding causal mask function (by doing the union of both). This is\n            useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n        and_mask_function (`Callable`, optional):\n            An optional mask function to combine with the sliding causal mask function (by doing the intersection of both). This is\n            useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n    \"\"\"\n    # If we have an hybrid cache structure, here we want to create the mask for the sliding layers\n    if hasattr(past_key_values, \"is_sliding\") and True in past_key_values.is_sliding:\n        layer_idx = past_key_values.is_sliding.index(True)\n    else:\n        layer_idx = 0\n\n    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n        config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx\n    )\n    if early_exit:\n        return attention_mask\n\n    sliding_window = getattr(config, \"sliding_window\", None)\n    if sliding_window is None:\n        raise ValueError(\"Could not find a `sliding_window` argument in the config, or it is not set\")\n\n    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n    mask_factory_function = sliding_window_causal_mask_function(sliding_window)\n    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n\n    # Do not allow skip if we are compiling (this is to match BC)\n    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n    allow_is_causal_skip = not getattr(past_key_values, \"is_compileable\", False)\n\n    # Allow slight deviations from causal mask\n    # Note that it is very important to apply this before any other deviations of the mask (such as packed sequence mask,\n    # padding mask, etc) as the resulting mask may otherwise not be correct!\n    if or_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n        allow_is_causal_skip = False\n    if and_mask_function is not None:\n        if not _is_torch_greater_or_equal_than_2_6:\n            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n        allow_is_causal_skip = False\n\n    # If we detected packing format\n    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:\n        mask_factory_function = and_masks(mask_factory_function, packed_sequence_mask_function(packed_sequence_mask))\n        allow_is_causal_skip = False\n\n    # We now create the mask\n    causal_mask = mask_interface(\n        batch_size=batch_size,\n        cache_position=cache_position,\n        kv_length=kv_length,\n        kv_offset=kv_offset,\n        mask_function=mask_factory_function,\n        attention_mask=attention_mask,\n        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n        local_size=sliding_window,  # Additional kwarg for sdpa\n        dtype=dtype,  # Additional kwarg for eager\n        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n    )\n    return causal_mask"
                },
                "component_dependencies": {
                    "create_sliding_window_causal_mask": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                        "transformers/masking_utils.py#_preprocess_mask_arguments",
                        "transformers/masking_utils.py#and_masks",
                        "transformers/masking_utils.py#or_masks",
                        "transformers/masking_utils.py#packed_sequence_mask_function",
                        "transformers/masking_utils.py#sliding_window_causal_mask_function"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig": {
                "sorted_modules": {
                    "Qwen3MoeConfig": "\n\nclass Qwen3MoeConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`Qwen3MoeModel`]. It is used to instantiate a\n    Qwen3MoE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of [Qwen/Qwen3-15B-A2B](https://huggingface.co/Qwen/Qwen3-15B-A2B).\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 151936):\n            Vocabulary size of the Qwen3MoE model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`Qwen3MoeModel`]\n        hidden_size (`int`, *optional*, defaults to 2048):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 6144):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 24):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        num_key_value_heads (`int`, *optional*, defaults to 4):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details, check out [this\n            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 32768):\n            The maximum sequence length that this model might ever be used with.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether the model's input and output word embeddings should be tied.\n        rope_parameters (`RopeParameters`, *optional*):\n            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n            with longer `max_position_embeddings`.\n        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n        use_sliding_window (`bool`, *optional*, defaults to `False`):\n            Whether to use sliding window attention.\n        sliding_window (`int`, *optional*, defaults to 4096):\n            Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        decoder_sparse_step (`int`, *optional*, defaults to 1):\n            The frequency of the MoE layer.\n        moe_intermediate_size (`int`, *optional*, defaults to 768):\n            Intermediate size of the routed expert.\n        num_experts_per_tok (`int`, *optional*, defaults to 8):\n            Number of selected experts.\n        num_experts (`int`, *optional*, defaults to 128):\n            Number of routed experts.\n        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n            Whether to normalize the topk probabilities.\n        output_router_logits (`bool`, *optional*, defaults to `False`):\n            Whether or not the router logits should be returned by the model. Enabling this will also\n            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n            The aux loss factor for the total loss.\n        mlp_only_layers (`list[int]`, *optional*, defaults to `[]`):\n            Indicate which layers use Qwen3MoeMLP rather than Qwen3MoeSparseMoeBlock\n            The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n            If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n\n    ```python\n    >>> from transformers import Qwen3MoeModel, Qwen3MoeConfig\n\n    >>> # Initializing a Qwen3MoE style configuration\n    >>> configuration = Qwen3MoeConfig()\n\n    >>> # Initializing a model from the Qwen3-15B-A2B\" style configuration\n    >>> model = Qwen3MoeModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"qwen3_moe\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    # Default tensor parallel plan for base model `Qwen3Moe`\n    base_model_tp_plan = {\n        \"layers.*.self_attn.q_proj\": \"colwise\",\n        \"layers.*.self_attn.k_proj\": \"colwise\",\n        \"layers.*.self_attn.v_proj\": \"colwise\",\n        \"layers.*.self_attn.o_proj\": \"rowwise\",\n        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n        \"layers.*.mlp.gate_proj\": \"colwise\",\n        \"layers.*.mlp.up_proj\": \"colwise\",\n        \"layers.*.mlp.down_proj\": \"rowwise\",\n    }\n    base_model_pp_plan = {\n        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n    }\n\n    def __init__(\n        self,\n        vocab_size: Optional[int] = 151936,\n        hidden_size: Optional[int] = 2048,\n        intermediate_size: Optional[int] = 6144,\n        num_hidden_layers: Optional[int] = 24,\n        num_attention_heads: Optional[int] = 32,\n        num_key_value_heads: Optional[int] = 4,\n        hidden_act: Optional[str] = \"silu\",\n        max_position_embeddings: Optional[int] = 32768,\n        initializer_range: Optional[float] = 0.02,\n        rms_norm_eps: Optional[int] = 1e-6,\n        use_cache: Optional[bool] = True,\n        tie_word_embeddings: Optional[bool] = False,\n        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n        attention_bias: Optional[bool] = False,\n        use_sliding_window: Optional[bool] = False,\n        sliding_window: Optional[int] = 4096,\n        attention_dropout: Optional[float] = 0.0,\n        decoder_sparse_step: Optional[int] = 1,\n        moe_intermediate_size: Optional[int] = 768,\n        num_experts_per_tok: Optional[int] = 8,\n        num_experts: Optional[int] = 128,\n        norm_topk_prob: Optional[bool] = False,\n        output_router_logits: Optional[bool] = False,\n        router_aux_loss_coef: Optional[float] = 0.001,\n        mlp_only_layers: Optional[bool] = None,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.use_sliding_window = use_sliding_window\n        self.sliding_window = sliding_window if use_sliding_window else None\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n        self.rope_parameters = rope_scaling or rope_parameters\n\n        # Validate the correctness of rotary position embeddings parameters\n        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n        standardize_rope_params(self, rope_theta=rope_theta)\n        rope_config_validation(self)\n\n        # MoE arguments\n        self.decoder_sparse_step = decoder_sparse_step\n        self.moe_intermediate_size = moe_intermediate_size\n        self.num_experts_per_tok = num_experts_per_tok\n        self.num_experts = num_experts\n        self.norm_topk_prob = norm_topk_prob\n        self.output_router_logits = output_router_logits\n        self.router_aux_loss_coef = router_aux_loss_coef\n        self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n\n        super().__init__(\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )"
                },
                "component_dependencies": {
                    "Qwen3MoeConfig": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#RopeParameters",
                        "transformers/modeling_rope_utils.py#rope_config_validation",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm": {
                "sorted_modules": {
                    "Qwen3MoeRMSNorm": "\n\n@use_kernel_forward_from_hub(\"RMSNorm\")\nclass Qwen3MoeRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Qwen3MoeRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n    def extra_repr(self):\n        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
                },
                "component_dependencies": {
                    "Qwen3MoeRMSNorm": [
                        "transformers/integrations.py#use_kernel_forward_from_hub"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRotaryEmbedding": {
                "sorted_modules": {
                    "Qwen3MoeRotaryEmbedding": "\n\nclass Qwen3MoeRotaryEmbedding(nn.Module):\n    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n\n    def __init__(self, config: Qwen3MoeConfig, device=None):\n        super().__init__()\n        self.max_seq_len_cached = config.max_position_embeddings\n        self.original_max_seq_len = config.max_position_embeddings\n\n        self.config = config\n\n        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n        rope_init_fn: Callable = self.compute_default_rope_parameters\n        if self.rope_type != \"default\":\n            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.original_inv_freq = inv_freq\n\n    @staticmethod\n    def compute_default_rope_parameters(\n        config: Optional[Qwen3MoeConfig] = None,\n        device: Optional[\"torch.device\"] = None,\n        seq_len: Optional[int] = None,\n    ) -> tuple[\"torch.Tensor\", float]:\n        \"\"\"\n        Computes the inverse frequencies according to the original RoPE implementation\n        Args:\n            config ([`~transformers.PreTrainedConfig`]):\n                The model configuration.\n            device (`torch.device`):\n                The device to use for initialization of the inverse frequencies.\n            seq_len (`int`, *optional*):\n                The current sequence length. Unused for this type of RoPE.\n        Returns:\n            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n        \"\"\"\n        base = config.rope_parameters[\"rope_theta\"]\n        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n\n        attention_factor = 1.0  # Unused in this type of RoPE\n\n        # Compute the inverse frequencies\n        inv_freq = 1.0 / (\n            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n        )\n        return inv_freq, attention_factor\n\n    @torch.no_grad()\n    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n    def forward(self, x, position_ids):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos() * self.attention_scaling\n            sin = emb.sin() * self.attention_scaling\n\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
                },
                "component_dependencies": {
                    "Qwen3MoeRotaryEmbedding": [
                        "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                        "transformers/modeling_rope_utils.py#dynamic_rope_update",
                        "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig"
                    ]
                },
                "warning": null
            },
            "transformers/utils/generic.py#check_model_inputs": {
                "sorted_modules": {
                    "check_model_inputs": "\n\ndef check_model_inputs(tie_last_hidden_states=True):\n    \"\"\"\n    Decorator to intercept specific layer outputs without using hooks.\n    Compatible with torch.compile (Dynamo tracing).\n\n    Args:\n        tie_last_hidden_states (`bool`, *optional*, defaults to `True`):\n            Whether to overwrite `out.hidden_states[-1]` with the `out.last_hidden_state`.\n            This is true for all language models and should be toggled off only if\n            `out.hidden_states[-1]` has to be the hidden state before last layer norm, which\n            is needed for some vision models (e.g. CLIP, SigLIP)\n    \"\"\"\n\n    def wrapped_fn(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            use_cache_arg_index = None\n            if \"use_cache\" in func.__code__.co_varnames:\n                use_cache_arg_index = func.__code__.co_varnames.index(\"use_cache\") - 1  # -1 for self\n\n            if (\n                use_cache_arg_index is not None\n                and len(args) > use_cache_arg_index\n                and args[use_cache_arg_index] is not None\n            ):\n                use_cache = args[use_cache_arg_index]\n            elif kwargs.get(\"use_cache\") is not None:\n                use_cache = kwargs[\"use_cache\"]\n            else:\n                use_cache = getattr(self.config, \"use_cache\", None)\n\n            if use_cache is not None:\n                if getattr(self, \"gradient_checkpointing\", False) and self.training and use_cache:\n                    logger.warning_once(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n                    )\n                    use_cache = False\n\n                if use_cache_arg_index is not None and len(args) > use_cache_arg_index:\n                    args = list(args)\n                    args[use_cache_arg_index] = use_cache\n                    args = tuple(args)\n                else:\n                    kwargs[\"use_cache\"] = use_cache\n\n            return_dict = kwargs.pop(\"return_dict\", None)\n            if return_dict is None:\n                return_dict = getattr(self.config, \"return_dict\", True)\n\n            all_args = kwargs.copy()\n            if \"kwargs\" in all_args:\n                for k, v in all_args[\"kwargs\"].items():\n                    all_args[k] = v\n\n            # _can_record_outputs is None by default\n            capture_flags = _CAN_RECORD_REGISTRY.get(str(self.__class__)) or {}  # there is a weak ref for executorch\n            recordable_keys = {\n                f\"output_{k}\": all_args.get(\n                    f\"output_{k}\",\n                    getattr(\n                        self.config,\n                        f\"output_{k}\",\n                        all_args.get(\"output_attentions\", getattr(self.config, \"output_attentions\", False)),\n                    ),\n                )\n                for k in capture_flags\n            }\n\n            # We let cross attentions to be saved separately because some models add `cross-attn` layer\n            # when certain condtions are met. Let's output cross attention if attentions are requested (for BC)\n            if \"output_attentions\" in recordable_keys:\n                recordable_keys[\"output_cross_attentions\"] = recordable_keys[\"output_attentions\"]\n\n            collected_outputs = defaultdict(tuple)\n            monkey_patched_layers = []\n\n            # Check attention implementation is properly set for capturing attention outputs\n            if recordable_keys.get(\"output_attentions\", False):\n                supported_attn = [\"eager\", \"eager_paged\", \"flex_attention\"]\n                config_attn = getattr(self.config, \"_attn_implementation\", None)\n                sub_configs = [getattr(self.config, key, None) for key in self.config.sub_configs]\n                sub_configs_attn = [\n                    getattr(config, \"_attn_implementation\", None) for config in sub_configs if config is not None\n                ]\n                if config_attn not in supported_attn or any(attn not in supported_attn for attn in sub_configs_attn):\n                    warnings.warn(\n                        f\"`output_attentions=True` is not supported with `attn_implementation` other than {supported_attn}. \"\n                        \"Please use `model.set_attn_implementation('eager')` to enable capturing attention outputs.\",\n                        UserWarning,\n                    )\n\n            def make_capture_wrapper(module, orig_forward, key, index):\n                @wraps(orig_forward)\n                def wrapped_forward(*args, **kwargs):\n                    if key == \"hidden_states\" and len(collected_outputs[key]) == 0:\n                        collected_outputs[key] += (args[0],)\n                    if kwargs.get(\"debug_io\", False):\n                        with model_addition_debugger_context(\n                            module, kwargs.get(\"debug_io_dir\", \"~/model_debug\"), kwargs.get(\"prune_layers\")\n                        ):\n                            output = orig_forward(*args, **kwargs)\n                    else:\n                        output = orig_forward(*args, **kwargs)\n                    if not isinstance(output, tuple):\n                        collected_outputs[key] += (output,)\n                    elif output[index] is not None:\n                        if key not in collected_outputs:\n                            collected_outputs[key] = (output[index],)\n                        else:\n                            collected_outputs[key] += (output[index],)\n                    return output\n\n                return wrapped_forward\n\n            if any(recordable_keys.values()):\n                capture_tasks = []\n                for key, layer_specs in capture_flags.items():\n                    if not recordable_keys.get(f\"output_{key}\", False):\n                        continue\n                    if not isinstance(layer_specs, list):\n                        layer_specs = [layer_specs]\n                    for specs in layer_specs:\n                        if not isinstance(specs, OutputRecorder):\n                            index = 0 if \"hidden_states\" in key else 1\n                            class_name = None if not isinstance(specs, str) else specs\n                            target_class = specs if not isinstance(specs, str) else None\n                            specs = OutputRecorder(target_class=target_class, index=index, class_name=class_name)\n                        capture_tasks.append((key, specs))\n\n                for name, module in self.named_modules():\n                    for key, specs in capture_tasks:\n                        # The second check is for multimodals where only backbone layer suffix is available\n                        if (specs.target_class is not None and isinstance(module, specs.target_class)) or (\n                            specs.class_name is not None and name.endswith(specs.class_name)\n                        ):\n                            if specs.layer_name is not None and specs.layer_name not in name:\n                                continue\n                            # Monkey patch forward\n                            original_forward = module.forward\n                            module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n                            monkey_patched_layers.append((module, original_forward))\n\n            try:\n                outputs = func(self, *args, **kwargs)\n            except TypeError as original_exception:\n                # If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\n                # Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\n                # Otherwise -> we're probably missing `**kwargs` in the decorated function\n                kwargs_without_recordable = {k: v for k, v in kwargs.items() if k not in recordable_keys}\n                try:\n                    outputs = func(self, *args, **kwargs_without_recordable)\n                except TypeError:\n                    raise original_exception\n                raise TypeError(\n                    \"Missing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \"\n                    f\"({func.__qualname__})\"\n                )\n\n            # Restore original forward methods\n            for module, original_forward in monkey_patched_layers:\n                module.forward = original_forward\n\n            # Inject collected outputs into model output\n            for key in collected_outputs:\n                if key == \"hidden_states\":\n                    if not tie_last_hidden_states:\n                        pass\n                    elif hasattr(outputs, \"vision_hidden_states\"):\n                        collected_outputs[key] = collected_outputs[key][:-1]\n                        collected_outputs[key] += (outputs.vision_hidden_states,)\n                    elif hasattr(outputs, \"last_hidden_state\"):\n                        collected_outputs[key] = collected_outputs[key][:-1]\n                        collected_outputs[key] += (outputs.last_hidden_state,)\n\n                    outputs[key] = collected_outputs[key]\n                elif key == \"attentions\":\n                    if isinstance(capture_flags[key], list) and len(capture_flags[key]) == 2:\n                        outputs[key] = collected_outputs[key][0::2]\n                        outputs[\"cross_\" + key] = collected_outputs[key][1::2]\n                    else:\n                        outputs[key] = collected_outputs[key]\n                else:\n                    outputs[key] = collected_outputs[key]\n            if return_dict is False:\n                outputs = outputs.to_tuple()\n            return outputs\n\n        return wrapper\n\n    return wrapped_fn"
                },
                "component_dependencies": {
                    "check_model_inputs": [
                        "transformers/utils/generic.py#OutputRecorder",
                        "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
                        "transformers/utils/generic.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_utils.py#PreTrainedModel": {
                "sorted_modules": {
                    "PreTrainedModel": "\n\nclass PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):\n    r\"\"\"\n    Base class for all models.\n\n    [`PreTrainedModel`] takes care of storing the configuration of the models and handles methods for loading,\n    downloading and saving models as well as a few methods common to all models to:\n\n        - resize the input embeddings\n\n    Class attributes (overridden by derived classes):\n\n        - **config_class** ([`PreTrainedConfig`]) -- A subclass of [`PreTrainedConfig`] to use as configuration class\n          for this model architecture.\n        - **base_model_prefix** (`str`) -- A string indicating the attribute associated to the base model in derived\n          classes of the same architecture adding modules on top of the base model.\n        - **main_input_name** (`str`) -- The name of the principal input to the model (often `input_ids` for NLP\n          models, `pixel_values` for vision models and `input_values` for speech models).\n        - **can_record_outputs** (dict):\n    \"\"\"\n\n    config_class = None\n    base_model_prefix = \"\"\n    main_input_name = \"input_ids\"\n    model_tags = None\n\n    _checkpoint_conversion_mapping = {}  # used for BC support in VLMs, not meant to be used by new models\n\n    _auto_class = None\n    _no_split_modules = None\n    _skip_keys_device_placement = None\n\n    _keep_in_fp32_modules = None\n    # the _keep_in_fp32_modules will avoid casting to anything other than float32, except bfloat16\n    # to also prevent bfloat16 casting, use the _keep_in_fp32_modules_strict flag\n    _keep_in_fp32_modules_strict = None\n\n    # a list of `re` patterns of `state_dict` keys that should be removed from the list of missing\n    # keys we find (keys inside the model but not in the checkpoint) and avoid unnecessary warnings.\n    _keys_to_ignore_on_load_missing = None\n    # a list of `re` patterns of `state_dict` keys that should be removed from the list of\n    # unexpected keys we find (keys inside the checkpoint but not the model) and avoid unnecessary\n    # warnings.\n    _keys_to_ignore_on_load_unexpected = None\n    # a list of `state_dict` keys to ignore when saving the model (useful for keys that aren't\n    # trained, but which are either deterministic or tied variables)\n    _keys_to_ignore_on_save = None\n    # a list of `state_dict` keys that are potentially tied to another key in the state_dict.\n    _tied_weights_keys = None\n\n    supports_gradient_checkpointing = False\n    _is_stateful = False\n\n    # Flash Attention support\n    _supports_flash_attn = False\n\n    # SDPA support\n    _supports_sdpa = False\n\n    # Flex Attention support\n    _supports_flex_attn = False\n\n    _can_compile_fullgraph = False\n\n    # A tensor parallel plan to be applied to the model when TP is enabled. For\n    # top-level models, this attribute is currently defined in respective model\n    # code. For base models, this attribute comes from\n    # `config.base_model_tp_plan` during `__init__`.\n    # It should identify the layers exactly: if you want to TP model.language_model.layers.fc1\n    # by passing `tp_plan` to the init, it should be {\"model.language_model.layers.fc1\":\"colwise\"}\n    # for example.\n    _tp_plan = None\n\n    # tensor parallel degree to which model is sharded to.\n    _tp_size = None\n\n    # A pipeline parallel plan specifying the layers which may not be present\n    # on all ranks when PP is enabled. For top-level models, this attribute is\n    # currently defined in respective model code. For base models, this\n    # attribute comes from `config.base_model_pp_plan` during `post_init`.\n    #\n    # The variable names for the inputs and outputs of the specified layers can\n    # be indexed using the `PipelineParallel` enum as follows:\n    # - `_pp_plan[\"layers\"][PipelineParallel.inputs]`\n    # - `_pp_plan[\"layers\"][PipelineParallel.outputs]`\n    _pp_plan = None\n\n    # This flag signal that the model can be used as an efficient backend in TGI and vLLM\n    # In practice, it means that they support attention (mask) interface functions, fully pass the kwargs\n    # through all modules up to the Attention layer, can slice logits with Tensor, and have a default TP plan\n    _supports_attention_backend = False\n    _can_record_outputs = None\n\n    # Attributes used mainly in multimodal LLMs, though all models contain a valid field for these\n    # Possible values are: text, image, video, audio and time\n    input_modalities: Union[str, list[str]] = \"text\"  # most models are text\n\n    @property\n    @torch._dynamo.allow_in_graph\n    def can_record_outputs(self) -> dict[str, OutputRecorder]:\n        \"\"\"\n         Maps output names (e.g., \"attentions\", \"hidden_states\")\n         to either:\n             - A module class (e.g., `LlamaDecoderLayer`), using default index conventions:\n                 * index=0 for \"hidden_states\"\n                 * index=1 for \"attentions\"\n             - Or an `OutputRecorder(...)` with `target_class`, optional `index`, and `layer_name`.\n\n         Examples:\n             These two are equivalent:\n\n         ```python\n             _can_record_outputs = {\n                 \"attentions\": LlamaAttention,\n                 \"hidden_states\": LlamaDecoderLayer\n             }\n\n             _can_record_outputs = {\n                 \"attentions\": OutputRecorder(LlamaAttention, index=1),\n                 \"hidden_states\": OutputRecorder(LlamaDecoderLayer, index=0)\n             }\n        ```\n\n         This means you can record outputs from the same class, by specifying a layer name. Before\n         collecting outputs, we check that they come from this layer.\n\n         If you have cross attention that come from `LlamaAttention` and self attention that also\n         come from `LlamaAttention` but from `self_attn` you can do this:\n\n         ```python\n         class LlamaModel(PreTrainedModel):\n             _can_record_outputs = {\n                 \"attentions\": OutputRecorder(LlamaAttention, index=1, layer-name=\"self_attn\"),\n                 \"cross_attentions\": OutputRecorder(LlamaAttention, index=1, layer_name=\"cross_attn\")\n             }\n\n        ```\n        \"\"\"\n        return self._can_record_outputs or {}\n\n    @property\n    def dummy_inputs(self) -> dict[str, torch.Tensor]:\n        \"\"\"\n        `dict[str, torch.Tensor]`: Dummy inputs to do a forward pass in the network.\n        \"\"\"\n        return {\"input_ids\": torch.tensor(DUMMY_INPUTS)}\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        # For BC we keep the original `config_class` definition in case\n        # there is a `config_class` attribute (e.g. remote code models),\n        # otherwise we derive it from the annotated `config` attribute.\n\n        # defined in this particular subclass\n        child_annotation = cls.__dict__.get(\"__annotations__\", {}).get(\"config\", None)\n        child_attribute = cls.__dict__.get(\"config_class\", None)\n\n        # defined in the class (this subclass or any parent class)\n        full_annotation = get_type_hints(cls).get(\"config\", None)\n        full_attribute = cls.config_class\n\n        # priority (child class_config -> child annotation -> global class_config -> global annotation)\n        if child_attribute is not None:\n            cls.config_class = child_attribute\n        elif child_annotation is not None:\n            cls.config_class = child_annotation\n        elif full_attribute is not None:\n            cls.config_class = full_attribute\n        elif full_annotation is not None:\n            cls.config_class = full_annotation\n\n    def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n        super().__init__()\n        if not isinstance(config, PreTrainedConfig):\n            raise TypeError(\n                f\"Parameter config in `{self.__class__.__name__}(config)` should be an instance of class \"\n                \"`PreTrainedConfig`. To create a model from a pretrained model use \"\n                f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n            )\n        self.config = config\n\n        # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n        # setting it recursively)\n        self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n            self.config._attn_implementation, is_init_check=True\n        )\n\n        # for initialization of the loss\n        loss_type = self.__class__.__name__\n        if loss_type not in LOSS_MAPPING:\n            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n            loss_type = re.findall(loss_groups, self.__class__.__name__)\n            if len(loss_type) > 0:\n                loss_type = loss_type[0]\n            else:\n                loss_type = None\n        self.loss_type = loss_type\n\n        self.name_or_path = config.name_or_path\n        self.warnings_issued = {}\n        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n        # Overwrite the class attribute to make it an instance attribute, so models like\n        # `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute\n        # when a different component (e.g. language_model) is used.\n        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n\n        self._no_split_modules = self._no_split_modules or []\n        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs  # added for executorch support only\n\n    def post_init(self):\n        \"\"\"\n        A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n        modules properly initialized (such as weight initialization).\n\n        This is also used when the user is running distributed code. We add hooks to the modules here, according to\n        the model's tp_plan!\n        \"\"\"\n        self.init_weights()\n        self._backward_compatibility_gradient_checkpointing()\n\n        # Make sure the modules correctly exist if the flag is active\n        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n            unique_module_names = set()\n            # Get all unique module names in the module graph, without the prefixes\n            for param in all_parameters:\n                unique_module_names.update(\n                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n                )\n            # Check that every module in the keep_in_fp32 list is part of the module graph\n            if self._keep_in_fp32_modules is not None:\n                for module in self._keep_in_fp32_modules:\n                    if module not in unique_module_names:\n                        raise ValueError(\n                            f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n                            f\" {self.__class__.__name__}\"\n                        )\n\n            if self._keep_in_fp32_modules_strict is not None:\n                for module in self._keep_in_fp32_modules_strict:\n                    if module not in unique_module_names:\n                        raise ValueError(\n                            f\"{module} was specified in the `_keep_in_fp32_modules_strict` list, but is not part of the modules in\"\n                            f\" {self.__class__.__name__}\"\n                        )\n\n        self._tp_plan, self._ep_plan, self._pp_plan = {}, {}, {}\n        # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n        if self.base_model is self:\n            self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n            self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n            self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n        for name, module in self.named_children():\n            if plan := getattr(module, \"_ep_plan\", None):\n                self._ep_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n            if plan := getattr(module, \"_tp_plan\", None):\n                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n            if plan := getattr(module, \"_pp_plan\", None):\n                self._pp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n\n    @property\n    def tp_plan(self) -> dict[str, str]:\n        \"\"\"\n        The full tp plan for the model's modules\n        \"\"\"\n        if hasattr(self.config, \"distributed_config\") and self.config.distributed_config.enable_expert_parallel:\n            return self._ep_plan\n        return self._tp_plan\n\n    @property\n    def pp_plan(self) -> dict[str, tuple[str, str]]:\n        return self._pp_plan\n\n    @tp_plan.setter\n    def tp_plan(self, plan: dict[str, str] | None):\n        if plan is None:\n            self._tp_plan = {}\n            return\n        if not isinstance(plan, dict):\n            raise ValueError(\"Can only set a dictionary as `tp_plan`\")\n\n        # Ensure the styles are all valid\n        for layer_pattern, parallel_style in plan.items():\n            if parallel_style not in ALL_PARALLEL_STYLES:\n                raise ValueError(\n                    f\"Unsupported tensor parallel style '{parallel_style}' for layer '{layer_pattern}'. \"\n                    f\"Supported styles are {list(ALL_PARALLEL_STYLES.keys())}\"\n                )\n\n        # Validate that the layer patterns match existing model structure. We check this by getting all parameter\n        # names and seeing if any match the patterns\n        model_param_names = [name for name, _ in self.named_parameters()]\n        for layer_pattern in plan.keys():\n            # Convert pattern to regex (replace * with .*)\n            regex_pattern = layer_pattern.replace(\"*\", r\"\\d+\")\n            pattern_matched = False\n            for param_name in model_param_names:\n                if re.match(regex_pattern, param_name):\n                    pattern_matched = True\n                    break\n            if not pattern_matched:\n                warnings.warn(\n                    f\"Layer pattern '{layer_pattern}' does not match any parameters in the model. This rule may not \"\n                    \"be applied during tensor parallelization, or may lead to dimension mismatches\"\n                )\n\n        # Set the plan\n        self._tp_plan = plan\n\n    @pp_plan.setter\n    def pp_plan(self, plan: dict[str, tuple[str, str]]):\n        self._pp_plan = plan\n\n    def dequantize(self):\n        \"\"\"\n        Potentially dequantize the model in case it has been quantized by a quantization method that support\n        dequantization.\n        \"\"\"\n        hf_quantizer = getattr(self, \"hf_quantizer\", None)\n\n        if hf_quantizer is None:\n            raise ValueError(\"You need to first quantize your model in order to dequantize it\")\n\n        return hf_quantizer.dequantize(self)\n\n    def _backward_compatibility_gradient_checkpointing(self):\n        if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n            self.gradient_checkpointing_enable()\n            # Remove the attribute now that is has been consumed, so it's no saved in the config.\n            delattr(self.config, \"gradient_checkpointing\")\n\n    def add_model_tags(self, tags: Union[list[str], str]) -> None:\n        r\"\"\"\n        Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\n        not overwrite existing tags in the model.\n\n        Args:\n            tags (`Union[list[str], str]`):\n                The desired tags to inject in the model\n\n        Examples:\n\n        ```python\n        from transformers import AutoModel\n\n        model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n\n        model.add_model_tags([\"custom\", \"custom-bert\"])\n\n        # Push the model to your namespace with the name \"my-custom-bert\".\n        model.push_to_hub(\"my-custom-bert\")\n        ```\n        \"\"\"\n        if isinstance(tags, str):\n            tags = [tags]\n\n        if self.model_tags is None:\n            self.model_tags = []\n\n        for tag in tags:\n            if tag not in self.model_tags:\n                self.model_tags.append(tag)\n\n    @classmethod\n    @restore_default_dtype\n    def _from_config(cls, config, **kwargs):\n        \"\"\"\n        All context managers that the model should be initialized under go here.\n\n        Args:\n            dtype (`torch.dtype`, *optional*):\n                Override the default `dtype` and load the model under this dtype.\n        \"\"\"\n        # when we init a model from within another model (e.g. VLMs) and dispatch on FA2\n        # a warning is raised that dtype should be fp16. Since we never pass dtype from within\n        # modeling code, we can try to infer it here same way as done in `from_pretrained`\n        # For BC on the old `torch_dtype`\n        dtype = kwargs.pop(\"dtype\", config.dtype)\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n            # if both kwargs are provided, use `dtype`\n            dtype = dtype if dtype != config.dtype else torch_dtype\n        if isinstance(dtype, str):\n            dtype = getattr(torch, dtype)\n\n        # override default dtype if needed\n        dtype_orig = None\n        if dtype is not None:\n            dtype_orig = cls._set_default_dtype(dtype)\n\n        # If passing `attn_implementation` as kwargs, respect it (it will be applied recursively on subconfigs)\n        if \"attn_implementation\" in kwargs:\n            config._attn_implementation = kwargs.pop(\"attn_implementation\")\n\n        if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n            logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n            # this immediately partitions the model across all gpus, to avoid the overhead in time\n            # and memory copying it on CPU or each GPU first\n            import deepspeed\n\n            init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()]\n            with ContextManagers(init_contexts):\n                model = cls(config, **kwargs)\n\n        else:\n            model = cls(config, **kwargs)\n\n        # restore default dtype if it was modified\n        if dtype_orig is not None:\n            torch.set_default_dtype(dtype_orig)\n\n        return model\n\n    @classmethod\n    def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n        \"\"\"\n        Change the default dtype and return the previous one. This is needed when wanting to instantiate the model\n        under specific dtype.\n\n        Args:\n            dtype (`torch.dtype`):\n                a floating dtype to set to.\n\n        Returns:\n            `torch.dtype`: the original `dtype` that can be used to restore `torch.set_default_dtype(dtype)` if it was\n            modified. If it wasn't, returns `None`.\n\n        Note `set_default_dtype` currently only works with floating-point types and asserts if for example,\n        `torch.int64` is passed. So if a non-float `dtype` is passed this functions will throw an exception.\n        \"\"\"\n        if not dtype.is_floating_point:\n            raise ValueError(\n                f\"Can't instantiate {cls.__name__} model under dtype={dtype} since it is not a floating point dtype\"\n            )\n\n        logger.info(f\"Instantiating {cls.__name__} model under default dtype {dtype}.\")\n        dtype_orig = torch.get_default_dtype()\n        torch.set_default_dtype(dtype)\n        return dtype_orig\n\n    @property\n    def base_model(self) -> nn.Module:\n        \"\"\"\n        `torch.nn.Module`: The main body of the model.\n        \"\"\"\n        return getattr(self, self.base_model_prefix, self)\n\n    @classmethod\n    def can_generate(cls) -> bool:\n        \"\"\"\n        Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\n        Under the hood, on classes where this function returns True, some generation-specific changes are triggered:\n        for instance, the model instance will have a populated `generation_config` attribute.\n\n        Returns:\n            `bool`: Whether this model can generate sequences with `.generate()`.\n        \"\"\"\n        # Directly inherits `GenerationMixin` -> can generate\n        if \"GenerationMixin\" in str(cls.__bases__):\n            return True\n        # The class inherits from a class that can generate (recursive check) -> can generate\n        for base in cls.__bases__:\n            if not hasattr(base, \"can_generate\"):\n                continue\n            if \"PreTrainedModel\" not in str(base) and base.can_generate():\n                return True\n        # Detects whether `prepare_inputs_for_generation` has been overwritten in the model. Prior to v4.45, this\n        # was how we detected whether a model could generate.\n        if hasattr(cls, \"prepare_inputs_for_generation\"):  # implicit: doesn't inherit `GenerationMixin`\n            logger.warning(\n                f\"{cls.__name__} has generative capabilities, as `prepare_inputs_for_generation` is explicitly \"\n                \"defined. However, it doesn't directly inherit from `GenerationMixin`. From \ud83d\udc49v4.50\ud83d\udc48 onwards, \"\n                \"`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability \"\n                \"to call `generate` and other related functions.\"\n                \"\\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the \"\n                \"model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\"\n                \"\\n  - If you are the owner of the model architecture code, please modify your model class such that \"\n                \"it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\"\n                \"\\n  - If you are not the owner of the model architecture class, please contact the model code owner \"\n                \"to update it.\"\n            )\n        # Otherwise, can't generate\n        return False\n\n    def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of Flash Attention 2 for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        dtype = self.config.dtype\n\n        # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n        if not (self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False)):\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support Flash Attention 2.0 yet. Please request to add support where\"\n                f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n            )\n\n        if not is_flash_attn_2_available():\n            preface = \"FlashAttention2 has been toggled on, but it cannot be used due to the following error:\"\n            install_message = \"Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\"\n\n            # package `flash-attn` can not be installed on Ascend NPU, following validation logics can be ignored.\n            if is_torch_npu_available():\n                logger.info(\"Detect using FlashAttention2 on Ascend NPU.\")\n                return True\n\n            if importlib.util.find_spec(\"flash_attn\") is None:\n                raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n            else:\n                # Check FA2 installed version compatibility\n                flash_attention_version = version.parse(importlib.metadata.version(\"flash_attn\"))\n                if torch.version.cuda:\n                    if flash_attention_version < version.parse(\"2.1.0\"):\n                        raise ImportError(\n                            f\"{preface} you need flash_attn package version to be greater or equal than 2.1.0. Detected version {flash_attention_version}. {install_message}\"\n                        )\n                    elif not torch.cuda.is_available():\n                        raise ValueError(\n                            f\"{preface} Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.\"\n                        )\n                    else:\n                        raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n                elif torch.version.hip:\n                    if flash_attention_version < version.parse(\"2.0.4\"):\n                        raise ImportError(\n                            f\"{preface} you need flash_attn package version to be greater or equal than 2.0.4. Detected version {flash_attention_version}. {install_message}\"\n                        )\n                    else:\n                        raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n\n        if dtype is None:\n            logger.warning_once(\n                \"You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\"\n            )\n        elif dtype is not None and dtype not in [torch.float16, torch.bfloat16]:\n            logger.warning_once(\n                \"Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n                f\" the current dype in {self.__class__.__name__} is {dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                ' or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", dtype=torch.float16)`'\n            )\n\n        # With the early check, the parameters are not yet initialized correctly\n        if not is_init_check:\n            param_devices = list({param.device for param in self.parameters()})\n            if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n                if torch.cuda.is_available():\n                    logger.warning_once(\n                        \"You are attempting to use Flash Attention 2 with a model not initialized on GPU. Make sure to move the model to GPU\"\n                        \" after initializing it on CPU with `model.to('cuda')`.\"\n                    )\n                elif is_torch_mlu_available():\n                    logger.warning_once(\n                        \"You are attempting to use Flash Attention 2 with a model not initialized on MLU. Make sure to move the model to MLU\"\n                        \" after initializing it on CPU with `model.to('mlu')`.\"\n                    )\n                else:\n                    raise ValueError(\n                        \"You are attempting to use Flash Attention 2 with a model not initialized on GPU and with no GPU available. \"\n                        \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n                        \"or initialising the model on CPU and then moving it to GPU.\"\n                    )\n\n        # If no error raise by this point, we can return `True`\n        return True\n\n    def _flash_attn_3_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of Flash Attention 3 for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        dtype = self.config.dtype\n\n        if not self._supports_flash_attn:\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support Flash Attention 3 yet. Please request to add support where\"\n                f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n            )\n\n        if not is_flash_attn_3_available():\n            preface = \"FlashAttention3 has been toggled on, but it cannot be used due to the following error:\"\n\n            if importlib.util.find_spec(\"flash_attn_3\") is None:\n                raise ImportError(f\"{preface} the package flash_attn_3 seems to be not installed.\")\n\n            if torch.cuda.is_available():\n                major, _ = torch.cuda.get_device_capability()\n                if major < 9:\n                    raise ValueError(\n                        f\"{preface} Flash Attention 3 requires compute capability >= 9.0, but found {torch.cuda.get_device_capability()} with compute capability {major}.0.\"\n                    )\n                else:\n                    raise ImportError(f\"{preface} Flash Attention 3 is not available.\")\n            else:\n                raise ValueError(\n                    f\"{preface} Flash Attention 3 is not available on CPU. Please make sure torch can access a CUDA device.\"\n                )\n\n        if dtype is None:\n            logger.warning_once(\n                \"You are attempting to use Flash Attention 3 without specifying a torch dtype. This might lead to unexpected behaviour\"\n            )\n        elif dtype is not None and dtype not in [torch.float16, torch.bfloat16]:\n            logger.warning_once(\n                \"Flash Attention 3 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n                f\" the current dype in {self.__class__.__name__} is {dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                ' or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_3\", dtype=torch.float16)`'\n            )\n\n        if getattr(self.config, \"alibi\", False) or getattr(self.config, \"use_alibi\", False):\n            raise ValueError(\"Model is configured to use ALiBi, which is not supported by Flash Attention 3.\")\n\n        # Check for attention dropout, which is incompatible with FA3\n        if hasattr(self.config, \"attention_dropout\") and self.config.attention_dropout > 0:\n            raise ValueError(\n                f\"Model has attention_dropout={self.config.attention_dropout}, which is not supported by Flash Attention 3.\"\n            )\n\n        # With the early check, the parameters are not yet initialized correctly\n        if not is_init_check:\n            param_devices = list({param.device for param in self.parameters()})\n            if len(param_devices) == 1 and param_devices[0].type == \"cpu\":\n                if torch.cuda.is_available():\n                    logger.warning_once(\n                        \"You are attempting to use Flash Attention 3 with a model not initialized on GPU. Make sure to move the model to GPU\"\n                        \" after initializing it on CPU with `model.to('cuda')`.\"\n                    )\n                else:\n                    raise ValueError(\n                        \"You are attempting to use Flash Attention 3 with a model not initialized on GPU and with no GPU available. \"\n                        \"This is not supported yet. Please make sure to have access to a GPU and either initialise the model on a GPU by passing a device_map \"\n                        \"or initialising the model on CPU and then moving it to GPU.\"\n                    )\n\n        return True\n\n    def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of SDPA for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        if not self._supports_sdpa:\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n                \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n                ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n            )\n\n        if (\n            torch.version.hip is not None\n            and torch.cuda.device_count() > 1\n            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n        ):\n            logger.warning_once(\n                \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n            )\n            torch.backends.cuda.enable_flash_sdp(False)\n\n        return True\n\n    def _flex_attn_can_dispatch(self, is_init_check: bool = False) -> bool:\n        \"\"\"\n        Check the availability of Flex Attention for a given model.\n\n        Args:\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n        \"\"\"\n        if not self._supports_flex_attn:\n            raise ValueError(\n                f\"{self.__class__.__name__} does not support an attention implementation through torch's flex_attention.\"\n                \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/34809.\"\n                \" If you believe this error is a bug, please open an issue in Transformers GitHub repository\"\n                ' and load your model with the argument `attn_implementation=\"eager\"` meanwhile.'\n                ' Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n            )\n        if not is_torch_flex_attn_available():\n            raise ImportError(\n                \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n            )\n\n        # If no error raise by this point, we can return `True`\n        return True\n\n    def _check_and_adjust_attn_implementation(\n        self, attn_implementation: Optional[str], is_init_check: bool = False\n    ) -> str:\n        \"\"\"\n        Check that the `attn_implementation` exists and is supported by the models, and try to get the kernel from hub if\n        it matches hf kernels pattern.\n\n        Args:\n            attn_implementation (`str` or `None`):\n                The attention implementation to check for existence/validity.\n            is_init_check (`bool`, *optional*):\n                Whether this check is performed early, i.e. at __init__ time, or later when the model and its weights are\n                fully instantiated. This is needed as we also check the devices of the weights, which are only available\n                later after __init__. This allows to raise proper exceptions early before instantiating the full models\n                if we know that the model does not support the requested attention.\n\n        Returns:\n            `str`: The final attention implementation to use, including potential fallbacks from sdpa to eager, or from\n            None to sdpa (to potentially eager).\n        \"\"\"\n        applicable_attn_implementation = attn_implementation\n\n        # If FA not installed, do not fail but use kernels instead\n        if (\n            attn_implementation is not None\n            and \"flash\" in attn_implementation\n            and self._supports_flash_attn\n            and not (is_flash_attn_2_available() or is_flash_attn_3_available())\n            and is_kernels_available()\n            and not is_torch_npu_available()\n        ):\n            if attn_implementation.endswith(\"2\"):\n                applicable_attn_implementation = \"kernels-community/flash-attn\"\n            else:\n                applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n\n        if is_kernel(applicable_attn_implementation):\n            try:\n                load_and_register_attn_kernel(applicable_attn_implementation)\n                # log that we used kernel fallback if successful\n                if \"flash_\" in attn_implementation:\n                    logger.warning_once(\n                        f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                        \"from the `kernels` library instead!\"\n                    )\n            except Exception as e:\n                # raise the proper exception for requested flash attention\n                if attn_implementation.startswith(\"flash_\"):\n                    if attn_implementation.endswith(\"2\"):\n                        self._flash_attn_2_can_dispatch()\n                    else:\n                        self._flash_attn_3_can_dispatch()\n\n                # error properly out if a kernel was specifically requested\n                raise e\n        else:\n            applicable_attn_implementation = self.get_correct_attn_implementation(\n                applicable_attn_implementation, is_init_check\n            )\n            # preload flash attention here to allow compile with fullgraph\n            if applicable_attn_implementation.startswith(\"flash_\"):\n                lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n        return applicable_attn_implementation\n\n    def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n        applicable_attention = \"sdpa\" if requested_attention is None else requested_attention\n        if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n            message = (\n                f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n                '`attn_implementation=\"eager\"`, `\"paged|eager\"`'\n            )\n            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n            if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`, `\"attn_implementation=paged|flash_attention_2\"`'\n            if self._supports_sdpa:\n                message += ', `\"attn_implementation=sdpa\"`, `\"attn_implementation=paged|spda\"`'\n            if self._supports_flex_attn:\n                message += ', `\"attn_implementation=flex_attention\"`'\n            raise ValueError(message + \".\")\n\n        # Perform relevant checks\n        if \"flash_attention_2\" in applicable_attention:\n            self._flash_attn_2_can_dispatch(is_init_check)\n        elif \"flash_attention_3\" in applicable_attention:\n            self._flash_attn_3_can_dispatch(is_init_check)\n        elif \"flex_attention\" in applicable_attention:\n            self._flex_attn_can_dispatch(is_init_check)\n        elif \"sdpa\" in applicable_attention:\n            # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n            try:\n                self._sdpa_can_dispatch(is_init_check)\n            except (ValueError, ImportError) as e:\n                if requested_attention is not None and \"sdpa\" in requested_attention:\n                    raise e\n                applicable_attention = \"eager\"\n\n        return applicable_attention\n\n    @classmethod\n    def _can_set_attn_implementation(cls) -> bool:\n        \"\"\"Detect whether the class supports setting its attention implementation dynamically. It is an ugly check based on\n        opening the file, but avoids maintaining yet another property flag.\n        \"\"\"\n        class_file = sys.modules[cls.__module__].__file__\n        with open(class_file, \"r\") as f:\n            code = f.read()\n        # heuristic -> if we find those patterns, the model uses the correct interface\n        if re.search(r\"class \\w+Attention\\(nn.Module\\)\", code):\n            return (\n                \"eager_attention_forward\" in code\n                and \"ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\" in code\n            )\n        else:\n            # If no attention layer, assume `True`. Most probably a multimodal model or inherits from existing models\n            return True\n\n    def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n        \"\"\"\n        Set the requested `attn_implementation` for this model.\n\n        Args:\n            attn_implementation (`str` or `dict`):\n                The attention implementation to set for this model. It can be either a `str`, in which case it will be\n                dispatched to all submodels if relevant, or a `dict` where keys are the sub_configs name, in which case each\n                submodel will dispatch the corresponding value.\n        \"\"\"\n        requested_implementation = (\n            attn_implementation\n            if not isinstance(attn_implementation, dict)\n            else attn_implementation.get(\"\", self.config._attn_implementation)\n        )\n\n        if requested_implementation != self.config._attn_implementation:\n            # In this case, raise\n            if not self._can_set_attn_implementation():\n                logger.warning(\n                    f\"{self.__class__.__name__} does not support setting its attention implementation dynamically, because it \"\n                    \"does not follow the functional approach based on AttentionInterface \"\n                    \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n                )\n            else:\n                requested_implementation = self._check_and_adjust_attn_implementation(\n                    requested_implementation, is_init_check=False\n                )\n                # Apply the change (on the internal attr, to avoid setting it recursively)\n                self.config._attn_implementation_internal = requested_implementation\n\n        # Apply it to all submodels as well\n        for submodule in self.modules():\n            # We found a submodel (which is not self) with a different config (otherwise, it may be the same \"actual model\",\n            # e.g. ForCausalLM has a Model inside, but no need to check it again)\n            if (\n                submodule is not self\n                and isinstance(submodule, PreTrainedModel)\n                and submodule.config.__class__ != self.config.__class__\n                # If it was already changed, no need to do it again\n                and not hasattr(submodule.config, \"_attn_was_changed\")\n            ):\n                # In this case, warn and skip\n                if not submodule._can_set_attn_implementation():\n                    logger.warning(\n                        f\"{submodule.__class__.__name__} does not support setting its attention implementation dynamically, because it \"\n                        \"does not follow the functional approach based on AttentionInterface \"\n                        \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n                    )\n                # Set the attn on the submodule\n                else:\n                    sub_implementation = requested_implementation\n                    if isinstance(attn_implementation, dict):\n                        for subconfig_key in self.config.sub_configs:\n                            # We need to check for exact object match here, with `is`\n                            if getattr(self.config, subconfig_key) is submodule.config:\n                                sub_implementation = attn_implementation.get(\n                                    subconfig_key, submodule.config._attn_implementation\n                                )\n                                break\n                    # Check the module can use correctly, otherwise we raise an error if requested attention can't be set for submodule\n                    sub_implementation = submodule.get_correct_attn_implementation(sub_implementation)\n                    submodule.config._attn_implementation_internal = sub_implementation\n\n                # Still add it as \"changed\" even if it was skipped, as we would otherwise try to set it in the dark afterwards\n                # We need to set it on the config itself, to differentiate 2 subconfigs of the same __class__ potentially\n                submodule.config._attn_was_changed = True\n\n        # We need this as some old and badly designed models use subconfigs without declaring the corresponding modules as PreTrainedModel\n        for subconfig_key in self.config.sub_configs:\n            if (subconfig := getattr(self.config, subconfig_key)) is not None:\n                sub_implementation = (\n                    requested_implementation\n                    if not isinstance(attn_implementation, dict)\n                    else attn_implementation.get(subconfig_key, subconfig._attn_implementation)\n                )\n                # This means we did not perform any check above for this particular subconfig -> set it in the dark if it is registered\n                if (\n                    not hasattr(subconfig, \"_attn_was_changed\")\n                    # If it's already the same, then no need to enter here and raise warnings\n                    and sub_implementation != subconfig._attn_implementation\n                ):\n                    if sub_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n                        raise ValueError(\n                            f'Specified `attn_implementation=\"{sub_implementation}\"` is not supported for {subconfig_key}. '\n                            'The only possible arguments are \"eager\" (manual attention implementation)'\n                            f\"or one of the following: {list(ALL_ATTENTION_FUNCTIONS.valid_keys())}\"\n                        )\n                    subconfig._attn_implementation_internal = sub_implementation\n                    logger.warning(\n                        f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{sub_implementation}` \"\n                        \"without finding the associated sub-model. For this reason we could not check if the model supports it. \"\n                        \"You may encounter undefined behavior.\"\n                    )\n                # Unset the attribute in this case, to avoid issues in the future\n                else:\n                    if hasattr(subconfig, \"_attn_was_changed\"):\n                        del subconfig._attn_was_changed\n\n    def enable_input_require_grads(self):\n        \"\"\"\n        Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping\n        the model weights fixed.\n        \"\"\"\n\n        def make_inputs_require_grads(module, input, output):\n            output.requires_grad_(True)\n\n        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n\n    def disable_input_require_grads(self):\n        \"\"\"\n        Removes the `_require_grads_hook`.\n        \"\"\"\n        self._require_grads_hook.remove()\n\n    def get_decoder(self):\n        \"\"\"\n        Best-effort lookup of the *decoder* module.\n\n        Order of attempts (covers ~85 % of current usages):\n\n        1. `self.decoder`\n        2. `self.model`                       (many wrappers store the decoder here)\n        3. `self.model.get_decoder()`         (nested wrappers)\n        4. fallback: raise for the few exotic models that need a bespoke rule\n        \"\"\"\n        if hasattr(self, \"decoder\"):\n            return self.decoder\n\n        if hasattr(self, \"model\"):\n            inner = self.model\n            # See: https://github.com/huggingface/transformers/issues/40815\n            if hasattr(inner, \"get_decoder\") and type(inner) is not type(self):\n                return inner.get_decoder()\n            return inner\n\n        # If this is a base transformer model (no decoder/model attributes), return self\n        # This handles cases like MistralModel which is itself the decoder\n        return self\n\n    def set_decoder(self, decoder):\n        \"\"\"\n        Symmetric setter. Mirrors the lookup logic used in `get_decoder`.\n        \"\"\"\n\n        if hasattr(self, \"decoder\"):\n            self.decoder = decoder\n            return\n\n        if hasattr(self, \"model\"):\n            inner = self.model\n            if hasattr(inner, \"set_decoder\"):\n                inner.set_decoder(decoder)\n            else:\n                self.model = decoder\n            return\n\n        return\n\n    def _init_weights(self, module):\n        \"\"\"\n        Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex\n        initialization scheme, it should be overridden by the derived `PreTrainedModel` class. In case a model adds an explicit\n        `nn.Parameter`, this method should also be overridden in order to initialize it correctly.\n        \"\"\"\n        if hasattr(self.config, \"initializer_range\"):\n            std = self.config.initializer_range\n        else:\n            # 0.02 is the standard default value across the library\n            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n\n        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.MultiheadAttention):\n            # This uses torch's original init\n            module._reset_parameters()\n        # We cannot use `isinstance` on the RMSNorms or LayerNorms, as they usually are custom modules which change names\n        # between modelings (because they are prefixed with the model name)\n        elif (\n            isinstance(module, (nn.GroupNorm, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d))\n            or \"LayerNorm\" in module.__class__.__name__\n            or \"RMSNorm\" in module.__class__.__name__\n        ):\n            # Norms can exist without weights (in which case they are None from torch primitives)\n            if hasattr(module, \"weight\") and module.weight is not None:\n                module.weight.data.fill_(1.0)\n            if hasattr(module, \"bias\") and module.bias is not None:\n                module.bias.data.zero_()\n\n    def _initialize_weights(self, module):\n        \"\"\"\n        Initialize the weights if they are not already initialized.\n        \"\"\"\n        if getattr(module, \"_is_hf_initialized\", False):\n            return\n        self._init_weights(module)\n        module._is_hf_initialized = True\n\n    @torch.no_grad()\n    def initialize_weights(self):\n        \"\"\"\n        This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composite models.\n        This function dynamically dispatches the correct `init_weights` function to the modules as we advance in the\n        module graph along the recursion. It can handle an arbitrary number of sub-models. Without it, every composite\n        model would have to recurse a second time on all sub-models explicitly in the outer-most `_init_weights`, which\n        is extremely error prone and inefficient.\n\n        Note that the `torch.no_grad()` decorator is very important as well, as most of our `_init_weights` do not use\n        `torch.nn.init` functions (which are all no_grad by default), but simply do in-place ops such as\n        `module.weight.data.zero_()`.\n        \"\"\"\n        if not hasattr(torch.nn.Module, \"smart_apply\"):\n            # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function\n            # to apply as we go down the graph\n            def smart_apply(self, fn):\n                for module in self.children():\n                    # We found a sub-model: recursively dispatch its own init function now!\n                    if isinstance(module, PreTrainedModel):\n                        module.smart_apply(module._initialize_weights)\n                    else:\n                        module.smart_apply(fn)\n                fn(self)\n                return self\n\n            torch.nn.Module.smart_apply = smart_apply\n\n        # Let the magic happen with this simple call\n        self.smart_apply(self._initialize_weights)\n\n    def tie_embeddings_and_encoder_decoder(self):\n        \"\"\"\n        If set in the config, tie the weights between the input embeddings and the output embeddings,\n        and the encoder and decoder.\n        \"\"\"\n        if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n            output_embeddings = self.get_output_embeddings()\n            if output_embeddings is not None:\n                self._tie_embedding_weights(output_embeddings, self.get_input_embeddings())\n\n        if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n            if hasattr(self, self.base_model_prefix):\n                self = getattr(self, self.base_model_prefix)\n            tied_weights = self._tie_encoder_decoder_weights(\n                self.encoder, self.decoder, self.base_model_prefix, \"encoder\"\n            )\n            # Setting a dynamic variable instead of `_tied_weights_keys` because it's a class\n            # attributed not an instance member, therefore modifying it will modify the entire class\n            # Leading to issues on subsequent calls by different tests or subsequent calls.\n            self._dynamic_tied_weights_keys = tied_weights\n\n    def tie_weights(self):\n        \"\"\"\n        Recursively (for all submodels) tie all the weights of the model.\n        \"\"\"\n        # Note that `self` is included in `self.modules` so we also apply to current PreTrainedModel with this call\n        for module in self.modules():\n            # If it's a PreTrainedModel, may need to tie the embeddings and/or encoder/decoder weights\n            if isinstance(module, PreTrainedModel):\n                module.tie_embeddings_and_encoder_decoder()\n            # Additionally, if it has a custom `_tie_weights`, honor it\n            if hasattr(module, \"_tie_weights\"):\n                module._tie_weights()\n\n    @staticmethod\n    def _tie_encoder_decoder_weights(\n        encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, base_encoder_name: str\n    ):\n        uninitialized_encoder_weights: list[str] = []\n        tied_weights: list[str] = []\n        if decoder.__class__ != encoder.__class__:\n            logger.info(\n                f\"{decoder.__class__} and {encoder.__class__} are not equal. In this case make sure that all encoder\"\n                \" weights are correctly initialized.\"\n            )\n\n        def tie_encoder_to_decoder_recursively(\n            decoder_pointer: nn.Module,\n            encoder_pointer: nn.Module,\n            module_name: str,\n            base_encoder_name: str,\n            uninitialized_encoder_weights: list[str],\n            depth=0,\n            total_decoder_name=\"\",\n            total_encoder_name=\"\",\n        ):\n            assert isinstance(decoder_pointer, nn.Module) and isinstance(encoder_pointer, nn.Module), (\n                f\"{decoder_pointer} and {encoder_pointer} have to be of type nn.Module\"\n            )\n            if hasattr(decoder_pointer, \"weight\"):\n                assert hasattr(encoder_pointer, \"weight\")\n                encoder_pointer.weight = decoder_pointer.weight\n                tied_weights.append(f\"{base_encoder_name}{total_encoder_name}.weight\")\n                if hasattr(decoder_pointer, \"bias\"):\n                    assert hasattr(encoder_pointer, \"bias\")\n                    tied_weights.append(f\"{base_encoder_name}{total_encoder_name}.bias\")\n                    encoder_pointer.bias = decoder_pointer.bias\n                return\n\n            encoder_modules = encoder_pointer._modules\n            decoder_modules = decoder_pointer._modules\n            if len(decoder_modules) > 0:\n                assert len(encoder_modules) > 0, (\n                    f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n                )\n\n                all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules}\n                encoder_layer_pos = 0\n                for name in decoder_modules:\n                    if name.isdigit():\n                        encoder_name = str(int(name) + encoder_layer_pos)\n                        decoder_name = name\n                        if not isinstance(decoder_modules[decoder_name], type(encoder_modules[encoder_name])) and len(\n                            encoder_modules\n                        ) != len(decoder_modules):\n                            # this can happen if the name corresponds to the position in a list module list of layers\n                            # in this case the decoder has added a cross-attention that the encoder does not have\n                            # thus skip this step and subtract one layer pos from encoder\n                            encoder_layer_pos -= 1\n                            continue\n                    elif name not in encoder_modules:\n                        continue\n                    elif depth > 500:\n                        raise ValueError(\n                            \"Max depth of recursive function `tie_encoder_to_decoder` reached. It seems that there is\"\n                            \" a circular dependency between two or more `nn.Modules` of your model.\"\n                        )\n                    else:\n                        decoder_name = encoder_name = name\n                    tie_encoder_to_decoder_recursively(\n                        decoder_modules[decoder_name],\n                        encoder_modules[encoder_name],\n                        module_name + \"/\" + name,\n                        base_encoder_name,\n                        uninitialized_encoder_weights,\n                        depth=depth + 1,\n                        total_encoder_name=f\"{total_encoder_name}.{encoder_name}\",\n                        total_decoder_name=f\"{total_decoder_name}.{decoder_name}\",\n                    )\n                    all_encoder_weights.remove(module_name + \"/\" + encoder_name)\n\n                uninitialized_encoder_weights += list(all_encoder_weights)\n\n        # tie weights recursively\n        tie_encoder_to_decoder_recursively(\n            decoder, encoder, base_model_prefix, base_encoder_name, uninitialized_encoder_weights\n        )\n\n        if len(uninitialized_encoder_weights) > 0:\n            logger.warning(\n                f\"The following encoder weights were not tied to the decoder {uninitialized_encoder_weights}\"\n            )\n        return tied_weights\n\n    def _tie_embedding_weights(self, output_embeddings, input_embeddings):\n        \"\"\"Tie weights, and add hooks and flags if using TP.\"\"\"\n        output_embeddings.weight = input_embeddings.weight\n\n        # Passing hooks over to the embeddings if needed\n        # (currently limited to tensor parallel hooks and flags only)\n        if hasattr(input_embeddings, \"_is_hooked\") and getattr(input_embeddings, \"_hf_tp_plan\", None):\n            output_embeddings._is_hooked = input_embeddings._is_hooked\n            output_embeddings._hf_tp_plan = input_embeddings._hf_tp_plan\n            output_embeddings._forward_hooks = input_embeddings._forward_hooks\n            output_embeddings._forward_pre_hooks = input_embeddings._forward_pre_hooks\n            output_embeddings.__repr__ = (\n                lambda: f\"{output_embeddings.__repr__()}\\nTP Plan: {output_embeddings._hf_tp_plan}\"\n            )\n\n        if getattr(output_embeddings, \"bias\", None) is not None:\n            output_embeddings.bias.data = nn.functional.pad(\n                output_embeddings.bias.data,\n                (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0]),\n                \"constant\",\n                0,\n            )\n        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n            output_embeddings.out_features = input_embeddings.num_embeddings\n\n    def _get_no_split_modules(self, device_map: str):\n        \"\"\"\n        Get the modules of the model that should not be spit when using device_map. We iterate through the modules to\n        get the underlying `_no_split_modules`.\n\n        Args:\n            device_map (`str`):\n                The device map value. Options are [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]\n\n        Returns:\n            `list[str]`: List of modules that should not be split\n        \"\"\"\n        _no_split_modules = set()\n        modules_to_check = [self]\n        while len(modules_to_check) > 0:\n            module = modules_to_check.pop(-1)\n            # if the module does not appear in _no_split_modules, we also check the children\n            if module.__class__.__name__ not in _no_split_modules:\n                if isinstance(module, PreTrainedModel):\n                    if module._no_split_modules is None:\n                        raise ValueError(\n                            f\"{module.__class__.__name__} does not support `device_map='{device_map}'`. To implement support, the model \"\n                            \"class needs to implement the `_no_split_modules` attribute.\"\n                        )\n                    else:\n                        _no_split_modules = _no_split_modules | set(module._no_split_modules)\n                modules_to_check += list(module.children())\n        return list(_no_split_modules)\n\n    def resize_token_embeddings(\n        self,\n        new_num_tokens: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n        mean_resizing: bool = True,\n    ) -> nn.Embedding:\n        \"\"\"\n        Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\n        Takes care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Arguments:\n            new_num_tokens (`int`, *optional*):\n                The new number of tokens in the embedding matrix. Increasing the size will add newly initialized\n                vectors at the end. Reducing the size will remove vectors from the end. If not provided or `None`, just\n                returns a pointer to the input tokens `torch.nn.Embedding` module of the model without doing anything.\n            pad_to_multiple_of (`int`, *optional*):\n                If set will pad the embedding matrix to a multiple of the provided value.If `new_num_tokens` is set to\n                `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.\n\n                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                details about this, or help on choosing the correct value for resizing, refer to this guide:\n                https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n            mean_resizing (`bool`):\n                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n\n                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n                where the generated tokens' probabilities won't be affected by the added embeddings because initializing the new embeddings with the\n                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\n        Return:\n            `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\n        \"\"\"\n        model_embeds = self._resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n        if new_num_tokens is None and pad_to_multiple_of is None:\n            return model_embeds\n\n        # Since we are basically reusing the same old embeddings with new weight values, gathering is required\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(model_embeds.weight, modifier_rank=None):\n                vocab_size = model_embeds.weight.shape[0]\n        else:\n            vocab_size = model_embeds.weight.shape[0]\n\n        # Update base model and current model config.\n        self.config.get_text_config().vocab_size = vocab_size\n        self.vocab_size = vocab_size\n\n        # Tie weights again if needed\n        self.tie_weights()\n\n        return model_embeds\n\n    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean_resizing=True):\n        old_embeddings = self.get_input_embeddings()\n        new_embeddings = self._get_resized_embeddings(\n            old_embeddings, new_num_tokens, pad_to_multiple_of, mean_resizing\n        )\n        if hasattr(old_embeddings, \"_hf_hook\"):\n            hook = old_embeddings._hf_hook\n            add_hook_to_module(new_embeddings, hook)\n        old_embeddings_requires_grad = old_embeddings.weight.requires_grad\n        new_embeddings.requires_grad_(old_embeddings_requires_grad)\n        self.set_input_embeddings(new_embeddings)\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n\n        # Update new_num_tokens with the actual size of new_embeddings\n        if pad_to_multiple_of is not None:\n            if is_deepspeed_zero3_enabled() and not is_quantized:\n                import deepspeed\n\n                with deepspeed.zero.GatheredParameters(new_embeddings.weight, modifier_rank=None):\n                    new_num_tokens = new_embeddings.weight.shape[0]\n            else:\n                new_num_tokens = new_embeddings.weight.shape[0]\n\n        # if word embeddings are not tied, make sure that lm head is resized as well\n        if (\n            self.get_output_embeddings() is not None\n            and not self.config.get_text_config(decoder=True).tie_word_embeddings\n        ):\n            old_lm_head = self.get_output_embeddings()\n            if isinstance(old_lm_head, torch.nn.Embedding):\n                new_lm_head = self._get_resized_embeddings(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n            else:\n                new_lm_head = self._get_resized_lm_head(old_lm_head, new_num_tokens, mean_resizing=mean_resizing)\n            if hasattr(old_lm_head, \"_hf_hook\"):\n                hook = old_lm_head._hf_hook\n                add_hook_to_module(new_lm_head, hook)\n            old_lm_head_requires_grad = old_lm_head.weight.requires_grad\n            new_lm_head.requires_grad_(old_lm_head_requires_grad)\n            self.set_output_embeddings(new_lm_head)\n\n        return self.get_input_embeddings()\n\n    def _get_resized_embeddings(\n        self,\n        old_embeddings: nn.Embedding,\n        new_num_tokens: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n        mean_resizing: bool = True,\n    ) -> nn.Embedding:\n        \"\"\"\n        Build a resized Embedding Module from a provided token Embedding Module. Increasing the size will add newly\n        initialized vectors at the end. Reducing the size will remove vectors from the end\n\n        Args:\n            old_embeddings (`torch.nn.Embedding`):\n                Old embeddings to be resized.\n            new_num_tokens (`int`, *optional*):\n                New number of tokens in the embedding matrix.\n\n                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n                vectors from the end. If not provided or `None`, just returns a pointer to the input tokens\n                `torch.nn.Embedding` module of the model without doing anything.\n            pad_to_multiple_of (`int`, *optional*):\n                If set will pad the embedding matrix to a multiple of the provided value. If `new_num_tokens` is set to\n                `None` will just pad the embedding to a multiple of `pad_to_multiple_of`.\n\n                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128. For more\n                details about this, or help on choosing the correct value for resizing, refer to this guide:\n                https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n            mean_resizing (`bool`):\n                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n\n                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n                where the generated tokens' probabilities will not be affected by the added embeddings because initializing the new embeddings with the\n                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\n\n        Return:\n            `torch.nn.Embedding`: Pointer to the resized Embedding Module or the old Embedding Module if\n            `new_num_tokens` is `None`\n        \"\"\"\n\n        if pad_to_multiple_of is not None:\n            if not isinstance(pad_to_multiple_of, int):\n                raise ValueError(\n                    f\"Asking to pad the embedding matrix to a multiple of `{pad_to_multiple_of}`, which is not and integer. Please make sure to pass an integer\"\n                )\n            if new_num_tokens is None:\n                new_num_tokens = old_embeddings.weight.shape[0]\n            new_num_tokens = ((new_num_tokens + pad_to_multiple_of - 1) // pad_to_multiple_of) * pad_to_multiple_of\n        else:\n            logger.info(\n                \"You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding\"\n                f\" dimension will be {new_num_tokens}. This might induce some performance reduction as *Tensor Cores* will not be available.\"\n                \" For more details about this, or help on choosing the correct value for resizing, refer to this guide:\"\n                \" https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\"\n            )\n\n        if new_num_tokens is None:\n            return old_embeddings\n\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(old_embeddings.weight, modifier_rank=None):\n                old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n        else:\n            old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n\n        if old_num_tokens == new_num_tokens and not is_deepspeed_zero3_enabled():\n            return old_embeddings\n\n        if not isinstance(old_embeddings, nn.Embedding):\n            raise TypeError(\n                f\"Old embeddings are of type {type(old_embeddings)}, which is not an instance of {nn.Embedding}. You\"\n                \" should either use a different resize function or make sure that `old_embeddings` are an instance of\"\n                f\" {nn.Embedding}.\"\n            )\n\n        # Build new embeddings\n\n        # When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init\n        # because the shape of the new embedding layer is used across various modeling files\n        # as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading\n        # to errors when training.\n        new_embeddings = nn.Embedding(\n            new_num_tokens,\n            old_embedding_dim,\n            device=old_embeddings.weight.device,\n            dtype=old_embeddings.weight.dtype,\n        )\n\n        if new_num_tokens > old_num_tokens and not mean_resizing:\n            # initialize new embeddings (in particular added tokens) with a mean of 0 and std equals `config.initializer_range`.\n            self._init_weights(new_embeddings)\n\n        elif new_num_tokens > old_num_tokens and mean_resizing:\n            # initialize new embeddings  (in particular added tokens). The new embeddings will be initialized\n            # from a multivariate normal distribution that has old embeddings' mean and covariance.\n            # as described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n            logger.warning_once(\n                \"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. \"\n                \"As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. \"\n                \"To disable this, use `mean_resizing=False`\"\n            )\n\n            added_num_tokens = new_num_tokens - old_num_tokens\n            if is_deepspeed_zero3_enabled() and not is_quantized:\n                import deepspeed\n\n                with deepspeed.zero.GatheredParameters([old_embeddings.weight], modifier_rank=None):\n                    self._init_added_embeddings_weights_with_mean(\n                        old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n                    )\n            else:\n                self._init_added_embeddings_weights_with_mean(\n                    old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n                )\n\n        # Copy token embeddings from the previous weights\n\n        # numbers of tokens to copy\n        n = min(old_num_tokens, new_num_tokens)\n\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            params = [old_embeddings.weight, new_embeddings.weight]\n            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n        else:\n            new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n\n        # Replace weights in old_embeddings and return to maintain the same embedding type.\n        # This ensures correct functionality when a Custom Embedding class is passed as input.\n        # The input and output embedding types remain consistent. (c.f. https://github.com/huggingface/transformers/pull/31979)\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            params = [old_embeddings.weight, new_embeddings.weight]\n            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                old_embeddings.weight = new_embeddings.weight\n                old_embeddings.num_embeddings = new_embeddings.weight.data.shape[0]\n\n                # If the new number of tokens is smaller than the original `padding_idx`, the `padding_idx`\n                # will be set to `None` in the resized embeddings.\n                if old_embeddings.padding_idx is not None and (new_num_tokens - 1) < old_embeddings.padding_idx:\n                    old_embeddings.padding_idx = None\n        else:\n            old_embeddings.weight.data = new_embeddings.weight.data\n            old_embeddings.num_embeddings = new_embeddings.weight.data.shape[0]\n            if old_embeddings.padding_idx is not None and (new_num_tokens - 1) < old_embeddings.padding_idx:\n                old_embeddings.padding_idx = None\n\n        return old_embeddings\n\n    def _get_resized_lm_head(\n        self,\n        old_lm_head: nn.Linear,\n        new_num_tokens: Optional[int] = None,\n        transposed: bool = False,\n        mean_resizing: bool = True,\n    ) -> nn.Linear:\n        \"\"\"\n        Build a resized Linear Module from a provided old Linear Module. Increasing the size will add newly initialized\n        vectors at the end. Reducing the size will remove vectors from the end\n\n        Args:\n            old_lm_head (`torch.nn.Linear`):\n                Old lm head liner layer to be resized.\n            new_num_tokens (`int`, *optional*):\n                New number of tokens in the linear matrix.\n\n                Increasing the size will add newly initialized vectors at the end. Reducing the size will remove\n                vectors from the end. If not provided or `None`, just returns a pointer to the input tokens\n                `torch.nn.Linear` module of the model without doing anything. transposed (`bool`, *optional*, defaults\n                to `False`): Whether `old_lm_head` is transposed or not. If True `old_lm_head.size()` is `lm_head_dim,\n                vocab_size` else `vocab_size, lm_head_dim`.\n            mean_resizing (`bool`):\n                Whether to initialize the added embeddings from a multivariate normal distribution that has old embeddings' mean and\n                covariance or to initialize them with a normal distribution that has a mean of zero and std equals `config.initializer_range`.\n\n                Setting `mean_resizing` to `True` is useful when increasing the size of the embeddings of causal language models,\n                where the generated tokens' probabilities will not be affected by the added embeddings because initializing the new embeddings with the\n                old embeddings' mean will reduce the kl-divergence between the next token probability before and after adding the new embeddings.\n                Refer to this article for more information: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n\n        Return:\n            `torch.nn.Linear`: Pointer to the resized Linear Module or the old Linear Module if `new_num_tokens` is\n            `None`\n        \"\"\"\n\n        if new_num_tokens is None:\n            return old_lm_head\n\n        is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(old_lm_head.weight, modifier_rank=None):\n                old_num_tokens, old_lm_head_dim = (\n                    old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n                )\n        else:\n            old_num_tokens, old_lm_head_dim = (\n                old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n            )\n\n        if old_num_tokens == new_num_tokens and not is_deepspeed_zero3_enabled():\n            return old_lm_head\n\n        if not isinstance(old_lm_head, nn.Linear):\n            raise TypeError(\n                f\"Old language model head is of type {type(old_lm_head)}, which is not an instance of {nn.Linear}. You\"\n                \" should either use a different resize function or make sure that `old_lm_head` are an instance of\"\n                f\" {nn.Linear}.\"\n            )\n\n        # Build new lm head\n        new_lm_head_shape = (old_lm_head_dim, new_num_tokens) if not transposed else (new_num_tokens, old_lm_head_dim)\n        has_new_lm_head_bias = old_lm_head.bias is not None\n\n        # When using DeepSpeed ZeRO-3, we shouldn't create new embeddings with DeepSpeed init\n        # because the shape of the new embedding layer is used across various modeling files\n        # as well as to update config vocab size. Shape will be 0 when using DeepSpeed init leading\n        # to errors when training.\n        new_lm_head = nn.Linear(\n            *new_lm_head_shape,\n            bias=has_new_lm_head_bias,\n            device=old_lm_head.weight.device,\n            dtype=old_lm_head.weight.dtype,\n        )\n\n        if new_num_tokens > old_num_tokens and not mean_resizing:\n            # initialize new embeddings (in particular added tokens) with a mean of 0 and std equals `config.initializer_range`.\n            self._init_weights(new_lm_head)\n\n        elif new_num_tokens > old_num_tokens and mean_resizing:\n            # initialize new lm_head weights (in particular added tokens). The new lm_head weights\n            # will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance.\n            # as described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n            logger.warning_once(\n                \"The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. \"\n                \"As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. \"\n                \"To disable this, use `mean_resizing=False`\"\n            )\n\n            added_num_tokens = new_num_tokens - old_num_tokens\n            if is_deepspeed_zero3_enabled() and not is_quantized:\n                import deepspeed\n\n                params = [old_lm_head.weight]\n                if has_new_lm_head_bias:\n                    params += [old_lm_head.bias]\n                with deepspeed.zero.GatheredParameters(params, modifier_rank=None):\n                    self._init_added_lm_head_weights_with_mean(\n                        old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed\n                    )\n                    if has_new_lm_head_bias:\n                        self._init_added_lm_head_bias_with_mean(old_lm_head, new_lm_head, added_num_tokens)\n\n            else:\n                self._init_added_lm_head_weights_with_mean(\n                    old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed\n                )\n                if has_new_lm_head_bias:\n                    self._init_added_lm_head_bias_with_mean(old_lm_head, new_lm_head, added_num_tokens)\n\n        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            params = [old_lm_head.weight, old_lm_head.bias, new_lm_head.weight, new_lm_head.bias]\n            with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                self._copy_lm_head_original_to_resized(\n                    new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n                )\n        else:\n            self._copy_lm_head_original_to_resized(\n                new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n            )\n\n        return new_lm_head\n\n    def _init_added_embeddings_weights_with_mean(\n        self, old_embeddings, new_embeddings, old_num_tokens, added_num_tokens\n    ):\n        old_embeddings_weight = old_embeddings.weight.data.to(torch.float32)\n        mean_embeddings = torch.mean(old_embeddings_weight, axis=0)\n        old_centered_embeddings = old_embeddings_weight - mean_embeddings\n        covariance = old_centered_embeddings.T @ old_centered_embeddings / old_num_tokens\n\n        # Check if the covariance is positive definite.\n        epsilon = 1e-9\n        is_covariance_psd = constraints.positive_definite.check(epsilon * covariance).all()\n        if is_covariance_psd:\n            # If covariances is positive definite, a distribution can be created. and we can sample new weights from it.\n            distribution = torch.distributions.multivariate_normal.MultivariateNormal(\n                mean_embeddings, covariance_matrix=epsilon * covariance\n            )\n            new_embeddings.weight.data[-1 * added_num_tokens :, :] = distribution.sample(\n                sample_shape=(added_num_tokens,)\n            ).to(old_embeddings.weight.dtype)\n        else:\n            # Otherwise, just initialize with the mean. because distribution will not be created.\n            new_embeddings.weight.data[-1 * added_num_tokens :, :] = (\n                mean_embeddings[None, :].repeat(added_num_tokens, 1).to(old_embeddings.weight.dtype)\n            )\n\n    def _init_added_lm_head_weights_with_mean(\n        self,\n        old_lm_head,\n        new_lm_head,\n        old_lm_head_dim,\n        old_num_tokens,\n        added_num_tokens,\n        transposed: bool = False,\n    ):\n        if transposed:\n            # Transpose to the desired shape for the function.\n            new_lm_head.weight.data = new_lm_head.weight.data.T\n            old_lm_head.weight.data = old_lm_head.weight.data.T\n\n        # The same initialization logic as Embeddings.\n        self._init_added_embeddings_weights_with_mean(old_lm_head, new_lm_head, old_num_tokens, added_num_tokens)\n\n        if transposed:\n            # Transpose again to the correct shape.\n            new_lm_head.weight.data = new_lm_head.weight.data.T\n            old_lm_head.weight.data = old_lm_head.weight.data.T\n\n    def _init_added_lm_head_bias_with_mean(self, old_lm_head, new_lm_head, added_num_tokens):\n        bias_mean = torch.mean(old_lm_head.bias.data, axis=0, dtype=torch.float32)\n        bias_std = torch.std(old_lm_head.bias.data, axis=0).to(torch.float32)\n        new_lm_head.bias.data[-1 * added_num_tokens :].normal_(mean=bias_mean, std=1e-9 * bias_std)\n\n    def _copy_lm_head_original_to_resized(\n        self, new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias\n    ):\n        # Copy old lm head weights to new lm head\n        if not transposed:\n            new_lm_head.weight.data[:num_tokens_to_copy, :] = old_lm_head.weight.data[:num_tokens_to_copy, :]\n        else:\n            new_lm_head.weight.data[:, :num_tokens_to_copy] = old_lm_head.weight.data[:, :num_tokens_to_copy]\n\n        # Copy bias weights to new lm head\n        if has_new_lm_head_bias:\n            new_lm_head.bias.data[:num_tokens_to_copy] = old_lm_head.bias.data[:num_tokens_to_copy]\n\n    def resize_position_embeddings(self, new_num_position_embeddings: int):\n        raise NotImplementedError(\n            f\"`resize_position_embeddings` is not implemented for {self.__class__}`. To implement it, you should \"\n            f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n        )\n\n    def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]:\n        raise NotImplementedError(\n            f\"`get_position_embeddings` is not implemented for {self.__class__}`. To implement it, you should \"\n            f\"overwrite this method in the class {self.__class__} in `modeling_{self.__class__.__module__}.py`\"\n        )\n\n    def init_weights(self):\n        \"\"\"\n        Maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n        initialization logic in `_init_weights`.\n        \"\"\"\n        if _init_weights:\n            # Initialize weights\n            self.initialize_weights()\n\n            # Tie weights should be skipped when not initializing all weights\n            # since from_pretrained(...) calls tie weights anyways\n            self.tie_weights()\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        \"\"\"\n        Activates gradient checkpointing for the current model.\n\n        We pass the `__call__` method of the modules instead of `forward` because `__call__` attaches all the hooks of\n        the module. https://discuss.pytorch.org/t/any-different-between-model-input-and-model-forward-input/3690/2\n\n        Args:\n            gradient_checkpointing_kwargs (dict, *optional*):\n                Additional keyword arguments passed along to the `torch.utils.checkpoint.checkpoint` function.\n        \"\"\"\n        if not self.supports_gradient_checkpointing:\n            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n\n        if gradient_checkpointing_kwargs is None:\n            gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n\n        gradient_checkpointing_func = functools.partial(checkpoint, **gradient_checkpointing_kwargs)\n\n        # For old GC format (transformers < 4.35.0) for models that live on the Hub\n        # we will fall back to the overwritten `_set_gradient_checkpointing` method\n        _is_using_old_format = \"value\" in inspect.signature(self._set_gradient_checkpointing).parameters\n\n        if not _is_using_old_format:\n            self._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=gradient_checkpointing_func)\n        else:\n            self.apply(partial(self._set_gradient_checkpointing, value=True))\n            logger.warning(\n                \"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).\"\n                \"Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\"\n            )\n\n        if getattr(self, \"_hf_peft_config_loaded\", False):\n            # When using PEFT + gradient checkpointing + Trainer we need to make sure the input has requires_grad=True\n            # we do it also on PEFT: https://github.com/huggingface/peft/blob/85013987aa82aa1af3da1236b6902556ce3e483e/src/peft/peft_model.py#L334\n            # When training with PEFT, only LoRA layers will have requires grad set to True, but the output of frozen layers need to propagate\n            # the gradients to make sure the gradient flows.\n            self.enable_input_require_grads()\n\n    def _set_gradient_checkpointing(self, enable: bool = True, gradient_checkpointing_func: Callable = checkpoint):\n        is_gradient_checkpointing_set = False\n\n        # Apply it on the top-level module in case the top-level modules supports it\n        # for example, LongT5Stack inherits from `PreTrainedModel`.\n        if hasattr(self, \"gradient_checkpointing\"):\n            self._gradient_checkpointing_func = gradient_checkpointing_func\n            self.gradient_checkpointing = enable\n            is_gradient_checkpointing_set = True\n\n        for module in self.modules():\n            if hasattr(module, \"gradient_checkpointing\"):\n                module._gradient_checkpointing_func = gradient_checkpointing_func\n                module.gradient_checkpointing = enable\n                is_gradient_checkpointing_set = True\n\n        if not is_gradient_checkpointing_set:\n            raise ValueError(\n                f\"{self.__class__.__name__} is not compatible with gradient checkpointing. Make sure all the architecture support it by setting a boolean attribute\"\n                \" `gradient_checkpointing` to modules of the model that uses checkpointing.\"\n            )\n\n    def gradient_checkpointing_disable(self):\n        \"\"\"\n        Deactivates gradient checkpointing for the current model.\n        \"\"\"\n        if self.supports_gradient_checkpointing:\n            # For old GC format (transformers < 4.35.0) for models that live on the Hub\n            # we will fall back to the overwritten `_set_gradient_checkpointing` method\n            _is_using_old_format = \"value\" in inspect.signature(self._set_gradient_checkpointing).parameters\n            if not _is_using_old_format:\n                self._set_gradient_checkpointing(enable=False)\n            else:\n                logger.warning(\n                    \"You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).\"\n                    \"Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\"\n                )\n                self.apply(partial(self._set_gradient_checkpointing, value=False))\n\n        if getattr(self, \"_hf_peft_config_loaded\", False):\n            self.disable_input_require_grads()\n\n    @property\n    def is_gradient_checkpointing(self) -> bool:\n        \"\"\"\n        Whether gradient checkpointing is activated for this model or not.\n        \"\"\"\n        return any(hasattr(m, \"gradient_checkpointing\") and m.gradient_checkpointing for m in self.modules())\n\n    def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        is_main_process: bool = True,\n        state_dict: Optional[dict] = None,\n        save_function: Callable = torch.save,\n        push_to_hub: bool = False,\n        max_shard_size: Union[int, str] = \"5GB\",\n        safe_serialization: bool = True,\n        variant: Optional[str] = None,\n        token: Optional[Union[str, bool]] = None,\n        save_peft_format: bool = True,\n        **kwargs,\n    ):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        [`~PreTrainedModel.from_pretrained`] class method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            is_main_process (`bool`, *optional*, defaults to `True`):\n                Whether the process calling this is the main process or not. Useful when in distributed training like\n                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n                the main process to avoid race conditions.\n            state_dict (nested dictionary of `torch.Tensor`):\n                The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only\n                save parts of the model or if special precautions need to be taken when recovering the state dictionary\n                of a model (like when using model parallelism).\n            save_function (`Callable`):\n                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n                need to replace `torch.save` by another method.\n            push_to_hub (`bool`, *optional*, defaults to `False`):\n                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                namespace).\n            max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n                The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n                lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n                We default it to 5GB in order for models to be able to run easily on free-tier google colab instances\n                without CPU OOM issues.\n\n                <Tip warning={true}>\n\n                If a single weight of the model is bigger than `max_shard_size`, it will be in its own checkpoint shard\n                which will be bigger than `max_shard_size`.\n\n                </Tip>\n\n            safe_serialization (`bool`, *optional*, defaults to `True`):\n                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n            variant (`str`, *optional*):\n                If specified, weights are saved in the format pytorch_model.<variant>.bin.\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n            save_peft_format (`bool`, *optional*, defaults to `True`):\n                For backward compatibility with PEFT library, in case adapter weights are attached to the model, all\n                keys of the state dict of adapters needs to be prepended with `base_model.model`. Advanced users can\n                disable this behaviours by setting `save_peft_format` to `False`.\n            kwargs (`dict[str, Any]`, *optional*):\n                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n        \"\"\"\n        ignore_metadata_errors = kwargs.pop(\"ignore_metadata_errors\", False)\n\n        if token is not None:\n            kwargs[\"token\"] = token\n\n        _hf_peft_config_loaded = getattr(self, \"_hf_peft_config_loaded\", False)\n\n        hf_quantizer = getattr(self, \"hf_quantizer\", None)\n        quantization_serializable = (\n            hf_quantizer is not None\n            and isinstance(hf_quantizer, HfQuantizer)\n            and hf_quantizer.is_serializable(safe_serialization=safe_serialization)\n        )\n\n        if hf_quantizer is not None and not _hf_peft_config_loaded and not quantization_serializable:\n            raise ValueError(\n                f\"The model is quantized with {hf_quantizer.quantization_config.quant_method} and is not serializable - check out the warnings from\"\n                \" the logger on the traceback to understand the reason why the quantized model is not serializable.\"\n            )\n\n        if \"save_config\" in kwargs:\n            warnings.warn(\n                \"`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\"\n            )\n            is_main_process = kwargs.pop(\"save_config\")\n\n        # we need to check against tp_size, not tp_plan, as tp_plan is substituted to the class one\n        if self._tp_size is not None and not is_huggingface_hub_greater_or_equal(\"0.31.4\"):\n            raise ImportError(\n                \"Saving a model with tensor parallelism requires `huggingface_hub` version 0.31.4 or higher.\"\n            )\n\n        if os.path.isfile(save_directory):\n            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n            return\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        if push_to_hub:\n            commit_message = kwargs.pop(\"commit_message\", None)\n            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n            create_pr = kwargs.pop(\"create_pr\", False)\n            repo_id = self._create_repo(repo_id, **kwargs)\n            files_timestamps = self._get_files_timestamps(save_directory)\n\n        metadata = {}\n        if hf_quantizer is not None:\n            state_dict, metadata = hf_quantizer.get_state_dict_and_metadata(self, safe_serialization)\n        metadata[\"format\"] = \"pt\"\n\n        # Only save the model itself if we are using distributed training\n        model_to_save = unwrap_model(self)\n        # save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\n        # we currently don't use this setting automatically, but may start to use with v5\n        dtype = get_parameter_dtype(model_to_save)\n        model_to_save.config.dtype = str(dtype).split(\".\")[1]\n\n        # Attach architecture to the config\n        # When using FSDP2, unwrapping is a noop, so the model name doesn't change back to the original model name\n        model_to_save.config.architectures = [model_to_save.__class__.__name__.removeprefix(\"FSDP\")]\n\n        # If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be\n        # loaded from the Hub.\n        if self._auto_class is not None:\n            custom_object_save(self, save_directory, config=self.config)\n\n        # Save the config\n        if is_main_process:\n            if not _hf_peft_config_loaded:\n                # If the model config has set attributes that should be in the generation config, move them there.\n                misplaced_generation_parameters = model_to_save.config._get_non_default_generation_parameters()\n                if self.can_generate() and len(misplaced_generation_parameters) > 0:\n                    warnings.warn(\n                        \"Moving the following attributes in the config to the generation config: \"\n                        f\"{misplaced_generation_parameters}. You are seeing this warning because you've set \"\n                        \"generation parameters in the model config, as opposed to in the generation config.\",\n                        UserWarning,\n                    )\n                    for param_name, param_value in misplaced_generation_parameters.items():\n                        setattr(model_to_save.generation_config, param_name, param_value)\n                        setattr(model_to_save.config, param_name, None)\n\n                model_to_save.config.save_pretrained(save_directory)\n            if self.can_generate():\n                model_to_save.generation_config.save_pretrained(save_directory)\n\n            if _hf_peft_config_loaded:\n                logger.info(\n                    \"Detected adapters on the model, saving the model in the PEFT format, only adapter weights will be saved.\"\n                )\n                state_dict = model_to_save.get_adapter_state_dict(state_dict=state_dict)\n\n                if save_peft_format:\n                    logger.info(\n                        \"To match the expected format of the PEFT library, all keys of the state dict of adapters will be prepended with `base_model.model`.\"\n                    )\n                    peft_state_dict = {}\n                    for key, value in state_dict.items():\n                        peft_state_dict[f\"base_model.model.{key}\"] = value\n                    state_dict = peft_state_dict\n\n                active_adapter = self.active_adapters()\n\n                if len(active_adapter) > 1:\n                    raise ValueError(\n                        \"Multiple active adapters detected, saving multiple active adapters is not supported yet. You can save adapters separately one by one \"\n                        \"by iteratively calling `model.set_adapter(adapter_name)` then `model.save_pretrained(...)`\"\n                    )\n                active_adapter = active_adapter[0]\n\n                current_peft_config = self.peft_config[active_adapter]\n                current_peft_config.save_pretrained(save_directory)\n\n        # for offloaded modules\n        module_map = {}\n\n        # Save the model\n        if state_dict is None:\n            # if any model parameters are offloaded, make module map\n            if (\n                hasattr(self, \"hf_device_map\")\n                and len(set(self.hf_device_map.values())) > 1\n                and (\"cpu\" in self.hf_device_map.values() or \"disk\" in self.hf_device_map.values())\n            ):\n                warnings.warn(\n                    \"Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\"\n                )\n                for name, module in model_to_save.named_modules():\n                    if name == \"\":\n                        continue\n                    module_state_dict = module.state_dict()\n\n                    for key in module_state_dict:\n                        module_map[name + f\".{key}\"] = module\n            state_dict = model_to_save.state_dict()\n\n        if any(\n            allowed_name in class_name.__name__.lower()\n            for class_name in self.__class__.__mro__[:-1]\n            for allowed_name in VLMS\n        ):\n            reverse_key_mapping = {v: k for k, v in self._checkpoint_conversion_mapping.items()}\n\n            original_state_dict = {}\n            for key, value in state_dict.items():\n                for pattern, replacement in reverse_key_mapping.items():\n                    replacement = replacement.lstrip(\"^\")  # strip off un-needed chars and patterns\n                    replacement = re.sub(r\"\\(.*\\)\", \"\", replacement)\n                    key, n_replace = re.subn(pattern, replacement, key)\n                    # Early exit of the loop\n                    if n_replace > 0:\n                        break\n                original_state_dict[key] = value\n            state_dict = original_state_dict\n\n        # Translate state_dict from smp to hf if saving with smp >= 1.10\n        if IS_SAGEMAKER_MP_POST_1_10:\n            for smp_to_hf, _ in smp.state.module_manager.translate_functions:\n                state_dict = smp_to_hf(state_dict)\n\n        # Handle the case where some state_dict keys shouldn't be saved\n        if self._keys_to_ignore_on_save is not None:\n            for ignore_key in self._keys_to_ignore_on_save:\n                if ignore_key in state_dict:\n                    del state_dict[ignore_key]\n\n        # Rename state_dict keys before saving to file. Do nothing unless overridden in a particular model.\n        # (initially introduced with TimmWrapperModel to remove prefix and make checkpoints compatible with timm)\n        state_dict = self._fix_state_dict_keys_on_save(state_dict)\n        # If model was sharded, we cannot properly determine sizes of tensors that `local_*` strategy was used,\n        # therefore we replace them with DTensors that are equivalently sharded\n        if self._tp_size is not None:\n            state_dict = replace_state_dict_local_with_dtensor(state_dict, self._tp_plan, self._device_mesh)\n\n        if safe_serialization:\n            # TODO: fix safe_serialization for tied weights\n            # Safetensors does not allow tensor aliasing.\n            # We're going to remove aliases before saving\n            ptrs = collections.defaultdict(list)\n            for name, tensor in state_dict.items():\n                if not isinstance(tensor, torch.Tensor):\n                    # Sometimes in the state_dict we have non-tensor objects.\n                    # e.g. in bitsandbytes we have some `str` objects in the state_dict\n                    # In the non-tensor case, fall back to the pointer of the object itself\n                    ptrs[id(tensor)].append(name)\n\n                elif tensor.device.type == \"meta\":\n                    # In offloaded cases, there may be meta tensors in the state_dict.\n                    # For these cases, key by the pointer of the original tensor object\n                    # (state_dict tensors are detached and therefore no longer shared)\n                    tensor = self.get_parameter(name)\n                    ptrs[id(tensor)].append(name)\n\n                else:\n                    ptrs[id_tensor_storage(tensor)].append(name)\n\n            shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n\n            # Recursively descend to find tied weight keys\n            _tied_weights_keys = _get_tied_weight_keys(self)\n            error_names = []\n            to_delete_names = set()\n            for names in shared_ptrs.values():\n                # Removing the keys which are declared as known duplicates on\n                # load. This allows to make sure the name which is kept is consistent.\n                if _tied_weights_keys is not None:\n                    found = 0\n                    for name in sorted(names):\n                        matches_pattern = any(re.search(pat, name) for pat in _tied_weights_keys)\n                        if matches_pattern and name in state_dict:\n                            found += 1\n                            if found < len(names):\n                                to_delete_names.add(name)\n            # We are entering a place where the weights and the transformers configuration do NOT match.\n            shared_names, disjoint_names = _find_disjoint(shared_ptrs.values(), state_dict)\n            # Those are actually tensor sharing but disjoint from each other, we can safely clone them\n            # Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\n            for name in disjoint_names:\n                state_dict[name] = state_dict[name].clone()\n\n            # When not all duplicates have been cleaned, still remove those keys, but put a clear warning.\n            # If the link between tensors was done at runtime then `from_pretrained` will not get\n            # the key back leading to random tensor. A proper warning will be shown\n            # during reload (if applicable), but since the file is not necessarily compatible with\n            # the config, better show a proper warning.\n            shared_names, identical_names = _find_identical(shared_names, state_dict)\n            # delete tensors that have identical storage\n            for inames in identical_names:\n                known = inames.intersection(to_delete_names)\n                for name in known:\n                    del state_dict[name]\n                unknown = inames.difference(to_delete_names)\n                if len(unknown) > 1:\n                    error_names.append(unknown)\n\n            if shared_names:\n                error_names.extend(shared_names)\n\n            if len(error_names) > 0:\n                raise RuntimeError(\n                    f\"The weights trying to be saved contained shared tensors {error_names} that are mismatching \"\n                    \"the transformers base configuration. Try saving using `safe_serialization=False`, setting the \"\n                    \"`_dynamic_tied_weights_keys` attribute for affected modules, or remove this tensor sharing.\",\n                )\n\n        # Shard the model if it is too big.\n        if not _hf_peft_config_loaded:\n            weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME\n            weights_name = _add_variant(weights_name, variant)\n        else:\n            weights_name = ADAPTER_SAFE_WEIGHTS_NAME if safe_serialization else ADAPTER_WEIGHTS_NAME\n\n        filename_pattern = weights_name.replace(\".bin\", \"{suffix}.bin\").replace(\".safetensors\", \"{suffix}.safetensors\")\n        state_dict_split = split_torch_state_dict_into_shards(\n            state_dict, filename_pattern=filename_pattern, max_shard_size=max_shard_size\n        )\n        # Save index if sharded\n        index = None\n        if state_dict_split.is_sharded:\n            index = {\n                \"metadata\": {\"total_parameters\": self.num_parameters(), **state_dict_split.metadata},\n                \"weight_map\": state_dict_split.tensor_to_filename,\n            }\n\n        # Clean the folder from a previous save\n        for filename in os.listdir(save_directory):\n            full_filename = os.path.join(save_directory, filename)\n            # If we have a shard file that is not going to be replaced, we delete it, but only from the main process\n            # in distributed settings to avoid race conditions.\n            weights_no_suffix = weights_name.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n\n            # make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005\n            filename_no_suffix = filename.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n            reg = re.compile(r\"(.*?)-\\d{5}-of-\\d{5}\")\n\n            if (\n                filename.startswith(weights_no_suffix)\n                and os.path.isfile(full_filename)\n                and filename not in state_dict_split.filename_to_tensors\n                and is_main_process\n                and reg.fullmatch(filename_no_suffix) is not None\n            ):\n                os.remove(full_filename)\n        # Save the model\n        filename_to_tensors = state_dict_split.filename_to_tensors.items()\n        if module_map:\n            filename_to_tensors = logging.tqdm(filename_to_tensors, desc=\"Saving checkpoint shards\")\n        for shard_file, tensors in filename_to_tensors:\n            shard = {}\n            for tensor in tensors:\n                if _is_dtensor_available and isinstance(state_dict[tensor], DTensor):\n                    full_tensor = state_dict[tensor].full_tensor()\n                    # to get the correctly ordered tensor we need to repack if packed\n                    if _get_parameter_tp_plan(tensor, self._tp_plan) == \"local_packed_rowwise\":\n                        full_tensor = repack_weights(full_tensor, -1, self._tp_size, 2)\n                    shard[tensor] = full_tensor.contiguous()  # only do contiguous after it's permuted correctly\n                else:\n                    shard[tensor] = state_dict[tensor].contiguous()\n                # delete reference, see https://github.com/huggingface/transformers/pull/34890\n                del state_dict[tensor]\n\n            # remake shard with onloaded parameters if necessary\n            if module_map:\n                # init state_dict for this shard\n                shard_state_dict = dict.fromkeys(shard, \"\")\n                for module_name in shard:\n                    # note that get_state_dict_from_offload can update with meta tensors\n                    # if both a parent module and its descendant are offloaded\n                    tensor = shard_state_dict[module_name]\n                    if tensor == \"\" or (isinstance(tensor, torch.Tensor) and tensor.device.type == \"meta\"):\n                        # update state dict with onloaded parameters\n                        module = module_map[module_name]\n                        shard_state_dict = get_state_dict_from_offload(module, module_name, shard_state_dict)\n\n                # assign shard to be the completed state dict\n                shard = shard_state_dict\n                del shard_state_dict\n                gc.collect()\n\n            if safe_serialization:\n                # At some point we will need to deal better with save_function (used for TPU and other distributed\n                # joyfulness), but for now this enough.\n                safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n            else:\n                save_function(shard, os.path.join(save_directory, shard_file))\n\n        del state_dict\n\n        if index is None:\n            path_to_weights = os.path.join(save_directory, weights_name)\n            logger.info(f\"Model weights saved in {path_to_weights}\")\n        else:\n            save_index_file = SAFE_WEIGHTS_INDEX_NAME if safe_serialization else WEIGHTS_INDEX_NAME\n            save_index_file = os.path.join(save_directory, _add_variant(save_index_file, variant))\n            # Save the index as well\n            with open(save_index_file, \"w\", encoding=\"utf-8\") as f:\n                content = json.dumps(index, indent=2, sort_keys=True) + \"\\n\"\n                f.write(content)\n            logger.info(\n                f\"The model is bigger than the maximum size per checkpoint ({max_shard_size}) and is going to be \"\n                f\"split in {len(state_dict_split.filename_to_tensors)} checkpoint shards. You can find where each parameters has been saved in the \"\n                f\"index located at {save_index_file}.\"\n            )\n\n        if push_to_hub:\n            # Eventually create an empty model card\n            model_card = create_and_tag_model_card(\n                repo_id, self.model_tags, token=token, ignore_metadata_errors=ignore_metadata_errors\n            )\n\n            # Update model card if needed:\n            model_card.save(os.path.join(save_directory, \"README.md\"))\n\n            self._upload_modified_files(\n                save_directory,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=token,\n                create_pr=create_pr,\n            )\n\n    @wraps(PushToHubMixin.push_to_hub)\n    def push_to_hub(self, *args, **kwargs):\n        tags = self.model_tags if self.model_tags is not None else []\n\n        tags_kwargs = kwargs.get(\"tags\", [])\n        if isinstance(tags_kwargs, str):\n            tags_kwargs = [tags_kwargs]\n\n        for tag in tags_kwargs:\n            if tag not in tags:\n                tags.append(tag)\n\n        if tags:\n            kwargs[\"tags\"] = tags\n        return super().push_to_hub(*args, **kwargs)\n\n    def get_memory_footprint(self, return_buffers=True):\n        r\"\"\"\n        Get the memory footprint of a model. This will return the memory footprint of the current model in bytes.\n        Useful to benchmark the memory footprint of the current model and design some tests. Solution inspired from the\n        PyTorch discussions: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822/2\n\n        Arguments:\n            return_buffers (`bool`, *optional*, defaults to `True`):\n                Whether to return the size of the buffer tensors in the computation of the memory footprint. Buffers\n                are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch\n                norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2\n        \"\"\"\n        mem = sum(param.nelement() * param.element_size() for param in self.parameters())\n        if return_buffers:\n            mem_bufs = sum(buf.nelement() * buf.element_size() for buf in self.buffers())\n            mem = mem + mem_bufs\n        return mem\n\n    @wraps(torch.nn.Module.cuda)\n    def cuda(self, *args, **kwargs):\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n            from hqq.core.quantize import HQQLinear\n\n            # Since HQQLinear stores some tensors in the 'meta' attribute,\n            # it's necessary to manually call the `cuda` method on HQQLinear layers.\n            super().cuda(*args, **kwargs)\n            for module in self.modules():\n                if isinstance(module, HQQLinear):\n                    if len(args) > 0:\n                        device = args[0]\n                    else:\n                        device = kwargs.get(\"device\", \"cuda\")\n                    module.cuda(device)\n            return self\n\n        # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n            if getattr(self, \"is_loaded_in_8bit\", False):\n                raise ValueError(\n                    \"Calling `cuda()` is not supported for `8-bit` quantized models. \"\n                    \" Please use the model as it is, since the model has already been set to the correct devices.\"\n                )\n        return super().cuda(*args, **kwargs)\n\n    @wraps(torch.nn.Module.to)\n    def to(self, *args, **kwargs):\n        # For BNB/GPTQ models, we prevent users from casting the model to another dtype to restrict unwanted behaviours.\n        # the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\n        dtype_present_in_args = \"dtype\" in kwargs\n\n        if not dtype_present_in_args:\n            for arg in args:\n                if isinstance(arg, torch.dtype):\n                    dtype_present_in_args = True\n                    break\n\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.HQQ:\n            from hqq.core.quantize import HQQLinear\n\n            # Since HQQLinear stores some tensors in the 'meta' attribute, we must\n            # explicitly move the parameters to the target device for each HQQLinear layer after `to`.\n            super().to(*args, **kwargs)\n            for module in self.modules():\n                if isinstance(module, HQQLinear):\n                    if \"device\" in kwargs:\n                        device = kwargs[\"device\"]\n                    else:\n                        device = args[0]\n                    if \"dtype\" in kwargs:\n                        dtype = kwargs[\"dtype\"]\n                    elif dtype_present_in_args:\n                        dtype = arg\n                    else:\n                        dtype = None\n                    # Due to the current messy implementation of HQQLinear, updating `compute_dtype`\n                    # followed by calling the `cuda` method achieves the intended behavior of `to`,\n                    # even when the target device is CPU.\n                    if dtype is not None:\n                        module.compute_dtype = dtype\n                    module.cuda(device)\n            return self\n\n        if dtype_present_in_args and getattr(self, \"quantization_method\", None) == QuantizationMethod.QUARK:\n            raise ValueError(\"Casting a Quark quantized model to a new `dtype` is not supported.\")\n\n        # Checks if the model has been loaded in 4-bit or 8-bit with BNB\n        if getattr(self, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES:\n            if dtype_present_in_args:\n                raise ValueError(\n                    \"You cannot cast a bitsandbytes model in a new `dtype`. Make sure to load the model using `from_pretrained` using the\"\n                    \" desired `dtype` by passing the correct `dtype` argument.\"\n                )\n\n            if getattr(self, \"is_loaded_in_8bit\", False):\n                raise ValueError(\n                    \"`.to` is not supported for `8-bit` bitsandbytes models. Please use the model as it is, since the\"\n                    \" model has already been set to the correct devices and casted to the correct `dtype`.\"\n                )\n        elif getattr(self, \"quantization_method\", None) == QuantizationMethod.GPTQ:\n            if dtype_present_in_args:\n                raise ValueError(\n                    \"You cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\"\n                    \" `dtype` by passing the correct `dtype` argument.\"\n                )\n        return super().to(*args, **kwargs)\n\n    def half(self, *args):\n        # Checks if the model is quantized\n        if getattr(self, \"is_quantized\", False):\n            raise ValueError(\n                \"`.half()` is not supported for quantized model. Please use the model as it is, since the\"\n                \" model has already been casted to the correct `dtype`.\"\n            )\n        else:\n            return super().half(*args)\n\n    def float(self, *args):\n        # Checks if the model is quantized\n        if getattr(self, \"is_quantized\", False):\n            raise ValueError(\n                \"`.float()` is not supported for quantized model. Please use the model as it is, since the\"\n                \" model has already been casted to the correct `dtype`.\"\n            )\n        else:\n            return super().float(*args)\n\n    @classmethod\n    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n        if is_deepspeed_zero3_enabled():\n            import deepspeed\n\n            init_contexts = [no_init_weights()]\n            # We cannot initialize the model on meta device with deepspeed when not quantized\n            if not is_quantized and not _is_ds_init_called:\n                logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n                init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n            elif is_quantized:\n                init_contexts.extend([init_empty_weights(), set_quantized_state()])\n        else:\n            init_contexts = [no_init_weights(), init_empty_weights()]\n\n        return init_contexts\n\n    def set_use_kernels(self, use_kernels, kernel_config):\n        if use_kernels:\n            if not is_kernels_available():\n                raise ValueError(\n                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n                )\n            from kernels import use_kernel_mapping\n\n            if kernel_config is not None and isinstance(kernel_config, KernelConfig):\n                # This will make sure the mapping is valid, and the layers are registered in the model\n                kernel_config.sanitize_kernel_mapping(self)\n\n                # This will create a compatible mapping for the model with the kernels library\n                kernel_config.create_compatible_mapping(self)\n\n                # This is a context manager to override the default kernel mapping\n                # We are calling kernelize inside this context manager using the use_kernels setter\n                with use_kernel_mapping(kernel_config.kernel_mapping):\n                    self.use_kernels = True\n            # We use the default kernel mapping in .integrations.hub_kernels\n            else:\n                self.use_kernels = True\n        else:\n            self.use_kernels = False\n\n    @classmethod\n    @restore_default_dtype\n    def from_pretrained(\n        cls: type[SpecificPreTrainedModelType],\n        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n        *model_args,\n        config: Optional[Union[PreTrainedConfig, str, os.PathLike]] = None,\n        cache_dir: Optional[Union[str, os.PathLike]] = None,\n        ignore_mismatched_sizes: bool = False,\n        force_download: bool = False,\n        local_files_only: bool = False,\n        token: Optional[Union[str, bool]] = None,\n        revision: str = \"main\",\n        use_safetensors: Optional[bool] = None,\n        weights_only: bool = True,\n        **kwargs,\n    ) -> SpecificPreTrainedModelType:\n        r\"\"\"\n        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n        the model, you should first set it back in training mode with `model.train()`.\n\n        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n        task.\n\n        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n        weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                    - A path to a *directory* containing model weights saved using\n                      [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n                    - `None` if you are both providing the configuration and state dictionary (resp. with keyword\n                      arguments `config` and `state_dict`).\n            model_args (sequence of positional arguments, *optional*):\n                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n            config (`Union[PreTrainedConfig, str, os.PathLike]`, *optional*):\n                Can be either:\n\n                    - an instance of a class derived from [`PreTrainedConfig`],\n                    - a string or path valid as input to [`~PreTrainedConfig.from_pretrained`].\n\n                Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n                be automatically loaded when:\n\n                    - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n                      model).\n                    - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n                      save directory.\n                    - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n                      configuration JSON file named *config.json* is found in the directory.\n            state_dict (`dict[str, torch.Tensor]`, *optional*):\n                A state dictionary to use instead of a state dictionary loaded from saved weights file.\n\n                This option can be used if you want to create a model from a pretrained configuration but load your own\n                weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n                [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n                Whether or not to raise an error if some of the weights from the checkpoint do not have the same size\n                as the weights of the model (if for instance, you are instantiating a model with 10 labels from a\n                checkpoint with 3 labels).\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            proxies (`dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            output_loading_info(`bool`, *optional*, defaults to `False`):\n                Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n            local_files_only(`bool`, *optional*, defaults to `False`):\n                Whether or not to only look at local files (i.e., do not try to download the model).\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n\n                <Tip>\n\n                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n\n                </Tip>\n            attn_implementation (`str`, *optional*):\n                The attention implementation to use in the model (if relevant). Can be any of `\"eager\"` (manual implementation of the attention), `\"sdpa\"` (using [`F.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html)), `\"flash_attention_2\"` (using [Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)), or `\"flash_attention_3\"` (using [Dao-AILab/flash-attention/hopper](https://github.com/Dao-AILab/flash-attention/tree/main/hopper)). By default, if available, SDPA will be used for torch>=2.1.1. The default is otherwise the manual `\"eager\"` implementation.\n\n                Accept HF kernel references in the form:\n                  <namespace>/<repo_name>[@<revision>][:<kernel_name>]\n\n                - <namespace> and <repo_name> are any non-\"/\" and non-\":\" sequences.\n                - \"@<revision>\" is optional (branch, tag, or commit-ish), e.g. \"@main\", \"@v1.2.0\", \"@abc123\".\n                - \":<kernel_name>\" is optional and selects a function inside the kernel repo.\n                - Both options can appear together and in this order only: @revision first, then :kernel_name.\n                - We intentionally allow a leading \"<wrapper>|\" prefix (e.g., \"flash|...\") because the code\n                  strips it before loading; '|' is not excluded in the character classes here.\n\n                Examples that match:\n                  \"org/model\"\n                  \"org/model@main\"\n                  \"org/model:custom_kernel\"\n                  \"org/model@v1.2.3:custom_kernel\"\n\n            > Parameters for big model inference\n\n            dtype (`str` or `torch.dtype`, *optional*):\n                Override the default `torch_dtype` and load the model under a specific `dtype`. The different options\n                are:\n\n                1. `torch.float16` or `torch.bfloat16` or `torch.float`: load in a specified\n                  `dtype`, ignoring the model's `config.dtype` if one exists. If not specified\n                  - the model will get loaded in `torch.float` (fp32).\n\n                2. `\"auto\"` - A `dtype` or `torch_dtype` entry in the `config.json` file of the model will be\n                  attempted to be used. If this entry isn't found then next check the `dtype` of the first weight in\n                  the checkpoint that's of a floating point type and use that as `dtype`. This will load the model\n                  using the `dtype` it was saved in at the end of the training. It can't be used as an indicator of how\n                  the model was trained. Since it could be trained in one of half precision dtypes, but saved in fp32.\n\n                3. A string that is a valid `torch.dtype`. E.g. \"float32\" loads the model in `torch.float32`, \"float16\" loads in `torch.float16` etc.\n\n                <Tip>\n\n                For some models the `dtype` they were trained in is unknown - you may try to check the model's paper or\n                reach out to the authors and ask them to add this information to the model's card and to insert the\n                `dtype` or `torch_dtype` entry in `config.json` on the hub.\n\n                </Tip>\n\n            device_map (`str` or `dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n                A map that specifies where each submodule should go. It doesn't need to be refined to each\n                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n                like `1`) on which the model will be allocated, the device map will map the entire model to this\n                device. Passing `device_map = 0` means put the whole model on GPU 0.\n\n                To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n                more information about each option see [designing a device\n                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n            max_memory (`Dict`, *optional*):\n                A dictionary device identifier to maximum memory if using `device_map`. Will default to the maximum memory available for each\n                GPU and the available CPU RAM if unset.\n            tp_plan (`Optional[Union[dict, str]]`, *optional*):\n                A torch tensor parallel plan, see [here](https://pytorch.org/tutorials/intermediate/TP_tutorial.html). Use `tp_plan=\"auto\"` to\n                use the predefined plan based on the model. If it's a dict, then it should match between module names and desired layout.\n                Note that if you use it, you should launch your script accordingly with `torchrun [args] script.py`. This will be much\n                faster than using a `device_map`, but has limitations.\n            tp_size (`str`, *optional*):\n                A torch tensor parallel degree. If not provided would default to world size.\n            device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n                A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.\n                If provided, it has to contain dimension named `\"tp\"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism\n            offload_folder (`str` or `os.PathLike`, *optional*):\n                If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n            offload_buffers (`bool`, *optional*):\n                Whether or not to offload the buffers with the model parameters.\n            quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n                A dictionary of configuration parameters or a QuantizationConfigMixin object for quantization (e.g\n                bitsandbytes, gptq).\n            subfolder (`str`, *optional*, defaults to `\"\"`):\n                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                specify the folder name here.\n            variant (`str`, *optional*):\n                If specified load weights from `variant` filename, *e.g.* pytorch_model.<variant>.bin.\n            use_safetensors (`bool`, *optional*, defaults to `None`):\n                Whether or not to use `safetensors` checkpoints. Defaults to `None`. If not specified and `safetensors`\n                is not installed, it will be set to `False`.\n            weights_only (`bool`, *optional*, defaults to `True`):\n                Indicates whether unpickler should be restricted to loading only tensors, primitive types,\n                dictionaries and any types added via torch.serialization.add_safe_globals().\n                When set to False, we can load wrapper tensor subclass weights.\n            key_mapping (`dict[str, str], *optional*):\n                A potential mapping of the weight names if using a model on the Hub which is compatible to a Transformers\n                architecture, but was not converted accordingly.\n            kwargs (remaining dictionary of keyword arguments, *optional*):\n                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n                `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n                automatically loaded:\n\n                    - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n                      underlying model's `__init__` method (we assume all relevant updates to the configuration have\n                      already been done)\n                    - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n                      initialization function ([`~PreTrainedConfig.from_pretrained`]). Each key of `kwargs` that\n                      corresponds to a configuration attribute will be used to override said attribute with the\n                      supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n                      will be passed to the underlying model's `__init__` function.\n\n        <Tip>\n\n        Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n        use this method in a firewalled environment.\n\n        </Tip>\n\n        Examples:\n\n        ```python\n        >>> from transformers import BertConfig, BertModel\n\n        >>> # Download model and configuration from huggingface.co and cache.\n        >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n        >>> # Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).\n        >>> model = BertModel.from_pretrained(\"./test/saved_model/\")\n        >>> # Update configuration during loading.\n        >>> model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True)\n        >>> assert model.config.output_attentions == True\n        ```\n        \"\"\"\n        state_dict = kwargs.pop(\"state_dict\", None)\n        proxies = kwargs.pop(\"proxies\", None)\n        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n        from_auto_class = kwargs.pop(\"_from_auto\", False)\n        dtype = kwargs.pop(\"dtype\", None)\n        torch_dtype = kwargs.pop(\"torch_dtype\", None)  # kept for BC\n        device_map = kwargs.pop(\"device_map\", None)\n        max_memory = kwargs.pop(\"max_memory\", None)\n        offload_folder = kwargs.pop(\"offload_folder\", None)\n        offload_buffers = kwargs.pop(\"offload_buffers\", False)\n        quantization_config = kwargs.pop(\"quantization_config\", None)\n        subfolder = kwargs.pop(\"subfolder\", \"\")\n        commit_hash = kwargs.pop(\"_commit_hash\", None)\n        variant = kwargs.pop(\"variant\", None)\n        adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n        adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n        generation_config = kwargs.pop(\"generation_config\", None)\n        gguf_file = kwargs.pop(\"gguf_file\", None)\n        tp_plan = kwargs.pop(\"tp_plan\", None)\n        tp_size = kwargs.pop(\"tp_size\", None)\n        distributed_config: DistributedConfig = kwargs.pop(\"distributed_config\", None)\n        device_mesh = kwargs.pop(\"device_mesh\", None)\n        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n        use_kernels = kwargs.pop(\"use_kernels\", False)\n        kernel_config = kwargs.pop(\"kernel_config\", None)\n\n        key_mapping = kwargs.pop(\"key_mapping\", None)\n        # Load models with key mapping\n        if key_mapping is None and any(\n            allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS\n        ):\n            key_mapping = cls._checkpoint_conversion_mapping\n\n        if distributed_config is not None and tp_plan is None:\n            tp_plan = \"auto\"\n\n        # Not used anymore -- remove them from the kwargs\n        for name in [\"mirror\", \"_fast_init\", \"low_cpu_mem_usage\", \"from_tf\", \"from_flax\", \"offload_state_dict\"]:\n            _ = kwargs.pop(name, None)\n\n        # For BC on torch_dtype argument\n        if torch_dtype is not None:\n            dtype = dtype if dtype is not None else torch_dtype\n\n        if is_offline_mode() and not local_files_only:\n            local_files_only = True\n\n        download_kwargs = {\n            \"cache_dir\": cache_dir,\n            \"force_download\": force_download,\n            \"proxies\": proxies,\n            \"local_files_only\": local_files_only,\n            \"token\": token,\n            \"revision\": revision,\n            \"subfolder\": subfolder,\n        }\n        download_kwargs_with_commit = {**download_kwargs, \"commit_hash\": commit_hash}\n\n        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):\n            raise ValueError(\n                \"`state_dict` cannot be passed together with a model name or a `gguf_file`. Use one of the two loading strategies.\"\n            )\n\n        if device_map == \"auto\" and int(os.environ.get(\"WORLD_SIZE\", \"0\")):\n            logger.info(\n                \"You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. \"\n                \"If your plan is to load the model on each device, you should set device_map={\"\n                \": PartialState().process_index} where PartialState comes from accelerate library\"\n            )\n\n        if tp_plan is not None or tp_size is not None:  # TP warnings, and setup\n            device_map, device_mesh, tp_size = initialize_tensor_parallelism(\n                tp_plan, tp_size=tp_size, device_mesh=device_mesh, device_map=device_map\n            )\n\n        if gguf_file is not None and not is_accelerate_available():\n            raise ValueError(\"accelerate is required when loading a GGUF file `pip install accelerate`.\")\n\n        if adapter_kwargs is None:\n            adapter_kwargs = {}\n\n        _adapter_model_path, pretrained_model_name_or_path = maybe_load_adapters(\n            pretrained_model_name_or_path,\n            download_kwargs_with_commit,\n            **adapter_kwargs,\n        )\n        device_map = check_and_set_device_map(device_map)  # warn, error and fix the device map\n\n        user_agent = {\"file_type\": \"model\", \"framework\": \"pytorch\", \"from_auto_class\": from_auto_class}\n        if from_pipeline is not None:\n            user_agent[\"using_pipeline\"] = from_pipeline\n\n        # Load config if we don't provide a configuration\n        if not isinstance(config, PreTrainedConfig):\n            config_path = config if config is not None else pretrained_model_name_or_path\n            config, model_kwargs = cls.config_class.from_pretrained(\n                config_path,\n                return_unused_kwargs=True,\n                gguf_file=gguf_file,\n                _from_auto=from_auto_class,\n                _from_pipeline=from_pipeline,\n                **download_kwargs,\n                **kwargs,\n            )\n            if \"gguf_file\" in model_kwargs:\n                model_kwargs.pop(\"gguf_file\")\n            commit_hash = model_kwargs.pop(\"_commit_hash\", commit_hash)\n        else:\n            config = copy.deepcopy(config)\n            model_kwargs = kwargs\n            commit_hash = getattr(config, \"_commit_hash\", commit_hash)\n\n        download_kwargs_with_commit[\"commit_hash\"] = commit_hash\n\n        # Because some composite configs call super().__init__ before instantiating the sub-configs, we need this call\n        # to correctly redispatch recursively if the kwarg is provided\n        if \"attn_implementation\" in kwargs:\n            config._attn_implementation = kwargs.pop(\"attn_implementation\")\n\n        hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n            config, quantization_config, dtype, device_map, weights_only, user_agent\n        )\n\n        if gguf_file:\n            if hf_quantizer is not None:\n                raise ValueError(\n                    \"You cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\"\n                )\n            if device_map is not None and (\n                (isinstance(device_map, dict) and \"disk\" in device_map.values()) or \"disk\" in device_map\n            ):\n                raise RuntimeError(\n                    \"One or more modules is configured to be mapped to disk. Disk offload is not supported for models \"\n                    \"loaded from GGUF files.\"\n                )\n\n        if kernel_config is not None and not use_kernels:\n            logger.warning_once(\n                \"A kernel_config was provided but use_kernels is False; setting use_kernels=True automatically. To suppress this warning, explicitly set use_kernels to True.\"\n            )\n            use_kernels = True\n\n        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            variant=variant,\n            gguf_file=gguf_file,\n            use_safetensors=use_safetensors,\n            download_kwargs=download_kwargs_with_commit,\n            user_agent=user_agent,\n            is_remote_code=cls._auto_class is not None,\n            transformers_explicit_filename=getattr(config, \"transformers_weights\", None),\n        )\n\n        is_quantized = hf_quantizer is not None\n\n        if gguf_file:\n            from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n\n            # we need a dummy model to get the state_dict - for this reason, we keep the state_dict as if it was\n            # passed directly as a kwarg from now on\n            with torch.device(\"meta\"):\n                dummy_model = cls(config)\n            state_dict = load_gguf_checkpoint(checkpoint_files[0], return_tensors=True, model_to_load=dummy_model)[\n                \"tensors\"\n            ]\n\n        # Find the correct dtype based on current state\n        config, dtype, dtype_orig = _get_dtype(\n            cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only\n        )\n\n        config.name_or_path = pretrained_model_name_or_path\n        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n        config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n        with ContextManagers(model_init_context):\n            # Let's make sure we don't run the init function of buffer modules\n            model = cls(config, *model_args, **model_kwargs)\n\n        # Potentially upcast some modules to avoid loosing precision\n        model.upcast_modules_in_fp32(hf_quantizer, dtype)\n        # Make sure to tie the weights correctly\n        model.tie_weights()\n\n        # make sure we use the model's config since the __init__ call might have copied it\n        config = model.config\n\n        if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n            hf_quantizer.preprocess_model(\n                model=model,\n                device_map=device_map,\n                keep_in_fp32_modules=model._keep_in_fp32_modules,\n                config=config,\n                checkpoint_files=checkpoint_files,\n                use_kernels=use_kernels,\n            )\n\n        if _torch_distributed_available and device_mesh is not None:  # add hooks to nn.Modules: no weights\n            model = distribute_model(model, tp_plan, distributed_config, device_mesh, tp_size)\n\n        # Prepare the full device map\n        if device_map is not None:\n            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype)\n\n        # restore default dtype\n        if dtype_orig is not None:\n            torch.set_default_dtype(dtype_orig)\n\n        # Finalize model weight initialization\n        model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = cls._load_pretrained_model(\n            model,\n            state_dict,\n            checkpoint_files,\n            pretrained_model_name_or_path,\n            ignore_mismatched_sizes=ignore_mismatched_sizes,\n            sharded_metadata=sharded_metadata,\n            device_map=device_map,\n            disk_offload_folder=offload_folder,\n            dtype=dtype,\n            hf_quantizer=hf_quantizer,\n            device_mesh=device_mesh,\n            key_mapping=key_mapping,\n            weights_only=weights_only,\n        )\n\n        model.tie_weights()  # make sure token embedding weights are still tied if needed\n        model.eval()  # Set model in evaluation mode to deactivate DropOut modules by default\n        model.set_use_kernels(use_kernels, kernel_config)\n\n        # If it is a model with generation capabilities, attempt to load generation files (generation config,\n        # custom generate function)\n        if model.can_generate() and hasattr(model, \"adjust_generation_fn\"):\n            model.adjust_generation_fn(\n                generation_config,\n                from_auto_class,\n                from_pipeline,\n                pretrained_model_name_or_path,\n                **download_kwargs,\n                trust_remote_code=trust_remote_code,\n                **kwargs,\n            )\n\n        # for device_map=\"auto\" : dispatch model with hooks on all devices if necessary (not needed with a tp_plan, so we skip it as it slightly\n        # harm performances).\n        if device_map is not None and device_mesh is None:\n            accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers)\n\n        if hf_quantizer is not None:\n            model.hf_quantizer = hf_quantizer\n            hf_quantizer.postprocess_model(model, config=config)  # usually a no-op\n\n        if _adapter_model_path is not None:\n            adapter_kwargs[\"key_mapping\"] = key_mapping  # TODO: Dynamic weight loader for adapters\n            model.load_adapter(\n                _adapter_model_path,\n                adapter_name=adapter_name,\n                token=token,\n                adapter_kwargs=adapter_kwargs,\n            )\n\n        if output_loading_info:\n            loading_info = {\n                \"missing_keys\": missing_keys,\n                \"unexpected_keys\": unexpected_keys,\n                \"mismatched_keys\": mismatched_keys,\n                \"error_msgs\": error_msgs,\n            }\n            return model, loading_info\n        return model\n\n    @staticmethod\n    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:\n        \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n        # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n        # This rename is logged.\n        if key.endswith(\"LayerNorm.beta\"):\n            return key.replace(\"LayerNorm.beta\", \"LayerNorm.bias\"), True\n        if key.endswith(\"LayerNorm.gamma\"):\n            return key.replace(\"LayerNorm.gamma\", \"LayerNorm.weight\"), True\n\n        # Rename weight norm parametrizations to match changes across torch versions.\n        # Impacts a number of speech/wav2vec models. e.g. Hubert, Wav2Vec2, and others.\n        # This rename is not logged.\n        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n            if key.endswith(\"weight_g\"):\n                return key.replace(\"weight_g\", \"parametrizations.weight.original0\"), True\n            if key.endswith(\"weight_v\"):\n                return key.replace(\"weight_v\", \"parametrizations.weight.original1\"), True\n        else:\n            if key.endswith(\"parametrizations.weight.original0\"):\n                return key.replace(\"parametrizations.weight.original0\", \"weight_g\"), True\n            if key.endswith(\"parametrizations.weight.original1\"):\n                return key.replace(\"parametrizations.weight.original1\", \"weight_v\"), True\n\n        return key, False\n\n    def _get_key_renaming_mapping(\n        self,\n        checkpoint_keys: list[str],\n        key_mapping: Optional[dict[str, str]] = None,\n        loading_base_model_from_task_state_dict: bool = False,\n        loading_task_model_from_base_state_dict: bool = False,\n    ):\n        \"\"\"\n        Compute a mapping between the serialized keys on disk `checkpoint_keys`, and the keys that the model\n        that we are loading expects. This is the single entry point for key renaming that will be used during\n        loading.\n        Log if any parameters have been renamed.\n        \"\"\"\n        prefix = self.base_model_prefix\n        _prefix = f\"{prefix}.\"\n\n        if loading_task_model_from_base_state_dict:\n            task_specific_expected_keys, base_model_keys = [], []\n            for key in self.state_dict():\n                if key.startswith(_prefix):\n                    base_model_keys.append(key[len(_prefix) :])\n                else:\n                    task_specific_expected_keys.append(key)\n\n        renamed_keys = {}\n        key_renaming_mapping = {}\n        for key in checkpoint_keys:\n            # Class specific rename\n            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n\n            # Optionally map the key according to `key_mapping`\n            if key_mapping is not None:\n                for pattern, replacement in key_mapping.items():\n                    new_key, n_replace = re.subn(pattern, replacement, new_key)\n                    # Early exit of the loop\n                    if n_replace > 0:\n                        has_changed = True\n                        break\n\n            # In this case, we need to add the prefix to the keys, to match them to the expected keys\n            if loading_task_model_from_base_state_dict:\n                # small sanity check: if we find a key that is only part of the task-specific keys, we raise\n                # (if it's also part of the base model, we do not raise and assume it comes from there)\n                if new_key in task_specific_expected_keys and new_key not in base_model_keys:\n                    raise ValueError(\n                        \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n                        \"properly saved?\"\n                    )\n                new_key = \".\".join([prefix, new_key])\n            # In this case we need to remove the prefix from the key to match them to the expected keys, and use\n            # only the keys starting with the prefix\n            elif loading_base_model_from_task_state_dict:\n                if not new_key.startswith(_prefix):\n                    continue\n                new_key = new_key[len(_prefix) :]\n\n            key_renaming_mapping[key] = new_key\n\n            # track gamma/beta rename for logging\n            if has_changed:\n                if key.endswith(\"LayerNorm.gamma\"):\n                    renamed_keys[\"LayerNorm.gamma\"] = (key, new_key)\n                elif key.endswith(\"LayerNorm.beta\"):\n                    renamed_keys[\"LayerNorm.beta\"] = (key, new_key)\n\n        if renamed_keys:\n            warning_msg = f\"A pretrained model of type `{self.__class__.__name__}` \"\n            warning_msg += \"contains parameters that have been renamed internally (a few are listed below but more are present in the model):\\n\"\n            for old_key, new_key in renamed_keys.values():\n                warning_msg += f\"* `{old_key}` -> `{new_key}`\\n\"\n            warning_msg += \"If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\"\n            logger.info_once(warning_msg)\n\n        return key_renaming_mapping\n\n    @staticmethod\n    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:\n        \"\"\"\n        Similar to `_fix_state_dict_key_on_load` allows to define hook for state dict key renaming on model save.\n        Do nothing by default, but can be overridden in particular models.\n        \"\"\"\n        return key, False\n\n    def _fix_state_dict_keys_on_save(self, state_dict):\n        \"\"\"\n        Similar to `_fix_state_dict_keys_on_load` allows to define hook for state dict key renaming on model save.\n        Apply `_fix_state_dict_key_on_save` to all keys in `state_dict`.\n        \"\"\"\n        return {self._fix_state_dict_key_on_save(key)[0]: value for key, value in state_dict.items()}\n\n    @classmethod\n    def _load_pretrained_model(\n        cls,\n        model: \"PreTrainedModel\",\n        state_dict: Optional[dict],\n        checkpoint_files: Optional[list[str]],\n        pretrained_model_name_or_path: Optional[str],\n        ignore_mismatched_sizes: bool = False,\n        sharded_metadata: Optional[dict] = None,\n        device_map: Optional[dict] = None,\n        disk_offload_folder: Optional[str] = None,\n        dtype: Optional[torch.dtype] = None,\n        hf_quantizer: Optional[HfQuantizer] = None,\n        device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n        key_mapping: Optional[dict[str, str]] = None,\n        weights_only: bool = True,\n    ):\n        # TODO: we should only be calling hf_quantizer.skip_placement or something like that\n        is_quantized = hf_quantizer is not None\n        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {\n            QuantizationMethod.HQQ,\n            QuantizationMethod.QUARK,\n        }\n\n        # Get all the keys of the state dicts that we have to initialize the model with\n        if sharded_metadata is not None:\n            original_checkpoint_keys = sharded_metadata[\"all_checkpoint_keys\"]\n        elif state_dict is not None:\n            original_checkpoint_keys = list(state_dict.keys())\n        else:\n            original_checkpoint_keys = list(\n                load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n            )\n\n        # Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\n        prefix = model.base_model_prefix\n        has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n        loading_base_model_from_task_state_dict = has_prefix_module and not expects_prefix_module\n\n        # Find the key names that the model expects from the serialized keys\n        key_renaming_mapping = model._get_key_renaming_mapping(\n            original_checkpoint_keys,\n            key_mapping,\n            loading_base_model_from_task_state_dict,\n            loading_task_model_from_base_state_dict,\n        )\n        checkpoint_keys = list(key_renaming_mapping.values())\n\n        # Find missing and unexpected keys from the state dict\n        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(\n            model, original_checkpoint_keys, checkpoint_keys, loading_base_model_from_task_state_dict, hf_quantizer\n        )\n        # Find all the keys with shape mismatch (if we ignore the mismatch, the weights need to be newly initialized the\n        # same way as missing keys)\n        mismatched_keys, mismatched_shapes = _find_mismatched_keys(\n            model,\n            state_dict,\n            checkpoint_files,\n            ignore_mismatched_sizes,\n            key_renaming_mapping,\n            is_quantized,\n            weights_only,\n        )\n\n        # We need to update both the mapping and the list of checkpoint keys to remove the mismatched and unexpected ones\n        key_renaming_mapping = {\n            k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys and v not in unexpected_keys\n        }\n        checkpoint_keys = list(key_renaming_mapping.values())\n\n        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n        # loading the weights as they are not in the loaded state dict)\n        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, dtype, hf_quantizer)\n\n        # correctly initialize the missing (and potentially mismatched) keys\n        model._initialize_missing_keys(missing_keys + mismatched_keys, is_quantized)\n\n        # Get reverse key mapping\n        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n\n        is_offloaded_safetensors = False\n        # This offload index if for params explicitly on the \"disk\" in the device_map\n        disk_offload_index = None\n        disk_only_shard_files = []\n        # Prepare parameters offloading if needed\n        if device_map is not None and \"disk\" in device_map.values():\n            disk_offload_index, disk_only_shard_files, is_offloaded_safetensors = accelerate_disk_offload(\n                disk_offload_folder,\n                checkpoint_files,\n                device_map,\n                checkpoint_keys,\n                key_renaming_mapping,\n                sharded_metadata,\n                dtype,\n                reverse_key_renaming_mapping,\n            )\n        # To be able to iterate, even if we don't use it if the state_dict is already provided\n        elif state_dict is not None:\n            checkpoint_files = [\"\"]\n\n        # Compute expected model keys\n        expected_keys = list(model.state_dict().keys())\n        if hf_quantizer is not None:\n            expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n\n        if logger.level >= logging.WARNING:\n            verify_tp_plan(expected_keys, getattr(model, \"_tp_plan\", None))\n\n        # Warmup cuda to load the weights much faster on devices\n        if device_map is not None and not is_hqq_or_quark:\n            expanded_device_map = expand_device_map(device_map, expected_keys)\n            caching_allocator_warmup(model, expanded_device_map, hf_quantizer)\n\n        # Prepare and compatabilize arguments for serial and parallel shard loading\n        args_list = [\n            (\n                shard_file,\n                state_dict,\n                disk_only_shard_files,\n                is_quantized,\n                device_map,\n                hf_quantizer,\n                key_renaming_mapping,\n                weights_only,\n                model,\n                reverse_key_renaming_mapping,\n                disk_offload_folder,\n                disk_offload_index,\n                device_mesh,\n            )\n            for shard_file in checkpoint_files\n        ]\n\n        error_msgs = []\n\n        if (\n            os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n            and not is_deepspeed_zero3_enabled()\n        ):\n            _error_msgs, disk_offload_index = load_shard_files_with_threadpool(args_list)\n            error_msgs += _error_msgs\n        else:\n            if len(args_list) > 1:\n                args_list = logging.tqdm(args_list, desc=\"Loading checkpoint shards\")\n\n            for args in args_list:\n                _error_msgs, disk_offload_index = load_shard_file(args)\n                error_msgs += _error_msgs\n\n        # Save offloaded index if needed\n        if disk_offload_index is not None and len(disk_offload_index) > 0 and not is_offloaded_safetensors:\n            save_offload_index(disk_offload_index, disk_offload_folder)\n            disk_offload_index = None\n\n        # Post-processing for tensor parallelism\n        if device_mesh is not None:\n            # When using TP, the device map is a single device for all parameters\n            tp_device = list(device_map.values())[0]\n            # This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\n            # not part of the state_dict (persistent=False)\n            for buffer in model.buffers():\n                if buffer.device != tp_device:\n                    buffer.data = buffer.to(tp_device)\n\n            # In this case, the top-most task module weights were not moved to device and parallelized as they\n            # were not part of the loaded weights: do it now\n            if loading_task_model_from_base_state_dict:\n                parameters_to_initialize = {\n                    name: param for name, param in model.named_parameters() if not name.startswith(prefix)\n                }\n                for name, param in parameters_to_initialize.items():\n                    # If it is still on meta here, it means that it's a tied weight that will be tied later anyway -> skip it\n                    if param.device.type == \"meta\":\n                        continue\n                    # Shard the param\n                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param)\n                    shard_and_distribute_module(\n                        model,\n                        param.to(tp_device),\n                        param,\n                        name,\n                        casting_dtype,\n                        to_contiguous,\n                        device_mesh.get_local_rank(),\n                        device_mesh,\n                    )\n\n        # Remove potential model-specific exceptions from the warnings\n        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(\n            missing_keys, unexpected_keys, loading_task_model_from_base_state_dict\n        )\n\n        # TODO: separate this in another function: it's not core....\n        # All potential warnings/infos\n        if len(error_msgs) > 0:\n            error_msg = \"\\n\\t\".join(error_msgs)\n            if \"size mismatch\" in error_msg:\n                error_msg += (\n                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n                )\n            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n        if len(unexpected_keys) > 0:\n            archs = [] if model.config.architectures is None else model.config.architectures\n            warner = logger.warning if model.__class__.__name__ in archs else logger.info\n            warner(\n                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n                f\" initializing {model.__class__.__name__}: {update_key_name(unexpected_keys)}\\n- This IS expected if you are\"\n                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task or\"\n                \" with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n                \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n            )\n        if len(missing_keys) > 0:\n            logger.warning(\n                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path} and are newly initialized: {update_key_name(missing_keys)}\\nYou should probably\"\n                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n            )\n        if len(mismatched_keys) > 0:\n            mismatched_warning = \"\\n\".join(\n                [\n                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n                    for key, (shape1, shape2) in zip(mismatched_keys, mismatched_shapes)\n                ]\n            )\n            logger.warning(\n                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be able\"\n                \" to use it for predictions and inference.\"\n            )\n\n        return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n\n    def retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False):\n        module_keys = {\".\".join(key.split(\".\")[:-1]) for key in names}\n\n        # torch.nn.ParameterList is a special case where two parameter keywords\n        # are appended to the module name, *e.g.* bert.special_embeddings.0\n        module_keys = module_keys.union(\n            {\".\".join(key.split(\".\")[:-2]) for key in names if len(key) > 0 and key[-1].isdigit()}\n        )\n\n        retrieved_modules = []\n        # retrieve all modules that has at least one missing weight name\n        for name, module in self.named_modules():\n            if remove_prefix:\n                _prefix = f\"{self.base_model_prefix}.\"\n                name = name.removeprefix(_prefix)\n            elif add_prefix:\n                name = \".\".join([self.base_model_prefix, name]) if len(name) > 0 else self.base_model_prefix\n\n            if name in module_keys:\n                retrieved_modules.append(module)\n\n        return retrieved_modules\n\n    @classmethod\n    def register_for_auto_class(cls, auto_class=\"AutoModel\"):\n        \"\"\"\n        Register this class with a given auto class. This should only be used for custom models as the ones in the\n        library are already mapped with an auto class.\n\n\n\n        Args:\n            auto_class (`str` or `type`, *optional*, defaults to `\"AutoModel\"`):\n                The auto class to register this new model with.\n        \"\"\"\n        if not isinstance(auto_class, str):\n            auto_class = auto_class.__name__\n\n        import transformers.models.auto as auto_module\n\n        if not hasattr(auto_module, auto_class):\n            raise ValueError(f\"{auto_class} is not a valid auto class.\")\n\n        cls._auto_class = auto_class\n\n    def warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask):\n        \"\"\"\n        Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.\n        \"\"\"\n\n        # Skip the check during tracing.\n        if is_tracing(input_ids):\n            return\n\n        if (attention_mask is not None) or (self.config.pad_token_id is None):\n            return\n\n        # Check only the first and last input IDs to reduce overhead.\n        if self.config.pad_token_id in input_ids[:, [-1, 0]]:\n            warn_string = (\n                \"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\n                \"https://huggingface.co/docs/transformers/troubleshooting\"\n                \"#incorrect-output-when-padding-tokens-arent-masked.\"\n            )\n\n            # If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\n            # attention_mask or not. In this case, we should still show a warning because this is a rare case.\n            if (\n                (self.config.bos_token_id is not None and self.config.bos_token_id == self.config.pad_token_id)\n                or (self.config.eos_token_id is not None and self.config.eos_token_id == self.config.pad_token_id)\n                or (self.config.sep_token_id is not None and self.config.sep_token_id == self.config.pad_token_id)\n            ):\n                warn_string += (\n                    f\"\\nYou may ignore this warning if your `pad_token_id` ({self.config.pad_token_id}) is identical \"\n                    f\"to the `bos_token_id` ({self.config.bos_token_id}), `eos_token_id` ({self.config.eos_token_id}), \"\n                    f\"or the `sep_token_id` ({self.config.sep_token_id}), and your input is not padded.\"\n                )\n\n            logger.warning_once(warn_string)\n\n    @property\n    def supports_tp_plan(self):\n        \"\"\"\n        Returns whether the model has a tensor parallelism plan.\n        \"\"\"\n        if self._tp_plan is not None:\n            return True\n        # Check if base model has a TP plan\n        if getattr(self.base_model, \"_tp_plan\", None) is not None:\n            return True\n        if self.config.base_model_tp_plan is not None:\n            return True\n        return False\n\n    @property\n    def tp_size(self):\n        \"\"\"\n        Returns the model's tensor parallelism degree.\n        \"\"\"\n        # if None, the model didn't undergo tensor parallel sharding\n        return self._tp_size\n\n    @property\n    def supports_pp_plan(self):\n        if self._pp_plan is not None:\n            return True\n        # Check if base model has PP plan\n        if getattr(self.base_model, \"_pp_plan\", None) is not None:\n            return True\n        return False\n\n    @property\n    def loss_function(self):\n        if hasattr(self, \"_loss_function\"):\n            return self._loss_function\n\n        loss_type = getattr(self, \"loss_type\", None)\n\n        if loss_type is None or loss_type not in LOSS_MAPPING:\n            logger.warning_once(\n                f\"`loss_type={loss_type}` was set in the config but it is unrecognized. \"\n                f\"Using the default loss: `ForCausalLMLoss`.\"\n            )\n            loss_type = \"ForCausalLM\"\n        return LOSS_MAPPING[loss_type]\n\n    @loss_function.setter\n    def loss_function(self, value):\n        self._loss_function = value\n\n    def kernelize(self, mode=None):\n        if not is_kernels_available():\n            raise ValueError(\n                \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n            )\n        from kernels import Device, Mode, kernelize\n\n        mode = Mode.INFERENCE if not self.training else Mode.TRAINING if mode is None else mode\n        kernelize(self, device=Device(type=self.device.type), mode=mode)\n        self._use_kernels = True\n\n    @property\n    def use_kernels(self) -> bool:\n        return getattr(self, \"_use_kernels\", False)\n\n    @use_kernels.setter\n    def use_kernels(self, value: bool) -> None:\n        # Avoid re-kernelizing if already enabled\n        if bool(value) and getattr(self, \"_use_kernels\", False):\n            return\n\n        if value:\n            self.kernelize()\n        else:\n            if getattr(self, \"_use_kernels\", False):\n                logger.warning_once(\n                    \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n                )\n            self._use_kernels = False\n\n    def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable:\n        \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n        non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't\n        want to use compiled version to avoid recomputing the graph with new shapes) and iterative decoding\n        (where we want the speed-ups of compiled version with static shapes).\"\"\"\n        # Only reset it if not present or different from previous config\n        if \"llama4\" in self.config.model_type:  # TODO try to enable for FULL COMPILE HYBRID CACHE SUPPORT\n            return self.__call__\n        compile_config = compile_config or CompileConfig()\n        default_config = getattr(self.generation_config, \"compile_config\", None) or CompileConfig()\n        if (\n            not hasattr(self, \"_compiled_call\")\n            or getattr(self, \"_last_compile_config\", default_config) != compile_config\n        ):\n            self._last_compile_config = compile_config\n            self._compiled_call = torch.compile(self.__call__, **compile_config.to_dict())\n        return self._compiled_call\n\n    @classmethod\n    def is_backend_compatible(cls):\n        return cls._supports_attention_backend\n\n    def _move_missing_keys_from_meta_to_cpu(\n        self, missing_keys: list[str], dtype: torch.dtype, hf_quantizer: Optional[HfQuantizer]\n    ) -> None:\n        \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts) back\n        from meta device to cpu.\n        \"\"\"\n        is_quantized = hf_quantizer is not None\n\n        # In this case we need to move everything back\n        if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n            # We only do it for the parameters, as the buffers are not initialized on the meta device by default\n            for key, param in self.named_parameters():\n                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n                _load_parameter_into_model(self, key, value)\n            return\n\n        model_state_dict = self.state_dict()\n        for key in missing_keys:\n            param = model_state_dict[key]\n            # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n            if param.device == torch.device(\"meta\"):\n                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n                if not is_quantized or not hf_quantizer.param_needs_quantization(self, key):\n                    _load_parameter_into_model(self, key, value)\n                else:\n                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\")\n\n    def _initialize_missing_keys(self, missing_keys: list[str], is_quantized: bool) -> None:\n        \"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\n        `_initialize_weights`. Indeed, since the corresponding weights are missing from the state dict, they will not be replaced and need to\n        be initialized correctly (i.e. weight initialization distribution).\n        Also take care of setting the `_is_hf_initialized` flag for keys that are not missing.\n        \"\"\"\n        for key in self.state_dict():\n            # If it's part of the keys that will be loaded, mark it as already initialized\n            if key not in missing_keys:\n                param_or_buffer = self.get_parameter_or_buffer(key)\n                param_or_buffer._is_hf_initialized = True\n\n        def set_is_initialized_for_modules(module):\n            # A module is already initialized if and only if all its children are also already initialized, and all\n            # its immediate `nn.Parameter` and persistent buffers are also already initialized\n            if (\n                # All immediate children are initialized\n                all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n                # All immediate parameters are initialized\n                and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n                # All immediate persistent buffers are initialized\n                and all(\n                    getattr(buffer, \"_is_hf_initialized\", False)\n                    for name, buffer in module.named_buffers(recurse=False)\n                    if name not in module._non_persistent_buffers_set\n                )\n            ):\n                module._is_hf_initialized = True\n\n        # Set the flag on the modules as well. We do it recursively (depth-first), as it's more efficient (we do not\n        # need to check the entire state dict of each module, only the immediate children, so we only iterate once over\n        # each param)\n        self.apply(set_is_initialized_for_modules)\n\n        # This will only initialize submodules that are not marked as initialized by the line above.\n        if is_deepspeed_zero3_enabled() and not is_quantized:\n            import deepspeed\n\n            # keep_vars=True as we need the original tensors, so that the \"_is_hf_initialized\" is present on them\n            not_initialized_parameters = list(\n                {v for v in self.state_dict(keep_vars=True).values() if not getattr(v, \"_is_hf_initialized\", False)}\n            )\n            with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n                self.initialize_weights()\n        else:\n            self.initialize_weights()\n\n    def _adjust_missing_and_unexpected_keys(\n        self, missing_keys: list[str], unexpected_keys: list[str], loading_task_model_from_base_state_dict: bool\n    ) -> tuple[list[str], list[str]]:\n        \"\"\"Adjust the `missing_keys` and `unexpected_keys` based on current model's exception rules, to avoid\n        raising unneeded warnings/errors.\n        \"\"\"\n        # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n        # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n        # `_keys_to_ignore_on_load_unexpected` as it touches many models -> we add it manually to the existing patterns\n        has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer, _ in self.named_buffers())\n        additional_unexpected_patterns = [r\"rotary_emb\\.inv_freq\"] if has_inv_freq_buffers else []\n\n        missing_patterns = self._keys_to_ignore_on_load_missing or []\n        unexpected_patterns = (self._keys_to_ignore_on_load_unexpected or []) + additional_unexpected_patterns\n        ignore_missing_regex, ignore_unexpected_regex = None, None\n        if len(missing_patterns) > 0:\n            ignore_missing_regex = re.compile(\"|\".join(rf\"({pattern})\" for pattern in missing_patterns))\n        if len(unexpected_patterns) > 0:\n            ignore_unexpected_regex = re.compile(\"|\".join(rf\"({pattern})\" for pattern in unexpected_patterns))\n\n        # Clean-up missing keys\n        if ignore_missing_regex is not None:\n            missing_keys = [key for key in missing_keys if ignore_missing_regex.search(key) is None]\n\n        # Clean-up unexpected keys\n        if ignore_unexpected_regex is not None:\n            unexpected_keys = [key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None]\n\n        # Note: only the unexpected keys should remove the added prefix here, to correctly display the original name\n        # in the warnings. For missing keys, we should show the prefix in the warning as it's part of the final model\n        if loading_task_model_from_base_state_dict:\n            _prefix = f\"{self.base_model_prefix}.\"\n            unexpected_keys = [k.removeprefix(_prefix) for k in unexpected_keys]\n\n        return missing_keys, unexpected_keys\n\n    def get_parameter_or_buffer(self, target: str):\n        \"\"\"\n        Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combines\n        `get_parameter()` and `get_buffer()` in a single handy function. If the target is an `_extra_state` attribute,\n        it will return the extra state provided by the module. Note that it only work if `target` is a leaf of the model.\n        \"\"\"\n        try:\n            return self.get_parameter(target)\n        except AttributeError:\n            pass\n        try:\n            return self.get_buffer(target)\n        except AttributeError:\n            pass\n        module, param_name = get_module_from_name(self, target)\n        if (\n            param_name == \"_extra_state\"\n            and getattr(module.__class__, \"get_extra_state\", torch.nn.Module.get_extra_state)\n            is not torch.nn.Module.get_extra_state\n        ):\n            return module.get_extra_state()\n\n        raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")\n\n    def train(self, mode: bool = True):\n        out = super().train(mode)\n        if self.use_kernels:\n            self.kernelize()\n        return out\n\n    def eval(self):\n        return self.train(False)\n\n    def upcast_modules_in_fp32(self, hf_quantizer: HfQuantizer | None, dtype: torch.dtype) -> None:\n        \"\"\"\n        Upcast modules defined in `_keep_in_fp32_modules` and `_keep_in_fp32_modules_strict` in fp32, if\n        `dtype` is different than fp32.\n        \"\"\"\n        # If the dtype is already fp32, we can skip\n        if dtype == torch.float32:\n            return\n\n        keep_in_fp32_modules = []\n        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n        if self._keep_in_fp32_modules is not None and (\n            dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n        ):\n            keep_in_fp32_modules.extend(self._keep_in_fp32_modules)\n\n        if self._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):\n            keep_in_fp32_modules.extend(self._keep_in_fp32_modules_strict)\n\n        if len(keep_in_fp32_modules) > 0:\n            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n            keep_in_fp32_regex = re.compile(\"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in keep_in_fp32_modules]))\n            for name, param in self.named_parameters():\n                if keep_in_fp32_regex.search(name):\n                    # param = param.to(torch.float32) does not work here as only in the local scope.\n                    param.data = param.data.to(torch.float32)\n\n\n\ndef caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]):\n    \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n    device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n    the model, which is actually the loading speed bottleneck.\n    Calling this function allows to cut the model loading time by a very large margin.\n\n    A few facts related to loading speed (taking into account the use of this function):\n    - When loading a model the first time, it is usually slower than the subsequent times, because the OS is very likely\n    to cache the different state dicts (if enough resources/RAM are available)\n    - Trying to force the OS to cache the files in advance (by e.g. accessing a small portion of them) is really hard,\n    and not a good idea in general as this is low level OS optimizations that depend on resource usage anyway\n    - As of 18/03/2025, loading a Llama 70B model with TP takes ~1 min without file cache, and ~13s with full file cache.\n    The baseline, i.e. only loading the tensor shards on device and adjusting dtype (i.e. copying them) is ~5s with full cache.\n    These numbers are reported for TP on 4 H100 GPUs.\n    - It is useless to pre-allocate more than the model size in this function (i.e. using an `allocation_factor` > 1) as\n    cudaMalloc is not a bottleneck at all anymore\n    - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n    However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n    \"\"\"\n    factor = 2 if hf_quantizer is None else hf_quantizer.get_accelerator_warm_up_factor()\n\n    # Remove disk, cpu and meta devices, and cast to proper torch.device\n    accelerator_device_map = {\n        param: torch.device(device) for param, device in expanded_device_map.items() if is_accelerator_device(device)\n    }\n    if not accelerator_device_map:\n        return\n\n    tp_plan = getattr(model, \"_tp_plan\", []) or []\n    tp_plan_regex = (\n        re.compile(\"|\".join([re.escape(plan) for plan in tp_plan]))\n        if _torch_distributed_available and torch.distributed.is_initialized()\n        else None\n    )\n    total_byte_count = defaultdict(lambda: 0)\n    tied_param_names = _get_tied_weight_keys(model)\n    for param_name, device in accelerator_device_map.items():\n        # Skip if the parameter has already been accounted for (tied weights)\n        if param_name in tied_param_names:\n            continue\n\n        # For example in the case of MXFP4 quantization, we need to update the param name to the original param name\n        # because the checkpoint contains blocks, and scales, but since we are dequantizing, we need to use the original param name\n        if hf_quantizer is not None:\n            param_name = hf_quantizer.get_param_name(param_name)\n\n        try:\n            param = model.get_parameter_or_buffer(param_name)\n        except AttributeError:\n            raise AttributeError(f\"Parameter {param_name} not found in model\")\n\n        # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`\n        param_byte_count = param.numel() * param.element_size()\n\n        if tp_plan_regex is not None:\n            generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n            param_byte_count //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n\n        total_byte_count[device] += param_byte_count\n\n    # This will kick off the caching allocator to avoid having to Malloc afterwards\n    for device, byte_count in total_byte_count.items():\n        if device.type in [\"cuda\", \"xpu\"]:\n            torch_accelerator_module = getattr(torch, device.type)\n            index = device.index if device.index is not None else torch_accelerator_module.current_device()\n            device_memory = torch_accelerator_module.mem_get_info(index)[0]\n            # Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\n            # than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\n            # and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\n            # the param size, instead of using the remaining reserved part, and allocating only the difference, which can lead\n            # to OOM. See https://github.com/huggingface/transformers/issues/37436#issuecomment-2808982161 for more details.\n            # Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\n            # if using e.g. 90% of device size, while a 140GiB device would allocate too little\n            byte_count = min(byte_count, max(0, int(device_memory - 1.2 * 1024**3)))\n            # If there is *unused* reserved cuda/xpu memory, we can skip/reduce the allocation.\n            unused_memory = torch_accelerator_module.memory_reserved(\n                index\n            ) - torch_accelerator_module.memory_allocated(index)\n            byte_count = max(0, byte_count - unused_memory)\n        # Allocate memory\n        _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)"
                },
                "component_dependencies": {
                    "PreTrainedModel": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/distributed.py#DistributedConfig",
                        "transformers/dynamic_module_utils.py#custom_object_save",
                        "transformers/generation.py#CompileConfig",
                        "transformers/generation.py#GenerationConfig.from_model_config",
                        "transformers/integrations.py#PeftAdapterMixin",
                        "transformers/integrations.py#deepspeed_config",
                        "transformers/integrations.py#is_deepspeed_zero3_enabled",
                        "transformers/integrations.py#is_fsdp_enabled",
                        "transformers/integrations/accelerate.py#_get_device_map",
                        "transformers/integrations/accelerate.py#accelerate_disk_offload",
                        "transformers/integrations/accelerate.py#accelerate_dispatch",
                        "transformers/integrations/accelerate.py#check_and_set_device_map",
                        "transformers/integrations/accelerate.py#expand_device_map",
                        "transformers/integrations/accelerate.py#init_empty_weights",
                        "transformers/integrations/hub_kernels.py#is_kernel",
                        "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel",
                        "transformers/integrations/peft.py#maybe_load_adapters",
                        "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES",
                        "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES.keys",
                        "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                        "transformers/integrations/tensor_parallel.py#distribute_model",
                        "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism",
                        "transformers/integrations/tensor_parallel.py#repack_weights",
                        "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor",
                        "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                        "transformers/integrations/tensor_parallel.py#verify_tp_plan",
                        "transformers/loss/loss_utils.py#LOSS_MAPPING",
                        "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention",
                        "transformers/modeling_utils.py#EmbeddingAccessMixin",
                        "transformers/modeling_utils.py#ModuleUtilsMixin",
                        "transformers/modeling_utils.py#SpecificPreTrainedModelType",
                        "transformers/modeling_utils.py#VLMS",
                        "transformers/modeling_utils.py#_add_variant",
                        "transformers/modeling_utils.py#_find_disjoint",
                        "transformers/modeling_utils.py#_find_identical",
                        "transformers/modeling_utils.py#_find_mismatched_keys",
                        "transformers/modeling_utils.py#_find_missing_and_unexpected_keys",
                        "transformers/modeling_utils.py#_get_dtype",
                        "transformers/modeling_utils.py#_get_resolved_checkpoint_files",
                        "transformers/modeling_utils.py#_get_tied_weight_keys",
                        "transformers/modeling_utils.py#_infer_parameter_dtype",
                        "transformers/modeling_utils.py#_init_weights",
                        "transformers/modeling_utils.py#_is_ds_init_called",
                        "transformers/modeling_utils.py#_is_dtensor_available",
                        "transformers/modeling_utils.py#_is_quantized",
                        "transformers/modeling_utils.py#_load_parameter_into_model",
                        "transformers/modeling_utils.py#_torch_distributed_available",
                        "transformers/modeling_utils.py#get_parameter_dtype",
                        "transformers/modeling_utils.py#is_accelerator_device",
                        "transformers/modeling_utils.py#is_local_dist_rank_0",
                        "transformers/modeling_utils.py#load_shard_file",
                        "transformers/modeling_utils.py#load_shard_files_with_threadpool",
                        "transformers/modeling_utils.py#load_state_dict",
                        "transformers/modeling_utils.py#logger",
                        "transformers/modeling_utils.py#no_init_weights",
                        "transformers/modeling_utils.py#restore_default_dtype",
                        "transformers/modeling_utils.py#set_quantized_state",
                        "transformers/modeling_utils.py#set_zero3_state",
                        "transformers/modeling_utils.py#unwrap_model",
                        "transformers/modeling_utils.py#update_key_name",
                        "transformers/pytorch_utils.py#id_tensor_storage",
                        "transformers/quantizers.py#HfQuantizer",
                        "transformers/quantizers/auto.py#get_hf_quantizer",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#ADAPTER_SAFE_WEIGHTS_NAME",
                        "transformers/utils.py#ADAPTER_WEIGHTS_NAME",
                        "transformers/utils.py#ContextManagers",
                        "transformers/utils.py#DUMMY_INPUTS",
                        "transformers/utils.py#KernelConfig",
                        "transformers/utils.py#PushToHubMixin",
                        "transformers/utils.py#PushToHubMixin.push_to_hub",
                        "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                        "transformers/utils.py#SAFE_WEIGHTS_NAME",
                        "transformers/utils.py#WEIGHTS_INDEX_NAME",
                        "transformers/utils.py#WEIGHTS_NAME",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_flash_attn_2_available",
                        "transformers/utils.py#is_flash_attn_3_available",
                        "transformers/utils.py#is_kernels_available",
                        "transformers/utils.py#is_offline_mode",
                        "transformers/utils.py#is_torch_flex_attn_available",
                        "transformers/utils.py#is_torch_mlu_available",
                        "transformers/utils.py#is_torch_npu_available",
                        "transformers/utils.py#logging.WARNING",
                        "transformers/utils.py#logging.tqdm",
                        "transformers/utils/generic.py#OutputRecorder",
                        "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
                        "transformers/utils/hub.py#create_and_tag_model_card",
                        "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES",
                        "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal",
                        "transformers/utils/import_utils.py#is_tracing",
                        "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                        "transformers/utils/quantization_config.py#QuantizationMethod.GPTQ",
                        "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                        "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeAttention": {
                "sorted_modules": {
                    "Qwen3MoeAttention": "\n\nclass Qwen3MoeAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n        self.scaling = self.head_dim**-0.5\n        self.attention_dropout = config.attention_dropout\n        self.is_causal = True\n\n        self.q_proj = nn.Linear(\n            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.k_proj = nn.Linear(\n            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.v_proj = nn.Linear(\n            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.o_proj = nn.Linear(\n            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n        )\n        self.q_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n        self.k_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n        self.sliding_window = getattr(config, \"sliding_window\", None)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n        attention_mask: Optional[torch.Tensor],\n        past_key_values: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs: Unpack[FlashAttentionKwargs],\n    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n        input_shape = hidden_states.shape[:-1]\n        hidden_shape = (*input_shape, -1, self.head_dim)\n\n        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n\n        cos, sin = position_embeddings\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_values is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        attention_interface: Callable = eager_attention_forward\n        if self.config._attn_implementation != \"eager\":\n            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n\n        attn_output, attn_weights = attention_interface(\n            self,\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            dropout=0.0 if not self.training else self.attention_dropout,\n            scaling=self.scaling,\n            sliding_window=self.sliding_window,  # diff with Llama\n            **kwargs,\n        )\n\n        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n        return attn_output, attn_weights"
                },
                "component_dependencies": {
                    "Qwen3MoeAttention": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs",
                        "transformers/modeling_utils.py#ALL_ATTENTION_FUNCTIONS",
                        "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#apply_rotary_pos_emb",
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#eager_attention_forward",
                        "transformers/processing_utils.py#Unpack"
                    ]
                },
                "warning": null
            },
            "transformers/utils/generic.py#OutputRecorder": {
                "sorted_modules": {
                    "OutputRecorder": "\n\n@dataclass\n@requires(backends=(\"torch\",))\nclass OutputRecorder:\n    \"\"\"\n    Configuration for recording outputs from a model via hooks.\n\n    Attributes:\n        target_class (Type): The class (e.g., nn.Module) to which the hook will be attached.\n        index (Optional[int]): If the output is a tuple/list, optionally record only at a specific index.\n        layer_name (Optional[str]): Name of the submodule to target (if needed), e.g., \"transformer.layer.3.attn\".\n        class_name (Optional[str]): Name of the class to which the hook will be attached. Could be the suffix of class name in some cases.\n    \"\"\"\n\n    target_class: \"type[torch.nn.Module]\"\n    index: int = 0\n    layer_name: str | None = None\n    class_name: str | None = None"
                },
                "component_dependencies": {
                    "OutputRecorder": [
                        "transformers/utils/import_utils.py#requires"
                    ]
                },
                "warning": null
            },
            "transformers/cache_utils.py#DynamicLayer": {
                "sorted_modules": {
                    "DynamicLayer": "\n\nclass DynamicLayer(CacheLayerMixin):\n    \"\"\"\n    A cache layer that grows dynamically as more tokens are generated. This is the default for generative models.\n    It stores the key and value states as tensors of shape `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    is_sliding = False\n\n    def lazy_initialization(self, key_states: torch.Tensor):\n        self.dtype, self.device = key_states.dtype, key_states.device\n        self.keys = torch.tensor([], dtype=self.dtype, device=self.device)\n        self.values = torch.tensor([], dtype=self.dtype, device=self.device)\n        self.is_initialized = True\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        cache_kwargs: Optional[dict[str, Any]] = None,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Update the key and value caches in-place, and return the necessary keys and value states.\n\n        Args:\n            key_states (`torch.Tensor`): The new key states to cache.\n            value_states (`torch.Tensor`): The new value states to cache.\n            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n\n        Returns:\n            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n        \"\"\"\n        # Lazy initialization\n        if not self.is_initialized:\n            self.lazy_initialization(key_states)\n\n        self.keys = torch.cat([self.keys, key_states], dim=-2)\n        self.values = torch.cat([self.values, value_states], dim=-2)\n        return self.keys, self.values\n\n    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n        kv_offset = 0\n        query_length = cache_position.shape[0]\n        kv_length = self.get_seq_length() + query_length\n        return kv_length, kv_offset\n\n    def get_seq_length(self) -> int:\n        \"\"\"Returns the sequence length of the cached states.\"\"\"\n        if not self.is_initialized or self.keys.numel() == 0:\n            return 0\n        return self.keys.shape[-2]\n\n    def get_max_cache_shape(self) -> int:\n        \"\"\"Returns the maximum sequence length of the cache object. DynamicLayer does not have a maximum length.\"\"\"\n        return -1\n\n    def crop(self, max_length: int) -> None:\n        \"\"\"\n        Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be negative\n        to remove `max_length` tokens.\n        \"\"\"\n        if max_length < 0:\n            max_length = self.get_seq_length() - abs(max_length)\n\n        if self.get_seq_length() <= max_length:\n            return\n\n        self.keys = self.keys[..., :max_length, :]\n        self.values = self.values[..., :max_length, :]\n\n    def batch_repeat_interleave(self, repeats: int) -> None:\n        \"\"\"Repeat the cache `repeats` times in the batch dimension.\"\"\"\n        if self.get_seq_length() > 0:\n            self.keys = self.keys.repeat_interleave(repeats, dim=0)\n            self.values = self.values.repeat_interleave(repeats, dim=0)\n\n    def batch_select_indices(self, indices: torch.Tensor) -> None:\n        \"\"\"Only keep the `indices` in the batch dimension of the cache.\"\"\"\n        if self.get_seq_length() > 0:\n            self.keys = self.keys[indices, ...]\n            self.values = self.values[indices, ...]"
                },
                "component_dependencies": {
                    "DynamicLayer": [
                        "transformers/cache_utils.py#CacheLayerMixin"
                    ]
                },
                "warning": null
            },
            "transformers/cache_utils.py#DynamicSlidingWindowLayer": {
                "sorted_modules": {
                    "DynamicSlidingWindowLayer": "\n\nclass DynamicSlidingWindowLayer(DynamicLayer):\n    \"\"\"\n    A cache layer that grows dynamically as more tokens are generated, up until the sliding window size.\n    It stores the key and value states as tensors of shape `[batch_size, num_heads, min(seq_len, sliding_window), head_dim]`.\n    \"\"\"\n\n    is_sliding = True\n\n    def __init__(self, sliding_window: int):\n        super().__init__()\n        self.sliding_window = sliding_window\n        self.cumulative_length = 0\n        self._sliding_window_tensor = torch.tensor(self.sliding_window, dtype=torch.long)\n\n    def lazy_initialization(self, key_states: torch.Tensor) -> None:\n        super().lazy_initialization(key_states)\n        self._sliding_window_tensor = self._sliding_window_tensor.to(self.device)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        cache_kwargs: Optional[dict[str, Any]] = None,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Update the key and value caches in-place, and return the necessary keys and value states.\n\n        Args:\n            key_states (`torch.Tensor`): The new key states to cache.\n            value_states (`torch.Tensor`): The new value states to cache.\n            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n\n        Returns:\n            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n        \"\"\"\n        # Lazy initialization\n        if not self.is_initialized:\n            self.lazy_initialization(key_states)\n\n        self.cumulative_length += key_states.shape[-2]\n\n        # Compute the full states\n        full_key_states = torch.cat([self.keys, key_states], dim=-2)\n        full_value_states = torch.cat([self.values, value_states], dim=-2)\n        # Only cache the last `self.sliding_window - 1` tokens (or all of them if lower than that)\n        self.keys = full_key_states[:, :, -self.sliding_window + 1 :, :]\n        self.values = full_value_states[:, :, -self.sliding_window + 1 :, :]\n\n        # Return the full states\n        return full_key_states, full_value_states\n\n    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n        \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n        query_length = cache_position.shape[0]\n        is_full = self.cumulative_length >= self.sliding_window\n\n        kv_offset = max(self.cumulative_length - self.sliding_window + 1, 0)\n        if is_full:\n            kv_length = self.sliding_window - 1 + query_length\n        else:\n            kv_length = self.cumulative_length + query_length\n\n        return kv_length, kv_offset\n\n    def get_seq_length(self) -> int:\n        \"\"\"Returns the sequence length of the cached states.\"\"\"\n        return self.cumulative_length\n\n    def get_max_cache_shape(self) -> int:\n        \"\"\"Return the maximum cache shape of the cache\"\"\"\n        return self.sliding_window\n\n    def crop(self, max_length: int) -> None:\n        \"\"\"\n        Crop the past key values up to a new `max_length` in terms of tokens. `max_length` can also be\n        negative to remove `max_length` tokens.\n        \"\"\"\n        if self.get_seq_length() >= self.sliding_window:\n            raise ValueError(\n                \"Cannot `crop` a `DynamicSlidingWindowLayer` after it has seen more tokens than its\"\n                \"sliding window (otherwise some states are lost)\"\n            )\n        super().crop(max_length)\n        self.cumulative_length = self.keys.shape[-2]"
                },
                "component_dependencies": {
                    "DynamicSlidingWindowLayer": [
                        "transformers/cache_utils.py#DynamicLayer"
                    ]
                },
                "warning": null
            },
            "transformers/configuration_utils.py#PreTrainedConfig": {
                "sorted_modules": {
                    "PreTrainedConfig": "\n\nclass PreTrainedConfig(PushToHubMixin):\n    # no-format\n    r\"\"\"\n    Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as\n    methods for loading/downloading/saving configurations.\n\n    <Tip>\n\n    A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to\n    initialize a model does **not** load the model weights. It only affects the model's configuration.\n\n    </Tip>\n\n    Class attributes (overridden by derived classes):\n\n    - **model_type** (`str`) -- An identifier for the model type, serialized into the JSON file, and used to recreate\n      the correct object in [`~transformers.AutoConfig`].\n    - **has_no_defaults_at_init** (`bool`) -- Whether the config class can be initialized without providing input arguments.\n      Some configurations requires inputs to be defined at init and have no default values, usually these are composite configs,\n      (but not necessarily) such as [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`]. They have to be initialized from\n      two or more configs of type [`~transformers.PreTrainedConfig`].\n    - **keys_to_ignore_at_inference** (`list[str]`) -- A list of keys to ignore by default when looking at dictionary\n      outputs of the model during inference.\n    - **attribute_map** (`dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n      naming of attributes.\n    - **base_model_tp_plan** (`dict[str, Any]`) -- A dict that maps sub-modules FQNs of a base model to a tensor\n      parallel plan applied to the sub-module when `model.tensor_parallel` is called.\n    - **base_model_pp_plan** (`dict[str, tuple[list[str]]]`) -- A dict that maps child-modules of a base model to a\n      pipeline parallel plan that enables users to place the child-module on the appropriate device.\n\n    Common attributes (present in all subclasses):\n\n    - **vocab_size** (`int`) -- The number of tokens in the vocabulary, which is also the first dimension of the\n      embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).\n    - **hidden_size** (`int`) -- The hidden size of the model.\n    - **num_attention_heads** (`int`) -- The number of attention heads used in the multi-head attention layers of the\n      model.\n    - **num_hidden_layers** (`int`) -- The number of blocks in the model.\n\n    <Tip warning={true}>\n\n    Setting parameters for sequence generation in the model config is deprecated. For backward compatibility, loading\n    some of them will still be possible, but attempting to overwrite them will throw an exception -- you should set\n    them in a [~transformers.GenerationConfig]. Check the documentation of [~transformers.GenerationConfig] for more\n    information about the individual parameters.\n\n    </Tip>\n\n    Arg:\n        name_or_path (`str`, *optional*, defaults to `\"\"`):\n            Store the string that was passed to [`PreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path`\n            if the configuration was created with such a method.\n        output_hidden_states (`bool`, *optional*, defaults to `False`):\n            Whether or not the model should return all hidden-states.\n        output_attentions (`bool`, *optional*, defaults to `False`):\n            Whether or not the model should returns all attentions.\n        return_dict (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return a [`~transformers.utils.ModelOutput`] instead of a plain tuple.\n        is_encoder_decoder (`bool`, *optional*, defaults to `False`):\n            Whether the model is used as an encoder/decoder or not.\n        is_decoder (`bool`, *optional*, defaults to `False`):\n            Whether to only use the decoder in an encoder-decoder architecture, otherwise it has no effect on\n            decoder-only or encoder-only architectures.\n        cross_attention_hidden_size (`bool`, *optional*):\n            The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder\n            setting and the cross-attention hidden dimension differs from `self.config.hidden_size`.\n        add_cross_attention (`bool`, *optional*, defaults to `False`):\n            Whether cross-attention layers should be added to the model. Note, this option is only relevant for models\n            that can be used as decoder models within the [`EncoderDecoderModel`] class, which consists of all models\n            in `AUTO_MODELS_FOR_CAUSAL_LM`.\n        tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n            and decoder model to have the exact same parameter names.\n        chunk_size_feed_forward (`int`, *optional*, defaults to `0`):\n            The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that\n            the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <\n            sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed\n            Forward Chunking work?](../glossary.html#feed-forward-chunking).\n\n        > Parameters for fine-tuning tasks\n\n        architectures (`list[str]`, *optional*):\n            Model architectures that can be used with the model pretrained weights.\n        finetuning_task (`str`, *optional*):\n            Name of the task used to fine-tune the model.\n        id2label (`dict[int, str]`, *optional*):\n            A map from index (for instance prediction index, or target index) to label.\n        label2id (`dict[str, int]`, *optional*):\n            A map from label to index for the model.\n        num_labels (`int`, *optional*):\n            Number of labels to use in the last layer added to the model, typically for a classification task.\n        task_specific_params (`dict[str, Any]`, *optional*):\n            Additional keyword arguments to store for the current task.\n        problem_type (`str`, *optional*):\n            Problem type for `XxxForSequenceClassification` models. Can be one of `\"regression\"`,\n            `\"single_label_classification\"` or `\"multi_label_classification\"`.\n\n        > Parameters linked to the tokenizer\n\n        tokenizer_class (`str`, *optional*):\n            The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the\n            model by default).\n        prefix (`str`, *optional*):\n            A specific prompt that should be added at the beginning of each text before calling the model.\n        bos_token_id (`int`, *optional*):\n            The id of the _beginning-of-stream_ token.\n        pad_token_id (`int`, *optional*):\n            The id of the _padding_ token.\n        eos_token_id (`int`, *optional*):\n            The id of the _end-of-stream_ token.\n        decoder_start_token_id (`int`, *optional*):\n            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.\n        sep_token_id (`int`, *optional*):\n            The id of the _separation_ token.\n\n        > PyTorch specific parameters\n\n        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n            Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the\n            model has a output word embedding layer.\n        dtype (`str`, *optional*):\n            The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`\n            (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved\n            model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load\n            `float16` weights.\n    \"\"\"\n\n    model_type: str = \"\"\n    base_config_key: str = \"\"\n    sub_configs: dict[str, type[\"PreTrainedConfig\"]] = {}\n    has_no_defaults_at_init: bool = False\n    attribute_map: dict[str, str] = {}\n    base_model_tp_plan: Optional[dict[str, Any]] = None\n    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n    base_model_ep_plan: Optional[dict[str, tuple[list[str]]]] = None\n    _auto_class: Optional[str] = None\n\n    def __setattr__(self, key, value):\n        if key in super().__getattribute__(\"attribute_map\"):\n            key = super().__getattribute__(\"attribute_map\")[key]\n        super().__setattr__(key, value)\n\n    def __getattribute__(self, key):\n        if key != \"attribute_map\" and key in super().__getattribute__(\"attribute_map\"):\n            key = super().__getattribute__(\"attribute_map\")[key]\n        return super().__getattribute__(key)\n\n    def __init__(\n        self,\n        *,\n        # All models common arguments\n        output_hidden_states: bool = False,\n        output_attentions: bool = False,\n        return_dict: bool = True,\n        dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n        # Common arguments\n        tie_word_embeddings: bool = True,\n        chunk_size_feed_forward: int = 0,\n        is_encoder_decoder: bool = False,\n        is_decoder: bool = False,\n        cross_attention_hidden_size: Optional[int] = None,\n        add_cross_attention: bool = False,\n        tie_encoder_decoder: bool = False,\n        # Fine-tuning task arguments\n        architectures: Optional[list[str]] = None,\n        finetuning_task: Optional[str] = None,\n        id2label: Optional[dict[int, str]] = None,\n        label2id: Optional[dict[str, int]] = None,\n        num_labels: Optional[int] = None,\n        task_specific_params: Optional[dict[str, Any]] = None,\n        problem_type: Optional[str] = None,\n        # Tokenizer kwargs\n        tokenizer_class: Optional[str] = None,\n        prefix: Optional[str] = None,\n        bos_token_id: Optional[int] = None,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[int] = None,\n        sep_token_id: Optional[int] = None,\n        decoder_start_token_id: Optional[int] = None,\n        **kwargs,\n    ):\n        # Validation for some arguments\n        if label2id is not None and not isinstance(label2id, dict):\n            raise ValueError(\"Argument label2id should be a dictionary.\")\n        if id2label is not None and not isinstance(id2label, dict):\n            raise ValueError(\"Argument id2label should be a dictionary.\")\n        if num_labels is not None and id2label is not None and len(id2label) != num_labels:\n            logger.warning(\n                f\"You passed `num_labels={num_labels}` which is incompatible to \"\n                f\"the `id2label` map of length `{len(id2label)}`.\"\n            )\n        if problem_type is not None and problem_type not in (\n            \"regression\",\n            \"single_label_classification\",\n            \"multi_label_classification\",\n        ):\n            raise ValueError(\n                f\"The config parameter `problem_type` was not understood: received {problem_type} \"\n                \"but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid.\"\n            )\n        # BC for the `torch_dtype` argument instead of the simpler `dtype`\n        # Do not warn, as it would otherwise always be triggered since most configs on the hub have `torch_dtype`\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            # If both are provided, keep `dtype`\n            dtype = dtype if dtype is not None else torch_dtype\n        if dtype is not None and isinstance(dtype, str) and is_torch_available():\n            # we will start using self.dtype in v5, but to be consistent with\n            # from_pretrained's dtype arg convert it to an actual torch.dtype object\n            import torch\n\n            dtype = getattr(torch, dtype)\n\n        # Attributes common for all models\n        self.return_dict = return_dict\n        self.output_hidden_states = output_hidden_states\n        self.dtype = dtype\n        self._output_attentions = output_attentions  # has public property\n\n        # Less common kwargs, only used by some models\n        self.tie_word_embeddings = tie_word_embeddings\n        self.chunk_size_feed_forward = chunk_size_feed_forward\n\n        # Encoder-decoder models attributes\n        self.is_encoder_decoder = is_encoder_decoder\n        self.is_decoder = is_decoder  # used in encoder-decoder models to differentiate encoder from decoder\n        self.cross_attention_hidden_size = cross_attention_hidden_size\n        self.add_cross_attention = add_cross_attention\n        self.tie_encoder_decoder = tie_encoder_decoder\n\n        # Fine-tuning task attributes\n        self.architectures = architectures\n        self.finetuning_task = finetuning_task\n        self.id2label = id2label\n        self.label2id = label2id\n        self.task_specific_params = task_specific_params\n        self.problem_type = problem_type\n\n        if self.id2label is None:\n            self._create_id_label_maps(num_labels if num_labels is not None else 2)\n        else:\n            # Keys are always strings in JSON so convert ids to int here.\n            self.id2label = {int(key): value for key, value in self.id2label.items()}\n\n        # Tokenizer attributes\n        self.tokenizer_class = tokenizer_class\n        self.prefix = prefix\n        self.bos_token_id = bos_token_id\n        self.pad_token_id = pad_token_id\n        self.eos_token_id = eos_token_id\n        self.sep_token_id = sep_token_id\n        self.decoder_start_token_id = decoder_start_token_id\n\n        # Retrocompatibility: Parameters for sequence generation. While we will keep the ability to load these\n        # parameters, saving them will be deprecated. In a distant future, we won't need to load them.\n        for parameter_name, default_value in self._get_global_generation_defaults().items():\n            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n\n        # Name or path to the pretrained checkpoint\n        self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n\n        # Attention implementation to use, if relevant (it sets it recursively on sub-configs)\n        self._attn_implementation = kwargs.pop(\"attn_implementation\", None)\n\n        # Drop the transformers version info\n        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n\n        # Deal with gradient checkpointing\n        if kwargs.get(\"gradient_checkpointing\", False):\n            warnings.warn(\n                \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n                \"Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the \"\n                \"`Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\"\n            )\n\n        # Additional attributes without default values\n        for key, value in kwargs.items():\n            try:\n                setattr(self, key, value)\n            except AttributeError as err:\n                logger.error(f\"Can't set {key} with value {value} for {self}\")\n                raise err\n\n    def _create_id_label_maps(self, num_labels: int):\n        self.id2label = {i: f\"LABEL_{i}\" for i in range(num_labels)}\n        self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))\n\n    @property\n    def name_or_path(self) -> Optional[str]:\n        return getattr(self, \"_name_or_path\", None)\n\n    @name_or_path.setter\n    def name_or_path(self, value):\n        self._name_or_path = str(value)  # Make sure that name_or_path is a string (for JSON encoding)\n\n    @property\n    def output_attentions(self):\n        \"\"\"\n        `bool`: Whether or not the model should returns all attentions.\n        \"\"\"\n        return self._output_attentions\n\n    @output_attentions.setter\n    def output_attentions(self, value: bool):\n        # If we set `output_attentions` explicitly before the attn implementation, dispatch eager\n        if value and self._attn_implementation is None:\n            self._attn_implementation = \"eager\"\n        if value and self._attn_implementation != \"eager\":\n            raise ValueError(\n                \"The `output_attentions` attribute is not supported when using the `attn_implementation` set to \"\n                f\"{self._attn_implementation}. Please set it to 'eager' instead.\"\n            )\n        self._output_attentions = value\n\n    @property\n    def use_return_dict(self) -> bool:\n        \"\"\"\n        `bool`: Whether or not return [`~utils.ModelOutput`] instead of tuples.\n        \"\"\"\n        return self.return_dict\n\n    @property\n    def num_labels(self) -> int:\n        \"\"\"\n        `int`: The number of labels for classification models.\n        \"\"\"\n        return len(self.id2label)\n\n    @num_labels.setter\n    def num_labels(self, num_labels: int):\n        # we do not store `num_labels` attribute in config, but instead\n        # compute it based on the length of the `id2label` map\n        if self.id2label is None or self.num_labels != num_labels:\n            self._create_id_label_maps(num_labels)\n\n    @property\n    def _attn_implementation(self):\n        return self._attn_implementation_internal\n\n    @_attn_implementation.setter\n    def _attn_implementation(self, value: str | dict | None):\n        \"\"\"We set it recursively on the sub-configs as well\"\"\"\n        # Set if for current config\n        current_attn = getattr(self, \"_attn_implementation\", None)\n        attn_implementation = value if not isinstance(value, dict) else value.get(\"\", current_attn)\n        self._attn_implementation_internal = attn_implementation\n\n        # Set it recursively on the subconfigs\n        for subconfig_key in self.sub_configs:\n            subconfig = getattr(self, subconfig_key, None)\n            if subconfig is not None:\n                current_subconfig_attn = getattr(subconfig, \"_attn_implementation\", None)\n                sub_implementation = (\n                    value if not isinstance(value, dict) else value.get(subconfig_key, current_subconfig_attn)\n                )\n                subconfig._attn_implementation = sub_implementation\n\n    @property\n    def torch_dtype(self):\n        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        return self.dtype\n\n    @torch_dtype.setter\n    def torch_dtype(self, value):\n        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n        self.dtype = value\n\n    @property\n    def rope_scaling(self):\n        return self.rope_parameters\n\n    @rope_scaling.setter\n    def rope_scaling(self, value):\n        self.rope_parameters = value\n\n    def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~PreTrainedConfig.from_pretrained`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n            push_to_hub (`bool`, *optional*, defaults to `False`):\n                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                namespace).\n            kwargs (`dict[str, Any]`, *optional*):\n                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        non_default_generation_parameters = self._get_non_default_generation_parameters()\n        if len(non_default_generation_parameters) > 0:\n            # TODO (joao): this should be an exception if the user has modified the loaded config. See #33886\n            warnings.warn(\n                \"Some non-default generation parameters are set in the model config. These should go into either a) \"\n                \"`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file \"\n                \"(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).\"\n                \"This warning will become an exception in the future.\"\n                f\"\\nNon-default generation parameters: {str(non_default_generation_parameters)}\",\n                UserWarning,\n            )\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        if push_to_hub:\n            commit_message = kwargs.pop(\"commit_message\", None)\n            repo_id = kwargs.pop(\"repo_id\", save_directory.split(os.path.sep)[-1])\n            repo_id = self._create_repo(repo_id, **kwargs)\n            files_timestamps = self._get_files_timestamps(save_directory)\n\n        # This attribute is important to know on load, but should not be serialized on save.\n        if \"transformers_weights\" in self:\n            delattr(self, \"transformers_weights\")\n\n        # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be\n        # loaded from the Hub.\n        if self._auto_class is not None:\n            custom_object_save(self, save_directory, config=self)\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n\n        self.to_json_file(output_config_file, use_diff=True)\n        logger.info(f\"Configuration saved in {output_config_file}\")\n\n        if push_to_hub:\n            self._upload_modified_files(\n                save_directory,\n                repo_id,\n                files_timestamps,\n                commit_message=commit_message,\n                token=kwargs.get(\"token\"),\n            )\n\n    @classmethod\n    def from_pretrained(\n        cls: type[SpecificPreTrainedConfigType],\n        pretrained_model_name_or_path: str | os.PathLike,\n        cache_dir: str | os.PathLike | None = None,\n        force_download: bool = False,\n        local_files_only: bool = False,\n        token: str | bool | None = None,\n        revision: str = \"main\",\n        **kwargs,\n    ) -> SpecificPreTrainedConfigType:\n        r\"\"\"\n        Instantiate a [`PreTrainedConfig`] (or a derived class) from a pretrained model configuration.\n\n        Args:\n            pretrained_model_name_or_path (`str` or `os.PathLike`):\n                This can be either:\n\n                - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n                  huggingface.co.\n                - a path to a *directory* containing a configuration file saved using the\n                  [`~PreTrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n                - a path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.\n            cache_dir (`str` or `os.PathLike`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force to (re-)download the configuration files and override the cached versions if\n                they exist.\n            proxies (`dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n            token (`str` or `bool`, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                the token generated when running `hf auth login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n\n                <Tip>\n\n                To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n\n                </Tip>\n\n            return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n                If `False`, then this function returns just the final configuration object.\n\n                If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\n                dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\n                part of `kwargs` which has not been used to update `config` and is otherwise ignored.\n            subfolder (`str`, *optional*, defaults to `\"\"`):\n                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n                specify the folder name here.\n            kwargs (`dict[str, Any]`, *optional*):\n                The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n                by the `return_unused_kwargs` keyword parameter.\n\n        Returns:\n            [`PreTrainedConfig`]: The configuration object instantiated from this pretrained model.\n\n        Examples:\n\n        ```python\n        # We can't instantiate directly the base class *PreTrainedConfig* so let's show the examples on a\n        # derived class: BertConfig\n        config = BertConfig.from_pretrained(\n            \"google-bert/bert-base-uncased\"\n        )  # Download configuration from huggingface.co and cache.\n        config = BertConfig.from_pretrained(\n            \"./test/saved_model/\"\n        )  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*\n        config = BertConfig.from_pretrained(\"./test/saved_model/my_configuration.json\")\n        config = BertConfig.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True, foo=False)\n        assert config.output_attentions == True\n        config, unused_kwargs = BertConfig.from_pretrained(\n            \"google-bert/bert-base-uncased\", output_attentions=True, foo=False, return_unused_kwargs=True\n        )\n        assert config.output_attentions == True\n        assert unused_kwargs == {\"foo\": False}\n        ```\"\"\"\n        kwargs[\"cache_dir\"] = cache_dir\n        kwargs[\"force_download\"] = force_download\n        kwargs[\"local_files_only\"] = local_files_only\n        kwargs[\"revision\"] = revision\n\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n        if cls.base_config_key and cls.base_config_key in config_dict:\n            config_dict = config_dict[cls.base_config_key]\n\n        if \"model_type\" in config_dict and hasattr(cls, \"model_type\") and config_dict[\"model_type\"] != cls.model_type:\n            # sometimes the config has no `base_config_key` if the config is used in several composite models\n            # e.g. LlamaConfig. In that case we try to see if there is match in `model_type` before raising a warning\n            for v in config_dict.values():\n                if isinstance(v, dict) and v.get(\"model_type\") == cls.model_type:\n                    config_dict = v\n\n            # raise warning only if we still can't see a match in `model_type`\n            if config_dict[\"model_type\"] != cls.model_type:\n                logger.warning(\n                    f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n                    f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n                )\n\n        return cls.from_dict(config_dict, **kwargs)\n\n    @classmethod\n    def get_config_dict(\n        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n    ) -> tuple[dict[str, Any], dict[str, Any]]:\n        \"\"\"\n        From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n        [`PreTrainedConfig`] using `from_dict`.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`):\n                The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n\n        Returns:\n            `tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.\n\n        \"\"\"\n        original_kwargs = copy.deepcopy(kwargs)\n        # Get config dict associated with the base config file\n        config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n        if config_dict is None:\n            return {}, kwargs\n        if \"_commit_hash\" in config_dict:\n            original_kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n        # That config file may point us toward another config file to use.\n        if \"configuration_files\" in config_dict:\n            configuration_file = get_configuration_file(config_dict[\"configuration_files\"])\n            config_dict, kwargs = cls._get_config_dict(\n                pretrained_model_name_or_path, _configuration_file=configuration_file, **original_kwargs\n            )\n\n        return config_dict, kwargs\n\n    @classmethod\n    def _get_config_dict(\n        cls, pretrained_model_name_or_path: str | os.PathLike, **kwargs\n    ) -> tuple[dict[str, Any], dict[str, Any]]:\n        cache_dir = kwargs.pop(\"cache_dir\", None)\n        force_download = kwargs.pop(\"force_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        token = kwargs.pop(\"token\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        revision = kwargs.pop(\"revision\", None)\n        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n        subfolder = kwargs.pop(\"subfolder\", \"\")\n        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n        from_auto_class = kwargs.pop(\"_from_auto\", False)\n        commit_hash = kwargs.pop(\"_commit_hash\", None)\n\n        gguf_file = kwargs.get(\"gguf_file\")\n\n        if trust_remote_code is True:\n            logger.warning(\n                \"The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\"\n                \" ignored.\"\n            )\n\n        user_agent = {\"file_type\": \"config\", \"from_auto_class\": from_auto_class}\n        if from_pipeline is not None:\n            user_agent[\"using_pipeline\"] = from_pipeline\n\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n\n        is_local = os.path.isdir(pretrained_model_name_or_path)\n        if os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n            # Special case when pretrained_model_name_or_path is a local file\n            resolved_config_file = pretrained_model_name_or_path\n            is_local = True\n        elif is_remote_url(pretrained_model_name_or_path):\n            configuration_file = pretrained_model_name_or_path if gguf_file is None else gguf_file\n            resolved_config_file = download_url(pretrained_model_name_or_path)\n        else:\n            configuration_file = kwargs.pop(\"_configuration_file\", CONFIG_NAME) if gguf_file is None else gguf_file\n\n            try:\n                # Load from local folder or from cache or download from model Hub and cache\n                resolved_config_file = cached_file(\n                    pretrained_model_name_or_path,\n                    configuration_file,\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    proxies=proxies,\n                    local_files_only=local_files_only,\n                    token=token,\n                    user_agent=user_agent,\n                    revision=revision,\n                    subfolder=subfolder,\n                    _commit_hash=commit_hash,\n                )\n                if resolved_config_file is None:\n                    return None, kwargs\n                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n            except OSError:\n                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                # the original exception.\n                raise\n            except Exception:\n                # For any other exception, we throw a generic error.\n                raise OSError(\n                    f\"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it\"\n                    \" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\n                    f\" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory\"\n                    f\" containing a {configuration_file} file\"\n                )\n\n        try:\n            if gguf_file:\n                config_dict = load_gguf_checkpoint(resolved_config_file, return_tensors=False)[\"config\"]\n            else:\n                # Load config dict\n                config_dict = cls._dict_from_json_file(resolved_config_file)\n\n            config_dict[\"_commit_hash\"] = commit_hash\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            raise OSError(f\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\")\n\n        if is_local:\n            logger.info(f\"loading configuration file {resolved_config_file}\")\n        else:\n            logger.info(f\"loading configuration file {configuration_file} from cache at {resolved_config_file}\")\n\n        # timm models are not saved with the model_type in the config file\n        if \"model_type\" not in config_dict and is_timm_config_dict(config_dict):\n            config_dict[\"model_type\"] = \"timm_wrapper\"\n\n        return config_dict, kwargs\n\n    @classmethod\n    def from_dict(\n        cls: type[SpecificPreTrainedConfigType], config_dict: dict[str, Any], **kwargs\n    ) -> SpecificPreTrainedConfigType:\n        \"\"\"\n        Instantiates a [`PreTrainedConfig`] from a Python dictionary of parameters.\n\n        Args:\n            config_dict (`dict[str, Any]`):\n                Dictionary that will be used to instantiate the configuration object. Such a dictionary can be\n                retrieved from a pretrained checkpoint by leveraging the [`~PreTrainedConfig.get_config_dict`] method.\n            kwargs (`dict[str, Any]`):\n                Additional parameters from which to initialize the configuration object.\n\n        Returns:\n            [`PreTrainedConfig`]: The configuration object instantiated from those parameters.\n        \"\"\"\n        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n        # Those arguments may be passed along for our internal telemetry.\n        # We remove them so they don't appear in `return_unused_kwargs`.\n        kwargs.pop(\"_from_auto\", None)\n        kwargs.pop(\"_from_pipeline\", None)\n        # The commit hash might have been updated in the `config_dict`, we don't want the kwargs to erase that update.\n        if \"_commit_hash\" in kwargs and \"_commit_hash\" in config_dict:\n            kwargs[\"_commit_hash\"] = config_dict[\"_commit_hash\"]\n\n        # For BC on the old `torch_dtype`\n        if (torch_dtype := kwargs.pop(\"torch_dtype\", None)) is not None:\n            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n            # If both are present, use `dtype`\n            kwargs[\"dtype\"] = kwargs.get(\"dtype\", torch_dtype)\n\n        # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\n        config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n\n        config = cls(**config_dict)\n\n        # Update config with kwargs if needed\n        if \"num_labels\" in kwargs and \"id2label\" in kwargs:\n            num_labels = kwargs[\"num_labels\"]\n            id2label = kwargs[\"id2label\"] if kwargs[\"id2label\"] is not None else []\n            if len(id2label) != num_labels:\n                raise ValueError(\n                    f\"You passed along `num_labels={num_labels}` with an incompatible id to label map: \"\n                    f\"{kwargs['id2label']}. Since those arguments are inconsistent with each other, you should remove \"\n                    \"one of them.\"\n                )\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(config, key):\n                current_attr = getattr(config, key)\n                # To authorize passing a custom subconfig as kwarg in models that have nested configs.\n                # We need to update only custom kwarg values instead and keep other attributes in subconfig.\n                if isinstance(current_attr, PreTrainedConfig) and isinstance(value, dict):\n                    current_attr_updated = current_attr.to_dict()\n                    current_attr_updated.update(value)\n                    value = current_attr.__class__(**current_attr_updated)\n                setattr(config, key, value)\n                if key != \"dtype\":\n                    to_remove.append(key)\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        logger.info(f\"Model config {config}\")\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    @classmethod\n    def from_json_file(\n        cls: type[SpecificPreTrainedConfigType], json_file: str | os.PathLike\n    ) -> SpecificPreTrainedConfigType:\n        \"\"\"\n        Instantiates a [`PreTrainedConfig`] from the path to a JSON file of parameters.\n\n        Args:\n            json_file (`str` or `os.PathLike`):\n                Path to the JSON file containing the parameters.\n\n        Returns:\n            [`PreTrainedConfig`]: The configuration object instantiated from that JSON file.\n\n        \"\"\"\n        config_dict = cls._dict_from_json_file(json_file)\n        return cls(**config_dict)\n\n    @classmethod\n    def _dict_from_json_file(cls, json_file: str | os.PathLike):\n        with open(json_file, encoding=\"utf-8\") as reader:\n            text = reader.read()\n        return json.loads(text)\n\n    def __eq__(self, other):\n        return isinstance(other, PreTrainedConfig) and (self.__dict__ == other.__dict__)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n\n    def __iter__(self):\n        yield from self.__dict__\n\n    def to_diff_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Removes all attributes from the configuration that correspond to the default config attributes for\n        better readability, while always retaining the `config` attribute from the class. Serializes to a\n        Python dictionary.\n\n        Returns:\n            dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        config_dict = self.to_dict()\n\n        # Get the default config dict (from a fresh PreTrainedConfig instance)\n        default_config_dict = PreTrainedConfig().to_dict()\n\n        # get class specific config dict\n        class_config_dict = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n\n        serializable_config_dict = {}\n\n        # Only serialize values that differ from the default config,\n        # except always keep the 'config' attribute.\n        for key, value in config_dict.items():\n            if (\n                isinstance(getattr(self, key, None), PreTrainedConfig)\n                and key in class_config_dict\n                and isinstance(class_config_dict[key], dict)\n            ):\n                # For nested configs we need to clean the diff recursively\n                diff = recursive_diff_dict(value, default_config_dict, config_obj=getattr(self, key, None))\n                if \"model_type\" in value:\n                    # Needs to be set even if it's not in the diff\n                    diff[\"model_type\"] = value[\"model_type\"]\n\n                serializable_config_dict[key] = diff\n            elif (\n                key not in default_config_dict\n                or key == \"transformers_version\"\n                or key == \"vocab_file\"\n                or value != default_config_dict[key]\n                or (key in default_config_dict and value != class_config_dict.get(key, value))\n            ):\n                serializable_config_dict[key] = value\n\n        self._remove_keys_not_serialized(serializable_config_dict)\n\n        # Key removed only in diff dict\n        if \"_name_or_path\" in serializable_config_dict:\n            del serializable_config_dict[\"_name_or_path\"]\n\n        if hasattr(self, \"quantization_config\"):\n            serializable_config_dict[\"quantization_config\"] = (\n                self.quantization_config.to_dict()\n                if not isinstance(self.quantization_config, dict)\n                else self.quantization_config\n            )\n        self.dict_dtype_to_str(serializable_config_dict)\n\n        return serializable_config_dict\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Serializes this instance to a Python dictionary.\n\n        Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        output = copy.deepcopy(self.__dict__)\n        if hasattr(self.__class__, \"model_type\"):\n            output[\"model_type\"] = self.__class__.model_type\n\n        # Transformers version when serializing the model\n        output[\"transformers_version\"] = __version__\n\n        for key, value in output.items():\n            # Deal with nested configs like CLIP\n            if isinstance(value, PreTrainedConfig):\n                value = value.to_dict()\n                del value[\"transformers_version\"]\n\n            output[key] = value\n\n        self._remove_keys_not_serialized(output)\n\n        if hasattr(self, \"quantization_config\"):\n            output[\"quantization_config\"] = (\n                self.quantization_config.to_dict()\n                if not isinstance(self.quantization_config, dict)\n                else self.quantization_config\n            )\n        self.dict_dtype_to_str(output)\n\n        return output\n\n    def to_json_string(self, use_diff: bool = True) -> str:\n        \"\"\"\n        Serializes this instance to a JSON string.\n\n        Args:\n            use_diff (`bool`, *optional*, defaults to `True`):\n                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                is serialized to JSON string.\n\n        Returns:\n            `str`: String containing all the attributes that make up this configuration instance in JSON format.\n        \"\"\"\n        if use_diff is True:\n            config_dict = self.to_diff_dict()\n        else:\n            config_dict = self.to_dict()\n        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n\n    def to_json_file(self, json_file_path: str | os.PathLike, use_diff: bool = True):\n        \"\"\"\n        Save this instance to a JSON file.\n\n        Args:\n            json_file_path (`str` or `os.PathLike`):\n                Path to the JSON file in which this configuration instance's parameters will be saved.\n            use_diff (`bool`, *optional*, defaults to `True`):\n                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                is serialized to JSON file.\n        \"\"\"\n        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n            writer.write(self.to_json_string(use_diff=use_diff))\n\n    def update(self, config_dict: dict[str, Any]):\n        \"\"\"\n        Updates attributes of this class with attributes from `config_dict`.\n\n        Args:\n            config_dict (`dict[str, Any]`): Dictionary of attributes that should be updated for this class.\n        \"\"\"\n        for key, value in config_dict.items():\n            setattr(self, key, value)\n\n    def update_from_string(self, update_str: str):\n        \"\"\"\n        Updates attributes of this class with attributes from `update_str`.\n\n        The expected format is ints, floats and strings as is, and for booleans use `true` or `false`. For example:\n        \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n\n        The keys to change have to already exist in the config object.\n\n        Args:\n            update_str (`str`): String with attributes that should be updated for this class.\n\n        \"\"\"\n\n        d = dict(x.split(\"=\") for x in update_str.split(\",\"))\n        for k, v in d.items():\n            if not hasattr(self, k):\n                raise ValueError(f\"key {k} isn't in the original config dict\")\n\n            old_v = getattr(self, k)\n            if isinstance(old_v, bool):\n                if v.lower() in [\"true\", \"1\", \"y\", \"yes\"]:\n                    v = True\n                elif v.lower() in [\"false\", \"0\", \"n\", \"no\"]:\n                    v = False\n                else:\n                    raise ValueError(f\"can't derive true or false from {v} (key {k})\")\n            elif isinstance(old_v, int):\n                v = int(v)\n            elif isinstance(old_v, float):\n                v = float(v)\n            elif not isinstance(old_v, str):\n                raise TypeError(\n                    f\"You can only update int, float, bool or string values in the config, got {v} for key {k}\"\n                )\n\n            setattr(self, k, v)\n\n    def dict_dtype_to_str(self, d: dict[str, Any]) -> None:\n        \"\"\"\n        Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\n        converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n        string, which can then be stored in the json format.\n        \"\"\"\n        if d.get(\"dtype\") is not None:\n            if isinstance(d[\"dtype\"], dict):\n                d[\"dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"dtype\"].items()}\n            # models like Emu3 can have \"dtype\" as token in config's vocabulary map,\n            # so we also exclude int type here to avoid error in this special case.\n            elif not isinstance(d[\"dtype\"], (str, int)):\n                d[\"dtype\"] = str(d[\"dtype\"]).split(\".\")[1]\n        for value in d.values():\n            if isinstance(value, dict):\n                self.dict_dtype_to_str(value)\n\n    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n        \"\"\"\n        Checks and removes if there are any keys in the dict that should not be serialized when saving the config.\n        Runs recursive check on the dict, to remove from all sub configs.\n        \"\"\"\n        if hasattr(self, \"quantization_config\"):\n            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n            _ = d.pop(\"_pre_quantization_dtype\", None)\n\n        if \"_auto_class\" in d:\n            del d[\"_auto_class\"]\n        if \"_output_attentions\" in d:\n            d[\"output_attentions\"] = d.pop(\"_output_attentions\")\n        if \"_commit_hash\" in d:\n            del d[\"_commit_hash\"]\n        if \"_attn_implementation_internal\" in d:\n            del d[\"_attn_implementation_internal\"]\n        # Do not serialize `base_model_tp_plan` for now\n        if \"base_model_tp_plan\" in d:\n            del d[\"base_model_tp_plan\"]\n        # Do not serialize `base_model_pp_plan` for now\n        if \"base_model_pp_plan\" in d:\n            del d[\"base_model_pp_plan\"]\n        for value in d.values():\n            if isinstance(value, dict):\n                self._remove_keys_not_serialized(value)\n\n    @classmethod\n    def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n        \"\"\"\n        Register this class with a given auto class. This should only be used for custom configurations as the ones in\n        the library are already mapped with `AutoConfig`.\n\n\n\n        Args:\n            auto_class (`str` or `type`, *optional*, defaults to `\"AutoConfig\"`):\n                The auto class to register this new configuration with.\n        \"\"\"\n        if not isinstance(auto_class, str):\n            auto_class = auto_class.__name__\n\n        import transformers.models.auto as auto_module\n\n        if not hasattr(auto_module, auto_class):\n            raise ValueError(f\"{auto_class} is not a valid auto class.\")\n\n        cls._auto_class = auto_class\n\n    @staticmethod\n    def _get_global_generation_defaults() -> dict[str, Any]:\n        return {\n            \"max_length\": 20,\n            \"min_length\": 0,\n            \"do_sample\": False,\n            \"early_stopping\": False,\n            \"num_beams\": 1,\n            \"temperature\": 1.0,\n            \"top_k\": 50,\n            \"top_p\": 1.0,\n            \"typical_p\": 1.0,\n            \"repetition_penalty\": 1.0,\n            \"length_penalty\": 1.0,\n            \"no_repeat_ngram_size\": 0,\n            \"encoder_no_repeat_ngram_size\": 0,\n            \"bad_words_ids\": None,\n            \"num_return_sequences\": 1,\n            \"output_scores\": False,\n            \"return_dict_in_generate\": False,\n            \"forced_bos_token_id\": None,\n            \"forced_eos_token_id\": None,\n            \"remove_invalid_values\": False,\n            \"exponential_decay_length_penalty\": None,\n            \"suppress_tokens\": None,\n            \"begin_suppress_tokens\": None,\n            # Deprecated arguments (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n            \"num_beam_groups\": 1,\n            \"diversity_penalty\": 0.0,\n        }\n\n    def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n        \"\"\"\n        Gets the non-default generation parameters on the PreTrainedConfig instance\n        \"\"\"\n        non_default_generation_parameters = {}\n        decoder_attribute_name = None\n\n        # Some composite models don't have a default config, use their decoder config as a fallback for default values\n        # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n        if not self.has_no_defaults_at_init:\n            default_config = self.__class__()\n        else:\n            decoder_config = self.get_text_config(decoder=True)\n            if decoder_config is not self:\n                default_config = decoder_config.__class__()\n            else:\n                default_config = None\n\n        # If it is a composite model, we want to check the subconfig that will be used for generation\n        self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n\n        for parameter_name, default_global_value in self._get_global_generation_defaults().items():\n            if hasattr(self_decoder_config, parameter_name):\n                is_default_in_config = is_default_generation_value = None\n                parameter_value = getattr(self_decoder_config, parameter_name)\n                # Three cases in which is okay for the model config to hold generation config parameters:\n                # 1. The parameter is set to `None`, effectively delegating its value to the generation config\n                if parameter_value is None:\n                    continue\n                # 2. If we have a default config, then the instance should hold the same generation defaults\n                if default_config is not None:\n                    is_default_in_config = parameter_value == getattr(default_config, parameter_name)\n                # 3. if we don't have a default config, then the instance should hold the global generation defaults\n                else:\n                    is_default_generation_value = parameter_value == default_global_value\n\n                is_non_default = (is_default_in_config is False) or (\n                    is_default_in_config is None and is_default_generation_value is False\n                )\n                if is_non_default:\n                    non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n\n        return non_default_generation_parameters\n\n    def get_text_config(self, decoder=None, encoder=None) -> \"PreTrainedConfig\":\n        \"\"\"\n        Returns the text config related to the text input (encoder) or text output (decoder) of the model. The\n        `decoder` and `encoder` input arguments can be used to specify which end of the model we are interested in,\n        which is useful on models that have both text input and output modalities.\n\n        There are three possible outcomes of using this method:\n        1. On most models, it returns the original config instance itself.\n        2. On newer (2024+) composite models, it returns the text section of the config, which is nested under a set\n            of valid names.\n        3. On older (2023-) composite models, it discards decoder-only parameters when `encoder=True` and vice-versa.\n\n        Args:\n            decoder (`Optional[bool]`, *optional*):\n                If set to `True`, then only search for decoder config names.\n            encoder (`Optional[bool]`, *optional*):\n                If set to `True`, then only search for encoder config names.\n        \"\"\"\n        return_both = decoder == encoder  # both unset or both set -> search all possible names\n\n        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n        encoder_possible_text_config_names = (\"text_encoder\",)\n        if return_both:\n            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n        elif decoder:\n            possible_text_config_names = decoder_possible_text_config_names\n        else:\n            possible_text_config_names = encoder_possible_text_config_names\n\n        valid_text_config_names = []\n        for text_config_name in possible_text_config_names:\n            if hasattr(self, text_config_name):\n                text_config = getattr(self, text_config_name, None)\n                if text_config is not None:\n                    valid_text_config_names += [text_config_name]\n\n        if len(valid_text_config_names) > 1:\n            raise ValueError(\n                f\"Multiple valid text configs were found in the model config: {valid_text_config_names}. In this \"\n                \"case, using `get_text_config()` would be ambiguous. Please specify the desired text config directly, \"\n                \"e.g. `text_config = config.sub_config_name`\"\n            )\n        elif len(valid_text_config_names) == 1:\n            config_to_return = getattr(self, valid_text_config_names[0])\n        else:\n            config_to_return = self\n\n        # handle legacy models with flat config structure, when we only want one of the configs\n        if not return_both and len(valid_text_config_names) == 0 and config_to_return.is_encoder_decoder:\n            config_to_return = copy.deepcopy(config_to_return)\n            prefix_to_discard = \"encoder\" if decoder else \"decoder\"\n            prefix_to_keep = \"decoder\" if decoder else \"encoder\"\n            for key in config_to_return.to_dict():\n                # NOTE: We don't want to discard the key if it is mapped from a different attribute name at read time\n                if key.startswith(prefix_to_discard) and key not in config_to_return.attribute_map.values():\n                    delattr(config_to_return, key)\n                if key.startswith(prefix_to_keep):\n                    # [encoder/decoder]_layers -> num_hidden_layers\n                    if key == prefix_to_keep + \"_layers\":\n                        new_key = \"num_hidden_layers\"\n                    # [encoder/decoder]_attention_heads -> num_attention_heads\n                    elif key == prefix_to_keep + \"_attention_heads\":\n                        new_key = \"num_attention_heads\"\n                    # e.g. encoder_hidden_act -> hidden_act\n                    else:\n                        new_key = key[len(prefix_to_keep) + 1 :]\n\n                    # Does the class map the new key into a different attribute name at read time? if so, let's write\n                    # into that attribute instead\n                    if new_key in config_to_return.attribute_map:\n                        new_key = config_to_return.attribute_map[new_key]\n\n                    value = getattr(config_to_return, key)\n                    delattr(config_to_return, key)\n                    setattr(config_to_return, new_key, value)\n\n        return config_to_return\n\n\n\ndef recursive_diff_dict(dict_a, dict_b, config_obj=None):\n    \"\"\"\n    Helper function to recursively take the diff between two nested dictionaries. The resulting diff only contains the\n    values from `dict_a` that are different from values in `dict_b`.\n\n    dict_b : the default config dictionary. We want to remove values that are in this one\n    \"\"\"\n    diff = {}\n    default = config_obj.__class__().to_dict() if config_obj is not None else {}\n    for key, value in dict_a.items():\n        obj_value = getattr(config_obj, str(key), None)\n        if isinstance(obj_value, PreTrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n            diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n            diff[key] = diff_value\n        elif key not in dict_b or (value != default[key]):\n            diff[key] = value\n    return diff"
                },
                "component_dependencies": {
                    "PreTrainedConfig": [
                        "transformers/__init__.py#__version__",
                        "transformers/configuration_utils.py#SpecificPreTrainedConfigType",
                        "transformers/configuration_utils.py#get_configuration_file",
                        "transformers/configuration_utils.py#logger",
                        "transformers/dynamic_module_utils.py#custom_object_save",
                        "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint",
                        "transformers/utils.py#CONFIG_NAME",
                        "transformers/utils.py#PushToHubMixin",
                        "transformers/utils.py#cached_file",
                        "transformers/utils.py#download_url",
                        "transformers/utils.py#extract_commit_hash",
                        "transformers/utils.py#is_remote_url",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils/generic.py#is_timm_config_dict"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6": {
                "sorted_modules": {
                    "_is_torch_greater_or_equal_than_2_6": "_is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)"
                },
                "component_dependencies": {
                    "_is_torch_greater_or_equal_than_2_6": [
                        "transformers/utils/import_utils.py#is_torch_greater_or_equal"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#_is_torch_xpu_available": {
                "sorted_modules": {
                    "_is_torch_xpu_available": "_is_torch_xpu_available = is_torch_xpu_available()"
                },
                "component_dependencies": {
                    "_is_torch_xpu_available": [
                        "transformers/utils.py#is_torch_xpu_available"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#_preprocess_mask_arguments": {
                "sorted_modules": {
                    "_preprocess_mask_arguments": "\n\ndef _preprocess_mask_arguments(\n    config: PreTrainedConfig,\n    input_embeds: torch.Tensor,\n    attention_mask: Optional[Union[torch.Tensor, BlockMask]],\n    cache_position: torch.Tensor,\n    past_key_values: Optional[Cache],\n    position_ids: Optional[torch.Tensor],\n    layer_idx: Optional[int],\n) -> tuple[bool, Optional[Union[torch.Tensor, BlockMask]], int, int]:\n    \"\"\"\n    Perform some common pre-processing of the mask arguments we get from the modeling code. Mostly determine the\n    key-value length and offsets, and if we should early exit or not.\n\n    Args:\n        config (`PreTrainedConfig`):\n            The model config.\n        input_embeds (`torch.Tensor`):\n            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n            batch size, query length and dtype.\n        attention_mask (`torch.Tensor`, optional):\n            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n            It can also be an already prepared 4D mask, in which case it is returned as-is.\n        cache_position (`torch.Tensor`):\n            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n        past_key_values (`Cache`, optional):\n            The past key values, if we use a cache.\n        position_ids (`torch.Tensor`, optional)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n        layer_idx (`int`, optional):\n            If `past_key_values` is not None, this is the layer index of the cache from which to get the key-value\n            length and offset. Indeed, for hybrid caches, different layers may return different lengths.\n\n    Returns:\n        early_exit (`bool`):\n            Whether we should early exit mask creation, and return the mask as-is.\n        attention_mask (`torch.Tensor` or `BlockMask` or `None`):\n            The attention mask to either return immediately, or to use in downstream mask creation.\n        packed_sequence_mask (`torch.Tensor`, optional):\n            In case we detected packed sequence format, this is a tensor where each similar integer indicates that\n            the tokens belong to the same sequence.\n        kv_length (`int`):\n            The size that the key and value states will have during the attention computation.\n        kv_offset (`int`):\n            An offset to indicate at which first position the key and values states will refer to.\n    \"\"\"\n    # If the mask is already 4D, simply return as-is (it was already prepared, or it is custom)\n    if isinstance(attention_mask, (torch.Tensor, BlockMask)) and len(attention_mask.shape) == 4:\n        return True, attention_mask, None, None, None\n\n    # For TGI/vLLM backends, or other custom attention without equivalent mask creation: we don't need a mask!\n    # Note: it's not ideal to check the `_global_mapping` attribute instead of the object itself, however otherwise\n    # full graph dynamo tracing (i.e. torch.export or compile with `fullgraph=True`) will fail on Python<3.11\n    # with `torch._dynamo.exc.Unsupported: 'inline in skipfiles:Mapping.__contains__ | __contains__, skipped\n    # according trace_rules.lookup SKIP_DIRS'` -- can be removed when we require Python>=3.11\n    if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS._global_mapping:\n        return True, None, None, None, None\n\n    # Move the mask to correct device, and potentially switch dtype for efficiency\n    if attention_mask is not None and attention_mask.ndim == 2:\n        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n\n    # If using a cache, it can give all information about mask sizes based on seen tokens\n    if past_key_values is not None:\n        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n    # Otherwise, the sizes are simply the input sizes\n    else:\n        kv_length, kv_offset = input_embeds.shape[1], 0\n\n    # We check the position_ids for potential packed sequence format (only if the 2D attention mask is explicitly None,\n    # and we don't have past_key_values, i.e. generally a training setup)\n    packed_sequence_mask = None\n    if position_ids is not None and attention_mask is None and past_key_values is None:\n        batch_size = input_embeds.shape[0]\n        # The position ids are sometimes just unsqueezed, without being expanded\n        if batch_size != position_ids.shape[0]:\n            position_ids = position_ids.expand(batch_size, -1)\n        packed_sequence_mask = find_packed_sequence_indices(position_ids)\n\n    return False, attention_mask, packed_sequence_mask, kv_length, kv_offset"
                },
                "component_dependencies": {
                    "_preprocess_mask_arguments": [
                        "transformers/cache_utils.py#Cache",
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/masking_utils.py#find_packed_sequence_indices"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#and_masks": {
                "sorted_modules": {
                    "and_masks": "\n\ndef and_masks(*mask_functions: Callable) -> Callable:\n    \"\"\"Returns a mask function that is the intersection of provided mask functions\"\"\"\n    if not all(callable(arg) for arg in mask_functions):\n        raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")\n\n    def and_mask(batch_idx, head_idx, q_idx, kv_idx):\n        result = q_idx.new_ones((), dtype=torch.bool)\n        for mask in mask_functions:\n            result = result & mask(batch_idx, head_idx, q_idx, kv_idx).to(result.device)\n        return result\n\n    return and_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#causal_mask_function": {
                "sorted_modules": {
                    "causal_mask_function": "\n\ndef causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n    \"\"\"\n    This creates a basic lower-diagonal causal mask.\n    \"\"\"\n    return kv_idx <= q_idx"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#or_masks": {
                "sorted_modules": {
                    "or_masks": "\n\ndef or_masks(*mask_functions: Callable) -> Callable:\n    \"\"\"Returns a mask function that is the union of provided mask functions\"\"\"\n    if not all(callable(arg) for arg in mask_functions):\n        raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")\n\n    def or_mask(batch_idx, head_idx, q_idx, kv_idx):\n        result = q_idx.new_zeros((), dtype=torch.bool)\n        for mask in mask_functions:\n            result = result | mask(batch_idx, head_idx, q_idx, kv_idx).to(result.device)\n        return result\n\n    return or_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#packed_sequence_mask_function": {
                "sorted_modules": {
                    "packed_sequence_mask_function": "\n\ndef packed_sequence_mask_function(packed_sequence_mask: torch.Tensor) -> Callable:\n    \"\"\"\n    This return the mask_function function corresponding to a 2D packed sequence mask.\n    \"\"\"\n\n    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n        return packed_sequence_mask[batch_idx, q_idx] == packed_sequence_mask[batch_idx, kv_idx]\n\n    return inner_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#sliding_window_causal_mask_function": {
                "sorted_modules": {
                    "sliding_window_causal_mask_function": "\n\ndef sliding_window_causal_mask_function(sliding_window: int) -> Callable:\n    \"\"\"\n    This return the mask_function function to create a sliding window mask.\n    \"\"\"\n    return and_masks(sliding_window_overlay(sliding_window), causal_mask_function)"
                },
                "component_dependencies": {
                    "sliding_window_causal_mask_function": [
                        "transformers/masking_utils.py#and_masks",
                        "transformers/masking_utils.py#causal_mask_function",
                        "transformers/masking_utils.py#sliding_window_overlay"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#RopeParameters": {
                "sorted_modules": {
                    "RopeParameters": "\n\nclass RopeParameters(TypedDict):\n    \"\"\"\n    Args:\n        rope_theta (`float`):\n            The base period of the RoPE embeddings.\n        rope_type (`str`, *optional*, defaults to \"default\"):\n            The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n            'llama3'], with 'default' being the original RoPE implementation.\n        factor (`float`, *optional*):\n            Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n            most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n            original maximum pre-trained length.\n        original_max_position_embeddings (`int`, *optional*):\n            Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n            pretraining.\n        attention_factor (`float`, *optional*):\n            Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n            computation. If unspecified, it defaults to value recommended by the implementation, using the\n            `factor` field to infer the suggested value.\n        beta_fast (`float`, *optional*):\n            Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n            ramp function. If unspecified, it defaults to 32.\n        beta_slow (`float`, *optional*):\n            Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n            ramp function. If unspecified, it defaults to 1.\n        short_factor (`list[float]`, *optional*):\n            Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n            `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n            size divided by the number of attention heads divided by 2\n        long_factor (`list[float]`, *optional*):\n            Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n            `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n            size divided by the number of attention heads divided by 2\n        low_freq_factor (`float`, *optional*):\n            Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n        high_freq_factor (`float`, *optional*):\n            Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n    \"\"\"\n\n    rope_theta: float\n    rope_type: Optional[str]\n    factor: Optional[float]\n    original_max_position_embeddings: Optional[int]\n    attention_factor: Optional[float]\n    beta_fast: Optional[float]\n    beta_slow: Optional[float]\n    short_factor: Optional[list[float]]\n    long_factor: Optional[list[float]]\n    low_freq_factor: Optional[float]\n    high_freq_factor: Optional[float]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_rope_utils.py#rope_config_validation": {
                "sorted_modules": {
                    "rope_config_validation": "\n\ndef rope_config_validation(config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n    \"\"\"\n    Validate the RoPE config arguments, given a `PreTrainedConfig` object\n    \"\"\"\n    rope_parameters_dict = getattr(config, \"rope_parameters\", None)  # not a default parameter in `PreTrainedConfig`\n    if rope_parameters_dict is None:\n        return\n\n    if getattr(config, \"layer_types\", None) is not None and all(\n        key in config.layer_types for key in rope_parameters_dict.keys()\n    ):\n        pass\n    else:\n        rope_parameters_dict = {\"full_attention\": rope_parameters_dict}\n\n    for rope_parameters in rope_parameters_dict.values():\n        rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n        validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n\n        rope_parameters[\"rope_type\"] = rope_type\n        # BC: \"rope_theta\" was originally saved in config\n        rope_parameters[\"rope_theta\"] = rope_parameters.get(\"rope_theta\", getattr(config, \"rope_theta\", None))\n\n        if validation_fn is not None:\n            validation_fn(rope_parameters, config=config, ignore_keys=ignore_keys)\n        else:\n            logger.warning(\n                f\"Missing validation function mapping in `ROPE_VALIDATION_FUNCTIONS` for 'rope_type'='{rope_type}'\"\n            )"
                },
                "component_dependencies": {
                    "rope_config_validation": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#standardize_rope_params": {
                "sorted_modules": {
                    "standardize_rope_params": "\n\ndef standardize_rope_params(config, rope_theta: float | dict[str, float] | None = None):\n    \"\"\"\n    Helper to standardize the config's rope params field by ensuring the params are defined for each\n    later type. For old model the fn will duplicate a single rope param in each layer type (backward compatibility)\n    \"\"\"\n    rope_parameters = getattr(config, \"rope_parameters\", None)\n    layer_types = getattr(config, \"layer_types\", None)\n    if rope_theta is None:\n        rope_theta = getattr(config, \"rope_theta\", None)\n\n    # Case 1: one RoPE theat = one RoPE param per model without nesting\n    if not isinstance(rope_theta, dict):\n        if rope_parameters is None:\n            rope_parameters = {\"rope_type\": \"default\", \"rope_theta\": rope_theta}\n        else:\n            # BC: if there is a 'type' field, copy it it to 'rope_type'.\n            rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\", \"default\"))\n            rope_theta = rope_parameters.get(\"rope_theta\") or rope_theta\n            rope_parameters.update({\"rope_theta\": rope_theta, \"rope_type\": rope_type})\n        config.rope_parameters = rope_parameters\n\n    # Case 2: different RoPE for each layer as nested dict\n    else:\n        rope_parameters_per_layer_type = {}\n        for layer_type in layer_types:\n            if rope_parameters is None:\n                rope_parameters_per_layer_type[layer_type] = {\n                    \"rope_type\": \"default\",\n                    \"rope_theta\": rope_theta[layer_type],\n                }\n            else:\n                is_field_in_new_format = any(layer_type in rope_parameters for layer_type in layer_types)\n                if not is_field_in_new_format:\n                    curr_rope_type = rope_parameters.get(\"rope_type\", rope_parameters.get(\"type\"))\n                    rope_parameters_per_layer_type[layer_type] = {\n                        **rope_parameters,\n                        \"rope_type\": curr_rope_type,\n                        \"rope_theta\": rope_theta[layer_type],\n                    }\n                else:\n                    curr_rope_type = rope_parameters[layer_type].get(\n                        \"rope_type\", rope_parameters[layer_type].get(\"type\")\n                    )\n                    rope_parameters_per_layer_type[layer_type] = {\n                        **rope_parameters[layer_type],\n                        \"rope_type\": curr_rope_type,\n                        \"rope_theta\": rope_theta[layer_type],\n                    }\n            config.rope_parameters = rope_parameters_per_layer_type"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS": {
                "sorted_modules": {
                    "ROPE_INIT_FUNCTIONS": "\n\n# This maps the \"rope_type\" string field in rope config to the corresponding function to compute the RoPE parameters\n# from the model config. You can append new {'rope_type': callable} pairs to this rope_parameters to enable custom RoPE\n# parameterizations, as long as the callable has the same signature.\nROPE_INIT_FUNCTIONS = {\n    \"linear\": _compute_linear_scaling_rope_parameters,\n    \"dynamic\": _compute_dynamic_ntk_parameters,\n    \"yarn\": _compute_yarn_parameters,\n    \"longrope\": _compute_longrope_parameters,\n    \"llama3\": _compute_llama3_parameters,\n}"
                },
                "component_dependencies": {
                    "ROPE_INIT_FUNCTIONS": [
                        "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                        "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                        "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                        "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                        "transformers/modeling_rope_utils.py#_compute_yarn_parameters"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#dynamic_rope_update": {
                "sorted_modules": {
                    "dynamic_rope_update": "\n\ndef dynamic_rope_update(rope_forward):\n    \"\"\"\n    Decorator function to update the RoPE parameters in the forward pass, if the model is using a dynamic RoPE\n    (i.e. a RoPE implementation that may recompute its frequencies in the forward pass).\n\n    Args:\n        rope_forward (Callable):\n            The forward pass of the RoPE implementation.\n\n    Returns:\n        The decorated forward pass.\n    \"\"\"\n\n    def longrope_frequency_update(self, position_ids, device, layer_type=None):\n        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n        seq_len = torch.max(position_ids) + 1\n        original_max_position_embeddings = getattr(\n            self.config, \"original_max_position_embeddings\", self.config.max_position_embeddings\n        )\n        if layer_type is None:\n            rope_type = self.rope_type\n            original_inv_freq = self.original_inv_freq\n            prefix = \"\"\n        else:\n            rope_type = self.rope_type[layer_type]\n            original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n            prefix = f\"{layer_type}_\"\n\n        if seq_len > original_max_position_embeddings:\n            if not hasattr(self, f\"{layer_type}_long_inv_freq\"):\n                rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n                long_inv_freq, _ = rope_init_fn(\n                    self.config,\n                    device,\n                    seq_len=original_max_position_embeddings + 1,\n                    layer_type=layer_type,\n                )\n            self.register_buffer(f\"{prefix}inv_freq\", long_inv_freq, persistent=False)\n            setattr(self, f\"{prefix}long_inv_freq\", long_inv_freq)\n        else:\n            # This .to() is needed if the model has been moved to a device after being initialized (because\n            # the buffer is automatically moved, but not the original copy)\n            original_inv_freq = original_inv_freq.to(device)\n            self.register_buffer(f\"{prefix}inv_freq\", original_inv_freq, persistent=False)\n            setattr(self, f\"{prefix}original_inv_freq\", original_inv_freq)\n\n    def dynamic_frequency_update(self, position_ids, device, layer_type=None):\n        \"\"\"\n        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n        1 - growing beyond the cached sequence length (allow scaling)\n        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n        \"\"\"\n        seq_len = torch.max(position_ids) + 1\n        if layer_type is None:\n            rope_type = self.rope_type\n            max_seq_len_cached = self.max_seq_len_cached\n            original_inv_freq = self.original_inv_freq\n            prefix = \"\"\n        else:\n            rope_type = self.rope_type[layer_type]\n            max_seq_len_cached = getattr(self, f\"{layer_type}_max_seq_len_cached\", self.max_seq_len_cached)\n            original_inv_freq = getattr(self, f\"{layer_type}_original_inv_freq\")\n            prefix = f\"{layer_type}_\"\n\n        if seq_len > max_seq_len_cached:  # growth\n            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n            inv_freq, self.attention_scaling = rope_init_fn(\n                self.config,\n                device,\n                seq_len=seq_len,\n                layer_type=layer_type,\n            )\n            # TODO joao: may break with compilation\n            self.register_buffer(f\"{prefix}inv_freq\", inv_freq, persistent=False)\n            setattr(self, f\"{layer_type}_max_seq_len_cached\", seq_len)\n\n        if seq_len < self.original_max_seq_len and max_seq_len_cached > self.original_max_seq_len:  # reset\n            # This .to() is needed if the model has been moved to a device after being initialized (because\n            # the buffer is automatically moved, but not the original copy)\n            original_inv_freq = original_inv_freq.to(device)\n            self.register_buffer(f\"{prefix}inv_freq\", original_inv_freq, persistent=False)\n            setattr(self, f\"{prefix}original_inv_freq\", original_inv_freq)\n            setattr(self, f\"{layer_type}_max_seq_len_cached\", self.original_max_seq_len)\n\n    @wraps(rope_forward)\n    def wrapper(self, x, position_ids, layer_type=None):\n        rope_type = self.rope_type if layer_type is None else self.rope_type[layer_type]\n        kwargs = {\"layer_type\": layer_type} if layer_type is not None else {}\n        if \"dynamic\" in rope_type:\n            dynamic_frequency_update(self, position_ids, device=x.device, **kwargs)\n        elif rope_type == \"longrope\":\n            longrope_frequency_update(self, position_ids, device=x.device, **kwargs)\n        return rope_forward(self, x, position_ids, **kwargs)\n\n    return wrapper"
                },
                "component_dependencies": {
                    "dynamic_rope_update": [
                        "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS"
                    ]
                },
                "warning": null
            },
            "transformers/utils/generic.py#_CAN_RECORD_REGISTRY": {
                "sorted_modules": {
                    "_CAN_RECORD_REGISTRY": "\n\n_CAN_RECORD_REGISTRY = {}"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/dynamic_module_utils.py#custom_object_save": {
                "sorted_modules": {
                    "custom_object_save": "\n\ndef custom_object_save(obj: Any, folder: Union[str, os.PathLike], config: Optional[dict] = None) -> list[str]:\n    \"\"\"\n    Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally\n    adds the proper fields in a config.\n\n    Args:\n        obj (`Any`): The object for which to save the module files.\n        folder (`str` or `os.PathLike`): The folder where to save.\n        config (`PreTrainedConfig` or dictionary, `optional`):\n            A config in which to register the auto_map corresponding to this custom object.\n\n    Returns:\n        `list[str]`: The list of files saved.\n    \"\"\"\n    if obj.__module__ == \"__main__\":\n        logger.warning(\n            f\"We can't save the code defining {obj} in {folder} as it's been defined in __main__. You should put \"\n            \"this code in a separate module so we can include it in the saved folder and make it easier to share via \"\n            \"the Hub.\"\n        )\n        return\n\n    def _set_auto_map_in_config(_config):\n        module_name = obj.__class__.__module__\n        last_module = module_name.split(\".\")[-1]\n        full_name = f\"{last_module}.{obj.__class__.__name__}\"\n        # Special handling for tokenizers\n        if \"Tokenizer\" in full_name:\n            slow_tokenizer_class = None\n            fast_tokenizer_class = None\n            if obj.__class__.__name__.endswith(\"Fast\"):\n                # Fast tokenizer: we have the fast tokenizer class and we may have the slow one has an attribute.\n                fast_tokenizer_class = f\"{last_module}.{obj.__class__.__name__}\"\n                if getattr(obj, \"slow_tokenizer_class\", None) is not None:\n                    slow_tokenizer = getattr(obj, \"slow_tokenizer_class\")\n                    slow_tok_module_name = slow_tokenizer.__module__\n                    last_slow_tok_module = slow_tok_module_name.split(\".\")[-1]\n                    slow_tokenizer_class = f\"{last_slow_tok_module}.{slow_tokenizer.__name__}\"\n            else:\n                # Slow tokenizer: no way to have the fast class\n                slow_tokenizer_class = f\"{last_module}.{obj.__class__.__name__}\"\n\n            full_name = (slow_tokenizer_class, fast_tokenizer_class)\n\n        if isinstance(_config, dict):\n            auto_map = _config.get(\"auto_map\", {})\n            auto_map[obj._auto_class] = full_name\n            _config[\"auto_map\"] = auto_map\n        elif getattr(_config, \"auto_map\", None) is not None:\n            _config.auto_map[obj._auto_class] = full_name\n        else:\n            _config.auto_map = {obj._auto_class: full_name}\n\n    # Add object class to the config auto_map\n    if isinstance(config, (list, tuple)):\n        for cfg in config:\n            _set_auto_map_in_config(cfg)\n    elif config is not None:\n        _set_auto_map_in_config(config)\n\n    result = []\n    # Copy module file to the output folder.\n    object_file = sys.modules[obj.__module__].__file__\n    dest_file = Path(folder) / (Path(object_file).name)\n    shutil.copy(object_file, dest_file)\n    result.append(dest_file)\n\n    # Gather all relative imports recursively and make sure they are copied as well.\n    for needed_file in get_relative_import_files(object_file):\n        dest_file = Path(folder) / (Path(needed_file).name)\n        shutil.copy(needed_file, dest_file)\n        result.append(dest_file)\n\n    return result"
                },
                "component_dependencies": {
                    "custom_object_save": [
                        "transformers/dynamic_module_utils.py#get_relative_import_files",
                        "transformers/dynamic_module_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#_get_device_map": {
                "sorted_modules": {
                    "_get_device_map": "\n\ndef _get_device_map(\n    model: \"PreTrainedModel\",\n    device_map: dict | str | None,\n    max_memory: dict | None,\n    hf_quantizer: \"HfQuantizer | None\",\n    dtype: torch.dtype | None,\n) -> dict:\n    \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n    Otherwise, we check for any device inconsistencies in the device_map.\n    \"\"\"\n    if isinstance(device_map, str):\n        special_dtypes = {}\n        if hf_quantizer is not None:\n            special_dtypes = hf_quantizer.get_special_dtypes_update(model, dtype)\n\n        target_dtype = dtype\n        if hf_quantizer is not None:\n            target_dtype = hf_quantizer.adjust_target_dtype(target_dtype)\n\n        no_split_modules = model._get_no_split_modules(device_map)\n\n        if device_map != \"sequential\":\n            inferred_max_memory = get_balanced_memory(\n                model,\n                max_memory=max_memory,\n                no_split_module_classes=no_split_modules,\n                hf_quantizer=hf_quantizer,\n                low_zero=(device_map == \"balanced_low_0\"),\n            )\n        else:\n            inferred_max_memory = get_max_memory(max_memory)\n        if hf_quantizer is not None:\n            inferred_max_memory = hf_quantizer.adjust_max_memory(inferred_max_memory)\n\n        # `inferred_max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU,\n        # which we can use to allocate parameters.\n        for device_name in inferred_max_memory:\n            if isinstance(device_name, int):  # it's a GPU device\n                if is_torch_xpu_available():\n                    unused_memory = torch.xpu.memory_reserved(device_name) - torch.xpu.memory_allocated(device_name)\n                else:\n                    unused_memory = torch.cuda.memory_reserved(device_name) - torch.cuda.memory_allocated(device_name)\n                inferred_max_memory[device_name] += unused_memory\n            # respect the `max_memory` passed by the user\n            if max_memory is not None and device_name in max_memory:\n                inferred_max_memory[device_name] = min(inferred_max_memory[device_name], max_memory[device_name])\n\n        device_map = infer_auto_device_map(\n            model,\n            max_memory=inferred_max_memory,\n            dtype=target_dtype,\n            no_split_module_classes=no_split_modules,\n            special_dtypes=special_dtypes,\n        )\n\n        if hf_quantizer is not None:\n            hf_quantizer.validate_environment(device_map=device_map)\n\n    elif device_map is not None:\n        tied_params = find_tied_parameters(model)\n        # check if we don't have tied param in different devices\n        check_tied_parameters_on_same_device(tied_params, device_map)\n\n    return device_map"
                },
                "component_dependencies": {
                    "_get_device_map": [
                        "transformers/integrations/accelerate.py#find_tied_parameters",
                        "transformers/integrations/accelerate.py#get_balanced_memory",
                        "transformers/utils.py#is_torch_xpu_available"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#accelerate_disk_offload": {
                "sorted_modules": {
                    "accelerate_disk_offload": "\n\ndef accelerate_disk_offload(\n    disk_offload_folder,\n    checkpoint_files,\n    device_map,\n    checkpoint_keys,\n    key_renaming_mapping,\n    sharded_metadata,\n    dtype,\n    reverse_key_renaming_mapping,\n):\n    disk_only_shard_files = []\n    if disk_offload_folder is not None:\n        os.makedirs(disk_offload_folder, exist_ok=True)\n    is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n    if disk_offload_folder is None and not is_offloaded_safetensors:\n        raise ValueError(\n            \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\"\n            \" for them. Alternatively, make sure you have `safetensors` installed if the model you are using\"\n            \" offers the weights in this format.\"\n        )\n    if is_offloaded_safetensors:\n        param_device_map = expand_device_map(device_map, checkpoint_keys)\n        str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n        if sharded_metadata is None:\n            weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n        else:\n            folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n            # Fix the weight map keys according to the key mapping\n            weight_map = {\n                key_renaming_mapping[k]: v\n                for k, v in sharded_metadata[\"weight_map\"].items()\n                if k in key_renaming_mapping\n            }\n            weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n            # Find potential checkpoints containing only offloaded weights\n            disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n        disk_offload_index = {\n            name: {\n                \"safetensors_file\": file,\n                \"weight_name\": reverse_key_renaming_mapping[name],\n                \"dtype\": str_dtype,\n            }\n            for name, file in weight_map.items()\n            if param_device_map[name] == \"disk\"\n        }\n    else:\n        disk_offload_index = {}\n    return disk_offload_index, disk_only_shard_files, is_offloaded_safetensors"
                },
                "component_dependencies": {
                    "accelerate_disk_offload": [
                        "transformers/integrations/accelerate.py#expand_device_map",
                        "transformers/integrations/accelerate.py#get_disk_only_shard_files"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#accelerate_dispatch": {
                "sorted_modules": {
                    "accelerate_dispatch": "\n\ndef accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers):\n    device_map_kwargs = {\n        \"device_map\": device_map,\n        \"offload_dir\": offload_folder,\n        \"offload_index\": offload_index,\n        \"offload_buffers\": offload_buffers,\n    }\n    if \"skip_keys\" in inspect.signature(dispatch_model).parameters:\n        device_map_kwargs[\"skip_keys\"] = model._skip_keys_device_placement\n    # For HQQ method we force-set the hooks for single GPU envs\n    if (\n        \"force_hooks\" in inspect.signature(dispatch_model).parameters\n        and hf_quantizer is not None\n        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.HQQ\n    ):\n        device_map_kwargs[\"force_hooks\"] = True\n    if (\n        hf_quantizer is not None\n        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.FBGEMM_FP8\n        and isinstance(device_map, dict)\n        and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n    ):\n        device_map_kwargs[\"offload_buffers\"] = True\n\n    if not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n        dispatch_model(model, **device_map_kwargs)"
                },
                "component_dependencies": {
                    "accelerate_dispatch": [
                        "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                        "transformers/integrations/fsdp.py#is_fsdp_enabled",
                        "transformers/utils/quantization_config.py#QuantizationMethod.FBGEMM_FP8",
                        "transformers/utils/quantization_config.py#QuantizationMethod.HQQ"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#check_and_set_device_map": {
                "sorted_modules": {
                    "check_and_set_device_map": "\n\ndef check_and_set_device_map(device_map: \"torch.device | int | str | dict | None\") -> dict | str | None:\n    from ..modeling_utils import get_torch_context_manager_or_global_device\n\n    # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n    if device_map is None and not is_deepspeed_zero3_enabled():\n        device_in_context = get_torch_context_manager_or_global_device()\n        if device_in_context == torch.device(\"meta\"):\n            raise RuntimeError(\n                \"You are using `from_pretrained` with a meta device context manager or `torch.set_default_device('meta')`.\\n\"\n                \"This is an anti-pattern as `from_pretrained` wants to load existing weights.\\nIf you want to initialize an \"\n                \"empty model on the meta device, use the context manager or global device with `from_config`, or `ModelClass(config)`\"\n            )\n        device_map = device_in_context\n\n    # change device_map into a map if we passed an int, a str or a torch.device\n    if isinstance(device_map, torch.device):\n        device_map = {\"\": device_map}\n    elif isinstance(device_map, str) and device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:\n        try:\n            device_map = {\"\": torch.device(device_map)}\n        except RuntimeError:\n            raise ValueError(\n                \"When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \"\n                f\"'auto', 'balanced', 'balanced_low_0', 'sequential' but found {device_map}.\"\n            )\n    elif isinstance(device_map, int):\n        if device_map < 0:\n            raise ValueError(\n                \"You can't pass device_map as a negative int. If you want to put the model on the cpu, pass device_map = 'cpu' \"\n            )\n        else:\n            device_map = {\"\": device_map}\n\n    if device_map is not None:\n        if is_deepspeed_zero3_enabled():\n            raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n        if not is_accelerate_available():\n            raise ValueError(\n                \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n                \"requires `accelerate`. You can install it with `pip install accelerate`\"\n            )\n    return device_map"
                },
                "component_dependencies": {
                    "check_and_set_device_map": [
                        "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                        "transformers/utils.py#is_accelerate_available"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#expand_device_map": {
                "sorted_modules": {
                    "expand_device_map": "\n\ndef expand_device_map(device_map, param_names):\n    \"\"\"\n    Expand a device map to return the correspondence parameter name to device.\n    \"\"\"\n    new_device_map = {}\n    for module, device in device_map.items():\n        new_device_map.update(\n            {p: device for p in param_names if p == module or p.startswith(f\"{module}.\") or module == \"\"}\n        )\n    return new_device_map"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/accelerate.py#init_empty_weights": {
                "sorted_modules": {
                    "init_empty_weights": "\n\n@contextmanager\ndef init_empty_weights(include_buffers: bool = False):\n    \"\"\"\n    A context manager under which models are initialized with all parameters on the meta device, therefore creating an\n    empty model. Useful when just initializing the model would blow the available RAM.\n\n    Args:\n        include_buffers (`bool`, *optional*):\n            Whether or not to also put all buffers on the meta device while initializing.\n\n    Example:\n\n    ```python\n    import torch.nn as nn\n    from accelerate import init_empty_weights\n\n    # Initialize a model with 100 billions parameters in no time and without using any RAM.\n    with init_empty_weights():\n        tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n    ```\n\n    <Tip warning={true}>\n\n    Any model created under this context manager has no weights. As such you can't do something like\n    `model.to(some_device)` with it. To load weights inside your empty model, see [`load_checkpoint_and_dispatch`].\n    Make sure to overwrite the default device_map param for [`load_checkpoint_and_dispatch`], otherwise dispatch is not\n    called.\n\n    </Tip>\n    \"\"\"\n    with init_on_device(torch.device(\"meta\"), include_buffers=include_buffers) as f:\n        yield f"
                },
                "component_dependencies": {
                    "init_empty_weights": [
                        "transformers/integrations/accelerate.py#init_on_device"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/hub_kernels.py#is_kernel": {
                "sorted_modules": {
                    "is_kernel": "\n\ndef is_kernel(attn_implementation: Optional[str]) -> bool:\n    \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n    return (\n        attn_implementation is not None\n        and re.search(r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", attn_implementation) is not None\n    )"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel": {
                "sorted_modules": {
                    "load_and_register_attn_kernel": "\n\ndef load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: Optional[Callable] = None) -> None:\n    \"\"\"\n    Load and register the kernel associated to `attn_implementation`.\n\n    Args:\n        attn_implementation: A string, usually a kernel repo like \"kernels-community/flash-mla\".\n        attn_wrapper: a callable for the wrapper around the attention implementation. In `transformers` we\n            have a wrapper around the `flash_attn_var_len` call, and the same goes for `sdpa` and `eager`.\n            They just prepare the arguments properly. This is mostly used for continious batching, where we\n            want the `paged` wrapper, which calls the paged cache.\n    \"\"\"\n    from ..masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n    from ..modeling_utils import ALL_ATTENTION_FUNCTIONS\n\n    actual_attn_name = attn_implementation.split(\"|\")[1] if \"|\" in attn_implementation else attn_implementation\n    if not is_kernel(actual_attn_name):\n        return\n    if not _kernels_available:\n        raise ImportError(\n            \"`kernels` is either not installed or uses an incompatible version. \"\n            \"Please install the latest version with `pip install -U kernels`.\"\n        )\n\n    # Extract repo_id and kernel_name from the string\n    if \":\" in actual_attn_name:\n        repo_id, kernel_name = actual_attn_name.split(\":\")\n        kernel_name = kernel_name.strip()\n    else:\n        repo_id = actual_attn_name\n        kernel_name = None\n    repo_id = repo_id.strip()\n    # extract the rev after the @ if it exists\n    repo_id, _, rev = repo_id.partition(\"@\")\n    repo_id = repo_id.strip()\n    rev = rev.strip() if rev else None\n\n    # Load the kernel from hub\n    try:\n        kernel = get_kernel(repo_id, revision=rev)\n    except Exception as e:\n        raise ValueError(f\"An error occurred while trying to load from '{repo_id}': {e}.\")\n    # correctly wrap the kernel\n    if hasattr(kernel, \"flash_attn_varlen_func\"):\n        if attention_wrapper is None:\n            attention_wrapper = flash_attention_forward\n        kernel_function = partial(attention_wrapper, implementation=kernel)\n        lazy_import_flash_attention(kernel, force_import=True)\n    elif kernel_name is not None:\n        kernel_function = getattr(kernel, kernel_name)\n    # Register the kernel as a valid attention\n    ALL_ATTENTION_FUNCTIONS.register(attn_implementation, kernel_function)\n    ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])"
                },
                "component_dependencies": {
                    "load_and_register_attn_kernel": [
                        "transformers/integrations/flash_attention.py#flash_attention_forward",
                        "transformers/integrations/hub_kernels.py#is_kernel",
                        "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/peft.py#maybe_load_adapters": {
                "sorted_modules": {
                    "maybe_load_adapters": "\n\ndef maybe_load_adapters(\n    pretrained_model_name_or_path,\n    download_kwargs: DownloadKwargs,\n    **adapter_kwargs,\n):\n    if pretrained_model_name_or_path is None or not is_peft_available():\n        return None, pretrained_model_name_or_path\n\n    token = download_kwargs.get(\"token\")\n\n    if download_kwargs.get(\"commit_hash\") is None:\n        resolved_config_file = cached_file(\n            pretrained_model_name_or_path,\n            CONFIG_NAME,\n            cache_dir=download_kwargs.get(\"cache_dir\"),\n            force_download=bool(download_kwargs.get(\"force_download\", False)),\n            proxies=download_kwargs.get(\"proxies\"),\n            local_files_only=bool(download_kwargs.get(\"local_files_only\", False)),\n            token=token,\n            revision=download_kwargs.get(\"revision\"),\n            subfolder=download_kwargs.get(\"subfolder\"),\n            _raise_exceptions_for_gated_repo=False,\n            _raise_exceptions_for_missing_entries=False,\n            _raise_exceptions_for_connection_errors=False,\n        )\n        download_kwargs[\"commit_hash\"] = extract_commit_hash(resolved_config_file, None)\n\n    _adapter_model_path = adapter_kwargs.pop(\"_adapter_model_path\", None)\n\n    if _adapter_model_path is None:\n        _adapter_model_path = find_adapter_config_file(\n            pretrained_model_name_or_path,\n            cache_dir=download_kwargs.get(\"cache_dir\"),\n            force_download=bool(download_kwargs.get(\"force_download\", False)),\n            proxies=download_kwargs.get(\"proxies\"),\n            token=token,\n            revision=download_kwargs.get(\"revision\"),\n            local_files_only=bool(download_kwargs.get(\"local_files_only\", False)),\n            subfolder=download_kwargs.get(\"subfolder\", \"\"),\n            _commit_hash=download_kwargs.get(\"commit_hash\"),\n            **adapter_kwargs,\n        )\n\n    if _adapter_model_path is not None and os.path.isfile(_adapter_model_path):\n        with open(_adapter_model_path, \"r\", encoding=\"utf-8\") as f:\n            _adapter_model_path = pretrained_model_name_or_path\n            pretrained_model_name_or_path = json.load(f)[\"base_model_name_or_path\"]\n\n    return _adapter_model_path, pretrained_model_name_or_path"
                },
                "component_dependencies": {
                    "maybe_load_adapters": [
                        "transformers/utils.py#CONFIG_NAME",
                        "transformers/utils.py#cached_file",
                        "transformers/utils.py#extract_commit_hash",
                        "transformers/utils.py#find_adapter_config_file",
                        "transformers/utils.py#is_peft_available",
                        "transformers/utils/hub.py#DownloadKwargs"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan": {
                "sorted_modules": {
                    "_get_parameter_tp_plan": "\n\ndef _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weight=True) -> str | None:\n    \"\"\"\n    Get the TP style for a parameter from the TP plan.\n\n    The TP plan is a dictionary that maps parameter names to TP styles.\n    The parameter name can be a generic name with wildcards (e.g. \"*.weight\") or a specific name (e.g. \"layer_1.weight\").\n\n    The `is_weight` is important because for weights, we want to support `.weights` and `.bias` cases seamlessly! but\n    not parent classes for `post_init` calls\n    \"\"\"\n    generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n    if generic_param_name in tp_plan:\n        return tp_plan[generic_param_name]\n    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan and is_weight:\n        return tp_plan[generic_param_name.rsplit(\".\", 1)[0]]\n    return None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#distribute_model": {
                "sorted_modules": {
                    "distribute_model": "\n\ndef distribute_model(model, tp_plan, distributed_config, device_mesh, tp_size):\n    model._tp_size = tp_size\n    model._device_mesh = device_mesh\n    if distributed_config is not None:\n        if isinstance(distributed_config, dict):\n            distributed_config = DistributedConfig.from_dict(distributed_config)\n        model.config.distributed_config = distributed_config\n    # Set the new requested tp_plan on the model\n    if isinstance(tp_plan, dict):\n        model.tp_plan = tp_plan\n    model_plan = model.tp_plan\n    if model_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n        for v in model_plan.values():\n            if v not in ALL_PARALLEL_STYLES:\n                raise ValueError(f\"Unsupported tensor parallel style {v}. Supported styles are {ALL_PARALLEL_STYLES}\")\n        for name, module in model.named_modules():\n            if not getattr(module, \"_is_hooked\", False):\n                plan = _get_parameter_tp_plan(parameter_name=name, tp_plan=model_plan, is_weight=False)\n                add_tensor_parallel_hooks_to_module(\n                    model=model,\n                    module=module,\n                    tp_plan=model_plan,\n                    layer_name=\"\",\n                    current_module_plan=plan,\n                    device_mesh=device_mesh,\n                )\n            module._is_hooked = True\n    return model"
                },
                "component_dependencies": {
                    "distribute_model": [
                        "transformers/distributed.py#DistributedConfig.from_dict",
                        "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                        "transformers/integrations/tensor_parallel.py#_torch_distributed_available",
                        "transformers/integrations/tensor_parallel.py#add_tensor_parallel_hooks_to_module",
                        "transformers/utils.py#is_torch_greater_or_equal"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism": {
                "sorted_modules": {
                    "initialize_tensor_parallelism": "\n\ndef initialize_tensor_parallelism(\n    tp_plan: str | dict[str, str] | None, tp_size: int | None = None, device_mesh=None, device_map=None\n):\n    r\"\"\"\n    Sets up the device mesh and initialized the backend for tensor parallelism.\n    This function is called when the model is loaded and the TP plan is set to 'auto'.\n    \"\"\"\n    if tp_size is not None and tp_plan is None:\n        raise ValueError(\"tp_plan has to be set when tp_size is passed.\")\n    if tp_plan is not None and device_map is not None:\n        raise ValueError(\"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\")\n    if device_mesh is None:\n        if not is_torch_greater_or_equal(\"2.5\"):\n            raise OSError(\"Tensor parallel is only supported for `torch>=2.5`.\")\n\n        # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n        device_type = torch._C._get_accelerator().type\n        if device_type == \"mps\":\n            device_type = \"cpu\"  # fallback\n        current_device = getattr(torch, device_type)\n        if not torch.distributed.is_initialized():\n            try:\n                rank = int(os.environ[\"RANK\"])\n                local_rank = int(os.environ[\"LOCAL_RANK\"])\n                world_size = int(os.environ[\"WORLD_SIZE\"])\n\n                backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"xccl\", \"hpu\": \"hccl\"}\n                backend = backend_map.get(device_type)\n                if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", \"0\")):\n                    backend = \"ccl\"\n                if device_type == \"xpu\" and not is_torch_greater_or_equal(\"2.8\", accept_dev=True):\n                    backend = \"ccl\"\n\n                torch.distributed.init_process_group(backend=backend, rank=rank, world_size=world_size)\n                current_device = getattr(torch, device_type)\n                if device_type != \"cpu\":\n                    current_device.set_device(local_rank)\n\n            except Exception as e:\n                raise OSError(\n                    \"We tried to initialize torch.distributed for you, but it failed. Make \"\n                    \"sure you init torch distributed in your script to use `tp_plan`.\"\n                ) from e\n\n        if device_type != \"cpu\":\n            current_device.set_device(int(os.environ[\"LOCAL_RANK\"]))\n            index = current_device.current_device()\n            tp_device = torch.device(device_type, index)\n            device_map = tp_device\n            # Silence output for non-primary ranks\n            if index > 0:\n                import sys\n\n                sys.stdout = open(os.devnull, \"w\")\n                sys.stderr = open(os.devnull, \"w\")\n\n        else:\n            tp_device = torch.device(device_type)\n            device_map = device_type or {}\n\n        tp_size = tp_size if tp_size is not None else torch.distributed.get_world_size()\n        device_mesh = torch.distributed.init_device_mesh(tp_device.type, (tp_size,))\n    else:\n        if device_mesh.ndim > 1:\n            if \"tp\" not in device_mesh.mesh_dim_names:\n                raise ValueError(\n                    \"When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. \"\n                    \"Please provide a valid `device_mesh`.\"\n                )\n            device_mesh = device_mesh[\"tp\"]\n        tp_size = device_mesh.size()\n        device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n\n    return device_map, device_mesh, tp_size"
                },
                "component_dependencies": {
                    "initialize_tensor_parallelism": [
                        "transformers/utils.py#is_torch_greater_or_equal"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#repack_weights": {
                "sorted_modules": {
                    "repack_weights": "\n\ndef repack_weights(\n    packed_parameter: torch.Tensor,\n    sharded_dim: int,  # The dimension index in the global tensor that was sharded\n    world_size: int,\n    num_blocks: int = 2,\n) -> torch.Tensor:\n    \"\"\"\n    Reorders a tensor that was reconstructed from sharded packed weights into its canonical packed format.\n\n    For example, if a weight was packed (e.g., gate_proj and up_proj) and then sharded,\n    DTensor.full_tensor() might produce an interleaved layout like [G0, U0, G1, U1, ...]\n    along the sharded dimension. This function reorders it to [G0, G1, ..., U0, U1, ...].\n    This is an inverse operation to get_packed_weights.\n\n    Args:\n        reconstructed_tensor: The tensor reconstructed from DTensor (e.g., via .full_tensor().contiguous()).\n        sharded_dim: The dimension index in the reconstructed_tensor that was originally sharded.\n        world_size: The tensor parallel world size.\n        num_packed_projs: The number of projections that were packed together (e.g., 2 for gate_up_proj).\n\n    Returns:\n        The reordered tensor in canonical packed format.\n    \"\"\"\n\n    if num_blocks != 2:\n        raise ValueError(\n            \"Num blocks different from 2 is not supported yet. This is most likely a bug in your implementation as we only pack gate and up projections together.\"\n        )\n\n    actual_sharded_dim = sharded_dim if sharded_dim >= 0 else sharded_dim + packed_parameter.ndim\n    total_size_on_sharded_dim = packed_parameter.shape[actual_sharded_dim]\n    original_block_size_on_dim = total_size_on_sharded_dim // num_blocks\n    shard_chunk_size = original_block_size_on_dim // world_size\n\n    prefix_shape = packed_parameter.shape[:actual_sharded_dim]\n    suffix_shape = packed_parameter.shape[actual_sharded_dim + 1 :]\n\n    tensor_view = packed_parameter.view(\n        *prefix_shape,\n        world_size,\n        num_blocks,\n        shard_chunk_size,\n        *suffix_shape,\n    )\n\n    # Permute to bring num_packed_projs first, then world_size, then shard_chunk_size\n    # This groups all chunks of G together, then all chunks of U together.\n    # Target order of these middle dimensions: (num_packed_projs, world_size, shard_chunk_size)\n    # Current order of view's middle dimensions: (world_size, num_packed_projs, shard_chunk_size)\n    # Absolute indices of the dimensions to be permuted (world_size, num_packed_projs)\n    axis_ws_abs = len(prefix_shape)\n    axis_npp_abs = len(prefix_shape) + 1\n\n    permute_order = list(range(tensor_view.ndim))\n    permute_order[axis_ws_abs], permute_order[axis_npp_abs] = permute_order[axis_npp_abs], permute_order[axis_ws_abs]\n\n    tensor_permuted = tensor_view.permute(*permute_order)\n\n    # Reshape back to the original tensor's ndim, with the sharded dimension now correctly ordered as [G_all, U_all].\n    # The final shape should be the same as reconstructed_tensor.\n    final_ordered_tensor = tensor_permuted.reshape_as(packed_parameter)\n\n    return final_ordered_tensor"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor": {
                "sorted_modules": {
                    "replace_state_dict_local_with_dtensor": "\n\ndef replace_state_dict_local_with_dtensor(\n    state_dict: dict[str, torch.Tensor],\n    tp_plan: dict[str, str],\n    device_mesh,\n) -> dict[str, torch.Tensor]:\n    \"\"\"\n    Replaces all tensors that were sharded with `local_*` strategy with DTensor to make determining their proper size possible.\n    \"\"\"\n    for key, value in state_dict.items():\n        if isinstance(value, torch.Tensor) and not isinstance(value, DTensor):\n            state_dict[key] = convert_local_tensor_to_dtensor(value, key, device_mesh, tp_plan)\n    return state_dict"
                },
                "component_dependencies": {
                    "replace_state_dict_local_with_dtensor": [
                        "transformers/integrations/tensor_parallel.py#convert_local_tensor_to_dtensor"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#shard_and_distribute_module": {
                "sorted_modules": {
                    "shard_and_distribute_module": "\n\ndef shard_and_distribute_module(\n    model, param, empty_param, parameter_name, param_casting_dtype, is_contiguous, rank, device_mesh\n):  # TODO: rename to shard_and_distribute_param\n    r\"\"\"\n    This function is called in `from_pretrained` when loading a model's checkpoints.\n    It receives the pointer to the parameter (or the parameter itself) and takes care of \"sharding\".\n    All process run this function, so they just load the partition of the tensor that they require.\n\n    Main uses cases:\n    - column / rowise parallelism, you just shard all the weights of the layer (weight and bias)\n    - packed layers: you slice the weights, then shard like above\n    - custom operation:\n        - you want to add an all-gather at the end of a local layer.\n        - you want to have a layer that is isolated from the rest of the world (because torch.DTensor does not work well with `.view` for instance)\n\n    \"\"\"\n    param_name, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n    tp_plan = model.tp_plan or {}\n    module_to_tp = model.get_submodule(param_name)  # TODO: can i loop over modules?\n    rank = int(rank)\n    current_shard_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n\n    if dist.get_rank() == 0:\n        if current_shard_plan is None:\n            logger.info(f\"Tensor sharding plan for {param_name} not found, using default 'replicate' plan.\")\n        else:\n            logger.info(f\"Tensor sharding plan for {param_name}: {current_shard_plan}\")\n\n    if current_shard_plan is not None:\n        try:\n            tp_layer = ALL_PARALLEL_STYLES[current_shard_plan]\n            param = tp_layer.partition_tensor(\n                param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n            )\n        except NotImplementedError as e:\n            print(\n                f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\"\n            )\n    else:\n        param = param[:].to(param_casting_dtype)\n\n    # SUPER IMPORTANT we have to use setattr\n    # otherwise loading is crazy slow\n    if not isinstance(param, torch.nn.Parameter):\n        param = torch.nn.Parameter(param, requires_grad=empty_param.is_floating_point())\n    setattr(module_to_tp, param_type, param)\n    # module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n    return param"
                },
                "component_dependencies": {
                    "shard_and_distribute_module": [
                        "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                        "transformers/integrations/tensor_parallel.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#verify_tp_plan": {
                "sorted_modules": {
                    "verify_tp_plan": "\n\ndef verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n    \"\"\"\n    Verify the TP plan of the model, log a warning if the layers that were not sharded and the rules that were not applied.\n    \"\"\"\n\n    if tp_plan is None:\n        return\n\n    generic_keys = {re.sub(r\"\\d+\", \"*\", key) for key in expected_keys}\n    unsharded_layers = set(generic_keys)\n    unused_rules = tp_plan\n\n    for key in generic_keys:\n        param_name = key.rsplit(\".\", 1)[0] if \".\" in key else key\n        generic_param_name = re.sub(r\"\\d+\", \"*\", param_name)\n\n        if generic_param_name in tp_plan:\n            unused_rules.pop(generic_param_name)\n            unsharded_layers.discard(key)\n        elif \".\" in generic_param_name and (parent_param_name := generic_param_name.rsplit(\".\", 1)[0]) in tp_plan:\n            unused_rules.pop(parent_param_name)\n            unsharded_layers.discard(key)\n        else:\n            pass  # we couldn't find the rule for this parameter, so it's not sharded\n\n    if len(unused_rules) > 0:\n        logger.warning(f\"The following TP rules were not applied on any of the layers: {unused_rules}\")\n    if len(unsharded_layers) > 0:\n        logger.warning(f\"The following layers were not sharded: {', '.join(unsharded_layers)}\")"
                },
                "component_dependencies": {
                    "verify_tp_plan": [
                        "transformers/integrations/tensor_parallel.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_utils.py#LOSS_MAPPING": {
                "sorted_modules": {
                    "LOSS_MAPPING": "\n\nLOSS_MAPPING = {\n    \"ForCausalLM\": ForCausalLMLoss,\n    \"ForMaskedLM\": ForMaskedLMLoss,\n    \"ForQuestionAnswering\": ForQuestionAnsweringLoss,\n    \"ForSequenceClassification\": ForSequenceClassificationLoss,\n    \"ForImageClassification\": ForSequenceClassificationLoss,\n    \"ForVideoClassification\": ForSequenceClassificationLoss,\n    \"ForAudioClassification\": ForSequenceClassificationLoss,\n    \"ForTokenClassification\": ForTokenClassification,\n    \"ForSegmentation\": ForSegmentationLoss,\n    \"ForObjectDetection\": ForObjectDetectionLoss,\n    \"ForConditionalGeneration\": ForCausalLMLoss,\n    \"DeformableDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n    \"ConditionalDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n    \"DabDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n    \"GroundingDinoForObjectDetection\": GroundingDinoForObjectDetectionLoss,\n    \"MMGroundingDinoForObjectDetection\": GroundingDinoForObjectDetectionLoss,\n    \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n    \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n    \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,\n    \"DFineForObjectDetection\": DFineForObjectDetectionLoss,\n    \"CsmForConditionalGeneration\": ForCausalLMLoss,\n}"
                },
                "component_dependencies": {
                    "LOSS_MAPPING": [
                        "transformers/loss/loss_d_fine.py#DFineForObjectDetectionLoss",
                        "transformers/loss/loss_deformable_detr.py#DeformableDetrForObjectDetectionLoss",
                        "transformers/loss/loss_deformable_detr.py#DeformableDetrForSegmentationLoss",
                        "transformers/loss/loss_for_object_detection.py#ForObjectDetectionLoss",
                        "transformers/loss/loss_for_object_detection.py#ForSegmentationLoss",
                        "transformers/loss/loss_grounding_dino.py#GroundingDinoForObjectDetectionLoss",
                        "transformers/loss/loss_rt_detr.py#RTDetrForObjectDetectionLoss",
                        "transformers/loss/loss_utils.py#ForCausalLMLoss",
                        "transformers/loss/loss_utils.py#ForMaskedLMLoss",
                        "transformers/loss/loss_utils.py#ForQuestionAnsweringLoss",
                        "transformers/loss/loss_utils.py#ForSequenceClassificationLoss",
                        "transformers/loss/loss_utils.py#ForTokenClassification"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention": {
                "sorted_modules": {
                    "lazy_import_flash_attention": "\n\ndef lazy_import_flash_attention(implementation: Optional[str], force_import: Optional[bool] = False):\n    \"\"\"\n    Lazily import flash attention and return the respective functions + flags.\n\n    NOTE: For fullgraph, this needs to be called before compile, while no fullgraph can\n    work without preloading. See `load_and_register_attn_kernel` in `integrations.hub_kernels`.\n    \"\"\"\n    global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn\n    if force_import or any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):\n        _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn = _lazy_imports(implementation)\n\n    global _process_flash_kwargs_fn\n    if force_import or _process_flash_kwargs_fn is None:\n        _process_flash_kwargs_fn = _lazy_define_process_function(_flash_varlen_fn)\n\n    return (_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn), _process_flash_kwargs_fn"
                },
                "component_dependencies": {
                    "lazy_import_flash_attention": [
                        "transformers/modeling_flash_attention_utils.py#_flash_fn",
                        "transformers/modeling_flash_attention_utils.py#_flash_varlen_fn",
                        "transformers/modeling_flash_attention_utils.py#_lazy_define_process_function",
                        "transformers/modeling_flash_attention_utils.py#_lazy_imports",
                        "transformers/modeling_flash_attention_utils.py#_pad_fn",
                        "transformers/modeling_flash_attention_utils.py#_process_flash_kwargs_fn",
                        "transformers/modeling_flash_attention_utils.py#_unpad_fn"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_utils.py#EmbeddingAccessMixin": {
                "sorted_modules": {
                    "EmbeddingAccessMixin": "\n\nclass EmbeddingAccessMixin:\n    \"\"\"\n    Base utilities to regroup getters and setters for embeddings.\n    Introduces the `input_layer_embed` attribute, which indicates\n    where the input embeddings come from and where they\n    should be set.\n    \"\"\"\n\n    _input_embed_layer = \"embed_tokens\"  # default layer that holds input embeddings.\n\n    def get_input_embeddings(self) -> nn.Module:\n        \"\"\"\n        Returns the model's input embeddings.\n\n        Returns:\n            `nn.Module`: A torch module mapping vocabulary to hidden states.\n        \"\"\"\n\n        # 1) Check if the model has an attribute named 'embed_tokens' (the standard input embedding layer\n        #  for most NLP models), and if so, return it.\n\n        name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n\n        if (default_embedding := getattr(self, name, None)) is not None:\n            return default_embedding\n        # 2) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n\n        if hasattr(self, \"model\") and hasattr(self.model, \"embed_tokens\"):\n            return self.model.embed_tokens\n\n        # 3) vanilla decoder\u2011only architectures\n        elif hasattr(self, \"embed_tokens\"):\n            return self.embed_tokens\n        else:\n            base_model = getattr(self, \"base_model_prefix\", None)\n            if base_model is not None:\n                base_model = getattr(self, base_model, None)\n                if base_model is not None and base_model is not self:\n                    return base_model.get_input_embeddings()\n            raise NotImplementedError(\n                f\"`get_input_embeddings` not auto\u2011handled for {self.__class__.__name__}; \"\n                \"please override in the subclass.\"\n            )\n\n    def set_input_embeddings(self, value: nn.Module):\n        \"\"\"Fallback setter that handles **~70%** of models in the code-base.\n\n        Order of attempts:\n        1. `self.model.embed_tokens`\n        2. `self.embed_tokens`\n        3. delegate to the *base model* if one exists\n        4. otherwise raise `NotImplementedError` so subclasses still can (and\n            should) override for exotic layouts.\n        \"\"\"\n\n        # 1) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n        name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n        if hasattr(self, \"model\") and hasattr(self.model, name):\n            setattr(self.model, name, value)\n        # 2) as well as vanilla decoder\u2011only architectures\n        elif hasattr(self, name):\n            setattr(self, name, value)\n        # 3) recurse once into the registered *base* model (e.g. for encoder/decoder)\n        elif getattr(self, self.base_model_prefix, self) is not self:\n            base_model = getattr(self, self.base_model_prefix, self)\n            base_model.set_input_embeddings(value)\n        else:\n            raise NotImplementedError(\n                f\"`set_input_embeddings` not auto\u2011handled for {self.__class__.__name__}; please override in the subclass.\"\n            )\n\n    def get_output_embeddings(self):\n        if not hasattr(self, \"lm_head\"):\n            return None\n        try:\n            # Speech / vision backbones raise here, so we return None.\n            # Legit use of get_input_embs?\n            self.get_input_embeddings()\n        except NotImplementedError:\n            return None\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        \"\"\"\n        Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.\n        \"\"\"\n        if getattr(self, \"lm_head\"):\n            self.lm_head = new_embeddings"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#ModuleUtilsMixin": {
                "sorted_modules": {
                    "ModuleUtilsMixin": "\n\nclass ModuleUtilsMixin:\n    \"\"\"\n    A few utilities for `torch.nn.Modules`, to be used as a mixin.\n    \"\"\"\n\n    @staticmethod\n    def _hook_rss_memory_pre_forward(module, *args, **kwargs):\n        try:\n            import psutil\n        except ImportError:\n            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n\n        process = psutil.Process(os.getpid())\n        mem = process.memory_info()\n        module.mem_rss_pre_forward = mem.rss\n        return None\n\n    @staticmethod\n    def _hook_rss_memory_post_forward(module, *args, **kwargs):\n        try:\n            import psutil\n        except ImportError:\n            raise ImportError(\"You need to install psutil (pip install psutil) to use memory tracing.\")\n\n        process = psutil.Process(os.getpid())\n        mem = process.memory_info()\n        module.mem_rss_post_forward = mem.rss\n        mem_rss_diff = module.mem_rss_post_forward - module.mem_rss_pre_forward\n        module.mem_rss_diff = mem_rss_diff + (module.mem_rss_diff if hasattr(module, \"mem_rss_diff\") else 0)\n        return None\n\n    def add_memory_hooks(self):\n        \"\"\"\n        Add a memory hook before and after each sub-module forward pass to record increase in memory consumption.\n\n        Increase in memory consumption is stored in a `mem_rss_diff` attribute for each module and can be reset to zero\n        with `model.reset_memory_hooks_state()`.\n        \"\"\"\n        for module in self.modules():\n            module.register_forward_pre_hook(self._hook_rss_memory_pre_forward)\n            module.register_forward_hook(self._hook_rss_memory_post_forward)\n        self.reset_memory_hooks_state()\n\n    def reset_memory_hooks_state(self):\n        \"\"\"\n        Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory_hooks`]).\n        \"\"\"\n        for module in self.modules():\n            module.mem_rss_diff = 0\n            module.mem_rss_post_forward = 0\n            module.mem_rss_pre_forward = 0\n\n    @property\n    def device(self) -> torch.device:\n        \"\"\"\n        `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n        device).\n        \"\"\"\n        return get_parameter_device(self)\n\n    @property\n    def dtype(self) -> torch.dtype:\n        \"\"\"\n        `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n        \"\"\"\n        return get_parameter_dtype(self)\n\n    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n        \"\"\"\n        Invert an attention mask (e.g., switches 0. and 1.).\n\n        Args:\n            encoder_attention_mask (`torch.Tensor`): An attention mask.\n\n        Returns:\n            `torch.Tensor`: The inverted attention mask.\n        \"\"\"\n        if encoder_attention_mask.dim() == 3:\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n        if encoder_attention_mask.dim() == 2:\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n        # encoder_extended_attention_mask.transpose(-1, -2))\n        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * torch.finfo(self.dtype).min\n\n        return encoder_extended_attention_mask\n\n    @staticmethod\n    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None):\n        if device is not None:\n            warnings.warn(\n                \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n            )\n        else:\n            device = attention_mask.device\n        batch_size, seq_length = input_shape\n        seq_ids = torch.arange(seq_length, device=device)\n        causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n        # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n        causal_mask = causal_mask.to(attention_mask.dtype)\n\n        if causal_mask.shape[1] < attention_mask.shape[1]:\n            prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n            causal_mask = torch.cat(\n                [\n                    torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n                    causal_mask,\n                ],\n                axis=-1,\n            )\n\n        extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n        return extended_attention_mask\n\n    def get_extended_attention_mask(\n        self,\n        attention_mask: Tensor,\n        input_shape: tuple[int, ...],\n        device: Optional[torch.device] = None,\n        dtype: Optional[torch.dtype] = None,\n    ) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (`tuple[int]`):\n                The shape of the input to the model.\n\n        Returns:\n            `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\n        \"\"\"\n        if dtype is None:\n            dtype = self.dtype\n\n        if not (attention_mask.dim() == 2 and self.config.is_decoder):\n            # show warning only if it won't be shown in `create_extended_attention_mask_for_decoder`\n            if device is not None:\n                warnings.warn(\n                    \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n                )\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if self.config.is_decoder:\n                extended_attention_mask = ModuleUtilsMixin.create_extended_attention_mask_for_decoder(\n                    input_shape, attention_mask, device\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                f\"Wrong shape for input_ids (shape {input_shape}) or attention_mask (shape {attention_mask.shape})\"\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and the dtype's smallest value for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(dtype).min\n        return extended_attention_mask\n\n    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n        \"\"\"\n        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\n        Args:\n            only_trainable (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of trainable parameters\n\n            exclude_embeddings (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of non-embeddings parameters\n\n        Returns:\n            `int`: The number of parameters.\n        \"\"\"\n\n        if exclude_embeddings:\n            embedding_param_names = [\n                f\"{name}.weight\" for name, module_type in self.named_modules() if isinstance(module_type, nn.Embedding)\n            ]\n            total_parameters = [\n                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n            ]\n        else:\n            total_parameters = list(self.parameters())\n\n        total_numel = []\n        is_loaded_in_4bit = getattr(self, \"is_loaded_in_4bit\", False)\n\n        if is_loaded_in_4bit:\n            if is_bitsandbytes_available():\n                import bitsandbytes as bnb\n            else:\n                raise ValueError(\n                    \"bitsandbytes is not installed but it seems that the model has been loaded in 4bit precision, something went wrong\"\n                    \" make sure to install bitsandbytes with `pip install bitsandbytes`. You also need a GPU. \"\n                )\n\n        for param in total_parameters:\n            if param.requires_grad or not only_trainable:\n                # For 4bit models, we need to multiply the number of parameters by 2 as half of the parameters are\n                # used for the 4bit quantization (uint8 tensors are stored)\n                if is_loaded_in_4bit and isinstance(param, bnb.nn.Params4bit):\n                    if hasattr(param, \"element_size\"):\n                        num_bytes = param.element_size()\n                    elif hasattr(param, \"quant_storage\"):\n                        num_bytes = param.quant_storage.itemsize\n                    else:\n                        num_bytes = 1\n                    total_numel.append(param.numel() * 2 * num_bytes)\n                else:\n                    total_numel.append(param.numel())\n\n        return sum(total_numel)\n\n    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int:\n        \"\"\"\n        Helper function to estimate the total number of tokens from the model inputs.\n\n        Args:\n            inputs (`dict`): The model inputs.\n\n        Returns:\n            `int`: The total number of tokens.\n        \"\"\"\n        if not hasattr(self, \"warnings_issued\"):\n            self.warnings_issued = {}\n        if self.main_input_name in input_dict:\n            return input_dict[self.main_input_name].numel()\n        elif \"estimate_tokens\" not in self.warnings_issued:\n            logger.warning(\n                \"Could not estimate the number of tokens of the input, floating-point operations will not be computed\"\n            )\n            self.warnings_issued[\"estimate_tokens\"] = True\n        return 0\n\n    def floating_point_ops(\n        self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool = True\n    ) -> int:\n        \"\"\"\n        Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\n        batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\n        tokens (valid if `12 * d_model << sequence_length`) as laid out in [this\n        paper](https://huggingface.co/papers/2001.08361) section 2.1. Should be overridden for transformers with parameter\n        re-use e.g. Albert or Universal Transformers, or if doing long-range modeling with very high sequence lengths.\n\n        Args:\n            batch_size (`int`):\n                The batch size for the forward pass.\n\n            sequence_length (`int`):\n                The number of tokens in each line of the batch.\n\n            exclude_embeddings (`bool`, *optional*, defaults to `True`):\n                Whether or not to count embedding and softmax operations.\n\n        Returns:\n            `int`: The number of floating-point operations.\n        \"\"\"\n\n        return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)"
                },
                "component_dependencies": {
                    "ModuleUtilsMixin": [
                        "transformers/modeling_utils.py#get_parameter_device",
                        "transformers/modeling_utils.py#get_parameter_dtype",
                        "transformers/modeling_utils.py#logger",
                        "transformers/utils.py#is_bitsandbytes_available"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#SpecificPreTrainedModelType": {
                "sorted_modules": {
                    "SpecificPreTrainedModelType": "SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#VLMS": {
                "sorted_modules": {
                    "VLMS": "\n# DO NOT MODIFY, KEPT FOR BC ONLY\nVLMS = [\n    \"aria\",\n    \"ayavision\",\n    \"colpali\",\n    \"emu3\",\n    \"fuyu\",\n    \"gotocr2\",\n    \"gemma3\",\n    \"internvl\",\n    \"llava\",  # all llava prefixed models fall under this check\n    \"mistral3\",\n    \"mllama\",\n    \"paligemma\",\n    \"shieldgemma2\",\n    \"qwen2vl\",\n    \"qwen2_5_vl\",\n    \"videollava\",\n    \"vipllava\",\n]"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_add_variant": {
                "sorted_modules": {
                    "_add_variant": "\n\ndef _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n    if variant is not None:\n        path, name = weights_name.rsplit(\".\", 1)\n        weights_name = f\"{path}.{variant}.{name}\"\n    return weights_name"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_find_disjoint": {
                "sorted_modules": {
                    "_find_disjoint": "\n\ndef _find_disjoint(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]) -> tuple[list[set[str]], list[str]]:\n    filtered_tensors = []\n    for shared in tensors:\n        if len(shared) < 2:\n            filtered_tensors.append(shared)\n            continue\n\n        areas = []\n        for name in shared:\n            tensor = state_dict[name]\n            areas.append((tensor.data_ptr(), _end_ptr(tensor), name))\n        areas.sort()\n\n        _, last_stop, last_name = areas[0]\n        filtered_tensors.append({last_name})\n        for start, stop, name in areas[1:]:\n            if start >= last_stop:\n                filtered_tensors.append({name})\n            else:\n                filtered_tensors[-1].add(name)\n            last_stop = stop\n    disjoint_tensors = []\n    shared_tensors = []\n    for tensors in filtered_tensors:\n        if len(tensors) == 1:\n            disjoint_tensors.append(tensors.pop())\n        else:\n            shared_tensors.append(tensors)\n    return shared_tensors, disjoint_tensors"
                },
                "component_dependencies": {
                    "_find_disjoint": [
                        "transformers/modeling_utils.py#_end_ptr"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_find_identical": {
                "sorted_modules": {
                    "_find_identical": "\n\ndef _find_identical(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]) -> tuple[list[set[str]], set[str]]:\n    shared_tensors = []\n    identical = []\n    for shared in tensors:\n        if len(shared) < 2:\n            continue\n\n        areas = collections.defaultdict(set)\n        for name in shared:\n            tensor = state_dict[name]\n            area = (tensor.device, tensor.data_ptr(), _end_ptr(tensor))\n            areas[area].add(name)\n        if len(areas) == 1:\n            identical.append(shared)\n        else:\n            shared_tensors.append(shared)\n    return shared_tensors, identical"
                },
                "component_dependencies": {
                    "_find_identical": [
                        "transformers/modeling_utils.py#_end_ptr"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_find_mismatched_keys": {
                "sorted_modules": {
                    "_find_mismatched_keys": "\n\ndef _find_mismatched_keys(\n    model: \"PreTrainedModel\",\n    state_dict: Optional[dict],\n    checkpoint_files: Optional[list[str]],\n    ignore_mismatched_sizes: bool,\n    keys_to_rename_mapping: dict[str, str],\n    is_quantized: bool,\n    weights_only: bool,\n) -> tuple[list[str], list[tuple[int, int]]]:\n    \"\"\"\n    Find potential shape mismatch between the different state dicts and the model parameters, but only if `ignore_mismatched_sizes`\n    is True. Otherwise, return immediately and any shape mismatch that may exist will be raised later on. This avoids checking\n    every parameter in advance, as shape mismatch are extremely rare in practice. If we want to ignore them however, we do\n    need to check in advance as we need to know which parameters we need to move back from meta to cpu, and initialize\n    correctly. Indeed, as our model initialization takes place at the module level, and not the weight level, in the\n    case of a sharded checkpoint we cannot correctly initialize the weights according to `model._init_weights()` if we perform\n    this check on each state dict at loading time (after the first loaded checkpoint, there are no way to initialize only the\n    mismatched weights if any, without overwriting the previously loaded weights as well because all the module will be\n    initialized, not only the weights that are mismatched).\n    \"\"\"\n\n    # An error will be raised later on anyway if there is a mismatch - this avoids running the rest of this function\n    # if there are no mismatch (which is almost always the case)\n    if not ignore_mismatched_sizes:\n        return [], []\n\n    if state_dict is not None:\n        checkpoint_files = [\"\"]\n\n    model_state_dict = model.state_dict()\n    mismatched_keys = []\n    mismatched_shapes = []\n    for shard_file in checkpoint_files:\n        # If shard_file is \"\", we use the existing state_dict instead of loading it\n        if shard_file != \"\":\n            state_dict = load_state_dict(\n                shard_file, is_quantized=is_quantized, map_location=\"meta\", weights_only=weights_only\n            )\n\n        # Fix the key names\n        new_state_dict = {keys_to_rename_mapping[k]: v for k, v in state_dict.items() if k in keys_to_rename_mapping}\n\n        for key, tensor in new_state_dict.items():\n            if key in model_state_dict and tensor.shape != model_state_dict[key].shape:\n                # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n                # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n                if not (\n                    is_quantized and tensor.shape[-1] == 1 and tensor.numel() * 2 == model_state_dict[key].numel()\n                ):\n                    mismatched_keys.append(key)\n                    mismatched_shapes.append((tensor.shape, model_state_dict[key].shape))\n\n    return mismatched_keys, mismatched_shapes"
                },
                "component_dependencies": {
                    "_find_mismatched_keys": [
                        "transformers/modeling_utils.py#load_state_dict"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_find_missing_and_unexpected_keys": {
                "sorted_modules": {
                    "_find_missing_and_unexpected_keys": "\n\ndef _find_missing_and_unexpected_keys(\n    model: \"PreTrainedModel\",\n    original_checkpoint_keys: list[str],\n    checkpoint_keys: list[str],\n    loading_base_model_from_task_state_dict: bool,\n    hf_quantizer: Optional[HfQuantizer],\n) -> tuple[list[str], list[str]]:\n    \"\"\"Find missing keys (keys that are part of the model parameters but were NOT found in the loaded state dict keys) and unexpected keys\n    (keys found in the loaded state dict keys, but that are NOT part of the model parameters)\n    \"\"\"\n    prefix = model.base_model_prefix\n\n    # Compute expected keys, i.e. keys that the full model expects\n    expected_keys = list(model.state_dict().keys())\n    if hf_quantizer is not None:\n        expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n\n    # Adjust prefix of the keys to make them match loaded keys before removing them\n    missing_keys = sorted(set(expected_keys) - set(checkpoint_keys))\n    unexpected_keys = set(checkpoint_keys) - set(expected_keys)\n    # If a module has the same name under the base and task specific model, we have to re-add it to unexpected keys\n    if loading_base_model_from_task_state_dict:\n        task_specific_keys = [k for k in original_checkpoint_keys if not k.startswith(f\"{prefix}.\")]\n        unexpected_keys.update(task_specific_keys)\n\n    # Remove nonpersistent buffers from unexpected keys: they are not in the expected keys (model state dict), but\n    # may be in the loaded keys. Note that removing all buffers does the job, as they were part of the expected keys anyway\n    model_buffers = {n for n, _ in model.named_buffers()}\n    unexpected_keys = sorted(unexpected_keys - model_buffers)\n\n    tied_params = find_tied_parameters(model)\n    for group in tied_params:\n        missing_in_group = [k for k in missing_keys if k in group]\n        if len(missing_in_group) > 0 and len(missing_in_group) < len(group):\n            missing_keys = [k for k in missing_keys if k not in missing_in_group]\n\n    if hf_quantizer is not None:\n        missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys)\n\n    return missing_keys, unexpected_keys"
                },
                "component_dependencies": {
                    "_find_missing_and_unexpected_keys": [
                        "transformers/integrations/accelerate.py#find_tied_parameters",
                        "transformers/quantizers.py#HfQuantizer"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_get_dtype": {
                "sorted_modules": {
                    "_get_dtype": "\n\ndef _get_dtype(\n    cls,\n    dtype: Optional[Union[str, torch.dtype, dict]],\n    checkpoint_files: Optional[list[str]],\n    config: PreTrainedConfig,\n    sharded_metadata: Optional[dict],\n    state_dict: Optional[dict],\n    weights_only: bool,\n) -> tuple[PreTrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:\n    \"\"\"Find the correct `dtype` to use based on provided arguments. Also update the `config` based on the\n    inferred dtype. We do the following:\n    1. If dtype is not None, we use that dtype\n    2. If dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\n        weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\n    we also may have config.dtype available, but we won't rely on it till v5\n    \"\"\"\n    dtype_orig = None\n    is_sharded = sharded_metadata is not None\n\n    if dtype is not None:\n        if isinstance(dtype, str):\n            if dtype == \"auto\":\n                if hasattr(config, \"dtype\") and config.dtype is not None:\n                    dtype = config.dtype\n                    logger.info(f\"Will use dtype={dtype} as defined in model's config object\")\n                else:\n                    if is_sharded and \"dtype\" in sharded_metadata:\n                        dtype = sharded_metadata[\"dtype\"]\n                    elif state_dict is not None:\n                        dtype = get_state_dict_dtype(state_dict)\n                    else:\n                        state_dict = load_state_dict(\n                            checkpoint_files[0], map_location=\"meta\", weights_only=weights_only\n                        )\n                        dtype = get_state_dict_dtype(state_dict)\n                    logger.info(\n                        \"Since the `dtype` attribute can't be found in model's config object, \"\n                        \"will use dtype={dtype} as derived from model's weights\"\n                    )\n            elif hasattr(torch, dtype):\n                dtype = getattr(torch, dtype)\n                config.dtype = dtype\n                for sub_config_key in config.sub_configs:\n                    if (sub_config := getattr(config, sub_config_key)) is not None:\n                        sub_config.dtype = dtype\n        elif isinstance(dtype, torch.dtype):\n            config.dtype = dtype\n            for sub_config_key in config.sub_configs:\n                if (sub_config := getattr(config, sub_config_key)) is not None:\n                    sub_config.dtype = dtype\n        elif isinstance(dtype, dict):\n            for key, curr_dtype in dtype.items():\n                if hasattr(config, key):\n                    value = getattr(config, key)\n                    curr_dtype = curr_dtype if not isinstance(curr_dtype, str) else getattr(torch, curr_dtype)\n                    value.dtype = curr_dtype\n            # main torch dtype for modules that aren't part of any sub-config\n            dtype = dtype.get(\"\")\n            dtype = dtype if not isinstance(dtype, str) else getattr(torch, dtype)\n            config.dtype = dtype\n            if dtype is None:\n                dtype = torch.float32\n        else:\n            raise ValueError(\n                f\"`dtype` can be one of: `torch.dtype`, `'auto'`, a string of a valid `torch.dtype` or a `dict` with valid `dtype` \"\n                f\"for each sub-config in composite configs, but received {dtype}\"\n            )\n\n        dtype_orig = cls._set_default_dtype(dtype)\n    else:\n        # set fp32 as the default dtype for BC\n        default_dtype = torch.get_default_dtype()\n        config.dtype = default_dtype\n        for key in config.sub_configs:\n            if (sub_config := getattr(config, key)) is not None:\n                sub_config.dtype = default_dtype\n\n    return config, dtype, dtype_orig"
                },
                "component_dependencies": {
                    "_get_dtype": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_utils.py#get_state_dict_dtype",
                        "transformers/modeling_utils.py#load_state_dict",
                        "transformers/modeling_utils.py#logger"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_get_resolved_checkpoint_files": {
                "sorted_modules": {
                    "_get_resolved_checkpoint_files": "\n\ndef _get_resolved_checkpoint_files(\n    pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n    variant: Optional[str],\n    gguf_file: Optional[str],\n    use_safetensors: Optional[bool],\n    download_kwargs: DownloadKwargs,\n    user_agent: dict,\n    is_remote_code: bool,  # Because we can't determine this inside this function, we need it to be passed in\n    transformers_explicit_filename: Optional[str] = None,\n) -> tuple[Optional[list[str]], Optional[dict]]:\n    \"\"\"Get all the checkpoint filenames based on `pretrained_model_name_or_path`, and optional metadata if the\n    checkpoints are sharded.\n    This function will download the data if necessary.\n    \"\"\"\n    cache_dir = download_kwargs.get(\"cache_dir\")\n    force_download = download_kwargs.get(\"force_download\", False)\n    proxies = download_kwargs.get(\"proxies\")\n    local_files_only = download_kwargs.get(\"local_files_only\", False)\n    token = download_kwargs.get(\"token\")\n    revision = download_kwargs.get(\"revision\") or \"main\"\n    subfolder = download_kwargs.get(\"subfolder\", \"\")\n    commit_hash = download_kwargs.get(\"commit_hash\")\n    if transformers_explicit_filename is not None:\n        if not transformers_explicit_filename.endswith(\".safetensors\") and not transformers_explicit_filename.endswith(\n            \".safetensors.index.json\"\n        ):\n            raise ValueError(\n                \"The transformers file in the config seems to be incorrect: it is neither a safetensors file \"\n                \"(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \"\n                f\"{transformers_explicit_filename}\"\n            )\n\n    is_sharded = False\n\n    if pretrained_model_name_or_path is not None and gguf_file is None:\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n        is_local = os.path.isdir(pretrained_model_name_or_path)\n        if is_local:\n            if transformers_explicit_filename is not None:\n                # If the filename is explicitly defined, load this by default.\n                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, transformers_explicit_filename)\n                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n            elif use_safetensors is not False and os.path.isfile(\n                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant))\n            ):\n                # Load from a safetensors checkpoint\n                archive_file = os.path.join(\n                    pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant)\n                )\n            elif use_safetensors is not False and os.path.isfile(\n                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant))\n            ):\n                # Load from a sharded safetensors checkpoint\n                archive_file = os.path.join(\n                    pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant)\n                )\n                is_sharded = True\n            elif not use_safetensors and os.path.isfile(\n                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant))\n            ):\n                # Load from a PyTorch checkpoint\n                archive_file = os.path.join(\n                    pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_NAME, variant)\n                )\n            elif not use_safetensors and os.path.isfile(\n                os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant))\n            ):\n                # Load from a sharded PyTorch checkpoint\n                archive_file = os.path.join(\n                    pretrained_model_name_or_path, subfolder, _add_variant(WEIGHTS_INDEX_NAME, variant)\n                )\n                is_sharded = True\n            elif use_safetensors:\n                raise OSError(\n                    f\"Error no file named {_add_variant(SAFE_WEIGHTS_NAME, variant)} found in directory\"\n                    f\" {pretrained_model_name_or_path}.\"\n                )\n            else:\n                raise OSError(\n                    f\"Error no file named {_add_variant(SAFE_WEIGHTS_NAME, variant)}, or {_add_variant(WEIGHTS_NAME, variant)},\"\n                    f\" found in directory {pretrained_model_name_or_path}.\"\n                )\n        elif os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n            archive_file = pretrained_model_name_or_path\n            is_local = True\n        elif is_remote_url(pretrained_model_name_or_path):\n            filename = pretrained_model_name_or_path\n            resolved_archive_file = download_url(pretrained_model_name_or_path)\n        else:\n            # set correct filename\n            if transformers_explicit_filename is not None:\n                filename = transformers_explicit_filename\n                is_sharded = transformers_explicit_filename.endswith(\".safetensors.index.json\")\n            elif use_safetensors is not False:\n                filename = _add_variant(SAFE_WEIGHTS_NAME, variant)\n            else:\n                filename = _add_variant(WEIGHTS_NAME, variant)\n\n            try:\n                # Load from URL or cache if already cached\n                cached_file_kwargs = {\n                    \"cache_dir\": cache_dir,\n                    \"force_download\": force_download,\n                    \"proxies\": proxies,\n                    \"local_files_only\": local_files_only,\n                    \"token\": token,\n                    \"user_agent\": user_agent,\n                    \"revision\": revision,\n                    \"subfolder\": subfolder,\n                    \"_raise_exceptions_for_gated_repo\": False,\n                    \"_raise_exceptions_for_missing_entries\": False,\n                    \"_commit_hash\": commit_hash,\n                }\n                resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n\n                # Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\n                # result when internet is up, the repo and revision exist, but the file does not.\n                if resolved_archive_file is None and filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n                    # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n                    resolved_archive_file = cached_file(\n                        pretrained_model_name_or_path,\n                        _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant),\n                        **cached_file_kwargs,\n                    )\n                    if resolved_archive_file is not None:\n                        is_sharded = True\n                    elif use_safetensors:\n                        if revision == \"main\":\n                            resolved_archive_file, revision, is_sharded = auto_conversion(\n                                pretrained_model_name_or_path, **cached_file_kwargs\n                            )\n                        cached_file_kwargs[\"revision\"] = revision\n                        if resolved_archive_file is None:\n                            raise OSError(\n                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n                                f\" {_add_variant(SAFE_WEIGHTS_NAME, variant)} or {_add_variant(SAFE_WEIGHTS_INDEX_NAME, variant)} \"\n                                \"and thus cannot be loaded with `safetensors`. Please make sure that the model has \"\n                                \"been saved with `safe_serialization=True` or do not set `use_safetensors=True`.\"\n                            )\n                    else:\n                        # This repo has no safetensors file of any kind, we switch to PyTorch.\n                        filename = _add_variant(WEIGHTS_NAME, variant)\n                        resolved_archive_file = cached_file(\n                            pretrained_model_name_or_path, filename, **cached_file_kwargs\n                        )\n                if resolved_archive_file is None and filename == _add_variant(WEIGHTS_NAME, variant):\n                    # Maybe the checkpoint is sharded, we try to grab the index name in this case.\n                    resolved_archive_file = cached_file(\n                        pretrained_model_name_or_path,\n                        _add_variant(WEIGHTS_INDEX_NAME, variant),\n                        **cached_file_kwargs,\n                    )\n                    if resolved_archive_file is not None:\n                        is_sharded = True\n                if not local_files_only and not is_offline_mode():\n                    if resolved_archive_file is not None:\n                        # In a CI environment (CircleCI / Github Actions workflow runs) or in a pytest run,\n                        # we set `DISABLE_SAFETENSORS_CONVERSION=true` to prevent the conversion.\n                        if (\n                            filename in [WEIGHTS_NAME, WEIGHTS_INDEX_NAME]\n                            and os.getenv(\"DISABLE_SAFETENSORS_CONVERSION\", None) != \"true\"\n                        ):\n                            # If the PyTorch file was found, check if there is a safetensors file on the repository\n                            # If there is no safetensors file on the repositories, start an auto conversion\n                            safe_weights_name = SAFE_WEIGHTS_INDEX_NAME if is_sharded else SAFE_WEIGHTS_NAME\n                            has_file_kwargs = {\n                                \"revision\": revision,\n                                \"proxies\": proxies,\n                                \"token\": token,\n                                \"cache_dir\": cache_dir,\n                                \"local_files_only\": local_files_only,\n                            }\n                            cached_file_kwargs = {\n                                \"cache_dir\": cache_dir,\n                                \"force_download\": force_download,\n                                \"local_files_only\": local_files_only,\n                                \"user_agent\": user_agent,\n                                \"subfolder\": subfolder,\n                                \"_raise_exceptions_for_gated_repo\": False,\n                                \"_raise_exceptions_for_missing_entries\": False,\n                                \"_commit_hash\": commit_hash,\n                                **has_file_kwargs,\n                            }\n                            if (\n                                not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs)\n                                and not is_remote_code\n                            ):\n                                Thread(\n                                    target=auto_conversion,\n                                    args=(pretrained_model_name_or_path,),\n                                    kwargs={\"ignore_errors_during_conversion\": True, **cached_file_kwargs},\n                                    name=\"Thread-auto_conversion\",\n                                ).start()\n                    else:\n                        # Otherwise, no PyTorch file was found\n                        has_file_kwargs = {\n                            \"revision\": revision,\n                            \"proxies\": proxies,\n                            \"token\": token,\n                            \"cache_dir\": cache_dir,\n                            \"local_files_only\": local_files_only,\n                        }\n                        if variant is not None and has_file(\n                            pretrained_model_name_or_path, WEIGHTS_NAME, **has_file_kwargs\n                        ):\n                            raise OSError(\n                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n                                f\" {_add_variant(WEIGHTS_NAME, variant)} but there is a file without the variant\"\n                                f\" {variant}. Use `variant=None` to load this model from those weights.\"\n                            )\n                        else:\n                            raise OSError(\n                                f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n                                f\" {_add_variant(WEIGHTS_NAME, variant)} or {_add_variant(SAFE_WEIGHTS_NAME, variant)}.\"\n                            )\n\n            except OSError:\n                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted\n                # to the original exception.\n                raise\n            except Exception as e:\n                # For any other exception, we throw a generic error.\n                raise OSError(\n                    f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it\"\n                    \" from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                    f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n                    f\" directory containing a file named {_add_variant(WEIGHTS_NAME, variant)}.\"\n                ) from e\n\n        if is_local:\n            logger.info(f\"loading weights file {archive_file}\")\n            resolved_archive_file = archive_file\n        else:\n            logger.info(f\"loading weights file {filename} from cache at {resolved_archive_file}\")\n\n    elif gguf_file:\n        # Case 1: the GGUF file is present locally\n        if os.path.isfile(gguf_file):\n            resolved_archive_file = gguf_file\n        # Case 2: The GGUF path is a location on the Hub\n        # Load from URL or cache if already cached\n        else:\n            cached_file_kwargs = {\n                \"cache_dir\": cache_dir,\n                \"force_download\": force_download,\n                \"proxies\": proxies,\n                \"local_files_only\": local_files_only,\n                \"token\": token,\n                \"user_agent\": user_agent,\n                \"revision\": revision,\n                \"subfolder\": subfolder,\n                \"_raise_exceptions_for_gated_repo\": False,\n                \"_raise_exceptions_for_missing_entries\": False,\n                \"_commit_hash\": commit_hash,\n            }\n\n            resolved_archive_file = cached_file(pretrained_model_name_or_path, gguf_file, **cached_file_kwargs)\n\n    # We now download and resolve all checkpoint files if the checkpoint is sharded\n    sharded_metadata = None\n    if is_sharded:\n        checkpoint_files, sharded_metadata = get_checkpoint_shard_files(\n            pretrained_model_name_or_path,\n            resolved_archive_file,\n            cache_dir=cache_dir,\n            force_download=force_download,\n            proxies=proxies,\n            local_files_only=local_files_only,\n            token=token,\n            user_agent=user_agent,\n            revision=revision,\n            subfolder=subfolder,\n            _commit_hash=commit_hash,\n        )\n    else:\n        checkpoint_files = [resolved_archive_file] if pretrained_model_name_or_path is not None else None\n\n    return checkpoint_files, sharded_metadata"
                },
                "component_dependencies": {
                    "_get_resolved_checkpoint_files": [
                        "transformers/modeling_utils.py#_add_variant",
                        "transformers/modeling_utils.py#logger",
                        "transformers/safetensors_conversion.py#auto_conversion",
                        "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                        "transformers/utils.py#SAFE_WEIGHTS_NAME",
                        "transformers/utils.py#WEIGHTS_INDEX_NAME",
                        "transformers/utils.py#WEIGHTS_NAME",
                        "transformers/utils.py#cached_file",
                        "transformers/utils.py#download_url",
                        "transformers/utils.py#has_file",
                        "transformers/utils.py#is_offline_mode",
                        "transformers/utils.py#is_remote_url",
                        "transformers/utils/hub.py#DownloadKwargs",
                        "transformers/utils/hub.py#get_checkpoint_shard_files"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_get_tied_weight_keys": {
                "sorted_modules": {
                    "_get_tied_weight_keys": "\n\ndef _get_tied_weight_keys(module: nn.Module, prefix=\"\"):\n    tied_weight_keys = []\n    if getattr(module, \"_tied_weights_keys\", None) is not None:\n        names = [f\"{prefix}.{k}\" if prefix else k for k in module._tied_weights_keys]\n        tied_weight_keys.extend(names)\n    if getattr(module, \"_dynamic_tied_weights_keys\", None) is not None:\n        names = [f\"{prefix}.{k}\" if prefix else k for k in module._dynamic_tied_weights_keys]\n        tied_weight_keys.extend(names)\n    for name, submodule in module.named_children():\n        local_prefix = f\"{prefix}.{name}\" if prefix else name\n        tied_weight_keys.extend(_get_tied_weight_keys(submodule, prefix=local_prefix))\n    return tied_weight_keys"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_infer_parameter_dtype": {
                "sorted_modules": {
                    "_infer_parameter_dtype": "\n\ndef _infer_parameter_dtype(\n    model: \"PreTrainedModel\",\n    param_name: str,\n    empty_param: torch.Tensor,\n    hf_quantizer: Optional[HfQuantizer] = None,\n) -> Union[bool, Optional[torch.dtype]]:\n    try:\n        old_param = model.get_parameter_or_buffer(param_name)\n    except Exception as e:\n        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method in {\n            QuantizationMethod.HQQ,\n            QuantizationMethod.QUARK,\n            QuantizationMethod.MXFP4,\n            QuantizationMethod.BITS_AND_BYTES,\n        }:\n            return True, None\n        else:\n            raise e\n    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n    # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n    # in int/uint/bool and not cast them.\n    casting_dtype = None\n    is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n    if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n        # dtype that was instantiated in the meta model -- note that this respects subconfigs dtypes\n        if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, param_name):\n            casting_dtype = model.config._pre_quantization_dtype\n        else:\n            casting_dtype = old_param.dtype\n    return old_param is not None and old_param.is_contiguous(), casting_dtype"
                },
                "component_dependencies": {
                    "_infer_parameter_dtype": [
                        "transformers/quantizers.py#HfQuantizer",
                        "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                        "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                        "transformers/utils/quantization_config.py#QuantizationMethod.MXFP4",
                        "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_init_weights": {
                "sorted_modules": {
                    "_init_weights": "_init_weights = True"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_is_ds_init_called": {
                "sorted_modules": {
                    "_is_ds_init_called": "_is_ds_init_called = False"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_is_dtensor_available": {
                "sorted_modules": {
                    "_is_dtensor_available": "_is_dtensor_available = _torch_distributed_available and is_torch_greater_or_equal(\"2.5\")"
                },
                "component_dependencies": {
                    "_is_dtensor_available": [
                        "transformers/modeling_utils.py#_torch_distributed_available",
                        "transformers/utils.py#is_torch_greater_or_equal"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_is_quantized": {
                "sorted_modules": {
                    "_is_quantized": "_is_quantized = False"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_load_parameter_into_model": {
                "sorted_modules": {
                    "_load_parameter_into_model": "\n\ndef _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n    \"\"\"Cast a single parameter `param_name` into the `model`, with value `tensor`.\"\"\"\n    module, param_type = get_module_from_name(model, param_name)\n    # This will check potential shape mismatch if skipped before\n    module.load_state_dict({param_type: tensor}, strict=False, assign=True)"
                },
                "component_dependencies": {
                    "_load_parameter_into_model": [
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_torch_distributed_available": {
                "sorted_modules": {
                    "_torch_distributed_available": "\n\n_torch_distributed_available = torch.distributed.is_available()"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#get_parameter_dtype": {
                "sorted_modules": {
                    "get_parameter_dtype": "\n\ndef get_parameter_dtype(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n    \"\"\"\n    Returns the first found floating dtype in parameters if there is one, otherwise returns the last dtype it found.\n    \"\"\"\n    last_dtype = None\n    for t in parameter.parameters():\n        last_dtype = t.dtype\n        if t.is_floating_point():\n            # Adding fix for https://github.com/pytorch/xla/issues/4152\n            # Fixes issue where the model code passes a value that is out of range for XLA_USE_BF16=1\n            # and XLA_DOWNCAST_BF16=1 so the conversion would cast it to -inf\n            # NOTE: `is_torch_xla_available()` is checked last as it induces a graph break in torch dynamo\n            if XLA_USE_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():\n                return torch.bfloat16\n            if XLA_DOWNCAST_BF16 in ENV_VARS_TRUE_VALUES and is_torch_xla_available():\n                if t.dtype == torch.float:\n                    return torch.bfloat16\n                if t.dtype == torch.double:\n                    return torch.float32\n            return t.dtype\n\n    if last_dtype is not None:\n        # if no floating dtype was found return whatever the first dtype is\n        return last_dtype\n\n    # For nn.DataParallel compatibility in PyTorch > 1.5\n    def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n        tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n        return tuples\n\n    gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n    last_tuple = None\n    for gen_tuple in gen:\n        last_tuple = gen_tuple\n        if gen_tuple[1].is_floating_point():\n            return gen_tuple[1].dtype\n\n    if last_tuple is not None:\n        # fallback to the last dtype\n        return last_tuple[1].dtype\n\n    # fallback to buffer dtype\n    for t in parameter.buffers():\n        last_dtype = t.dtype\n        if t.is_floating_point():\n            return t.dtype\n    return last_dtype"
                },
                "component_dependencies": {
                    "get_parameter_dtype": [
                        "transformers/modeling_utils.py#XLA_DOWNCAST_BF16",
                        "transformers/modeling_utils.py#XLA_USE_BF16",
                        "transformers/utils.py#is_torch_xla_available",
                        "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#is_accelerator_device": {
                "sorted_modules": {
                    "is_accelerator_device": "\n\ndef is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n    \"\"\"Check if the device is an accelerator. We need to function, as device_map can be \"disk\" as well, which is not\n    a proper `torch.device`.\n    \"\"\"\n    if device == \"disk\":\n        return False\n    else:\n        return torch.device(device).type not in [\"meta\", \"cpu\"]"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#is_local_dist_rank_0": {
                "sorted_modules": {
                    "is_local_dist_rank_0": "\n\ndef is_local_dist_rank_0():\n    return (\n        torch.distributed.is_available()\n        and torch.distributed.is_initialized()\n        and int(os.environ.get(\"LOCAL_RANK\", \"-1\")) == 0\n    )"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#load_shard_file": {
                "sorted_modules": {
                    "load_shard_file": "\n\ndef load_shard_file(args):\n    (\n        shard_file,\n        state_dict,\n        disk_only_shard_files,\n        is_quantized,\n        device_map,\n        hf_quantizer,\n        key_renaming_mapping,\n        weights_only,\n        model,\n        reverse_key_renaming_mapping,\n        disk_offload_folder,\n        disk_offload_index,\n        device_mesh,\n    ) = args\n\n    # Skip the load for shards that only contain disk-offloaded weights\n    if shard_file in disk_only_shard_files:\n        return [], disk_offload_index\n\n    map_location = \"cpu\"\n    if shard_file.endswith(\".safetensors\") and not (is_deepspeed_zero3_enabled() and not is_quantized):\n        map_location = \"meta\"\n\n    # If shard_file is \"\", we use the existing state_dict instead of loading it\n    if shard_file != \"\":\n        state_dict = load_state_dict(\n            shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only\n        )\n\n    # Fix the key names\n    state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n\n    error_msgs = []\n    if is_deepspeed_zero3_enabled() and not is_quantized:\n        error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n    # Skip it with fsdp on ranks other than 0\n    elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n        disk_offload_index = _load_state_dict_into_meta_model(\n            model,\n            state_dict,\n            shard_file,\n            reverse_key_renaming_mapping,\n            device_map=device_map,\n            disk_offload_folder=disk_offload_folder,\n            disk_offload_index=disk_offload_index,\n            hf_quantizer=hf_quantizer,\n            device_mesh=device_mesh,\n        )\n\n    return error_msgs, disk_offload_index"
                },
                "component_dependencies": {
                    "load_shard_file": [
                        "transformers/integrations.py#is_deepspeed_zero3_enabled",
                        "transformers/integrations.py#is_fsdp_enabled",
                        "transformers/integrations/deepspeed.py#_load_state_dict_into_zero3_model",
                        "transformers/modeling_utils.py#_load_state_dict_into_meta_model",
                        "transformers/modeling_utils.py#is_local_dist_rank_0",
                        "transformers/modeling_utils.py#load_state_dict"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#load_shard_files_with_threadpool": {
                "sorted_modules": {
                    "load_shard_files_with_threadpool": "\n\ndef load_shard_files_with_threadpool(args_list):\n    num_workers = int(os.environ.get(\"HF_PARALLEL_LOADING_WORKERS\", \"8\"))\n\n    # Do not spawn anymore workers than you need\n    num_workers = min(len(args_list), num_workers)\n\n    logger.info(f\"Loading model weights in parallel with {num_workers} workers...\")\n\n    error_msgs = []\n\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        with logging.tqdm(total=len(args_list), desc=\"Loading checkpoint shards\") as pbar:\n            futures = [executor.submit(load_shard_file, arg) for arg in args_list]\n            for future in as_completed(futures):\n                _error_msgs, disk_offload_index = future.result()\n\n                error_msgs += _error_msgs\n\n                pbar.update(1)\n\n    return error_msgs, disk_offload_index"
                },
                "component_dependencies": {
                    "load_shard_files_with_threadpool": [
                        "transformers/modeling_utils.py#load_shard_file",
                        "transformers/modeling_utils.py#logger",
                        "transformers/utils.py#logging.tqdm"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#load_state_dict": {
                "sorted_modules": {
                    "load_state_dict": "\n\ndef load_state_dict(\n    checkpoint_file: Union[str, os.PathLike],\n    is_quantized: bool = False,\n    map_location: Optional[Union[str, torch.device]] = \"cpu\",\n    weights_only: bool = True,\n):\n    \"\"\"\n    Reads a `safetensor` or a `.bin` checkpoint file. We load the checkpoint on \"cpu\" by default.\n    \"\"\"\n    # Use safetensors if possible\n    if checkpoint_file.endswith(\".safetensors\"):\n        with safe_open(checkpoint_file, framework=\"pt\") as f:\n            state_dict = {}\n            for k in f.keys():\n                if map_location == \"meta\":\n                    _slice = f.get_slice(k)\n                    k_dtype = _slice.get_dtype()\n                    if k_dtype in str_to_torch_dtype:\n                        dtype = str_to_torch_dtype[k_dtype]\n                    else:\n                        raise ValueError(f\"Cannot load safetensors of unknown dtype {k_dtype}\")\n                    state_dict[k] = torch.empty(size=_slice.get_shape(), dtype=dtype, device=\"meta\")\n                else:\n                    state_dict[k] = f.get_tensor(k)\n            return state_dict\n\n    # Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\n    if weights_only:\n        check_torch_load_is_safe()\n    try:\n        if map_location is None:\n            if (\n                (\n                    is_deepspeed_zero3_enabled()\n                    and torch.distributed.is_initialized()\n                    and torch.distributed.get_rank() > 0\n                )\n                or (is_fsdp_enabled() and not is_local_dist_rank_0())\n            ) and not is_quantized:\n                map_location = \"meta\"\n            else:\n                map_location = \"cpu\"\n        extra_args = {}\n        # mmap can only be used with files serialized with zipfile-based format.\n        if isinstance(checkpoint_file, str) and map_location != \"meta\" and is_zipfile(checkpoint_file):\n            extra_args = {\"mmap\": True}\n        return torch.load(\n            checkpoint_file,\n            map_location=map_location,\n            weights_only=weights_only,\n            **extra_args,\n        )\n    except Exception as e:\n        try:\n            with open(checkpoint_file) as f:\n                if f.read(7) == \"version\":\n                    raise OSError(\n                        \"You seem to have cloned a repository without having git-lfs installed. Please install \"\n                        \"git-lfs and run `git lfs install` followed by `git lfs pull` in the folder \"\n                        \"you cloned.\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\n                        \"model. Make sure you have saved the model properly.\"\n                    ) from e\n        except (UnicodeDecodeError, ValueError):\n            raise OSError(f\"Unable to load weights from pytorch checkpoint file '{checkpoint_file}'.\")"
                },
                "component_dependencies": {
                    "load_state_dict": [
                        "transformers/integrations.py#is_deepspeed_zero3_enabled",
                        "transformers/integrations.py#is_fsdp_enabled",
                        "transformers/modeling_utils.py#is_local_dist_rank_0",
                        "transformers/modeling_utils.py#str_to_torch_dtype",
                        "transformers/utils.py#check_torch_load_is_safe"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#no_init_weights": {
                "sorted_modules": {
                    "no_init_weights": "\n\n@contextmanager\ndef no_init_weights():\n    \"\"\"\n    Context manager to globally disable weight initialization to speed up loading large models.\n    \"\"\"\n    global _init_weights\n    old_init_weights = _init_weights\n\n    _init_weights = False\n\n    def _skip_init(*args, **kwargs):\n        pass\n\n    # Save the original initialization functions\n    for name, init_func in TORCH_INIT_FUNCTIONS.items():\n        setattr(torch.nn.init, name, _skip_init)\n\n    try:\n        yield\n    finally:\n        _init_weights = old_init_weights\n        # Restore the original initialization functions\n        for name, init_func in TORCH_INIT_FUNCTIONS.items():\n            setattr(torch.nn.init, name, init_func)"
                },
                "component_dependencies": {
                    "no_init_weights": [
                        "transformers/modeling_utils.py#TORCH_INIT_FUNCTIONS",
                        "transformers/modeling_utils.py#_init_weights"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#restore_default_dtype": {
                "sorted_modules": {
                    "restore_default_dtype": "\n\ndef restore_default_dtype(func):\n    \"\"\"\n    Decorator to restore the default torch dtype\n    at the end of the function. Serves\n    as a backup in case calling the function raises\n    an error after the function has changed the default dtype but before it could restore it.\n    \"\"\"\n\n    @wraps(func)\n    def _wrapper(*args, **kwargs):\n        old_dtype = torch.get_default_dtype()\n        try:\n            return func(*args, **kwargs)\n        finally:\n            torch.set_default_dtype(old_dtype)\n\n    return _wrapper"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#set_quantized_state": {
                "sorted_modules": {
                    "set_quantized_state": "\n\n@contextmanager\ndef set_quantized_state():\n    global _is_quantized\n    _is_quantized = True\n    try:\n        yield\n    finally:\n        _is_quantized = False"
                },
                "component_dependencies": {
                    "set_quantized_state": [
                        "transformers/modeling_utils.py#_is_quantized"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#set_zero3_state": {
                "sorted_modules": {
                    "set_zero3_state": "\n\n# Skip recursive calls to deepspeed.zero.Init to avoid pinning errors.\n# This issue occurs with ZeRO stage 3 when using NVMe offloading.\n# For more details, refer to issue #34429.\n@contextmanager\ndef set_zero3_state():\n    global _is_ds_init_called\n    _is_ds_init_called = True\n    try:\n        yield\n    finally:\n        _is_ds_init_called = False"
                },
                "component_dependencies": {
                    "set_zero3_state": [
                        "transformers/modeling_utils.py#_is_ds_init_called"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#unwrap_model": {
                "sorted_modules": {
                    "unwrap_model": "\n\ndef unwrap_model(model: nn.Module, recursive: bool = False) -> nn.Module:\n    \"\"\"\n    Recursively unwraps a model from potential containers (as used in distributed training).\n\n    Args:\n        model (`torch.nn.Module`): The model to unwrap.\n        recursive (`bool`, *optional*, defaults to `False`):\n            Whether to recursively extract all cases of `module.module` from `model` as well as unwrap child sublayers\n            recursively, not just the top-level distributed containers.\n    \"\"\"\n    # Use accelerate implementation if available (should always be the case when using torch)\n    # This is for pytorch, as we also have to handle things like dynamo\n    if is_accelerate_available():\n        kwargs = {}\n        if recursive:\n            kwargs[\"recursive\"] = recursive\n        return extract_model_from_parallel(model, **kwargs)\n    else:\n        # since there could be multiple levels of wrapping, unwrap recursively\n        if hasattr(model, \"module\"):\n            return unwrap_model(model.module)\n        else:\n            return model"
                },
                "component_dependencies": {
                    "unwrap_model": [
                        "transformers/utils.py#is_accelerate_available"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/pytorch_utils.py#id_tensor_storage": {
                "sorted_modules": {
                    "id_tensor_storage": "\n\ndef id_tensor_storage(tensor: torch.Tensor) -> tuple[torch.device, int, int]:\n    \"\"\"\n    Unique identifier to a tensor storage. Multiple different tensors can share the same underlying storage. For\n    example, \"meta\" tensors all share the same storage, and thus their identifier will all be equal. This identifier is\n    guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\n    non-overlapping lifetimes may have the same id.\n    \"\"\"\n    if _torch_distributed_available and is_torch_greater_or_equal(\"2.5\"):\n        from torch.distributed.tensor import DTensor\n\n        if isinstance(tensor, DTensor):\n            local_tensor = tensor.to_local()\n            return tensor.device, local_tensor.storage().data_ptr(), tensor.nbytes\n\n    if tensor.device.type == \"xla\" and is_torch_xla_available():\n        # NOTE: xla tensors dont have storage\n        # use some other unique id to distinguish.\n        # this is a XLA tensor, it must be created using torch_xla's\n        # device. So the following import is safe:\n        import torch_xla\n\n        unique_id = torch_xla._XLAC._xla_get_tensor_id(tensor)\n    else:\n        unique_id = storage_ptr(tensor)\n\n    return tensor.device, unique_id, storage_size(tensor)"
                },
                "component_dependencies": {
                    "id_tensor_storage": [
                        "transformers/pytorch_utils.py#_torch_distributed_available",
                        "transformers/utils.py#is_torch_greater_or_equal",
                        "transformers/utils.py#is_torch_xla_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/auto.py#get_hf_quantizer": {
                "sorted_modules": {
                    "get_hf_quantizer": "\n\ndef get_hf_quantizer(config, quantization_config, dtype, device_map, weights_only, user_agent):\n    pre_quantized = hasattr(config, \"quantization_config\")\n    if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n        pre_quantized = False\n\n    if pre_quantized or quantization_config is not None:\n        if pre_quantized:\n            config.quantization_config = AutoHfQuantizer.merge_quantization_configs(\n                config.quantization_config, quantization_config\n            )\n        else:\n            config.quantization_config = quantization_config\n\n        hf_quantizer = AutoHfQuantizer.from_config(\n            config.quantization_config,\n            pre_quantized=pre_quantized,\n        )\n    else:\n        hf_quantizer = None\n\n    if hf_quantizer is not None:\n        hf_quantizer.validate_environment(\n            dtype=dtype,\n            device_map=device_map,\n            weights_only=weights_only,\n        )\n        dtype = hf_quantizer.update_dtype(dtype)\n        device_map = hf_quantizer.update_device_map(device_map)\n        config = hf_quantizer.update_tp_plan(config)\n        config = hf_quantizer.update_ep_plan(config)\n\n        # In order to ensure popular quantization methods are supported. Can be disable with `disable_telemetry`\n        if not getattr(hf_quantizer.quantization_config, \"dequantize\", False):\n            quant_method = hf_quantizer.quantization_config.quant_method\n            user_agent[\"quant\"] = getattr(quant_method, \"value\", quant_method)\n    return hf_quantizer, config, dtype, device_map"
                },
                "component_dependencies": {
                    "get_hf_quantizer": [
                        "transformers/quantizers/auto.py#AutoHfQuantizer"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizers_utils.py#get_module_from_name": {
                "sorted_modules": {
                    "get_module_from_name": "\n\ndef get_module_from_name(module, tensor_name: str) -> tuple[Any, str]:\n    if \".\" in tensor_name:\n        module_name, tensor_name = tensor_name.rsplit(\".\", 1)\n        module = module.get_submodule(module_name)\n    return module, tensor_name"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils.py#DUMMY_INPUTS": {
                "sorted_modules": {
                    "DUMMY_INPUTS": "DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME": {
                "sorted_modules": {
                    "SAFE_WEIGHTS_INDEX_NAME": "SAFE_WEIGHTS_INDEX_NAME = \"model.safetensors.index.json\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils.py#SAFE_WEIGHTS_NAME": {
                "sorted_modules": {
                    "SAFE_WEIGHTS_NAME": "SAFE_WEIGHTS_NAME = \"model.safetensors\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils.py#WEIGHTS_INDEX_NAME": {
                "sorted_modules": {
                    "WEIGHTS_INDEX_NAME": "WEIGHTS_INDEX_NAME = \"pytorch_model.bin.index.json\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils.py#WEIGHTS_NAME": {
                "sorted_modules": {
                    "WEIGHTS_NAME": "\n\nWEIGHTS_NAME = \"pytorch_model.bin\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES": {
                "sorted_modules": {
                    "ENV_VARS_TRUE_VALUES": "\n\nENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal": {
                "sorted_modules": {
                    "is_huggingface_hub_greater_or_equal": "\n\n@lru_cache\ndef is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n    is_available, hub_version = _is_package_available(\"huggingface_hub\", return_version=True)\n    if not is_available:\n        return False\n\n    if accept_dev:\n        return version.parse(version.parse(hub_version).base_version) >= version.parse(library_version)\n    else:\n        return version.parse(hub_version) >= version.parse(library_version)"
                },
                "component_dependencies": {
                    "is_huggingface_hub_greater_or_equal": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs": {
                "sorted_modules": {
                    "FlashAttentionKwargs": "\n\nclass FlashAttentionKwargs(TypedDict, total=False):\n    \"\"\"\n    Keyword arguments for Flash Attention with Compile.\n\n    Attributes:\n        cu_seq_lens_q (`torch.LongTensor`, *optional*)\n            Gets cumulative sequence length for query state.\n        cu_seq_lens_k (`torch.LongTensor`, *optional*)\n            Gets cumulative sequence length for key state.\n        max_length_q (`int`, *optional*):\n            Maximum sequence length for query state.\n        max_length_k (`int`, *optional*):\n            Maximum sequence length for key state.\n    \"\"\"\n\n    cu_seq_lens_q: Optional[torch.LongTensor]\n    cu_seq_lens_k: Optional[torch.LongTensor]\n    max_length_q: Optional[int]\n    max_length_k: Optional[int]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#apply_rotary_pos_emb": {
                "sorted_modules": {
                    "apply_rotary_pos_emb": "\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed"
                },
                "component_dependencies": {
                    "apply_rotary_pos_emb": [
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#rotate_half"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#eager_attention_forward": {
                "sorted_modules": {
                    "eager_attention_forward": "\n\ndef eager_attention_forward(\n    module: nn.Module,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    scaling: float,\n    dropout: float = 0.0,\n    **kwargs: Unpack[TransformersKwargs],\n):\n    key_states = repeat_kv(key, module.num_key_value_groups)\n    value_states = repeat_kv(value, module.num_key_value_groups)\n\n    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n    if attention_mask is not None:\n        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n        attn_weights = attn_weights + causal_mask\n\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n    attn_output = torch.matmul(attn_weights, value_states)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    return attn_output, attn_weights"
                },
                "component_dependencies": {
                    "eager_attention_forward": [
                        "transformers/models/qwen3_moe/modeling_qwen3_moe.py#repeat_kv",
                        "transformers/processing_utils.py#Unpack",
                        "transformers/utils.py#TransformersKwargs"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#requires": {
                "sorted_modules": {
                    "requires": "\n\ndef requires(*, backends=()):\n    \"\"\"\n    This decorator enables two things:\n    - Attaching a `__backends` tuple to an object to see what are the necessary backends for it\n      to execute correctly without instantiating it\n    - The '@requires' string is used to dynamically import objects\n    \"\"\"\n\n    if not isinstance(backends, tuple):\n        raise TypeError(\"Backends should be a tuple.\")\n\n    applied_backends = []\n    for backend in backends:\n        if backend in BACKENDS_MAPPING:\n            applied_backends.append(backend)\n        else:\n            if any(key in backend for key in [\"=\", \"<\", \">\"]):\n                applied_backends.append(Backend(backend))\n            else:\n                raise ValueError(f\"Backend should be defined in the BACKENDS_MAPPING. Offending backend: {backend}\")\n\n    def inner_fn(fun):\n        fun.__backends = applied_backends\n        return fun\n\n    return inner_fn"
                },
                "component_dependencies": {
                    "requires": [
                        "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                        "transformers/utils/import_utils.py#Backend"
                    ]
                },
                "warning": null
            },
            "transformers/__init__.py#__version__": {
                "sorted_modules": {
                    "__version__": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# When adding a new object to this init, remember to add it twice: once inside the `_import_structure` dictionary and\n# once inside the `if TYPE_CHECKING` branch. The `TYPE_CHECKING` should have import statements as usual, but they are\n# only there for type checking. The `_import_structure` is a dictionary submodule to list of object names, and is used\n# to defer the actual importing for when the objects are requested. This way `import transformers` provides the names\n# in the namespace without actually importing anything (and especially none of the backends).\n\n__version__ = \"5.0.0.dev0\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/configuration_utils.py#SpecificPreTrainedConfigType": {
                "sorted_modules": {
                    "SpecificPreTrainedConfigType": "\n\n# type hinting: specifying the type of config class that inherits from PreTrainedConfig\nSpecificPreTrainedConfigType = TypeVar(\"SpecificPreTrainedConfigType\", bound=\"PreTrainedConfig\")"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/configuration_utils.py#get_configuration_file": {
                "sorted_modules": {
                    "get_configuration_file": "\n\ndef get_configuration_file(configuration_files: list[str]) -> str:\n    \"\"\"\n    Get the configuration file to use for this version of transformers.\n\n    Args:\n        configuration_files (`list[str]`): The list of available configuration files.\n\n    Returns:\n        `str`: The configuration file to use.\n    \"\"\"\n    configuration_files_map = {}\n    for file_name in configuration_files:\n        if file_name.startswith(\"config.\") and file_name.endswith(\".json\") and file_name != \"config.json\":\n            v = file_name.removeprefix(\"config.\").removesuffix(\".json\")\n            configuration_files_map[v] = file_name\n    available_versions = sorted(configuration_files_map.keys())\n\n    # Defaults to FULL_CONFIGURATION_FILE and then try to look at some newer versions.\n    configuration_file = CONFIG_NAME\n    transformers_version = version.parse(__version__)\n    for v in available_versions:\n        if version.parse(v) <= transformers_version:\n            configuration_file = configuration_files_map[v]\n        else:\n            # No point going further since the versions are sorted.\n            break\n\n    return configuration_file"
                },
                "component_dependencies": {
                    "get_configuration_file": [
                        "transformers/__init__.py#__version__",
                        "transformers/utils.py#CONFIG_NAME"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint": {
                "sorted_modules": {
                    "load_gguf_checkpoint": "\n\ndef load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_load=None):\n    \"\"\"\n    Load a GGUF file and return a dictionary of parsed parameters containing tensors, the parsed\n    tokenizer and config attributes.\n\n    Args:\n        gguf_checkpoint_path (`str`):\n            The path the to GGUF file to load\n        return_tensors (`bool`, defaults to `False`):\n            Whether to read the tensors from the file and return them. Not doing so is faster\n            and only loads the metadata in memory.\n    \"\"\"\n    if is_gguf_available() and is_torch_available():\n        from gguf import GGUFReader, dequantize\n    else:\n        logger.error(\n            \"Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see \"\n            \"https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions.\"\n        )\n        raise ImportError(\"Please install torch and gguf>=0.10.0 to load a GGUF checkpoint in PyTorch.\")\n\n    reader = GGUFReader(gguf_checkpoint_path)\n    fields = reader.fields\n    reader_keys = list(fields.keys())\n\n    parsed_parameters = {k: {} for k in GGUF_TO_TRANSFORMERS_MAPPING}\n\n    architecture = read_field(reader, \"general.architecture\")[0]\n    # NOTE: Some GGUF checkpoints may miss `general.name` field in metadata\n    model_name = read_field(reader, \"general.name\")\n\n    updated_architecture = None\n    # in llama.cpp mistral models use the same architecture as llama. We need\n    # to add this patch to ensure things work correctly on our side.\n    if \"llama\" in architecture and \"mistral\" in model_name:\n        updated_architecture = \"mistral\"\n    # FIXME: Currently this implementation is only for flan-t5 architecture.\n    # It needs to be developed for supporting legacy t5.\n    elif \"t5\" in architecture or \"t5encoder\" in architecture:\n        parsed_parameters[\"config\"][\"is_gated_act\"] = True\n        if model_name and \"umt5\" in model_name[0].lower():\n            updated_architecture = \"umt5\"\n            if \"t5encoder\" in architecture:\n                parsed_parameters[\"config\"][\"architectures\"] = [\"UMT5EncoderModel\"]\n        else:\n            if \"t5encoder\" in architecture:\n                parsed_parameters[\"config\"][\"architectures\"] = [\"T5EncoderModel\"]\n            updated_architecture = \"t5\"\n    else:\n        updated_architecture = architecture\n\n    if \"qwen2moe\" in architecture:\n        updated_architecture = \"qwen2_moe\"\n    elif \"qwen3moe\" in architecture:\n        updated_architecture = \"qwen3_moe\"\n\n    # For stablelm architecture, we need to set qkv_bias and use_parallel_residual from tensors\n    # If `qkv_bias=True`, qkv_proj with bias will be present in the tensors\n    # If `use_parallel_residual=False`, ffn_norm will be present in the tensors\n    if \"stablelm\" in architecture:\n        attn_bias_name = {\"attn_q.bias\", \"attn_k.bias\", \"attn_v.bias\"}\n        ffn_norm_name = \"ffn_norm\"\n        qkv_bias = any(bias_name in tensor.name for tensor in reader.tensors for bias_name in attn_bias_name)\n        use_parallel_residual = any(ffn_norm_name in tensor.name for tensor in reader.tensors)\n        parsed_parameters[\"config\"][\"use_qkv_bias\"] = qkv_bias\n        parsed_parameters[\"config\"][\"use_parallel_residual\"] = not use_parallel_residual\n\n    if architecture not in GGUF_SUPPORTED_ARCHITECTURES and updated_architecture not in GGUF_SUPPORTED_ARCHITECTURES:\n        raise ValueError(f\"GGUF model with architecture {architecture} is not supported yet.\")\n\n    # Handle tie_word_embeddings, if lm_head.weight is not present in tensors,\n    # tie_word_embeddings is true otherwise false\n    exceptions = [\"falcon\", \"bloom\"]\n    parsed_parameters[\"config\"][\"tie_word_embeddings\"] = (\n        all(\"output.weight\" != tensor.name for tensor in reader.tensors) or architecture in exceptions\n    )\n\n    # List all key-value pairs in a columnized format\n    for gguf_key, field in reader.fields.items():\n        gguf_key = gguf_key.replace(architecture, updated_architecture)\n        split = gguf_key.split(\".\")\n        prefix = split[0]\n        config_key = \".\".join(split[1:])\n\n        value = [_gguf_parse_value(field.parts[_data_index], field.types) for _data_index in field.data]\n\n        if len(value) == 1:\n            value = value[0]\n\n        if isinstance(value, str) and architecture in value:\n            value = value.replace(architecture, updated_architecture)\n\n        for parameter, parameter_renames in GGUF_TO_TRANSFORMERS_MAPPING.items():\n            if prefix in parameter_renames and config_key in parameter_renames[prefix]:\n                renamed_config_key = parameter_renames[prefix][config_key]\n                if renamed_config_key == -1:\n                    continue\n\n                if renamed_config_key is not None:\n                    parsed_parameters[parameter][renamed_config_key] = value\n\n                if gguf_key in reader_keys:\n                    reader_keys.remove(gguf_key)\n\n        if gguf_key in reader_keys:\n            logger.info(f\"Some keys were not parsed and added into account {gguf_key} | {value}\")\n\n    # Gemma3 GGUF checkpoint only contains weights of text backbone\n    if parsed_parameters[\"config\"][\"model_type\"] == \"gemma3\":\n        parsed_parameters[\"config\"][\"model_type\"] = \"gemma3_text\"\n\n    if parsed_parameters[\"config\"][\"model_type\"] == \"lfm2\":\n        gguf_num_key_value_heads = parsed_parameters[\"config\"][\"num_key_value_heads\"]\n        # LFM2 GGUF checkpoint defines num_key_value_heads as a list of integers .e.g [0, 0, 8, 0, 0, 8, 0, 0, 8, 0, 8, 0, 8, 0, 8, 0] but we need to set it to the max value for HF\n        parsed_parameters[\"config\"][\"num_key_value_heads\"] = max(gguf_num_key_value_heads)\n        ## we already read the correct intermediate_size from the GGUF checkpoint so we need to set block_auto_adjust_ff_dim to False\n        parsed_parameters[\"config\"][\"block_auto_adjust_ff_dim\"] = False\n\n        ## llama.cpp defines the layers that are full-attention by looking at num_key_value_heads\n        ## we need to set the full_attn_idxs to the layers that are full-attention\n        parsed_parameters[\"config\"][\"full_attn_idxs\"] = [\n            i for i, num_kv_heads in enumerate(gguf_num_key_value_heads) if num_kv_heads > 0\n        ]\n\n    # retrieve config vocab_size from tokenizer\n    # Please refer to https://github.com/huggingface/transformers/issues/32526 for more details\n    if \"vocab_size\" not in parsed_parameters[\"config\"]:\n        tokenizer_parameters = parsed_parameters[\"tokenizer\"]\n        if \"tokens\" in tokenizer_parameters:\n            parsed_parameters[\"config\"][\"vocab_size\"] = len(tokenizer_parameters[\"tokens\"])\n        else:\n            logger.warning(\n                \"Can't find a way to retrieve missing config vocab_size from tokenizer parameters. \"\n                \"This will use default value from model config class and cause unexpected behavior.\"\n            )\n\n    if return_tensors:\n        parsed_parameters[\"tensors\"] = {}\n\n        tensor_key_mapping = get_gguf_hf_weights_map(model_to_load)\n        config = parsed_parameters.get(\"config\", {})\n\n        ProcessorClass = TENSOR_PROCESSORS.get(architecture, TensorProcessor)\n        processor = ProcessorClass(config=config)\n\n        for tensor in tqdm(reader.tensors, desc=\"Converting and de-quantizing GGUF tensors...\"):\n            name = tensor.name\n            weights = dequantize(tensor.data, tensor.tensor_type)\n\n            result = processor.process(\n                weights=weights,\n                name=name,\n                tensor_key_mapping=tensor_key_mapping,\n                parsed_parameters=parsed_parameters,\n            )\n\n            weights = result.weights\n            name = result.name\n\n            if name not in tensor_key_mapping:\n                continue\n\n            name = tensor_key_mapping[name]\n\n            parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n\n    if len(reader_keys) > 0:\n        logger.info(f\"Some keys of the GGUF file were not considered: {reader_keys}\")\n\n    return parsed_parameters"
                },
                "component_dependencies": {
                    "load_gguf_checkpoint": [
                        "transformers/integrations.py#_gguf_parse_value",
                        "transformers/modeling_gguf_pytorch_utils.py#GGUF_SUPPORTED_ARCHITECTURES",
                        "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING",
                        "transformers/modeling_gguf_pytorch_utils.py#TENSOR_PROCESSORS",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#get_gguf_hf_weights_map",
                        "transformers/modeling_gguf_pytorch_utils.py#logger",
                        "transformers/modeling_gguf_pytorch_utils.py#read_field",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils/import_utils.py#is_gguf_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils.py#CONFIG_NAME": {
                "sorted_modules": {
                    "CONFIG_NAME": "CONFIG_NAME = \"config.json\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/generic.py#is_timm_config_dict": {
                "sorted_modules": {
                    "is_timm_config_dict": "\n\ndef is_timm_config_dict(config_dict: dict[str, Any]) -> bool:\n    \"\"\"Checks whether a config dict is a timm config dict.\"\"\"\n    return \"pretrained_cfg\" in config_dict"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#is_torch_greater_or_equal": {
                "sorted_modules": {
                    "is_torch_greater_or_equal": "\n\n@lru_cache\ndef is_torch_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n    \"\"\"\n    Accepts a library version and returns True if the current version of the library is greater than or equal to the\n    given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n    2.7.0).\n    \"\"\"\n    if not is_torch_available():\n        return False\n\n    if accept_dev:\n        return version.parse(version.parse(get_torch_version()).base_version) >= version.parse(library_version)\n    else:\n        return version.parse(get_torch_version()) >= version.parse(library_version)"
                },
                "component_dependencies": {
                    "is_torch_greater_or_equal": [
                        "transformers/utils/import_utils.py#get_torch_version",
                        "transformers/utils/import_utils.py#is_torch_available"
                    ]
                },
                "warning": null
            },
            "transformers/masking_utils.py#find_packed_sequence_indices": {
                "sorted_modules": {
                    "find_packed_sequence_indices": "\n\ndef find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Find the indices of the sequence to which each new query token in the sequence belongs when using packed\n    tensor format (i.e. several sequences packed in the same batch dimension).\n\n    Args:\n        position_ids (`torch.Tensor`)\n            A 2D tensor of shape (batch_size, query_length) indicating the positions of each token in the sequences.\n\n    Returns:\n        A 2D tensor where each similar integer indicates that the tokens belong to the same sequence. For example, if we\n        pack 3 sequences of 2, 3 and 1 tokens respectively along a single batch dim, this will return [[0, 0, 1, 1, 1, 2]].\n    \"\"\"\n    # What separate different sequences is when 2 consecutive positions_ids are separated by more than 1. So\n    # taking the diff (by prepending the first value - 1 to keep correct indexing) and applying cumsum to the result\n    # gives exactly the sequence indices\n    # Note that we assume that a single sequence cannot span several batch dimensions, i.e. 1 single sequence\n    # cannot be part of the end of the first batch dim and the start of the 2nd one for example\n    first_dummy_value = position_ids[:, :1] - 1  # We just need the diff on this first value to be 1\n    position_diff = torch.diff(position_ids, prepend=first_dummy_value, dim=-1)\n    packed_sequence_mask = (position_diff != 1).cumsum(-1)\n\n    # Here it would be nice to return None if we did not detect packed sequence format, i.e. if `packed_sequence_mask[:, -1] == 0`\n    # but it causes issues with export\n    return packed_sequence_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/masking_utils.py#sliding_window_overlay": {
                "sorted_modules": {
                    "sliding_window_overlay": "\n\ndef sliding_window_overlay(sliding_window: int) -> Callable:\n    \"\"\"\n    This is an overlay depicting a sliding window pattern. Add it on top of a causal mask for a proper sliding\n    window mask.\n    \"\"\"\n\n    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n        return kv_idx > q_idx - sliding_window\n\n    return inner_mask"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS": {
                "sorted_modules": {
                    "ROPE_VALIDATION_FUNCTIONS": "\n\n# Like `ROPE_INIT_FUNCTIONS`, this validation function mapping can be dynamically updated for custom RoPE types.\nROPE_VALIDATION_FUNCTIONS = {\n    \"default\": _validate_default_rope_parameters,\n    \"linear\": _validate_linear_scaling_rope_parameters,\n    \"dynamic\": _validate_dynamic_scaling_rope_parameters,\n    \"yarn\": _validate_yarn_parameters,\n    \"longrope\": _validate_longrope_parameters,\n    \"llama3\": _validate_llama3_parameters,\n}"
                },
                "component_dependencies": {
                    "ROPE_VALIDATION_FUNCTIONS": [
                        "transformers/modeling_rope_utils.py#_validate_default_rope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_llama3_parameters",
                        "transformers/modeling_rope_utils.py#_validate_longrope_parameters",
                        "transformers/modeling_rope_utils.py#_validate_yarn_parameters"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters": {
                "sorted_modules": {
                    "_compute_dynamic_ntk_parameters": "\n\ndef _compute_dynamic_ntk_parameters(\n    config: Optional[PreTrainedConfig] = None,\n    device: Optional[\"torch.device\"] = None,\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   max_position_embeddings (`int`): The default sequence length used to update the dynamic RoPE at\n                inference time\n            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which `factor`\n                will be accessed. The value of `factor` is used to determine the new base frequency, along with the\n                current sequence length (seq_len), the maximum positional embeddings (max_position_embeddings), and the\n                computed dimensionality (dim) of the rotary embeddings. If seq_len <= max_position_embeddings, this\n                factor has no effect. If seq_len <= max_position_embeddings, this factor effectively stretches the\n                context window using an exponent derived from `dim`.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n                the first fraction of the head_dim. Defaults to 1.0.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length, used to update the dynamic RoPE at inference time. If `None` or shorter than\n            max_position_embeddings, this value will be overridden by max_position_embeddings.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n    \"\"\"\n    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n    max_position_embeddings = config.max_position_embeddings\n    factor = rope_parameters_dict[\"factor\"]\n    attention_factor = 1.0  # Unused in this type of RoPE\n\n    # seq_len: default to max_position_embeddings, e.g. at init time\n    if seq_len is None:\n        seq_len = max_position_embeddings\n    elif isinstance(seq_len, torch.Tensor):\n        seq_len = torch.maximum(\n            seq_len,\n            torch.tensor(max_position_embeddings, dtype=seq_len.dtype, device=seq_len.device),\n        )\n    else:\n        seq_len = max(seq_len, max_position_embeddings)\n\n    # Compute the inverse frequencies\n    base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_dynamic_ntk_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters": {
                "sorted_modules": {
                    "_compute_linear_scaling_rope_parameters": "\n\ndef _compute_linear_scaling_rope_parameters(\n    config: Optional[PreTrainedConfig] = None,\n    device: Optional[\"torch.device\"] = None,\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n                the first fraction of the head_dim. Defaults to 1.0.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length. Unused for this type of RoPE.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n    \"\"\"\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n    factor = rope_parameters_dict[\"factor\"]\n\n    # Gets the default RoPE parameters\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n    dim = int(head_dim * partial_rotary_factor)\n    attention_factor = 1.0  # Unused in this type of RoPE\n\n    # Compute the inverse frequencies\n    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n\n    # Then applies linear scaling to the frequencies.\n    # NOTE: originally, scaling was applied to the position_ids. However, we get `embs = inv_freq @ position_ids`, so\n    # applying scaling to the inverse frequencies is equivalent.\n    inv_freq /= factor\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_linear_scaling_rope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_llama3_parameters": {
                "sorted_modules": {
                    "_compute_llama3_parameters": "\n\ndef _compute_llama3_parameters(\n    config: PreTrainedConfig,\n    device: \"torch.device\",\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies for llama 3.1.\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   rope_parameters (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n                keys will be accessed:\n                *   `factor` (`float`, *optional*): The scaling factor applied to the inverse frequencies when 1) the\n                    wavelength is greater than `low_freq_wavelen` prior to smoothing, and 2) to all inverse frequencies\n                    during smoothing.\n                *   `high_freq_factor` (`float`): The scale factor used to compute `high_freq_wavelen` and\n                    the value for the denominator of the smoothing factor prior to the `low_freq_factor` shift.\n                *   `low_freq_factor` (`float`): The scale factor used to compute `low_freq_wavelen` and\n                    the shift applied to the numerator and denominator of the smoothing factor.\n                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n                *   `original_max_position_embeddings` (`int`): The original max position embeddings used\n                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*): If less than 1.0, inverse frequencies will be returned for\n                the first fraction of the head_dim. Defaults to 1.0.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length. Unused for this type of RoPE.\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin.\n    \"\"\"\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    # Gets the default RoPE parameters\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n    dim = int(head_dim * partial_rotary_factor)\n    attention_factor = 1.0  # Unused in this type of RoPE\n\n    # Compute the inverse frequencies\n    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n\n    factor = rope_parameters_dict[\"factor\"]  # `8` in the original implementation\n    low_freq_factor = rope_parameters_dict[\"low_freq_factor\"]  # `1` in the original implementation\n    high_freq_factor = rope_parameters_dict[\"high_freq_factor\"]  # `4` in the original implementation\n    old_context_len = rope_parameters_dict[\"original_max_position_embeddings\"]  # `8192` in the original implementation\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n\n    wavelen = 2 * math.pi / inv_freq\n    # wavelen < high_freq_wavelen: do nothing\n    # wavelen > low_freq_wavelen: divide by factor\n    inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)\n    # otherwise: interpolate between the two, using a smooth factor\n    smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n    smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama\n    is_medium_freq = ~(wavelen < high_freq_wavelen) * ~(wavelen > low_freq_wavelen)\n    inv_freq_llama = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)\n\n    return inv_freq_llama, attention_factor"
                },
                "component_dependencies": {
                    "_compute_llama3_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_longrope_parameters": {
                "sorted_modules": {
                    "_compute_longrope_parameters": "\n\ndef _compute_longrope_parameters(\n    config: PreTrainedConfig,\n    device: \"torch.device\",\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n    [original implementation](https://github.com/microsoft/LongRoPE)\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n            *   original_max_position_embeddings (`int`, *optional*): The original max position embeddings used during\n                pretraining. If not provided, defaults to `max_position_embeddings`.\n            *   rope_parameters (`dict[str, float]`): The standard RoPE scaling parameters, from which the following keys\n                will be accessed:\n                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied on the attention\n                    computation. If unspecified, it defaults to value recommended by the implementation, inferred from\n                    the value of `factor`.\n                *   `factor` (`float`, *optional*): The scaling factor to apply to the RoPE embeddings. If both\n                    `max_position_embeddings` and `original_max_position_embeddings` are provided, this value will be\n                    overridden s the ratio between those values.\n                *   `long_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n                    frequencies if `seq_len` is provided and greater than `original_max_position_embeddings`.\n                *   `short_factor` (`float`, *optional*): The scale factor applied when computing the inverse\n                    frequencies if `seq_len` is None or less-than-or-equal-to `original_max_position_embeddings`.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n                will be returned for the first fraction of the head_dim.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin.\n    \"\"\"\n    # TODO (joao): use the new `original_max_position_embeddings` from rope_parameters\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n\n    long_factor = rope_parameters_dict[\"long_factor\"]\n    short_factor = rope_parameters_dict[\"short_factor\"]\n    factor = rope_parameters_dict.get(\"factor\")\n    attention_factor = rope_parameters_dict.get(\"attention_factor\")\n\n    # NOTE: Phi3 (and potentially other models) modify `max_position_embeddings` and have a\n    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n    # values to compute the default attention scaling factor, instead of using `factor`.\n    if original_max_position_embeddings := getattr(config, \"original_max_position_embeddings\", None):\n        factor = config.max_position_embeddings / original_max_position_embeddings\n    else:\n        original_max_position_embeddings = config.max_position_embeddings\n\n    # Sets the attention factor as suggested in the paper\n    if attention_factor is None:\n        if factor <= 1.0:\n            attention_factor = 1.0\n        else:\n            attention_factor = math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))\n\n    # Compute the inverse frequencies -- scaled based on the target sequence length\n    if seq_len and seq_len > original_max_position_embeddings:\n        ext_factors = torch.tensor(long_factor, dtype=torch.float32, device=device)\n    else:\n        ext_factors = torch.tensor(short_factor, dtype=torch.float32, device=device)\n    inv_freq_shape = torch.arange(0, dim, 2, dtype=torch.int64, device=device).float() / dim\n    inv_freq = 1.0 / (ext_factors * base**inv_freq_shape)\n\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_longrope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_compute_yarn_parameters": {
                "sorted_modules": {
                    "_compute_yarn_parameters": "\n\ndef _compute_yarn_parameters(\n    config: PreTrainedConfig,\n    device: \"torch.device\",\n    seq_len: Optional[int] = None,\n    layer_type: Optional[str] = None,\n) -> tuple[\"torch.Tensor\", float]:\n    \"\"\"\n    Computes the inverse frequencies with NTK scaling. Please refer to the\n    [original paper](https://huggingface.co/papers/2309.00071)\n\n    Args:\n        config ([`~transformers.PreTrainedConfig`]):\n            The model configuration. This function assumes that the config will provide at least the following\n            properties:\n\n            *   rope_theta (`float`): The base wavelength from which the inverse frequencies will be derived.\n            *   hidden_size (`int`): The numerator when deriving a head_dim, if not provided directly.\n            *   num_attention_heads (`int`): The denominator when deriving a head_dim, if not provided directly.\n            *   max_position_embeddings (`int`): The maximum length of the positional embeddings.\n            *   rope_parameters (`dict[str, float | int]`): The standard RoPE scaling parameters, from which the following\n                keys will be accessed:\n                *   `attention_factor` (`float`, *optional*): The scaling factor to be applied to the computed cos/sin.\n                    If None, the value is inferred from `factor`, `mscale`, and `mscale_all_dim` as avaialble.\n                *   `beta_fast` (`float`, *optional*, defaults to 32): Parameter to set the boundary for extrapolation\n                    (only) in the linear ramp function.\n                *   `beta_slow` (`float`, *optional*, defaults to 1): Parameter to set the boundary for interpolation\n                    (only) in the linear ramp function.\n                *   `factor` (`float`, *optional*): The scaling factor applied when interpolating the position IDs to\n                    extend the possible context length. Additionally, if `attention_factor` is None, the log of this\n                    value is used to compute a value for `attention_factor`, possibly in conjunciton with `mscale` and\n                    `mscale_all_dim`, if provided.\n                *   `mscale` (`float`, *optional*): If `attention_factor` is None and both `mscale` and\n                    `mscale_all_dim` are provided, `mscale` acts scalar augmenting `log(factor)` when computing the\n                    numerator for the inferred value of `attention_factor`. If not provided, `attention_factor` will be\n                    calculated based on `factor` only.\n                *   `mscale_all_dim` (`float`, *optional*): If `attention_factor` is None and both `mscale` and\n                    `mscale_all_dim` are provided, `mscale_all_dim` acts scalar augmenting `log(factor)` when computing\n                    the denominator for the inferred value of `attention_factor`. If not provided, `attention_factor`\n                    will be calculated based on `factor` only.\n                *   `original_max_position_embeddings` (`int`, *optional*): The original max position embeddings used\n                    during pretraining. If not provided, the function falls back to `max_position_embeddings`.\n                *   `truncate` (`bool`, *optional*): Whether to truncate the correction range.\n\n            Additionally, this function will make use of the following properties if they are found in the config:\n\n            *   head_dim (`int`, *optional*): The size of the key-value heads in the model. If None, this value will be\n                derived as hidden_size // num_attention_heads.\n            *   partial_rotary_factor (`float`, *optional*, defaults to 1.0): If less than 1.0, inverse frequencies\n                will be returned for the first fraction of the head_dim.\n        device (`torch.device`):\n            The device to use for initialization of the inverse frequencies.\n        seq_len (`int`, *optional*):\n            The current sequence length. Unused for this type of RoPE.\n\n    Returns:\n        Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n        post-processing scaling factor applied to the computed cos/sin.\n    \"\"\"\n    # For backward compatibility standardize the `rope_parameters_dict` if it uses old format\n    standardize_rope_params(config)\n    rope_parameters_dict = config.rope_parameters[layer_type] if layer_type is not None else config.rope_parameters\n\n    base = rope_parameters_dict[\"rope_theta\"]\n    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n\n    factor = rope_parameters_dict[\"factor\"]\n    attention_factor = rope_parameters_dict.get(\"attention_factor\")\n    mscale = rope_parameters_dict.get(\"mscale\")\n    mscale_all_dim = rope_parameters_dict.get(\"mscale_all_dim\")\n\n    # NOTE: DeekSeek-V3 (and potentially other models) modify `max_position_embeddings` and have a\n    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n    # values to compute the default attention scaling factor, instead of using `factor`.\n    if \"original_max_position_embeddings\" in rope_parameters_dict:\n        original_max_position_embeddings = rope_parameters_dict[\"original_max_position_embeddings\"]\n        factor = config.max_position_embeddings / original_max_position_embeddings\n    else:\n        original_max_position_embeddings = config.max_position_embeddings\n\n    def get_mscale(scale, mscale=1):\n        if scale <= 1:\n            return 1.0\n        return 0.1 * mscale * math.log(scale) + 1.0\n\n    # Sets the attention factor as suggested in the paper\n    if attention_factor is None:\n        if mscale and mscale_all_dim:\n            attention_factor = float(get_mscale(factor, mscale) / get_mscale(factor, mscale_all_dim))\n        else:\n            attention_factor = get_mscale(factor)\n\n    # Optional config options\n    # beta_fast/beta_slow: as suggested in the paper, default to 32/1 (correspondingly)\n    beta_fast = rope_parameters_dict.get(\"beta_fast\") or 32\n    beta_slow = rope_parameters_dict.get(\"beta_slow\") or 1\n\n    # Compute the inverse frequencies\n    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n        \"\"\"Inverse dimension formula to find the dimension based on the number of rotations\"\"\"\n        return (dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi))) / (2 * math.log(base))\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings, truncate):\n        \"\"\"Find dimension range bounds based on rotations\"\"\"\n        low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n        high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n        if truncate:\n            low = math.floor(low)\n            high = math.ceil(high)\n        return max(low, 0), min(high, dim - 1)\n\n    def linear_ramp_factor(min, max, dim):\n        if min == max:\n            max += 0.001  # Prevent singularity\n\n        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n        ramp_func = torch.clamp(linear_func, 0, 1)\n        return ramp_func\n\n    # Note on variable naming: \"interpolation\" comes from the original technique, where we interpolate the position IDs\n    # to expand the possible context length. In other words, interpolation = apply scaling factor.\n    pos_freqs = base ** (torch.arange(0, dim, 2).to(device=device, dtype=torch.float) / dim)\n    inv_freq_extrapolation = 1.0 / pos_freqs\n    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n\n    truncate = config.rope_parameters.get(\"truncate\", True)\n    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings, truncate)\n\n    # Get n-dimensional rotational scaling corrected for extrapolation\n    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(device=device, dtype=torch.float)\n    inv_freq = (\n        inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)\n        + inv_freq_extrapolation * inv_freq_extrapolation_factor\n    )\n    return inv_freq, attention_factor"
                },
                "component_dependencies": {
                    "_compute_yarn_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#standardize_rope_params"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#find_tied_parameters": {
                "sorted_modules": {
                    "find_tied_parameters": "\n\ndef find_tied_parameters(model: \"nn.Module\", **kwargs):\n    \"\"\"\n    Find the tied parameters in a given model.\n\n    <Tip warning={true}>\n\n    The signature accepts keyword arguments, but they are for the recursive part of this function and you should ignore\n    them.\n\n    </Tip>\n\n    Args:\n        model (`torch.nn.Module`): The model to inspect.\n\n    Returns:\n        list[list[str]]: A list of lists of parameter names being all tied together.\n\n    Example:\n\n    ```py\n    >>> from collections import OrderedDict\n    >>> import torch.nn as nn\n\n    >>> model = nn.Sequential(OrderedDict([(\"linear1\", nn.Linear(4, 4)), (\"linear2\", nn.Linear(4, 4))]))\n    >>> model.linear2.weight = model.linear1.weight\n    >>> find_tied_parameters(model)\n    [['linear1.weight', 'linear2.weight']]\n    ```\n    \"\"\"\n\n    # get ALL model parameters and their names\n    all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n\n    # get ONLY unique named parameters,\n    # if parameter is tied and have multiple names, it will be included only once\n    no_duplicate_named_parameters = dict(model.named_parameters(remove_duplicate=True))\n\n    # the difference of the two sets will give us the tied parameters\n    tied_param_names = set(all_named_parameters.keys()) - set(no_duplicate_named_parameters.keys())\n\n    # 'tied_param_names' contains the names of parameters that are tied in the model, but we do not know\n    # which names refer to the same parameter. To identify this, we need to group them together.\n    tied_param_groups = {}\n    for tied_param_name in tied_param_names:\n        tied_param = all_named_parameters[tied_param_name]\n        for param_name, param in no_duplicate_named_parameters.items():\n            # compare if parameters are the same, if so, group their names together\n            if param is tied_param:\n                if param_name not in tied_param_groups:\n                    tied_param_groups[param_name] = []\n                tied_param_groups[param_name].append(tied_param_name)\n\n    return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/accelerate.py#get_balanced_memory": {
                "sorted_modules": {
                    "get_balanced_memory": "\n\ndef get_balanced_memory(\n    model: \"PreTrainedModel\",\n    max_memory: dict[int | str, int | str] | None = None,\n    no_split_module_classes: list[str] | None = None,\n    hf_quantizer: \"HfQuantizer | None\" = None,\n    low_zero: bool = False,\n):\n    \"\"\"\n    Compute a `max_memory` dictionary for [`infer_auto_device_map`] that will balance the use of each available GPU.\n\n    <Tip>\n\n    All computation is done analyzing sizes and dtypes of the model parameters. As a result, the model can be on the\n    meta device (as it would if initialized within the `init_empty_weights` context manager).\n\n    </Tip>\n\n    Args:\n        model (`PreTrainedModel`):\n            The model to analyze.\n        max_memory (`Dict`, *optional*):\n            A dictionary device identifier to maximum memory. Will default to the maximum memory available if unset.\n            Example: `max_memory={0: \"1GB\"}`.\n        no_split_module_classes (`List[str]`, *optional*):\n            A list of layer class names that should never be split across device (for instance any layer that has a\n            residual connection).\n        hf_quantizer (`HfQuantizer`, *optional*):\n            A quantizer for the model.\n        low_zero (`bool`, *optional*):\n            Minimizes the number of weights on GPU 0, which is convenient when it's used for other operations (like the\n            Transformers generate function).\n    \"\"\"\n    # Get default / clean up max_memory\n    user_not_set_max_memory = max_memory is None\n    max_memory = get_max_memory(max_memory)\n    # Check the number of accelerators available\n    accelerator_max_memory = copy.deepcopy(max_memory)\n    _, _ = accelerator_max_memory.pop(\"cpu\", None), accelerator_max_memory.pop(\"disk\", None)\n    num_devices = len([d for d in accelerator_max_memory if accelerator_max_memory[d] > 0])\n\n    if num_devices == 0:\n        return max_memory\n\n    if num_devices == 1:\n        # We cannot do low_zero on just one GPU, but we will still reserve some memory for the buffer\n        low_zero = False\n        # If user just asked us to handle memory usage, we should avoid OOM\n        if user_not_set_max_memory:\n            for key in max_memory.keys():\n                if isinstance(key, int):\n                    max_memory[key] *= 0.9  # 90% is a good compromise\n                    logger.info(\n                        f\"We will use 90% of the memory on device {key} for storing the model, and 10% for the buffer to avoid OOM. \"\n                        \"You can set `max_memory` in to a higher value to use more memory (at your own risk).\"\n                    )\n                    break  # only one device\n\n    module_sizes, leave_modules_sizes = compute_module_sizes(model, hf_quantizer)\n    per_gpu = module_sizes[\"\"] // (num_devices - 1 if low_zero else num_devices)\n\n    # We can't just set the memory to model_size // num_devices as it will end being too small: each GPU will get\n    # slightly less layers and some layers will end up offload at the end. So this function computes a buffer size to\n    # add which is the biggest of:\n    # - the size of no split block (if applicable)\n    # - the mean of the layer sizes\n    if no_split_module_classes is None:\n        no_split_module_classes = []\n    elif not isinstance(no_split_module_classes, (list, tuple)):\n        no_split_module_classes = [no_split_module_classes]\n\n    # Identify the size of the no_split_block modules\n    buffer = 0\n    if len(no_split_module_classes) > 0:\n        no_split_children = {}\n        for name, size in module_sizes.items():\n            if name == \"\":\n                continue\n            submodule = model.get_submodule(name)\n            class_name = submodule.__class__.__name__\n            if class_name in no_split_module_classes and class_name not in no_split_children:\n                no_split_children[class_name] = size\n\n            if set(no_split_children.keys()) == set(no_split_module_classes):\n                break\n        buffer = max(no_split_children.values()) if len(no_split_children) > 0 else 0\n\n    mean_leaves = int(sum(leave_modules_sizes.values()) / max(len(leave_modules_sizes), 1))\n    buffer = int(1.25 * max(buffer, mean_leaves))\n    per_gpu += buffer\n\n    # Sorted list of GPUs id (we may have some gpu ids not included in the our max_memory list - let's ignore them)\n    gpus_idx_list = sorted(\n        device_id for device_id, device_mem in max_memory.items() if isinstance(device_id, int) and device_mem > 0\n    )\n    # The last device is left with max_memory just in case the buffer is not enough.\n    for idx in gpus_idx_list[:-1]:\n        max_memory[idx] = min(max_memory[0] if low_zero and idx == 0 else per_gpu, max_memory[idx])\n\n    if low_zero:\n        min_zero = max(0, module_sizes[\"\"] - sum([max_memory[i] for i in range(1, num_devices)]))\n        max_memory[0] = min(min_zero, max_memory[0])\n\n    return max_memory"
                },
                "component_dependencies": {
                    "get_balanced_memory": [
                        "transformers/integrations/accelerate.py#compute_module_sizes",
                        "transformers/integrations/accelerate.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled": {
                "sorted_modules": {
                    "is_deepspeed_zero3_enabled": "\n\ndef is_deepspeed_zero3_enabled():\n    if _hf_deepspeed_config_weak_ref is not None and _hf_deepspeed_config_weak_ref() is not None:\n        return _hf_deepspeed_config_weak_ref().is_zero3()\n    else:\n        return False"
                },
                "component_dependencies": {
                    "is_deepspeed_zero3_enabled": [
                        "transformers/integrations/deepspeed.py#_hf_deepspeed_config_weak_ref"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/fsdp.py#is_fsdp_enabled": {
                "sorted_modules": {
                    "is_fsdp_enabled": "\n\ndef is_fsdp_enabled():\n    if is_torch_available():\n        import torch\n\n        return (\n            torch.distributed.is_available()\n            and torch.distributed.is_initialized()\n            and strtobool(os.environ.get(\"ACCELERATE_USE_FSDP\", \"False\")) == 1\n            and strtobool(os.environ.get(\"FSDP_CPU_RAM_EFFICIENT_LOADING\", \"False\")) == 1\n        )\n\n    return False"
                },
                "component_dependencies": {
                    "is_fsdp_enabled": [
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils.py#strtobool"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#init_on_device": {
                "sorted_modules": {
                    "init_on_device": "\n\n@contextmanager\ndef init_on_device(device: \"torch.device\", include_buffers: bool = False):\n    \"\"\"\n    A context manager under which models are initialized with all parameters on the specified device.\n\n    Args:\n        device (`torch.device`):\n            Device to initialize all parameters on.\n        include_buffers (`bool`, *optional*):\n            Whether or not to also put all buffers on the meta device while initializing.\n\n    Example:\n\n    ```python\n    import torch.nn as nn\n    from accelerate import init_on_device\n\n    with init_on_device(device=torch.device(\"cuda\")):\n        tst = nn.Linear(100, 100)  # on `cuda` device\n    ```\n    \"\"\"\n    if include_buffers:\n        with device:\n            yield\n        return\n\n    old_register_parameter = nn.Module.register_parameter\n    if include_buffers:\n        old_register_buffer = nn.Module.register_buffer\n\n    def register_empty_parameter(module, name, param):\n        old_register_parameter(module, name, param)\n        if param is not None:\n            param_cls = type(module._parameters[name])\n            kwargs = module._parameters[name].__dict__\n            kwargs[\"requires_grad\"] = param.requires_grad\n            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n\n    def register_empty_buffer(module, name, buffer, persistent=True):\n        old_register_buffer(module, name, buffer, persistent=persistent)\n        if buffer is not None:\n            module._buffers[name] = module._buffers[name].to(device)\n\n    # Patch tensor creation\n    if include_buffers:\n        tensor_constructors_to_patch = {\n            torch_function_name: getattr(torch, torch_function_name)\n            for torch_function_name in [\"empty\", \"zeros\", \"ones\", \"full\"]\n        }\n    else:\n        tensor_constructors_to_patch = {}\n\n    def patch_tensor_constructor(fn):\n        def wrapper(*args, **kwargs):\n            kwargs[\"device\"] = device\n            return fn(*args, **kwargs)\n\n        return wrapper\n\n    try:\n        nn.Module.register_parameter = register_empty_parameter\n        if include_buffers:\n            nn.Module.register_buffer = register_empty_buffer\n        for torch_function_name in tensor_constructors_to_patch:\n            setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n        yield\n    finally:\n        nn.Module.register_parameter = old_register_parameter\n        if include_buffers:\n            nn.Module.register_buffer = old_register_buffer\n        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items():\n            setattr(torch, torch_function_name, old_torch_function)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/flash_attention.py#flash_attention_forward": {
                "sorted_modules": {
                    "flash_attention_forward": "\n\ndef flash_attention_forward(\n    module: torch.nn.Module,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    dropout: float = 0.0,\n    scaling: Optional[float] = None,\n    sliding_window: Optional[int] = None,\n    softcap: Optional[float] = None,\n    is_causal: Optional[bool] = None,\n    **kwargs,\n) -> tuple[torch.Tensor, None]:\n    if kwargs.get(\"output_attentions\", False):\n        logger.warning_once(\n            \"`flash_attention_2` does not support `output_attentions=True`.\"\n            \" Please set your attention to `eager` if you want any of these features.\"\n        )\n\n    # This is before the transpose\n    seq_len = query.shape[2]\n\n    if any(dim == 0 for dim in query.shape):\n        raise ValueError(\n            \"Tensor query has shape  with a zero dimension.\\n\"\n            \"FlashAttention does not support inputs with dim=0.\\n\"\n            \"Please check your input shapes or use SDPA instead.\"\n        )\n    # FA2 uses non-transposed inputs\n    query = query.transpose(1, 2)\n    key = key.transpose(1, 2)\n    value = value.transpose(1, 2)\n\n    # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n    # therefore the input hidden states gets silently casted in float32. Hence, we need\n    # cast them back in the correct dtype just to be sure everything works as expected.\n    # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n    # in fp32. (usually our RMSNorm modules handle it correctly)\n    target_dtype = get_target_dtype(query, module)\n\n    # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n    is_causal = is_causal if is_causal is not None else module.is_causal\n\n    attn_output = _flash_attention_forward(\n        query,\n        key,\n        value,\n        attention_mask,\n        query_length=seq_len,\n        is_causal=is_causal,\n        dropout=dropout,\n        softmax_scale=scaling,\n        sliding_window=sliding_window,\n        softcap=softcap,\n        use_top_left_mask=_use_top_left_mask,\n        target_dtype=target_dtype,\n        attn_implementation=module.config._attn_implementation,\n        layer_idx=module.layer_idx if hasattr(module, \"layer_idx\") else None,\n        **kwargs,\n    )\n\n    return attn_output, None"
                },
                "component_dependencies": {
                    "flash_attention_forward": [
                        "transformers/integrations/flash_attention.py#_use_top_left_mask",
                        "transformers/integrations/flash_attention.py#get_target_dtype",
                        "transformers/integrations/flash_attention.py#logger",
                        "transformers/modeling_flash_attention_utils.py#_flash_attention_forward"
                    ]
                },
                "warning": null
            },
            "transformers/utils/hub.py#DownloadKwargs": {
                "sorted_modules": {
                    "DownloadKwargs": "\n\nclass DownloadKwargs(TypedDict, total=False):\n    cache_dir: str | os.PathLike | None\n    force_download: bool\n    proxies: dict[str, str] | None\n    local_files_only: bool\n    token: str | bool | None\n    revision: str | None\n    subfolder: str\n    commit_hash: str | None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#_torch_distributed_available": {
                "sorted_modules": {
                    "_torch_distributed_available": "\n# Cache this result has it's a C FFI call which can be pretty time-consuming\n_torch_distributed_available = torch.distributed.is_available()"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#add_tensor_parallel_hooks_to_module": {
                "sorted_modules": {
                    "add_tensor_parallel_hooks_to_module": "\n\ndef add_tensor_parallel_hooks_to_module(\n    model, module, tp_plan, layer_name, current_module_plan, device_mesh, parameter_name=None\n):\n    r\"\"\"\n    This function is called in `PretrainedModel.post_init()`. It is responsible of adding hooks\n    to the modules of the `model`, based on the `PretrainedModel._tp_plan`.\n\n    This is the place where we add the `pre_forward` and `post_forwards` hooks. These are defined\n    for each `TensorParallelLayer` as `_prepare_input_fn` and `_prepare_output_fn`.\n\n    \"\"\"\n    if current_module_plan is not None:\n        tp_layer = ALL_PARALLEL_STYLES[current_module_plan]\n        try:\n            tp_layer.prepare_module_tp(module, device_mesh)\n        except NotImplementedError as e:\n            print(\n                f\"Trying to prepare {layer_name}, but it's not supported. Corresponding module: {module} Fix it's TP plan: {e}\"\n            )\n\n        module._hf_tp_plan = current_module_plan\n        module.__repr__ = lambda: f\"{module.__repr__()}\\nTP Plan: {current_module_plan}\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/tensor_parallel.py#convert_local_tensor_to_dtensor": {
                "sorted_modules": {
                    "convert_local_tensor_to_dtensor": "\n\ndef convert_local_tensor_to_dtensor(\n    parameter: torch.Tensor, parameter_name: str, device_mesh, tp_plan: dict[str, str]\n) -> DTensor:\n    \"\"\"\n    Converts a local variant of weights to a DTensor with corresponding placements. Shouldn't be done ever except of before saving the model.\n    \"\"\"\n    _, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n    tp_style = _get_parameter_tp_plan(parameter_name, tp_plan)\n    if not tp_style:\n        return parameter\n\n    if tp_style not in [\"local_packed_rowwise\", \"local_rowwise\", \"local_colwise\"]:\n        return parameter\n    # TODO: this logic should be wrapped in a function, this is copied from corresponding tp classes.\n    if tp_style == \"local_packed_rowwise\":\n        placements = [Shard(-1)]\n    elif tp_style == \"local_rowwise\":\n        if param_type == \"bias\":\n            placements = [Replicate()]\n        else:\n            placements = [Shard(-1)]\n    elif tp_style == \"local_colwise\":\n        if param_type == \"bias\":\n            placements = [Shard(-1)]\n        else:\n            placements = [Shard(-2)]\n    return DTensor.from_local(parameter, device_mesh, placements, run_check=False)"
                },
                "component_dependencies": {
                    "convert_local_tensor_to_dtensor": [
                        "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#DFineForObjectDetectionLoss": {
                "sorted_modules": {
                    "DFineForObjectDetectionLoss": "\n\ndef DFineForObjectDetectionLoss(\n    logits,\n    labels,\n    device,\n    pred_boxes,\n    config,\n    outputs_class=None,\n    outputs_coord=None,\n    enc_topk_logits=None,\n    enc_topk_bboxes=None,\n    denoising_meta_values=None,\n    predicted_corners=None,\n    initial_reference_points=None,\n    **kwargs,\n):\n    criterion = DFineLoss(config)\n    criterion.to(device)\n    # Second: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes.clamp(min=0, max=1)\n    auxiliary_outputs = None\n    if config.auxiliary_loss:\n        if denoising_meta_values is not None:\n            dn_out_coord, outputs_coord = torch.split(\n                outputs_coord.clamp(min=0, max=1), denoising_meta_values[\"dn_num_split\"], dim=2\n            )\n            dn_out_class, outputs_class = torch.split(outputs_class, denoising_meta_values[\"dn_num_split\"], dim=2)\n            dn_out_corners, out_corners = torch.split(predicted_corners, denoising_meta_values[\"dn_num_split\"], dim=2)\n            dn_out_refs, out_refs = torch.split(initial_reference_points, denoising_meta_values[\"dn_num_split\"], dim=2)\n\n            auxiliary_outputs = _set_aux_loss2(\n                outputs_class[:, :-1].transpose(0, 1),\n                outputs_coord[:, :-1].transpose(0, 1),\n                out_corners[:, :-1].transpose(0, 1),\n                out_refs[:, :-1].transpose(0, 1),\n                out_corners[:, -1],\n                outputs_class[:, -1],\n            )\n\n            outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n            outputs_loss[\"auxiliary_outputs\"].extend(\n                _set_aux_loss([enc_topk_logits], [enc_topk_bboxes.clamp(min=0, max=1)])\n            )\n\n            dn_auxiliary_outputs = _set_aux_loss2(\n                dn_out_class.transpose(0, 1),\n                dn_out_coord.transpose(0, 1),\n                dn_out_corners.transpose(0, 1),\n                dn_out_refs.transpose(0, 1),\n                dn_out_corners[:, -1],\n                dn_out_class[:, -1],\n            )\n            outputs_loss[\"dn_auxiliary_outputs\"] = dn_auxiliary_outputs\n            outputs_loss[\"denoising_meta_values\"] = denoising_meta_values\n\n    loss_dict = criterion(outputs_loss, labels)\n\n    loss = sum(loss_dict.values())\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "DFineForObjectDetectionLoss": [
                        "transformers/loss/loss_d_fine.py#DFineLoss",
                        "transformers/loss/loss_d_fine.py#_set_aux_loss",
                        "transformers/loss/loss_d_fine.py#_set_aux_loss2"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_deformable_detr.py#DeformableDetrForObjectDetectionLoss": {
                "sorted_modules": {
                    "DeformableDetrForObjectDetectionLoss": "\n\ndef DeformableDetrForObjectDetectionLoss(\n    logits, labels, device, pred_boxes, config, outputs_class=None, outputs_coord=None, **kwargs\n):\n    # First: create the matcher\n    matcher = DeformableDetrHungarianMatcher(\n        class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost\n    )\n    # Second: create the criterion\n    losses = [\"labels\", \"boxes\", \"cardinality\"]\n    criterion = DeformableDetrImageLoss(\n        matcher=matcher,\n        num_classes=config.num_labels,\n        focal_alpha=config.focal_alpha,\n        losses=losses,\n    )\n    criterion.to(device)\n    # Third: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    auxiliary_outputs = None\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes\n    if config.auxiliary_loss:\n        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n\n    loss_dict = criterion(outputs_loss, labels)\n    # Fourth: compute total loss, as a weighted sum of the various losses\n    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n    if config.auxiliary_loss:\n        aux_weight_dict = {}\n        for i in range(config.decoder_layers - 1):\n            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n        weight_dict.update(aux_weight_dict)\n    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "DeformableDetrForObjectDetectionLoss": [
                        "transformers/loss/loss_deformable_detr.py#DeformableDetrHungarianMatcher",
                        "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                        "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_deformable_detr.py#DeformableDetrForSegmentationLoss": {
                "sorted_modules": {
                    "DeformableDetrForSegmentationLoss": "\n\ndef DeformableDetrForSegmentationLoss(\n    logits, labels, device, pred_boxes, pred_masks, config, outputs_class=None, outputs_coord=None, **kwargs\n):\n    # First: create the matcher\n    matcher = HungarianMatcher(class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost)\n    # Second: create the criterion\n    losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n    criterion = DeformableDetrImageLoss(\n        matcher=matcher,\n        num_classes=config.num_labels,\n        focal_alpha=config.focal_alpha,\n        losses=losses,\n    )\n    criterion.to(device)\n    # Third: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes\n    outputs_loss[\"pred_masks\"] = pred_masks\n\n    auxiliary_outputs = None\n    if config.auxiliary_loss:\n        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n\n    loss_dict = criterion(outputs_loss, labels)\n    # Fourth: compute total loss, as a weighted sum of the various losses\n    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n    weight_dict[\"loss_mask\"] = config.mask_loss_coefficient\n    weight_dict[\"loss_dice\"] = config.dice_loss_coefficient\n    if config.auxiliary_loss:\n        aux_weight_dict = {}\n        for i in range(config.decoder_layers - 1):\n            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n        weight_dict.update(aux_weight_dict)\n\n    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "DeformableDetrForSegmentationLoss": [
                        "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                        "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                        "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#ForObjectDetectionLoss": {
                "sorted_modules": {
                    "ForObjectDetectionLoss": "\n\ndef ForObjectDetectionLoss(\n    logits, labels, device, pred_boxes, config, outputs_class=None, outputs_coord=None, **kwargs\n):\n    # First: create the matcher\n    matcher = HungarianMatcher(class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost)\n    # Second: create the criterion\n    losses = [\"labels\", \"boxes\", \"cardinality\"]\n    criterion = ImageLoss(\n        matcher=matcher,\n        num_classes=config.num_labels,\n        eos_coef=config.eos_coefficient,\n        losses=losses,\n    )\n    criterion.to(device)\n    # Third: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    auxiliary_outputs = None\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes\n    if config.auxiliary_loss:\n        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n\n    loss_dict = criterion(outputs_loss, labels)\n    # Fourth: compute total loss, as a weighted sum of the various losses\n    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n    if config.auxiliary_loss:\n        aux_weight_dict = {}\n        for i in range(config.decoder_layers - 1):\n            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n        weight_dict.update(aux_weight_dict)\n    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "ForObjectDetectionLoss": [
                        "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                        "transformers/loss/loss_for_object_detection.py#ImageLoss",
                        "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#ForSegmentationLoss": {
                "sorted_modules": {
                    "ForSegmentationLoss": "\n\ndef ForSegmentationLoss(\n    logits, labels, device, pred_boxes, pred_masks, config, outputs_class=None, outputs_coord=None, **kwargs\n):\n    # First: create the matcher\n    matcher = HungarianMatcher(class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost)\n    # Second: create the criterion\n    losses = [\"labels\", \"boxes\", \"cardinality\", \"masks\"]\n    criterion = ImageLoss(\n        matcher=matcher,\n        num_classes=config.num_labels,\n        eos_coef=config.eos_coefficient,\n        losses=losses,\n    )\n    criterion.to(device)\n    # Third: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes\n    outputs_loss[\"pred_masks\"] = pred_masks\n\n    auxiliary_outputs = None\n    if config.auxiliary_loss:\n        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n\n    loss_dict = criterion(outputs_loss, labels)\n    # Fourth: compute total loss, as a weighted sum of the various losses\n    weight_dict = {\"loss_ce\": 1, \"loss_bbox\": config.bbox_loss_coefficient}\n    weight_dict[\"loss_giou\"] = config.giou_loss_coefficient\n    weight_dict[\"loss_mask\"] = config.mask_loss_coefficient\n    weight_dict[\"loss_dice\"] = config.dice_loss_coefficient\n    if config.auxiliary_loss:\n        aux_weight_dict = {}\n        for i in range(config.decoder_layers - 1):\n            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n        weight_dict.update(aux_weight_dict)\n    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "ForSegmentationLoss": [
                        "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                        "transformers/loss/loss_for_object_detection.py#ImageLoss",
                        "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_grounding_dino.py#GroundingDinoForObjectDetectionLoss": {
                "sorted_modules": {
                    "GroundingDinoForObjectDetectionLoss": "\n\ndef GroundingDinoForObjectDetectionLoss(\n    logits,\n    labels,\n    device,\n    pred_boxes,\n    config,\n    label_maps,\n    text_mask,\n    outputs_class=None,\n    outputs_coord=None,\n    encoder_logits=None,\n    encoder_pred_boxes=None,\n):\n    # First: create the matcher\n    matcher = GroundingDinoHungarianMatcher(\n        class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost\n    )\n    # Second: create the criterion\n    losses = [\"labels\", \"boxes\", \"cardinality\"]\n    criterion = GroundingDinoImageLoss(\n        matcher=matcher,\n        focal_alpha=config.focal_alpha,\n        losses=losses,\n    )\n    criterion.to(device)\n    # Third: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes\n    outputs_loss[\"label_maps\"] = label_maps\n    outputs_loss[\"text_mask\"] = text_mask\n\n    auxiliary_outputs = None\n    if config.auxiliary_loss:\n        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n        for aux_output in auxiliary_outputs:\n            aux_output[\"label_maps\"] = label_maps\n            aux_output[\"text_mask\"] = text_mask\n        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n\n    loss_dict = criterion(outputs_loss, labels)\n\n    if config.two_stage:\n        encoder_outputs_loss = {\n            \"logits\": encoder_logits,\n            \"pred_boxes\": encoder_pred_boxes,\n            \"label_maps\": label_maps,\n            \"text_mask\": text_mask,\n        }\n        encoder_loss_dict = criterion(encoder_outputs_loss, labels)\n        encoder_loss_dict = {k + \"_enc\": v for k, v in encoder_loss_dict.items()}\n        loss_dict.update(encoder_loss_dict)\n    # Fourth: compute total loss, as a weighted sum of the various losses\n    weight_dict = {\n        \"loss_ce\": 2.0,\n        \"loss_bbox\": config.bbox_loss_coefficient,\n        \"loss_giou\": config.giou_loss_coefficient,\n    }\n\n    if config.two_stage:\n        enc_weight_dict = {k + \"_enc\": v for k, v in weight_dict.items()}\n        weight_dict.update(enc_weight_dict)\n\n    if config.auxiliary_loss:\n        aux_weight_dict = {}\n        for i in range(config.decoder_layers - 1):\n            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n        weight_dict.update(aux_weight_dict)\n\n    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "GroundingDinoForObjectDetectionLoss": [
                        "transformers/loss/loss_for_object_detection.py#_set_aux_loss",
                        "transformers/loss/loss_grounding_dino.py#GroundingDinoHungarianMatcher",
                        "transformers/loss/loss_grounding_dino.py#GroundingDinoImageLoss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_rt_detr.py#RTDetrForObjectDetectionLoss": {
                "sorted_modules": {
                    "RTDetrForObjectDetectionLoss": "\n\ndef RTDetrForObjectDetectionLoss(\n    logits,\n    labels,\n    device,\n    pred_boxes,\n    config,\n    outputs_class=None,\n    outputs_coord=None,\n    enc_topk_logits=None,\n    enc_topk_bboxes=None,\n    denoising_meta_values=None,\n    **kwargs,\n):\n    criterion = RTDetrLoss(config)\n    criterion.to(device)\n    # Second: compute the losses, based on outputs and labels\n    outputs_loss = {}\n    outputs_loss[\"logits\"] = logits\n    outputs_loss[\"pred_boxes\"] = pred_boxes\n    if config.auxiliary_loss:\n        if denoising_meta_values is not None:\n            dn_out_coord, outputs_coord = torch.split(outputs_coord, denoising_meta_values[\"dn_num_split\"], dim=2)\n            dn_out_class, outputs_class = torch.split(outputs_class, denoising_meta_values[\"dn_num_split\"], dim=2)\n\n        auxiliary_outputs = _set_aux_loss(outputs_class[:, :-1].transpose(0, 1), outputs_coord[:, :-1].transpose(0, 1))\n        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n        outputs_loss[\"auxiliary_outputs\"].extend(_set_aux_loss([enc_topk_logits], [enc_topk_bboxes]))\n        if denoising_meta_values is not None:\n            outputs_loss[\"dn_auxiliary_outputs\"] = _set_aux_loss(\n                dn_out_class.transpose(0, 1), dn_out_coord.transpose(0, 1)\n            )\n            outputs_loss[\"denoising_meta_values\"] = denoising_meta_values\n\n    loss_dict = criterion(outputs_loss, labels)\n\n    loss = sum(loss_dict.values())\n    return loss, loss_dict, auxiliary_outputs"
                },
                "component_dependencies": {
                    "RTDetrForObjectDetectionLoss": [
                        "transformers/loss/loss_rt_detr.py#RTDetrLoss",
                        "transformers/loss/loss_rt_detr.py#_set_aux_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_utils.py#ForCausalLMLoss": {
                "sorted_modules": {
                    "ForCausalLMLoss": "\n\ndef ForCausalLMLoss(\n    logits,\n    labels,\n    vocab_size: int,\n    num_items_in_batch: Optional[torch.Tensor] = None,\n    ignore_index: int = -100,\n    shift_labels: Optional[torch.Tensor] = None,\n    **kwargs,\n) -> torch.Tensor:\n    # Upcast to float if we need to compute the loss to avoid potential precision issues\n    logits = logits.float()\n\n    if shift_labels is None:\n        # Shift so that tokens < n predict n\n        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n        shift_labels = labels[..., 1:].contiguous()\n\n    # Flatten the tokens\n    logits = logits.view(-1, vocab_size)\n    shift_labels = shift_labels.view(-1)\n    shift_labels = shift_labels.to(logits.device)\n    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n    return loss"
                },
                "component_dependencies": {
                    "ForCausalLMLoss": [
                        "transformers/loss/loss_utils.py#fixed_cross_entropy"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_utils.py#ForMaskedLMLoss": {
                "sorted_modules": {
                    "ForMaskedLMLoss": "\n\ndef ForMaskedLMLoss(\n    logits: torch.Tensor,\n    labels: torch.Tensor,\n    vocab_size: int,\n    num_items_in_batch: Optional[torch.Tensor] = None,\n    ignore_index: int = -100,\n    **kwargs,\n):\n    # Upcast to float if we need to compute the loss to avoid potential precision issues\n    logits = logits.float()\n\n    # Flatten the tokens\n    logits = logits.view(-1, vocab_size)\n    labels = labels.view(-1)\n\n    labels = labels.to(logits.device)\n    loss = fixed_cross_entropy(logits, labels, num_items_in_batch, ignore_index, **kwargs)\n    return loss"
                },
                "component_dependencies": {
                    "ForMaskedLMLoss": [
                        "transformers/loss/loss_utils.py#fixed_cross_entropy"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_utils.py#ForQuestionAnsweringLoss": {
                "sorted_modules": {
                    "ForQuestionAnsweringLoss": "\n\ndef ForQuestionAnsweringLoss(start_logits, end_logits, start_positions, end_positions, **kwargs):\n    total_loss = None\n    if start_positions is not None and end_positions is not None:\n        # If we are on multi-GPU, split add a dimension\n        if len(start_positions.size()) > 1:\n            start_positions = start_positions.squeeze(-1).to(start_logits.device)\n        if len(end_positions.size()) > 1:\n            end_positions = end_positions.squeeze(-1).to(end_logits.device)\n        # sometimes the start/end positions are outside our model inputs, we ignore these terms\n        ignored_index = start_logits.size(1)\n        start_positions = start_positions.clamp(0, ignored_index)\n        end_positions = end_positions.clamp(0, ignored_index)\n\n        start_loss = fixed_cross_entropy(start_logits, start_positions, ignore_index=ignored_index, **kwargs)\n        end_loss = fixed_cross_entropy(end_logits, end_positions, ignore_index=ignored_index, **kwargs)\n        total_loss = (start_loss + end_loss) / 2\n    return total_loss"
                },
                "component_dependencies": {
                    "ForQuestionAnsweringLoss": [
                        "transformers/loss/loss_utils.py#fixed_cross_entropy"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_utils.py#ForSequenceClassificationLoss": {
                "sorted_modules": {
                    "ForSequenceClassificationLoss": "\n\ndef ForSequenceClassificationLoss(labels: torch.Tensor, pooled_logits: torch.Tensor, config, **kwargs) -> torch.Tensor:\n    num_labels = config.num_labels\n    if config.problem_type is None:\n        if num_labels == 1:\n            config.problem_type = \"regression\"\n        elif num_labels > 1 and (labels.dtype in (torch.long, torch.int)):\n            config.problem_type = \"single_label_classification\"\n        else:\n            config.problem_type = \"multi_label_classification\"\n\n    labels = labels.to(pooled_logits.device)\n    if config.problem_type == \"regression\":\n        loss_fct = MSELoss()\n        if num_labels == 1:\n            return loss_fct(pooled_logits.squeeze(), labels.squeeze())\n        else:\n            return loss_fct(pooled_logits, labels)\n    if config.problem_type == \"single_label_classification\":\n        return fixed_cross_entropy(pooled_logits.view(-1, num_labels), labels.view(-1), **kwargs)\n\n    if config.problem_type == \"multi_label_classification\":\n        loss_fct = BCEWithLogitsLoss()\n        return loss_fct(pooled_logits, labels)\n\n    raise RuntimeError(f\"Invalid problem type: {config.problem_type}\")"
                },
                "component_dependencies": {
                    "ForSequenceClassificationLoss": [
                        "transformers/loss/loss_utils.py#fixed_cross_entropy"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_utils.py#ForTokenClassification": {
                "sorted_modules": {
                    "ForTokenClassification": "\n\ndef ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n    # Upcast to float if we need to compute the loss to avoid potential precision issues\n    logits = logits.view(-1, config.num_labels)\n    labels = labels.view(-1).to(logits.device)\n    logits = logits.float()\n    # Flatten the tokens\n    return fixed_cross_entropy(logits, labels, **kwargs)"
                },
                "component_dependencies": {
                    "ForTokenClassification": [
                        "transformers/loss/loss_utils.py#fixed_cross_entropy"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_flash_fn": {
                "sorted_modules": {
                    "_flash_fn": "\n\n# `globals()` is not compatible with dynamo, hence we have do define them in global scope ourselves\n_flash_fn = None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_flash_varlen_fn": {
                "sorted_modules": {
                    "_flash_varlen_fn": "_flash_varlen_fn = None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_lazy_define_process_function": {
                "sorted_modules": {
                    "_lazy_define_process_function": "\n\ndef _lazy_define_process_function(flash_function):\n    \"\"\"\n    Depending on the version and kernel some features are not supported. Due to limitations in\n    `torch.compile`, we opt to statically type which (optional) kwarg parameters are supported\n    within `_process_flash_attention_kwargs`.\n\n    NOTE: While all supported kwargs are marked as `True`, everything else is marked as `False`.\n          This might be confusing for kwargs that we use in any case, e.g. `is_causal`.\n    \"\"\"\n\n    flash_parameters = inspect.signature(flash_function).parameters\n    process_parameters = inspect.signature(_process_flash_attention_kwargs).parameters\n\n    supports_mapping = {}\n    for param in process_parameters:\n        fa_param = _hf_api_to_flash_mapping.get(param, param)\n        supports_mapping[fa_param] = fa_param in flash_parameters\n\n    return partial(_process_flash_attention_kwargs, supports_mapping=supports_mapping)"
                },
                "component_dependencies": {
                    "_lazy_define_process_function": [
                        "transformers/modeling_flash_attention_utils.py#_hf_api_to_flash_mapping",
                        "transformers/modeling_flash_attention_utils.py#_process_flash_attention_kwargs"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_lazy_imports": {
                "sorted_modules": {
                    "_lazy_imports": "\n\ndef _lazy_imports(implementation: Optional[str]):\n    \"\"\"\n    Lazy loads the respective flash attention implementations.\n\n    Return:\n        flash_attn_func: The base flash attention function.\n        flash_attn_varlen_func: The flash attention function supporting variable sequence lengths,\n                                e.g. for padding-free training.\n        pad_input: The function to pad inputs into one sequence and returning the respective kwargs.\n        unpad_input: The function to unpad outputs based on the kwargs (from pad_input).\n    \"\"\"\n    is_fa2 = is_flash_attn_2_available()\n    is_fa3 = is_flash_attn_3_available()\n\n    pad_input, unpad_input = _pad_input, _unpad_input\n\n    if (implementation == \"flash_attention_2\" and is_fa2) or (implementation is None and is_fa2 and not is_fa3):\n        from flash_attn import flash_attn_func, flash_attn_varlen_func\n        from flash_attn.bert_padding import pad_input, unpad_input\n    elif is_torch_npu_available():\n        # Package `flash-attn` is unavailable on Ascend NPU, which will cause ImportError\n        # Flash-Attention2 related apis for Ascend NPU must be imported from `.integrations.npu_flash_attention` module\n        from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_func\n        from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_varlen_func\n    else:\n        if implementation == \"flash_attention_3\" or (implementation is None and is_fa3):\n            from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n        # Kernels fallback\n        else:\n            flash_attn_func = getattr(implementation, \"flash_attn_func\", None)\n            flash_attn_varlen_func = getattr(implementation, \"flash_attn_varlen_func\", None)\n            if flash_attn_varlen_func is None or flash_attn_func is None:\n                raise ValueError(\n                    f\"Could not find the currently requested flash attention implementation at `{implementation}`.\"\n                    f\"Make sure that you request a valid kernel from the hub, e.g. `kernels-community/flash-attn`.\"\n                )\n\n    return flash_attn_func, flash_attn_varlen_func, pad_input, unpad_input"
                },
                "component_dependencies": {
                    "_lazy_imports": [
                        "transformers/modeling_flash_attention_utils.py#_pad_input",
                        "transformers/modeling_flash_attention_utils.py#_unpad_input",
                        "transformers/utils.py#is_flash_attn_2_available",
                        "transformers/utils.py#is_flash_attn_3_available",
                        "transformers/utils.py#is_torch_npu_available"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_pad_fn": {
                "sorted_modules": {
                    "_pad_fn": "_pad_fn = None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_process_flash_kwargs_fn": {
                "sorted_modules": {
                    "_process_flash_kwargs_fn": "\n# function that processes kwargs, generalized to handle any supported kwarg within the function\n_process_flash_kwargs_fn = None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_unpad_fn": {
                "sorted_modules": {
                    "_unpad_fn": "_unpad_fn = None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_utils.py#get_parameter_device": {
                "sorted_modules": {
                    "get_parameter_device": "\n\ndef get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n    try:\n        return next(parameter.parameters()).device\n    except StopIteration:\n        # For nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].device"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_end_ptr": {
                "sorted_modules": {
                    "_end_ptr": "\n\ndef _end_ptr(tensor: torch.Tensor) -> int:\n    # extract the end of the pointer if the tensor is a slice of a bigger tensor\n    if tensor.nelement():\n        stop = tensor.view(-1)[-1].data_ptr() + tensor.element_size()\n    else:\n        stop = tensor.data_ptr()\n    return stop"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#get_state_dict_dtype": {
                "sorted_modules": {
                    "get_state_dict_dtype": "\n\ndef get_state_dict_dtype(state_dict):\n    \"\"\"\n    Returns the first found floating dtype in `state_dict` if there is one, otherwise returns the first dtype.\n    \"\"\"\n    for t in state_dict.values():\n        if t.is_floating_point():\n            return t.dtype\n\n    # if no floating dtype was found return whatever the first dtype is\n    return next(state_dict.values()).dtype"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/safetensors_conversion.py#auto_conversion": {
                "sorted_modules": {
                    "auto_conversion": "\n\ndef auto_conversion(pretrained_model_name_or_path: str, ignore_errors_during_conversion=False, **cached_file_kwargs):\n    try:\n        api = HfApi(token=cached_file_kwargs.get(\"token\"), headers={\"user-agent\": http_user_agent()})\n        sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n\n        if sha is None:\n            return None, None\n        cached_file_kwargs[\"revision\"] = sha\n        del cached_file_kwargs[\"_commit_hash\"]\n\n        # This is an additional HEAD call that could be removed if we could infer sharded/non-sharded from the PR\n        # description.\n        sharded = api.file_exists(\n            pretrained_model_name_or_path,\n            \"model.safetensors.index.json\",\n            revision=sha,\n            token=cached_file_kwargs.get(\"token\"),\n        )\n        filename = \"model.safetensors.index.json\" if sharded else \"model.safetensors\"\n\n        resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n        return resolved_archive_file, sha, sharded\n    except Exception as e:\n        if not ignore_errors_during_conversion:\n            raise e"
                },
                "component_dependencies": {
                    "auto_conversion": [
                        "transformers/safetensors_conversion.py#get_conversion_pr_reference",
                        "transformers/utils.py#cached_file",
                        "transformers/utils.py#http_user_agent"
                    ]
                },
                "warning": null
            },
            "transformers/utils/hub.py#get_checkpoint_shard_files": {
                "sorted_modules": {
                    "get_checkpoint_shard_files": "\n\ndef get_checkpoint_shard_files(\n    pretrained_model_name_or_path,\n    index_filename,\n    cache_dir=None,\n    force_download=False,\n    proxies=None,\n    local_files_only=False,\n    token=None,\n    user_agent=None,\n    revision=None,\n    subfolder=\"\",\n    _commit_hash=None,\n    **deprecated_kwargs,\n):\n    \"\"\"\n    For a given model:\n\n    - download and cache all the shards of a sharded checkpoint if `pretrained_model_name_or_path` is a model ID on the\n      Hub\n    - returns the list of paths to all the shards, as well as some metadata.\n\n    For the description of each arg, see [`PreTrainedModel.from_pretrained`]. `index_filename` is the full path to the\n    index (downloaded and cached if `pretrained_model_name_or_path` is a model ID on the Hub).\n    \"\"\"\n    if not os.path.isfile(index_filename):\n        raise ValueError(f\"Can't find a checkpoint index ({index_filename}) in {pretrained_model_name_or_path}.\")\n\n    with open(index_filename) as f:\n        index = json.loads(f.read())\n\n    shard_filenames = sorted(set(index[\"weight_map\"].values()))\n    sharded_metadata = index[\"metadata\"]\n    sharded_metadata[\"all_checkpoint_keys\"] = list(index[\"weight_map\"].keys())\n    sharded_metadata[\"weight_map\"] = index[\"weight_map\"].copy()\n\n    # First, let's deal with local folder.\n    if os.path.isdir(pretrained_model_name_or_path):\n        shard_filenames = [os.path.join(pretrained_model_name_or_path, subfolder, f) for f in shard_filenames]\n        return shard_filenames, sharded_metadata\n\n    # At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\n    # or download the files\n    cached_filenames = cached_files(\n        pretrained_model_name_or_path,\n        shard_filenames,\n        cache_dir=cache_dir,\n        force_download=force_download,\n        proxies=proxies,\n        local_files_only=local_files_only,\n        token=token,\n        user_agent=user_agent,\n        revision=revision,\n        subfolder=subfolder,\n        _commit_hash=_commit_hash,\n    )\n\n    return cached_filenames, sharded_metadata"
                },
                "component_dependencies": {
                    "get_checkpoint_shard_files": [
                        "transformers/utils/hub.py#cached_files"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_utils.py#XLA_DOWNCAST_BF16": {
                "sorted_modules": {
                    "XLA_DOWNCAST_BF16": "XLA_DOWNCAST_BF16 = os.environ.get(\"XLA_DOWNCAST_BF16\", \"0\").upper()"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#XLA_USE_BF16": {
                "sorted_modules": {
                    "XLA_USE_BF16": "\nXLA_USE_BF16 = os.environ.get(\"XLA_USE_BF16\", \"0\").upper()"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#_load_state_dict_into_meta_model": {
                "sorted_modules": {
                    "_load_state_dict_into_meta_model": "\n\n@torch.no_grad()\ndef _load_state_dict_into_meta_model(\n    model: \"PreTrainedModel\",\n    state_dict: dict,\n    shard_file: str,\n    reverse_renaming_mapping: dict[str, str],\n    device_map: Optional[dict] = None,\n    disk_offload_folder: Optional[str] = None,\n    disk_offload_index: Optional[dict] = None,\n    hf_quantizer: Optional[HfQuantizer] = None,\n    device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n) -> tuple[Optional[dict], Optional[dict]]:\n    \"\"\"Load parameters from `meta_state_dict` into the model. The parameters of the `meta_state_dict` are on the meta\n    device in order to easily infer the shapes and dtypes that they will have. Then proper parameters are then loaded\n    from `shard_file`, which is the actual state dict file on disk.\n    This function takes care of correctly casting dtypes, devices, and sharding tensors in case of tensor parallelism.\n    \"\"\"\n    tensor_device = \"cpu\"\n    if device_map is not None and device_map.get(\"\", None) is not None:\n        if device_map[\"\"] not in (\"cpu\", torch.device(\"cpu\")):\n            tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n    if device_map is not None:\n        device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n\n    is_quantized = hf_quantizer is not None\n    is_safetensors = shard_file.endswith(\".safetensors\")\n    is_meta_state_dict = is_safetensors\n    file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device) if is_meta_state_dict else None\n    params_to_load = list(state_dict.keys())\n\n    for param_name in params_to_load:\n        empty_param = state_dict[param_name]\n        # we need to use serialized_param_name as file pointer is untouched\n        if is_meta_state_dict:\n            # This is the name of the parameter as it appears on disk file\n            serialized_param_name = reverse_renaming_mapping[param_name]\n            param = file_pointer.get_slice(serialized_param_name)\n        else:\n            param = empty_param.to(tensor_device)  # It is actually not empty!\n        to_contiguous, casting_dtype = _infer_parameter_dtype(model, param_name, empty_param, hf_quantizer)\n\n        if device_mesh is not None:\n            if not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n                # In this case, the param is already on the correct device!\n                shard_and_distribute_module(\n                    model,\n                    param,\n                    empty_param,\n                    param_name,\n                    casting_dtype,\n                    to_contiguous,\n                    device_mesh.get_local_rank(),\n                    device_mesh,\n                )\n            else:\n                # we have a device mesh but the param needs to be quantized, so we shard inside create_quantized_param\n                sharding_kwargs = {\n                    \"empty_param\": empty_param,\n                    \"casting_dtype\": casting_dtype,\n                    \"to_contiguous\": to_contiguous,\n                    \"rank\": device_mesh.get_local_rank(),\n                    \"device_mesh\": device_mesh,\n                }\n                hf_quantizer.create_quantized_param(\n                    model,\n                    param,\n                    param_name,\n                    device_mesh.get_local_rank(),\n                    **sharding_kwargs,\n                )\n        else:\n            param = param[...]\n            if casting_dtype is not None:\n                param = param.to(casting_dtype)\n            if to_contiguous:\n                param = param.contiguous()\n\n            if device_map is None:\n                param_device = \"cpu\"\n            else:\n                module_layer = re.search(device_map_regex, param_name)\n                if not module_layer:\n                    raise ValueError(f\"{param_name} doesn't have any device set.\")\n                else:\n                    param_device = device_map[module_layer.group()]\n\n            if param_device == \"disk\":\n                if not is_safetensors:\n                    disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n            elif not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n                if is_fsdp_enabled():\n                    param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n\n                _load_parameter_into_model(model, param_name, param.to(param_device))\n\n            else:\n                # TODO naming is stupid it loads it as well\n                hf_quantizer.create_quantized_param(model, param, param_name, param_device)\n\n                # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n                # and then cast it to CPU to avoid excessive memory usage on each GPU\n                # in comparison to the sharded model across GPUs.\n                if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n                    param_name = hf_quantizer.get_param_name(param_name)\n                    module, param_type = get_module_from_name(model, param_name)\n                    value = getattr(module, param_type)\n                    # We need to wait until the quantized value is created\n                    if value.device.type == \"meta\":\n                        continue\n                    val_kwargs = value.__dict__\n                    if not value.is_floating_point():\n                        val_kwargs[\"requires_grad\"] = False\n                    device = \"meta\" if is_fsdp_enabled() and not is_local_dist_rank_0() else \"cpu\"\n                    value = type(value)(value.data.to(device), **val_kwargs)\n                    setattr(module, param_type, value)\n\n        # Remove the param from the state dict if it was not loaded on the fly to avoid wasting memory\n        if not is_meta_state_dict:\n            del state_dict[param_name]\n\n    if file_pointer is not None:\n        file_pointer.__exit__(None, None, None)\n\n    return disk_offload_index"
                },
                "component_dependencies": {
                    "_load_state_dict_into_meta_model": [
                        "transformers/integrations.py#is_deepspeed_zero3_enabled",
                        "transformers/integrations.py#is_fsdp_enabled",
                        "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                        "transformers/modeling_utils.py#_infer_parameter_dtype",
                        "transformers/modeling_utils.py#_load_parameter_into_model",
                        "transformers/modeling_utils.py#is_local_dist_rank_0",
                        "transformers/quantizers.py#HfQuantizer",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name"
                    ]
                },
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#str_to_torch_dtype": {
                "sorted_modules": {
                    "str_to_torch_dtype": "\n\nstr_to_torch_dtype = {\n    \"BOOL\": torch.bool,\n    \"U8\": torch.uint8,\n    \"I8\": torch.int8,\n    \"I16\": torch.int16,\n    \"F16\": torch.float16,\n    \"BF16\": torch.bfloat16,\n    \"I32\": torch.int32,\n    \"F32\": torch.float32,\n    \"F64\": torch.float64,\n    \"I64\": torch.int64,\n    \"F8_E4M3\": torch.float8_e4m3fn,\n    \"F8_E5M2\": torch.float8_e5m2,\n}"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/modeling_utils.py#TORCH_INIT_FUNCTIONS": {
                "sorted_modules": {
                    "TORCH_INIT_FUNCTIONS": "\n\nTORCH_INIT_FUNCTIONS = {\n    \"uniform_\": nn.init.uniform_,\n    \"normal_\": nn.init.normal_,\n    \"trunc_normal_\": nn.init.trunc_normal_,\n    \"constant_\": nn.init.constant_,\n    \"xavier_uniform_\": nn.init.xavier_uniform_,\n    \"xavier_normal_\": nn.init.xavier_normal_,\n    \"kaiming_uniform_\": nn.init.kaiming_uniform_,\n    \"kaiming_normal_\": nn.init.kaiming_normal_,\n    \"uniform\": nn.init.uniform,\n    \"normal\": nn.init.normal,\n    \"xavier_uniform\": nn.init.xavier_uniform,\n    \"xavier_normal\": nn.init.xavier_normal,\n    \"kaiming_uniform\": nn.init.kaiming_uniform,\n    \"kaiming_normal\": nn.init.kaiming_normal,\n}"
                },
                "component_dependencies": {},
                "warning": "Warning: Circular dependencies were detected and grouped into single components ending in '_cycle'."
            },
            "transformers/pytorch_utils.py#_torch_distributed_available": {
                "sorted_modules": {
                    "_torch_distributed_available": "\n# Cache this result has it's a C FFI call which can be pretty time-consuming\n_torch_distributed_available = torch.distributed.is_available()"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/quantizers/auto.py#AutoHfQuantizer": {
                "sorted_modules": {
                    "AutoHfQuantizer": "\n\nclass AutoHfQuantizer:\n    \"\"\"\n     The Auto-HF quantizer class that takes care of automatically instantiating to the correct\n    `HfQuantizer` given the `QuantizationConfig`.\n    \"\"\"\n\n    @classmethod\n    def from_config(cls, quantization_config: Union[QuantizationConfigMixin, dict], **kwargs):\n        # Convert it to a QuantizationConfig if the q_config is a dict\n        if isinstance(quantization_config, dict):\n            quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n\n        quant_method = quantization_config.quant_method\n\n        # Again, we need a special care for bnb as we have a single quantization config\n        # class for both 4-bit and 8-bit quantization\n        if quant_method == QuantizationMethod.BITS_AND_BYTES:\n            if quantization_config.load_in_8bit:\n                quant_method += \"_8bit\"\n            else:\n                quant_method += \"_4bit\"\n\n        if quant_method not in AUTO_QUANTIZER_MAPPING:\n            raise ValueError(\n                f\"Unknown quantization type, got {quant_method} - supported types are:\"\n                f\" {list(AUTO_QUANTIZER_MAPPING.keys())}\"\n            )\n\n        target_cls = AUTO_QUANTIZER_MAPPING[quant_method]\n        return target_cls(quantization_config, **kwargs)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        quantization_config = AutoQuantizationConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        return cls.from_config(quantization_config)\n\n    @classmethod\n    def merge_quantization_configs(\n        cls,\n        quantization_config: Union[dict, QuantizationConfigMixin],\n        quantization_config_from_args: Optional[QuantizationConfigMixin],\n    ):\n        \"\"\"\n        handles situations where both quantization_config from args and quantization_config from model config are present.\n        \"\"\"\n        if quantization_config_from_args is not None:\n            warning_msg = (\n                \"You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading\"\n                \" already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\"\n            )\n        else:\n            warning_msg = \"\"\n\n        if isinstance(quantization_config, dict):\n            # Convert the config based on the type of quantization_config_from_args (e.g., AutoRoundConfig), which takes priority before automatic configuration dispatch.\n            if isinstance(quantization_config_from_args, AutoRoundConfig):\n                quantization_config = AutoRoundConfig.from_dict(quantization_config)\n            else:\n                quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n\n        if (\n            quantization_config_from_args is not None\n            and quantization_config.__class__.__name__ != quantization_config_from_args.__class__.__name__\n        ):\n            raise ValueError(\n                f\"The model is quantized with {quantization_config.__class__.__name__} but you are passing a {quantization_config_from_args.__class__.__name__} config. \"\n                \"Please make sure to pass the same quantization config class to `from_pretrained` with different loading attributes.\"\n            )\n\n        if (\n            isinstance(\n                quantization_config,\n                (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig, Mxfp4Config),\n            )\n            and quantization_config_from_args is not None\n        ):\n            loading_attr_dict = quantization_config_from_args.get_loading_attributes()\n            for attr, val in loading_attr_dict.items():\n                setattr(quantization_config, attr, val)\n\n            warning_msg += f\"However, loading attributes (e.g. {list(loading_attr_dict.keys())}) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\"\n\n        if warning_msg != \"\" and not isinstance(quantization_config, Mxfp4Config):\n            warnings.warn(warning_msg)\n        else:\n            # in the case of mxfp4, we don't want to print the warning message, bit confusing for users\n            logger.info(warning_msg)\n        return quantization_config\n\n    @staticmethod\n    def supports_quant_method(quantization_config_dict):\n        quant_method = quantization_config_dict.get(\"quant_method\", None)\n        if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n            suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n            quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n        elif quant_method is None:\n            raise ValueError(\n                \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n            )\n\n        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING:\n            logger.warning(\n                f\"Unknown quantization type, got {quant_method} - supported types are:\"\n                f\" {list(AUTO_QUANTIZER_MAPPING.keys())}. Hence, we will skip the quantization. \"\n                \"To remove the warning, you can delete the quantization_config attribute in config.json\"\n            )\n            return False\n        return True"
                },
                "component_dependencies": {
                    "AutoHfQuantizer": [
                        "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                        "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING",
                        "transformers/quantizers/auto.py#AutoQuantizationConfig",
                        "transformers/quantizers/auto.py#logger",
                        "transformers/utils/quantization_config.py#AutoRoundConfig",
                        "transformers/utils/quantization_config.py#AutoRoundConfig.from_dict",
                        "transformers/utils/quantization_config.py#AwqConfig",
                        "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                        "transformers/utils/quantization_config.py#FbgemmFp8Config",
                        "transformers/utils/quantization_config.py#GPTQConfig",
                        "transformers/utils/quantization_config.py#Mxfp4Config",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES"
                    ]
                },
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#rotate_half": {
                "sorted_modules": {
                    "rotate_half": "\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#repeat_kv": {
                "sorted_modules": {
                    "repeat_kv": "\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#BACKENDS_MAPPING": {
                "sorted_modules": {
                    "BACKENDS_MAPPING": "\n\nBACKENDS_MAPPING = OrderedDict(\n    [\n        (\"av\", (is_av_available, AV_IMPORT_ERROR)),\n        (\"bs4\", (is_bs4_available, BS4_IMPORT_ERROR)),\n        (\"cv2\", (is_cv2_available, CV2_IMPORT_ERROR)),\n        (\"datasets\", (is_datasets_available, DATASETS_IMPORT_ERROR)),\n        (\"decord\", (is_decord_available, DECORD_IMPORT_ERROR)),\n        (\"detectron2\", (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n        (\"essentia\", (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n        (\"faiss\", (is_faiss_available, FAISS_IMPORT_ERROR)),\n        (\"ftfy\", (is_ftfy_available, FTFY_IMPORT_ERROR)),\n        (\"g2p_en\", (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n        (\"pandas\", (is_pandas_available, PANDAS_IMPORT_ERROR)),\n        (\"phonemizer\", (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n        (\"uroman\", (is_uroman_available, UROMAN_IMPORT_ERROR)),\n        (\"pretty_midi\", (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n        (\"levenshtein\", (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n        (\"librosa\", (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n        (\"protobuf\", (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n        (\"pyctcdecode\", (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n        (\"pytesseract\", (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n        (\"sacremoses\", (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n        (\"pytorch_quantization\", (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n        (\"sentencepiece\", (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n        (\"sklearn\", (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n        (\"speech\", (is_speech_available, SPEECH_IMPORT_ERROR)),\n        (\"timm\", (is_timm_available, TIMM_IMPORT_ERROR)),\n        (\"torchaudio\", (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n        (\"natten\", (is_natten_available, NATTEN_IMPORT_ERROR)),\n        (\"nltk\", (is_nltk_available, NLTK_IMPORT_ERROR)),\n        (\"tokenizers\", (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n        (\"torch\", (is_torch_available, PYTORCH_IMPORT_ERROR)),\n        (\"torchvision\", (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n        (\"torchcodec\", (is_torchcodec_available, TORCHCODEC_IMPORT_ERROR)),\n        (\"vision\", (is_vision_available, VISION_IMPORT_ERROR)),\n        (\"scipy\", (is_scipy_available, SCIPY_IMPORT_ERROR)),\n        (\"accelerate\", (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n        (\"oneccl_bind_pt\", (is_ccl_available, CCL_IMPORT_ERROR)),\n        (\"cython\", (is_cython_available, CYTHON_IMPORT_ERROR)),\n        (\"rjieba\", (is_rjieba_available, RJIEBA_IMPORT_ERROR)),\n        (\"peft\", (is_peft_available, PEFT_IMPORT_ERROR)),\n        (\"jinja\", (is_jinja_available, JINJA_IMPORT_ERROR)),\n        (\"yt_dlp\", (is_yt_dlp_available, YT_DLP_IMPORT_ERROR)),\n        (\"rich\", (is_rich_available, RICH_IMPORT_ERROR)),\n        (\"pydantic\", (is_pydantic_available, PYDANTIC_IMPORT_ERROR)),\n        (\"fastapi\", (is_fastapi_available, FASTAPI_IMPORT_ERROR)),\n        (\"uvicorn\", (is_uvicorn_available, UVICORN_IMPORT_ERROR)),\n        (\"openai\", (is_openai_available, OPENAI_IMPORT_ERROR)),\n        (\"mistral-common\", (is_mistral_common_available, MISTRAL_COMMON_IMPORT_ERROR)),\n    ]\n)"
                },
                "component_dependencies": {
                    "BACKENDS_MAPPING": [
                        "transformers/utils/import_utils.py#ACCELERATE_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#AV_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#BS4_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#CCL_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#CV2_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#CYTHON_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#DATASETS_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#DECORD_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#DETECTRON2_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#ESSENTIA_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#FAISS_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#FASTAPI_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#FTFY_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#G2P_EN_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#JINJA_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#LEVENSHTEIN_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#LIBROSA_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#MISTRAL_COMMON_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#NATTEN_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#NLTK_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#OPENAI_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PANDAS_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PEFT_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PHONEMIZER_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PRETTY_MIDI_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PROTOBUF_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PYCTCDECODE_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PYDANTIC_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PYTESSERACT_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PYTORCH_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#PYTORCH_QUANTIZATION_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#RICH_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#RJIEBA_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#SACREMOSES_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#SCIPY_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#SENTENCEPIECE_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#SKLEARN_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#SPEECH_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#TIMM_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#TOKENIZERS_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#TORCHAUDIO_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#TORCHCODEC_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#TORCHVISION_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#UROMAN_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#UVICORN_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#VISION_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#YT_DLP_IMPORT_ERROR",
                        "transformers/utils/import_utils.py#is_accelerate_available",
                        "transformers/utils/import_utils.py#is_av_available",
                        "transformers/utils/import_utils.py#is_bs4_available",
                        "transformers/utils/import_utils.py#is_ccl_available",
                        "transformers/utils/import_utils.py#is_cv2_available",
                        "transformers/utils/import_utils.py#is_cython_available",
                        "transformers/utils/import_utils.py#is_datasets_available",
                        "transformers/utils/import_utils.py#is_decord_available",
                        "transformers/utils/import_utils.py#is_detectron2_available",
                        "transformers/utils/import_utils.py#is_essentia_available",
                        "transformers/utils/import_utils.py#is_faiss_available",
                        "transformers/utils/import_utils.py#is_fastapi_available",
                        "transformers/utils/import_utils.py#is_ftfy_available",
                        "transformers/utils/import_utils.py#is_g2p_en_available",
                        "transformers/utils/import_utils.py#is_jinja_available",
                        "transformers/utils/import_utils.py#is_levenshtein_available",
                        "transformers/utils/import_utils.py#is_librosa_available",
                        "transformers/utils/import_utils.py#is_mistral_common_available",
                        "transformers/utils/import_utils.py#is_natten_available",
                        "transformers/utils/import_utils.py#is_nltk_available",
                        "transformers/utils/import_utils.py#is_openai_available",
                        "transformers/utils/import_utils.py#is_pandas_available",
                        "transformers/utils/import_utils.py#is_peft_available",
                        "transformers/utils/import_utils.py#is_phonemizer_available",
                        "transformers/utils/import_utils.py#is_pretty_midi_available",
                        "transformers/utils/import_utils.py#is_protobuf_available",
                        "transformers/utils/import_utils.py#is_pyctcdecode_available",
                        "transformers/utils/import_utils.py#is_pydantic_available",
                        "transformers/utils/import_utils.py#is_pytesseract_available",
                        "transformers/utils/import_utils.py#is_pytorch_quantization_available",
                        "transformers/utils/import_utils.py#is_rich_available",
                        "transformers/utils/import_utils.py#is_rjieba_available",
                        "transformers/utils/import_utils.py#is_sacremoses_available",
                        "transformers/utils/import_utils.py#is_scipy_available",
                        "transformers/utils/import_utils.py#is_sentencepiece_available",
                        "transformers/utils/import_utils.py#is_sklearn_available",
                        "transformers/utils/import_utils.py#is_speech_available",
                        "transformers/utils/import_utils.py#is_timm_available",
                        "transformers/utils/import_utils.py#is_tokenizers_available",
                        "transformers/utils/import_utils.py#is_torch_available",
                        "transformers/utils/import_utils.py#is_torchaudio_available",
                        "transformers/utils/import_utils.py#is_torchcodec_available",
                        "transformers/utils/import_utils.py#is_torchvision_available",
                        "transformers/utils/import_utils.py#is_uroman_available",
                        "transformers/utils/import_utils.py#is_uvicorn_available",
                        "transformers/utils/import_utils.py#is_vision_available",
                        "transformers/utils/import_utils.py#is_yt_dlp_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#Backend": {
                "sorted_modules": {
                    "Backend": "\n\nclass Backend:\n    def __init__(self, backend_requirement: str):\n        self.package_name, self.version_comparison, self.version = split_package_version(backend_requirement)\n\n        if self.package_name not in BACKENDS_MAPPING:\n            raise ValueError(\n                f\"Backends should be defined in the BACKENDS_MAPPING. Offending backend: {self.package_name}\"\n            )\n\n    def get_installed_version(self) -> str:\n        \"\"\"Return the currently installed version of the backend\"\"\"\n        is_available, current_version = _is_package_available(self.package_name, return_version=True)\n        if not is_available:\n            raise RuntimeError(f\"Backend {self.package_name} is not available.\")\n        return current_version\n\n    def is_satisfied(self) -> bool:\n        return VersionComparison.from_string(self.version_comparison)(\n            version.parse(self.get_installed_version()), version.parse(self.version)\n        )\n\n    def __repr__(self) -> str:\n        return f'Backend(\"{self.package_name}\", {VersionComparison[self.version_comparison]}, \"{self.version}\")'\n\n    @property\n    def error_message(self):\n        return (\n            f\"{{0}} requires the {self.package_name} library version {self.version_comparison}{self.version}. That\"\n            f\" library was not found with this version in your environment.\"\n        )"
                },
                "component_dependencies": {
                    "Backend": [
                        "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                        "transformers/utils/import_utils.py#VersionComparison",
                        "transformers/utils/import_utils.py#_is_package_available",
                        "transformers/utils/import_utils.py#split_package_version"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#GGUF_SUPPORTED_ARCHITECTURES": {
                "sorted_modules": {
                    "GGUF_SUPPORTED_ARCHITECTURES": "\nGGUF_SUPPORTED_ARCHITECTURES = list(GGUF_TO_TRANSFORMERS_MAPPING[\"config\"].keys())"
                },
                "component_dependencies": {
                    "GGUF_SUPPORTED_ARCHITECTURES": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING": {
                "sorted_modules": {
                    "GGUF_TO_TRANSFORMERS_MAPPING": "\n\nGGUF_TO_TRANSFORMERS_MAPPING = {\n    \"ignore\": {\n        \"GGUF\": {\n            \"version\": \"version\",\n            \"tensor_count\": \"tensor_count\",\n            \"kv_count\": \"kv_count\",\n        },\n        \"general\": {\"file_type\": \"file_type\", \"quantization_version\": \"quantization_version\"},\n    },\n    \"config\": GGUF_CONFIG_MAPPING,\n    \"tokenizer\": {\"tokenizer\": GGUF_TOKENIZER_MAPPING[\"tokenizer\"]},\n    \"tokenizer_config\": {\"tokenizer\": GGUF_TOKENIZER_MAPPING[\"tokenizer_config\"]},\n}"
                },
                "component_dependencies": {
                    "GGUF_TO_TRANSFORMERS_MAPPING": [
                        "transformers/integrations.py#GGUF_CONFIG_MAPPING",
                        "transformers/integrations.py#GGUF_TOKENIZER_MAPPING"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#TENSOR_PROCESSORS": {
                "sorted_modules": {
                    "TENSOR_PROCESSORS": "\n\nTENSOR_PROCESSORS = {\n    \"llama\": LlamaTensorProcessor,\n    \"qwen2moe\": Qwen2MoeTensorProcessor,\n    \"qwen3moe\": Qwen2MoeTensorProcessor,\n    \"bloom\": BloomTensorProcessor,\n    \"t5\": T5TensorProcessor,\n    \"t5encoder\": T5TensorProcessor,\n    \"gpt2\": GPT2TensorProcessor,\n    \"mamba\": MambaTensorProcessor,\n    \"nemotron\": NemotronTensorProcessor,\n    \"gemma2\": Gemma2TensorProcessor,\n    \"gemma3\": Gemma2TensorProcessor,\n    \"lfm2\": Lfm2TensorProcessor,\n}"
                },
                "component_dependencies": {
                    "TENSOR_PROCESSORS": [
                        "transformers/modeling_gguf_pytorch_utils.py#BloomTensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#GPT2TensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#Gemma2TensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#Lfm2TensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#LlamaTensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#MambaTensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#NemotronTensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#Qwen2MoeTensorProcessor",
                        "transformers/modeling_gguf_pytorch_utils.py#T5TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor": {
                "sorted_modules": {
                    "TensorProcessor": "\n\nclass TensorProcessor:\n    def __init__(self, config=None):\n        self.config = config or {}\n\n    def process(self, weights, name, **kwargs):\n        return GGUFTensor(weights, name, {})"
                },
                "component_dependencies": {
                    "TensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#get_gguf_hf_weights_map": {
                "sorted_modules": {
                    "get_gguf_hf_weights_map": "\n\n# modified from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/model_executor/model_loader/loader.py#L1115-L1147\ndef get_gguf_hf_weights_map(\n    hf_model,\n    model_type: Optional[str] = None,\n    num_layers: Optional[int] = None,\n    qual_name: str = \"\",\n):\n    \"\"\"\n    GGUF uses this naming convention for their tensors from HF checkpoint:\n    `blk.N.BB.weight` and `blk.N.BB.bias`\n    where N signifies the block number of a layer, and BB signifies the\n    attention/mlp layer components.\n    See \"Standardized tensor names\" in\n    https://github.com/ggerganov/ggml/blob/master/docs/gguf.md for details.\n    \"\"\"\n    if is_gguf_available() and is_torch_available():\n        from gguf import MODEL_ARCH_NAMES, get_tensor_name_map\n    else:\n        logger.error(\n            \"Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see \"\n            \"https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions.\"\n        )\n        raise ImportError(\"Please install torch and gguf>=0.10.0 to load a GGUF checkpoint in PyTorch.\")\n\n    model_type = hf_model.config.model_type if model_type is None else model_type\n    num_layers = hf_model.config.num_hidden_layers if num_layers is None else num_layers\n    # hack: ggufs have a different name for cohere\n    if model_type == \"cohere\":\n        model_type = \"command-r\"\n    elif model_type == \"qwen2_moe\":\n        model_type = \"qwen2moe\"\n    elif model_type == \"qwen3_moe\":\n        model_type = \"qwen3moe\"\n    elif model_type == \"gemma3_text\":\n        model_type = \"gemma3\"\n    elif model_type == \"umt5\":\n        model_type = \"t5\"\n    arch = None\n    for key, value in MODEL_ARCH_NAMES.items():\n        if value == model_type:\n            arch = key\n            break\n    if arch is None:\n        raise NotImplementedError(\n            f\"Unknown gguf model_type: {model_type} in gguf-py. \"\n            \"This might because you're using an outdated version of gguf-py package, \"\n            \"you can install `gguf` package from source refer to \"\n            \"https://github.com/ggerganov/llama.cpp/tree/master/gguf-py#development\"\n        )\n    name_map = get_tensor_name_map(arch, num_layers)\n\n    # Use a dummy conversion to get the mapping, because\n    # hf => gguf and gguf => hf mappings are reversed\n    gguf_to_hf_name_map = {}\n    state_dict = hf_model.state_dict()\n    for hf_name in state_dict:\n        # An exception for qwen2moe/qwen3moe model, where the expert layers are packed\n        if model_type in (\"qwen2moe\", \"qwen3moe\") and \"mlp.experts.\" in hf_name:\n            hf_name = re.sub(r\"mlp.experts.\\d+.\", \"mlp.experts.\", hf_name)\n\n        name, suffix = hf_name, \"\"\n        if hf_name.endswith(\".weight\") or hf_name.endswith(\".bias\"):\n            name, suffix = hf_name.rsplit(\".\", 1)\n            suffix = \".\" + suffix\n\n        gguf_name = name_map.get_name(name)\n        if gguf_name is None:\n            continue\n\n        gguf_to_hf_name_map[gguf_name + suffix] = qual_name + hf_name\n\n    # Some model like Bloom converted from BloomModel instead of BloomForCausalLM\n    # Therefore, we need to check submodule as well to get a correct mapping\n    if named_children := hf_model.named_children():\n        for name, child in named_children:\n            sub_map = get_gguf_hf_weights_map(child, model_type, num_layers, qual_name=f\"{qual_name}{name}.\")\n            # Ignore the keys that are already in the main map to avoid overwriting\n            sub_map = {k: v for k, v in sub_map.items() if k not in gguf_to_hf_name_map}\n            gguf_to_hf_name_map.update(sub_map)\n\n    return gguf_to_hf_name_map"
                },
                "component_dependencies": {
                    "get_gguf_hf_weights_map": [
                        "transformers/modeling_gguf_pytorch_utils.py#logger",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils/import_utils.py#is_gguf_available"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#read_field": {
                "sorted_modules": {
                    "read_field": "\n\ndef read_field(reader, field):\n    if field not in reader.fields:\n        return []\n    value = reader.fields[field]\n    return [_gguf_parse_value(value.parts[_data_index], value.types) for _data_index in value.data]"
                },
                "component_dependencies": {
                    "read_field": [
                        "transformers/integrations.py#_gguf_parse_value"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_gguf_available": {
                "sorted_modules": {
                    "is_gguf_available": "\n\n@lru_cache\ndef is_gguf_available(min_version: str = GGUF_MIN_VERSION) -> bool:\n    is_available, gguf_version = _is_package_available(\"gguf\", return_version=True)\n    return is_available and version.parse(gguf_version) >= version.parse(min_version)"
                },
                "component_dependencies": {
                    "is_gguf_available": [
                        "transformers/utils/import_utils.py#GGUF_MIN_VERSION",
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_validate_default_rope_parameters": {
                "sorted_modules": {
                    "_validate_default_rope_parameters": "\n\ndef _validate_default_rope_parameters(\n    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n):\n    required_keys = {\"rope_type\", \"rope_theta\"}\n    received_keys = set(rope_parameters.keys())\n    rope_type = rope_parameters[\"rope_type\"]\n    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)"
                },
                "component_dependencies": {
                    "_validate_default_rope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#_check_received_keys"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters": {
                "sorted_modules": {
                    "_validate_dynamic_scaling_rope_parameters": "\n\ndef _validate_dynamic_scaling_rope_parameters(\n    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n):\n    # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n    optional_keys = {\"original_max_position_embeddings\"}\n    required_keys = {\"rope_type\", \"factor\"}\n    received_keys = set(rope_parameters.keys())\n    rope_type = rope_parameters[\"rope_type\"]\n    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n\n    factor = rope_parameters[\"factor\"]\n    if factor is None or not isinstance(factor, float) or factor < 1.0:\n        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")"
                },
                "component_dependencies": {
                    "_validate_dynamic_scaling_rope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#_check_received_keys",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters": {
                "sorted_modules": {
                    "_validate_linear_scaling_rope_parameters": "\n\ndef _validate_linear_scaling_rope_parameters(\n    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n):\n    required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n    received_keys = set(rope_parameters.keys())\n    rope_type = rope_parameters[\"rope_type\"]\n    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n\n    factor = rope_parameters[\"factor\"]\n    if factor is None or not isinstance(factor, float) or factor < 1.0:\n        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")"
                },
                "component_dependencies": {
                    "_validate_linear_scaling_rope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#_check_received_keys",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_validate_llama3_parameters": {
                "sorted_modules": {
                    "_validate_llama3_parameters": "\n\ndef _validate_llama3_parameters(rope_parameters: dict, config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n    required_keys = {\n        \"rope_type\",\n        \"factor\",\n        \"original_max_position_embeddings\",\n        \"low_freq_factor\",\n        \"high_freq_factor\",\n        \"rope_theta\",\n    }\n    rope_type = rope_parameters[\"rope_type\"]\n    received_keys = set(rope_parameters.keys())\n    _check_received_keys(rope_type, received_keys, required_keys, ignore_keys=ignore_keys)\n\n    factor = rope_parameters[\"factor\"]\n    if factor is None or not isinstance(factor, float) or factor < 1.0:\n        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n\n    low_freq_factor = rope_parameters[\"low_freq_factor\"]\n    high_freq_factor = rope_parameters[\"high_freq_factor\"]\n    if low_freq_factor is None or not isinstance(low_freq_factor, float):\n        logger.warning(f\"`rope_parameters`'s low_freq_factor field must be a float, got {low_freq_factor}\")\n    if high_freq_factor is None or not isinstance(high_freq_factor, float):\n        logger.warning(f\"`rope_parameters`'s high_freq_factor field must be a float, got {high_freq_factor}\")\n    if high_freq_factor <= low_freq_factor:\n        logger.warning(\n            \"`rope_parameters`'s high_freq_factor field must be greater than low_freq_factor, got high_freq_factor=\"\n            f\"{high_freq_factor} and low_freq_factor={low_freq_factor}\"\n        )\n\n    original_max_position_embeddings = rope_parameters[\"original_max_position_embeddings\"]\n    if original_max_position_embeddings is None or not isinstance(original_max_position_embeddings, int):\n        logger.warning(\n            \"`rope_parameters`'s original_max_position_embeddings field must be an integer, got \"\n            f\"{original_max_position_embeddings}\"\n        )\n    if original_max_position_embeddings >= config.max_position_embeddings:\n        logger.warning(\n            \"`rope_parameters`'s original_max_position_embeddings field must be less than max_position_embeddings, got \"\n            f\"{original_max_position_embeddings} and max_position_embeddings={config.max_position_embeddings}\"\n        )"
                },
                "component_dependencies": {
                    "_validate_llama3_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#_check_received_keys",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_validate_longrope_parameters": {
                "sorted_modules": {
                    "_validate_longrope_parameters": "\n\ndef _validate_longrope_parameters(rope_parameters: dict, config: PreTrainedConfig, ignore_keys: Optional[set] = None):\n    required_keys = {\"rope_type\", \"short_factor\", \"long_factor\", \"rope_theta\"}\n    # TODO (joao): update logic for the inclusion of `original_max_position_embeddings`\n    optional_keys = {\"attention_factor\", \"factor\", \"original_max_position_embeddings\"}\n    received_keys = set(rope_parameters.keys())\n    rope_type = rope_parameters[\"rope_type\"]\n    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n\n    partial_rotary_factor = getattr(config, \"partial_rotary_factor\", 1.0)\n    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n    dim = int(head_dim * partial_rotary_factor)\n\n    short_factor = rope_parameters.get(\"short_factor\")\n    if not isinstance(short_factor, list) and all(isinstance(x, (int, float)) for x in short_factor):\n        logger.warning(f\"`rope_parameters`'s short_factor field must be a list of numbers, got {short_factor}\")\n    if len(short_factor) != dim // 2:\n        logger.warning(f\"`rope_parameters`'s short_factor field must have length {dim // 2}, got {len(short_factor)}\")\n\n    long_factor = rope_parameters.get(\"long_factor\")\n    if not isinstance(long_factor, list) and all(isinstance(x, (int, float)) for x in long_factor):\n        logger.warning(f\"`rope_parameters`'s long_factor field must be a list of numbers, got {long_factor}\")\n    if len(long_factor) != dim // 2:\n        logger.warning(f\"`rope_parameters`'s long_factor field must have length {dim // 2}, got {len(long_factor)}\")\n\n    # Handle Phi3 divergence: prefer the use of `attention_factor` and/or `factor` over\n    # `original_max_position_embeddings` to compute internal variables. The latter lives outside `rope_parameters` and is\n    # unique to longrope (= undesirable)\n    if hasattr(config, \"original_max_position_embeddings\"):\n        logger.warning_once(\n            \"This model has set a `original_max_position_embeddings` field, to be used together with \"\n            \"`max_position_embeddings` to determine a scaling factor. Please set the `factor` field of `rope_parameters`\"\n            \"with this ratio instead -- we recommend the use of this field over `original_max_position_embeddings`, \"\n            \"as it is compatible with most model architectures.\"\n        )\n    else:\n        factor = rope_parameters.get(\"factor\")\n        if factor is None:\n            logger.warning(\"Missing required keys in `rope_parameters`: 'factor'\")\n        elif not isinstance(factor, float) or factor < 1.0:\n            logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n\n        attention_factor = rope_parameters.get(\"attention_factor\")\n        if attention_factor is not None:\n            if not isinstance(attention_factor, float) or attention_factor < 0.0:\n                logger.warning(\n                    f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n                )"
                },
                "component_dependencies": {
                    "_validate_longrope_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#_check_received_keys",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_validate_yarn_parameters": {
                "sorted_modules": {
                    "_validate_yarn_parameters": "\n\ndef _validate_yarn_parameters(\n    rope_parameters: dict, config: Optional[PreTrainedConfig] = None, ignore_keys: Optional[set] = None\n):\n    required_keys = {\"rope_type\", \"factor\", \"rope_theta\"}\n    optional_keys = {\n        \"attention_factor\",\n        \"beta_fast\",\n        \"beta_slow\",\n        \"original_max_position_embeddings\",\n        \"mscale\",\n        \"mscale_all_dim\",\n    }\n    received_keys = set(rope_parameters.keys())\n    rope_type = rope_parameters[\"rope_type\"]\n    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n\n    factor = rope_parameters[\"factor\"]\n    if factor is None or not isinstance(factor, float) or factor < 1.0:\n        logger.warning(f\"`rope_parameters`'s factor field must be a float >= 1, got {factor}\")\n\n    attention_factor = rope_parameters.get(\"attention_factor\")\n    if attention_factor is not None and (not isinstance(attention_factor, float) or attention_factor < 0):\n        logger.warning(\n            f\"`rope_parameters`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n        )\n    beta_fast = rope_parameters.get(\"beta_fast\")\n    if beta_fast is not None and not isinstance(beta_fast, float):\n        logger.warning(f\"`rope_parameters`'s beta_fast field must be a float, got {beta_fast}\")\n    beta_slow = rope_parameters.get(\"beta_slow\")\n    if beta_slow is not None and not isinstance(beta_slow, float):\n        logger.warning(f\"`rope_parameters`'s beta_slow field must be a float, got {beta_slow}\")\n\n    if (beta_fast or 32) < (beta_slow or 1):\n        logger.warning(\n            f\"`rope_parameters`'s beta_fast field must be greater than beta_slow, got beta_fast={beta_fast} \"\n            f\"(defaults to 32 if None) and beta_slow={beta_slow} (defaults to 1 if None)\"\n        )\n\n    # Models should set `config.rope_parameters[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n    # length, with `config.max_position_embeddings` corresponding to their post-yarn context length.\n    # However, for BC purposes, we allow the former to be unset.\n    original_max_position_embeddings = config.rope_parameters.get(\"original_max_position_embeddings\")\n    if original_max_position_embeddings is not None:\n        # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n        implicit_factor = config.max_position_embeddings / original_max_position_embeddings\n        if implicit_factor != factor:\n            logger.warning_once(\n                f\"The explicitly set RoPE scaling factor (config.rope_parameters['factor'] = {factor}) does not match \"\n                \"the ratio implicitly set by other parameters (implicit factor = \"\n                \"post-yarn context length / pre-yarn context length = \"\n                \"config.max_position_embeddings / config.rope_parameters['original_max_position_embeddings'] = \"\n                f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n                \"behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.\"\n            )\n    # No `config.rope_parameters[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n    # pre-yarn or the post-yarn context length?\n    # BC: we assume it is the pre-yarn context length.\n    else:\n        logger.warning_once(\n            \"config.rope_parameters['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n            \"**assume** config.max_position_embeddings holds the pre-yarn context length. Some use cases may expect \"\n            \"config.max_position_embeddings to hold the post-yarn context length (pre-yarn context length * \"\n            \"factor) -- we recommend updating both fields for optimal downstream model usage.\"\n        )"
                },
                "component_dependencies": {
                    "_validate_yarn_parameters": [
                        "transformers/configuration_utils.py#PreTrainedConfig",
                        "transformers/modeling_rope_utils.py#_check_received_keys",
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/accelerate.py#compute_module_sizes": {
                "sorted_modules": {
                    "compute_module_sizes": "\n\ndef compute_module_sizes(\n    model: \"PreTrainedModel\", hf_quantizer: \"HfQuantizer | None\"\n) -> tuple[dict[str, int], dict[str, int]]:\n    \"\"\"\n    Compute the size of each submodule of a given model (in bytes).\n    Returns a tuple of 2 dicts, the fist one containing a mapping of all the modules and the corresponding size\n    in bytes, and the 2nd one containing a mapping from all leaf modules (modules containing parameters, the end of\n    the model graph) and the corresponding sizes.\n    \"\"\"\n    all_module_sizes = defaultdict(int)\n    leaves_module_sizes = defaultdict(int)\n    for name, param in model.state_dict().items():\n        if hf_quantizer is not None:\n            dtype_size = hf_quantizer.param_element_size(model, name)\n        else:\n            dtype_size = param.element_size()\n        size = param.numel() * dtype_size\n        name_parts = name.split(\".\")\n        for idx in range(len(name_parts)):\n            all_module_sizes[\".\".join(name_parts[:idx])] += size\n        if \".\" in name:\n            leaves_module_sizes[name.rsplit(\".\", 1)[0]] += size\n\n    return all_module_sizes, leaves_module_sizes"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/integrations/flash_attention.py#_use_top_left_mask": {
                "sorted_modules": {
                    "_use_top_left_mask": "\n_use_top_left_mask = flash_attn_supports_top_left_mask()"
                },
                "component_dependencies": {
                    "_use_top_left_mask": [
                        "transformers/modeling_flash_attention_utils.py#flash_attn_supports_top_left_mask"
                    ]
                },
                "warning": null
            },
            "transformers/integrations/flash_attention.py#get_target_dtype": {
                "sorted_modules": {
                    "get_target_dtype": "\n\ndef get_target_dtype(query: torch.Tensor, module: torch.nn.Module) -> torch.dtype:\n    \"\"\"If the query is in float32, return a target dtype compatible with flash attention. Return None otherwise.\"\"\"\n    if query.dtype == torch.float32:\n        if torch.is_autocast_enabled():\n            return torch.get_autocast_gpu_dtype()\n        # Handle the case where the model is quantized\n        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n            return module.config._pre_quantization_dtype\n        else:\n            return next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n    return None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_flash_attention_forward": {
                "sorted_modules": {
                    "_flash_attention_forward": "\n\ndef _flash_attention_forward(\n    query_states: torch.Tensor,\n    key_states: torch.Tensor,\n    value_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    query_length: int,\n    is_causal: bool,\n    dropout: float = 0.0,\n    position_ids: Optional[torch.Tensor] = None,\n    softmax_scale: Optional[float] = None,\n    sliding_window: Optional[int] = None,\n    use_top_left_mask: bool = False,\n    softcap: Optional[float] = None,\n    deterministic: Optional[bool] = None,\n    cu_seq_lens_q: Optional[torch.LongTensor] = None,\n    cu_seq_lens_k: Optional[torch.LongTensor] = None,\n    max_length_q: Optional[int] = None,\n    max_length_k: Optional[int] = None,\n    target_dtype: Optional[torch.dtype] = None,\n    attn_implementation: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n    first unpad the input, then computes the attention scores and pad the final attention scores.\n\n    (Optional) kwargs are described further in `_process_flash_attention_kwargs` and `FlashAttentionKwargs`.\n\n    Args:\n        query_states (`torch.Tensor`):\n            Input query states to be passed to Flash Attention API\n        key_states (`torch.Tensor`):\n            Input key states to be passed to Flash Attention API\n        value_states (`torch.Tensor`):\n            Input value states to be passed to Flash Attention API\n        attention_mask (`torch.Tensor`, *optional*):\n            The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n            position of padding tokens and 1 for the position of non-padding tokens.\n        attn_implementation (`str`, *optional*):\n            The attention implementation to use. If None, will default to the one based on the environment.\n    \"\"\"\n    (flash_fn, flash_varlen_fn, pad_fn, unpad_fn), process_flash_kwargs_fn = lazy_import_flash_attention(\n        attn_implementation\n    )\n\n    # PEFT possibly silently casts tensors to fp32, this potentially reconverts to correct dtype or is a no op\n    query_states, key_states, value_states = fa_peft_integration_check(\n        query_states, key_states, value_states, target_dtype\n    )\n\n    # Extract the flash attention kwargs that have been requested (and are supported by the implementation)\n    flash_kwargs = process_flash_kwargs_fn(\n        query_length=query_length,\n        key_length=key_states.size(1),\n        is_causal=is_causal,\n        dropout=dropout,\n        softmax_scale=softmax_scale,\n        sliding_window=sliding_window,\n        use_top_left_mask=use_top_left_mask,\n        softcap=softcap,\n        deterministic=deterministic,\n        **kwargs,\n    )\n\n    # We will use `flash_varlen_fn` to prevent cross-example attention and also allow padding free approach under two cases:\n    # Case 1. If position ids is provided and the position ids indicate packed sequences, see `_is_packed_sequence`.\n    # Case 2. Some models pass directly pre-computed `cu_seqlens` so we don't need to infer it from position ids. It is safe to\n    # use `flash_varlen_fn` knowing we already have all necessary the kwargs.\n    #\n    # NOTE: it is user's responsibility to take care of flattening `position_ids` if that's needed by the model.\n    # See #39121 for more information.\n    is_fa_with_position_ids = _is_packed_sequence(position_ids, batch_size=query_states.size(0))\n    is_fa_with_varlen_kwargs = all(\n        kwarg is not None for kwarg in (cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k)\n    )\n\n    # Contains at least one padding token in the sequence\n    if attention_mask is not None:\n        q, k, v, indices_q, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _upad_input(\n            query_states, key_states, value_states, attention_mask, query_length, unpad_fn\n        )\n\n        # TODO for now this is required to work with\n        # https://huggingface.co/kernels-community/metal-flash-sdpa/blob/main/torch-ext/metal_flash_sdpa/__init__.py\n        if \"mps\" in str(q.device):\n            cu_seq_lens_k = cu_seq_lens_k.clone()\n\n        out_unpad = flash_varlen_fn(\n            q,\n            k,\n            v,\n            cu_seqlens_q=cu_seq_lens_q,\n            cu_seqlens_k=cu_seq_lens_k,\n            max_seqlen_q=max_length_q,\n            max_seqlen_k=max_length_k,\n            **flash_kwargs,\n        )\n        if isinstance(out_unpad, tuple):\n            out_unpad = out_unpad[0]\n\n        out = pad_fn(out_unpad, indices_q, query_states.size(0), query_length)\n\n    # Padding free, i.e. sequences flattened into one total sequence\n    elif is_fa_with_varlen_kwargs or is_fa_with_position_ids:\n        if cu_seq_lens_q is None or cu_seq_lens_k is None:\n            q, k, v, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _prepare_from_posids(\n                query_states, key_states, value_states, position_ids\n            )\n        else:\n            q = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))\n            k = key_states.reshape(-1, key_states.size(-2), key_states.size(-1))\n            v = value_states.reshape(-1, value_states.size(-2), value_states.size(-1))\n\n        # TODO for now this is required to work with\n        # https://huggingface.co/kernels-community/metal-flash-sdpa/blob/main/torch-ext/metal_flash_sdpa/__init__.py\n        if \"mps\" in str(q.device):\n            cu_seq_lens_k = cu_seq_lens_k.clone()\n\n        out = flash_varlen_fn(\n            q,\n            k,\n            v,\n            cu_seqlens_q=cu_seq_lens_q,\n            cu_seqlens_k=cu_seq_lens_k,\n            max_seqlen_q=max_length_q,\n            max_seqlen_k=max_length_k,\n            **flash_kwargs,\n        )\n        if isinstance(out, tuple):\n            out = out[0]\n\n        out = out.view(query_states.size(0), -1, out.size(-2), out.size(-1))\n\n    # No padding\n    else:\n        out = flash_fn(query_states, key_states, value_states, **flash_kwargs)\n        if isinstance(out, tuple):\n            out = out[0]\n\n    return out"
                },
                "component_dependencies": {
                    "_flash_attention_forward": [
                        "transformers/modeling_flash_attention_utils.py#_is_packed_sequence",
                        "transformers/modeling_flash_attention_utils.py#_prepare_from_posids",
                        "transformers/modeling_flash_attention_utils.py#_upad_input",
                        "transformers/modeling_flash_attention_utils.py#fa_peft_integration_check",
                        "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#DFineLoss": {
                "sorted_modules": {
                    "DFineLoss": "\n\nclass DFineLoss(RTDetrLoss):\n    \"\"\"\n    This class computes the losses for D-FINE. The process happens in two steps: 1) we compute hungarian assignment\n    between ground truth boxes and the outputs of the model 2) we supervise each pair of matched ground-truth /\n    prediction (supervise class and box).\n\n    Args:\n        matcher (`DetrHungarianMatcher`):\n            Module able to compute a matching between targets and proposals.\n        weight_dict (`Dict`):\n            Dictionary relating each loss with its weights. These losses are configured in DFineConf as\n            `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`, `weight_loss_fgl`, `weight_loss_ddf`\n        losses (`list[str]`):\n            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n        alpha (`float`):\n            Parameter alpha used to compute the focal loss.\n        gamma (`float`):\n            Parameter gamma used to compute the focal loss.\n        eos_coef (`float`):\n            Relative classification weight applied to the no-object category.\n        num_classes (`int`):\n            Number of object categories, omitting the special no-object category.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.matcher = RTDetrHungarianMatcher(config)\n        self.max_num_bins = config.max_num_bins\n        self.weight_dict = {\n            \"loss_vfl\": config.weight_loss_vfl,\n            \"loss_bbox\": config.weight_loss_bbox,\n            \"loss_giou\": config.weight_loss_giou,\n            \"loss_fgl\": config.weight_loss_fgl,\n            \"loss_ddf\": config.weight_loss_ddf,\n        }\n        self.losses = [\"vfl\", \"boxes\", \"local\"]\n        self.reg_scale = config.reg_scale\n        self.up = nn.Parameter(torch.tensor([config.up]), requires_grad=False)\n\n    def unimodal_distribution_focal_loss(\n        self, pred, label, weight_right, weight_left, weight=None, reduction=\"sum\", avg_factor=None\n    ):\n        dis_left = label.long()\n        dis_right = dis_left + 1\n\n        loss = F.cross_entropy(pred, dis_left, reduction=\"none\") * weight_left.reshape(-1) + F.cross_entropy(\n            pred, dis_right, reduction=\"none\"\n        ) * weight_right.reshape(-1)\n\n        if weight is not None:\n            weight = weight.float()\n            loss = loss * weight\n\n        if avg_factor is not None:\n            loss = loss.sum() / avg_factor\n        elif reduction == \"mean\":\n            loss = loss.mean()\n        elif reduction == \"sum\":\n            loss = loss.sum()\n\n        return loss\n\n    def loss_local(self, outputs, targets, indices, num_boxes, T=5):\n        \"\"\"Compute Fine-Grained Localization (FGL) Loss\n        and Decoupled Distillation Focal (DDF) Loss.\"\"\"\n\n        losses = {}\n        if \"pred_corners\" in outputs:\n            idx = self._get_source_permutation_idx(indices)\n            target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n\n            pred_corners = outputs[\"pred_corners\"][idx].reshape(-1, (self.max_num_bins + 1))\n            ref_points = outputs[\"ref_points\"][idx].detach()\n            with torch.no_grad():\n                self.fgl_targets = bbox2distance(\n                    ref_points,\n                    center_to_corners_format(target_boxes),\n                    self.max_num_bins,\n                    self.reg_scale,\n                    self.up,\n                )\n\n            target_corners, weight_right, weight_left = self.fgl_targets\n\n            ious = torch.diag(\n                box_iou(center_to_corners_format(outputs[\"pred_boxes\"][idx]), center_to_corners_format(target_boxes))[\n                    0\n                ]\n            )\n            weight_targets = ious.unsqueeze(-1).repeat(1, 1, 4).reshape(-1).detach()\n\n            losses[\"loss_fgl\"] = self.unimodal_distribution_focal_loss(\n                pred_corners,\n                target_corners,\n                weight_right,\n                weight_left,\n                weight_targets,\n                avg_factor=num_boxes,\n            )\n\n            pred_corners = outputs[\"pred_corners\"].reshape(-1, (self.max_num_bins + 1))\n            target_corners = outputs[\"teacher_corners\"].reshape(-1, (self.max_num_bins + 1))\n            if torch.equal(pred_corners, target_corners):\n                losses[\"loss_ddf\"] = pred_corners.sum() * 0\n            else:\n                weight_targets_local = outputs[\"teacher_logits\"].sigmoid().max(dim=-1)[0]\n                mask = torch.zeros_like(weight_targets_local, dtype=torch.bool)\n                mask[idx] = True\n                mask = mask.unsqueeze(-1).repeat(1, 1, 4).reshape(-1)\n\n                weight_targets_local[idx] = ious.reshape_as(weight_targets_local[idx]).to(weight_targets_local.dtype)\n                weight_targets_local = weight_targets_local.unsqueeze(-1).repeat(1, 1, 4).reshape(-1).detach()\n\n                loss_match_local = (\n                    weight_targets_local\n                    * (T**2)\n                    * (\n                        nn.KLDivLoss(reduction=\"none\")(\n                            F.log_softmax(pred_corners / T, dim=1),\n                            F.softmax(target_corners.detach() / T, dim=1),\n                        )\n                    ).sum(-1)\n                )\n\n                batch_scale = 1 / outputs[\"pred_boxes\"].shape[0]  # it should be refined\n                self.num_pos, self.num_neg = (\n                    (mask.sum() * batch_scale) ** 0.5,\n                    ((~mask).sum() * batch_scale) ** 0.5,\n                )\n                loss_match_local1 = loss_match_local[mask].mean() if mask.any() else 0\n                loss_match_local2 = loss_match_local[~mask].mean() if (~mask).any() else 0\n                losses[\"loss_ddf\"] = (loss_match_local1 * self.num_pos + loss_match_local2 * self.num_neg) / (\n                    self.num_pos + self.num_neg\n                )\n\n        return losses\n\n    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n        loss_map = {\n            \"cardinality\": self.loss_cardinality,\n            \"local\": self.loss_local,\n            \"boxes\": self.loss_boxes,\n            \"focal\": self.loss_labels_focal,\n            \"vfl\": self.loss_labels_vfl,\n        }\n        if loss not in loss_map:\n            raise ValueError(f\"Loss {loss} not supported\")\n        return loss_map[loss](outputs, targets, indices, num_boxes)"
                },
                "component_dependencies": {
                    "DFineLoss": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_d_fine.py#bbox2distance",
                        "transformers/loss/loss_for_object_detection.py#box_iou",
                        "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher",
                        "transformers/loss/loss_rt_detr.py#RTDetrLoss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#_set_aux_loss": {
                "sorted_modules": {
                    "_set_aux_loss": "\n\ndef _set_aux_loss(outputs_class, outputs_coord):\n    return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#_set_aux_loss2": {
                "sorted_modules": {
                    "_set_aux_loss2": "\n\ndef _set_aux_loss2(\n    outputs_class, outputs_coord, outputs_corners, outputs_ref, teacher_corners=None, teacher_logits=None\n):\n    return [\n        {\n            \"logits\": a,\n            \"pred_boxes\": b,\n            \"pred_corners\": c,\n            \"ref_points\": d,\n            \"teacher_corners\": teacher_corners,\n            \"teacher_logits\": teacher_logits,\n        }\n        for a, b, c, d in zip(outputs_class, outputs_coord, outputs_corners, outputs_ref)\n    ]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_deformable_detr.py#DeformableDetrHungarianMatcher": {
                "sorted_modules": {
                    "DeformableDetrHungarianMatcher": "\n\nclass DeformableDetrHungarianMatcher(HungarianMatcher):\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\"\n        Differences:\n        - out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid() instead of softmax\n        - class_cost uses alpha and gamma\n        \"\"\"\n        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n\n        # We flatten to compute the cost matrices in a batch\n        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, num_classes]\n        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n\n        # Also concat the target labels and boxes\n        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n\n        # Compute the classification cost.\n        alpha = 0.25\n        gamma = 2.0\n        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n        class_cost = pos_cost_class[:, target_ids] - neg_cost_class[:, target_ids]\n\n        # Compute the L1 cost between boxes\n        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n\n        # Compute the giou cost between boxes\n        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n\n        # Final cost matrix\n        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n\n        sizes = [len(v[\"boxes\"]) for v in targets]\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
                },
                "component_dependencies": {
                    "DeformableDetrHungarianMatcher": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                        "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss": {
                "sorted_modules": {
                    "DeformableDetrImageLoss": "\n\nclass DeformableDetrImageLoss(ImageLoss):\n    def __init__(self, matcher, num_classes, focal_alpha, losses):\n        nn.Module.__init__(self)\n        self.matcher = matcher\n        self.num_classes = num_classes\n        self.focal_alpha = focal_alpha\n        self.losses = losses\n\n    # removed logging parameter, which was part of the original implementation\n    def loss_labels(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n        of dim [nb_target_boxes]\n        \"\"\"\n        if \"logits\" not in outputs:\n            raise KeyError(\"No logits were found in the outputs\")\n        source_logits = outputs[\"logits\"]\n\n        idx = self._get_source_permutation_idx(indices)\n        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n        target_classes = torch.full(\n            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n        )\n        target_classes[idx] = target_classes_o\n\n        target_classes_onehot = torch.zeros(\n            [source_logits.shape[0], source_logits.shape[1], source_logits.shape[2] + 1],\n            dtype=source_logits.dtype,\n            layout=source_logits.layout,\n            device=source_logits.device,\n        )\n        target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n\n        target_classes_onehot = target_classes_onehot[:, :, :-1]\n        loss_ce = (\n            sigmoid_focal_loss(source_logits, target_classes_onehot, num_boxes, alpha=self.focal_alpha, gamma=2)\n            * source_logits.shape[1]\n        )\n        losses = {\"loss_ce\": loss_ce}\n\n        return losses"
                },
                "component_dependencies": {
                    "DeformableDetrImageLoss": [
                        "transformers/loss/loss_for_object_detection.py#ImageLoss",
                        "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#_set_aux_loss": {
                "sorted_modules": {
                    "_set_aux_loss": "\n\n# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\ndef _set_aux_loss(outputs_class, outputs_coord):\n    return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#HungarianMatcher": {
                "sorted_modules": {
                    "HungarianMatcher": "\n\n# taken from https://github.com/facebookresearch/detr/blob/master/models/matcher.py\nclass HungarianMatcher(nn.Module):\n    \"\"\"\n    This class computes an assignment between the targets and the predictions of the network.\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n    un-matched (and thus treated as non-objects).\n\n    Args:\n        class_cost:\n            The relative weight of the classification error in the matching cost.\n        bbox_cost:\n            The relative weight of the L1 error of the bounding box coordinates in the matching cost.\n        giou_cost:\n            The relative weight of the giou loss of the bounding box in the matching cost.\n    \"\"\"\n\n    def __init__(self, class_cost: float = 1, bbox_cost: float = 1, giou_cost: float = 1):\n        super().__init__()\n        requires_backends(self, [\"scipy\"])\n\n        self.class_cost = class_cost\n        self.bbox_cost = bbox_cost\n        self.giou_cost = giou_cost\n        if class_cost == 0 and bbox_cost == 0 and giou_cost == 0:\n            raise ValueError(\"All costs of the Matcher can't be 0\")\n\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\"\n        Args:\n            outputs (`dict`):\n                A dictionary that contains at least these entries:\n                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n            targets (`list[dict]`):\n                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n                  ground-truth\n                 objects in the target) containing the class labels\n                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n\n        Returns:\n            `list[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n            - index_i is the indices of the selected predictions (in order)\n            - index_j is the indices of the corresponding selected targets (in order)\n            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n        \"\"\"\n        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n\n        # We flatten to compute the cost matrices in a batch\n        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n\n        # Also concat the target labels and boxes\n        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n\n        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n        # but approximate it in 1 - proba[target class].\n        # The 1 is a constant that doesn't change the matching, it can be omitted.\n        class_cost = -out_prob[:, target_ids]\n\n        # Compute the L1 cost between boxes\n        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n\n        # Compute the giou cost between boxes\n        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n\n        # Final cost matrix\n        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n\n        sizes = [len(v[\"boxes\"]) for v in targets]\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
                },
                "component_dependencies": {
                    "HungarianMatcher": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                        "transformers/utils.py#requires_backends"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#ImageLoss": {
                "sorted_modules": {
                    "ImageLoss": "\n\n# taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\nclass ImageLoss(nn.Module):\n    \"\"\"\n    This class computes the losses for DetrForObjectDetection/DetrForSegmentation. The process happens in two steps: 1)\n    we compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair\n    of matched ground-truth / prediction (supervise class and box).\n\n    A note on the `num_classes` argument (copied from original repo in detr.py): \"the naming of the `num_classes`\n    parameter of the criterion is somewhat misleading. It indeed corresponds to `max_obj_id` + 1, where `max_obj_id` is\n    the maximum id for a class in your dataset. For example, COCO has a `max_obj_id` of 90, so we pass `num_classes` to\n    be 91. As another example, for a dataset that has a single class with `id` 1, you should pass `num_classes` to be 2\n    (`max_obj_id` + 1). For more details on this, check the following discussion\n    https://github.com/facebookresearch/detr/issues/108#issuecomment-650269223\"\n\n\n    Args:\n        matcher (`DetrHungarianMatcher`):\n            Module able to compute a matching between targets and proposals.\n        num_classes (`int`):\n            Number of object categories, omitting the special no-object category.\n        eos_coef (`float`):\n            Relative classification weight applied to the no-object category.\n        losses (`list[str]`):\n            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n    \"\"\"\n\n    def __init__(self, matcher, num_classes, eos_coef, losses):\n        super().__init__()\n        self.matcher = matcher\n        self.num_classes = num_classes\n        self.eos_coef = eos_coef\n        self.losses = losses\n        empty_weight = torch.ones(self.num_classes + 1)\n        empty_weight[-1] = self.eos_coef\n        self.register_buffer(\"empty_weight\", empty_weight)\n\n    # removed logging parameter, which was part of the original implementation\n    def loss_labels(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Classification loss (NLL) targets dicts must contain the key \"class_labels\" containing a tensor of dim\n        [nb_target_boxes]\n        \"\"\"\n        if \"logits\" not in outputs:\n            raise KeyError(\"No logits were found in the outputs\")\n        source_logits = outputs[\"logits\"]\n\n        idx = self._get_source_permutation_idx(indices)\n        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)])\n        target_classes = torch.full(\n            source_logits.shape[:2], self.num_classes, dtype=torch.int64, device=source_logits.device\n        )\n        target_classes[idx] = target_classes_o\n\n        loss_ce = nn.functional.cross_entropy(source_logits.transpose(1, 2), target_classes, self.empty_weight)\n        losses = {\"loss_ce\": loss_ce}\n\n        return losses\n\n    @torch.no_grad()\n    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes.\n\n        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n        \"\"\"\n        logits = outputs[\"logits\"]\n        device = logits.device\n        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n        losses = {\"cardinality_error\": card_err}\n        return losses\n\n    def loss_boxes(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss.\n\n        Targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes\n        are expected in format (center_x, center_y, w, h), normalized by the image size.\n        \"\"\"\n        if \"pred_boxes\" not in outputs:\n            raise KeyError(\"No predicted boxes found in outputs\")\n        idx = self._get_source_permutation_idx(indices)\n        source_boxes = outputs[\"pred_boxes\"][idx]\n        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n\n        loss_bbox = nn.functional.l1_loss(source_boxes, target_boxes, reduction=\"none\")\n\n        losses = {}\n        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n\n        loss_giou = 1 - torch.diag(\n            generalized_box_iou(center_to_corners_format(source_boxes), center_to_corners_format(target_boxes))\n        )\n        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n        return losses\n\n    def loss_masks(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Compute the losses related to the masks: the focal loss and the dice loss.\n\n        Targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n        \"\"\"\n        if \"pred_masks\" not in outputs:\n            raise KeyError(\"No predicted masks found in outputs\")\n\n        source_idx = self._get_source_permutation_idx(indices)\n        target_idx = self._get_target_permutation_idx(indices)\n        source_masks = outputs[\"pred_masks\"]\n        source_masks = source_masks[source_idx]\n        masks = [t[\"masks\"] for t in targets]\n        # TODO use valid to mask invalid areas due to padding in loss\n        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n        target_masks = target_masks.to(source_masks)\n        target_masks = target_masks[target_idx]\n\n        # upsample predictions to the target size\n        source_masks = nn.functional.interpolate(\n            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n        )\n        source_masks = source_masks[:, 0].flatten(1)\n\n        target_masks = target_masks.flatten(1)\n        target_masks = target_masks.view(source_masks.shape)\n        losses = {\n            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n        }\n        return losses\n\n    def _get_source_permutation_idx(self, indices):\n        # permute predictions following indices\n        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n        source_idx = torch.cat([source for (source, _) in indices])\n        return batch_idx, source_idx\n\n    def _get_target_permutation_idx(self, indices):\n        # permute targets following indices\n        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n        target_idx = torch.cat([target for (_, target) in indices])\n        return batch_idx, target_idx\n\n    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n        loss_map = {\n            \"labels\": self.loss_labels,\n            \"cardinality\": self.loss_cardinality,\n            \"boxes\": self.loss_boxes,\n            \"masks\": self.loss_masks,\n        }\n        if loss not in loss_map:\n            raise ValueError(f\"Loss {loss} not supported\")\n        return loss_map[loss](outputs, targets, indices, num_boxes)\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        This performs the loss computation.\n\n        Args:\n             outputs (`dict`, *optional*):\n                Dictionary of tensors, see the output specification of the model for the format.\n             targets (`list[dict]`, *optional*):\n                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n                losses applied, see each loss' doc.\n        \"\"\"\n        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outputs\"}\n\n        # Retrieve the matching between the outputs of the last layer and the targets\n        indices = self.matcher(outputs_without_aux, targets)\n\n        # Compute the average number of target boxes across all nodes, for normalization purposes\n        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n        world_size = 1\n        if is_accelerate_available():\n            if PartialState._shared_state != {}:\n                num_boxes = reduce(num_boxes)\n                world_size = PartialState().num_processes\n        num_boxes = torch.clamp(num_boxes / world_size, min=1).item()\n\n        # Compute all the requested losses\n        losses = {}\n        for loss in self.losses:\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n\n        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n        if \"auxiliary_outputs\" in outputs:\n            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n                indices = self.matcher(auxiliary_outputs, targets)\n                for loss in self.losses:\n                    if loss == \"masks\":\n                        # Intermediate masks losses are too costly to compute, we ignore them.\n                        continue\n                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n\n        return losses"
                },
                "component_dependencies": {
                    "ImageLoss": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_for_object_detection.py#dice_loss",
                        "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                        "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                        "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss",
                        "transformers/utils.py#is_accelerate_available"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_grounding_dino.py#GroundingDinoHungarianMatcher": {
                "sorted_modules": {
                    "GroundingDinoHungarianMatcher": "\n\nclass GroundingDinoHungarianMatcher(HungarianMatcher):\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\"\n        Args:\n            outputs (`dict`):\n                A dictionary that contains at least these entries:\n                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n                * \"label_maps\": Tuple of tensors of dim [num_classes, hidden_dim].\n            targets (`list[dict]`):\n                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n                  ground-truth\n                 objects in the target) containing the class labels\n                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n\n        Returns:\n            `list[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n            - index_i is the indices of the selected predictions (in order)\n            - index_j is the indices of the corresponding selected targets (in order)\n            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n        \"\"\"\n        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n\n        # We flatten to compute the cost matrices in a batch\n        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, hidden_dim]\n        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n        label_maps = outputs[\"label_maps\"]\n\n        # First take the label map for each class in each batch and then concatenate them\n        label_maps = torch.cat([label_map[target[\"class_labels\"]] for label_map, target in zip(label_maps, targets)])\n        # Normalize label maps based on number of tokens per class\n        label_maps = label_maps / label_maps.sum(dim=-1, keepdim=True)\n\n        # Also concat the target labels and boxes\n        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n\n        # Compute the classification cost.\n        alpha = 0.25\n        gamma = 2.0\n        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n        # Compute the classification cost by taking pos and neg cost in the appropriate index\n        class_cost = (pos_cost_class - neg_cost_class) @ label_maps.t()\n\n        # Compute the L1 cost between boxes\n        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n\n        # Compute the giou cost between boxes\n        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n\n        # Final cost matrix\n        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n\n        sizes = [len(v[\"boxes\"]) for v in targets]\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
                },
                "component_dependencies": {
                    "GroundingDinoHungarianMatcher": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                        "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_grounding_dino.py#GroundingDinoImageLoss": {
                "sorted_modules": {
                    "GroundingDinoImageLoss": "\n\nclass GroundingDinoImageLoss(ImageLoss):\n    \"\"\"\n    This class computes the losses for `GroundingDinoForObjectDetection`. The process happens in two steps: 1) we\n    compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair of\n    matched ground-truth / prediction (supervise class and box).\n\n    Args:\n        matcher (`GroundingDinoHungarianMatcher`):\n            Module able to compute a matching between targets and proposals.\n        focal_alpha (`float`):\n            Alpha parameter in focal loss.\n        losses (`list[str]`):\n            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n    \"\"\"\n\n    def __init__(self, matcher, focal_alpha, losses):\n        nn.Module.__init__(self)\n        self.matcher = matcher\n        self.focal_alpha = focal_alpha\n        self.losses = losses\n\n    def _get_target_classes_one_hot(self, outputs, targets, indices):\n        \"\"\"\n        Create one_hot based on the matching indices\n        \"\"\"\n        logits = outputs[\"logits\"]\n        # Add offsets to class_labels to select the correct label map\n        class_labels = torch.cat(\n            [\n                target[\"class_labels\"][J] + len(outputs[\"label_maps\"][i]) if i > 0 else target[\"class_labels\"][J]\n                for i, (target, (_, J)) in enumerate(zip(targets, indices))\n            ]\n        )\n        label_maps = torch.cat(outputs[\"label_maps\"], dim=0)\n\n        idx = self._get_source_permutation_idx(indices)\n        target_classes_onehot = torch.zeros_like(logits, device=logits.device, dtype=torch.long)\n        target_classes_onehot[idx] = label_maps[class_labels].to(torch.long)\n\n        return target_classes_onehot\n\n    def loss_labels(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n        of dim [nb_target_boxes]\n        \"\"\"\n        if \"logits\" not in outputs:\n            raise KeyError(\"No logits were found in the outputs\")\n        if \"text_mask\" not in outputs:\n            raise KeyError(\"No text_mask were found in the outputs\")\n\n        target_classes_onehot = self._get_target_classes_one_hot(outputs, targets, indices)\n        source_logits = outputs[\"logits\"]\n        text_mask = outputs[\"text_mask\"]\n\n        # Select only valid logits\n        source_logits = torch.masked_select(source_logits, text_mask)\n        target_classes_onehot = torch.masked_select(target_classes_onehot, text_mask)\n\n        target_classes_onehot = target_classes_onehot.float()\n        loss_ce = sigmoid_focal_loss(\n            inputs=source_logits,\n            targets=target_classes_onehot,\n            num_boxes=num_boxes,\n            alpha=self.focal_alpha,\n            gamma=2,\n        )\n\n        losses = {\"loss_ce\": loss_ce}\n\n        return losses"
                },
                "component_dependencies": {
                    "GroundingDinoImageLoss": [
                        "transformers/loss/loss_for_object_detection.py#ImageLoss",
                        "transformers/loss/loss_grounding_dino.py#sigmoid_focal_loss"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_rt_detr.py#RTDetrLoss": {
                "sorted_modules": {
                    "RTDetrLoss": "\n\nclass RTDetrLoss(nn.Module):\n    \"\"\"\n    This class computes the losses for RTDetr. The process happens in two steps: 1) we compute hungarian assignment\n    between ground truth boxes and the outputs of the model 2) we supervise each pair of matched ground-truth /\n    prediction (supervise class and box).\n\n    Args:\n        matcher (`DetrHungarianMatcher`):\n            Module able to compute a matching between targets and proposals.\n        weight_dict (`Dict`):\n            Dictionary relating each loss with its weights. These losses are configured in RTDetrConf as\n            `weight_loss_vfl`, `weight_loss_bbox`, `weight_loss_giou`\n        losses (`list[str]`):\n            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n        alpha (`float`):\n            Parameter alpha used to compute the focal loss.\n        gamma (`float`):\n            Parameter gamma used to compute the focal loss.\n        eos_coef (`float`):\n            Relative classification weight applied to the no-object category.\n        num_classes (`int`):\n            Number of object categories, omitting the special no-object category.\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n\n        self.matcher = RTDetrHungarianMatcher(config)\n        self.num_classes = config.num_labels\n        self.weight_dict = {\n            \"loss_vfl\": config.weight_loss_vfl,\n            \"loss_bbox\": config.weight_loss_bbox,\n            \"loss_giou\": config.weight_loss_giou,\n        }\n        self.losses = [\"vfl\", \"boxes\"]\n        self.eos_coef = config.eos_coefficient\n        empty_weight = torch.ones(config.num_labels + 1)\n        empty_weight[-1] = self.eos_coef\n        self.register_buffer(\"empty_weight\", empty_weight)\n        self.alpha = config.focal_loss_alpha\n        self.gamma = config.focal_loss_gamma\n\n    def loss_labels_vfl(self, outputs, targets, indices, num_boxes, log=True):\n        if \"pred_boxes\" not in outputs:\n            raise KeyError(\"No predicted boxes found in outputs\")\n        if \"logits\" not in outputs:\n            raise KeyError(\"No predicted logits found in outputs\")\n        idx = self._get_source_permutation_idx(indices)\n\n        src_boxes = outputs[\"pred_boxes\"][idx]\n        target_boxes = torch.cat([_target[\"boxes\"][i] for _target, (_, i) in zip(targets, indices)], dim=0)\n        ious, _ = box_iou(center_to_corners_format(src_boxes.detach()), center_to_corners_format(target_boxes))\n        ious = torch.diag(ious)\n\n        src_logits = outputs[\"logits\"]\n        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n        target_classes = torch.full(\n            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n        )\n        target_classes[idx] = target_classes_original\n        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n\n        target_score_original = torch.zeros_like(target_classes, dtype=src_logits.dtype)\n        target_score_original[idx] = ious.to(target_score_original.dtype)\n        target_score = target_score_original.unsqueeze(-1) * target\n\n        pred_score = F.sigmoid(src_logits.detach())\n        weight = self.alpha * pred_score.pow(self.gamma) * (1 - target) + target_score\n\n        loss = F.binary_cross_entropy_with_logits(src_logits, target_score, weight=weight, reduction=\"none\")\n        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n        return {\"loss_vfl\": loss}\n\n    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n        \"\"\"Classification loss (NLL)\n        targets dicts must contain the key \"class_labels\" containing a tensor of dim [nb_target_boxes]\n        \"\"\"\n        if \"logits\" not in outputs:\n            raise KeyError(\"No logits were found in the outputs\")\n\n        src_logits = outputs[\"logits\"]\n\n        idx = self._get_source_permutation_idx(indices)\n        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n        target_classes = torch.full(\n            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n        )\n        target_classes[idx] = target_classes_original\n\n        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.class_weight)\n        losses = {\"loss_ce\": loss_ce}\n        return losses\n\n    @torch.no_grad()\n    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Compute the cardinality error, i.e. the absolute error in the number of predicted non-empty boxes. This is not\n        really a loss, it is intended for logging purposes only. It doesn't propagate gradients.\n        \"\"\"\n        logits = outputs[\"logits\"]\n        device = logits.device\n        target_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n        card_pred = (logits.argmax(-1) != logits.shape[-1] - 1).sum(1)\n        card_err = nn.functional.l1_loss(card_pred.float(), target_lengths.float())\n        losses = {\"cardinality_error\": card_err}\n        return losses\n\n    def loss_boxes(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss. Targets dicts must\n        contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]. The target boxes are expected in\n        format (center_x, center_y, w, h), normalized by the image size.\n        \"\"\"\n        if \"pred_boxes\" not in outputs:\n            raise KeyError(\"No predicted boxes found in outputs\")\n        idx = self._get_source_permutation_idx(indices)\n        src_boxes = outputs[\"pred_boxes\"][idx]\n        target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(targets, indices)], dim=0)\n\n        losses = {}\n\n        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=\"none\")\n        losses[\"loss_bbox\"] = loss_bbox.sum() / num_boxes\n\n        loss_giou = 1 - torch.diag(\n            generalized_box_iou(center_to_corners_format(src_boxes), center_to_corners_format(target_boxes))\n        )\n        losses[\"loss_giou\"] = loss_giou.sum() / num_boxes\n        return losses\n\n    def loss_masks(self, outputs, targets, indices, num_boxes):\n        \"\"\"\n        Compute the losses related to the masks: the focal loss and the dice loss. Targets dicts must contain the key\n        \"masks\" containing a tensor of dim [nb_target_boxes, h, w].\n        \"\"\"\n        if \"pred_masks\" not in outputs:\n            raise KeyError(\"No predicted masks found in outputs\")\n\n        source_idx = self._get_source_permutation_idx(indices)\n        target_idx = self._get_target_permutation_idx(indices)\n        source_masks = outputs[\"pred_masks\"]\n        source_masks = source_masks[source_idx]\n        masks = [t[\"masks\"] for t in targets]\n        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n        target_masks = target_masks.to(source_masks)\n        target_masks = target_masks[target_idx]\n\n        # upsample predictions to the target size\n        source_masks = nn.functional.interpolate(\n            source_masks[:, None], size=target_masks.shape[-2:], mode=\"bilinear\", align_corners=False\n        )\n        source_masks = source_masks[:, 0].flatten(1)\n\n        target_masks = target_masks.flatten(1)\n        target_masks = target_masks.view(source_masks.shape)\n        losses = {\n            \"loss_mask\": sigmoid_focal_loss(source_masks, target_masks, num_boxes),\n            \"loss_dice\": dice_loss(source_masks, target_masks, num_boxes),\n        }\n        return losses\n\n    def loss_labels_bce(self, outputs, targets, indices, num_boxes, log=True):\n        src_logits = outputs[\"logits\"]\n        idx = self._get_source_permutation_idx(indices)\n        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n        target_classes = torch.full(\n            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n        )\n        target_classes[idx] = target_classes_original\n\n        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n        loss = F.binary_cross_entropy_with_logits(src_logits, target * 1.0, reduction=\"none\")\n        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n        return {\"loss_bce\": loss}\n\n    def _get_source_permutation_idx(self, indices):\n        # permute predictions following indices\n        batch_idx = torch.cat([torch.full_like(source, i) for i, (source, _) in enumerate(indices)])\n        source_idx = torch.cat([source for (source, _) in indices])\n        return batch_idx, source_idx\n\n    def _get_target_permutation_idx(self, indices):\n        # permute targets following indices\n        batch_idx = torch.cat([torch.full_like(target, i) for i, (_, target) in enumerate(indices)])\n        target_idx = torch.cat([target for (_, target) in indices])\n        return batch_idx, target_idx\n\n    def loss_labels_focal(self, outputs, targets, indices, num_boxes, log=True):\n        if \"logits\" not in outputs:\n            raise KeyError(\"No logits found in outputs\")\n\n        src_logits = outputs[\"logits\"]\n\n        idx = self._get_source_permutation_idx(indices)\n        target_classes_original = torch.cat([_target[\"class_labels\"][i] for _target, (_, i) in zip(targets, indices)])\n        target_classes = torch.full(\n            src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device\n        )\n        target_classes[idx] = target_classes_original\n\n        target = F.one_hot(target_classes, num_classes=self.num_classes + 1)[..., :-1]\n        loss = sigmoid_focal_loss(src_logits, target, self.alpha, self.gamma)\n        loss = loss.mean(1).sum() * src_logits.shape[1] / num_boxes\n        return {\"loss_focal\": loss}\n\n    def get_loss(self, loss, outputs, targets, indices, num_boxes):\n        loss_map = {\n            \"labels\": self.loss_labels,\n            \"cardinality\": self.loss_cardinality,\n            \"boxes\": self.loss_boxes,\n            \"masks\": self.loss_masks,\n            \"bce\": self.loss_labels_bce,\n            \"focal\": self.loss_labels_focal,\n            \"vfl\": self.loss_labels_vfl,\n        }\n        if loss not in loss_map:\n            raise ValueError(f\"Loss {loss} not supported\")\n        return loss_map[loss](outputs, targets, indices, num_boxes)\n\n    @staticmethod\n    def get_cdn_matched_indices(dn_meta, targets):\n        dn_positive_idx, dn_num_group = dn_meta[\"dn_positive_idx\"], dn_meta[\"dn_num_group\"]\n        num_gts = [len(t[\"class_labels\"]) for t in targets]\n        device = targets[0][\"class_labels\"].device\n\n        dn_match_indices = []\n        for i, num_gt in enumerate(num_gts):\n            if num_gt > 0:\n                gt_idx = torch.arange(num_gt, dtype=torch.int64, device=device)\n                gt_idx = gt_idx.tile(dn_num_group)\n                assert len(dn_positive_idx[i]) == len(gt_idx)\n                dn_match_indices.append((dn_positive_idx[i], gt_idx))\n            else:\n                dn_match_indices.append(\n                    (\n                        torch.zeros(0, dtype=torch.int64, device=device),\n                        torch.zeros(0, dtype=torch.int64, device=device),\n                    )\n                )\n\n        return dn_match_indices\n\n    def forward(self, outputs, targets):\n        \"\"\"\n        This performs the loss computation.\n\n        Args:\n             outputs (`dict`, *optional*):\n                Dictionary of tensors, see the output specification of the model for the format.\n             targets (`list[dict]`, *optional*):\n                List of dicts, such that `len(targets) == batch_size`. The expected keys in each dict depends on the\n                losses applied, see each loss' doc.\n        \"\"\"\n        outputs_without_aux = {k: v for k, v in outputs.items() if \"auxiliary_outputs\" not in k}\n\n        # Retrieve the matching between the outputs of the last layer and the targets\n        indices = self.matcher(outputs_without_aux, targets)\n\n        # Compute the average number of target boxes across all nodes, for normalization purposes\n        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n        num_boxes = torch.clamp(num_boxes, min=1).item()\n\n        # Compute all the requested losses\n        losses = {}\n        for loss in self.losses:\n            l_dict = self.get_loss(loss, outputs, targets, indices, num_boxes)\n            l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n            losses.update(l_dict)\n\n        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n        if \"auxiliary_outputs\" in outputs:\n            for i, auxiliary_outputs in enumerate(outputs[\"auxiliary_outputs\"]):\n                indices = self.matcher(auxiliary_outputs, targets)\n                for loss in self.losses:\n                    if loss == \"masks\":\n                        # Intermediate masks losses are too costly to compute, we ignore them.\n                        continue\n                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes)\n                    l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n                    l_dict = {k + f\"_aux_{i}\": v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n\n        # In case of cdn auxiliary losses. For rtdetr\n        if \"dn_auxiliary_outputs\" in outputs:\n            if \"denoising_meta_values\" not in outputs:\n                raise ValueError(\n                    \"The output must have the 'denoising_meta_values` key. Please, ensure that 'outputs' includes a 'denoising_meta_values' entry.\"\n                )\n            indices = self.get_cdn_matched_indices(outputs[\"denoising_meta_values\"], targets)\n            num_boxes = num_boxes * outputs[\"denoising_meta_values\"][\"dn_num_group\"]\n\n            for i, auxiliary_outputs in enumerate(outputs[\"dn_auxiliary_outputs\"]):\n                # indices = self.matcher(auxiliary_outputs, targets)\n                for loss in self.losses:\n                    if loss == \"masks\":\n                        # Intermediate masks losses are too costly to compute, we ignore them.\n                        continue\n                    kwargs = {}\n                    l_dict = self.get_loss(loss, auxiliary_outputs, targets, indices, num_boxes, **kwargs)\n                    l_dict = {k: l_dict[k] * self.weight_dict[k] for k in l_dict if k in self.weight_dict}\n                    l_dict = {k + f\"_dn_{i}\": v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n\n        return losses"
                },
                "component_dependencies": {
                    "RTDetrLoss": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_for_object_detection.py#box_iou",
                        "transformers/loss/loss_for_object_detection.py#dice_loss",
                        "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                        "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                        "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss",
                        "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_rt_detr.py#_set_aux_loss": {
                "sorted_modules": {
                    "_set_aux_loss": "\n\n# different for RT-DETR: not slicing the last element like in DETR one\ndef _set_aux_loss(outputs_class, outputs_coord):\n    return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_utils.py#fixed_cross_entropy": {
                "sorted_modules": {
                    "fixed_cross_entropy": "\n\ndef fixed_cross_entropy(\n    source: torch.Tensor,\n    target: torch.Tensor,\n    num_items_in_batch: Optional[torch.Tensor] = None,\n    ignore_index: int = -100,\n    **kwargs,\n) -> torch.Tensor:\n    reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n    if reduction == \"sum\":\n        # just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\n        if torch.is_tensor(num_items_in_batch):\n            num_items_in_batch = num_items_in_batch.to(loss.device)\n        loss = loss / num_items_in_batch\n    return loss"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_hf_api_to_flash_mapping": {
                "sorted_modules": {
                    "_hf_api_to_flash_mapping": "# exceptions where hf API doesn't match the original flash attention API\n_hf_api_to_flash_mapping = {\n    \"dropout\": \"dropout_p\",\n    \"sliding_window\": \"window_size\",\n}"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_process_flash_attention_kwargs": {
                "sorted_modules": {
                    "_process_flash_attention_kwargs": "\n\ndef _process_flash_attention_kwargs(\n    query_length: int,\n    key_length: int,\n    is_causal: bool,\n    dropout: float = 0.0,\n    softmax_scale: Optional[float] = None,\n    sliding_window: Optional[int] = None,\n    use_top_left_mask: bool = False,\n    softcap: Optional[float] = None,\n    deterministic: Optional[bool] = None,\n    s_aux: Optional[torch.Tensor] = None,\n    supports_mapping: Optional[dict[str, bool]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Returns a set of kwargs that are passed down to the according flash attention function based on\n    requested features and whether it is supported - depends on the version and kernel implementation\n    which is dynamically configured at `lazy_import_flash_attention`. The (un)supported features can be\n    inspected in `supports_mapping`, see `_lazy_define_process_function` for more details.\n\n    Args:\n        query_length (`int`):\n            Length of the query states\n        key_length (`int`):\n            Length of the key states\n        is_causal (`bool`):\n            Whether we perform causal (decoder) attention or full attention.\n        dropout (`float`):\n            Attention dropout.\n        softmax_scale (`float`, *optional*):\n            The scaling of QK^T before applying softmax. Default to `1 / sqrt(head_dim)`.\n        sliding_window (`int`, *optional*):\n            The size of the sliding window, i.e. we look at a max of `sliding_window` tokens back.\n        use_top_left_mask (`bool`):\n            Deprecated behavior of older versions of flash attention requiring different masking.\n        softcap (`float`, *optional*):\n            Softcap for the attention logits, used e.g. in gemma2.\n        deterministic (`bool`, *optional*):\n            Determines if the deterministic option introduced in flash_attn>=2.4.1 is enabled.\n        s_aux (`torch.Tensor`, *optional*):\n            Attention sink auxiliary that adds a `bias` to the attention calculation via an additional head.\n    Return:\n        flash_kwargs (`dict`):\n            A dict of kwargs that are requested and supported.\n    \"\"\"\n    flash_kwargs = {\n        \"causal\": is_causal and not (use_top_left_mask and query_length == 1),\n        \"softmax_scale\": softmax_scale,\n    }\n\n    if supports_mapping[\"dropout_p\"]:\n        flash_kwargs[\"dropout_p\"] = dropout\n\n    if supports_mapping[\"window_size\"] and sliding_window is not None and key_length > sliding_window:\n        # The flash attention API sets inclusive boundaries, i.e. (4, 0) would take 4 tokens to the left\n        # and the current token for a total size of 5. However, we usually define our window sizes by\n        # their total window size (when causal). Encoder models as of now seldom use SWA and when they\n        # do, they have a custom workaround (e.g. ModernBERT) which would align with this symmetric logic, i.e.\n        # for a total of `2*sliding_window + 1`.\n        flash_kwargs[\"window_size\"] = (sliding_window - 1, sliding_window - 1)\n\n    if supports_mapping[\"deterministic\"]:\n        flash_kwargs[\"deterministic\"] = (\n            deterministic if deterministic is not None else os.getenv(\"FLASH_ATTENTION_DETERMINISTIC\", \"0\") == \"1\"\n        )\n\n    if supports_mapping[\"softcap\"] and softcap is not None:\n        flash_kwargs[\"softcap\"] = softcap\n\n    # Only within kernel implementation atm\n    if supports_mapping[\"s_aux\"] and s_aux is not None:\n        flash_kwargs[\"s_aux\"] = s_aux\n\n    return flash_kwargs"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_pad_input": {
                "sorted_modules": {
                    "_pad_input": "\n\ndef _pad_input(hidden_states, indices, batch, seqlen):\n    \"\"\"\n    pad_input function for flash attention variants that do not have them within their pkg themselves, e.g. fa3.\n\n    Arguments:\n        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n        indices: (total_nnz), the indices that represent the non-masked tokens of the original padded input sequence.\n        batch: int, batch size for the padded sequence.\n        seqlen: int, maximum sequence length for the padded sequence.\n\n    Return:\n        hidden_states: (batch, seqlen, ...)\n    \"\"\"\n    dim = hidden_states.shape[1:]\n    output = torch.zeros((batch * seqlen), *dim, device=hidden_states.device, dtype=hidden_states.dtype)\n    output[indices] = hidden_states\n    return output.view(batch, seqlen, *dim)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_unpad_input": {
                "sorted_modules": {
                    "_unpad_input": "\n\ndef _unpad_input(hidden_states, attention_mask, unused_mask=None):\n    \"\"\"\n    unpad_input function for flash attention variants that do not have them within their pkg themselves, e.g. fa3.\n\n    Arguments:\n        hidden_states: (batch, seqlen, ...)\n        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n        unused_mask: (batch, seqlen), bool / int, 1 means the element is allocated but unused.\n\n    Return:\n        hidden_states: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask + unused_mask.\n        indices: (total_nnz), the indices of masked tokens from the flattened input sequence.\n        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n        max_seqlen_in_batch: int\n        seqused: (batch), returns the number of tokens selected in attention_mask + unused_mask.\n    \"\"\"\n    all_masks = (attention_mask + unused_mask) if unused_mask is not None else attention_mask\n    seqlens_in_batch = all_masks.sum(dim=-1, dtype=torch.int32)\n    used_seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(all_masks.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n\n    return (\n        _index_first_axis(hidden_states, indices),\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n        used_seqlens_in_batch,\n    )"
                },
                "component_dependencies": {
                    "_unpad_input": [
                        "transformers/modeling_flash_attention_utils.py#_index_first_axis"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING": {
                "sorted_modules": {
                    "AUTO_QUANTIZATION_CONFIG_MAPPING": "\nAUTO_QUANTIZATION_CONFIG_MAPPING = {\n    \"awq\": AwqConfig,\n    \"bitsandbytes_4bit\": BitsAndBytesConfig,\n    \"bitsandbytes_8bit\": BitsAndBytesConfig,\n    \"eetq\": EetqConfig,\n    \"gptq\": GPTQConfig,\n    \"aqlm\": AqlmConfig,\n    \"quanto\": QuantoConfig,\n    \"quark\": QuarkConfig,\n    \"fp_quant\": FPQuantConfig,\n    \"hqq\": HqqConfig,\n    \"compressed-tensors\": CompressedTensorsConfig,\n    \"fbgemm_fp8\": FbgemmFp8Config,\n    \"higgs\": HiggsConfig,\n    \"torchao\": TorchAoConfig,\n    \"bitnet\": BitNetQuantConfig,\n    \"vptq\": VptqConfig,\n    \"spqr\": SpQRConfig,\n    \"fp8\": FineGrainedFP8Config,\n    \"auto-round\": AutoRoundConfig,\n    \"mxfp4\": Mxfp4Config,\n}"
                },
                "component_dependencies": {
                    "AUTO_QUANTIZATION_CONFIG_MAPPING": [
                        "transformers/utils/quantization_config.py#AqlmConfig",
                        "transformers/utils/quantization_config.py#AutoRoundConfig",
                        "transformers/utils/quantization_config.py#AwqConfig",
                        "transformers/utils/quantization_config.py#BitNetQuantConfig",
                        "transformers/utils/quantization_config.py#BitsAndBytesConfig",
                        "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                        "transformers/utils/quantization_config.py#EetqConfig",
                        "transformers/utils/quantization_config.py#FPQuantConfig",
                        "transformers/utils/quantization_config.py#FbgemmFp8Config",
                        "transformers/utils/quantization_config.py#FineGrainedFP8Config",
                        "transformers/utils/quantization_config.py#GPTQConfig",
                        "transformers/utils/quantization_config.py#HiggsConfig",
                        "transformers/utils/quantization_config.py#HqqConfig",
                        "transformers/utils/quantization_config.py#Mxfp4Config",
                        "transformers/utils/quantization_config.py#QuantoConfig",
                        "transformers/utils/quantization_config.py#QuarkConfig",
                        "transformers/utils/quantization_config.py#SpQRConfig",
                        "transformers/utils/quantization_config.py#TorchAoConfig",
                        "transformers/utils/quantization_config.py#VptqConfig"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING": {
                "sorted_modules": {
                    "AUTO_QUANTIZER_MAPPING": "\n\nAUTO_QUANTIZER_MAPPING = {\n    \"awq\": AwqQuantizer,\n    \"bitsandbytes_4bit\": Bnb4BitHfQuantizer,\n    \"bitsandbytes_8bit\": Bnb8BitHfQuantizer,\n    \"gptq\": GptqHfQuantizer,\n    \"aqlm\": AqlmHfQuantizer,\n    \"quanto\": QuantoHfQuantizer,\n    \"quark\": QuarkHfQuantizer,\n    \"fp_quant\": FPQuantHfQuantizer,\n    \"eetq\": EetqHfQuantizer,\n    \"higgs\": HiggsHfQuantizer,\n    \"hqq\": HqqHfQuantizer,\n    \"compressed-tensors\": CompressedTensorsHfQuantizer,\n    \"fbgemm_fp8\": FbgemmFp8HfQuantizer,\n    \"torchao\": TorchAoHfQuantizer,\n    \"bitnet\": BitNetHfQuantizer,\n    \"vptq\": VptqHfQuantizer,\n    \"spqr\": SpQRHfQuantizer,\n    \"fp8\": FineGrainedFP8HfQuantizer,\n    \"auto-round\": AutoRoundQuantizer,\n    \"mxfp4\": Mxfp4HfQuantizer,\n}"
                },
                "component_dependencies": {
                    "AUTO_QUANTIZER_MAPPING": [
                        "transformers/quantizers/quantizer_aqlm.py#AqlmHfQuantizer",
                        "transformers/quantizers/quantizer_auto_round.py#AutoRoundQuantizer",
                        "transformers/quantizers/quantizer_awq.py#AwqQuantizer",
                        "transformers/quantizers/quantizer_bitnet.py#BitNetHfQuantizer",
                        "transformers/quantizers/quantizer_bnb_4bit.py#Bnb4BitHfQuantizer",
                        "transformers/quantizers/quantizer_bnb_8bit.py#Bnb8BitHfQuantizer",
                        "transformers/quantizers/quantizer_compressed_tensors.py#CompressedTensorsHfQuantizer",
                        "transformers/quantizers/quantizer_eetq.py#EetqHfQuantizer",
                        "transformers/quantizers/quantizer_fbgemm_fp8.py#FbgemmFp8HfQuantizer",
                        "transformers/quantizers/quantizer_finegrained_fp8.py#FineGrainedFP8HfQuantizer",
                        "transformers/quantizers/quantizer_fp_quant.py#FPQuantHfQuantizer",
                        "transformers/quantizers/quantizer_gptq.py#GptqHfQuantizer",
                        "transformers/quantizers/quantizer_higgs.py#HiggsHfQuantizer",
                        "transformers/quantizers/quantizer_hqq.py#HqqHfQuantizer",
                        "transformers/quantizers/quantizer_mxfp4.py#Mxfp4HfQuantizer",
                        "transformers/quantizers/quantizer_quanto.py#QuantoHfQuantizer",
                        "transformers/quantizers/quantizer_quark.py#QuarkHfQuantizer",
                        "transformers/quantizers/quantizer_spqr.py#SpQRHfQuantizer",
                        "transformers/quantizers/quantizer_torchao.py#TorchAoHfQuantizer",
                        "transformers/quantizers/quantizer_vptq.py#VptqHfQuantizer"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/auto.py#AutoQuantizationConfig": {
                "sorted_modules": {
                    "AutoQuantizationConfig": "\n\nclass AutoQuantizationConfig:\n    \"\"\"\n    The Auto-HF quantization config class that takes care of automatically dispatching to the correct\n    quantization config given a quantization config stored in a dictionary.\n    \"\"\"\n\n    @classmethod\n    def from_dict(cls, quantization_config_dict: dict):\n        quant_method = quantization_config_dict.get(\"quant_method\")\n        # We need a special care for bnb models to make sure everything is BC ..\n        if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n            suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n            quant_method = QuantizationMethod.BITS_AND_BYTES + suffix\n        elif quant_method is None:\n            raise ValueError(\n                \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n            )\n\n        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING:\n            raise ValueError(\n                f\"Unknown quantization type, got {quant_method} - supported types are:\"\n                f\" {list(AUTO_QUANTIZER_MAPPING.keys())}\"\n            )\n\n        target_cls = AUTO_QUANTIZATION_CONFIG_MAPPING[quant_method]\n        return target_cls.from_dict(quantization_config_dict)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        if getattr(model_config, \"quantization_config\", None) is None:\n            raise ValueError(\n                f\"Did not found a `quantization_config` in {pretrained_model_name_or_path}. Make sure that the model is correctly quantized.\"\n            )\n        quantization_config_dict = model_config.quantization_config\n        quantization_config = cls.from_dict(quantization_config_dict)\n        # Update with potential kwargs that are passed through from_pretrained.\n        quantization_config.update(**kwargs)\n        return quantization_config"
                },
                "component_dependencies": {
                    "AutoQuantizationConfig": [
                        "transformers/models/auto/configuration_auto.py#AutoConfig.from_pretrained",
                        "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                        "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING",
                        "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#AutoRoundConfig": {
                "sorted_modules": {
                    "AutoRoundConfig": "\n\n@dataclass\nclass AutoRoundConfig(QuantizationConfigMixin):\n    \"\"\"This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded AutoRound quantization.\n\n    Args:\n        bits (`int`, *optional*, defaults to 4):\n            The number of bits to quantize to, supported numbers are (2, 3, 4, 8).\n        group_size (`int`, *optional*, defaults to 128): Group-size value\n        sym (`bool`, *optional*, defaults to `True`): Symmetric quantization or not\n        backend (`str`, *optional*, defaults to `\"auto\"`): The kernel to use, e.g., ipex,marlin, exllamav2, triton, etc. Ref. https://github.com/intel/auto-round?tab=readme-ov-file#specify-backend\n    \"\"\"\n\n    def __init__(\n        self,\n        bits: int = 4,\n        group_size: int = 128,\n        sym: bool = True,\n        backend: str = \"auto\",\n        **kwargs,\n    ):\n        self.bits = bits\n        self.group_size = group_size\n        self.sym = sym\n        self.backend = backend\n        self.packing_format = \"auto_round:gptq\"\n        if kwargs is not None:\n            for key, value in kwargs.items():\n                setattr(self, key, value)\n        self.quant_method = QuantizationMethod.AUTOROUND\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"Safety checker that arguments are correct.\"\"\"\n        if self.bits not in [2, 3, 4, 8]:\n            raise ValueError(f\"Only support quantization to [2,3,4,8] bits but found {self.bits}\")\n        if self.group_size != -1 and self.group_size <= 0:\n            raise ValueError(\"group_size must be greater than 0 or equal to -1\")\n\n    def get_loading_attributes(self):\n        loading_attributes_dict = {\"backend\": self.backend}\n        return loading_attributes_dict\n\n    def to_dict(self):\n        config_dict = super().to_dict()\n        return config_dict\n\n    @classmethod\n    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n        quant_method = config_dict[\"quant_method\"]\n        if \"auto-round\" not in quant_method and \"gptq\" not in quant_method and \"awq\" not in quant_method:\n            raise NotImplementedError(\n                \"Failed to convert to auto_round format. Only `gptqv1`, `awq`, and `auto-round` formats are supported.\"\n            )\n\n        if \"gptq\" in quant_method and \"meta\" in config_dict:\n            raise NotImplementedError(\"Failed to convert gptq format to auto_round format. Only supports `gptqv1`\")\n\n        if \"awq\" in quant_method and config_dict.get(\"version\", \"gemm\") != \"gemm\":\n            raise NotImplementedError(\n                \"Failed to convert awq format to auto_round format. Only supports awq format with gemm version\"\n            )\n\n        if \"auto-round\" not in quant_method:\n            config_dict[\"packing_format\"] = f\"auto_round:{quant_method}\"\n\n        return super().from_dict(config_dict, return_unused_kwargs=return_unused_kwargs, **kwargs)"
                },
                "component_dependencies": {
                    "AutoRoundConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#AwqConfig": {
                "sorted_modules": {
                    "AwqConfig": "\n\n@dataclass\nclass AwqConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using `auto-awq` library awq quantization relying on auto_awq backend.\n\n    Args:\n        bits (`int`, *optional*, defaults to 4):\n            The number of bits to quantize to.\n        group_size (`int`, *optional*, defaults to 128):\n            The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.\n        zero_point (`bool`, *optional*, defaults to `True`):\n            Whether to use zero point quantization.\n        version (`AWQLinearVersion`, *optional*, defaults to `AWQLinearVersion.GEMM`):\n            The version of the quantization algorithm to use. GEMM is better for big batch_size (e.g. >= 8) otherwise,\n            GEMV is better (e.g. < 8 ). GEMM models are compatible with Exllama kernels.\n        backend (`AwqBackendPackingMethod`, *optional*, defaults to `AwqBackendPackingMethod.AUTOAWQ`):\n            The quantization backend. Some models might be quantized using `llm-awq` backend. This is useful for users\n            that quantize their own models using `llm-awq` library.\n        do_fuse (`bool`, *optional*, defaults to `False`):\n            Whether to fuse attention and mlp layers together for faster inference\n        fuse_max_seq_len (`int`, *optional*):\n            The Maximum sequence length to generate when using fusing.\n        modules_to_fuse (`dict`, *optional*, default to `None`):\n            Overwrite the natively supported fusing scheme with the one specified by the users.\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision (e.g. Whisper encoder, Llava encoder, Mixtral gate layers).\n            Note you cannot quantize directly with transformers, please refer to `AutoAWQ` documentation for quantizing HF models.\n        exllama_config (`dict[str, Any]`, *optional*):\n            You can specify the version of the exllama kernel through the `version` key, the maximum sequence\n            length through the `max_input_len` key, and the maximum batch size through the `max_batch_size` key.\n            Defaults to `{\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8}` if unset.\n    \"\"\"\n\n    def __init__(\n        self,\n        bits: int = 4,\n        group_size: int = 128,\n        zero_point: bool = True,\n        version: AWQLinearVersion = AWQLinearVersion.GEMM,\n        backend: AwqBackendPackingMethod = AwqBackendPackingMethod.AUTOAWQ,\n        do_fuse: bool | None = None,\n        fuse_max_seq_len: int | None = None,\n        modules_to_fuse: dict | None = None,\n        modules_to_not_convert: list | None = None,\n        exllama_config: dict[str, int] | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.AWQ\n\n        self.bits = bits\n        self.group_size = group_size\n        self.zero_point = zero_point\n        self.version = version\n        self.backend = backend\n        self.fuse_max_seq_len = fuse_max_seq_len\n        self.modules_to_not_convert = modules_to_not_convert\n        self.exllama_config = exllama_config\n\n        self.modules_to_fuse = modules_to_fuse\n        if do_fuse is None:\n            self.do_fuse = modules_to_fuse is not None and len(modules_to_fuse) > 0\n        else:\n            self.do_fuse = do_fuse\n        self.fuse_max_seq_len = fuse_max_seq_len\n\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        if self.backend not in [AwqBackendPackingMethod.AUTOAWQ, AwqBackendPackingMethod.LLMAWQ]:\n            raise ValueError(\n                f\"Only supported quantization backends in {AwqBackendPackingMethod.AUTOAWQ} and {AwqBackendPackingMethod.LLMAWQ} - not recognized backend {self.backend}\"\n            )\n\n        self.version = AWQLinearVersion.from_str(self.version)\n        if self.version not in [\n            AWQLinearVersion.GEMM,\n            AWQLinearVersion.GEMV,\n            AWQLinearVersion.EXLLAMA,\n            AWQLinearVersion.IPEX,\n        ]:\n            raise ValueError(\n                f\"Only supported versions are in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.EXLLAMA, AWQLinearVersion.IPEX] - not recognized version {self.version}\"\n            )\n\n        if self.backend == AwqBackendPackingMethod.LLMAWQ:\n            # Only cuda device can run this function\n            if not (torch.cuda.is_available() or torch.xpu.is_available()):\n                raise ValueError(\"LLM-AWQ backend is only supported on CUDA and XPU\")\n            if torch.cuda.is_available():\n                compute_capability = torch.cuda.get_device_capability()\n                major, minor = compute_capability\n                if major < 8:\n                    raise ValueError(\"LLM-AWQ backend is only supported on CUDA GPUs with compute capability >= 8.0\")\n\n        if self.do_fuse and self.fuse_max_seq_len is None:\n            raise ValueError(\n                \"You cannot enable fused modules without specifying a `fuse_max_seq_len`, make sure to pass a valid `fuse_max_seq_len` for your usecase\"\n            )\n\n        if self.do_fuse:\n            awq_version_supports_fusing = False\n            MIN_AWQ_VERSION = \"0.1.7\"\n            if is_auto_awq_available():\n                awq_version_supports_fusing = version.parse(importlib.metadata.version(\"autoawq\")) >= version.parse(\n                    MIN_AWQ_VERSION\n                )\n\n            if not awq_version_supports_fusing:\n                raise ValueError(\n                    f\"You current version of `autoawq` does not support module fusing, please upgrade `autoawq` package to at least {MIN_AWQ_VERSION}.\"\n                )\n\n        if self.modules_to_not_convert is not None:\n            awq_version_supports_non_conversion = False\n            MIN_AWQ_VERSION = \"0.1.8\"\n            if is_auto_awq_available():\n                awq_version_supports_non_conversion = version.parse(\n                    importlib.metadata.version(\"autoawq\")\n                ) >= version.parse(MIN_AWQ_VERSION)\n\n            if not awq_version_supports_non_conversion:\n                raise ValueError(\n                    f\"You current version of `autoawq` does not support module quantization skipping, please upgrade `autoawq` package to at least {MIN_AWQ_VERSION}.\"\n                )\n\n        if self.do_fuse and self.modules_to_fuse is not None:\n            required_keys = [\n                \"hidden_size\",\n                \"num_attention_heads\",\n                \"num_key_value_heads\",\n                \"mlp\",\n                \"attention\",\n                \"layernorm\",\n                \"use_alibi\",\n            ]\n            if not all(key in self.modules_to_fuse for key in required_keys):\n                raise ValueError(\n                    f\"Required fields are missing in the fusing mapping, required fields are {required_keys}\"\n                )\n\n        if self.version == AWQLinearVersion.EXLLAMA:\n            awq_version_supports_exllama = False\n            MIN_AWQ_VERSION = \"0.2.0\"\n            if is_auto_awq_available():\n                awq_version_supports_exllama = version.parse(importlib.metadata.version(\"autoawq\")) >= version.parse(\n                    MIN_AWQ_VERSION\n                )\n\n            if not awq_version_supports_exllama:\n                raise ValueError(\n                    f\"You current version of `autoawq` does not support exllama backend, \"\n                    f\"please upgrade `autoawq` package to at least {MIN_AWQ_VERSION}.\"\n                )\n\n            if self.exllama_config is None:\n                self.exllama_config = {\"version\": ExllamaVersion.TWO, \"max_input_len\": 2048, \"max_batch_size\": 8}\n            else:\n                if \"version\" not in self.exllama_config:\n                    raise ValueError(\"`exllama_config` needs to have a `version` key.\")\n                elif self.exllama_config[\"version\"] not in [ExllamaVersion.ONE, ExllamaVersion.TWO]:\n                    exllama_version = self.exllama_config[\"version\"]\n                    raise ValueError(\n                        f\"Only supported versions are in [ExllamaVersion.ONE, ExllamaVersion.TWO] - not recognized version {exllama_version}\"\n                    )\n\n    def get_loading_attributes(self):\n        attributes_dict = copy.deepcopy(self.__dict__)\n        loading_attributes = [\"version\", \"do_fuse\", \"modules_to_fuse\", \"fuse_max_seq_len\", \"exllama_config\"]\n        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n        return loading_attributes_dict"
                },
                "component_dependencies": {
                    "AwqConfig": [
                        "transformers/utils.py#is_auto_awq_available",
                        "transformers/utils/quantization_config.py#AWQLinearVersion",
                        "transformers/utils/quantization_config.py#AwqBackendPackingMethod",
                        "transformers/utils/quantization_config.py#ExllamaVersion",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#CompressedTensorsConfig": {
                "sorted_modules": {
                    "CompressedTensorsConfig": "\n\nclass CompressedTensorsConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class that handles compressed-tensors quantization config options.\n    It is a wrapper around `compressed_tensors.QuantizationConfig`\n    Args:\n        config_groups (`typing.dict[str, typing.Union[ForwardRef('QuantizationScheme'), typing.list[str]]]`, *optional*):\n            dictionary mapping group name to a quantization scheme definition\n        format (`str`, *optional*, defaults to `\"dense\"`):\n            format the model is represented as. Set `run_compressed` True to execute model as the\n            compressed format if not `dense`\n        quantization_status (`QuantizationStatus`, *optional*, defaults to `\"initialized\"`):\n            status of model in the quantization lifecycle, ie 'initialized', 'calibration', 'frozen'\n        kv_cache_scheme (`typing.Union[QuantizationArgs, NoneType]`, *optional*):\n            specifies quantization of the kv cache. If None, kv cache is not quantized.\n        global_compression_ratio (`typing.Union[float, NoneType]`, *optional*):\n            0-1 float percentage of model compression\n        ignore (`typing.Union[typing.list[str], NoneType]`, *optional*):\n            layer names or types to not quantize, supports regex prefixed by 're:'\n        sparsity_config (`typing.dict[str, typing.Any]`, *optional*):\n            configuration for sparsity compression\n        quant_method (`str`, *optional*, defaults to `\"compressed-tensors\"`):\n            do not override, should be compressed-tensors\n        run_compressed (`bool`, *optional*, defaults to `True`): alter submodules (usually linear) in order to\n            emulate compressed model execution if True, otherwise use default submodule\n    \"\"\"\n\n    def __init__(\n        self,\n        config_groups: dict[str, Union[\"QuantizationScheme\", list[str]]] | None = None,  # noqa: F821\n        format: str = \"dense\",\n        quantization_status: \"QuantizationStatus\" = \"initialized\",  # noqa: F821\n        kv_cache_scheme: Optional[\"QuantizationArgs\"] = None,  # noqa: F821\n        global_compression_ratio: float | None = None,\n        ignore: list[str] | None = None,\n        sparsity_config: dict[str, Any] | None = None,\n        quant_method: str = \"compressed-tensors\",\n        run_compressed: bool = True,\n        **kwargs,\n    ):\n        if is_compressed_tensors_available():\n            from compressed_tensors.config import SparsityCompressionConfig\n            from compressed_tensors.quantization import QuantizationConfig\n        else:\n            raise ImportError(\n                \"compressed_tensors is not installed and is required for compressed-tensors quantization. Please install it with `pip install compressed-tensors`.\"\n            )\n        self.quantization_config = None\n        self.sparsity_config = None\n\n        self.run_compressed = run_compressed\n\n        # parse from dict to load nested QuantizationScheme objects\n        if config_groups or kv_cache_scheme:\n            self.quantization_config = QuantizationConfig.model_validate(\n                {\n                    \"config_groups\": config_groups,\n                    \"quant_method\": quant_method,\n                    \"format\": format,\n                    \"quantization_status\": quantization_status,\n                    \"kv_cache_scheme\": kv_cache_scheme,\n                    \"global_compression_ratio\": global_compression_ratio,\n                    \"ignore\": ignore,\n                    **kwargs,\n                }\n            )\n\n        if sparsity_config:\n            self.sparsity_config = SparsityCompressionConfig.load_from_registry(\n                sparsity_config.get(\"format\"), **sparsity_config\n            )\n\n        self.quant_method = QuantizationMethod.COMPRESSED_TENSORS\n\n    def post_init(self):\n        if self.run_compressed:\n            if self.is_sparsification_compressed:\n                logger.warning(\n                    \"`run_compressed` is only supported for quantized_compressed models\"\n                    \" and not for sparsified models. Setting `run_compressed=False`\"\n                )\n                self.run_compressed = False\n            elif not self.is_quantization_compressed:\n                logger.warning(\n                    \"`run_compressed` is only supported for compressed models. Setting `run_compressed=False`\"\n                )\n                self.run_compressed = False\n\n    @classmethod\n    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n        \"\"\"\n        Instantiates a [`CompressedTensorsConfig`] from a Python dictionary of parameters.\n        Optionally unwraps any args from the nested quantization_config\n\n        Args:\n            config_dict (`dict[str, Any]`):\n                Dictionary that will be used to instantiate the configuration object.\n            return_unused_kwargs (`bool`,*optional*, defaults to `False`):\n                Whether or not to return a list of unused keyword arguments. Used for `from_pretrained` method in\n                `PreTrainedModel`.\n            kwargs (`dict[str, Any]`):\n                Additional parameters from which to initialize the configuration object.\n\n        Returns:\n            [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\n\n        \"\"\"\n\n        if \"quantization_config\" in config_dict:\n            config_dict = dict(\n                sparsity_config=config_dict.get(\"sparsity_config\"),\n                **config_dict[\"quantization_config\"],\n            )\n\n        return super().from_dict(config_dict, return_unused_kwargs=return_unused_kwargs, **kwargs)\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Quantization config to be added to config.json\n\n        Serializes this instance to a Python dictionary. Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        quantization_config = {}\n        if self.quantization_config is not None:\n            quantization_config = self.quantization_config.model_dump()\n        else:\n            quantization_config[\"quant_method\"] = QuantizationMethod.COMPRESSED_TENSORS\n\n        if self.sparsity_config is not None:\n            quantization_config[\"sparsity_config\"] = self.sparsity_config.model_dump()\n        else:\n            quantization_config[\"sparsity_config\"] = {}\n\n        return quantization_config\n\n    def to_diff_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Removes all attributes from config which correspond to the default config attributes for better readability and\n        serializes to a Python dictionary.\n        Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"\n        config_dict = self.to_dict()\n\n        # get the default config dict\n        default_config_dict = CompressedTensorsConfig().to_dict()\n\n        serializable_config_dict = {}\n\n        # only serialize values that differ from the default config\n        for key, value in config_dict.items():\n            if key not in default_config_dict or value != default_config_dict[key]:\n                serializable_config_dict[key] = value\n\n        return serializable_config_dict\n\n    def get_loading_attributes(self):\n        return {\"run_compressed\": self.run_compressed}\n\n    @property\n    def is_quantized(self):\n        return bool(self.quantization_config) and bool(self.quantization_config.config_groups)\n\n    @property\n    def is_quantization_compressed(self):\n        from compressed_tensors.quantization import QuantizationStatus\n\n        return self.is_quantized and self.quantization_config.quantization_status == QuantizationStatus.COMPRESSED\n\n    @property\n    def is_sparsification_compressed(self):\n        from compressed_tensors.config import (\n            CompressionFormat,\n            SparsityCompressionConfig,\n        )\n\n        return (\n            isinstance(self.sparsity_config, SparsityCompressionConfig)\n            and self.sparsity_config.format != CompressionFormat.dense.value\n        )"
                },
                "component_dependencies": {
                    "CompressedTensorsConfig": [
                        "transformers/utils.py#is_compressed_tensors_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod",
                        "transformers/utils/quantization_config.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#FbgemmFp8Config": {
                "sorted_modules": {
                    "FbgemmFp8Config": "\n\n@dataclass\nclass FbgemmFp8Config(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using fbgemm fp8 quantization.\n\n    Args:\n        activation_scale_ub (`float`, *optional*, defaults to 1200.0):\n            The activation scale upper bound. This is used when quantizing the input activation.\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision.\n    \"\"\"\n\n    def __init__(\n        self,\n        activation_scale_ub: float = 1200.0,\n        modules_to_not_convert: list | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.FBGEMM_FP8\n        self.activation_scale_ub = activation_scale_ub\n        self.modules_to_not_convert = modules_to_not_convert\n\n    def get_loading_attributes(self):\n        attributes_dict = copy.deepcopy(self.__dict__)\n        loading_attributes = [\"activation_scale_ub\"]\n        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n        return loading_attributes_dict"
                },
                "component_dependencies": {
                    "FbgemmFp8Config": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#GPTQConfig": {
                "sorted_modules": {
                    "GPTQConfig": "\n\n@dataclass\nclass GPTQConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using `optimum` api for gptq quantization relying on auto_gptq backend.\n\n    Args:\n        bits (`int`):\n            The number of bits to quantize to, supported numbers are (2, 3, 4, 8).\n        tokenizer (`str` or `PreTrainedTokenizerBase`, *optional*):\n            The tokenizer used to process the dataset. You can pass either:\n                - A custom tokenizer object.\n                - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n                - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n                    using the [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n        dataset (`Union[list[str]]`, *optional*):\n            The dataset used for quantization. You can provide your own dataset in a list of string or just use the\n            original datasets used in GPTQ paper ['wikitext2','c4','c4-new']\n        group_size (`int`, *optional*, defaults to 128):\n            The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.\n        damp_percent (`float`, *optional*, defaults to 0.1):\n            The percent of the average Hessian diagonal to use for dampening. Recommended value is 0.1.\n        desc_act (`bool`, *optional*, defaults to `False`):\n            Whether to quantize columns in order of decreasing activation size. Setting it to False can significantly\n            speed up inference but the perplexity may become slightly worse. Also known as act-order.\n        sym (`bool`, *optional*, defaults to `True`):\n            Whether to use symmetric quantization.\n        true_sequential (`bool`, *optional*, defaults to `True`):\n            Whether to perform sequential quantization even within a single Transformer block. Instead of quantizing\n            the entire block at once, we perform layer-wise quantization. As a result, each layer undergoes\n            quantization using inputs that have passed through the previously quantized layers.\n        checkpoint_format (`str`, *optional*, defaults to `\"gptq\"`):\n            GPTQ weight format. `gptq`(v1) is supported by both gptqmodel and auto-gptq. `gptq_v2` is gptqmodel only.\n        meta (`dict[str, any]`, *optional*):\n            Properties, such as tooling:version, that do not directly contributes to quantization or quant inference are stored in meta.\n            i.e. `meta.quantizer`: [\"optimum:_version_\", \"gptqmodel:_version_\"]\n        backend (`str`, *optional*):\n            Controls which gptq kernel to be used. Valid values for gptqmodel are `auto`, `auto_trainable` and more. For auto-gptq, only\n            valid value is None and `auto_trainable`. Ref gptqmodel backends: https://github.com/ModelCloud/GPTQModel/blob/main/gptqmodel/utils/backend.py\n        use_cuda_fp16 (`bool`, *optional*, defaults to `False`):\n            Whether or not to use optimized cuda kernel for fp16 model. Need to have model in fp16. Auto-gptq only.\n        model_seqlen (`int`, *optional*):\n            The maximum sequence length that the model can take.\n        block_name_to_quantize (`str`, *optional*):\n            The transformers block name to quantize. If None, we will infer the block name using common patterns (e.g. model.layers)\n        module_name_preceding_first_block (`list[str]`, *optional*):\n            The layers that are preceding the first Transformer block.\n        batch_size (`int`, *optional*, defaults to 1):\n            The batch size used when processing the dataset\n        pad_token_id (`int`, *optional*):\n            The pad token id. Needed to prepare the dataset when `batch_size` > 1.\n        use_exllama (`bool`, *optional*):\n            Whether to use exllama backend. Defaults to `True` if unset. Only works with `bits` = 4.\n        max_input_length (`int`, *optional*):\n            The maximum input length. This is needed to initialize a buffer that depends on the maximum expected input\n            length. It is specific to the exllama backend with act-order.\n        exllama_config (`dict[str, Any]`, *optional*):\n            The exllama config. You can specify the version of the exllama kernel through the `version` key. Defaults\n            to `{\"version\": 1}` if unset.\n        cache_block_outputs (`bool`, *optional*, defaults to `True`):\n            Whether to cache block outputs to reuse as inputs for the succeeding block.\n        modules_in_block_to_quantize (`list[list[str]]`, *optional*):\n            List of list of module names to quantize in the specified block. This argument is useful to exclude certain linear modules from being quantized.\n            The block to quantize can be specified by setting `block_name_to_quantize`. We will quantize each list sequentially. If not set, we will quantize all linear layers.\n            Example: `modules_in_block_to_quantize =[[\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\"], [\"self_attn.o_proj\"]]`.\n            In this example, we will first quantize the q,k,v layers simultaneously since they are independent.\n            Then, we will quantize `self_attn.o_proj` layer with the q,k,v layers quantized. This way, we will get\n            better results since it reflects the real input `self_attn.o_proj` will get when the model is quantized.\n    \"\"\"\n\n    def __init__(\n        self,\n        bits: int,\n        tokenizer: Any = None,\n        dataset: list[str] | str | None = None,\n        group_size: int = 128,\n        damp_percent: float = 0.1,\n        desc_act: bool = False,\n        sym: bool = True,\n        true_sequential: bool = True,\n        checkpoint_format: str = \"gptq\",\n        meta: dict[str, Any] | None = None,\n        backend: str | None = None,\n        use_cuda_fp16: bool = False,\n        model_seqlen: int | None = None,\n        block_name_to_quantize: str | None = None,\n        module_name_preceding_first_block: list[str] | None = None,\n        batch_size: int = 1,\n        pad_token_id: int | None = None,\n        use_exllama: bool | None = None,\n        max_input_length: int | None = None,\n        exllama_config: dict[str, Any] | None = None,\n        cache_block_outputs: bool = True,\n        modules_in_block_to_quantize: list[list[str]] | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.GPTQ\n        self.bits = bits\n        self.tokenizer = tokenizer\n        self.dataset = dataset\n        self.group_size = group_size\n        self.damp_percent = damp_percent\n        self.desc_act = desc_act\n        self.sym = sym\n        self.true_sequential = true_sequential\n        self.checkpoint_format = checkpoint_format.lower()\n        self.meta = meta\n        self.backend = backend.lower() if isinstance(backend, str) else backend\n        self.use_cuda_fp16 = use_cuda_fp16\n        self.model_seqlen = model_seqlen\n        self.block_name_to_quantize = block_name_to_quantize\n        self.module_name_preceding_first_block = module_name_preceding_first_block\n        self.batch_size = batch_size\n        self.pad_token_id = pad_token_id\n        self.use_exllama = use_exllama\n        self.max_input_length = max_input_length\n        self.exllama_config = exllama_config\n        self.cache_block_outputs = cache_block_outputs\n        self.modules_in_block_to_quantize = modules_in_block_to_quantize\n        self.post_init()\n\n    def get_loading_attributes(self):\n        attributes_dict = copy.deepcopy(self.__dict__)\n        loading_attributes = [\n            \"use_exllama\",\n            \"exllama_config\",\n            \"use_cuda_fp16\",\n            \"max_input_length\",\n            \"backend\",\n        ]\n        loading_attributes_dict = {i: j for i, j in attributes_dict.items() if i in loading_attributes}\n        return loading_attributes_dict\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        if self.bits not in [2, 3, 4, 8]:\n            raise ValueError(f\"Only support quantization to [2,3,4,8] bits but found {self.bits}\")\n        if self.group_size != -1 and self.group_size <= 0:\n            raise ValueError(\"group_size must be greater than 0 or equal to -1\")\n        if not (0 < self.damp_percent < 1):\n            raise ValueError(\"damp_percent must between 0 and 1.\")\n        if self.dataset is not None:\n            if isinstance(self.dataset, str):\n                if self.dataset not in [\"wikitext2\", \"c4\", \"c4-new\"]:\n                    raise ValueError(\n                        f\"\"\"You have entered a string value for dataset. You can only choose between\n                        ['wikitext2','c4','c4-new'], but we found {self.dataset}\"\"\"\n                    )\n            elif not isinstance(self.dataset, list):\n                raise ValueError(\n                    f\"\"\"dataset needs to be either a list of string or a value in\n                    ['wikitext2','c4','c4-new'], but we found {self.dataset}\"\"\"\n                )\n\n        # make sure backend is back/forward compatible with both gptqmodel (full) and auto-gptq (partial)\n        if is_gptqmodel_available():\n            # convert auto-gptq control into gptqmodel backend\n            if self.backend is None:\n                self.backend = \"auto_trainable\" if self.use_exllama is not None and not self.use_exllama else \"auto\"\n        else:\n            # convert gptqmodel backend `auto_trainable` into auto-gptq control\n            if self.backend == \"auto_trainable\":\n                self.use_exllama = False\n\n        # auto-gptq specific kernel control logic\n        if self.use_exllama is None:\n            # New default behaviour\n            self.use_exllama = True\n\n        if self.exllama_config is None:\n            self.exllama_config = {\"version\": ExllamaVersion.ONE}\n        else:\n            if \"version\" not in self.exllama_config:\n                raise ValueError(\"`exllama_config` needs to have a `version` key.\")\n            elif self.exllama_config[\"version\"] not in [ExllamaVersion.ONE, ExllamaVersion.TWO]:\n                exllama_version = self.exllama_config[\"version\"]\n                raise ValueError(\n                    f\"Only supported versions are in [ExllamaVersion.ONE, ExllamaVersion.TWO] - not recognized version {exllama_version}\"\n                )\n\n        if self.bits == 4 and self.use_exllama:\n            if self.exllama_config[\"version\"] == ExllamaVersion.ONE:\n                logger.info(\n                    \"You have activated exllama backend. Note that you can get better inference \"\n                    \"speed using exllamav2 kernel by setting `exllama_config`.\"\n                )\n            elif self.exllama_config[\"version\"] == ExllamaVersion.TWO:\n                if is_auto_gptq_available():\n                    optimum_version = version.parse(importlib.metadata.version(\"optimum\"))\n                    autogptq_version = version.parse(importlib.metadata.version(\"auto_gptq\"))\n                    if optimum_version <= version.parse(\"1.13.2\") or autogptq_version <= version.parse(\"0.4.2\"):\n                        raise ValueError(\n                            f\"You need optimum > 1.13.2 and auto-gptq > 0.4.2 . Make sure to have that version installed - detected version : optimum {optimum_version} and autogptq {autogptq_version}\"\n                        )\n        if self.modules_in_block_to_quantize is not None:\n            optimum_version = version.parse(importlib.metadata.version(\"optimum\"))\n            if optimum_version < version.parse(\"1.15.0\"):\n                raise ValueError(\n                    \"You current version of `optimum` does not support `modules_in_block_to_quantize` quantization argument, please upgrade `optimum` package to a version superior than 1.15.0 .\"\n                )\n\n    def to_dict(self) -> dict[str, Any]:\n        config_dict = super().to_dict()\n        config_dict.pop(\"disable_exllama\", None)\n        return config_dict\n\n    def to_dict_optimum(self):\n        \"\"\"\n        Get compatible dict for optimum gptq config\n        \"\"\"\n        quant_dict = self.to_dict()\n        # make it compatible with optimum config\n        quant_dict[\"disable_exllama\"] = not self.use_exllama\n        return quant_dict\n\n    @classmethod\n    def from_dict_optimum(cls, config_dict):\n        \"\"\"\n        Get compatible class with optimum gptq config dict\n        \"\"\"\n\n        if \"disable_exllama\" in config_dict:\n            config_dict[\"use_exllama\"] = not config_dict[\"disable_exllama\"]\n            # switch to None to not trigger the warning\n            config_dict.pop(\"disable_exllama\")\n\n        config = cls(**config_dict)\n        return config"
                },
                "component_dependencies": {
                    "GPTQConfig": [
                        "transformers/utils.py#is_gptqmodel_available",
                        "transformers/utils/import_utils.py#is_auto_gptq_available",
                        "transformers/utils/quantization_config.py#ExllamaVersion",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod",
                        "transformers/utils/quantization_config.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#Mxfp4Config": {
                "sorted_modules": {
                    "Mxfp4Config": "\n\n@dataclass\nclass Mxfp4Config(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using mxfp4 quantization.\n\n    Args:\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision.\n        dequantize (`bool`, *optional*, default to `False`):\n            Whether we dequantize the model to bf16 precision or not\n    \"\"\"\n\n    def __init__(\n        self,\n        modules_to_not_convert: list | None = None,\n        dequantize: bool = False,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.MXFP4\n        self.modules_to_not_convert = modules_to_not_convert\n        self.dequantize = dequantize\n\n    def get_loading_attributes(self):\n        return {\"dequantize\": self.dequantize}\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Serializes this instance to a Python dictionary. Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        return {\"quant_method\": self.quant_method, \"modules_to_not_convert\": self.modules_to_not_convert}"
                },
                "component_dependencies": {
                    "Mxfp4Config": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#QuantizationConfigMixin": {
                "sorted_modules": {
                    "QuantizationConfigMixin": "\n\n@dataclass\nclass QuantizationConfigMixin:\n    \"\"\"\n    Mixin class for quantization config\n    \"\"\"\n\n    quant_method: QuantizationMethod\n\n    @classmethod\n    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n        \"\"\"\n        Instantiates a [`QuantizationConfigMixin`] from a Python dictionary of parameters.\n\n        Args:\n            config_dict (`dict[str, Any]`):\n                Dictionary that will be used to instantiate the configuration object.\n            return_unused_kwargs (`bool`,*optional*, defaults to `False`):\n                Whether or not to return a list of unused keyword arguments. Used for `from_pretrained` method in\n                `PreTrainedModel`.\n            kwargs (`dict[str, Any]`):\n                Additional parameters from which to initialize the configuration object.\n\n        Returns:\n            [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\n        \"\"\"\n        config = cls(**config_dict)\n\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n                to_remove.append(key)\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    def to_json_file(self, json_file_path: str | os.PathLike):\n        \"\"\"\n        Save this instance to a JSON file.\n\n        Args:\n            json_file_path (`str` or `os.PathLike`):\n                Path to the JSON file in which this configuration instance's parameters will be saved.\n            use_diff (`bool`, *optional*, defaults to `True`):\n                If set to `True`, only the difference between the config instance and the default\n                `QuantizationConfig()` is serialized to JSON file.\n        \"\"\"\n        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n            config_dict = self.to_dict()\n            json_string = json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n\n            writer.write(json_string)\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Serializes this instance to a Python dictionary. Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        return copy.deepcopy(self.__dict__)\n\n    def __iter__(self):\n        \"\"\"allows `dict(obj)` for situations where obj may be a dict or QuantizationConfigMixin\"\"\"\n        for attr, value in copy.deepcopy(self.__dict__).items():\n            yield attr, value\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n\n    def to_json_string(self, use_diff: bool = True) -> str:\n        \"\"\"\n        Serializes this instance to a JSON string.\n\n        Args:\n            use_diff (`bool`, *optional*, defaults to `True`):\n                If set to `True`, only the difference between the config instance and the default `PreTrainedConfig()`\n                is serialized to JSON string.\n\n        Returns:\n            `str`: String containing all the attributes that make up this configuration instance in JSON format.\n        \"\"\"\n        if use_diff is True:\n            config_dict = self.to_diff_dict()\n        else:\n            config_dict = self.to_dict()\n        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n\n    def update(self, **kwargs):\n        \"\"\"\n        Updates attributes of this class instance with attributes from `kwargs` if they match existing attributes,\n        returning all the unused kwargs.\n\n        Args:\n            kwargs (`dict[str, Any]`):\n                Dictionary of attributes to tentatively update this class.\n\n        Returns:\n            `dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n        \"\"\"\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(self, key):\n                setattr(self, key, value)\n                to_remove.append(key)\n\n        # Remove all the attributes that were updated, without modifying the input dict\n        unused_kwargs = {key: value for key, value in kwargs.items() if key not in to_remove}\n        return unused_kwargs"
                },
                "component_dependencies": {
                    "QuantizationConfigMixin": [
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#ACCELERATE_IMPORT_ERROR": {
                "sorted_modules": {
                    "ACCELERATE_IMPORT_ERROR": "\n# docstyle-ignore\nACCELERATE_IMPORT_ERROR = \"\"\"\n{0} requires the accelerate library >= {ACCELERATE_MIN_VERSION} it was not found in your environment.\nYou can install or update it with pip: `pip install --upgrade accelerate`. Please note that you may need to restart your\nruntime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#AV_IMPORT_ERROR": {
                "sorted_modules": {
                    "AV_IMPORT_ERROR": "\n\n# docstyle-ignore\nAV_IMPORT_ERROR = \"\"\"\n{0} requires the PyAv library but it was not found in your environment. You can install it with:\n```\npip install av\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#BS4_IMPORT_ERROR": {
                "sorted_modules": {
                    "BS4_IMPORT_ERROR": "\n# docstyle-ignore\nBS4_IMPORT_ERROR = \"\"\"\n{0} requires the Beautiful Soup library but it was not found in your environment. You can install it with pip:\n`pip install beautifulsoup4`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#CCL_IMPORT_ERROR": {
                "sorted_modules": {
                    "CCL_IMPORT_ERROR": "\n# docstyle-ignore\nCCL_IMPORT_ERROR = \"\"\"\n{0} requires the torch ccl library but it was not found in your environment. You can install it with pip:\n`pip install oneccl_bind_pt -f https://developer.intel.com/ipex-whl-stable`\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#CV2_IMPORT_ERROR": {
                "sorted_modules": {
                    "CV2_IMPORT_ERROR": "\n# docstyle-ignore\nCV2_IMPORT_ERROR = \"\"\"\n{0} requires the OpenCV library but it was not found in your environment. You can install it with:\n```\npip install opencv-python\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#CYTHON_IMPORT_ERROR": {
                "sorted_modules": {
                    "CYTHON_IMPORT_ERROR": "\n\nCYTHON_IMPORT_ERROR = \"\"\"\n{0} requires the Cython library but it was not found in your environment. You can install it with pip: `pip install\nCython`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#DATASETS_IMPORT_ERROR": {
                "sorted_modules": {
                    "DATASETS_IMPORT_ERROR": "\n\n# docstyle-ignore\nDATASETS_IMPORT_ERROR = \"\"\"\n{0} requires the \ud83e\udd17 Datasets library but it was not found in your environment. You can install it with:\n```\npip install datasets\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install datasets\n```\nthen restarting your kernel.\n\nNote that if you have a local folder named `datasets` or a local python file named `datasets.py` in your current\nworking directory, python may try to import this instead of the \ud83e\udd17 Datasets library. You should rename this folder or\nthat python file if that's the case. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#DECORD_IMPORT_ERROR": {
                "sorted_modules": {
                    "DECORD_IMPORT_ERROR": "\nDECORD_IMPORT_ERROR = \"\"\"\n{0} requires the PyAv library but it was not found in your environment. You can install it with:\n```\npip install decord\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#DETECTRON2_IMPORT_ERROR": {
                "sorted_modules": {
                    "DETECTRON2_IMPORT_ERROR": "\n\n# docstyle-ignore\nDETECTRON2_IMPORT_ERROR = \"\"\"\n{0} requires the detectron2 library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#ESSENTIA_IMPORT_ERROR": {
                "sorted_modules": {
                    "ESSENTIA_IMPORT_ERROR": "\n# docstyle-ignore\nESSENTIA_IMPORT_ERROR = \"\"\"\n{0} requires essentia library. But that was not found in your environment. You can install them with pip:\n`pip install essentia==2.1b6.dev1034`\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#FAISS_IMPORT_ERROR": {
                "sorted_modules": {
                    "FAISS_IMPORT_ERROR": "\n\n# docstyle-ignore\nFAISS_IMPORT_ERROR = \"\"\"\n{0} requires the faiss library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/facebookresearch/faiss/blob/master/INSTALL.md and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#FASTAPI_IMPORT_ERROR": {
                "sorted_modules": {
                    "FASTAPI_IMPORT_ERROR": "\n# docstyle-ignore\nFASTAPI_IMPORT_ERROR = \"\"\"\n{0} requires the fastapi library but it was not found in your environment. You can install it with pip:\n`pip install fastapi`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#FTFY_IMPORT_ERROR": {
                "sorted_modules": {
                    "FTFY_IMPORT_ERROR": "\n\n# docstyle-ignore\nFTFY_IMPORT_ERROR = \"\"\"\n{0} requires the ftfy library but it was not found in your environment. Check out the instructions on the\ninstallation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#G2P_EN_IMPORT_ERROR": {
                "sorted_modules": {
                    "G2P_EN_IMPORT_ERROR": "\n# docstyle-ignore\nG2P_EN_IMPORT_ERROR = \"\"\"\n{0} requires the g2p-en library but it was not found in your environment. You can install it with pip:\n`pip install g2p-en`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#JINJA_IMPORT_ERROR": {
                "sorted_modules": {
                    "JINJA_IMPORT_ERROR": "\nJINJA_IMPORT_ERROR = \"\"\"\n{0} requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\njinja2`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#LEVENSHTEIN_IMPORT_ERROR": {
                "sorted_modules": {
                    "LEVENSHTEIN_IMPORT_ERROR": "\nLEVENSHTEIN_IMPORT_ERROR = \"\"\"\n{0} requires the python-Levenshtein library but it was not found in your environment. You can install it with pip: `pip\ninstall python-Levenshtein`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#LIBROSA_IMPORT_ERROR": {
                "sorted_modules": {
                    "LIBROSA_IMPORT_ERROR": "\n# docstyle-ignore\nLIBROSA_IMPORT_ERROR = \"\"\"\n{0} requires the librosa library. But that was not found in your environment. You can install them with pip:\n`pip install librosa`\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#MISTRAL_COMMON_IMPORT_ERROR": {
                "sorted_modules": {
                    "MISTRAL_COMMON_IMPORT_ERROR": "\nMISTRAL_COMMON_IMPORT_ERROR = \"\"\"\n{0} requires the mistral-common library but it was not found in your environment. You can install it with pip: `pip install mistral-common`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#NATTEN_IMPORT_ERROR": {
                "sorted_modules": {
                    "NATTEN_IMPORT_ERROR": "\n# docstyle-ignore\nNATTEN_IMPORT_ERROR = \"\"\"\n{0} requires the natten library but it was not found in your environment. You can install it by referring to:\nshi-labs.com/natten . You can also install it with pip (may take longer to build):\n`pip install natten`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#NLTK_IMPORT_ERROR": {
                "sorted_modules": {
                    "NLTK_IMPORT_ERROR": "\n\n# docstyle-ignore\nNLTK_IMPORT_ERROR = \"\"\"\n{0} requires the NLTK library but it was not found in your environment. You can install it by referring to:\nhttps://www.nltk.org/install.html. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#OPENAI_IMPORT_ERROR": {
                "sorted_modules": {
                    "OPENAI_IMPORT_ERROR": "\n# docstyle-ignore\nOPENAI_IMPORT_ERROR = \"\"\"\n{0} requires the openai library but it was not found in your environment. You can install it with pip:\n`pip install openai`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PANDAS_IMPORT_ERROR": {
                "sorted_modules": {
                    "PANDAS_IMPORT_ERROR": "\n# docstyle-ignore\nPANDAS_IMPORT_ERROR = \"\"\"\n{0} requires the pandas library but it was not found in your environment. You can install it with pip as\nexplained here: https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html.\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PEFT_IMPORT_ERROR": {
                "sorted_modules": {
                    "PEFT_IMPORT_ERROR": "\nPEFT_IMPORT_ERROR = \"\"\"\n{0} requires the peft library but it was not found in your environment. You can install it with pip: `pip install\npeft`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PHONEMIZER_IMPORT_ERROR": {
                "sorted_modules": {
                    "PHONEMIZER_IMPORT_ERROR": "\n\n# docstyle-ignore\nPHONEMIZER_IMPORT_ERROR = \"\"\"\n{0} requires the phonemizer library but it was not found in your environment. You can install it with pip:\n`pip install phonemizer`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PRETTY_MIDI_IMPORT_ERROR": {
                "sorted_modules": {
                    "PRETTY_MIDI_IMPORT_ERROR": "\n# docstyle-ignore\nPRETTY_MIDI_IMPORT_ERROR = \"\"\"\n{0} requires the pretty_midi library. But that was not found in your environment. You can install them with pip:\n`pip install pretty_midi`\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PROTOBUF_IMPORT_ERROR": {
                "sorted_modules": {
                    "PROTOBUF_IMPORT_ERROR": "\n\n# docstyle-ignore\nPROTOBUF_IMPORT_ERROR = \"\"\"\n{0} requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PYCTCDECODE_IMPORT_ERROR": {
                "sorted_modules": {
                    "PYCTCDECODE_IMPORT_ERROR": "\n# docstyle-ignore\nPYCTCDECODE_IMPORT_ERROR = \"\"\"\n{0} requires the pyctcdecode library but it was not found in your environment. You can install it with pip:\n`pip install pyctcdecode`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PYDANTIC_IMPORT_ERROR": {
                "sorted_modules": {
                    "PYDANTIC_IMPORT_ERROR": "\n# docstyle-ignore\nPYDANTIC_IMPORT_ERROR = \"\"\"\n{0} requires the pydantic library but it was not found in your environment. You can install it with pip:\n`pip install pydantic`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PYTESSERACT_IMPORT_ERROR": {
                "sorted_modules": {
                    "PYTESSERACT_IMPORT_ERROR": "\n# docstyle-ignore\nPYTESSERACT_IMPORT_ERROR = \"\"\"\n{0} requires the PyTesseract library but it was not found in your environment. You can install it with pip:\n`pip install pytesseract`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PYTORCH_IMPORT_ERROR": {
                "sorted_modules": {
                    "PYTORCH_IMPORT_ERROR": "\n\n# docstyle-ignore\nPYTORCH_IMPORT_ERROR = \"\"\"\n{0} requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PYTORCH_QUANTIZATION_IMPORT_ERROR": {
                "sorted_modules": {
                    "PYTORCH_QUANTIZATION_IMPORT_ERROR": "\n# docstyle-ignore\nPYTORCH_QUANTIZATION_IMPORT_ERROR = \"\"\"\n{0} requires the pytorch-quantization library but it was not found in your environment. You can install it with pip:\n`pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com`\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#RICH_IMPORT_ERROR": {
                "sorted_modules": {
                    "RICH_IMPORT_ERROR": "\nRICH_IMPORT_ERROR = \"\"\"\n{0} requires the rich library but it was not found in your environment. You can install it with pip: `pip install\nrich`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#RJIEBA_IMPORT_ERROR": {
                "sorted_modules": {
                    "RJIEBA_IMPORT_ERROR": "\nRJIEBA_IMPORT_ERROR = \"\"\"\n{0} requires the rjieba library but it was not found in your environment. You can install it with pip: `pip install\nrjieba`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#SACREMOSES_IMPORT_ERROR": {
                "sorted_modules": {
                    "SACREMOSES_IMPORT_ERROR": "\n\n# docstyle-ignore\nSACREMOSES_IMPORT_ERROR = \"\"\"\n{0} requires the sacremoses library but it was not found in your environment. You can install it with pip:\n`pip install sacremoses`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#SCIPY_IMPORT_ERROR": {
                "sorted_modules": {
                    "SCIPY_IMPORT_ERROR": "\n# docstyle-ignore\nSCIPY_IMPORT_ERROR = \"\"\"\n{0} requires the scipy library but it was not found in your environment. You can install it with pip:\n`pip install scipy`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#SENTENCEPIECE_IMPORT_ERROR": {
                "sorted_modules": {
                    "SENTENCEPIECE_IMPORT_ERROR": "\n\n# docstyle-ignore\nSENTENCEPIECE_IMPORT_ERROR = \"\"\"\n{0} requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#SKLEARN_IMPORT_ERROR": {
                "sorted_modules": {
                    "SKLEARN_IMPORT_ERROR": "\n\n# docstyle-ignore\nSKLEARN_IMPORT_ERROR = \"\"\"\n{0} requires the scikit-learn library but it was not found in your environment. You can install it with:\n```\npip install -U scikit-learn\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install -U scikit-learn\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#SPEECH_IMPORT_ERROR": {
                "sorted_modules": {
                    "SPEECH_IMPORT_ERROR": "\n# docstyle-ignore\nSPEECH_IMPORT_ERROR = \"\"\"\n{0} requires the torchaudio library but it was not found in your environment. You can install it with pip:\n`pip install torchaudio`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#TIMM_IMPORT_ERROR": {
                "sorted_modules": {
                    "TIMM_IMPORT_ERROR": "\n# docstyle-ignore\nTIMM_IMPORT_ERROR = \"\"\"\n{0} requires the timm library but it was not found in your environment. You can install it with pip:\n`pip install timm`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#TOKENIZERS_IMPORT_ERROR": {
                "sorted_modules": {
                    "TOKENIZERS_IMPORT_ERROR": "\n\n# docstyle-ignore\nTOKENIZERS_IMPORT_ERROR = \"\"\"\n{0} requires the \ud83e\udd17 Tokenizers library but it was not found in your environment. You can install it with:\n```\npip install tokenizers\n```\nIn a notebook or a colab, you can install it by executing a cell with\n```\n!pip install tokenizers\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#TORCHAUDIO_IMPORT_ERROR": {
                "sorted_modules": {
                    "TORCHAUDIO_IMPORT_ERROR": "\n# docstyle-ignore\nTORCHAUDIO_IMPORT_ERROR = \"\"\"\n{0} requires the torchaudio library but it was not found in your environment. Please install it and restart your\nruntime.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#TORCHCODEC_IMPORT_ERROR": {
                "sorted_modules": {
                    "TORCHCODEC_IMPORT_ERROR": "\nTORCHCODEC_IMPORT_ERROR = \"\"\"\n{0} requires the TorchCodec (https://github.com/pytorch/torchcodec) library, but it was not found in your environment. You can install it with:\n```\npip install torchcodec\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#TORCHVISION_IMPORT_ERROR": {
                "sorted_modules": {
                    "TORCHVISION_IMPORT_ERROR": "\n\n# docstyle-ignore\nTORCHVISION_IMPORT_ERROR = \"\"\"\n{0} requires the Torchvision library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#UROMAN_IMPORT_ERROR": {
                "sorted_modules": {
                    "UROMAN_IMPORT_ERROR": "# docstyle-ignore\nUROMAN_IMPORT_ERROR = \"\"\"\n{0} requires the uroman library but it was not found in your environment. You can install it with pip:\n`pip install uroman`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#UVICORN_IMPORT_ERROR": {
                "sorted_modules": {
                    "UVICORN_IMPORT_ERROR": "\n# docstyle-ignore\nUVICORN_IMPORT_ERROR = \"\"\"\n{0} requires the uvicorn library but it was not found in your environment. You can install it with pip:\n`pip install uvicorn`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#VISION_IMPORT_ERROR": {
                "sorted_modules": {
                    "VISION_IMPORT_ERROR": "\n\n# docstyle-ignore\nVISION_IMPORT_ERROR = \"\"\"\n{0} requires the PIL library but it was not found in your environment. You can install it with pip:\n`pip install pillow`. Please note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#YT_DLP_IMPORT_ERROR": {
                "sorted_modules": {
                    "YT_DLP_IMPORT_ERROR": "\n# docstyle-ignore\nYT_DLP_IMPORT_ERROR = \"\"\"\n{0} requires the YT-DLP library but it was not found in your environment. You can install it with:\n```\npip install yt-dlp\n```\nPlease note that you may need to restart your runtime after installation.\n\"\"\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#is_accelerate_available": {
                "sorted_modules": {
                    "is_accelerate_available": "\n\n@lru_cache\ndef is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION) -> bool:\n    is_available, accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n    return is_available and version.parse(accelerate_version) >= version.parse(min_version)"
                },
                "component_dependencies": {
                    "is_accelerate_available": [
                        "transformers/utils/import_utils.py#ACCELERATE_MIN_VERSION",
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_av_available": {
                "sorted_modules": {
                    "is_av_available": "\n\n@lru_cache\ndef is_av_available() -> bool:\n    return _is_package_available(\"av\")"
                },
                "component_dependencies": {
                    "is_av_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_bs4_available": {
                "sorted_modules": {
                    "is_bs4_available": "\n\n@lru_cache\ndef is_bs4_available() -> bool:\n    return _is_package_available(\"bs4\")"
                },
                "component_dependencies": {
                    "is_bs4_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_ccl_available": {
                "sorted_modules": {
                    "is_ccl_available": "\n\n@lru_cache\ndef is_ccl_available() -> bool:\n    return _is_package_available(\"torch_ccl\") or _is_package_available(\"oneccl_bindings_for_pytorch\")"
                },
                "component_dependencies": {
                    "is_ccl_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_cv2_available": {
                "sorted_modules": {
                    "is_cv2_available": "\n\n@lru_cache\ndef is_cv2_available() -> bool:\n    return _is_package_available(\"cv2\")"
                },
                "component_dependencies": {
                    "is_cv2_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_cython_available": {
                "sorted_modules": {
                    "is_cython_available": "\n\n@lru_cache\ndef is_cython_available() -> bool:\n    return _is_package_available(\"pyximport\")"
                },
                "component_dependencies": {
                    "is_cython_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_datasets_available": {
                "sorted_modules": {
                    "is_datasets_available": "\n\n@lru_cache\ndef is_datasets_available() -> bool:\n    return _is_package_available(\"datasets\")"
                },
                "component_dependencies": {
                    "is_datasets_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_decord_available": {
                "sorted_modules": {
                    "is_decord_available": "\n\n@lru_cache\ndef is_decord_available() -> bool:\n    return _is_package_available(\"decord\")"
                },
                "component_dependencies": {
                    "is_decord_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_detectron2_available": {
                "sorted_modules": {
                    "is_detectron2_available": "\n\n@lru_cache\ndef is_detectron2_available() -> bool:\n    # We need this try/except block because otherwise after uninstalling the library, it stays available for some reason\n    # i.e. `import detectron2` and `import detectron2.modeling` still work, even though the library is uninstalled\n    # (the package exists but the objects are not reachable) - so here we explicitly try to import an object from it\n    try:\n        from detectron2.modeling import META_ARCH_REGISTRY  # noqa\n\n        return True\n    except Exception:\n        return False"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#is_essentia_available": {
                "sorted_modules": {
                    "is_essentia_available": "\n\n@lru_cache\ndef is_essentia_available() -> bool:\n    return _is_package_available(\"essentia\")"
                },
                "component_dependencies": {
                    "is_essentia_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_faiss_available": {
                "sorted_modules": {
                    "is_faiss_available": "\n\n@lru_cache\ndef is_faiss_available() -> bool:\n    return _is_package_available(\"faiss\")"
                },
                "component_dependencies": {
                    "is_faiss_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_fastapi_available": {
                "sorted_modules": {
                    "is_fastapi_available": "\n\n@lru_cache\ndef is_fastapi_available() -> bool:\n    return _is_package_available(\"fastapi\")"
                },
                "component_dependencies": {
                    "is_fastapi_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_ftfy_available": {
                "sorted_modules": {
                    "is_ftfy_available": "\n\n@lru_cache\ndef is_ftfy_available() -> bool:\n    return _is_package_available(\"ftfy\")"
                },
                "component_dependencies": {
                    "is_ftfy_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_g2p_en_available": {
                "sorted_modules": {
                    "is_g2p_en_available": "\n\n@lru_cache\ndef is_g2p_en_available() -> bool:\n    return _is_package_available(\"g2p_en\")"
                },
                "component_dependencies": {
                    "is_g2p_en_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_jinja_available": {
                "sorted_modules": {
                    "is_jinja_available": "\n\n@lru_cache\ndef is_jinja_available() -> bool:\n    return _is_package_available(\"jinja2\")"
                },
                "component_dependencies": {
                    "is_jinja_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_levenshtein_available": {
                "sorted_modules": {
                    "is_levenshtein_available": "\n\n@lru_cache\ndef is_levenshtein_available() -> bool:\n    return _is_package_available(\"Levenshtein\")"
                },
                "component_dependencies": {
                    "is_levenshtein_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_librosa_available": {
                "sorted_modules": {
                    "is_librosa_available": "\n\n@lru_cache\ndef is_librosa_available() -> bool:\n    return _is_package_available(\"librosa\")"
                },
                "component_dependencies": {
                    "is_librosa_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_mistral_common_available": {
                "sorted_modules": {
                    "is_mistral_common_available": "\n\n@lru_cache\ndef is_mistral_common_available() -> bool:\n    return _is_package_available(\"mistral_common\")"
                },
                "component_dependencies": {
                    "is_mistral_common_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_natten_available": {
                "sorted_modules": {
                    "is_natten_available": "\n\n@lru_cache\ndef is_natten_available() -> bool:\n    return _is_package_available(\"natten\")"
                },
                "component_dependencies": {
                    "is_natten_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_nltk_available": {
                "sorted_modules": {
                    "is_nltk_available": "\n\n@lru_cache\ndef is_nltk_available() -> bool:\n    return _is_package_available(\"nltk\")"
                },
                "component_dependencies": {
                    "is_nltk_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_openai_available": {
                "sorted_modules": {
                    "is_openai_available": "\n\n@lru_cache\ndef is_openai_available() -> bool:\n    return _is_package_available(\"openai\")"
                },
                "component_dependencies": {
                    "is_openai_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_pandas_available": {
                "sorted_modules": {
                    "is_pandas_available": "\n\n@lru_cache\ndef is_pandas_available() -> bool:\n    return _is_package_available(\"pandas\")"
                },
                "component_dependencies": {
                    "is_pandas_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_peft_available": {
                "sorted_modules": {
                    "is_peft_available": "\n\n@lru_cache\ndef is_peft_available() -> bool:\n    return _is_package_available(\"peft\")"
                },
                "component_dependencies": {
                    "is_peft_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_phonemizer_available": {
                "sorted_modules": {
                    "is_phonemizer_available": "\n\n@lru_cache\ndef is_phonemizer_available() -> bool:\n    return _is_package_available(\"phonemizer\")"
                },
                "component_dependencies": {
                    "is_phonemizer_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_pretty_midi_available": {
                "sorted_modules": {
                    "is_pretty_midi_available": "\n\n@lru_cache\ndef is_pretty_midi_available() -> bool:\n    return _is_package_available(\"pretty_midi\")"
                },
                "component_dependencies": {
                    "is_pretty_midi_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_protobuf_available": {
                "sorted_modules": {
                    "is_protobuf_available": "\n\n@lru_cache\ndef is_protobuf_available() -> bool:\n    return _is_package_available(\"google\") and _is_package_available(\"google.protobuf\")"
                },
                "component_dependencies": {
                    "is_protobuf_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_pyctcdecode_available": {
                "sorted_modules": {
                    "is_pyctcdecode_available": "\n\n@lru_cache\ndef is_pyctcdecode_available() -> bool:\n    return _is_package_available(\"pyctcdecode\")"
                },
                "component_dependencies": {
                    "is_pyctcdecode_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_pydantic_available": {
                "sorted_modules": {
                    "is_pydantic_available": "\n\n@lru_cache\ndef is_pydantic_available() -> bool:\n    return _is_package_available(\"pydantic\")"
                },
                "component_dependencies": {
                    "is_pydantic_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_pytesseract_available": {
                "sorted_modules": {
                    "is_pytesseract_available": "\n\n@lru_cache\ndef is_pytesseract_available() -> bool:\n    return _is_package_available(\"pytesseract\")"
                },
                "component_dependencies": {
                    "is_pytesseract_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_pytorch_quantization_available": {
                "sorted_modules": {
                    "is_pytorch_quantization_available": "\n\n@lru_cache\ndef is_pytorch_quantization_available() -> bool:\n    return _is_package_available(\"pytorch_quantization\")"
                },
                "component_dependencies": {
                    "is_pytorch_quantization_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_rich_available": {
                "sorted_modules": {
                    "is_rich_available": "\n\n@lru_cache\ndef is_rich_available() -> bool:\n    return _is_package_available(\"rich\")"
                },
                "component_dependencies": {
                    "is_rich_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_rjieba_available": {
                "sorted_modules": {
                    "is_rjieba_available": "\n\n@lru_cache\ndef is_rjieba_available() -> bool:\n    return _is_package_available(\"rjieba\")"
                },
                "component_dependencies": {
                    "is_rjieba_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_sacremoses_available": {
                "sorted_modules": {
                    "is_sacremoses_available": "\n\n@lru_cache\ndef is_sacremoses_available() -> bool:\n    return _is_package_available(\"sacremoses\")"
                },
                "component_dependencies": {
                    "is_sacremoses_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_scipy_available": {
                "sorted_modules": {
                    "is_scipy_available": "\n\n@lru_cache\ndef is_scipy_available() -> bool:\n    return _is_package_available(\"scipy\")"
                },
                "component_dependencies": {
                    "is_scipy_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_sentencepiece_available": {
                "sorted_modules": {
                    "is_sentencepiece_available": "\n\n@lru_cache\ndef is_sentencepiece_available() -> bool:\n    return _is_package_available(\"sentencepiece\")"
                },
                "component_dependencies": {
                    "is_sentencepiece_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_sklearn_available": {
                "sorted_modules": {
                    "is_sklearn_available": "\n\n@lru_cache\ndef is_sklearn_available() -> bool:\n    return _is_package_available(\"sklearn\")"
                },
                "component_dependencies": {
                    "is_sklearn_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_speech_available": {
                "sorted_modules": {
                    "is_speech_available": "\n\n@lru_cache\ndef is_speech_available() -> bool:\n    # For now this depends on torchaudio but the exact dependency might evolve in the future.\n    return is_torchaudio_available()"
                },
                "component_dependencies": {
                    "is_speech_available": [
                        "transformers/utils/import_utils.py#is_torchaudio_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_timm_available": {
                "sorted_modules": {
                    "is_timm_available": "\n\n@lru_cache\ndef is_timm_available() -> bool:\n    return _is_package_available(\"timm\")"
                },
                "component_dependencies": {
                    "is_timm_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_tokenizers_available": {
                "sorted_modules": {
                    "is_tokenizers_available": "\n\n@lru_cache\ndef is_tokenizers_available() -> bool:\n    return _is_package_available(\"tokenizers\")"
                },
                "component_dependencies": {
                    "is_tokenizers_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_torch_available": {
                "sorted_modules": {
                    "is_torch_available": "\n\n@lru_cache\ndef is_torch_available() -> bool:\n    is_available, torch_version = _is_package_available(\"torch\", return_version=True)\n    if is_available and version.parse(torch_version) < version.parse(\"2.2.0\"):\n        logger.warning_once(f\"Disabling PyTorch because PyTorch >= 2.2 is required but found {torch_version}\")\n    return is_available and version.parse(torch_version) >= version.parse(\"2.2.0\")"
                },
                "component_dependencies": {
                    "is_torch_available": [
                        "transformers/utils/import_utils.py#_is_package_available",
                        "transformers/utils/import_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_torchaudio_available": {
                "sorted_modules": {
                    "is_torchaudio_available": "\n\n@lru_cache\ndef is_torchaudio_available() -> bool:\n    return _is_package_available(\"torchaudio\")"
                },
                "component_dependencies": {
                    "is_torchaudio_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_torchcodec_available": {
                "sorted_modules": {
                    "is_torchcodec_available": "\n\n@lru_cache\ndef is_torchcodec_available() -> bool:\n    return _is_package_available(\"torchcodec\")"
                },
                "component_dependencies": {
                    "is_torchcodec_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_torchvision_available": {
                "sorted_modules": {
                    "is_torchvision_available": "\n\n@lru_cache\ndef is_torchvision_available() -> bool:\n    return _is_package_available(\"torchvision\")"
                },
                "component_dependencies": {
                    "is_torchvision_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_uroman_available": {
                "sorted_modules": {
                    "is_uroman_available": "\n\n@lru_cache\ndef is_uroman_available() -> bool:\n    return _is_package_available(\"uroman\")"
                },
                "component_dependencies": {
                    "is_uroman_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_uvicorn_available": {
                "sorted_modules": {
                    "is_uvicorn_available": "\n\n@lru_cache\ndef is_uvicorn_available() -> bool:\n    return _is_package_available(\"uvicorn\")"
                },
                "component_dependencies": {
                    "is_uvicorn_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_vision_available": {
                "sorted_modules": {
                    "is_vision_available": "\n\n@lru_cache\ndef is_vision_available() -> bool:\n    return _is_package_available(\"PIL\")"
                },
                "component_dependencies": {
                    "is_vision_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#is_yt_dlp_available": {
                "sorted_modules": {
                    "is_yt_dlp_available": "\n\n@lru_cache\ndef is_yt_dlp_available() -> bool:\n    return _is_package_available(\"yt_dlp\")"
                },
                "component_dependencies": {
                    "is_yt_dlp_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#VersionComparison": {
                "sorted_modules": {
                    "VersionComparison": "\n\nclass VersionComparison(Enum):\n    EQUAL = operator.eq\n    NOT_EQUAL = operator.ne\n    GREATER_THAN = operator.gt\n    LESS_THAN = operator.lt\n    GREATER_THAN_OR_EQUAL = operator.ge\n    LESS_THAN_OR_EQUAL = operator.le\n\n    @staticmethod\n    def from_string(version_string: str) -> \"VersionComparison\":\n        string_to_operator = {\n            \"=\": VersionComparison.EQUAL.value,\n            \"==\": VersionComparison.EQUAL.value,\n            \"!=\": VersionComparison.NOT_EQUAL.value,\n            \">\": VersionComparison.GREATER_THAN.value,\n            \"<\": VersionComparison.LESS_THAN.value,\n            \">=\": VersionComparison.GREATER_THAN_OR_EQUAL.value,\n            \"<=\": VersionComparison.LESS_THAN_OR_EQUAL.value,\n        }\n\n        return string_to_operator[version_string]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#_is_package_available": {
                "sorted_modules": {
                    "_is_package_available": "\n\ndef _is_package_available(pkg_name: str, return_version: bool = False) -> tuple[bool, str] | bool:\n    \"\"\"Check if `pkg_name` exist, and optionally try to get its version\"\"\"\n    spec = importlib.util.find_spec(pkg_name)\n    # the spec might be not None but not importable\n    package_exists = spec is not None and spec.loader is not None\n    package_version = \"N/A\"\n    if package_exists and return_version:\n        try:\n            # importlib.metadata works with the distribution package, which may be different from the import\n            # name (e.g. `PIL` is the import name, but `pillow` is the distribution name)\n            distributions = PACKAGE_DISTRIBUTION_MAPPING[pkg_name]\n            # In most cases, the packages are well-behaved and both have the same name. If it's not the case, we\n            # pick the first item of the list as best guess (it's almost always a list of length 1 anyway)\n            distribution_name = pkg_name if pkg_name in distributions else distributions[0]\n            package_version = importlib.metadata.version(distribution_name)\n        except importlib.metadata.PackageNotFoundError:\n            # If we cannot find the metadata (because of editable install for example), try to import directly.\n            # Note that this branch will almost never be run, so we do not import packages for nothing here\n            package = importlib.import_module(pkg_name)\n            package_version = getattr(package, \"__version__\", \"N/A\")\n        logger.debug(f\"Detected {pkg_name} version: {package_version}\")\n    if return_version:\n        return package_exists, package_version\n    else:\n        return package_exists"
                },
                "component_dependencies": {
                    "_is_package_available": [
                        "transformers/utils/import_utils.py#PACKAGE_DISTRIBUTION_MAPPING",
                        "transformers/utils/import_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#split_package_version": {
                "sorted_modules": {
                    "split_package_version": "\n\n@lru_cache\ndef split_package_version(package_version_str) -> tuple[str, str, str]:\n    pattern = r\"([a-zA-Z0-9_-]+)([!<>=~]+)([0-9.]+)\"\n    match = re.match(pattern, package_version_str)\n    if match:\n        return (match.group(1), match.group(2), match.group(3))\n    else:\n        raise ValueError(f\"Invalid package version string: {package_version_str}\")"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#BloomTensorProcessor": {
                "sorted_modules": {
                    "BloomTensorProcessor": "\n\nclass BloomTensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        if \"attn_qkv\" in name:\n            num_heads = self.config[\"n_head\"]\n            n_embed = self.config[\"hidden_size\"]\n            if \"weight\" in name:\n                weights = self._reverse_reshape_weights(weights, num_heads, n_embed)\n            else:\n                weights = self._reverse_reshape_bias(weights, num_heads, n_embed)\n        return GGUFTensor(weights, name, {})\n\n    def _reverse_reshape_weights(self, weights: np.ndarray, n_head: int, n_embed: int):\n        # Original reshape implementation\n        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L972-L985\n        q, k, v = np.array_split(weights, 3, axis=0)\n\n        q = q.reshape(n_head, n_embed // n_head, n_embed)\n        k = k.reshape(n_head, n_embed // n_head, n_embed)\n        v = v.reshape(n_head, n_embed // n_head, n_embed)\n        qkv_weights = np.stack([q, k, v], axis=1)\n\n        return qkv_weights.reshape(n_head * 3 * (n_embed // n_head), n_embed)\n\n    def _reverse_reshape_bias(self, weights: np.ndarray, n_head: int, n_embed: int):\n        # Original reshape implementation\n        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L986-L998\n        q_bias, k_bias, v_bias = np.array_split(weights, 3)\n\n        q_bias = q_bias.reshape(n_head, n_embed // n_head)\n        k_bias = k_bias.reshape(n_head, n_embed // n_head)\n        v_bias = v_bias.reshape(n_head, n_embed // n_head)\n\n        qkv_bias = np.stack([q_bias, k_bias, v_bias], axis=1).flatten()\n        return qkv_bias"
                },
                "component_dependencies": {
                    "BloomTensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#GPT2TensorProcessor": {
                "sorted_modules": {
                    "GPT2TensorProcessor": "\n\nclass GPT2TensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        # Original transpose implementation\n        # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L2060-L2061\n        if (\n            \"attn_qkv.weight\" in name\n            or \"ffn_down.weight\" in name\n            or \"ffn_up.weight\" in name\n            or \"attn_output.weight\" in name\n        ):\n            weights = weights.T\n\n        # Handle special case for output.weight\n        if name == \"output.weight\":\n            # output.weight has conflicts with attn_output.weight in name checking\n            # Store the tensor directly and signal to skip further processing\n            name = \"lm_head.weight\"\n            parsed_parameters = kwargs.get(\"parsed_parameters\", {})\n            parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n            name = None  # Signal to skip further processing\n        return GGUFTensor(weights, name, {})"
                },
                "component_dependencies": {
                    "GPT2TensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#Gemma2TensorProcessor": {
                "sorted_modules": {
                    "Gemma2TensorProcessor": "\n\nclass Gemma2TensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    # ref: https://github.com/ggerganov/llama.cpp/blob/d79d8f39b4da6deca4aea8bf130c6034c482b320/convert_hf_to_gguf.py#L3191\n    # ref: https://github.com/huggingface/transformers/blob/fc37f38915372c15992b540dfcbbe00a916d4fc6/src/transformers/models/gemma/modeling_gemma.py#L89\n    def process(self, weights, name, **kwargs):\n        if \"norm.weight\" in name:\n            weights = weights - 1\n        return GGUFTensor(weights, name, {})"
                },
                "component_dependencies": {
                    "Gemma2TensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#Lfm2TensorProcessor": {
                "sorted_modules": {
                    "Lfm2TensorProcessor": "\n\nclass Lfm2TensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        if \"shortconv.conv.weight\" in name:\n            ## GGUF shape is [hidden_dim, L_cache], HF expects [hidden_dim, 1, L_cache]\n            weights = np.expand_dims(weights, axis=1)  ## equivalent to unsqueeze(1)\n        return GGUFTensor(weights, name, {})"
                },
                "component_dependencies": {
                    "Lfm2TensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#LlamaTensorProcessor": {
                "sorted_modules": {
                    "LlamaTensorProcessor": "\n\nclass LlamaTensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        if \".attn_k.\" in name or \".attn_q.\" in name:\n            num_heads = self.config.get(\"num_attention_heads\")\n            num_kv_heads = self.config.get(\"num_key_value_heads\")\n\n            if None in (num_heads, num_kv_heads):\n                return GGUFTensor(weights, name, {})\n            if \".attn_q.\" in name:\n                weights = self._reverse_permute_weights(weights, num_heads, num_heads)\n            elif \".attn_k.\" in name:\n                weights = self._reverse_permute_weights(weights, num_heads, num_kv_heads)\n        return GGUFTensor(weights, name, {})\n\n    def _reverse_permute_weights(\n        self, weights: np.ndarray, n_head: int, num_kv_heads: Optional[int] = None\n    ) -> np.ndarray:\n        # Original permutation implementation\n        # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L1402-L1408\n        if num_kv_heads is not None and n_head != num_kv_heads:\n            n_head = num_kv_heads\n\n        dim = weights.shape[0] // n_head // 2\n        w = weights.reshape(n_head, dim, 2, *weights.shape[1:])\n        return w.swapaxes(2, 1).reshape(weights.shape)"
                },
                "component_dependencies": {
                    "LlamaTensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#MambaTensorProcessor": {
                "sorted_modules": {
                    "MambaTensorProcessor": "\n\nclass MambaTensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        if \"ssm_conv1d.weight\" in name:\n            # for compatibility tensor ssm_conv1d must be (5120, 1, 4]) dim,\n            # quantized one is (5120, 4)\n            weights = np.expand_dims(weights, axis=1)\n        if \"ssm_a\" in name:\n            # Original exponential implementation\n            # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L2975-L2977\n            weights = np.log(-weights)\n        return GGUFTensor(weights, name, {})"
                },
                "component_dependencies": {
                    "MambaTensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#NemotronTensorProcessor": {
                "sorted_modules": {
                    "NemotronTensorProcessor": "\n\nclass NemotronTensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    # ref : https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L4666\n    def process(self, weights, name, **kwargs):\n        if \"norm.weight\" in name:\n            weights = weights - 1\n        return GGUFTensor(weights, name, {})"
                },
                "component_dependencies": {
                    "NemotronTensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#Qwen2MoeTensorProcessor": {
                "sorted_modules": {
                    "Qwen2MoeTensorProcessor": "\n\nclass Qwen2MoeTensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        if \"_exp\" in name:\n            tensor_key_mapping = kwargs.get(\"tensor_key_mapping\")\n            parsed_parameters = kwargs.get(\"parsed_parameters\")\n            if tensor_key_mapping:\n                self._split_moe_expert_tensor(weights, parsed_parameters, name, tensor_key_mapping)\n                return GGUFTensor(weights, None, {})\n        if \"ffn_gate_inp_shexp\" in name:\n            # for compatibility tensor shared_expert_gate must be (1, 2048) dim,\n            # quantized one is (2048)\n            weights = np.expand_dims(weights, axis=0)\n        return GGUFTensor(weights, name, {})\n\n    def _split_moe_expert_tensor(\n        self, weights: np.ndarray, parsed_parameters: dict[str, dict], name: str, tensor_key_mapping: dict\n    ):\n        # Original merge implementation\n        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L1994-L2022\n        name = tensor_key_mapping[name]\n        w_counter = self.config.get(\"num_experts\", 60)\n        for i in range(0, w_counter):\n            temp_name = name.replace(\"mlp.experts.\", f\"mlp.experts.{i}.\")\n            exp_weight = weights[i]\n            parsed_parameters[\"tensors\"][temp_name] = torch.from_numpy(np.copy(exp_weight))"
                },
                "component_dependencies": {
                    "Qwen2MoeTensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#T5TensorProcessor": {
                "sorted_modules": {
                    "T5TensorProcessor": "\n\nclass T5TensorProcessor(TensorProcessor):\n    def __init__(self, config=None):\n        super().__init__(config=config)\n\n    def process(self, weights, name, **kwargs):\n        bid = None\n        for chunk in name.split(\".\"):\n            if chunk.isdigit():\n                bid = int(chunk)\n                break\n        return GGUFTensor(weights, name, {\"bid\": bid})"
                },
                "component_dependencies": {
                    "T5TensorProcessor": [
                        "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                        "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor": {
                "sorted_modules": {
                    "GGUFTensor": "\n\nclass GGUFTensor(NamedTuple):\n    weights: np.ndarray\n    name: str\n    metadata: dict"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#GGUF_MIN_VERSION": {
                "sorted_modules": {
                    "GGUF_MIN_VERSION": "GGUF_MIN_VERSION = \"0.10.0\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_rope_utils.py#_check_received_keys": {
                "sorted_modules": {
                    "_check_received_keys": "\n\ndef _check_received_keys(\n    rope_type: str,\n    received_keys: set,\n    required_keys: set,\n    optional_keys: Optional[set] = None,\n    ignore_keys: Optional[set] = None,\n):\n    \"\"\"Compare the received keys in `config.rope_parameters` against the expected and optional keys\"\"\"\n    # BC: \"rope_type\" was originally \"type\" -- let's check for \"rope_type\" when \"type\" is present\n    if \"type\" in received_keys:\n        received_keys -= {\"type\"}\n        required_keys.add(\"rope_type\")\n\n    # Some models need to store model-specific keys, and we don't want to throw warning at them\n    if ignore_keys is not None:\n        received_keys -= ignore_keys\n\n    missing_keys = required_keys - received_keys\n    if missing_keys:\n        raise KeyError(f\"Missing required keys in `rope_parameters` for 'rope_type'='{rope_type}': {missing_keys}\")\n\n    if optional_keys is not None:\n        unused_keys = received_keys - required_keys - optional_keys\n    else:\n        unused_keys = received_keys - required_keys\n    if unused_keys:\n        logger.warning(f\"Unrecognized keys in `rope_parameters` for 'rope_type'='{rope_type}': {unused_keys}\")"
                },
                "component_dependencies": {
                    "_check_received_keys": [
                        "transformers/modeling_rope_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#flash_attn_supports_top_left_mask": {
                "sorted_modules": {
                    "flash_attn_supports_top_left_mask": "\n\n# TODO Deprecate when all models have the attention interface\ndef flash_attn_supports_top_left_mask():\n    if is_flash_attn_3_available():\n        return False\n    if is_flash_attn_2_available():\n        return not is_flash_attn_greater_or_equal_2_10()\n\n    from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n\n    return is_npu_fa2_top_left_aligned_causal_mask()"
                },
                "component_dependencies": {
                    "flash_attn_supports_top_left_mask": [
                        "transformers/utils.py#is_flash_attn_2_available",
                        "transformers/utils.py#is_flash_attn_3_available",
                        "transformers/utils.py#is_flash_attn_greater_or_equal_2_10"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_is_packed_sequence": {
                "sorted_modules": {
                    "_is_packed_sequence": "\n\ndef _is_packed_sequence(position_ids, batch_size):\n    \"\"\"\n    Check the position ids whether packed sequences are indicated or not\n        1. Position ids exist\n        2. Flattened sequences only are supported\n        3. Compile-friendly `not (torch.diff(position_ids, dim=-1) >= 0).all()`, i.e. we have multiple increasing sequences\n    \"\"\"\n    if position_ids is None:\n        return False\n\n    increasing_position_sequences = (\n        torch.arange(position_ids.shape[1], device=position_ids.device) + position_ids.min()\n    )\n    return batch_size == 1 and (increasing_position_sequences - position_ids).abs().sum().bool()"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_prepare_from_posids": {
                "sorted_modules": {
                    "_prepare_from_posids": "\n\ndef _prepare_from_posids(query, key, value, position_ids):\n    \"\"\"\n    This function returns necessary arguments to call `flash_attn_varlen_func`.\n    All three query, key, value states will be flattened.\n    Cumulative lengths of each examples in the batch will be extracted from position_ids.\n    NOTE: ideally cumulative lengths should be prepared at the data collator stage\n\n    Arguments:\n        query (`torch.Tensor`):\n            Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n        key (`torch.Tensor`):\n            Key state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n        value (`torch.Tensor`):\n            Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n        position_ids (`torch.Tensor`):\n            Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n\n    Return:\n        query (`torch.Tensor`):\n            Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n        key (`torch.Tensor`):\n            Key state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n        value (`torch.Tensor`):\n            Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n            The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n    \"\"\"\n    query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n    key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n    value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n\n    (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = prepare_fa_kwargs_from_position_ids(position_ids)\n\n    return (query, key, value, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k))"
                },
                "component_dependencies": {
                    "_prepare_from_posids": [
                        "transformers/modeling_flash_attention_utils.py#prepare_fa_kwargs_from_position_ids"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_upad_input": {
                "sorted_modules": {
                    "_upad_input": "\n\ndef _upad_input(\n    query_layer: torch.Tensor,\n    key_layer: torch.Tensor,\n    value_layer: torch.Tensor,\n    attention_mask: torch.Tensor,\n    query_length: int,\n    unpad_input_func,\n):\n    \"\"\"\n    Unpads query, key, and values tensors, using a single dimension for all tokens even though they belong to different batches.\n    This function is used instead of `flash_attn.bert_padding.unpad_input` in order to avoid the recomputation of the same intermediary\n    tensors for query, key, value tensors.\n\n    Arguments:\n        query_layer (`torch.Tensor`):\n            Query state with padding. Shape: (batch_size, query_length, num_heads, head_dim).\n        key_layer (`torch.Tensor`):\n            Key state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n        value_layer (`torch.Tensor`):\n            Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n        attention_mask (`torch.Tensor`):\n            Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n        query_length (`int`):\n            Target length.\n        unpad_input_func:\n            The function to use for unpadding the input tensors.\n\n    Return:\n        query_layer (`torch.Tensor`):\n            Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n        key_layer (`torch.Tensor`):\n            Key state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n        value_layer (`torch.Tensor`):\n            Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n        indices_q (`torch.Tensor`):\n            The indices of non-masked tokens from the flattened input target sequence.\n        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n            The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n    \"\"\"\n    indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n\n    # With static caches, the k/v states may be larger than the mask -> we need to slice them to avoid generating garbage\n    # It's a bit of an anti-pattern, but otherwise we silently compute wrong attentions scores\n    if key_layer.shape[1] > (seq_len := attention_mask.shape[-1]):\n        key_layer, value_layer = key_layer[:, :seq_len, :, :], value_layer[:, :seq_len, :, :]\n\n    batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n    key_layer = _index_first_axis(key_layer, indices_k)\n    value_layer = _index_first_axis(value_layer, indices_k)\n    if query_length == kv_seq_len:\n        query_layer = _index_first_axis(query_layer, indices_k)\n        cu_seqlens_q = cu_seqlens_k\n        max_seqlen_in_batch_q = max_seqlen_in_batch_k\n        indices_q = indices_k\n    elif query_length == 1:\n        max_seqlen_in_batch_q = 1\n        cu_seqlens_q = torch.arange(\n            batch_size + 1, dtype=torch.int32, device=query_layer.device\n        )  # There is a memcpy here, that is very bad.\n        indices_q = cu_seqlens_q[:-1]\n        query_layer = query_layer.squeeze(1)\n    else:\n        # The -q_len: slice assumes left padding.\n        attention_mask = attention_mask[:, -query_length:]\n        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q, *_ = unpad_input_func(query_layer, attention_mask)\n\n    return (\n        query_layer,\n        key_layer,\n        value_layer,\n        indices_q,\n        (cu_seqlens_q, cu_seqlens_k),\n        (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n    )"
                },
                "component_dependencies": {
                    "_upad_input": [
                        "transformers/modeling_flash_attention_utils.py#_get_unpad_data",
                        "transformers/modeling_flash_attention_utils.py#_index_first_axis"
                    ]
                },
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#fa_peft_integration_check": {
                "sorted_modules": {
                    "fa_peft_integration_check": "\n\ndef fa_peft_integration_check(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    target_dtype: Optional[torch.dtype] = None,\n):\n    \"\"\"\n    PEFT usually casts the layer norms in float32 for training stability reasons\n    therefore the input hidden states gets silently casted in float32. Hence, we need\n    cast them back in float16 / bfloat16 just to be sure everything works as expected.\n    This might slowdown training & inference so it is recommended to not cast the LayerNorms!\n    \"\"\"\n    if target_dtype and q.dtype == torch.float32:\n        logger.warning_once(f\"Casting fp32 inputs back to {target_dtype} for flash-attn compatibility.\")\n        q, k, v = q.to(target_dtype), k.to(target_dtype), v.to(target_dtype)\n    return q, k, v"
                },
                "component_dependencies": {
                    "fa_peft_integration_check": [
                        "transformers/modeling_flash_attention_utils.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/image_transforms.py#center_to_corners_format": {
                "sorted_modules": {
                    "center_to_corners_format": "\n\n# 2 functions below inspired by https://github.com/facebookresearch/detr/blob/master/util/box_ops.py\ndef center_to_corners_format(bboxes_center: TensorType) -> TensorType:\n    \"\"\"\n    Converts bounding boxes from center format to corners format.\n\n    center format: contains the coordinate for the center of the box and its width, height dimensions\n        (center_x, center_y, width, height)\n    corners format: contains the coordinates for the top-left and bottom-right corners of the box\n        (top_left_x, top_left_y, bottom_right_x, bottom_right_y)\n    \"\"\"\n    # Function is used during model forward pass, so we use torch if relevant, without converting to numpy\n    if is_torch_tensor(bboxes_center):\n        return _center_to_corners_format_torch(bboxes_center)\n    elif isinstance(bboxes_center, np.ndarray):\n        return _center_to_corners_format_numpy(bboxes_center)\n\n    raise ValueError(f\"Unsupported input type {type(bboxes_center)}\")"
                },
                "component_dependencies": {
                    "center_to_corners_format": [
                        "transformers/image_transforms.py#_center_to_corners_format_numpy",
                        "transformers/image_transforms.py#_center_to_corners_format_torch",
                        "transformers/utils.py#TensorType",
                        "transformers/utils.py#is_torch_tensor"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#bbox2distance": {
                "sorted_modules": {
                    "bbox2distance": "\n\ndef bbox2distance(points, bbox, max_num_bins, reg_scale, up, eps=0.1):\n    \"\"\"\n    Converts bounding box coordinates to distances from a reference point.\n\n    Args:\n        points (Tensor): (n, 4) [x, y, w, h], where (x, y) is the center.\n        bbox (Tensor): (n, 4) bounding boxes in \"xyxy\" format.\n        max_num_bins (float): Maximum bin value.\n        reg_scale (float): Controlling curvarture of W(n).\n        up (Tensor): Controlling upper bounds of W(n).\n        eps (float): Small value to ensure target < max_num_bins.\n\n    Returns:\n        Tensor: Decoded distances.\n    \"\"\"\n\n    reg_scale = abs(reg_scale)\n    left = (points[:, 0] - bbox[:, 0]) / (points[..., 2] / reg_scale + 1e-16) - 0.5 * reg_scale\n    top = (points[:, 1] - bbox[:, 1]) / (points[..., 3] / reg_scale + 1e-16) - 0.5 * reg_scale\n    right = (bbox[:, 2] - points[:, 0]) / (points[..., 2] / reg_scale + 1e-16) - 0.5 * reg_scale\n    bottom = (bbox[:, 3] - points[:, 1]) / (points[..., 3] / reg_scale + 1e-16) - 0.5 * reg_scale\n    four_lens = torch.stack([left, top, right, bottom], -1)\n    four_lens, weight_right, weight_left = translate_gt(four_lens, max_num_bins, reg_scale, up)\n    if max_num_bins is not None:\n        four_lens = four_lens.clamp(min=0, max=max_num_bins - eps)\n    return four_lens.reshape(-1).detach(), weight_right.detach(), weight_left.detach()"
                },
                "component_dependencies": {
                    "bbox2distance": [
                        "transformers/loss/loss_d_fine.py#translate_gt"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#box_iou": {
                "sorted_modules": {
                    "box_iou": "\n\n# modified from torchvision to also return the union\ndef box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n\n    left_top = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n    right_bottom = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n\n    width_height = (right_bottom - left_top).clamp(min=0)  # [N,M,2]\n    inter = width_height[:, :, 0] * width_height[:, :, 1]  # [N,M]\n\n    union = area1[:, None] + area2 - inter\n\n    iou = inter / union\n    return iou, union"
                },
                "component_dependencies": {
                    "box_iou": [
                        "transformers/loss/loss_for_object_detection.py#box_area"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher": {
                "sorted_modules": {
                    "RTDetrHungarianMatcher": "\n\nclass RTDetrHungarianMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n\n    For efficiency reasons, the targets don't include the no_object. Because of this, in general, there are more\n    predictions than targets. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n    un-matched (and thus treated as non-objects).\n\n    Args:\n        config: RTDetrConfig\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        requires_backends(self, [\"scipy\"])\n\n        self.class_cost = config.matcher_class_cost\n        self.bbox_cost = config.matcher_bbox_cost\n        self.giou_cost = config.matcher_giou_cost\n\n        self.use_focal_loss = config.use_focal_loss\n        self.alpha = config.matcher_alpha\n        self.gamma = config.matcher_gamma\n\n        if self.class_cost == self.bbox_cost == self.giou_cost == 0:\n            raise ValueError(\"All costs of the Matcher can't be 0\")\n\n    @torch.no_grad()\n    def forward(self, outputs, targets):\n        \"\"\"Performs the matching\n\n        Params:\n            outputs: This is a dict that contains at least these entries:\n                 \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n\n            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n                 \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n                           objects in the target) containing the class labels\n                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n\n        Returns:\n            A list of size batch_size, containing tuples of (index_i, index_j) where:\n                - index_i is the indices of the selected predictions (in order)\n                - index_j is the indices of the corresponding selected targets (in order)\n            For each batch element, it holds:\n                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n        \"\"\"\n        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n\n        # We flatten to compute the cost matrices in a batch\n        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n        # Also concat the target labels and boxes\n        target_ids = torch.cat([v[\"class_labels\"] for v in targets])\n        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n        # but approximate it in 1 - proba[target class].\n        # The 1 is a constant that doesn't change the matching, it can be omitted.\n        if self.use_focal_loss:\n            out_prob = F.sigmoid(outputs[\"logits\"].flatten(0, 1))\n            out_prob = out_prob[:, target_ids]\n            neg_cost_class = (1 - self.alpha) * (out_prob**self.gamma) * (-(1 - out_prob + 1e-8).log())\n            pos_cost_class = self.alpha * ((1 - out_prob) ** self.gamma) * (-(out_prob + 1e-8).log())\n            class_cost = pos_cost_class - neg_cost_class\n        else:\n            out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n            class_cost = -out_prob[:, target_ids]\n\n        # Compute the L1 cost between boxes\n        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n        # Compute the giou cost between boxes\n        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n        # Compute the final cost matrix\n        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n\n        sizes = [len(v[\"boxes\"]) for v in targets]\n        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n\n        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]"
                },
                "component_dependencies": {
                    "RTDetrHungarianMatcher": [
                        "transformers/image_transforms.py#center_to_corners_format",
                        "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                        "transformers/utils.py#requires_backends"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#generalized_box_iou": {
                "sorted_modules": {
                    "generalized_box_iou": "\n\ndef generalized_box_iou(boxes1, boxes2):\n    \"\"\"\n    Generalized IoU from https://giou.stanford.edu/. The boxes should be in [x0, y0, x1, y1] (corner) format.\n\n    Returns:\n        `torch.FloatTensor`: a [N, M] pairwise matrix, where N = len(boxes1) and M = len(boxes2)\n    \"\"\"\n    # degenerate boxes gives inf / nan results\n    # so do an early check\n    if not (boxes1[:, 2:] >= boxes1[:, :2]).all():\n        raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {boxes1}\")\n    if not (boxes2[:, 2:] >= boxes2[:, :2]).all():\n        raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {boxes2}\")\n    iou, union = box_iou(boxes1, boxes2)\n\n    top_left = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n    bottom_right = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n\n    width_height = (bottom_right - top_left).clamp(min=0)  # [N,M,2]\n    area = width_height[:, :, 0] * width_height[:, :, 1]\n\n    return iou - (area - union) / area"
                },
                "component_dependencies": {
                    "generalized_box_iou": [
                        "transformers/loss/loss_for_object_detection.py#box_iou"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss": {
                "sorted_modules": {
                    "sigmoid_focal_loss": "\n\ndef sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://huggingface.co/papers/1708.02002.\n\n    Args:\n        inputs (`torch.FloatTensor` of arbitrary shape):\n            The predictions for each example.\n        targets (`torch.FloatTensor` with the same shape as `inputs`)\n            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n            and 1 for the positive class).\n        alpha (`float`, *optional*, defaults to `0.25`):\n            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n        gamma (`int`, *optional*, defaults to `2`):\n            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n\n    Returns:\n        Loss tensor\n    \"\"\"\n    prob = inputs.sigmoid()\n    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n    # add modulating factor\n    p_t = prob * targets + (1 - prob) * (1 - targets)\n    loss = ce_loss * ((1 - p_t) ** gamma)\n\n    if alpha >= 0:\n        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n        loss = alpha_t * loss\n\n    return loss.mean(1).sum() / num_boxes"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#dice_loss": {
                "sorted_modules": {
                    "dice_loss": "\n\ndef dice_loss(inputs, targets, num_boxes):\n    \"\"\"\n    Compute the DICE loss, similar to generalized IOU for masks\n\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs (0 for the negative class and 1 for the positive\n                 class).\n    \"\"\"\n    inputs = inputs.sigmoid()\n    inputs = inputs.flatten(1)\n    numerator = 2 * (inputs * targets).sum(1)\n    denominator = inputs.sum(-1) + targets.sum(-1)\n    loss = 1 - (numerator + 1) / (denominator + 1)\n    return loss.sum() / num_boxes"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list": {
                "sorted_modules": {
                    "nested_tensor_from_tensor_list": "\n\ndef nested_tensor_from_tensor_list(tensor_list: list[Tensor]):\n    if tensor_list[0].ndim == 3:\n        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n        batch_shape = [len(tensor_list)] + max_size\n        batch_size, num_channels, height, width = batch_shape\n        dtype = tensor_list[0].dtype\n        device = tensor_list[0].device\n        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n        mask = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n        for img, pad_img, m in zip(tensor_list, tensor, mask):\n            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n            m[: img.shape[1], : img.shape[2]] = False\n    else:\n        raise ValueError(\"Only 3-dimensional tensors are supported\")\n    return NestedTensor(tensor, mask)"
                },
                "component_dependencies": {
                    "nested_tensor_from_tensor_list": [
                        "transformers/loss/loss_for_object_detection.py#NestedTensor",
                        "transformers/loss/loss_for_object_detection.py#_max_by_axis"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_grounding_dino.py#sigmoid_focal_loss": {
                "sorted_modules": {
                    "sigmoid_focal_loss": "\n\n# Similar to the one used in `DeformableDetr` but we reduce with sum and normalize by num_boxes\n# instead of mean.\ndef sigmoid_focal_loss(\n    inputs: torch.Tensor,\n    targets: torch.Tensor,\n    num_boxes: int,\n    alpha: float = 0.25,\n    gamma: float = 2,\n):\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://huggingface.co/papers/1708.02002.\n\n    Args:\n        inputs (`torch.FloatTensor` of arbitrary shape):\n            The predictions for each example.\n        targets (`torch.FloatTensor` with the same shape as `inputs`)\n            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n            and 1 for the positive class).\n        num_boxes (`int`):\n            The total number of boxes in the batch.\n        alpha (`float`, *optional*, defaults to 0.25):\n            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n        gamma (`int`, *optional*, defaults to 2):\n            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n\n    Returns:\n        Loss tensor\n    \"\"\"\n    prob = inputs.sigmoid()\n    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n    # add modulating factor\n    p_t = prob * targets + (1 - prob) * (1 - targets)\n    loss = ce_loss * ((1 - p_t) ** gamma)\n\n    if alpha >= 0:\n        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n        loss = alpha_t * loss\n\n    return loss.sum() / num_boxes"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_index_first_axis": {
                "sorted_modules": {
                    "_index_first_axis": "\n\ndef _index_first_axis(tensor, indices):\n    \"\"\"\n    A local implementation of the PyTorch indexing operation `tensor[indices]` on the first axis,\n    after flattening the first two dimensions of the tensor. This is functionally equivalent to\n    FA2's `index_first_axis` and replaces the need to import it.\n    \"\"\"\n    # The input tensor is expected to be of shape (batch, seq_len, ...). We flatten the first\n    # two dimensions to get (total_tokens, ...) before indexing.\n    reshaped_tensor = tensor.reshape(-1, *tensor.shape[2:])\n    return reshaped_tensor[indices]"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/quantization_config.py#AqlmConfig": {
                "sorted_modules": {
                    "AqlmConfig": "\n\n@dataclass\nclass AqlmConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about `aqlm` parameters.\n\n    Args:\n        in_group_size (`int`, *optional*, defaults to 8):\n            The group size along the input dimension.\n        out_group_size (`int`, *optional*, defaults to 1):\n            The group size along the output dimension. It's recommended to always use 1.\n        num_codebooks (`int`, *optional*, defaults to 1):\n            Number of codebooks for the Additive Quantization procedure.\n        nbits_per_codebook (`int`, *optional*, defaults to 16):\n            Number of bits encoding a single codebook vector. Codebooks size is 2**nbits_per_codebook.\n        linear_weights_not_to_quantize (`Optional[list[str]]`, *optional*):\n            List of full paths of `nn.Linear` weight parameters that shall not be quantized.\n        kwargs (`dict[str, Any]`, *optional*):\n            Additional parameters from which to initialize the configuration object.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_group_size: int = 8,\n        out_group_size: int = 1,\n        num_codebooks: int = 1,\n        nbits_per_codebook: int = 16,\n        linear_weights_not_to_quantize: list[str] | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.AQLM\n        self.in_group_size = in_group_size\n        self.out_group_size = out_group_size\n        self.num_codebooks = num_codebooks\n        self.nbits_per_codebook = nbits_per_codebook\n        self.linear_weights_not_to_quantize = linear_weights_not_to_quantize\n\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n        \"\"\"\n        if not isinstance(self.in_group_size, int):\n            raise TypeError(\"in_group_size must be a float\")\n        if not isinstance(self.out_group_size, int):\n            raise TypeError(\"out_group_size must be a float\")\n        if not isinstance(self.num_codebooks, int):\n            raise TypeError(\"num_codebooks must be a float\")\n        if not isinstance(self.nbits_per_codebook, int):\n            raise TypeError(\"nbits_per_codebook must be a float\")\n\n        if self.linear_weights_not_to_quantize is not None and not isinstance(\n            self.linear_weights_not_to_quantize, list\n        ):\n            raise ValueError(\"linear_weights_not_to_quantize must be a list of strings\")\n\n        if self.linear_weights_not_to_quantize is None:\n            self.linear_weights_not_to_quantize = []"
                },
                "component_dependencies": {
                    "AqlmConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#BitNetQuantConfig": {
                "sorted_modules": {
                    "BitNetQuantConfig": "\n\n@dataclass\nclass BitNetQuantConfig(QuantizationConfigMixin):\n    \"\"\"\n    Configuration class for applying BitNet quantization.\n\n    Args:\n        modules_to_not_convert (`Optional[List]`, *optional*):\n            Optionally, provides a list of full paths of `nn.Linear` weight parameters\n            that shall not be quantized. Defaults to None.\n        linear_class (`str`, *optional*, defaults to `\"bitlinear\"`):\n            The type of linear class to use. Can be either `bitlinear` or `autobitlinear`.\n        quantization_mode (`str`, *optional*, defaults to `\"offline\"`):\n            The quantization mode to use. Can be either `online` or `offline`.\n            In `online` mode, the weight quantization parameters are calculated dynamically\n            during each forward pass (e.g., based on the current weight values). This can\n            adapt to weight changes during training (Quantization-Aware Training - QAT).\n            In `offline` mode, quantization parameters are pre-calculated *before* inference.\n            These parameters are then fixed and loaded into the quantized model. This\n            generally results in lower runtime overhead compared to online quantization.\n        use_rms_norm (`bool`, *optional*, defaults to `False`):\n            Whether to apply RMSNorm on the activations before quantization. This matches the original BitNet paper's approach\n            of normalizing activations before quantization/packing.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon value used in the RMSNorm layer for numerical stability.\n        kwargs (`dict[str, Any]`, *optional*):\n            Additional keyword arguments that may be used by specific quantization\n            backends or future versions.\n    \"\"\"\n\n    def __init__(\n        self,\n        modules_to_not_convert: list | None = None,\n        linear_class: str = \"bitlinear\",\n        quantization_mode: str = \"offline\",\n        use_rms_norm: bool = False,\n        rms_norm_eps: float | None = 1e-6,\n        **kwargs,\n    ):\n        if linear_class not in [\"bitlinear\", \"autobitlinear\"]:\n            raise ValueError(f\"linear_class must be either 'bitlinear' or 'autobitlinear', but got {linear_class}\")\n        if quantization_mode not in [\"online\", \"offline\"]:\n            raise ValueError(f\"quantization_mode must be either 'online' or 'offline', but got {quantization_mode}\")\n        self.quant_method = QuantizationMethod.BITNET\n        self.modules_to_not_convert = modules_to_not_convert\n        self.linear_class = linear_class\n        self.quantization_mode = quantization_mode\n        self.use_rms_norm = use_rms_norm\n        self.rms_norm_eps = rms_norm_eps\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\""
                },
                "component_dependencies": {
                    "BitNetQuantConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#BitsAndBytesConfig": {
                "sorted_modules": {
                    "BitsAndBytesConfig": "\n\n@dataclass\nclass BitsAndBytesConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using `bitsandbytes`.\n\n    Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization. If more methods are added to `bitsandbytes`,\n    then more arguments will be added to this class.\n\n    Args:\n        load_in_8bit (`bool`, *optional*, defaults to `False`):\n            This flag is used to enable 8-bit quantization with LLM.int8().\n        load_in_4bit (`bool`, *optional*, defaults to `False`):\n            This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from\n            `bitsandbytes`.\n        llm_int8_threshold (`float`, *optional*, defaults to 6.0):\n            This corresponds to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit Matrix\n            Multiplication for Transformers at Scale` paper: https://huggingface.co/papers/2208.07339 Any hidden states value\n            that is above this threshold will be considered an outlier and the operation on those values will be done\n            in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but\n            there are some exceptional systematic outliers that are very differently distributed for large models.\n            These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of\n            magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,\n            but a lower threshold might be needed for more unstable models (small models, fine-tuning).\n        llm_int8_skip_modules (`list[str]`, *optional*):\n            An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as\n            Jukebox that has several heads in different places and not necessarily at the last position. For example\n            for `CausalLM` models, the last `lm_head` is kept in its original `dtype`.\n        llm_int8_enable_fp32_cpu_offload (`bool`, *optional*, defaults to `False`):\n            This flag is used for advanced use cases and users that are aware of this feature. If you want to split\n            your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use\n            this flag. This is useful for offloading large models such as `google/flan-t5-xxl`. Note that the int8\n            operations will not be run on CPU.\n        llm_int8_has_fp16_weight (`bool`, *optional*, defaults to `False`):\n            This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not\n            have to be converted back and forth for the backward pass.\n        bnb_4bit_compute_dtype (`torch.dtype` or str, *optional*, defaults to `torch.float32`):\n            This sets the computational type which might be different than the input type. For example, inputs might be\n            fp32, but computation can be set to bf16 for speedups.\n        bnb_4bit_quant_type (`str`,  *optional*, defaults to `\"fp4\"`):\n            This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types\n            which are specified by `fp4` or `nf4`.\n        bnb_4bit_use_double_quant (`bool`, *optional*, defaults to `False`):\n            This flag is used for nested quantization where the quantization constants from the first quantization are\n            quantized again.\n        bnb_4bit_quant_storage (`torch.dtype` or str, *optional*, defaults to `torch.uint8`):\n            This sets the storage type to pack the quantized 4-bit params.\n        kwargs (`dict[str, Any]`, *optional*):\n            Additional parameters from which to initialize the configuration object.\n    \"\"\"\n\n    def __init__(\n        self,\n        load_in_8bit=False,\n        load_in_4bit=False,\n        llm_int8_threshold=6.0,\n        llm_int8_skip_modules=None,\n        llm_int8_enable_fp32_cpu_offload=False,\n        llm_int8_has_fp16_weight=False,\n        bnb_4bit_compute_dtype=None,\n        bnb_4bit_quant_type=\"fp4\",\n        bnb_4bit_use_double_quant=False,\n        bnb_4bit_quant_storage=None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.BITS_AND_BYTES\n\n        if load_in_4bit and load_in_8bit:\n            raise ValueError(\"load_in_4bit and load_in_8bit are both True, but only one can be used at the same time\")\n\n        self._load_in_8bit = load_in_8bit\n        self._load_in_4bit = load_in_4bit\n        self.llm_int8_threshold = llm_int8_threshold\n        self.llm_int8_skip_modules = llm_int8_skip_modules\n        self.llm_int8_enable_fp32_cpu_offload = llm_int8_enable_fp32_cpu_offload\n        self.llm_int8_has_fp16_weight = llm_int8_has_fp16_weight\n        self.bnb_4bit_quant_type = bnb_4bit_quant_type\n        self.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant\n\n        if bnb_4bit_compute_dtype is None:\n            self.bnb_4bit_compute_dtype = torch.float32\n        elif isinstance(bnb_4bit_compute_dtype, str):\n            self.bnb_4bit_compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n        elif isinstance(bnb_4bit_compute_dtype, torch.dtype):\n            self.bnb_4bit_compute_dtype = bnb_4bit_compute_dtype\n        else:\n            raise ValueError(\"bnb_4bit_compute_dtype must be a string or a torch.dtype\")\n\n        if bnb_4bit_quant_storage is None:\n            self.bnb_4bit_quant_storage = torch.uint8\n        elif isinstance(bnb_4bit_quant_storage, str):\n            if bnb_4bit_quant_storage not in [\"float16\", \"float32\", \"int8\", \"uint8\", \"float64\", \"bfloat16\"]:\n                raise ValueError(\n                    \"`bnb_4bit_quant_storage` must be a valid string (one of 'float16', 'float32', 'int8', 'uint8', 'float64', 'bfloat16') \"\n                )\n            self.bnb_4bit_quant_storage = getattr(torch, bnb_4bit_quant_storage)\n        elif isinstance(bnb_4bit_quant_storage, torch.dtype):\n            self.bnb_4bit_quant_storage = bnb_4bit_quant_storage\n        else:\n            raise ValueError(\"bnb_4bit_quant_storage must be a string or a torch.dtype\")\n\n        if kwargs:\n            logger.info(f\"Unused kwargs: {list(kwargs.keys())}. These kwargs are not used in {self.__class__}.\")\n\n        self.post_init()\n\n    @property\n    def load_in_4bit(self):\n        return self._load_in_4bit\n\n    @load_in_4bit.setter\n    def load_in_4bit(self, value: bool):\n        if not isinstance(value, bool):\n            raise TypeError(\"load_in_4bit must be a boolean\")\n\n        if self.load_in_8bit and value:\n            raise ValueError(\"load_in_4bit and load_in_8bit are both True, but only one can be used at the same time\")\n        self._load_in_4bit = value\n\n    @property\n    def load_in_8bit(self):\n        return self._load_in_8bit\n\n    @load_in_8bit.setter\n    def load_in_8bit(self, value: bool):\n        if not isinstance(value, bool):\n            raise TypeError(\"load_in_8bit must be a boolean\")\n\n        if self.load_in_4bit and value:\n            raise ValueError(\"load_in_4bit and load_in_8bit are both True, but only one can be used at the same time\")\n        self._load_in_8bit = value\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n        \"\"\"\n        if not isinstance(self.load_in_4bit, bool):\n            raise TypeError(\"load_in_4bit must be a boolean\")\n\n        if not isinstance(self.load_in_8bit, bool):\n            raise TypeError(\"load_in_8bit must be a boolean\")\n\n        if not isinstance(self.llm_int8_threshold, float):\n            raise TypeError(\"llm_int8_threshold must be a float\")\n\n        if self.llm_int8_skip_modules is not None and not isinstance(self.llm_int8_skip_modules, list):\n            raise TypeError(\"llm_int8_skip_modules must be a list of strings\")\n        if not isinstance(self.llm_int8_enable_fp32_cpu_offload, bool):\n            raise TypeError(\"llm_int8_enable_fp32_cpu_offload must be a boolean\")\n\n        if not isinstance(self.llm_int8_has_fp16_weight, bool):\n            raise TypeError(\"llm_int8_has_fp16_weight must be a boolean\")\n\n        if self.bnb_4bit_compute_dtype is not None and not isinstance(self.bnb_4bit_compute_dtype, torch.dtype):\n            raise TypeError(\"bnb_4bit_compute_dtype must be torch.dtype\")\n\n        if not isinstance(self.bnb_4bit_quant_type, str):\n            raise TypeError(\"bnb_4bit_quant_type must be a string\")\n\n        if not isinstance(self.bnb_4bit_use_double_quant, bool):\n            raise TypeError(\"bnb_4bit_use_double_quant must be a boolean\")\n\n    def is_quantizable(self):\n        r\"\"\"\n        Returns `True` if the model is quantizable, `False` otherwise.\n        \"\"\"\n        return self.load_in_8bit or self.load_in_4bit\n\n    def quantization_method(self):\n        r\"\"\"\n        This method returns the quantization method used for the model. If the model is not quantizable, it returns\n        `None`.\n        \"\"\"\n        if self.load_in_8bit:\n            return \"llm_int8\"\n        elif self.load_in_4bit and self.bnb_4bit_quant_type == \"fp4\":\n            return \"fp4\"\n        elif self.load_in_4bit and self.bnb_4bit_quant_type == \"nf4\":\n            return \"nf4\"\n        else:\n            return None\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Serializes this instance to a Python dictionary. Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        output = copy.deepcopy(self.__dict__)\n        output[\"bnb_4bit_compute_dtype\"] = str(output[\"bnb_4bit_compute_dtype\"]).split(\".\")[1]\n        output[\"bnb_4bit_quant_storage\"] = str(output[\"bnb_4bit_quant_storage\"]).split(\".\")[1]\n        output[\"load_in_4bit\"] = self.load_in_4bit\n        output[\"load_in_8bit\"] = self.load_in_8bit\n\n        return output\n\n    def __repr__(self):\n        config_dict = self.to_dict()\n        return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n\n    def to_diff_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Removes all attributes from config which correspond to the default config attributes for better readability and\n        serializes to a Python dictionary.\n\n        Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"\n        config_dict = self.to_dict()\n\n        # get the default config dict\n        default_config_dict = BitsAndBytesConfig().to_dict()\n\n        serializable_config_dict = {}\n\n        # only serialize values that differ from the default config\n        for key, value in config_dict.items():\n            if value != default_config_dict[key]:\n                serializable_config_dict[key] = value\n\n        return serializable_config_dict"
                },
                "component_dependencies": {
                    "BitsAndBytesConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod",
                        "transformers/utils/quantization_config.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#EetqConfig": {
                "sorted_modules": {
                    "EetqConfig": "\n\n@dataclass\nclass EetqConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using `eetq`.\n\n    Args:\n        weights (`str`, *optional*, defaults to `\"int8\"`):\n            The target dtype for the weights. Supported value is only \"int8\"\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision.\n    \"\"\"\n\n    def __init__(\n        self,\n        weights: str = \"int8\",\n        modules_to_not_convert: list | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.EETQ\n        self.weights = weights\n        self.modules_to_not_convert = modules_to_not_convert\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        accepted_weights = [\"int8\"]\n        if self.weights not in accepted_weights:\n            raise ValueError(f\"Only support weights in {accepted_weights} but found {self.weights}\")"
                },
                "component_dependencies": {
                    "EetqConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#FPQuantConfig": {
                "sorted_modules": {
                    "FPQuantConfig": "\n\n@dataclass\nclass FPQuantConfig(QuantizationConfigMixin):\n    \"\"\"\n    FPQuantConfig is a configuration class for quantization using the FPQuant method.\n\n    Args:\n        forward_dtype (`str`, *optional*, defaults to `\"nvfp4\"`):\n            The dtype to use for the forward pass.\n        forward_method (`str`, *optional*, defaults to `\"abs_max\"`):\n            The scaling to use for the forward pass. Can be `\"abs_max\"` or `\"quest\"`. `\"abs_max\"` is better for PTQ, `\"quest\"` is better for QAT.\n        backward_dtype (`str`, *optional*, defaults to `\"bf16\"`):\n            The dtype to use for the backward pass.\n        store_master_weights (`bool`, *optional*, defaults to `False`):\n            Whether to store the master weights. Needed for QAT over layer weights.\n        hadamard_group_size (`int`, *optional*):\n            The group size for the hadamard transform before quantization for `\"quest\"` it matches the MXFP4 group size (32). If `None`, it will be set to 16 for `\"nvfp4\"` and 32 for `\"mxfp4\"`.\n        pseudoquantization (`bool`, *optional*, defaults to `False`):\n            Whether to use Triton-based pseudo-quantization. Is mandatory for non-Blackwell GPUs. Doesn't provide any speedup. For debugging purposes.\n        transform_init (`str`, *optional*, defaults to `\"hadamard\"`): a method to initialize the pre-processing matrix with. Can be `\"hadamard\"`, `\"identity\"` or `\"gsr\"`.\n        modules_to_not_convert (`list`, *optional*):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision.\n    \"\"\"\n\n    def __init__(\n        self,\n        forward_dtype: str = \"nvfp4\",\n        forward_method: str = \"abs_max\",\n        backward_dtype: str = \"bf16\",\n        store_master_weights: bool = False,\n        hadamard_group_size: int | None = None,\n        pseudoquantization: bool = False,\n        transform_init: str = \"hadamard\",\n        modules_to_not_convert: list[str] | None = None,\n        **kwargs,\n    ):\n        self.forward_dtype = forward_dtype\n        self.forward_method = forward_method\n        self.backward_dtype = backward_dtype\n        self.store_master_weights = store_master_weights\n        self.hadamard_group_size = hadamard_group_size\n        self.pseudoquantization = pseudoquantization\n        self.transform_init = transform_init\n        self.modules_to_not_convert = modules_to_not_convert\n\n        self.quant_method = QuantizationMethod.FPQUANT\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n        \"\"\"\n\n        if self.hadamard_group_size is None:\n            if self.forward_dtype == \"nvfp4\":\n                self.hadamard_group_size = 16\n            else:\n                self.hadamard_group_size = 32\n\n        if self.forward_dtype == \"mxfp4\":\n            if self.forward_method not in [\"abs_max\", \"quest\"]:\n                raise ValueError(\"Only 'abs_max' and 'quest' are supported for forward_method for 'mxfp4'.\")\n            if self.hadamard_group_size is None:\n                self.hadamard_group_size = 32\n            if self.hadamard_group_size not in [32, 64, 128]:\n                raise ValueError(\"Only a `hadamard_group_size` of [32, 64, 128] is supported for 'mxfp4'.\")\n        elif self.forward_dtype == \"nvfp4\":\n            if self.forward_method != \"abs_max\":\n                raise ValueError(\"Only 'abs_max' is supported for forward_method for 'nvfp4'.\")\n            if self.hadamard_group_size is None:\n                self.hadamard_group_size = 16\n            if self.hadamard_group_size not in [16, 32, 64, 128]:\n                raise ValueError(\"Only a `hadamard_group_size` of [16, 32, 64, 128] is supported for 'nvfp4'.\")\n        else:\n            raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n\n        if self.backward_dtype != \"bf16\":\n            raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n        if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n            raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n\n        if self.modules_to_not_convert is None:\n            self.modules_to_not_convert = [\"lm_head\"]"
                },
                "component_dependencies": {
                    "FPQuantConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#FineGrainedFP8Config": {
                "sorted_modules": {
                    "FineGrainedFP8Config": "\n\n@dataclass\nclass FineGrainedFP8Config(QuantizationConfigMixin):\n    \"\"\"\n    FineGrainedFP8Config is a configuration class for fine-grained FP8 quantization used mainly for deepseek models.\n\n    Args:\n        activation_scheme (`str`, *optional*, defaults to `\"dynamic\"`):\n            The scheme used for activation, the defaults and only support scheme for now is \"dynamic\".\n        weight_block_size (`typing.tuple[int, int]`, *optional*, defaults to `(128, 128)`):\n            The size of the weight blocks for quantization, default is (128, 128).\n        modules_to_not_convert (`list`, *optional*):\n            A list of module names that should not be converted during quantization.\n    \"\"\"\n\n    def __init__(\n        self,\n        activation_scheme: str = \"dynamic\",\n        weight_block_size: tuple[int, int] = (128, 128),\n        modules_to_not_convert: list | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.FP8\n        self.modules_to_not_convert = modules_to_not_convert\n        self.activation_scheme = activation_scheme\n        self.weight_block_size = weight_block_size\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        self.activation_scheme = self.activation_scheme.lower()\n        if self.activation_scheme != \"dynamic\":\n            raise ValueError(f\"Activation scheme {self.activation_scheme} not supported\")\n        if len(self.weight_block_size) != 2:\n            raise ValueError(\"weight_block_size must be a tuple of two integers\")\n        if self.weight_block_size[0] <= 0 or self.weight_block_size[1] <= 0:\n            raise ValueError(\"weight_block_size must be a tuple of two positive integers\")"
                },
                "component_dependencies": {
                    "FineGrainedFP8Config": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#HiggsConfig": {
                "sorted_modules": {
                    "HiggsConfig": "\n\n@dataclass\nclass HiggsConfig(QuantizationConfigMixin):\n    \"\"\"\n    HiggsConfig is a configuration class for quantization using the HIGGS method.\n\n    Args:\n        bits (int, *optional*, defaults to 4):\n            Number of bits to use for quantization. Can be 2, 3 or 4. Default is 4.\n        p (int, *optional*, defaults to 2):\n            Quantization grid dimension. 1 and 2 are supported. 2 is always better in practice. Default is 2.\n        modules_to_not_convert (`list`, *optional*, default to [\"lm_head\"]):\n            List of linear layers that should not be quantized.\n        hadamard_size (int, *optional*, defaults to 512):\n            Hadamard size for the HIGGS method. Default is 512. Input dimension of matrices is padded to this value. Decreasing this below 512 will reduce the quality of the quantization.\n        group_size (int, *optional*, defaults to 256):\n            Group size for the HIGGS method. Can be 64, 128 or 256. Decreasing it barely affects the performance. Default is 256. Must be a divisor of hadamard_size.\n        tune_metadata ('dict', *optional*, defaults to {}):\n            Module-wise metadata (gemm block shapes, GPU metadata, etc.) for saving the kernel tuning results. Default is an empty dictionary. Is set automatically during tuning.\n    \"\"\"\n\n    def __init__(\n        self,\n        bits: int = 4,\n        p: int = 2,\n        modules_to_not_convert: list[str] | None = None,\n        hadamard_size: int = 512,\n        group_size: int = 256,\n        tune_metadata: dict[str, Any] | None = None,\n        **kwargs,\n    ):\n        if tune_metadata is None:\n            tune_metadata = {}\n        self.quant_method = QuantizationMethod.HIGGS\n        self.bits = bits\n        self.p = p\n        self.modules_to_not_convert = modules_to_not_convert\n        self.hadamard_size = hadamard_size\n        self.group_size = group_size\n        self.tune_metadata = tune_metadata\n\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n        \"\"\"\n        if self.bits not in [2, 3, 4]:\n            raise ValueError(\"bits must be 2, 3, or 4\")\n        if self.p not in [1, 2]:\n            raise ValueError(\"p must be 1 or 2. 2 is always better in practice\")\n        if self.group_size not in [64, 128, 256]:\n            raise ValueError(\"group_size must be 64, 128, or 256\")\n        if self.hadamard_size % self.group_size != 0:\n            raise ValueError(\"hadamard_size must be divisible by group_size\")"
                },
                "component_dependencies": {
                    "HiggsConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#HqqConfig": {
                "sorted_modules": {
                    "HqqConfig": "\n\n@dataclass\nclass HqqConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is wrapper around hqq's BaseQuantizeConfig.\n\n    Args:\n        nbits (`int`, *optional*, defaults to 4):\n            Number of bits. Supported values are (8, 4, 3, 2, 1).\n        group_size (`int`, *optional*, defaults to 64):\n            Group-size value. Supported values are any value that is divisible by weight.shape[axis]).\n        view_as_float (`bool`, *optional*, defaults to `False`):\n            View the quantized weight as float (used in distributed training) if set to `True`.\n        axis (`Optional[int]`, *optional*):\n            Axis along which grouping is performed. Supported values are 0 or 1.\n        dynamic_config (dict, *optional*):\n            Parameters for dynamic configuration. The key is the name tag of the layer and the value is a quantization config.\n            If set, each layer specified by its id will use its dedicated quantization configuration.\n        skip_modules (`list[str]`, *optional*, defaults to `['lm_head']`):\n            List of `nn.Linear` layers to skip.\n        kwargs (`dict[str, Any]`, *optional*):\n            Additional parameters from which to initialize the configuration object.\n    \"\"\"\n\n    def __init__(\n        self,\n        nbits: int = 4,\n        group_size: int = 64,\n        view_as_float: bool = False,\n        axis: int | None = None,\n        dynamic_config: dict | None = None,\n        skip_modules: list[str] = [\"lm_head\"],\n        **kwargs,\n    ):\n        if is_hqq_available():\n            from hqq.core.quantize import BaseQuantizeConfig as HQQBaseQuantizeConfig\n        else:\n            raise ImportError(\n                \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n            )\n\n        if axis is None:\n            axis = 1\n            logger.info(\"Setting axis=1 as faster backends such as TorchAO or BitBlas are only compatible with it.\")\n\n        if axis not in [0, 1]:\n            raise ValueError(\"Invalid axis value. Only 0 and 1 are allowed.\")\n\n        if dynamic_config is not None:\n            self.quant_config = {}\n            for key in dynamic_config:\n                self.quant_config[key] = HQQBaseQuantizeConfig(**dynamic_config[key])\n        else:\n            self.quant_config = HQQBaseQuantizeConfig(\n                nbits=nbits, group_size=group_size, view_as_float=view_as_float, axis=axis\n            )\n\n        self.quant_method = QuantizationMethod.HQQ\n        self.skip_modules = skip_modules\n\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n        \"\"\"\n\n    @classmethod\n    def from_dict(cls, config: dict[str, Any]):\n        \"\"\"\n        Override from_dict, used in AutoQuantizationConfig.from_dict in quantizers/auto.py\n        \"\"\"\n        instance = cls()\n        instance.quant_config = config[\"quant_config\"]\n        instance.skip_modules = config[\"skip_modules\"]\n        return instance\n\n    def to_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Serializes this instance to a Python dictionary. Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n        \"\"\"\n        return {\n            \"quant_config\": self.quant_config,\n            \"quant_method\": self.quant_method,\n            \"skip_modules\": self.skip_modules,\n        }\n\n    def __repr__(self):\n        config_dict = self.to_dict()\n        return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n\n    def to_diff_dict(self) -> dict[str, Any]:\n        \"\"\"\n        Removes all attributes from config which correspond to the default config attributes for better readability and\n        serializes to a Python dictionary.\n        Returns:\n            `dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"\n        config_dict = self.to_dict()\n\n        # get the default config dict\n        default_config_dict = HqqConfig().to_dict()\n\n        serializable_config_dict = {}\n\n        # only serialize values that differ from the default config\n        for key, value in config_dict.items():\n            if value != default_config_dict[key]:\n                serializable_config_dict[key] = value\n\n        return serializable_config_dict"
                },
                "component_dependencies": {
                    "HqqConfig": [
                        "transformers/utils.py#is_hqq_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod",
                        "transformers/utils/quantization_config.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#QuantoConfig": {
                "sorted_modules": {
                    "QuantoConfig": "\n\n@dataclass\nclass QuantoConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n    loaded using `quanto`.\n\n    Args:\n        weights (`str`, *optional*, defaults to `\"int8\"`):\n            The target dtype for the weights after quantization. Supported values are (\"float8\",\"int8\",\"int4\",\"int2\")\n        activations (`str`, *optional*):\n            The target dtype for the activations after quantization. Supported values are (None,\"int8\",\"float8\")\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision (e.g. Whisper encoder, Llava encoder, Mixtral gate layers).\n    \"\"\"\n\n    def __init__(\n        self,\n        weights=\"int8\",\n        activations=None,\n        modules_to_not_convert: list | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.QUANTO\n        self.weights = weights\n        self.activations = activations\n        self.modules_to_not_convert = modules_to_not_convert\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        accepted_weights = [\"float8\", \"int8\", \"int4\", \"int2\"]\n        accepted_activations = [None, \"int8\", \"float8\"]\n        if self.weights not in accepted_weights:\n            raise ValueError(f\"Only support weights in {accepted_weights} but found {self.weights}\")\n        if self.activations not in accepted_activations:\n            raise ValueError(f\"Only support weights in {accepted_activations} but found {self.activations}\")"
                },
                "component_dependencies": {
                    "QuantoConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#QuarkConfig": {
                "sorted_modules": {
                    "QuarkConfig": "\n\nclass QuarkConfig(QuantizationConfigMixin):\n    def __init__(\n        self,\n        **kwargs,\n    ):\n        if is_torch_available() and is_quark_available():\n            from quark import __version__ as quark_version\n            from quark.torch.export.config.config import JsonExporterConfig\n            from quark.torch.export.main_export.quant_config_parser import QuantConfigParser\n            from quark.torch.quantization.config.config import Config\n        else:\n            raise ImportError(\n                \"Quark is not installed. Please refer to https://quark.docs.amd.com/latest/install.html.\"\n            )\n        # This might be e.g. `\"fp8\"` or `\"awq\"`.\n        self.custom_mode = kwargs[\"quant_method\"]\n        self.legacy = \"export\" not in kwargs\n\n        if self.custom_mode in [\"awq\", \"fp8\"]:\n            # Legacy (quark<1.0) or custom export.\n            self.quant_config = QuantConfigParser.from_custom_config(kwargs, is_bias_quantized=False)\n            self.json_export_config = JsonExporterConfig()\n        else:\n            self.quant_config = Config.from_dict(kwargs)\n\n            if \"export\" in kwargs:\n                # TODO: Remove this check once configuration version is handled natively by Quark.\n                if \"min_kv_scale\" in kwargs[\"export\"] and version.parse(quark_version) < version.parse(\"0.8\"):\n                    min_kv_scale = kwargs[\"export\"].pop(\"min_kv_scale\")\n                    logger.warning(\n                        f\"The parameter `min_kv_scale={min_kv_scale}` was found in the model config.json's `quantization_config.export` configuration, but this parameter is supported only for quark>=0.8. Ignoring this configuration parameter. Please update the `amd-quark` package.\"\n                    )\n\n                self.json_export_config = JsonExporterConfig(**kwargs[\"export\"])\n            else:\n                # Legacy (quark<1.0) or custom export.\n                self.json_export_config = JsonExporterConfig()\n\n        self.quant_method = QuantizationMethod.QUARK"
                },
                "component_dependencies": {
                    "QuarkConfig": [
                        "transformers/utils.py#is_quark_available",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod",
                        "transformers/utils/quantization_config.py#logger"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#SpQRConfig": {
                "sorted_modules": {
                    "SpQRConfig": "\n\n@dataclass\nclass SpQRConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about `spqr` parameters. Refer to the original publication for more details.\n\n    Args:\n        bits (`int`, *optional*, defaults to 3):\n            Specifies the bit count for the weights and first order zero-points and scales.\n            Currently only bits = 3 is supported.\n        beta1 (`int`, *optional*, defaults to 16):\n            SpQR tile width. Currently only beta1 = 16 is supported.\n        beta2 (`int`, *optional*, defaults to 16):\n            SpQR tile height. Currently only beta2 = 16 is supported.\n        shapes (`Optional`, *optional*):\n            A dictionary holding the shape of each object. We need this because it's impossible\n            to deduce the exact size of the parameters just from bits, beta1, beta2.\n        modules_to_not_convert (`Optional[list[str]]`, *optional*):\n            Optionally, provides a list of full paths of `nn.Linear` weight parameters that shall not be quantized.\n            Defaults to None.\n        kwargs (`dict[str, Any]`, *optional*):\n            Additional parameters from which to initialize the configuration object.\n    \"\"\"\n\n    def __init__(\n        self,\n        bits: int = 3,\n        beta1: int = 16,\n        beta2: int = 16,\n        shapes: dict[str, int] | None = None,\n        modules_to_not_convert: list[str] | None = None,\n        **kwargs,\n    ):\n        if shapes is None:\n            shapes = {}\n        self.shapes = shapes\n        self.quant_method = QuantizationMethod.SPQR\n        self.bits = bits\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.modules_to_not_convert = modules_to_not_convert\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n        \"\"\"\n        if not isinstance(self.bits, int):\n            raise TypeError(\"bits must be an int\")\n        if not isinstance(self.beta1, int):\n            raise TypeError(\"beta1 must be an int\")\n        if not isinstance(self.beta2, int):\n            raise TypeError(\"beta2 must be an int\")\n\n        if self.bits != 3:\n            raise ValueError(\"SpQR currently only supports bits = 3\")\n        if self.beta1 != 16:\n            raise ValueError(\"SpQR currently only supports beta1 = 16\")\n        if self.beta2 != 16:\n            raise ValueError(\"SpQR currently only supports beta2 = 16\")\n        if not isinstance(self.shapes, dict):\n            raise TypeError(\"shapes must be a dict\")"
                },
                "component_dependencies": {
                    "SpQRConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#TorchAoConfig": {
                "sorted_modules": {
                    "TorchAoConfig": "\n\n@dataclass\nclass TorchAoConfig(QuantizationConfigMixin):\n    quant_method: QuantizationMethod\n    quant_type: Union[str, \"AOBaseConfig\"]  # noqa: F821\n    modules_to_not_convert: list | None\n    quant_type_kwargs: dict[str, Any]\n    include_input_output_embeddings: bool\n    untie_embedding_weights: bool\n\n    \"\"\"This is a config class for torchao quantization/sparsity techniques.\n\n    Args:\n        quant_type (`Union[str, AOBaseConfig]`):\n            The type of quantization we want to use. Can be either:\n            - A string: currently supporting: `int4_weight_only`, `int8_weight_only` and `int8_dynamic_activation_int8_weight`.\n            - An AOBaseConfig instance: for more advanced configuration options.\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision.\n        include_input_output_embeddings (`bool`, default to `False`):\n            Whether to include embedding in quantization or not, input embedding will be removed from\n            the module_not_to_convert list as well if this flag is set.\n        untie_embedding_weights (`bool`, default to `False`):\n            Whether to untie the weights when we are quantizing input embedding weights that is tied\n            to other weights.\n        kwargs (`dict[str, Any]`, *optional*):\n            The keyword arguments for the chosen type of quantization, for example, int4_weight_only quantization supports two keyword arguments\n            `group_size` and `inner_k_tiles` currently. More API examples and documentation of arguments can be found in\n            https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques\n\n    Example:\n\n    ```python\n    # AOBaseConfig-based configuration\n    config = Int4WeightOnlyConfig(group_size=32)\n    quantization_config = TorchAoConfig(config)\n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", dtype=torch.bfloat16, quantization_config=quantization_config)\n\n    # String-based configuration\n    quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=32)\n    # int4_weight_only quant is only working with *torch.bfloat16* dtype right now\n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", dtype=torch.bfloat16, quantization_config=quantization_config)\n\n    # autoquant\n    # `autoquant` is a convenient way for users to search for the best quantization for each layer\n    # `min_sqnr` is an option to control the accuracy of the model, higher value means the model is more\n    # accurate, we can start with 30 and adjust it to larger or smaller (e.g. 40, 20)\n    # defaults to None, which means we'll try to get the best performing quantized model without\n    # considering accuracy\n    quantization_config = TorchAoConfig(\"autoquant\", min_sqnr=30)\n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", dtype=torch.bfloat16, quantization_config=quantization_config)\n    # run through example inputs, quantization methods will be selected based on the shape of example input\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    input_text = \"What are we having for dinner?\"\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    MAX_NEW_TOKENS = 1000\n    model.generate(**input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\")\n    # manually ran finalize_autoquant if needed\n    if hasattr(quantized_model, \"finalize_autoquant\"):\n      print(\"finalizing autoquant\")\n      quantized_model.finalize_autoquant()\n\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        quant_type: Union[str, \"AOBaseConfig\"],  # noqa: F821\n        modules_to_not_convert: list | None = None,\n        include_input_output_embeddings: bool = False,\n        untie_embedding_weights: bool = False,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.TORCHAO\n        self.quant_type = quant_type\n        self.modules_to_not_convert = modules_to_not_convert\n        self.quant_type_kwargs = kwargs.get(\"quant_type_kwargs\", kwargs)\n        self.include_input_output_embeddings = include_input_output_embeddings\n        self.untie_embedding_weights = untie_embedding_weights\n        self.post_init()\n\n    @staticmethod\n    def _get_ao_version() -> version.Version:\n        \"\"\"Centralized check for TorchAO availability and version requirements.\"\"\"\n        if not is_torchao_available():\n            raise ValueError(\"TorchAoConfig requires torchao to be installed. Install with `pip install torchao`\")\n\n        return version.parse(importlib.metadata.version(\"torchao\"))\n\n    def post_init(self):\n        \"\"\"Validate configuration and set defaults.\"\"\"\n        ao_version = self._get_ao_version()\n\n        # Handle quant_type based on type and version\n        if isinstance(self.quant_type, str):\n            self._validate_string_quant_type()\n        elif ao_version > version.parse(\"0.9.0\"):\n            from torchao.quantization.quant_api import AOBaseConfig\n\n            if not isinstance(self.quant_type, AOBaseConfig):\n                raise TypeError(\n                    f\"quant_type must be either a string or an AOBaseConfig instance, got {type(self.quant_type)}\"\n                )\n        else:\n            raise ValueError(\n                f\"In torchao <= 0.9.0, quant_type must be a string. Got {type(self.quant_type)}. \"\n                f\"Please upgrade to torchao > 0.9.0 to use AOBaseConfig instances.\"\n            )\n\n    def _validate_string_quant_type(self):\n        \"\"\"Validate string quant_type and its kwargs.\"\"\"\n        methods = self._get_torchao_quant_type_to_method()\n\n        if self.quant_type not in methods:\n            raise ValueError(\n                f\"Unsupported string quantization type: {self.quant_type}. \"\n                f\"Supported types: {', '.join(methods.keys())}\"\n            )\n\n        # Validate kwargs against method signature\n        method = methods[self.quant_type]\n        sig = signature(method)\n        valid_kwargs = {\n            param.name\n            for param in sig.parameters.values()\n            if param.kind in [Parameter.KEYWORD_ONLY, Parameter.POSITIONAL_OR_KEYWORD]\n        }\n\n        invalid_kwargs = set(self.quant_type_kwargs) - valid_kwargs\n        if invalid_kwargs:\n            raise ValueError(\n                f\"Unexpected keyword arg for {self.quant_type}: {', '.join(invalid_kwargs)}. \"\n                f\"Valid kwargs: {', '.join(valid_kwargs)}\"\n            )\n\n    def _get_torchao_quant_type_to_method(self):\n        \"\"\"Get mapping of quant_type strings to their corresponding methods.\"\"\"\n        from torchao.quantization import (\n            autoquant,\n            int4_weight_only,\n            int8_dynamic_activation_int8_weight,\n            int8_weight_only,\n        )\n\n        return {\n            \"int4_weight_only\": int4_weight_only,\n            \"int8_weight_only\": int8_weight_only,\n            \"int8_dynamic_activation_int8_weight\": int8_dynamic_activation_int8_weight,\n            \"autoquant\": autoquant,\n        }\n\n    def get_apply_tensor_subclass(self):\n        \"\"\"Create the appropriate quantization method based on configuration.\"\"\"\n        if isinstance(self.quant_type, str):\n            methods = self._get_torchao_quant_type_to_method()\n            quant_type_kwargs = self.quant_type_kwargs.copy()\n            if (\n                not torch.cuda.is_available()\n                and is_torchao_available()\n                and self.quant_type == \"int4_weight_only\"\n                and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n                and quant_type_kwargs.get(\"layout\", None) is None\n            ):\n                if torch.xpu.is_available():\n                    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\n                        \"0.11.0\"\n                    ) and version.parse(importlib.metadata.version(\"torch\")) > version.parse(\"2.7.9\"):\n                        from torchao.dtypes import Int4XPULayout\n                        from torchao.quantization.quant_primitives import ZeroPointDomain\n\n                        quant_type_kwargs[\"layout\"] = Int4XPULayout()\n                        quant_type_kwargs[\"zero_point_domain\"] = ZeroPointDomain.INT\n                    else:\n                        raise ValueError(\n                            \"TorchAoConfig requires torchao >= 0.11.0 and torch >= 2.8.0 for XPU support. Please upgrade the version or use run on CPU with the cpu version pytorch.\"\n                        )\n                else:\n                    from torchao.dtypes import Int4CPULayout\n\n                    quant_type_kwargs[\"layout\"] = Int4CPULayout()\n\n            return methods[self.quant_type](**quant_type_kwargs)\n        else:\n            return self.quant_type\n\n    def to_dict(self):\n        \"\"\"Convert configuration to a dictionary.\"\"\"\n        d = super().to_dict()\n\n        if isinstance(self.quant_type, str):\n            # Handle layout serialization if present\n            if \"quant_type_kwargs\" in d and \"layout\" in d[\"quant_type_kwargs\"]:\n                if is_dataclass(d[\"quant_type_kwargs\"][\"layout\"]):\n                    d[\"quant_type_kwargs\"][\"layout\"] = [\n                        d[\"quant_type_kwargs\"][\"layout\"].__class__.__name__,\n                        dataclasses.asdict(d[\"quant_type_kwargs\"][\"layout\"]),\n                    ]\n                if isinstance(d[\"quant_type_kwargs\"][\"layout\"], list):\n                    assert len(d[\"quant_type_kwargs\"][\"layout\"]) == 2, \"layout saves layout name and layout kwargs\"\n                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][0], str), \"layout name must be a string\"\n                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][1], dict), \"layout kwargs must be a dict\"\n                else:\n                    raise ValueError(\"layout must be a list\")\n        else:\n            # Handle AOBaseConfig serialization\n            from torchao.core.config import config_to_dict\n\n            # For now we assume there is 1 config per Transformer, however in the future\n            # We may want to support a config per fqn.\n            d[\"quant_type\"] = {\"default\": config_to_dict(self.quant_type)}\n\n        return d\n\n    @classmethod\n    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n        \"\"\"Create configuration from a dictionary.\"\"\"\n        ao_version = cls._get_ao_version()\n        assert ao_version > version.parse(\"0.9.0\"), \"TorchAoConfig requires torchao > 0.9.0 for construction from dict\"\n        config_dict = config_dict.copy()\n        quant_type = config_dict.pop(\"quant_type\")\n\n        if isinstance(quant_type, str):\n            return cls(quant_type=quant_type, **config_dict)\n        # Check if we only have one key which is \"default\"\n        # In the future we may update this\n        assert len(quant_type) == 1 and \"default\" in quant_type, (\n            \"Expected only one key 'default' in quant_type dictionary\"\n        )\n        quant_type = quant_type[\"default\"]\n\n        # Deserialize quant_type if needed\n        from torchao.core.config import config_from_dict\n\n        quant_type = config_from_dict(quant_type)\n\n        return cls(quant_type=quant_type, **config_dict)"
                },
                "component_dependencies": {
                    "TorchAoConfig": [
                        "transformers/utils.py#is_torchao_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#VptqConfig": {
                "sorted_modules": {
                    "VptqConfig": "\n\n@dataclass\nclass VptqConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is a wrapper class about `vptq` parameters.\n\n    Args:\n        enable_proxy_error (`bool`, *optional*, defaults to `False`): calculate proxy error for each layer\n        config_for_layers (`Dict`, *optional*, defaults to `{}`): quantization params for each layer\n        shared_layer_config (`Dict`, *optional*, defaults to `{}`): shared quantization params among layers\n        modules_to_not_convert (`list`, *optional*, default to `None`):\n            The list of modules to not quantize, useful for quantizing models that explicitly require to have\n            some modules left in their original precision (e.g. Whisper encoder, Llava encoder, Mixtral gate layers).\n        kwargs (`dict[str, Any]`, *optional*):\n            Additional parameters from which to initialize the configuration object.\n    \"\"\"\n\n    def __init__(\n        self,\n        enable_proxy_error: bool = False,\n        config_for_layers: dict[str, Any] = {},\n        shared_layer_config: dict[str, Any] = {},\n        modules_to_not_convert: list | None = None,\n        **kwargs,\n    ):\n        self.quant_method = QuantizationMethod.VPTQ\n        self.enable_proxy_error = enable_proxy_error\n        self.config_for_layers: dict[str, Any] = config_for_layers\n        self.shared_layer_config: dict[str, Any] = shared_layer_config\n        self.modules_to_not_convert = modules_to_not_convert\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        for layer_param in self.config_for_layers.values():\n            VptqLayerConfig(**layer_param)\n        if self.enable_proxy_error is True:\n            raise ValueError(\"enable_proxy_error should always be False until we support training\")"
                },
                "component_dependencies": {
                    "VptqConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                        "transformers/utils/quantization_config.py#QuantizationMethod",
                        "transformers/utils/quantization_config.py#VptqLayerConfig"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_aqlm.py#AqlmHfQuantizer": {
                "sorted_modules": {
                    "AqlmHfQuantizer": "\n\nclass AqlmHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer of the AQLM method. Enables the loading of prequantized models.\n    \"\"\"\n\n    requires_calibration = True\n    required_packages = [\"aqlm\"]\n    optimum_quantizer = None\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_accelerate_available():\n            raise ImportError(\"Using `aqlm` quantization requires Accelerate: `pip install accelerate`\")\n\n        if not is_aqlm_available():\n            raise ImportError(\"Using `aqlm` quantization requires AQLM: `pip install aqlm[gpu,cpu]`\")\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            if torch.cuda.is_available():\n                dtype = torch.float16\n                logger.info(\n                    \"CUDA available. Assuming AQLM inference on GPU and loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\"\n                )\n            else:\n                dtype = torch.float32\n                logger.info(\n                    \"CUDA is unavailable. Assuming AQLM inference on CPU and loading the model in `torch.float32`. To overwrite it, set `dtype` manually.\"\n                )\n        return dtype\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        **kwargs,\n    ):\n        replace_with_aqlm_linear(\n            model,\n            quantization_config=self.quantization_config,\n            linear_weights_not_to_quantize=self.quantization_config.linear_weights_not_to_quantize,\n        )\n        model.config.quantization_config = self.quantization_config\n\n    @property\n    def is_trainable(self) -> bool:\n        aqlm_supports_training = version.parse(importlib.metadata.version(\"aqlm\")) >= version.parse(\"1.0.2\")\n        if aqlm_supports_training:\n            return True\n        else:\n            logger.warning(\n                f\"Currently installed `aqlm` version ({importlib.metadata.version('aqlm')}) doesn't support training. If you wish to train a quantized model, please update `aqlm` with `pip install aqlm>=1.0.2`\"\n            )\n            return False\n\n    def is_serializable(self, safe_serialization=None):\n        return True"
                },
                "component_dependencies": {
                    "AqlmHfQuantizer": [
                        "transformers/integrations.py#replace_with_aqlm_linear",
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_aqlm.py#logger",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_aqlm_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_auto_round.py#AutoRoundQuantizer": {
                "sorted_modules": {
                    "AutoRoundQuantizer": "\n\nclass AutoRoundQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer of the AutoRound method. (https://huggingface.co/papers/2309.05516)\n    \"\"\"\n\n    # AutoRound requires data calibration - we support only inference\n    requires_calibration = True\n    required_packages = [\"auto_round\"]\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n    def validate_environment(self, *args, **kwargs):\n        self.device_map = kwargs.get(\"device_map\")\n        if not is_auto_round_available():\n            raise ImportError(\n                \"Loading an AutoRound quantized model requires auto-round library (`pip install 'auto-round>=0.5'`)\"\n            )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            dtype = torch.bfloat16\n            logger.info(\"Loading the model in `torch.bfloat16`. To overwrite it, set `dtype` manually.\")\n        return dtype\n\n    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        if model.__class__.main_input_name != \"input_ids\":\n            logger.warning(\"AutoRound offers only limited support for models that are not strictly text-based.\")\n        from auto_round.inference.convert_model import convert_hf_model, infer_target_device\n\n        if self.pre_quantized:\n            target_device = infer_target_device(self.device_map)\n            model, used_backends = convert_hf_model(model, target_device)\n            self.used_backends = used_backends\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        if self.pre_quantized:\n            from auto_round.inference.convert_model import post_init\n\n            post_init(model, self.used_backends)\n        else:\n            raise ValueError(\"AutoRound only sports pre-quantized models.\")\n\n    @property\n    def is_trainable(self) -> bool:\n        return False\n\n    def is_serializable(self, safe_serialization=None):\n        ## for gptq/awq models, the quantization config will be changed\n        return True"
                },
                "component_dependencies": {
                    "AutoRoundQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_auto_round.py#logger",
                        "transformers/utils.py#is_auto_round_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_awq.py#AwqQuantizer": {
                "sorted_modules": {
                    "AwqQuantizer": "\n\nclass AwqQuantizer(HfQuantizer):\n    \"\"\"\n    4-bit quantization for Activation-aware Weight Quantization(AWQ) (https://huggingface.co/papers/2306.00978)\n    \"\"\"\n\n    # AWQ requires data calibration - we support only inference\n    requires_calibration = True\n\n    required_packages = [\"awq\", \"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n    def validate_environment(self, device_map, **kwargs):\n        if not is_auto_awq_available():\n            raise ImportError(\"Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)\")\n\n        if not is_accelerate_available():\n            raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n\n        if (\n            self.quantization_config.version == AWQLinearVersion.GEMM\n            and not torch.cuda.is_available()\n            and not torch.xpu.is_available()\n        ):\n            logger.warning_once(\"No CUDA or XPU found, consider switching to the IPEX version for CPU-only execution.\")\n            self.quantization_config.version = AWQLinearVersion.IPEX\n\n        if self.quantization_config.version == AWQLinearVersion.IPEX:\n            if version.parse(importlib.metadata.version(\"autoawq\")) < version.parse(\"0.2.6\"):\n                raise RuntimeError(\n                    \"To use IPEX backend, you need autoawq>0.2.6. Please install the latest version or from source.\"\n                )\n            if device_map is None:\n                logger.warning_once(\n                    \"You have loaded an AWQ model without setting device_map, please set 'cpu' or 'xpu' or 'auto'\"\n                )\n            elif isinstance(device_map, dict) and \"disk\" in device_map.values():\n                raise ValueError(\n                    \"You are attempting to load an IPEX version AWQ model with a device_map that contains disk device.\"\n                    \" This is not supported. Please make sure only cpu and xpu in the device_map.\"\n                )\n        else:\n            if not torch.cuda.is_available() and not torch.xpu.is_available():\n                raise RuntimeError(\n                    \"GPU is required to run AWQ quantized model. You can use IPEX version AWQ if you have an Intel CPU\"\n                )\n\n            if device_map is None:\n                logger.warning_once(\n                    \"You have loaded an AWQ model on CPU and have a CUDA/XPU device available, make sure to set \"\n                    \"your model on a GPU device in order to run your model.\"\n                )\n            elif device_map is not None:\n                if isinstance(device_map, dict) and any(\n                    forbidden in device_map.values() for forbidden in (\"cpu\", torch.device(\"cpu\"), \"disk\")\n                ):\n                    raise ValueError(\n                        \"You are attempting to load an AWQ model with a device_map that contains a CPU or disk device.\"\n                        \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n                    )\n\n    def update_dtype(self, dtype):\n        if dtype is None:\n            dtype = torch.float16\n            logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\")\n        elif dtype == torch.bfloat16 and (torch.cuda.is_available() or torch.xpu.is_available()):\n            logger.warning(\n                \"`torch.bfloat16` is not supported for AWQ CUDA/XPU kernels yet. Casting to `torch.float16`.\"\n            )\n            dtype = torch.float16\n        elif dtype != torch.float16 and (torch.cuda.is_available() or torch.xpu.is_available()):\n            logger.warning(\"We suggest you to set `dtype=torch.float16` for better efficiency on CUDA/XPU with AWQ.\")\n        return dtype\n\n    def _process_model_before_weight_loading(\n        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs\n    ):\n        from ..integrations import replace_quantization_scales, replace_with_awq_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules, add_default_skips=True\n        )\n\n        model, has_been_replaced = replace_with_awq_linear(\n            model, quantization_config=self.quantization_config, modules_to_not_convert=self.modules_to_not_convert\n        )\n\n        model = replace_quantization_scales(model, model.config.model_type)\n\n        if not has_been_replaced:\n            logger.warning(\n                \"You are loading an AWQ model but no linear modules were found in your model.\"\n                \" Please double check your model architecture, or submit an issue on github if you think this is a bug.\"\n            )\n\n    def _process_model_after_weight_loading(self, model, **kwargs):\n        if self.quantization_config.do_fuse:\n            from ..integrations import fuse_awq_modules\n\n            model = fuse_awq_modules(model, self.quantization_config)\n            model._awq_is_fused = True  # TODO: consider storing this flag in model.config instead\n\n        if self.quantization_config.version == AWQLinearVersion.EXLLAMA:\n            from ..integrations import post_init_awq_exllama_modules\n\n            model = post_init_awq_exllama_modules(model, self.quantization_config.exllama_config)\n\n        if self.quantization_config.version == AWQLinearVersion.IPEX:\n            from ..integrations import post_init_awq_ipex_modules\n\n            model = post_init_awq_ipex_modules(model)\n\n    def is_serializable(self, safe_serialization=None):\n        # AWQ through auto-awq has been always serializable, except if the model is fused.\n        if self.quantization_config.do_fuse:\n            logger.warning(\"You cannot save an AWQ model that uses fused modules!\")\n            return False\n\n        if self.quantization_config.version == AWQLinearVersion.EXLLAMA:\n            logger.warning(\"You cannot save an AWQ model that uses Exllama backend!\")\n            return False\n\n        return True\n\n    @property\n    def is_trainable(self):\n        # AWQ supports PEFT fine-tuning from version 0.2.0\n        MIN_AWQ_VERSION_FOR_PEFT = \"0.2.0\"\n        return version.parse(importlib.metadata.version(\"autoawq\")) >= version.parse(MIN_AWQ_VERSION_FOR_PEFT)"
                },
                "component_dependencies": {
                    "AwqQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_awq.py#logger",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_auto_awq_available",
                        "transformers/utils/quantization_config.py#AWQLinearVersion.EXLLAMA",
                        "transformers/utils/quantization_config.py#AWQLinearVersion.GEMM",
                        "transformers/utils/quantization_config.py#AWQLinearVersion.IPEX"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_bitnet.py#BitNetHfQuantizer": {
                "sorted_modules": {
                    "BitNetHfQuantizer": "\n\nclass BitNetHfQuantizer(HfQuantizer):\n    \"\"\"\n    1.58-bit quantization from BitNet quantization method:\n    Before loading: it converts the linear layers into BitLinear layers during loading.\n\n    Check out the paper introducing this method: https://huggingface.co/papers/2402.17764\n    \"\"\"\n\n    requires_parameters_quantization = False\n    requires_calibration = True\n\n    required_packages = [\"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_accelerate_available():\n            raise ImportError(\"Loading a BitNet quantized model requires accelerate (`pip install accelerate`)\")\n\n        if not torch.cuda.is_available():\n            logger.warning_once(\n                \"You don't have a GPU available to load the model, the inference will be slow because of weight unpacking\"\n            )\n            return\n\n        device_map = kwargs.get(\"device_map\")\n        if device_map is None:\n            logger.warning_once(\n                \"You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set \"\n                \"your model on a GPU device in order to run your model.\"\n            )\n        elif device_map is not None:\n            if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n                raise ValueError(\n                    \"You are attempting to load a BitNet model with a device_map that contains a CPU or disk device.\"\n                    \"This is not supported. Please remove the CPU or disk device from the device_map.\"\n                )\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_bitnet_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        model = replace_with_bitnet_linear(\n            model,\n            modules_to_not_convert=self.modules_to_not_convert,\n            quantization_config=self.quantization_config,\n            pre_quantized=self.pre_quantized,\n        )\n\n    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n        max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n        return max_memory\n\n    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n        target_dtype = torch.int8\n        return target_dtype\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return (\n            self.quantization_config.linear_class == \"autobitlinear\"\n            and self.quantization_config.quantization_mode == \"online\"\n        )\n\n    @property\n    def is_qat_trainable(self) -> bool:\n        \"\"\"Flag indicating whether the quantized model can carry out quantization aware training\"\"\"\n        return (\n            self.quantization_config.linear_class == \"autobitlinear\"\n            and self.quantization_config.quantization_mode == \"online\"\n        )"
                },
                "component_dependencies": {
                    "BitNetHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_bitnet.py#logger",
                        "transformers/utils.py#is_accelerate_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_bnb_4bit.py#Bnb4BitHfQuantizer": {
                "sorted_modules": {
                    "Bnb4BitHfQuantizer": "\n\nclass Bnb4BitHfQuantizer(HfQuantizer):\n    \"\"\"\n    4-bit quantization from bitsandbytes quantization method:\n        before loading: converts transformer layers into Linear4bit during loading: load 16bit weight and pass to the\n        layer object after: quantizes individual weights in Linear4bit into 4bit at the first .cuda() call\n        saving:\n            from state dict, as usual; saves weights and `quant_state` components\n        loading:\n            need to locate `quant_state` components and pass to Param4bit constructor\n    \"\"\"\n\n    use_keep_in_fp32_modules = True\n    requires_parameters_quantization = True\n    requires_calibration = False\n\n    required_packages = [\"bitsandbytes\", \"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n        if self.quantization_config.llm_int8_skip_modules is not None:\n            self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n\n        # This describes the additional items that are saved on the state dict (on the params themselves)\n        self.bnb_keys = [\n            f\"quant_state.bitsandbytes__{self.quantization_config.bnb_4bit_quant_type}\",\n            \"absmax\",\n            \"quant_map\",\n        ]\n        if self.quantization_config.bnb_4bit_use_double_quant:\n            self.bnb_keys.extend([\"nested_absmax\", \"nested_quant_map\"])\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_accelerate_available():\n            raise ImportError(\n                f\"Using `bitsandbytes` 4-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n            )\n        if not is_bitsandbytes_available():\n            raise ImportError(\n                f\"Using `bitsandbytes` 4-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n            )\n\n        from ..integrations import validate_bnb_backend_availability\n\n        validate_bnb_backend_availability(raise_exception=True)\n\n        device_map = kwargs.get(\"device_map\")\n        if (\n            device_map is not None\n            and isinstance(device_map, dict)\n            and not self.quantization_config.llm_int8_enable_fp32_cpu_offload\n        ):\n            device_map_without_lm_head = {\n                key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n            }\n            if set(device_map.values()) == {\"cpu\"}:\n                pass\n            elif \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n                raise ValueError(\n                    \"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\n                    \"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\n                    \"in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \"\n                    \"`from_pretrained`. Check \"\n                    \"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \"\n                    \"for more details. \"\n                )\n\n    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n        from accelerate.utils import CustomDtype\n\n        if target_dtype != torch.int8:\n            logger.info(\"target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\")\n        return CustomDtype.INT4\n\n    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n        return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.bnb_keys)]\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        import bitsandbytes as bnb\n\n        # They are on the params themselves, so we cannot easily extract the module from the name\n        if any(param_name.endswith(x) for x in self.bnb_keys):\n            return True\n        module, name = get_module_from_name(model, param_name)\n        return isinstance(module, bnb.nn.Linear4bit) and name != \"bias\"\n\n    def get_param_name(self, param_name: str) -> str:\n        \"\"\"\n        Get the right param_name in order to get the module associated with the param.\n        This is useful for quantized stats lile absmax or quant_map as we need to update the param_name to get the module as they are stored in ...weight.absmax.\n        \"\"\"\n        if self.pre_quantized:\n            # We need to get the param name of quantized weights and not its components. Otherwise, we won't be able to get the nn.Module associated.\n            if any(param_name.endswith(x) for x in self.bnb_keys):\n                param_name = (\n                    param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n                )\n        return param_name\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        import bitsandbytes as bnb\n\n        full_name = param_name\n\n        # update param name to get the weights instead of the quantized stats\n        param_name = self.get_param_name(param_name)\n        module, tensor_name = get_module_from_name(model, param_name)\n\n        # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\n        if isinstance(target_device, int) and is_torch_npu_available():\n            target_device = f\"npu:{target_device}\"\n\n        # construct `new_value` for the module._parameters[tensor_name]\n        if self.pre_quantized:\n            module_name = param_name.rsplit(\".\", 1)[0]\n            # Save the states for later quantization when they are all gathered\n            if not hasattr(self, \"param_quant_stats\"):\n                self.param_quant_stats = defaultdict(dict)\n            self.param_quant_stats[module_name].update({full_name: param_value})\n\n            # We are ready for quantization in this case (note, the +1 is for the weight itself)\n            if len(self.param_quant_stats[module_name]) == len(self.bnb_keys) + 1:\n                param_kwargs = {}\n                if self.is_bnb_supports_quant_storage_module:\n                    param_kwargs[\"module\"] = module\n\n                weight = self.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n                new_value = bnb.nn.Params4bit.from_prequantized(\n                    data=weight,\n                    quantized_stats=self.param_quant_stats[module_name],\n                    requires_grad=False,\n                    device=target_device,\n                    **param_kwargs,\n                )\n                # Set it\n                module._parameters[tensor_name] = new_value\n                # Delete the states\n                del self.param_quant_stats[module_name]\n        else:\n            new_value = param_value.to(\"cpu\")\n            old_value = getattr(module, tensor_name)\n\n            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n            if issubclass(module.source_cls, Conv1D):\n                new_value = new_value.T\n\n            kwargs = old_value.__dict__\n            kwargs.pop(\"_is_hf_initialized\", None)\n            new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)\n\n            module._parameters[tensor_name] = new_value\n\n    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.adjust_max_memory\n    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n        # need more space for buffers that are created during quantization\n        max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n        return max_memory\n\n    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.update_dtype\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n            logger.info(\n                \"Overriding dtype=%s with `dtype=torch.float16` due to \"\n                \"requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n                \" dtype=torch.float16 to remove this warning.\",\n                dtype,\n            )\n            dtype = torch.float16\n        return dtype\n\n    def update_device_map(self, device_map):\n        if device_map is None:\n            if torch.cuda.is_available():\n                device_map = {\"\": torch.cuda.current_device()}\n            elif is_torch_npu_available():\n                device_map = {\"\": f\"npu:{torch.npu.current_device()}\"}\n            elif is_torch_hpu_available():\n                device_map = {\"\": f\"hpu:{torch.hpu.current_device()}\"}\n            elif is_torch_xpu_available():\n                device_map = {\"\": torch.xpu.current_device()}\n            else:\n                device_map = {\"\": \"cpu\"}\n            logger.info(\n                \"The device_map was not initialized. \"\n                f\"Setting device_map to {device_map}. \"\n                \"If you want to use the model for inference, please set device_map ='auto' \"\n            )\n        return device_map\n\n    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer._process_model_before_weight_loading\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        device_map,\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_bnb_linear\n\n        llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n        )\n\n        # Extend `self.modules_to_not_convert` to keys that are supposed to be offloaded to `cpu` or `disk`\n        if isinstance(device_map, dict) and len(device_map.keys()) > 1:\n            keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n\n            if len(keys_on_cpu) > 0 and not llm_int8_enable_fp32_cpu_offload:\n                raise ValueError(\n                    \"If you want to offload some keys to `cpu` or `disk`, you need to set \"\n                    \"`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \"\n                    \" converted to 8-bit but kept in 32-bit.\"\n                )\n            self.modules_to_not_convert.extend(keys_on_cpu)\n\n        model = replace_with_bnb_linear(\n            model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n        )\n\n        model.config.quantization_config = self.quantization_config\n\n    # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer._process_model_after_weight_loading with 8bit->4bit\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        model.is_loaded_in_4bit = True\n        model.is_4bit_serializable = self.is_serializable()\n        return model\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @cached_property\n    def is_bnb_supports_quant_storage_module(self) -> bool:\n        \"\"\"\n        determines if the current version of bitsandbytes supports\n        the `module` parameter in `Params4bit.from_prequantized`\n        :return:\n        \"\"\"\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return True\n\n    def _dequantize(self, model):\n        from ..integrations import dequantize_and_replace\n\n        model = dequantize_and_replace(\n            model, self.modules_to_not_convert, quantization_config=self.quantization_config\n        )\n        return model"
                },
                "component_dependencies": {
                    "Bnb4BitHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_bnb_4bit.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#ACCELERATE_MIN_VERSION",
                        "transformers/utils.py#BITSANDBYTES_MIN_VERSION",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_bitsandbytes_available",
                        "transformers/utils.py#is_torch_hpu_available",
                        "transformers/utils.py#is_torch_npu_available",
                        "transformers/utils.py#is_torch_xpu_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_bnb_8bit.py#Bnb8BitHfQuantizer": {
                "sorted_modules": {
                    "Bnb8BitHfQuantizer": "\n\nclass Bnb8BitHfQuantizer(HfQuantizer):\n    \"\"\"\n    8-bit quantization from bitsandbytes quantization method:\n        before loading: converts transformer layers into Linear8bitLt during loading: load 16bit weight and pass to the\n        layer object after: quantizes individual weights in Linear8bitLt into 8bit at fitst .cuda() call\n    saving:\n        from state dict, as usual; saves weights and 'SCB' component\n    loading:\n        need to locate SCB component and pass to the Linear8bitLt object\n    \"\"\"\n\n    use_keep_in_fp32_modules = True\n    requires_parameters_quantization = True\n    requires_calibration = False\n\n    required_packages = [\"bitsandbytes\", \"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n        if self.quantization_config.llm_int8_skip_modules is not None:\n            self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_accelerate_available():\n            raise ImportError(\n                f\"Using `bitsandbytes` 8-bit quantization requires accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n            )\n        if not is_bitsandbytes_available():\n            raise ImportError(\n                f\"Using `bitsandbytes` 8-bit quantization requires bitsandbytes: `pip install -U bitsandbytes>={BITSANDBYTES_MIN_VERSION}`\"\n            )\n\n        from ..integrations import validate_bnb_backend_availability\n\n        validate_bnb_backend_availability(raise_exception=True)\n\n        device_map = kwargs.get(\"device_map\")\n        if (\n            device_map is not None\n            and isinstance(device_map, dict)\n            and not self.quantization_config.llm_int8_enable_fp32_cpu_offload\n        ):\n            device_map_without_lm_head = {\n                key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n            }\n            if set(device_map.values()) == {\"cpu\"}:\n                pass\n            elif \"cpu\" in device_map_without_lm_head.values() or \"disk\" in device_map_without_lm_head.values():\n                raise ValueError(\n                    \"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\n                    \"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\n                    \"in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \"\n                    \"`from_pretrained`. Check \"\n                    \"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \"\n                    \"for more details. \"\n                )\n\n    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n        # need more space for buffers that are created during quantization\n        max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n        return max_memory\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n            logger.info(\n                \"Overriding dtype=%s with `dtype=torch.float16` due to \"\n                \"requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n                \" dtype=torch.float16 to remove this warning.\",\n                dtype,\n            )\n            dtype = torch.float16\n        return dtype\n\n    def update_device_map(self, device_map):\n        if device_map is None:\n            if torch.cuda.is_available():\n                device_map = {\"\": torch.cuda.current_device()}\n            elif is_torch_xpu_available():\n                device_map = {\"\": torch.xpu.current_device()}\n            else:\n                device_map = {\"\": \"cpu\"}\n            logger.info(\n                \"The device_map was not initialized. \"\n                f\"Setting device_map to {device_map}. \"\n                \"If you want to use the model for inference, please set device_map ='auto' \"\n            )\n        return device_map\n\n    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if target_dtype != torch.int8:\n            logger.info(\"target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\")\n        return torch.int8\n\n    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n        bnb_keys = [\"SCB\", \"weight_format\"]\n        return [k for k in unexpected_keys if not any(k.endswith(x) for x in bnb_keys)]\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        import bitsandbytes as bnb\n\n        module, name = get_module_from_name(model, param_name)\n        return isinstance(module, bnb.nn.Linear8bitLt) and name != \"bias\"\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        import bitsandbytes as bnb\n\n        module, tensor_name = get_module_from_name(model, param_name)\n\n        if self.pre_quantized and not self.is_serializable():\n            raise ValueError(\n                \"Detected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \"\n                \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n            )\n        # Those 2 can only happen when self.pre_quantized == True\n        if tensor_name == \"SCB\":\n            setattr(module.weight, \"SCB\", param_value.to(target_device))\n            return\n        # It's not used, but it's getting serialized for BC reason...\n        elif tensor_name == \"weight_format\":\n            return\n\n        # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n        # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n        if issubclass(module.source_cls, Conv1D) and not self.pre_quantized:\n            param_value = param_value.T\n\n        old_value = getattr(module, tensor_name)\n        kwargs = old_value.__dict__\n        kwargs.pop(\"_is_hf_initialized\", None)\n        # Need to pop SCB and reset it because of bnb internals that modifies its value when switching devices ...\n        SCB = kwargs.pop(\"SCB\", None)\n        new_value = bnb.nn.Int8Params(param_value.to(\"cpu\"), requires_grad=False, **kwargs).to(target_device)\n        if SCB is not None:\n            setattr(new_value, \"SCB\", SCB)\n        # Set it to the module\n        module._parameters[tensor_name] = new_value\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        model.is_loaded_in_8bit = True\n        model.is_8bit_serializable = self.is_serializable()\n        return model\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        device_map,\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_bnb_linear\n\n        llm_int8_enable_fp32_cpu_offload = self.quantization_config.llm_int8_enable_fp32_cpu_offload\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n        )\n\n        # Extend `self.modules_to_not_convert` to keys that are supposed to be offloaded to `cpu` or `disk`\n        if isinstance(device_map, dict) and len(device_map.keys()) > 1:\n            keys_on_cpu = [key for key, value in device_map.items() if value in [\"disk\", \"cpu\"]]\n\n            if len(keys_on_cpu) > 0 and not llm_int8_enable_fp32_cpu_offload:\n                raise ValueError(\n                    \"If you want to offload some keys to `cpu` or `disk`, you need to set \"\n                    \"`llm_int8_enable_fp32_cpu_offload=True`. Note that these modules will not be \"\n                    \" converted to 8-bit but kept in 32-bit.\"\n                )\n            self.modules_to_not_convert.extend(keys_on_cpu)\n\n        model = replace_with_bnb_linear(\n            model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n        )\n\n        model.config.quantization_config = self.quantization_config\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return True\n\n    def _dequantize(self, model):\n        from ..integrations import dequantize_and_replace\n\n        model = dequantize_and_replace(\n            model, self.modules_to_not_convert, quantization_config=self.quantization_config\n        )\n        return model"
                },
                "component_dependencies": {
                    "Bnb8BitHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_bnb_8bit.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#ACCELERATE_MIN_VERSION",
                        "transformers/utils.py#BITSANDBYTES_MIN_VERSION",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_bitsandbytes_available",
                        "transformers/utils.py#is_torch_xpu_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_compressed_tensors.py#CompressedTensorsHfQuantizer": {
                "sorted_modules": {
                    "CompressedTensorsHfQuantizer": "\n\nclass CompressedTensorsHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer for the compressed_tensors package.  Loads and restores models to\n    quantized state with compressed_tensors\n    \"\"\"\n\n    requires_calibration = True\n    required_packages = [\"compressed_tensors\"]\n\n    def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n        if not is_compressed_tensors_available():\n            raise ImportError(\n                \"Using `compressed_tensors` quantized models requires the compressed-tensors library: \"\n                \"`pip install compressed-tensors`\"\n            )\n\n        # Call post_init here to ensure proper config setup when `run_compressed`\n        # is provided directly via CompressedTensorsConfig, and to avoid duplicate logging.\n\n        quantization_config.post_init()\n        from compressed_tensors.compressors import ModelCompressor\n\n        self.compressor = ModelCompressor.from_compression_config(quantization_config)\n        self.run_compressed = quantization_config.run_compressed\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_compressed_tensors_available():\n            raise ImportError(\n                \"Using `compressed_tensors` quantized models requires the compressed-tensors library: \"\n                \"`pip install compressed-tensors`\"\n            )\n        if not is_torch_available():\n            # torch already should be installed as part of compressed tensors\n            raise ImportError(\"torch is required for using compressed-tensors quantization\")\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            logger.info(\"Loading model using torch.float16 for compressed-tensors quantization\")\n            dtype = torch.float16\n        elif dtype != torch.float16:\n            logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with compressed_tensors.\")\n        return dtype\n\n    def _process_model_before_weight_loading(self, model, **kwargs):\n        from compressed_tensors.quantization import apply_quantization_config\n\n        ct_quantization_config = self.compressor.quantization_config\n\n        # Always initialize compressed wrappers to match the checkpoint\n        apply_quantization_config(model, ct_quantization_config, self.run_compressed)\n        if (\n            self.quantization_config.is_quantization_compressed\n            or self.quantization_config.is_sparsification_compressed\n        ):\n            self.compressor.compress_model(model=model)\n\n    def _process_model_after_weight_loading(self, model, **kwargs):\n        \"\"\"Decompress loaded model if necessary - need for qat\"\"\"\n\n        if (\n            self.quantization_config.is_quantization_compressed and not self.run_compressed\n        ) or self.quantization_config.is_sparsification_compressed:\n            self.compressor.decompress_model(model=model)\n\n    def update_tp_plan(self, config):\n        additional_plan = {\n            \"layers.*.feed_forward.experts.*.gate_proj.weight\": \"local_colwise\",\n            \"layers.*.feed_forward.experts.*.gate_proj.weight_scale\": \"local_colwise\",\n            \"layers.*.feed_forward.experts.*.up_proj.weight\": \"local_colwise\",\n            \"layers.*.feed_forward.experts.*.up_proj.weight_scale\": \"local_colwise\",\n            \"layers.*.feed_forward.experts.*.down_proj.weight\": \"local_rowwise\",\n        }\n        if config.get_text_config() is not None and config.get_text_config().base_model_tp_plan is not None:\n            config.get_text_config().base_model_tp_plan.update(additional_plan)\n\n        return config\n\n    @property\n    def is_trainable(self):\n        return True\n\n    def is_qat_trainable(self) -> bool:\n        \"\"\"Loaded Models can carry out quantization aware training\"\"\"\n        # models need to be decompressed carry out qat\n        return not self.run_compressed or not self.quantization_config.is_quantization_compressed\n\n    def is_serializable(self, safe_serialization=None) -> bool:\n        \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\"\n        return True"
                },
                "component_dependencies": {
                    "CompressedTensorsHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_compressed_tensors.py#logger",
                        "transformers/utils.py#is_compressed_tensors_available",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils/quantization_config.py#CompressedTensorsConfig"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_eetq.py#EetqHfQuantizer": {
                "sorted_modules": {
                    "EetqHfQuantizer": "\n\nclass EetqHfQuantizer(HfQuantizer):\n    \"\"\"\n    8-bit quantization from EETQ quantization method:\n        before loading: converts transformer layers into W8A16Linear during loading: load 16bit weight and pass to the\n        layer object after: quantizes individual weights in Linear8bitLt into 8bit at first .cuda() call\n    \"\"\"\n\n    requires_parameters_quantization = True\n    requires_calibration = False\n\n    required_packages = [\"eetq\", \"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_eetq_available():\n            raise ImportError(\n                \"Using `eetq` 8-bit quantization requires eetq.\"\n                \"Please install the latest version of eetq from : https://github.com/NetEase-FuXi/EETQ\"\n            )\n\n        try:\n            import eetq  # noqa: F401\n        except ImportError as exc:\n            if \"shard_checkpoint\" in str(exc):\n                # EETQ 1.0.0 is currently broken with the latest transformers because it tries to import the removed\n                # shard_checkpoint function, see https://github.com/NetEase-FuXi/EETQ/issues/34.\n                # TODO: Update message once eetq releases a fix\n                raise ImportError(\n                    \"You are using a version of EETQ that is incompatible with the current transformers version. \"\n                    \"Either downgrade transformers to <= v4.46.3 or, if available, upgrade EETQ to > v1.0.0.\"\n                ) from exc\n            else:\n                raise\n\n        if not is_accelerate_available():\n            raise ImportError(\"Loading an EETQ quantized model requires accelerate (`pip install accelerate`)\")\n\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n\n        device_map = kwargs.get(\"device_map\")\n        if device_map is None:\n            logger.warning_once(\n                \"You have loaded an EETQ model on CPU and have a CUDA device available, make sure to set \"\n                \"your model on a GPU device in order to run your model.\"\n            )\n        elif device_map is not None:\n            if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n                raise ValueError(\n                    \"You are attempting to load an EETQ model with a device_map that contains a CPU or disk device.\"\n                    \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n                )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            dtype = torch.float16\n            logger.info(\n                \"Overriding dtype=%s with `dtype=torch.float16` due to \"\n                \"requirements of `eetq` to enable model loading in 8-bit. \"\n                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n                \" dtype=torch.float16 to remove this warning.\",\n                dtype,\n            )\n        elif dtype != torch.float16:\n            logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with EETQ.\")\n        return dtype\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        from eetq import EetqLinear\n\n        module, tensor_name = get_module_from_name(model, param_name)\n\n        if isinstance(module, EetqLinear):\n            if self.pre_quantized or tensor_name == \"bias\":\n                return False\n            else:\n                return True\n        return False\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        from eetq import EetqLinear, quantize_and_preprocess_weights\n\n        module, tensor_name = get_module_from_name(model, param_name)\n        new_value, weight_scale = quantize_and_preprocess_weights(param_value)\n\n        # Samity check\n        if isinstance(module, EetqLinear):\n            if self.pre_quantized or tensor_name == \"bias\":\n                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n            else:\n                if tensor_name == \"weight_scale\":\n                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n\n        module._buffers[tensor_name] = new_value.to(target_device)\n        module.register(\"weight_scales\", weight_scale.to(target_device))\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_eetq_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        model = replace_with_eetq_linear(\n            model,\n            modules_to_not_convert=self.modules_to_not_convert,\n            quantization_config=self.quantization_config,\n            pre_quantized=self.pre_quantized,\n        )\n\n        model.config.quantization_config = self.quantization_config\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return True"
                },
                "component_dependencies": {
                    "EetqHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_eetq.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_eetq_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_fbgemm_fp8.py#FbgemmFp8HfQuantizer": {
                "sorted_modules": {
                    "FbgemmFp8HfQuantizer": "\n\nclass FbgemmFp8HfQuantizer(HfQuantizer):\n    \"\"\"\n    FP8 quantization using fbgemm kernels\n    \"\"\"\n\n    requires_parameters_quantization = True\n    requires_calibration = False\n\n    required_packages = [\"fbgemm-gpu\", \"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_torch_available():\n            raise ImportError(\n                \"Using fbgemm fp8 quantization requires torch >= 2.1.0\"\n                \"Please install the latest version of torch ( pip install --upgrade torch )\"\n            )\n        if not is_fbgemm_gpu_available():\n            raise ImportError(\n                \"Using fbgemm fp8 quantization requires fbgemm-gpu library\"\n                \"Please install the latest version of fbgemm-gpu library by following : https://pytorch.org/FBGEMM/fbgemm_gpu-development/InstallationInstructions.html#fbgemm-gpu-install-libraries\"\n            )\n\n        if not is_accelerate_available():\n            raise ImportError(\n                \"Loading an FP8 quantized model requires accelerate (`pip install --upgrade accelerate`)\"\n            )\n\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"Using FP8 quantized models with fbgemm kernels requires a GPU\")\n\n        compute_capability = torch.cuda.get_device_capability()\n        major, minor = compute_capability\n        if major < 9:\n            raise ValueError(\n                \"FP8 quantized models is only supported on GPUs with compute capability >= 9.0 (e.g H100)\"\n            )\n\n        device_map = kwargs.get(\"device_map\")\n        if device_map is None:\n            logger.warning_once(\n                \"You have loaded an FP8 model on CPU and have a CUDA device available, make sure to set \"\n                \"your model on a GPU device in order to run your model. To remove this warning, pass device_map = 'cuda'. \"\n            )\n        elif device_map is not None:\n            if (\n                not self.pre_quantized\n                and isinstance(device_map, dict)\n                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n            ):\n                raise ValueError(\n                    \"You are attempting to load an FP8 model with a device_map that contains a CPU or disk device.\"\n                    \"This is not supported when the model is quantized on the fly. \"\n                    \"Please use a quantized checkpoint or remove the CPU or disk device from the device_map.\"\n                )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            dtype = torch.bfloat16\n            logger.info(\n                \"Overriding dtype=%s with `dtype=torch.bloat16` due to \"\n                \"requirements of `fbgemm-gpu` to enable model loading in fp8. \"\n                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n                \" dtype=torch.bfloat16 to remove this warning.\",\n                dtype,\n            )\n        elif dtype == torch.float16:\n            raise ValueError(\n                \"You cannot use FP8 with dtype=torch.float16.We recommend you passing dtype=torch.bfloat16\"\n            )\n        return dtype\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n\n        module, tensor_name = get_module_from_name(model, param_name)\n\n        if isinstance(module, FbgemmFp8Linear):\n            if self.pre_quantized or tensor_name == \"bias\":\n                return False\n            else:\n                return True\n        if isinstance(module, FbgemmFp8Llama4TextExperts):\n            if self.pre_quantized or tensor_name == \"bias\":\n                return False\n            else:\n                return True\n        return False\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n\n        module, tensor_name = get_module_from_name(model, param_name)\n\n        # Sanity checks\n        if isinstance(module, FbgemmFp8Linear):\n            if self.pre_quantized or tensor_name == \"bias\":\n                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n            else:\n                if tensor_name == \"weight_scale\":\n                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n        if isinstance(module, FbgemmFp8Llama4TextExperts):\n            if not (self.pre_quantized or tensor_name == \"bias\"):\n                if tensor_name == \"gate_up_proj_scale\" or tensor_name == \"down_proj_scale\":\n                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n\n        if isinstance(module, FbgemmFp8Llama4TextExperts):\n            if tensor_name == \"gate_up_proj\":\n                # Process each expert separately\n                # Transpose the second and third dimension\n                transposed_param = param_value.transpose(1, 2)\n\n                # Reshape to 2D for quantization\n                original_shape = transposed_param.shape\n                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n\n                # Quantize using per row instead of per column\n                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n\n                # Reshape back to original dimensions\n                new_value = new_value_flat.reshape(original_shape)\n                new_value = new_value.transpose(1, 2)\n                weight_scale = weight_scale_flat.reshape(original_shape[0], 1, original_shape[1])\n            elif tensor_name == \"down_proj\":\n                # Process each expert separately\n                # Transpose the weights for proper quantization\n                transposed_param = param_value.transpose(1, 2)\n\n                # Reshape to 2D for quantization\n                original_shape = transposed_param.shape\n                flattened_param = transposed_param.reshape(-1, original_shape[-1])\n\n                # Quantize using per column\n                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n\n                # Reshape back to original dimensions\n                new_value = new_value_flat.reshape(original_shape)\n                new_value = new_value.transpose(1, 2)\n                weight_scale = weight_scale_flat.reshape(original_shape[0], original_shape[1], 1)\n\n            module._parameters[f\"{tensor_name}_scale\"] = torch.nn.Parameter(weight_scale.to(target_device))\n        else:\n            new_value, weight_scale = torch.ops.fbgemm.quantize_fp8_per_row(param_value)\n            module._parameters[f\"{tensor_name}_scale\"] = torch.nn.Parameter(\n                weight_scale.view(weight_scale.shape[0], 1).to(target_device)\n            )\n\n        module._parameters[tensor_name] = torch.nn.Parameter(new_value.to(target_device))\n\n        del param_name\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_fbgemm_fp8_linear\n\n        tp_plan = model._tp_plan\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        config = model.config\n        model = replace_with_fbgemm_fp8_linear(\n            model,\n            modules_to_not_convert=self.modules_to_not_convert,\n            quantization_config=self.quantization_config,\n            pre_quantized=self.pre_quantized,\n            config=config,\n            tp_plan=tp_plan,\n        )\n\n        model.config.quantization_config = self.quantization_config\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n\n        not_missing_keys = []\n        for name, module in model.named_modules():\n            if isinstance(module, (FbgemmFp8Linear, FbgemmFp8Llama4TextExperts)):\n                for missing in missing_keys:\n                    if (\n                        (name in missing or name in f\"{prefix}.{missing}\")\n                        and not missing.endswith(\".weight\")\n                        and not missing.endswith(\".bias\")\n                    ):\n                        not_missing_keys.append(missing)\n        return [k for k in missing_keys if k not in not_missing_keys]\n\n    def update_tp_plan(self, config):\n        if \"Llama4\" in config.__class__.__name__:\n            text_plan = {\n                # We are using a different tp plan with local_colwise and local_rowwise for the attention because fbgemm operations cannot be parallelized\n                # With local_colwise and local_rowwise, all the operations are done locally, and we add a gather operation to gather the results instead of\n                # using dtensors\n                \"layers.*.self_attn.q_proj.weight\": \"local_colwise\",\n                \"layers.*.self_attn.q_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.self_attn.k_proj.weight\": \"local_colwise\",\n                \"layers.*.self_attn.k_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.self_attn.v_proj.weight\": \"local_colwise\",\n                \"layers.*.self_attn.v_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.self_attn.o_proj.weight\": \"local_rowwise\",\n                \"layers.*.self_attn\": \"gather\",\n                # We keep the same sequence_parallel plan for layernorms\n                \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n                \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n                \"norm.weight\": \"sequence_parallel\",\n                # We keep the same local_colwise and local_rowwise plan for the feed forward shared expert\n                # We also add scales for the shared expert, for local_colwise the scale is also local_colwise\n                # For local_rowwise the scale is replicated, so we don't need to add it\n                \"layers.*.feed_forward.shared_expert.gate_proj.weight\": \"local_colwise\",\n                \"layers.*.feed_forward.shared_expert.gate_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.feed_forward.shared_expert.up_proj.weight\": \"local_colwise\",\n                \"layers.*.feed_forward.shared_expert.up_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.feed_forward.shared_expert.down_proj.weight\": \"local_rowwise\",\n                \"layers.*.feed_forward.experts\": \"local\",\n                \"layers.*.feed_forward\": \"gather\",\n                \"layers.*.feed_forward.experts.*.gate_proj.weight\": \"local_colwise\",\n                \"layers.*.feed_forward.experts.*.gate_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.feed_forward.experts.*.up_proj.weight\": \"local_colwise\",\n                \"layers.*.feed_forward.experts.*.up_proj.weight_scale\": \"local_colwise\",\n                \"layers.*.feed_forward.experts.*.down_proj.weight\": \"local_rowwise\",\n                # For Fused implementation we use local_packed_rowwise for the gate_up_proj, and the same for the packed scales\n                # We use local_colwise for the down_proj, and the scales are replicated so we don't add them\n                \"layers.*.feed_forward.experts.gate_up_proj\": \"local_packed_rowwise\",\n                \"layers.*.feed_forward.experts.gate_up_proj_scale\": \"local_packed_rowwise\",\n                \"layers.*.feed_forward.experts.down_proj\": \"local_colwise\",\n            }\n            if config.get_text_config() is not None:\n                config.get_text_config().base_model_tp_plan = text_plan\n            else:\n                config.base_model_tp_plan = text_plan\n            return config\n\n        return config\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return False"
                },
                "component_dependencies": {
                    "FbgemmFp8HfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_fbgemm_fp8.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_fbgemm_gpu_available",
                        "transformers/utils.py#is_torch_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_finegrained_fp8.py#FineGrainedFP8HfQuantizer": {
                "sorted_modules": {
                    "FineGrainedFP8HfQuantizer": "\n\nclass FineGrainedFP8HfQuantizer(HfQuantizer):\n    \"\"\"\n    FP8 quantization implementation supporting both standard and MoE models.\n    Supports both e4m3fn formats based on platform.\n    \"\"\"\n\n    requires_parameters_quantization = True\n    requires_calibration = False\n    required_packages = [\"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_torch_available():\n            raise ImportError(\n                \"Using fp8 quantization requires torch >= 2.1.0\"\n                \"Please install the latest version of torch ( pip install --upgrade torch )\"\n            )\n\n        if not is_accelerate_available():\n            raise ImportError(\"Loading an FP8 quantized model requires accelerate (`pip install accelerate`)\")\n\n        if not (torch.cuda.is_available() or is_torch_xpu_available()):\n            raise RuntimeError(\"No GPU or XPU found. A GPU or XPU is needed for FP8 quantization.\")\n\n        if torch.cuda.is_available():\n            compute_capability = torch.cuda.get_device_capability()\n            major, minor = compute_capability\n            if (major < 8) or (major == 8 and minor < 9):\n                raise ValueError(\n                    \"FP8 quantized models is only supported on GPUs with compute capability >= 8.9 (e.g 4090/H100)\"\n                    f\", actual = `{major}.{minor}`\"\n                )\n\n        device_map = kwargs.get(\"device_map\")\n        if device_map is None:\n            logger.warning_once(\n                \"You have loaded an FP8 model on CPU and have a CUDA or XPU device available, make sure to set \"\n                \"your model on a GPU or XPU device in order to run your model. To remove this warning, \"\n                \"pass device_map = 'cuda' or 'xpu'. \"\n            )\n        elif device_map is not None:\n            if (\n                not self.pre_quantized\n                and isinstance(device_map, dict)\n                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n            ):\n                raise ValueError(\n                    \"You are attempting to load an FP8 model with a device_map that contains a cpu/disk device.\"\n                    \"This is not supported when the model is quantized on the fly. \"\n                    \"Please use a quantized checkpoint or remove the cpu/disk device from the device_map.\"\n                )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            logger.info(\"Setting dtype to torch.float32 as no dtype was specified in from_pretrained\")\n            dtype = torch.float32\n        return dtype\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        from ..integrations.finegrained_fp8 import FP8Linear\n        from ..modeling_utils import _load_parameter_into_model\n\n        # Sanity checks\n        module, tensor_name = get_module_from_name(model, param_name)\n        if isinstance(module, FP8Linear):\n            if self.pre_quantized or tensor_name == \"bias\":\n                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n            else:\n                if tensor_name == \"weight_scale_inv\":\n                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n\n        param_value = param_value.to(target_device)\n\n        # Get FP8 min/max values\n        fp8_min = torch.finfo(torch.float8_e4m3fn).min\n        fp8_max = torch.finfo(torch.float8_e4m3fn).max\n\n        block_size_m, block_size_n = self.quantization_config.weight_block_size\n\n        rows, cols = param_value.shape[-2:]\n\n        if rows % block_size_m != 0 or cols % block_size_n != 0:\n            raise ValueError(\n                f\"Matrix dimensions ({rows}, {cols}) must be divisible by block sizes ({block_size_m}, {block_size_n})\"\n            )\n        param_value_orig_shape = param_value.shape\n\n        param_value = param_value.reshape(\n            -1, rows // block_size_m, block_size_m, cols // block_size_n, block_size_n\n        ).permute(0, 1, 3, 2, 4)\n\n        # Calculate scaling factor for each block\n        max_abs = torch.amax(torch.abs(param_value), dim=(-1, -2))\n        scale = fp8_max / max_abs\n        scale_orig_shape = scale.shape\n        scale = scale.unsqueeze(-1).unsqueeze(-1)\n\n        # Quantize the weights\n        quantized_param = torch.clamp(param_value * scale, min=fp8_min, max=fp8_max).to(torch.float8_e4m3fn)\n\n        quantized_param = quantized_param.permute(0, 1, 3, 2, 4)\n        # Reshape back to matrix shape\n        quantized_param = quantized_param.reshape(param_value_orig_shape)\n\n        # Reshape scale to match the number of blocks\n        scale = scale.reshape(scale_orig_shape).squeeze().reciprocal()\n\n        # Load into the model\n        _load_parameter_into_model(model, param_name, quantized_param)\n        _load_parameter_into_model(model, param_name.rsplit(\".\", 1)[0] + \".weight_scale_inv\", scale)\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        from ..integrations.finegrained_fp8 import FP8Linear\n\n        module, tensor_name = get_module_from_name(model, param_name)\n        if isinstance(module, FP8Linear):\n            if self.pre_quantized or tensor_name == \"bias\":\n                return False\n            else:\n                return True\n        return False\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations.finegrained_fp8 import replace_with_fp8_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        model = replace_with_fp8_linear(\n            model,\n            modules_to_not_convert=self.modules_to_not_convert,\n            quantization_config=self.quantization_config,\n        )\n\n        model.config.quantization_config = self.quantization_config\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        from ..integrations import FP8Linear\n\n        not_missing_keys = []\n        for name, module in model.named_modules():\n            if isinstance(module, FP8Linear):\n                for missing in missing_keys:\n                    if (\n                        (name in missing or name in f\"{prefix}.{missing}\")\n                        and not missing.endswith(\".weight\")\n                        and not missing.endswith(\".bias\")\n                    ):\n                        not_missing_keys.append(missing)\n        return [k for k in missing_keys if k not in not_missing_keys]\n\n    def update_tp_plan(self, config):\n        if \"Qwen3\" in config.__class__.__name__:\n            text_plan = {\n                \"layers.*.self_attn.q_proj.weight\": \"local_colwise\",\n                \"layers.*.self_attn.q_proj.weight_scale_inv\": \"local_colwise\",\n                \"layers.*.self_attn.k_proj.weight\": \"local_colwise\",\n                \"layers.*.self_attn.k_proj.weight_scale_inv\": \"local_colwise\",\n                \"layers.*.self_attn.v_proj.weight\": \"local_colwise\",\n                \"layers.*.self_attn.v_proj.weight_scale_inv\": \"local_colwise\",\n                \"layers.*.self_attn.o_proj.weight\": \"local_rowwise\",\n                \"layers.*.self_attn.o_proj.weight_scale_inv\": \"local_rowwise\",\n                \"layers.*.self_attn\": \"gather\",\n                \"layers.*.mlp.gate_proj.weight\": \"local_colwise\",\n                \"layers.*.mlp.gate_proj.weight_scale_inv\": \"local_colwise\",\n                \"layers.*.mlp.up_proj.weight\": \"local_colwise\",\n                \"layers.*.mlp.up_proj.weight_scale_inv\": \"local_colwise\",\n                \"layers.*.mlp.down_proj.weight\": \"local_rowwise\",\n                \"layers.*.mlp.down_proj.weight_scale_inv\": \"local_rowwise\",\n                \"layers.*.mlp\": \"gather\",\n            }\n\n            config.base_model_tp_plan = text_plan\n\n        return config\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return False\n\n    def get_accelerator_warm_up_factor(self):\n        # Pre-processing is done cleanly, so we can allocate everything here\n        return 2"
                },
                "component_dependencies": {
                    "FineGrainedFP8HfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_finegrained_fp8.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils.py#is_torch_xpu_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_fp_quant.py#FPQuantHfQuantizer": {
                "sorted_modules": {
                    "FPQuantHfQuantizer": "\n\nclass FPQuantHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer for the FP-Quant method. Enables the loading of prequantized models and in-flight quantization of full-precision models.\n    \"\"\"\n\n    requires_calibration = False\n    requires_parameters_quantization = True\n    is_qat_trainable = True\n    required_packages = [\"fp_quant\"]\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, device_map, **kwargs):\n        if not torch.cuda.is_available():\n            raise NotImplementedError(\n                \"FPQuant quantization is only supported on GPU. Please use a different quantizer.\"\n            )\n\n        if not is_qutlass_available() and not self.quantization_config.pseudoquantization:\n            raise ImportError(\n                \"Using `fp_quant` with real quantization requires a **Blackwell GPU** and qutlass: `git clone https://github.com/IST-DASLab/qutlass.git && cd qutlass && pip install --no-build-isolation .`. You can use `FPQuantConfig(pseudoquantization=True, ...)` to use Triton-based pseudo-quantization. It doesn't provide any speedups but emulates the quantization behavior of the real quantization.\"\n            )\n\n        if self.quantization_config.pseudoquantization:\n            logger.warning(\n                \"Using pseudo-quantization for FP-Quant. This doesn't provide any speedups but emulates the quantization behavior of the real quantization.\"\n            )\n\n        if not is_fp_quant_available():\n            raise ImportError(\"Using `fp_quant` quantization requires fp_quant: `pip install fp_quant`\")\n\n        if device_map is None and not self.quantization_config.pseudoquantization:\n            raise ValueError(\n                \"You are attempting to load a FPQuant model without setting device_map.\"\n                \" Please set device_map comprised of 'cuda' devices.\"\n            )\n        elif (\n            isinstance(device_map, dict)\n            and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n            and not self.quantization_config.pseudoquantization\n        ):\n            raise ValueError(\n                \"You are attempting to load a FPQuant model with a device_map that contains a CPU or disk device.\"\n                \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n            )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            logger.info(\"`dtype` is None. Setting `dtype=torch.bfloat16` for qutlass compatibility.\")\n            dtype = torch.bfloat16\n        elif dtype != torch.bfloat16:\n            raise ValueError(f\"Invalid `dtype` {dtype}. fp_quant quantization only supports `dtype=torch.bfloat16`.\")\n\n        return dtype\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        module, _ = get_module_from_name(model, param_name)\n\n        if target_device == \"cpu\" and param_name.endswith(\"weight\"):\n            # Works agains hard-coded missing key dispatch to CPU\n            return\n\n        # The module holds either:\n        #  * `weight` when `store_master_weights=True`\n        #  * `qweight` and `scales` when `store_master_weights=False` and `pseudoquantization=False`\n        #  * `dqweight` when `store_master_weights=False` and `pseudoquantization=True`\n\n        if param_name.endswith(\".qweight\"):\n            # Loading a real quantized checkpoint without master weights\n            module.qweight = torch.nn.Parameter(\n                param_value.to(target_device),\n                requires_grad=False,\n            )\n            module.weight = None\n            module.dqweight = None\n            return\n\n        if param_name.endswith(\".dqweight\"):\n            # Loading a pseudo-quantized checkpoint without master weights\n            module.dqweight = torch.nn.Parameter(param_value.to(target_device))\n            module.weight = None\n            module.qweight = None\n            module.scales = None\n            return\n\n        # Loading master weights or an unquantized checkpoint\n        module.weight = torch.nn.Parameter(param_value.to(target_device))\n        # Let pre-forward handle the quantization and set None where necessary\n        module.pre_forward()\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        **kwargs,\n    ):\n        from fp_quant import replace_with_fp_quant_linear\n\n        from ..integrations.fp_quant import adapt_fp_quant_config\n\n        replace_with_fp_quant_linear(\n            model,\n            fp_quant_linear_config=adapt_fp_quant_config(self.quantization_config),\n        )\n        model.config.quantization_config = self.quantization_config\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        from fp_quant import FPQuantLinear\n\n        fp_quant_names = {name for name, module in model.named_modules() if isinstance(module, FPQuantLinear)}\n\n        def should_exclude(key: str) -> bool:\n            if key.endswith(\".weight\") or key.endswith(\".bias\"):\n                return False\n            full_key = f\"{prefix}.{key}\"\n            return any(name in key or name in full_key for name in fp_quant_names)\n\n        return [key for key in missing_keys if not should_exclude(key)]\n\n    @property\n    def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n        trainable = self.quantization_config.store_master_weights\n        if not trainable:\n            logger.warning(\n                \"You are attempting to train a model with FPQuant quantization. This is only supported when `store_master_weights=True`. Please set `store_master_weights=True` to train the model.\"\n            )\n        return trainable\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        from fp_quant import FPQuantLinear\n\n        module, tensor_name = get_module_from_name(model, param_name)\n        if isinstance(module, FPQuantLinear) and tensor_name in [\"weight\", \"qweight\", \"dqweight\"]:\n            # Only quantize weights of FPQuantLinear modules that are not already quantized\n            return True\n        else:\n            return False"
                },
                "component_dependencies": {
                    "FPQuantHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_fp_quant.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_fp_quant_available",
                        "transformers/utils.py#is_qutlass_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_gptq.py#GptqHfQuantizer": {
                "sorted_modules": {
                    "GptqHfQuantizer": "\n\nclass GptqHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer of the GPTQ method - for GPTQ the quantizer support calibration of the model through\n    `auto_gptq` or `gptqmodel` package. Quantization is done under the hood for users if they load a non-prequantized model.\n    \"\"\"\n\n    requires_calibration = False\n    required_packages = [\"optimum\", \"auto_gptq\", \"gptqmodel\"]\n    optimum_quantizer = None\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n        if not is_optimum_available():\n            raise ImportError(\"Loading a GPTQ quantized model requires optimum (`pip install optimum`)\")\n        from optimum.gptq import GPTQQuantizer\n\n        self.optimum_quantizer = GPTQQuantizer.from_dict(self.quantization_config.to_dict_optimum())\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_optimum_available():\n            raise ImportError(\"Loading a GPTQ quantized model requires optimum (`pip install optimum`)\")\n        if is_auto_gptq_available() and is_gptqmodel_available():\n            logger.warning(\"Detected gptqmodel and auto-gptq, will use gptqmodel\")\n\n        gptq_supports_cpu = (\n            is_auto_gptq_available()\n            and version.parse(importlib.metadata.version(\"auto-gptq\")) > version.parse(\"0.4.2\")\n        ) or is_gptqmodel_available()\n        if not gptq_supports_cpu and not torch.cuda.is_available():\n            raise RuntimeError(\"GPU is required to quantize or run quantize model.\")\n        elif not (is_auto_gptq_available() or is_gptqmodel_available()):\n            raise ImportError(\n                \"Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. \"\n            )\n        elif is_auto_gptq_available() and version.parse(importlib.metadata.version(\"auto_gptq\")) < version.parse(\n            \"0.4.2\"\n        ):\n            raise ImportError(\n                \"You need a version of auto_gptq >= 0.4.2 to use GPTQ: `pip install --upgrade auto-gptq` or use gptqmodel by `pip install gptqmodel>=1.4.3`.\"\n            )\n        elif is_gptqmodel_available() and (\n            version.parse(importlib.metadata.version(\"gptqmodel\")) < version.parse(\"1.4.3\")\n            or version.parse(importlib.metadata.version(\"optimum\")) < version.parse(\"1.23.99\")\n        ):\n            raise ImportError(\"The gptqmodel version should be >= 1.4.3, optimum version should >= 1.24.0\")\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            dtype = torch.float16\n            logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\")\n        elif dtype != torch.float16:\n            logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with GPTQ.\")\n        return dtype\n\n    def update_device_map(self, device_map):\n        if device_map is None:\n            device_map = {\"\": torch.device(\"cpu\")}\n        # Only with auto-gptq do not support CPU, we should move the model to cuda if available.\n        if not is_gptqmodel_available() and device_map in (\"cpu\", {\"\": torch.device(\"cpu\")}):\n            device_map = {\"\": 0}\n        return device_map\n\n    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        if model.__class__.main_input_name != \"input_ids\":\n            raise RuntimeError(\"We can only quantize pure text model.\")\n\n        if self.pre_quantized:\n            # compat: latest optimum has gptqmodel refactor\n            if version.parse(importlib.metadata.version(\"optimum\")) <= version.parse(\"1.23.99\"):\n                model = self.optimum_quantizer.convert_model(model)\n            else:\n                model = self.optimum_quantizer.convert_model(model, **kwargs)\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        if self.pre_quantized:\n            model = self.optimum_quantizer.post_init_model(model)\n        else:\n            if self.quantization_config.tokenizer is None:\n                self.quantization_config.tokenizer = model.name_or_path\n\n            self.optimum_quantizer.quantize_model(model, self.quantization_config.tokenizer)\n            model.config.quantization_config = GPTQConfig.from_dict(self.optimum_quantizer.to_dict())\n\n    @property\n    def is_trainable(self) -> bool:\n        return True\n\n    def is_serializable(self, safe_serialization=None):\n        return True"
                },
                "component_dependencies": {
                    "GptqHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_gptq.py#logger",
                        "transformers/utils.py#is_auto_gptq_available",
                        "transformers/utils.py#is_gptqmodel_available",
                        "transformers/utils.py#is_optimum_available",
                        "transformers/utils/quantization_config.py#GPTQConfig.from_dict",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_higgs.py#HiggsHfQuantizer": {
                "sorted_modules": {
                    "HiggsHfQuantizer": "\n\nclass HiggsHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer of the HIGGS method. Enables the loading of prequantized models and in-flight quantization of full-precision models.\n    \"\"\"\n\n    requires_calibration = False\n    requires_parameters_quantization = True\n    required_packages = [\"flute-kernel\", \"fast_hadamard_transform\"]\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, device_map, **kwargs):\n        if not torch.cuda.is_available():\n            raise NotImplementedError(\"HIGGS quantization is only supported on GPU. Please use a different quantizer.\")\n\n        if not is_accelerate_available():\n            raise ImportError(\"Using `higgs` quantization requires Accelerate: `pip install accelerate`\")\n\n        if not is_flute_available():\n            raise ImportError(\"Using `higgs` quantization requires FLUTE: `pip install flute-kernel>=0.3.0`\")\n\n        if not is_hadamard_available():\n            raise ImportError(\n                \"Using `higgs` quantization requires fast_hadamard_transform: `pip install fast_hadamard_transform`\"\n            )\n\n        if device_map is None:\n            raise ValueError(\n                \"You are attempting to load a HIGGS model without setting device_map.\"\n                \" Please set device_map comprised of 'cuda' devices.\"\n            )\n        elif isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n            raise ValueError(\n                \"You are attempting to load a HIGGS model with a device_map that contains a CPU or disk device.\"\n                \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n            )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            logger.info(\"`dtype` is None. Setting `dtype=torch.float16` for FLUTE compatibility.\")\n            dtype = torch.float16\n        elif dtype != torch.float16 and dtype != torch.bfloat16:\n            raise ValueError(\n                f\"Invalid `dtype` {dtype}. HIGGS quantization only supports `dtype=torch.float16` or `dtype=torch.bfloat16`.\"\n            )\n\n        return dtype\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        from ..integrations import quantize_with_higgs\n\n        flute_dict = quantize_with_higgs(\n            param_value.to(target_device),\n            self.quantization_config.bits,\n            self.quantization_config.p,\n            self.quantization_config.group_size,\n            self.quantization_config.hadamard_size,\n        )\n        del param_value\n\n        module, _ = get_module_from_name(model, param_name)\n        module_name = \".\".join(param_name.split(\".\")[:-1])\n        for key, value in flute_dict.items():\n            if key in module._parameters:\n                module._parameters[key] = torch.nn.Parameter(value, requires_grad=False)\n            elif key in module._buffers:\n                module._buffers[key] = torch.nn.Buffer(value)\n            elif key == \"tune_metadata\":\n                module.tune_metadata = value\n                self.quantization_config.tune_metadata[module_name] = value.to_dict()\n            else:\n                raise ValueError(f\"Unexpected key {key} in module {module}\")\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_higgs_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        replace_with_higgs_linear(\n            model,\n            quantization_config=self.quantization_config,\n            modules_to_not_convert=self.modules_to_not_convert,\n        )\n        model.config.quantization_config = self.quantization_config\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        from flute.tune import TuneMetaData, maybe_tune_and_repack\n        from flute.utils import make_workspace_streamk\n\n        from ..integrations import HiggsLinear\n\n        flute_workspaces = {}\n        flute_modules = {name: module for name, module in model.named_modules() if isinstance(module, HiggsLinear)}\n        for name, module in tqdm(flute_modules.items(), desc=\"Repacking HIGGS modules\", leave=False):\n            # Every HiggsLinear needs a \"workspace\": a buffer for the unpacking operation.\n            # This buffer needs to be on the same device as the weights, but can be reused across modules otherwise.\n            if module.weight.device not in flute_workspaces:\n                flute_workspaces[module.weight.device] = make_workspace_streamk(device=module.weight.device)\n            module.workspace = flute_workspaces[module.weight.device]\n\n            # FLUTE weights are packed in a way that is optimized for a specific number of SMs (GPU streaming multiprocessors).\n            # If the model is loaded on a different device than the one it was saved on, we need to repack the weights.\n            module.tune_metadata = TuneMetaData.from_dict(self.quantization_config.tune_metadata[name])\n            module.weight.data, module.tune_metadata = maybe_tune_and_repack(\n                weight=module.weight.data,\n                scales=module.scales.data,\n                metadata=module.tune_metadata,\n            )\n            self.quantization_config.tune_metadata[name] = module.tune_metadata.to_dict()\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        from ..integrations import HiggsLinear\n\n        higgs_names = {name for name, module in model.named_modules() if isinstance(module, HiggsLinear)}\n\n        def should_update(key: str) -> bool:\n            if key.endswith(\".weight\") or key.endswith(\".bias\"):\n                return False\n            full_key = f\"{prefix}.{key}\"\n            return any(name in key or name in full_key for name in higgs_names)\n\n        return [key for key in missing_keys if not should_update(key)]\n\n    @property\n    def is_trainable(self) -> bool:\n        return False\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        from ..integrations import HiggsLinear\n\n        module, tensor_name = get_module_from_name(model, param_name)\n        if isinstance(module, HiggsLinear) and tensor_name == \"weight\":\n            # Only quantize weights of HiggsLinear modules that are not already quantized\n            return True\n        else:\n            return False\n\n    def _dequantize(self, model):\n        from ..integrations import dequantize_higgs\n\n        model = dequantize_higgs(model)\n        return model"
                },
                "component_dependencies": {
                    "HiggsHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_higgs.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_flute_available",
                        "transformers/utils.py#is_hadamard_available",
                        "transformers/utils/logging.py#tqdm",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_hqq.py#HqqHfQuantizer": {
                "sorted_modules": {
                    "HqqHfQuantizer": "\n\nclass HqqHfQuantizer(HfQuantizer):\n    \"\"\"\n    HQQ quantizer base HF class.\n    nn.Linear modules are first tagged with quant_config in _process_model_before_weight_loading().\n    \"\"\"\n\n    use_keep_in_fp32_modules = False\n    requires_parameters_quantization = True\n    requires_calibration = False\n    required_packages = [\"hqq\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        if not is_hqq_available():\n            raise ImportError(\n                \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n            )\n        super().__init__(quantization_config, **kwargs)\n        self.dtype = None\n        self.using_multi_gpu = False\n        # Keys that are serialized specifically by hqq\n        self.hqq_keys = HQQLinear(None, None).state_dict_keys() - {\"bias\"}\n\n    def validate_environment(self, *args, **kwargs):\n        if self.dtype is None:\n            if \"dtype\" in kwargs:\n                self.dtype = kwargs[\"dtype\"]\n            else:\n                self.dtype = torch.float32\n                logger.info(\"Setting dtype to torch.float32 as the default value since it was not specified.\")\n\n        device_map = kwargs.get(\"device_map\")\n        if isinstance(device_map, dict):\n            if \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n                raise ValueError(\n                    \"You are attempting to use an HQQ model with a device_map that contains a CPU or disk device.\"\n                    \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n                )\n            else:\n                self.using_multi_gpu = len(set(device_map.values())) > 1\n\n    def update_missing_keys(\n        self, model: \"PreTrainedModel\", missing_keys: list[str], prefix: str, **kwargs\n    ) -> list[str]:\n        if self.pre_quantized:\n            return [key for key in missing_keys if (\"weight\" not in key)]\n        else:\n            return missing_keys\n\n    # Adds missing keys for HQQLinear modules that are loaded but the model with initialized with torch.nn.Linear\n    def update_expected_keys(\n        self, model: \"PreTrainedModel\", expected_keys: list[str], loaded_keys: list[str]\n    ) -> list[str]:\n        if not self.pre_quantized:\n            return expected_keys\n\n        # Collects all quantizable (linear) layers\n        def _find_hqq_quantizable_layers(model, layers):\n            for name, module in model.named_children():\n                if isinstance(module, (torch.nn.Linear)):\n                    layers.add(module.name)\n                _find_hqq_quantizable_layers(module, layers)\n\n        new_keys = set(expected_keys)\n\n        # Name modules\n        for name, module in model.named_modules():\n            module.name = name\n\n        # valid modules are Linear layers that have HQQLinear state_dict. We ignore skip_modules and any layers with Linear state_dict() params\n        _valid_modules = set()\n        _find_hqq_quantizable_layers(model, _valid_modules)\n\n        # Remove skipped modules\n        _skipped_modules = set()\n        for _module in _valid_modules:\n            for _skip_module in model.config.quantization_config[\"skip_modules\"]:\n                if _skip_module in _module:\n                    _skipped_modules.add(_module)\n        _valid_modules -= _skipped_modules\n\n        # Append new expected layers based on _ref_keys\n        _ref_keys = HQQLinear(\n            linear_layer=None,\n            quant_config=None,\n            compute_dtype=torch.float16,\n            device=\"cpu\",\n            del_orig=False,\n        ).state_dict_keys() - {\"bias\"}\n\n        # Clean-up\n        _rm_keys = set()\n        for key in new_keys:\n            if any(_module in key for _module in _valid_modules):\n                _rm_keys.add(key)\n        new_keys -= _rm_keys\n        # At this point, new_keys contains all the keys of the layers that are NOT HQQLinear or torch.nn.Linear\n\n        # Re-populate Linear/HQQLinear\n        for _module in _valid_modules:\n            if _module + \".weight\" in loaded_keys:\n                new_keys.add(_module + \".weight\")\n            else:\n                new_keys.update({_module + \".\" + _ref_key for _ref_key in _ref_keys})\n            if _module + \".bias\" in loaded_keys:\n                new_keys.add(_module + \".bias\")\n\n        return list(new_keys)\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        module, _ = get_module_from_name(model, param_name)\n        # Since we do not prepare the modules in advance, we need every param of the Linear layer to go through\n        # `create_quantized_param`, even when `self.is_quantized == True`\n        return isinstance(module, torch.nn.Linear)\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        module, tensor_name = get_module_from_name(model, param_name)\n        module_name = param_name.rsplit(\".\", 1)[0]\n        parent_module, node = get_module_from_name(model, module_name)\n\n        quant_config = model.config.quantization_config[\"quant_config\"]\n        skip_modules = model.config.quantization_config[\"skip_modules\"]\n\n        # In this case we do not quantize this layer (it's explicitly skipped) -> simply load param\n        if any(skip_module in module.name for skip_module in skip_modules):\n            module.load_state_dict(\n                {tensor_name: param_value.to(device=target_device, dtype=self.dtype)}, strict=False, assign=True\n            )\n            return\n\n        # We need this hack as the model is not pre-prepared as an empty skeleton on meta device\n        if self.pre_quantized:\n            # Save them for later\n            if not hasattr(self, \"hqq_params\"):\n                self.hqq_params = defaultdict(dict)\n            self.hqq_params[module_name].update({tensor_name: param_value})\n            hqq_params = self.hqq_params[module_name]\n\n            # If they are all present and saved, make it a HQQLinear layer! (we cannot do it param after param because\n            # hqq does not support it...)\n            if all(k in hqq_params for k in self.hqq_keys) and (\"bias\" in hqq_params or module.bias is None):\n                hqq_layer = HQQLinear(\n                    linear_layer=None,\n                    quant_config=None,\n                    compute_dtype=self.dtype,\n                    device=target_device,\n                    del_orig=False,\n                )\n                hqq_layer.load_state_dict(hqq_params)\n\n                if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n                    hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n                if self.using_multi_gpu:\n                    hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n\n                setattr(parent_module, node, hqq_layer)\n                del self.hqq_params[module_name], module\n            return\n\n        # Load param in the module (without caring about device or dtype, it will be changed later)\n        module.load_state_dict({tensor_name: param_value}, strict=False, assign=True)\n\n        # If both the weight and bias have already been loaded, time to quantize!\n        module_is_ready = module.weight.device.type != \"meta\" and (\n            module.bias is None or module.bias.device.type != \"meta\"\n        )\n\n        if module_is_ready:\n            module_tag = \".\".join(module.name.split(\".\")[-2:])\n            if \"weight_quant_params\" in quant_config:\n                module_quant_config = quant_config\n            elif module_tag in quant_config:\n                module_quant_config = quant_config[module_tag]\n\n            hqq_layer = HQQLinear(\n                module,\n                quant_config=module_quant_config,\n                compute_dtype=self.dtype,\n                device=target_device,\n                del_orig=True,\n            )\n\n            if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n                hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n\n            if self.using_multi_gpu:\n                hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n\n            setattr(parent_module, node, hqq_layer)\n\n    def _patch_layer_for_multigpu(self, hqq_layer):\n        def forward_with_device(self, x):\n            out = torch.matmul(x.to(self.device), self.dequantize().t())\n            if self.bias is not None:\n                out += self.bias\n            return out\n\n        hqq_layer.forward = lambda x: forward_with_device(hqq_layer, x)\n        return hqq_layer\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        **kwargs,\n    ):\n        # Add the corresponding quant_config to each valid module. This allows us to do the actual nn.Linear -> HQQLinear conversion in create_quantized_param().\n        # prepare_for_hqq_linear() also sets the right quantization config inside the model (model.config.quantization_config) and the layers (hqq_layer.quant_config)\n        model = prepare_for_hqq_linear(model, quantization_config=self.quantization_config)\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        model.is_hqq_quantized = True\n        model.is_hqq_serializable = self.is_serializable()\n        return model\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        return True"
                },
                "component_dependencies": {
                    "HqqHfQuantizer": [
                        "transformers/integrations.py#prepare_for_hqq_linear",
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_hqq.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_hqq_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_mxfp4.py#Mxfp4HfQuantizer": {
                "sorted_modules": {
                    "Mxfp4HfQuantizer": "\n\nclass Mxfp4HfQuantizer(HfQuantizer):\n    \"\"\"\n    FP4 quantization using fbgemm kernels\n    \"\"\"\n\n    requires_parameters_quantization = True\n    requires_calibration = False\n\n    required_packages = [\"accelerate\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n        self.triton_kernels_hub = None\n\n    def _lazy_import_kernels(self):\n        \"\"\"Lazy import and initialize kernels only when needed\"\"\"\n        if self.triton_kernels_hub is None:\n            try:\n                from kernels import get_kernel\n\n                self.triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n            except ImportError:\n                raise ImportError(\"kernels package is required for MXFP4 quantization\")\n        return self.triton_kernels_hub\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_torch_available():\n            raise ImportError(\n                \"Using mxfp4 quantization requires torch\"\n                \"Please install the latest version of torch ( pip install --upgrade torch )\"\n            )\n\n        if self.quantization_config.dequantize:\n            return\n\n        if not (torch.cuda.is_available() or torch.xpu.is_available()):\n            if self.pre_quantized:\n                logger.warning_once(\n                    \"Using MXFP4 quantized models requires a GPU, we will default to dequantizing the model to bf16\"\n                )\n                self.quantization_config.dequantize = True\n                return\n            else:\n                raise RuntimeError(\"Quantizing a model using MXFP4 requires a GPU\")\n\n        if not is_accelerate_available():\n            raise ImportError(\"Using mxfp4 requires Accelerate: `pip install accelerate`\")\n\n        if torch.xpu.is_available():\n            gpu_is_supported = True\n            kernels_available = is_triton_available(\"3.5.0\") and is_kernels_available()\n        else:\n            compute_capability = torch.cuda.get_device_capability()\n            gpu_is_supported = compute_capability >= (7, 5)\n            kernels_available = is_triton_available(\"3.4.0\") and is_kernels_available()\n\n        if self.pre_quantized:\n            # On unsupported GPUs or without kernels, we will dequantize the model to bf16\n            if not gpu_is_supported:\n                logger.warning_once(\n                    \"MXFP4 quantization is only supported on GPUs with compute capability >= 7.5 (e.g T4, A100, L4, H100, or B200) or XPUs (e.g Intel\u00ae Data Center GPU Max Series) \"\n                    \"We will default to dequantizing the model to bf16.\"\n                )\n                self.quantization_config.dequantize = True\n                return\n\n            if not kernels_available:\n                logger.warning_once(\n                    \"MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\"\n                )\n                self.quantization_config.dequantize = True\n                return\n        elif not gpu_is_supported:\n            # we can't quantize the model in this case so we raise an error\n            raise ValueError(\n                \"MXFP4 quantization is only supported on GPUs with compute capability >= 7.5 (e.g T4, A100, L4, H100, or B200) or XPUs (e.g Intel\u00ae Data Center GPU Max Series) \"\n            )\n        elif not kernels_available:\n            # we can't quantize the model in this case so we raise an error\n            raise ValueError(\n                \"MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0\"\n            )\n\n        if not self.pre_quantized:\n            self._lazy_import_kernels()\n\n        device_map = kwargs.get(\"device_map\")\n        if device_map is None:\n            logger.warning_once(\n                \"You have loaded an FP4 model on CPU and have a CUDA/XPU device available, make sure to set \"\n                \"your model on a GPU/XPU device in order to run your model. To remove this warning, pass device_map = 'cuda' or device_map = 'xpu'. \"\n            )\n        elif device_map is not None:\n            if (\n                not self.pre_quantized\n                and isinstance(device_map, dict)\n                and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n            ):\n                raise ValueError(\n                    \"You are attempting to load an FP4 model with a device_map that contains a CPU or disk device.\"\n                    \"This is not supported when the model is quantized on the fly. \"\n                    \"Please use a quantized checkpoint or remove the CPU or disk device from the device_map.\"\n                )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            dtype = torch.bfloat16\n            logger.info(\n                \"Overriding dtype=%s with `dtype=torch.bfloat16` due to \"\n                \"requirements of `fbgemm-gpu` to enable model loading in fp4. \"\n                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n                \" dtype=torch.bfloat16 to remove this warning.\",\n                dtype,\n            )\n        return dtype\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        from ..integrations import Mxfp4GptOssExperts\n        from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n\n        # if we are dequantizing, the model doesn't have scales, and blocks only params like gate_up_proj and down_proj so we need to handle this case differently\n        if self.quantization_config.dequantize and (\"blocks\" in param_name or \"scales\" in param_name):\n            module, tensor_name = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n        else:\n            module, tensor_name = get_module_from_name(model, param_name)\n        if isinstance(module, Mxfp4GptOssExperts) or (\n            isinstance(module, GptOssExperts) and self.quantization_config.dequantize\n        ):\n            if tensor_name in [\"down_proj_bias\", \"gate_up_proj_bias\"]:\n                return False\n            return True\n        return False\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        from ..integrations import (\n            Mxfp4GptOssExperts,\n            dequantize,\n            load_and_swizzle_mxfp4,\n            quantize_to_mxfp4,\n            swizzle_mxfp4,\n        )\n        from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n\n        if not self.pre_quantized:\n            triton_kernels_hub = self._lazy_import_kernels()\n            module, _ = get_module_from_name(model, param_name)\n            with torch.device(target_device):\n                if isinstance(module, Mxfp4GptOssExperts):\n                    triton_weight_tensor, weight_scale = quantize_to_mxfp4(param_value, triton_kernels_hub)\n                    PrecisionConfig, FlexCtx, InFlexData = (\n                        triton_kernels_hub.matmul_ogs.PrecisionConfig,\n                        triton_kernels_hub.matmul_ogs.FlexCtx,\n                        triton_kernels_hub.matmul_ogs.InFlexData,\n                    )\n                    triton_weight_tensor, weight_scale = swizzle_mxfp4(\n                        triton_weight_tensor, weight_scale, triton_kernels_hub\n                    )\n\n                    proj = \"gate_up_proj\" if \"gate_up_proj\" in param_name else \"down_proj\"\n                    setattr(module, proj, triton_weight_tensor)\n                    setattr(\n                        module,\n                        f\"{proj}_precision_config\",\n                        PrecisionConfig(weight_scale=weight_scale, flex_ctx=FlexCtx(rhs_data=InFlexData())),\n                    )\n\n                    delattr(module, f\"{proj}_blocks\")\n                    delattr(module, f\"{proj}_scales\")\n\n        # The params going here are either gate_up_proj_blocks, or down_proj_blocks, or gate_up_proj_scales, or down_proj_scales\n        else:\n            #  This is when loading a quantized model (blocks and scales exist)\n            empty_param = kwargs.get(\"empty_param\")\n            casting_dtype = kwargs.get(\"casting_dtype\")\n            to_contiguous = kwargs.get(\"to_contiguous\")\n            rank = kwargs.get(\"rank\")\n            device_mesh = kwargs.get(\"device_mesh\")\n            if (\"blocks\" in param_name or \"scales\" in param_name) and self.quantization_config.dequantize:\n                # blocks and scales have the same length that's why this works for both\n                module, _ = get_module_from_name(model, param_name[: -len(\"_blocks\")])\n            else:\n                module, _ = get_module_from_name(model, param_name)\n\n            shard_kwargs = {\n                \"empty_param\": empty_param,\n                \"casting_dtype\": casting_dtype,\n                \"to_contiguous\": to_contiguous,\n                \"rank\": rank,\n                \"device_mesh\": device_mesh,\n                \"model\": model,\n            }\n\n            if isinstance(module, Mxfp4GptOssExperts) or (\n                isinstance(module, GptOssExperts) and self.quantization_config.dequantize\n            ):\n                if self.quantization_config.dequantize:\n                    # dq_param_name is the name of the parameter without the blocks or scales suffix, it's used in this case since we don't switch linears\n                    # so we only have the original param name\n                    dq_param_name = param_name[: -len(\"_blocks\")]\n                    dequantize(module, param_name, param_value, target_device, dq_param_name, **shard_kwargs)\n                else:\n                    load_and_swizzle_mxfp4(\n                        module,\n                        param_name,\n                        param_value,\n                        target_device,\n                        self._lazy_import_kernels(),\n                        **shard_kwargs,\n                    )\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        # we are not really dequantizing, we are just removing everything related to quantization here\n        if self.quantization_config.dequantize:\n            self.remove_quantization_config(model)\n        # clean cache due to triton ops\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        elif torch.xpu.is_available():\n            torch.xpu.empty_cache()\n\n    def update_expected_keys(self, model: \"PreTrainedModel\", expected_keys: list[str], checkpoint_keys: list[str]):\n        # Replace expected_keys for experts' gate_up_proj and down_proj with their _blocks and _scales variants\n        new_expected_keys = []\n        for key in expected_keys:\n            if key.endswith(\".mlp.experts.gate_up_proj\"):\n                base = key[: -len(\"gate_up_proj\")]\n                new_expected_keys.append(base + \"gate_up_proj_blocks\")\n                new_expected_keys.append(base + \"gate_up_proj_scales\")\n            elif key.endswith(\".mlp.experts.down_proj\"):\n                base = key[: -len(\"down_proj\")]\n                new_expected_keys.append(base + \"down_proj_blocks\")\n                new_expected_keys.append(base + \"down_proj_scales\")\n            elif not self.pre_quantized:\n                # in this case, we are quantizing the model so we need to update the keys as we changed the layers\n                if key.endswith(\".mlp.experts.down_proj_blocks\"):\n                    base = key[: -len(\"down_proj_blocks\")]\n                    new_expected_keys.append(base + \"down_proj\")\n                elif key.endswith(\".mlp.experts.gate_up_proj_blocks\"):\n                    base = key[: -len(\"gate_up_proj_blocks\")]\n                    new_expected_keys.append(base + \"gate_up_proj\")\n                elif key.endswith(\"scales\"):\n                    # we remove it the scales as the checkpoint don't contain them\n                    continue\n                else:\n                    new_expected_keys.append(key)\n            else:\n                new_expected_keys.append(key)\n        return new_expected_keys\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        from ..integrations import replace_with_mxfp4_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        use_kernels = kwargs.get(\"use_kernels\", False)\n        # if we are using kernels, we can't use the quantized model, since the forward pass is different and needs special handling\n        if use_kernels:\n            logger.warning_once(\n                \"You are using full precision kernels, we will dequantize the model to bf16. \"\n                \"To use the quantized model with quantization kernels, please set use_kernels=False\"\n            )\n            self.quantization_config.dequantize = True\n\n        config = model.config\n        model = replace_with_mxfp4_linear(\n            model,\n            modules_to_not_convert=self.modules_to_not_convert,\n            quantization_config=self.quantization_config,\n            config=config,\n        )\n\n        model.config.quantization_config = self.quantization_config\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        from ..integrations import Mxfp4GptOssExperts\n\n        not_missing_keys = []\n        for name, module in model.named_modules():\n            if isinstance(module, Mxfp4GptOssExperts):\n                for missing in missing_keys:\n                    if (\n                        (name in missing or name in f\"{prefix}.{missing}\")\n                        and not missing.endswith(\".weight\")\n                        and not missing.endswith(\".bias\")\n                    ):\n                        not_missing_keys.append(missing)\n        return [k for k in missing_keys if k not in not_missing_keys]\n\n    def update_tp_plan(self, config):\n        if \"GptOssConfig\" in config.__class__.__name__:\n            if getattr(config, \"base_model_tp_plan\", None) is not None:\n                config.base_model_tp_plan.update(\n                    {\n                        \"layers.*.mlp.experts.gate_up_proj_blocks\": \"grouped_gemm\",\n                        \"layers.*.mlp.experts.gate_up_proj_scales\": \"grouped_gemm\",\n                        \"layers.*.mlp.experts.down_proj_blocks\": \"grouped_gemm\",\n                        \"layers.*.mlp.experts.down_proj_scales\": \"grouped_gemm\",\n                    }\n                )\n        return config\n\n    def update_ep_plan(self, config):\n        if \"GptOssConfig\" in config.__class__.__name__:\n            if getattr(config, \"base_model_ep_plan\", None) is not None:\n                config.base_model_ep_plan.update(\n                    {\n                        \"layers.*.mlp.experts.gate_up_proj_blocks\": \"grouped_gemm\",\n                        \"layers.*.mlp.experts.gate_up_proj_scales\": \"grouped_gemm\",\n                        \"layers.*.mlp.experts.down_proj_blocks\": \"grouped_gemm\",\n                        \"layers.*.mlp.experts.down_proj_scales\": \"grouped_gemm\",\n                    }\n                )\n        return config\n\n    def get_param_name(self, param_name: str) -> str:\n        if self.quantization_config.dequantize:\n            if \"_blocks\" in param_name:\n                return param_name.replace(\"_blocks\", \"\")\n            elif \"_scales\" in param_name:\n                return param_name.replace(\"_scales\", \"\")\n        elif not self.pre_quantized:\n            if param_name.endswith(\"gate_up_proj\"):\n                return param_name.replace(\"gate_up_proj\", \"gate_up_proj_blocks\")\n            if param_name.endswith(\"down_proj\"):\n                return param_name.replace(\"down_proj\", \"down_proj_blocks\")\n        return param_name\n\n    def get_state_dict_and_metadata(self, model, safe_serialization: bool = False):\n        from ..integrations import Mxfp4GptOssExperts\n\n        state_dict = model.state_dict()\n\n        for name, module in model.named_modules():\n            if (\n                isinstance(module, Mxfp4GptOssExperts)\n                and hasattr(module, \"gate_up_proj\")\n                and hasattr(module, \"down_proj\")\n            ):\n                state_dict[f\"{name}.gate_up_proj_blocks\"] = (\n                    module.gate_up_proj.storage.layout.unswizzle_data(module.gate_up_proj.storage.data)\n                    .transpose(-1, -2)\n                    .reshape(32, -1, 90, 16)\n                )\n                state_dict[f\"{name}.gate_up_proj_scales\"] = (\n                    module.gate_up_proj_precision_config.weight_scale.storage.layout.unswizzle_data(\n                        module.gate_up_proj_precision_config.weight_scale.storage.data\n                    ).transpose(-1, -2)\n                )\n                state_dict[f\"{name}.down_proj_blocks\"] = (\n                    module.down_proj.storage.layout.unswizzle_data(module.down_proj.storage.data)\n                    .transpose(-1, -2)\n                    .reshape(32, 2880, 90, -1)\n                )\n                state_dict[f\"{name}.down_proj_scales\"] = (\n                    module.down_proj_precision_config.weight_scale.storage.layout.unswizzle_data(\n                        module.down_proj_precision_config.weight_scale.storage.data\n                    ).transpose(-1, -2)\n                )\n\n        metadata = {}\n        return state_dict, metadata\n\n    def is_serializable(self, safe_serialization=None):\n        return True\n\n    @property\n    def is_trainable(self) -> bool:\n        logger.warning_once(\n            \"MXFP4 quantization don't support training, please consider dequantizing the model first by passing quantization_config=Mxfp4Config(dequantize=True) to .from_pretrained()\"\n        )\n        return False"
                },
                "component_dependencies": {
                    "Mxfp4HfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_mxfp4.py#logger",
                        "transformers/quantizers/quantizer_mxfp4.py#triton_kernels_hub",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_kernels_available",
                        "transformers/utils.py#is_torch_available",
                        "transformers/utils.py#is_triton_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_quanto.py#QuantoHfQuantizer": {
                "sorted_modules": {
                    "QuantoHfQuantizer": "\n\nclass QuantoHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer for the quanto library\n    \"\"\"\n\n    required_packages = [\"quanto\", \"accelerate\"]\n    requires_parameters_quantization = True\n    requires_calibration = False\n\n    def __init__(self, quantization_config: QuantoConfig, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker\n        \"\"\"\n        if self.quantization_config.activations is not None and not self.pre_quantized:\n            raise ValueError(\n                \"We don't support quantizing the activations with transformers library.\"\n                \"Use quanto library for more complex use cases such as activations quantization, calibration and quantization aware training.\"\n            )\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_optimum_quanto_available():\n            raise ImportError(\n                \"Loading an optimum-quanto quantized model requires optimum-quanto library (`pip install optimum-quanto`)\"\n            )\n        if not is_accelerate_available():\n            raise ImportError(\n                \"Loading an optimum-quanto quantized model requires accelerate library (`pip install accelerate`)\"\n            )\n\n    def update_device_map(self, device_map):\n        if device_map is None:\n            device_map = {\"\": \"cpu\"}\n            logger.info(\n                \"The device_map was not initialized. \"\n                \"Setting device_map to {'':'cpu'}. \"\n                \"If you want to use the model for inference, please set device_map ='auto'\"\n            )\n        return device_map\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            logger.info(\"You did not specify `dtype` in `from_pretrained`. Setting it to `torch.float32`.\")\n            dtype = torch.float32\n        return dtype\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        if is_optimum_quanto_available():\n            from optimum.quanto import QModuleMixin\n\n        not_missing_keys = []\n        for name, module in model.named_modules():\n            if isinstance(module, QModuleMixin):\n                for missing in missing_keys:\n                    if (\n                        (name in missing or name in f\"{prefix}.{missing}\")\n                        and not missing.endswith(\".weight\")\n                        and not missing.endswith(\".bias\")\n                    ):\n                        not_missing_keys.append(missing)\n        return [k for k in missing_keys if k not in not_missing_keys]\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        if is_optimum_quanto_available():\n            from optimum.quanto import QModuleMixin\n\n        module, tensor_name = get_module_from_name(model, param_name)\n        # We only quantize the weights and the bias is not quantized.\n        if isinstance(module, QModuleMixin) and \"weight\" in tensor_name:\n            # if the weights are quantized, don't need to recreate it again with `create_quantized_param`\n            return not module.frozen\n        else:\n            return False\n\n    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n        max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n        return max_memory\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        from ..modeling_utils import _load_parameter_into_model\n\n        _load_parameter_into_model(model, param_name, param_value.to(target_device))\n        module, _ = get_module_from_name(model, param_name)\n        module.freeze()\n        module.weight.requires_grad = False\n\n    def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n        from accelerate.utils import CustomDtype\n\n        mapping = {\n            \"int8\": torch.int8,\n            \"float8\": CustomDtype.FP8,\n            \"int4\": CustomDtype.INT4,\n            \"int2\": CustomDtype.INT2,\n        }\n        target_dtype = mapping[self.quantization_config.weights]\n        return target_dtype\n\n    def _process_model_before_weight_loading(\n        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs\n    ):\n        from ..integrations import replace_with_quanto_layers\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        model, _ = replace_with_quanto_layers(\n            model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n        )\n        model.config.quantization_config = self.quantization_config\n\n    @property\n    def is_trainable(self) -> bool:\n        return True\n\n    def is_serializable(self, safe_serialization=None):\n        return False"
                },
                "component_dependencies": {
                    "QuantoHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_quanto.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_optimum_quanto_available",
                        "transformers/utils/quantization_config.py#QuantoConfig"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_quark.py#QuarkHfQuantizer": {
                "sorted_modules": {
                    "QuarkHfQuantizer": "\n\nclass QuarkHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quark quantizer (https://quark.docs.amd.com/latest/).\n    \"\"\"\n\n    requires_calibration = True  # On-the-fly quantization with quark is not supported for now.\n    required_packages = [\"quark\"]\n\n    # Checkpoints are expected to be already quantized when loading a quark model. However, as some keys from\n    # the checkpoint might mismatch the model parameters keys, we use the `create_quantized_param` method\n    # to load the checkpoints, remapping the keys.\n    requires_parameters_quantization = True\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n        self.json_export_config = quantization_config.json_export_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_quark_available():\n            raise ImportError(\n                \"Loading a Quark quantized model requires the `quark` library but it was not found in the environment. Please refer to https://quark.docs.amd.com/latest/install.html.\"\n            )\n\n    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        from quark.torch.export.api import _map_to_quark\n\n        _map_to_quark(\n            model,\n            self.quantization_config.quant_config,\n            pack_method=self.json_export_config.pack_method,\n            custom_mode=self.quantization_config.custom_mode,\n        )\n\n        return model\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        return True\n\n    def create_quantized_param(self, model, param, param_name, param_device, **kwargs):\n        from ..modeling_utils import _load_parameter_into_model\n\n        postfix = param_name.split(\".\")[-1]\n\n        if postfix in CHECKPOINT_KEYS:\n            param_name = param_name.replace(postfix, CHECKPOINT_KEYS[postfix])\n\n        _load_parameter_into_model(model, param_name, param.to(param_device))\n\n    def is_serializable(self, safe_serialization=None):\n        return False\n\n    @property\n    def is_trainable(self):\n        return False"
                },
                "component_dependencies": {
                    "QuarkHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_quark.py#CHECKPOINT_KEYS",
                        "transformers/utils.py#is_quark_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_spqr.py#SpQRHfQuantizer": {
                "sorted_modules": {
                    "SpQRHfQuantizer": "\n\nclass SpQRHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer of the SpQR method. Enables the loading of prequantized models.\n    \"\"\"\n\n    requires_calibration = True\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not torch.cuda.is_available():\n            raise RuntimeError(\"GPU is required to run SpQR quantized model.\")\n\n        if not is_accelerate_available():\n            raise ImportError(\"Using `spqr` quantization requires Accelerate: `pip install accelerate`\")\n\n        if not is_spqr_available():\n            raise ImportError(\"Using `spqr` quantization requires SpQR: `pip install spqr_quant[gpu]`\")\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            dtype = torch.float16\n            logger.info(\"Assuming SpQR inference on GPU and loading the model in `torch.float16`.\")\n        elif dtype != torch.float16:\n            raise ValueError(\n                \"You cannot use any type other than torch.float16 for SpQR. Please either leave it None or set it to\"\n                \"torch.float16 explicitly.\"\n            )\n        return dtype\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        replace_with_spqr_linear(\n            model,\n            quantization_config=self.quantization_config,\n            modules_to_not_convert=self.modules_to_not_convert,\n        )\n        model.config.quantization_config = self.quantization_config\n\n    @property\n    def is_trainable(self):\n        return False\n\n    def is_serializable(self, safe_serialization=None):\n        return True"
                },
                "component_dependencies": {
                    "SpQRHfQuantizer": [
                        "transformers/integrations.py#replace_with_spqr_linear",
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_spqr.py#logger",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_spqr_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_torchao.py#TorchAoHfQuantizer": {
                "sorted_modules": {
                    "TorchAoHfQuantizer": "\n\nclass TorchAoHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer for torchao: https://github.com/pytorch/ao/\n    \"\"\"\n\n    requires_parameters_quantization = True\n    requires_calibration = False\n    required_packages = [\"torchao\"]\n\n    def __init__(self, quantization_config, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n\n        if isinstance(self.quantization_config.quant_type, str):\n            is_int_4 = \"int4\" in self.quantization_config.quant_type\n        else:\n            config_name = self.quantization_config.quant_type.__class__.__name__\n            is_int_4 = fuzzy_match_size(config_name) == \"4\"\n\n        # TODO: better way to get the serialized key names? Hard to read from torchao codebase\n        if is_int_4:\n            self.weight_ao_keys = [\"qdata\", \"scale\", \"zero_point\"]\n        else:\n            self.weight_ao_keys = [\"qdata\", \"scale\"]\n        # Instead of serializing the simple torch.Tensor like usual, torchao adds a `:_data` suffix so we need this\n        self.full_ao_keys = self.weight_ao_keys + [\"_data\"]\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_torchao_available():\n            raise ImportError(\"Loading an torchao quantized model requires torchao library (`pip install torchao`)\")\n\n        self.offload = False\n        device_map = kwargs.get(\"device_map\")\n        if isinstance(device_map, dict):\n            if (\"disk\" in device_map.values() or \"cpu\" in device_map.values()) and len(device_map) > 1:\n                self.offload = True\n                if self.pre_quantized and \"disk\" in device_map.values():\n                    raise ValueError(\n                        \"You are attempting to perform disk offload with a pre-quantized torchao model \"\n                        \"This is not supported yet . Please remove the disk device from the device_map.\"\n                    )\n        if self.pre_quantized:\n            weights_only = kwargs.get(\"weights_only\")\n            if weights_only:\n                torch_version = version.parse(importlib.metadata.version(\"torch\"))\n                if torch_version < version.parse(\"2.5.0\"):\n                    raise RuntimeError(\n                        f\"In order to use torchao pre-quantized model, you need to have torch>=2.5.0. However, the current version is {torch_version}.\"\n                        f\" You can also set with `weights_only=False` in `from_pretrained` if you don't want to update torch\"\n                    )\n\n    def update_dtype(self, dtype):\n        if self.quantization_config.quant_type == \"int4_weight_only\":\n            if dtype is not None and dtype != torch.bfloat16:\n                logger.warning_once(\n                    f\"Setting dtype to {dtype} for int4_weight_only quantization, but only bfloat16 is supported right now. Please set the dtype to bfloat16.\"\n                )\n            if dtype is None:\n                logger.warning_once(\n                    \"Setting dtype to torch.bfloat16 for int4_weight_only quantization since only bfloat16 is supported right now. Please set dtype=torch.bfloat16 to remove this warning.\"\n                )\n                dtype = torch.bfloat16\n        if self.quantization_config.quant_type == \"int8_dynamic_activation_int8_weight\":\n            if dtype is None:\n                logger.info(\n                    \"Setting dtype to torch.float32 for int8_dynamic_activation_int8_weight quantization as no dtype was specified in from_pretrained\"\n                )\n                # we need to set the dtype, otherwise we have dtype mismatch when performing the quantized linear op\n                dtype = torch.float32\n        return dtype\n\n    def get_state_dict_and_metadata(self, model, safe_serialization: Optional[bool] = False):\n        \"\"\"\n        If the model is safe serializable, we flatten the state dict of tensor subclasses so that it is compatible with\n        the safetensors format.\n        \"\"\"\n        if type(self.quantization_config.quant_type) in SUPPORTED_SAFE_SERIALIZATION_CONFIGS and safe_serialization:\n            if TORCHAO_VERSION >= version.parse(\"0.14.0\"):\n                return flatten_tensor_state_dict(model.state_dict())\n            else:\n                raise RuntimeError(\n                    f\"In order to use safetensors with torchao, please use torchao version >= 0.14.0. Current version: {TORCHAO_VERSION}\"\n                )\n        else:\n            return None, {}\n\n    def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        from accelerate.utils import CustomDtype\n\n        # Import AOBaseConfig directly since we know we have the right version\n        if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n            from torchao.core.config import AOBaseConfig\n\n            quant_type = self.quantization_config.quant_type\n            if isinstance(quant_type, AOBaseConfig):\n                # Extract size digit using fuzzy match on the class name\n                config_name = quant_type.__class__.__name__\n                size_digit = fuzzy_match_size(config_name)\n\n                # Map the extracted digit to appropriate dtype\n                if size_digit == \"4\":\n                    return CustomDtype.INT4\n                else:\n                    # Default to int8\n                    return torch.int8\n\n            # Original mapping for non-AOBaseConfig types\n            map_to_target_dtype = {\n                \"int4_weight_only\": CustomDtype.INT4,\n                \"int8_weight_only\": torch.int8,\n                \"int8_dynamic_activation_int8_weight\": torch.int8,\n                \"autoquant\": None,\n            }\n            return map_to_target_dtype[self.quantization_config.quant_type]\n        else:\n            raise ValueError(\n                \"You are using `device_map='auto'` on a torchao quantized model. To automatically compute\"\n                \" the appropriate device map, you should upgrade your `accelerate` library with \"\n                \"`pip install --upgrade accelerate`\"\n            )\n\n    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n        # need more space for the quantization parameters (e.g. scale). Tested with int4 wo and group size = 128\n        max_memory = {key: val * 0.9 for key, val in max_memory.items()}\n        return max_memory\n\n    def _process_model_before_weight_loading(\n        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs\n    ):\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n        if self.quantization_config.include_input_output_embeddings:\n            input_emb = model.get_input_embeddings()\n            input_emb_names = [name for name, module in model.named_modules() if id(module) == id(input_emb)]\n            output_emb = model.get_output_embeddings()\n            output_emb_names = [name for name, module in model.named_modules() if id(module) == id(output_emb)]\n            self.modules_to_not_convert = [\n                x for x in self.modules_to_not_convert if x not in input_emb_names + output_emb_names\n            ]\n        return\n\n    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n        return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.full_ao_keys)]\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        if self.quantization_config.quant_type == \"autoquant\":\n            return False\n\n        # check if the param_name is not in self.modules_to_not_convert\n        if any(key + \".\" in param_name or key == param_name for key in self.modules_to_not_convert):\n            return False\n        elif any(param_name.endswith(f\":{x}\") for x in self.full_ao_keys):\n            return True\n        else:\n            # we only quantize the weight of nn.Linear and nn.Embedding\n            module, tensor_name = get_module_from_name(model, param_name)\n            _QUANTIZABLE = [torch.nn.Linear]\n            if self.quantization_config.include_input_output_embeddings:\n                _QUANTIZABLE.append(torch.nn.Embedding)\n            return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n\n    def create_quantized_param(\n        self,\n        model: \"PreTrainedModel\",\n        param_value: \"torch.Tensor\",\n        param_name: str,\n        target_device: \"torch.device\",\n        **kwargs,\n    ):\n        \"\"\"\n        Each nn.Linear layer that needs to be quantized is processed here.\n        First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.\n        \"\"\"\n        from torchao.quantization import quantize_\n\n        full_name = param_name\n        # Those are the pre quantized weights\n        if \":\" in param_name:\n            param_name = param_name.rsplit(\":\", 1)[0]\n        module, tensor_name = get_module_from_name(model, param_name)\n\n        if self.pre_quantized:\n            # If it's a bias, no need to do anything special (except removing the \":_data\" part of the key, but was\n            # already done) - if it's unsafe-serialized (i.e. not safetensors), not need for anything either\n            is_unsafe_serialization = \":\" not in full_name\n            if tensor_name == \"bias\" or is_unsafe_serialization:\n                module._parameters[tensor_name] = torch.nn.Parameter(\n                    param_value.to(target_device), requires_grad=param_value.requires_grad\n                )\n                return\n            # Sanity check for the new serialization format\n            elif not (TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(self.metadata)):\n                raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.14.0` installed\")\n\n            # Save the states for later quantization when they are all gathered\n            if not hasattr(self, \"ao_params\"):\n                self.ao_params = defaultdict(dict)\n            self.ao_params[param_name].update({full_name: param_value})\n\n            # We are ready for quantization in this case (we retrieved all the needed keys)\n            if len(self.ao_params[param_name]) == len(self.weight_ao_keys):\n                new_param = unflatten_tensor_state_dict(self.ao_params[param_name], self.metadata)[param_name]\n                # Set it\n                module._parameters[tensor_name] = torch.nn.Parameter(\n                    new_param.to(target_device), requires_grad=new_param.requires_grad\n                )\n\n                # Free memory\n                del self.ao_params[param_name]\n\n            # Add repr to the module\n            if isinstance(module, nn.Linear):\n                module.extra_repr = types.MethodType(_linear_extra_repr, module)\n        else:\n            module._parameters[tensor_name] = torch.nn.Parameter(\n                param_value, requires_grad=param_value.requires_grad\n            ).to(target_device)\n            # if we are quantizing tied parameters, to avoid tying the quantized weights\n            # the correct order to do it is\n            # 1. load the weight to model\n            # 2. run tie_weights to populate the weights\n            # 3. quantize\n            input_embed = model.get_input_embeddings()\n            if self.quantization_config.untie_embedding_weights and id(module) == id(input_embed):\n                model.tie_weights()\n                setattr(model.config.get_text_config(decoder=True), \"tie_word_embeddings\", False)\n\n            # handle ModuleFqnToConfig, introduced in torchao 0.12.0+\n            if self.quantization_config._get_ao_version() >= version.Version(\"0.12.0\"):\n                from torchao.quantization import ModuleFqnToConfig\n\n                config = self.quantization_config.get_apply_tensor_subclass()\n                if isinstance(config, ModuleFqnToConfig):\n                    module_fqn, _ = param_name.rsplit(\".\", 1)\n                    c = None\n                    if module_fqn in config.module_fqn_to_config:\n                        assert not module_fqn.startswith(\"re:\"), (\n                            \"module fqn should not start with`re:`, which is used for specifying regex\"\n                        )\n                        c = config.module_fqn_to_config[module_fqn]\n                    else:\n                        for maybe_module_fqn_pattern in config.module_fqn_to_config:\n                            if not maybe_module_fqn_pattern.startswith(\"re:\"):\n                                continue\n                            elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n                                # we'll apply the config for first fully matched pattern\n                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n                                break\n                        else:\n                            c = config.module_fqn_to_config.get(\"_default\", None)\n\n                    if c is not None:\n                        # filter_fn: not filtering out any modules\n                        quantize_(module, c, filter_fn=lambda x, fqn: True)\n                    return\n\n            quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n\n    def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n        \"\"\"\n        Setting model attributes and/or converting model before weights loading. At this point\n        the model should be initialized on the meta device so you can freely manipulate the skeleton\n        of the model in order to replace modules in-place. Make sure to override the abstract method `_process_model_before_weight_loading`.\n\n        Args:\n            model (`~transformers.PreTrainedModel`):\n                The model to quantize\n            kwargs (`dict`, *optional*):\n                The keyword arguments that are passed along `_process_model_before_weight_loading`.\n        \"\"\"\n        super().preprocess_model(model, config, dtype, checkpoint_files, **kwargs)\n        # Torchao needs access to all metadata later\n        self.set_metadata(checkpoint_files)\n\n    def _process_model_after_weight_loading(self, model, **kwargs):\n        \"\"\"No process required for torchao quantized model\"\"\"\n        if self.quantization_config.quant_type == \"autoquant\":\n            from torchao import autoquant\n            from torchao.quantization import ALL_AUTOQUANT_CLASS_LIST\n\n            model = torch.compile(model, mode=\"max-autotune\")\n            model = autoquant(\n                model,\n                qtensor_class_list=ALL_AUTOQUANT_CLASS_LIST,\n                set_inductor_config=False,\n                **self.quantization_config.quant_type_kwargs,\n            )\n            return model\n        return\n\n    def is_serializable(self, safe_serialization=None) -> bool:\n        if safe_serialization:\n            _is_torchao_serializable = type(\n                self.quantization_config.quant_type\n            ) in SUPPORTED_SAFE_SERIALIZATION_CONFIGS and TORCHAO_VERSION >= version.parse(\"0.14.0\")\n            if not _is_torchao_serializable:\n                logger.warning(\n                    f\"torchao quantized model only supports safe serialization for {SUPPORTED_SAFE_SERIALIZATION_CONFIGS}, \\\n                    and torchao version >= 0.14.0, please set `safe_serialization` to False for \\\n                    {type(self.quantization_config.quant_type)} and {TORCHAO_VERSION}.\"\n                )\n            return _is_torchao_serializable\n\n        _is_torchao_serializable = version.parse(importlib.metadata.version(\"huggingface_hub\")) >= version.parse(\n            \"0.25.0\"\n        )\n        if not _is_torchao_serializable:\n            logger.warning(\"torchao quantized model is only serializable after huggingface_hub >= 0.25.0 \")\n        if self.offload and self.quantization_config.modules_to_not_convert is None:\n            logger.warning(\n                \"The model contains offloaded modules and these modules are not quantized. We don't recommend saving the model as we won't be able to reload them.\"\n                \"If you want to specify modules to not quantize, please specify modules_to_not_convert in the quantization_config.\"\n            )\n            return False\n        return _is_torchao_serializable\n\n    def get_accelerator_warm_up_factor(self):\n        \"\"\"\n        This factor is used in caching_allocator_warmup to determine how many bytes to pre-allocate for accelerator warmup.\n        - A factor of 2 means we pre-allocate the full memory footprint of the model.\n        - A factor of 4 means we pre-allocate half of that, and so on\n\n        However, when using TorchAO, calculating memory usage with param.numel() * param.element_size() doesn't give the correct size for quantized weights (like int4 or int8)\n        That's because TorchAO internally represents quantized tensors using subtensors and metadata, and the reported element_size() still corresponds to the dtype\n        not the actual bit-width of the quantized data.\n\n        To correct for this:\n        - Use a division factor of 8 for int4 weights\n        - Use a division factor of 4 for int8 weights\n        \"\"\"\n        if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n            from torchao.core.config import AOBaseConfig\n\n            quant_type = self.quantization_config.quant_type\n            # For autoquant case, it will be treated in the string implementation below in map_to_target_dtype\n            if isinstance(quant_type, AOBaseConfig):\n                # Extract size digit using fuzzy match on the class name\n                config_name = quant_type.__class__.__name__\n                size_digit = fuzzy_match_size(config_name)\n\n                if size_digit == \"4\":\n                    return 8\n                else:\n                    return 4\n\n        # Original mapping for non-AOBaseConfig types\n        map_to_target_dtype = {\n            \"int4_weight_only\": 8,\n            \"int8_weight_only\": 4,\n            \"int8_dynamic_activation_int8_weight\": 4,\n            \"autoquant\": 4,\n        }\n\n        return map_to_target_dtype[self.quantization_config.quant_type]\n\n    @property\n    def is_trainable(self) -> bool:\n        supported_quant_types_for_training = [\n            \"int8_weight_only\",\n            \"int8_dynamic_activation_int8_weight\",\n        ]\n        return self.quantization_config.quant_type in supported_quant_types_for_training\n\n    @property\n    def is_compileable(self) -> bool:\n        return True\n\n    def set_metadata(self, checkpoint_files: list[str]):\n        if checkpoint_files[0].endswith(\".safetensors\"):\n            metadata = {}\n            for checkpoint in checkpoint_files:\n                with safe_open(checkpoint, framework=\"pt\") as f:\n                    metadata_ = f.metadata() or {}\n                    metadata.update(metadata_)\n            # Save it\n            self.metadata = metadata"
                },
                "component_dependencies": {
                    "TorchAoHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_torchao.py#_linear_extra_repr",
                        "transformers/quantizers/quantizer_torchao.py#fuzzy_match_size",
                        "transformers/quantizers/quantizer_torchao.py#logger",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils.py#is_torchao_available"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_vptq.py#VptqHfQuantizer": {
                "sorted_modules": {
                    "VptqHfQuantizer": "\n\nclass VptqHfQuantizer(HfQuantizer):\n    \"\"\"\n    Quantizer of the VPTQ method. Enables the loading of prequantized models.\n    \"\"\"\n\n    requires_calibration = True\n    required_packages = [\"vptq\"]\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        super().__init__(quantization_config, **kwargs)\n        self.quantization_config = quantization_config\n\n    def validate_environment(self, *args, **kwargs):\n        if not is_accelerate_available():\n            raise ImportError(\"Using `vptq` quantization requires Accelerate: `pip install accelerate`\")\n\n        if not is_vptq_available():\n            raise ImportError(\"Using `vptq` quantization requires VPTQ>=0.0.4: `pip install -U vptq`\")\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        if dtype is None:\n            if torch.cuda.is_available():\n                dtype = torch.float16\n                logger.info(\n                    \"CUDA available. Assuming VPTQ inference on GPU and loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\"\n                )\n            else:\n                import vptq\n\n                device_availability = getattr(vptq, \"device_availability\", lambda device: False)\n                if device_availability(\"cpu\") is True:\n                    raise RuntimeError(\"No GPU found. Please wait for the next release of VPTQ to use CPU inference\")\n                dtype = torch.float32\n                logger.info(\"No GPU found. Assuming VPTQ inference on CPU and loading the model in `torch.float32`.\")\n        return dtype\n\n    def _process_model_before_weight_loading(\n        self,\n        model: \"PreTrainedModel\",\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        we don't have param like modules_to_not_convert to indicate which layers should not be quantized\n        because `quantization_config` include the layers that should be quantized\n        \"\"\"\n        from ..integrations import replace_with_vptq_linear\n\n        self.modules_to_not_convert = self.get_modules_to_not_convert(\n            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n        )\n\n        replace_with_vptq_linear(\n            model,\n            quantization_config=self.quantization_config,\n            modules_to_not_convert=self.modules_to_not_convert,\n        )\n        model.config.quantization_config = self.quantization_config\n\n    @property\n    def is_trainable(self) -> bool:\n        return False\n\n    def is_serializable(self, safe_serialization=None):\n        return True"
                },
                "component_dependencies": {
                    "VptqHfQuantizer": [
                        "transformers/quantizers/base.py#HfQuantizer",
                        "transformers/quantizers/quantizer_vptq.py#logger",
                        "transformers/utils.py#is_accelerate_available",
                        "transformers/utils.py#is_vptq_available",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/utils/quantization_config.py#QuantizationMethod": {
                "sorted_modules": {
                    "QuantizationMethod": "\n\nclass QuantizationMethod(str, Enum):\n    BITS_AND_BYTES = \"bitsandbytes\"\n    GPTQ = \"gptq\"\n    AWQ = \"awq\"\n    AQLM = \"aqlm\"\n    VPTQ = \"vptq\"\n    QUANTO = \"quanto\"\n    EETQ = \"eetq\"\n    HIGGS = \"higgs\"\n    HQQ = \"hqq\"\n    COMPRESSED_TENSORS = \"compressed-tensors\"\n    FBGEMM_FP8 = \"fbgemm_fp8\"\n    TORCHAO = \"torchao\"\n    BITNET = \"bitnet\"\n    SPQR = \"spqr\"\n    FP8 = \"fp8\"\n    QUARK = \"quark\"\n    FPQUANT = \"fp_quant\"\n    AUTOROUND = \"auto-round\"\n    MXFP4 = \"mxfp4\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/quantization_config.py#AWQLinearVersion": {
                "sorted_modules": {
                    "AWQLinearVersion": "\n\nclass AWQLinearVersion(str, Enum):\n    GEMM = \"gemm\"\n    GEMV = \"gemv\"\n    EXLLAMA = \"exllama\"\n    IPEX = \"ipex\"\n\n    @staticmethod\n    def from_str(version: str):\n        version = version.lower()\n        if version == \"gemm\":\n            return AWQLinearVersion.GEMM\n        elif version == \"gemv\":\n            return AWQLinearVersion.GEMV\n        elif version == \"exllama\":\n            return AWQLinearVersion.EXLLAMA\n        elif version == \"ipex\":\n            return AWQLinearVersion.IPEX\n        else:\n            raise ValueError(f\"Unknown AWQLinearVersion {version}\")"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/quantization_config.py#AwqBackendPackingMethod": {
                "sorted_modules": {
                    "AwqBackendPackingMethod": "\n\nclass AwqBackendPackingMethod(str, Enum):\n    AUTOAWQ = \"autoawq\"\n    LLMAWQ = \"llm-awq\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/quantization_config.py#ExllamaVersion": {
                "sorted_modules": {
                    "ExllamaVersion": "\n\nclass ExllamaVersion(int, Enum):\n    ONE = 1\n    TWO = 2"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#is_auto_gptq_available": {
                "sorted_modules": {
                    "is_auto_gptq_available": "\n\n@lru_cache\ndef is_auto_gptq_available() -> bool:\n    return _is_package_available(\"auto_gptq\")"
                },
                "component_dependencies": {
                    "is_auto_gptq_available": [
                        "transformers/utils/import_utils.py#_is_package_available"
                    ]
                },
                "warning": null
            },
            "transformers/utils/import_utils.py#ACCELERATE_MIN_VERSION": {
                "sorted_modules": {
                    "ACCELERATE_MIN_VERSION": "\nACCELERATE_MIN_VERSION = \"1.1.0\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/import_utils.py#PACKAGE_DISTRIBUTION_MAPPING": {
                "sorted_modules": {
                    "PACKAGE_DISTRIBUTION_MAPPING": "\n\nPACKAGE_DISTRIBUTION_MAPPING = importlib.metadata.packages_distributions()"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#prepare_fa_kwargs_from_position_ids": {
                "sorted_modules": {
                    "prepare_fa_kwargs_from_position_ids": "\n\ndef prepare_fa_kwargs_from_position_ids(position_ids):\n    \"\"\"\n    This function returns all the necessary kwargs to call `flash_attn_varlen_func` extracted from position_ids.\n\n    Arguments:\n        position_ids (`torch.Tensor`):\n            Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n\n    Return:\n        (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n            The cumulative sequence lengths for the target (query) and source (key, value), used to index into\n            ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n        (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n            Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query,\n            `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n    \"\"\"\n    tensor_kwargs = {\"dtype\": torch.int32, \"device\": position_ids.device}\n\n    position_ids = position_ids.view(-1)\n    indices_q = (position_ids == 0).nonzero().view(-1)\n\n    cu_seq_lens_q = torch.cat(\n        (\n            indices_q.to(**tensor_kwargs),\n            torch.tensor(position_ids.size(), **tensor_kwargs),\n        )\n    )\n    cu_seq_lens_k = cu_seq_lens_q\n\n    # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n    # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n    # for some models (e.g. qwen2-vl).\n    max_length_q = cu_seq_lens_q.diff().max()\n    # NOTE: With torch compile, this will cause a graph break if you don't set\n    # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n    # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n    # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n    # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n    max_length_q = max_length_q.item()\n    max_length_k = max_length_q\n\n    return (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/modeling_flash_attention_utils.py#_get_unpad_data": {
                "sorted_modules": {
                    "_get_unpad_data": "\n\ndef _get_unpad_data(attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, int]:\n    \"\"\"\n    Retrieves indexing data required to repad unpadded (ragged) tensors.\n\n    Arguments:\n        attention_mask (`torch.Tensor`):\n            Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n\n    Return:\n        indices (`torch.Tensor`):\n            The indices of non-masked tokens from the flattened input sequence.\n        cu_seqlens (`torch.Tensor`):\n            The cumulative sequence lengths, used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n        max_seqlen_in_batch (`int`):\n            Maximum sequence length in batch.\n    \"\"\"\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    # NOTE: Similar to the `.item()` in prepare_fa2_from_position_ids, with torch compile,\n    # this might cause a graph break\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/image_transforms.py#_center_to_corners_format_numpy": {
                "sorted_modules": {
                    "_center_to_corners_format_numpy": "\n\ndef _center_to_corners_format_numpy(bboxes_center: np.ndarray) -> np.ndarray:\n    center_x, center_y, width, height = bboxes_center.T\n    bboxes_corners = np.stack(\n        # top left x, top left y, bottom right x, bottom right y\n        [center_x - 0.5 * width, center_y - 0.5 * height, center_x + 0.5 * width, center_y + 0.5 * height],\n        axis=-1,\n    )\n    return bboxes_corners"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/image_transforms.py#_center_to_corners_format_torch": {
                "sorted_modules": {
                    "_center_to_corners_format_torch": "\n\ndef _center_to_corners_format_torch(bboxes_center: \"torch.Tensor\") -> \"torch.Tensor\":\n    center_x, center_y, width, height = bboxes_center.unbind(-1)\n    bbox_corners = torch.stack(\n        # top left x, top left y, bottom right x, bottom right y\n        [(center_x - 0.5 * width), (center_y - 0.5 * height), (center_x + 0.5 * width), (center_y + 0.5 * height)],\n        dim=-1,\n    )\n    return bbox_corners"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#translate_gt": {
                "sorted_modules": {
                    "translate_gt": "\n\ndef translate_gt(gt: torch.Tensor, max_num_bins: int, reg_scale: int, up: torch.Tensor):\n    \"\"\"\n    Decodes bounding box ground truth (GT) values into distribution-based GT representations.\n\n    This function maps continuous GT values into discrete distribution bins, which can be used\n    for regression tasks in object detection models. It calculates the indices of the closest\n    bins to each GT value and assigns interpolation weights to these bins based on their proximity\n    to the GT value.\n\n    Args:\n        gt (Tensor): Ground truth bounding box values, shape (N, ).\n        max_num_bins (int): Maximum number of discrete bins for the distribution.\n        reg_scale (float): Controls the curvature of the Weighting Function.\n        up (Tensor): Controls the upper bounds of the Weighting Function.\n\n    Returns:\n        tuple[Tensor, Tensor, Tensor]:\n            - indices (Tensor): Index of the left bin closest to each GT value, shape (N, ).\n            - weight_right (Tensor): Weight assigned to the right bin, shape (N, ).\n            - weight_left (Tensor): Weight assigned to the left bin, shape (N, ).\n    \"\"\"\n    gt = gt.reshape(-1)\n    function_values = weighting_function(max_num_bins, up, reg_scale)\n\n    # Find the closest left-side indices for each value\n    diffs = function_values.unsqueeze(0) - gt.unsqueeze(1)\n    mask = diffs <= 0\n    closest_left_indices = torch.sum(mask, dim=1) - 1\n\n    # Calculate the weights for the interpolation\n    indices = closest_left_indices.float()\n\n    weight_right = torch.zeros_like(indices)\n    weight_left = torch.zeros_like(indices)\n\n    valid_idx_mask = (indices >= 0) & (indices < max_num_bins)\n    valid_indices = indices[valid_idx_mask].long()\n\n    # Obtain distances\n    left_values = function_values[valid_indices]\n    right_values = function_values[valid_indices + 1]\n\n    left_diffs = torch.abs(gt[valid_idx_mask] - left_values)\n    right_diffs = torch.abs(right_values - gt[valid_idx_mask])\n\n    # Valid weights\n    weight_right[valid_idx_mask] = left_diffs / (left_diffs + right_diffs)\n    weight_left[valid_idx_mask] = 1.0 - weight_right[valid_idx_mask]\n\n    # Invalid weights (out of range)\n    invalid_idx_mask_neg = indices < 0\n    weight_right[invalid_idx_mask_neg] = 0.0\n    weight_left[invalid_idx_mask_neg] = 1.0\n    indices[invalid_idx_mask_neg] = 0.0\n\n    invalid_idx_mask_pos = indices >= max_num_bins\n    weight_right[invalid_idx_mask_pos] = 1.0\n    weight_left[invalid_idx_mask_pos] = 0.0\n    indices[invalid_idx_mask_pos] = max_num_bins - 0.1\n\n    return indices, weight_right, weight_left"
                },
                "component_dependencies": {
                    "translate_gt": [
                        "transformers/loss/loss_d_fine.py#weighting_function"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#box_area": {
                "sorted_modules": {
                    "box_area": "\n\ndef box_area(boxes: Tensor) -> Tensor:\n    \"\"\"\n    Computes the area of a set of bounding boxes, which are specified by its (x1, y1, x2, y2) coordinates.\n\n    Args:\n        boxes (`torch.FloatTensor` of shape `(number_of_boxes, 4)`):\n            Boxes for which the area will be computed. They are expected to be in (x1, y1, x2, y2) format with `0 <= x1\n            < x2` and `0 <= y1 < y2`.\n\n    Returns:\n        `torch.FloatTensor`: a tensor containing the area for each box.\n    \"\"\"\n    boxes = _upcast(boxes)\n    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])"
                },
                "component_dependencies": {
                    "box_area": [
                        "transformers/loss/loss_for_object_detection.py#_upcast"
                    ]
                },
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#NestedTensor": {
                "sorted_modules": {
                    "NestedTensor": "\n\nclass NestedTensor:\n    def __init__(self, tensors, mask: Optional[Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n\n    def to(self, device):\n        cast_tensor = self.tensors.to(device)\n        mask = self.mask\n        if mask is not None:\n            cast_mask = mask.to(device)\n        else:\n            cast_mask = None\n        return NestedTensor(cast_tensor, cast_mask)\n\n    def decompose(self):\n        return self.tensors, self.mask\n\n    def __repr__(self):\n        return str(self.tensors)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#_max_by_axis": {
                "sorted_modules": {
                    "_max_by_axis": "\n\n# below: taken from https://github.com/facebookresearch/detr/blob/master/util/misc.py#L306\ndef _max_by_axis(the_list):\n    # type: (list[list[int]]) -> list[int]\n    maxes = the_list[0]\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):\n            maxes[index] = max(maxes[index], item)\n    return maxes"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/utils/quantization_config.py#VptqLayerConfig": {
                "sorted_modules": {
                    "VptqLayerConfig": "\n\n@dataclass\nclass VptqLayerConfig(QuantizationConfigMixin):\n    \"\"\"\n    This is used to explain vptq config params for each layer\n    Args:\n        enable_norm (`bool`, *optional*, defaults to `True`): to control if we have scale/bias for fp-weight\n        enable_perm (`bool`, *optional*, defaults to `True`): to perm input_channel or not\n        group_num (`int`, *optional*, defaults to `1`): how many single groups for vector-quantization\n        group_size (`int`, *optional*, defaults to `-1`): depends on out-features\n        indices_as_float (`bool`, *optional*, defaults to `False`): for Finetuning\n        is_indice_packed (`bool`, *optional*, defaults to `True`): should always be True\n        num_centroids (`list`, *optional*, defaults to `[-1, -1]`): centroid numbers of clusters\n        num_res_centroids (`list`, *optional*, defaults to `[-1, -1]`): ditto for residual\n        outlier_size (`int`, *optional*, defaults to `1`): outliers\n        vector_lens (`list`, *optional*, defaults to `[-1, -1]`): centroid vector length in quantization\n    \"\"\"\n\n    def __init__(\n        self,\n        enable_norm: bool = True,\n        enable_perm: bool = True,\n        group_num: int = 1,\n        group_size: int = -1,\n        in_features: int = -1,\n        indices_as_float: bool = False,\n        is_indice_packed: bool = True,\n        num_centroids: list = [-1, -1],\n        num_res_centroids: list = [-1, -1],\n        out_features: int = -1,\n        outlier_size: int = 0,\n        vector_lens: list = [-1, -1],\n        **kwargs,\n    ):\n        self.enable_norm = enable_norm\n        self.enable_perm = enable_perm\n        self.group_num = group_num\n        self.group_size = group_size\n        self.in_features = in_features\n        self.indices_as_float = indices_as_float\n        self.is_indice_packed = is_indice_packed\n        self.num_centroids = num_centroids\n        self.num_res_centroids = num_res_centroids\n        self.out_features = out_features\n        self.outlier_size = outlier_size\n        self.vector_lens = vector_lens\n        self.post_init()\n\n    def post_init(self):\n        r\"\"\"\n        Safety checker that arguments are correct\n        \"\"\"\n        if self.is_indice_packed is False:\n            raise ValueError(\"is_indice_packed should always be True\")"
                },
                "component_dependencies": {
                    "VptqLayerConfig": [
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/base.py#HfQuantizer": {
                "sorted_modules": {
                    "HfQuantizer": "\n\nclass HfQuantizer(ABC):\n    \"\"\"\n    Abstract class of the HuggingFace quantizer. Supports for now quantizing HF transformers models for inference and/or quantization.\n    This class is used only for transformers.PreTrainedModel.from_pretrained and cannot be easily used outside the scope of that method\n    yet.\n\n    Attributes\n        quantization_config (`transformers.utils.quantization_config.QuantizationConfigMixin`):\n            The quantization config that defines the quantization parameters of your model that you want to quantize.\n        modules_to_not_convert (`list[str]`, *optional*):\n            The list of module names to not convert when quantizing the model.\n        required_packages (`list[str]`, *optional*):\n            The list of required pip packages to install prior to using the quantizer\n        requires_calibration (`bool`):\n            Whether the quantization method requires to calibrate the model before using it.\n        requires_parameters_quantization (`bool`):\n            Whether the quantization method requires to create a new Parameter. For example, for bitsandbytes, it is\n            required to create a new xxxParameter in order to properly quantize the model.\n    \"\"\"\n\n    requires_calibration = False\n    required_packages = None\n    requires_parameters_quantization = False\n\n    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n        self.quantization_config = quantization_config\n\n        # -- Handle extra kwargs below --\n        self.modules_to_not_convert = kwargs.pop(\"modules_to_not_convert\", [])\n        self.pre_quantized = kwargs.pop(\"pre_quantized\", True)\n\n        if not self.pre_quantized and self.requires_calibration:\n            raise ValueError(\n                f\"The quantization method {quantization_config.quant_method} does require the model to be pre-quantized.\"\n                f\" You explicitly passed `pre_quantized=False` meaning your model weights are not quantized. Make sure to \"\n                f\"pass `pre_quantized=True` while knowing what you are doing.\"\n            )\n\n    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        \"\"\"\n        Some quantization methods require to explicitly set the dtype of the model to a\n        target dtype. You need to override this method in case you want to make sure that behavior is\n        preserved\n\n        Args:\n            dtype (`torch.dtype`):\n                The input dtype that is passed in `from_pretrained`\n        \"\"\"\n        return dtype\n\n    def update_device_map(self, device_map: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n        \"\"\"\n        Override this method if you want to pass a override the existing device map with a new\n        one. E.g. for bitsandbytes, since `accelerate` is a hard requirement, if no device_map is\n        passed, the device_map is set to `\"auto\"``\n\n        Args:\n            device_map (`Union[dict, str]`, *optional*):\n                The device_map that is passed through the `from_pretrained` method.\n        \"\"\"\n        return device_map\n\n    def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n        \"\"\"\n        Override this method if you want to adjust the `target_dtype` variable used in `from_pretrained`\n        to compute the device_map in case the device_map is a `str`. E.g. for bitsandbytes we force-set `target_dtype`\n        to `torch.int8` and for 4-bit we pass a custom enum `accelerate.CustomDtype.int4`.\n\n        Args:\n            dtype (`torch.dtype`, *optional*):\n                The dtype that is used to compute the device_map.\n        \"\"\"\n        return dtype\n\n    def param_element_size(self, model: \"PreTrainedModel\", param_name: str) -> float:\n        \"Return the element size (in bytes) for `param_name`.\"\n        if self.param_needs_quantization(model, param_name):\n            from accelerate.utils import CustomDtype\n\n            mapping = {\n                torch.int8: 1,\n                CustomDtype.INT4: 0.5,\n                CustomDtype.FP8: 1,\n                CustomDtype.INT2: 0.25,\n            }\n            # The value passed is actually not used when the method is overriden\n            if (custom_dtype := self.adjust_target_dtype(torch.float16)) in mapping:\n                return mapping[custom_dtype]\n        return model.get_parameter_or_buffer(param_name).element_size()\n\n    def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n        \"\"\"\n        Override this method if you want to adjust the `missing_keys`.\n\n        Args:\n            missing_keys (`list[str]`, *optional*):\n                The list of missing keys in the checkpoint compared to the state dict of the model\n        \"\"\"\n        return missing_keys\n\n    def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]:\n        \"\"\"\n        Override this method if you want to adjust the `update_expected_keys`.\n\n        Args:\n            expected_keys (`list[str]`, *optional*):\n                The list of the expected keys in the initialized model.\n            loaded_keys (`list[str]`, *optional*):\n                The list of the loaded keys in the checkpoint.\n        \"\"\"\n        return expected_keys\n\n    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n        return unexpected_keys\n\n    def get_special_dtypes_update(self, model, dtype: \"torch.dtype\") -> dict[str, \"torch.dtype\"]:\n        \"\"\"\n        returns dtypes for modules that are not quantized - used for the computation of the device_map in case\n        one passes a str as a device_map. The method will use the `modules_to_not_convert` that is modified\n        in `_process_model_before_weight_loading`.\n\n        Args:\n            model (`~transformers.PreTrainedModel`):\n                The model to quantize\n            dtype (`torch.dtype`):\n                The dtype passed in `from_pretrained` method.\n        \"\"\"\n\n        return {\n            name: dtype for name, _ in model.named_parameters() if any(m in name for m in self.modules_to_not_convert)\n        }\n\n    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n        \"\"\"adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization\"\"\"\n        return max_memory\n\n    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n        \"\"\"\n        Check whether a given param needs quantization as defined by `create_quantized_param`.\n        \"\"\"\n        return False\n\n    def create_quantized_param(self, *args, **kwargs):\n        \"\"\"\n        Take needed components from state_dict (those from which `param_needs_quantization` is True) and create\n        quantized param.\n        It usually also load the new param directly in the `model`.\n        Note: only applicable if requires_parameters_quantization == True.\n        \"\"\"\n        if not self.requires_parameters_quantization:\n            raise AttributeError(\n                f\"`.create_quantized_param()` method is not supported by quantizer class {self.__class__.__name__}.\"\n            )\n\n    def validate_environment(self, *args, **kwargs):\n        \"\"\"\n        This method is used to potentially check for potential conflicts with arguments that are\n        passed in `from_pretrained`. You need to define it for all future quantizers that are integrated with transformers.\n        If no explicit check are needed, simply return nothing.\n        \"\"\"\n        return\n\n    def update_tp_plan(self, config):\n        \"updates the tp plan for the scales\"\n        return config\n\n    def update_ep_plan(self, config):\n        \"updates the tp plan for the scales\"\n        return config\n\n    def _process_model_before_weight_loading(self, model, **kwargs):\n        return model\n\n    def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n        \"\"\"\n        Setting model attributes and/or converting model before weights loading. At this point\n        the model should be initialized on the meta device so you can freely manipulate the skeleton\n        of the model in order to replace modules in-place. Make sure to override the abstract method `_process_model_before_weight_loading`.\n\n        Args:\n            model (`~transformers.PreTrainedModel`):\n                The model to quantize\n            kwargs (`dict`, *optional*):\n                The keyword arguments that are passed along `_process_model_before_weight_loading`.\n        \"\"\"\n        model.is_quantized = True\n        model.quantization_method = self.quantization_config.quant_method\n        if self.pre_quantized:\n            self._convert_model_for_quantization(model)\n        self._process_model_before_weight_loading(model, **kwargs)\n\n        # We store the original dtype for quantized models as we cannot easily retrieve it\n        # once the weights have been quantized\n        # Note that once you have loaded a quantized model, you can't change its dtype so this will\n        # remain a single source of truth\n        original_dtype = dtype if dtype is not None else torch.get_default_dtype()\n        config._pre_quantization_dtype = original_dtype\n        _assign_original_dtype(model, original_dtype)\n\n    def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n        return model\n\n    def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n        \"\"\"\n        Post-process the model post weights loading.\n        Make sure to override the abstract method `_process_model_after_weight_loading`.\n\n        Args:\n            model (`~transformers.PreTrainedModel`):\n                The model to quantize\n            kwargs (`dict`, *optional*):\n                The keyword arguments that are passed along `_process_model_after_weight_loading`.\n        \"\"\"\n        return self._process_model_after_weight_loading(model, **kwargs)\n\n    def remove_quantization_config(self, model):\n        \"\"\"\n        Remove the quantization config from the model.\n        \"\"\"\n        if hasattr(model, \"hf_quantizer\"):\n            del model.hf_quantizer\n        if hasattr(model.config, \"quantization_config\"):\n            del model.config.quantization_config\n        if hasattr(model.config, \"_pre_quantization_dtype\"):\n            del model.config._pre_quantization_dtype\n        if hasattr(model, \"quantization_method\"):\n            del model.quantization_method\n        model.is_quantized = False\n\n    def dequantize(self, model):\n        \"\"\"\n        Potentially dequantize the model to retrieve the original model, with some loss in accuracy / performance.\n        Note not all quantization schemes support this.\n        \"\"\"\n        model = self._dequantize(model)\n\n        # Delete quantizer and quantization config\n        del model.hf_quantizer\n        del model.config.quantization_config\n        del model.config._pre_quantization_dtype\n        del model.quantization_method\n        model.is_quantized = False\n\n        return model\n\n    def get_accelerator_warm_up_factor(self):\n        \"\"\"\n        The factor to be used in `caching_allocator_warmup` to get the number of bytes to pre-allocate to warm up accelerator.\n        A factor of 2 means we allocate all bytes in the empty model (since we allocate in fp16), a factor of 4 means\n        we allocate half the memory of the weights residing in the empty model, etc...\n        \"\"\"\n        # By default we return 4, i.e. half the model size (this corresponds to the case where the model is not\n        # really pre-processed, i.e. we do not have the info that weights are going to be 8 bits before actual\n        # weight loading)\n        return 4\n\n    def _dequantize(self, model):\n        raise NotImplementedError(\n            f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n        )\n\n    def get_param_name(self, param_name: str) -> str:\n        \"\"\"\n        Override this method if you want to adjust the `param_name`.\n        \"\"\"\n        return param_name\n\n    @staticmethod\n    def get_modules_to_not_convert(\n        model: \"PreTrainedModel\",\n        skip_modules: Optional[list[str]] = None,\n        keep_in_fp32_modules: Optional[list[str]] = None,\n        add_default_skips: bool = False,\n    ):\n        from ..integrations import get_keys_to_not_convert\n\n        if skip_modules is None or add_default_skips:\n            modules_to_not_convert = get_keys_to_not_convert(model)\n        else:\n            modules_to_not_convert = []\n\n        if skip_modules is not None:\n            modules_to_not_convert.extend(skip_modules)\n\n        if keep_in_fp32_modules is not None:\n            modules_to_not_convert.extend(keep_in_fp32_modules)\n\n        return modules_to_not_convert\n\n    @property\n    def is_qat_trainable(self) -> bool:\n        \"\"\"Flag indicating whether the quantized model can carry out quantization aware training\"\"\"\n        return False\n\n    @property\n    def is_compileable(self) -> bool:\n        \"\"\"Flag indicating whether the quantized model can be compiled\"\"\"\n        return False\n\n    def get_state_dict_and_metadata(self, model, safe_serialization=False):\n        \"\"\"Get state dict and metadata. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n        return None, {}\n\n    def update_state_dict_with_metadata(self, state_dict, metadata):\n        \"\"\"Update state dict with metadata. Default behaviour returns state_dict\"\"\"\n        return state_dict\n\n    @abstractmethod\n    def is_serializable(self, safe_serialization=None): ...\n\n    @property\n    @abstractmethod\n    def is_trainable(self): ...\n\n    def _convert_model_for_quantization(self, model):\n        from accelerate import init_empty_weights\n\n        for name, module in model.named_modules():\n            module_class_name = module.__class__.__name__\n            if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION and (\n                self.quantization_config.quant_method\n                in MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"quantization_methods\"]\n            ):\n                with init_empty_weights():\n                    parent_module, name = get_module_from_name(model, name)\n                    parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"module_name\"](\n                        model.config.get_text_config()\n                    )"
                },
                "component_dependencies": {
                    "HfQuantizer": [
                        "transformers/quantizers/base.py#MODULES_TO_PATCH_FOR_QUANTIZATION",
                        "transformers/quantizers/base.py#_assign_original_dtype",
                        "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                        "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_quark.py#CHECKPOINT_KEYS": {
                "sorted_modules": {
                    "CHECKPOINT_KEYS": "\n\nCHECKPOINT_KEYS = {\n    \"weight_scale\": \"weight_quantizer.scale\",\n    \"bias_scale\": \"bias_quantizer.scale\",\n    \"input_scale\": \"input_quantizer.scale\",\n    \"output_scale\": \"output_quantizer.scale\",\n    \"weight_zero_point\": \"weight_quantizer.zero_point\",\n    \"bias_zero_point\": \"bias_quantizer.zero_point\",\n    \"input_zero_point\": \"input_quantizer.zero_point\",\n    \"output_zero_point\": \"output_quantizer.zero_point\",\n}"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/quantizers/quantizer_torchao.py#_linear_extra_repr": {
                "sorted_modules": {
                    "_linear_extra_repr": "\n\ndef _linear_extra_repr(self):\n    weight = _quantization_type(self.weight)\n    if weight is None:\n        return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight=None\"\n    else:\n        return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight={weight}\""
                },
                "component_dependencies": {
                    "_linear_extra_repr": [
                        "transformers/quantizers/quantizer_torchao.py#_quantization_type"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/quantizer_torchao.py#fuzzy_match_size": {
                "sorted_modules": {
                    "fuzzy_match_size": "\n\ndef fuzzy_match_size(config_name: str) -> Optional[str]:\n    \"\"\"\n    Extract the size digit from strings like \"4weight\", \"8weight\".\n    Returns the digit as an integer if found, otherwise None.\n    \"\"\"\n    config_name = config_name.lower()\n\n    str_match = re.search(r\"(\\d)weight\", config_name)\n\n    if str_match:\n        return str_match.group(1)\n\n    return None"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_d_fine.py#weighting_function": {
                "sorted_modules": {
                    "weighting_function": "\n\ndef weighting_function(max_num_bins: int, up: torch.Tensor, reg_scale: int) -> torch.Tensor:\n    \"\"\"\n    Generates the non-uniform Weighting Function W(n) for bounding box regression.\n\n    Args:\n        max_num_bins (int): Max number of the discrete bins.\n        up (Tensor): Controls upper bounds of the sequence,\n                     where maximum offset is \u00b1up * H / W.\n        reg_scale (float): Controls the curvature of the Weighting Function.\n                           Larger values result in flatter weights near the central axis W(max_num_bins/2)=0\n                           and steeper weights at both ends.\n    Returns:\n        Tensor: Sequence of Weighting Function.\n    \"\"\"\n    upper_bound1 = abs(up[0]) * abs(reg_scale)\n    upper_bound2 = abs(up[0]) * abs(reg_scale) * 2\n    step = (upper_bound1 + 1) ** (2 / (max_num_bins - 2))\n    left_values = [-((step) ** i) + 1 for i in range(max_num_bins // 2 - 1, 0, -1)]\n    right_values = [(step) ** i - 1 for i in range(1, max_num_bins // 2)]\n    values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n    values = [v if v.dim() > 0 else v.unsqueeze(0) for v in values]\n    values = torch.cat(values, 0)\n    return values"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/loss/loss_for_object_detection.py#_upcast": {
                "sorted_modules": {
                    "_upcast": "\n\n# below: bounding box utilities taken from https://github.com/facebookresearch/detr/blob/master/util/box_ops.py\n\n\ndef _upcast(t: Tensor) -> Tensor:\n    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n    if t.is_floating_point():\n        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n    else:\n        return t if t.dtype in (torch.int32, torch.int64) else t.int()"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/quantizers/base.py#MODULES_TO_PATCH_FOR_QUANTIZATION": {
                "sorted_modules": {
                    "MODULES_TO_PATCH_FOR_QUANTIZATION": "\n\nMODULES_TO_PATCH_FOR_QUANTIZATION = {\n    \"Llama4TextExperts\": {\n        \"module_name\": SequentialLlama4TextExperts,\n        \"quantization_methods\": [\n            QuantizationMethod.COMPRESSED_TENSORS,\n            QuantizationMethod.BITS_AND_BYTES,\n        ],\n    }\n}"
                },
                "component_dependencies": {
                    "MODULES_TO_PATCH_FOR_QUANTIZATION": [
                        "transformers/quantizers/base.py#SequentialLlama4TextExperts",
                        "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                        "transformers/utils/quantization_config.py#QuantizationMethod.COMPRESSED_TENSORS"
                    ]
                },
                "warning": null
            },
            "transformers/quantizers/base.py#_assign_original_dtype": {
                "sorted_modules": {
                    "_assign_original_dtype": "\n\ndef _assign_original_dtype(module, original_dtype):\n    # not very nice in a recursive function but it avoids a circular import\n    from ..modeling_utils import PreTrainedModel\n\n    for child in module.children():\n        if isinstance(child, PreTrainedModel):\n            child.config._pre_quantization_dtype = original_dtype\n        _assign_original_dtype(child, original_dtype)"
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/quantizers/quantizer_torchao.py#_quantization_type": {
                "sorted_modules": {
                    "_quantization_type": "\n\ndef _quantization_type(weight):\n    from torchao.dtypes import AffineQuantizedTensor\n    from torchao.quantization.linear_activation_quantized_tensor import LinearActivationQuantizedTensor\n\n    if isinstance(weight, AffineQuantizedTensor):\n        return f\"{weight.__class__.__name__}({weight._quantization_type()})\"\n\n    if isinstance(weight, LinearActivationQuantizedTensor):\n        return f\"{weight.__class__.__name__}(activation={weight.input_quant_func}, weight={_quantization_type(weight.original_weight_tensor)})\""
                },
                "component_dependencies": {},
                "warning": null
            },
            "transformers/quantizers/base.py#SequentialLlama4TextExperts": {
                "sorted_modules": {
                    "SequentialLlama4TextExperts": "\n\nclass SequentialLlama4TextExperts(ModuleList):\n    \"\"\"\n    A module that implements a compressed version of a list of expert modules.\n    This is specifically designed to work with Llama4TextExperts in MoE layers.\n    \"\"\"\n\n    def __init__(self, config):\n        from transformers.models.llama4.modeling_llama4 import Llama4TextMLP\n\n        super().__init__([Llama4TextMLP(config) for _ in range(config.num_local_experts)])\n        self.num_experts = config.num_local_experts\n\n    def forward(\n        self,\n        hidden_states: \"torch.Tensor\",\n    ) -> \"torch.Tensor\":\n        hidden_states = hidden_states.reshape(self.num_experts, -1, hidden_states.shape[-1])\n        routed_out = torch.zeros_like(hidden_states)\n        for expert_idx in range(self.num_experts):\n            routed_out[expert_idx] = self[expert_idx](hidden_states[expert_idx])\n        return routed_out"
                },
                "component_dependencies": {},
                "warning": null
            }
        },
        "files_to_convert": [
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "load_balancing_loss_func",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#CacheLayerMixin",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "CacheLayerMixin",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "_is_torch_greater_or_equal_than_2_7",
                "Dependencies": [
                    "transformers/utils.py#is_torch_greater_or_equal"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#Cache",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "Cache",
                "Dependencies": [
                    "transformers/cache_utils.py#CacheLayerMixin",
                    "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                "filepath": "transformers/modeling_outputs.py",
                "comp_name": "MoeCausalLMOutputWithPast",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/utils.py#ModelOutput"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "filepath": "transformers/modeling_outputs.py",
                "comp_name": "MoeModelOutputWithPast",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/utils.py#ModelOutput"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#DynamicLayer",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "DynamicLayer",
                "Dependencies": [
                    "transformers/cache_utils.py#CacheLayerMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#DynamicSlidingWindowLayer",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "DynamicSlidingWindowLayer",
                "Dependencies": [
                    "transformers/cache_utils.py#DynamicLayer"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#and_masks",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "and_masks",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#causal_mask_function",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "causal_mask_function",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#or_masks",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "or_masks",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#packed_sequence_mask_function",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "packed_sequence_mask_function",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#RopeParameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "RopeParameters",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#standardize_rope_params",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "standardize_rope_params",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "Qwen3MoeRMSNorm",
                "Dependencies": [
                    "transformers/integrations.py#use_kernel_forward_from_hub"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
                "filepath": "transformers/utils/generic.py",
                "comp_name": "_CAN_RECORD_REGISTRY",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/dynamic_module_utils.py#custom_object_save",
                "filepath": "transformers/dynamic_module_utils.py",
                "comp_name": "custom_object_save",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#accelerate_disk_offload",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "accelerate_disk_offload",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#expand_device_map",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "expand_device_map",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/hub_kernels.py#is_kernel",
                "filepath": "transformers/integrations/hub_kernels.py",
                "comp_name": "is_kernel",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "_get_parameter_tp_plan",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "initialize_tensor_parallelism",
                "Dependencies": [
                    "transformers/utils.py#is_torch_greater_or_equal"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#repack_weights",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "repack_weights",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "shard_and_distribute_module",
                "Dependencies": [
                    "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#verify_tp_plan",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "verify_tp_plan",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#EmbeddingAccessMixin",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "EmbeddingAccessMixin",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#SpecificPreTrainedModelType",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "SpecificPreTrainedModelType",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#VLMS",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "VLMS",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_add_variant",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_add_variant",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_find_mismatched_keys",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_find_mismatched_keys",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_get_tied_weight_keys",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_get_tied_weight_keys",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_init_weights",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_init_weights",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_is_ds_init_called",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_is_ds_init_called",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_is_quantized",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_is_quantized",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_torch_distributed_available",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_torch_distributed_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#is_accelerator_device",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "is_accelerator_device",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#is_local_dist_rank_0",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "is_local_dist_rank_0",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#restore_default_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "restore_default_dtype",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#set_quantized_state",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "set_quantized_state",
                "Dependencies": [
                    "transformers/modeling_utils.py#_is_quantized"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#set_zero3_state",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "set_zero3_state",
                "Dependencies": [
                    "transformers/modeling_utils.py#_is_ds_init_called"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#unwrap_model",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "unwrap_model",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "filepath": "transformers/quantizers/quantizers_utils.py",
                "comp_name": "get_module_from_name",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils.py#DUMMY_INPUTS",
                "filepath": "transformers/utils.py",
                "comp_name": "DUMMY_INPUTS",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                "filepath": "transformers/utils.py",
                "comp_name": "SAFE_WEIGHTS_INDEX_NAME",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils.py#SAFE_WEIGHTS_NAME",
                "filepath": "transformers/utils.py",
                "comp_name": "SAFE_WEIGHTS_NAME",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils.py#WEIGHTS_INDEX_NAME",
                "filepath": "transformers/utils.py",
                "comp_name": "WEIGHTS_INDEX_NAME",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils.py#WEIGHTS_NAME",
                "filepath": "transformers/utils.py",
                "comp_name": "WEIGHTS_NAME",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "ENV_VARS_TRUE_VALUES",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_huggingface_hub_greater_or_equal",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "FlashAttentionKwargs",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/__init__.py#__version__",
                "filepath": "transformers/__init__.py",
                "comp_name": "__version__",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/configuration_utils.py#SpecificPreTrainedConfigType",
                "filepath": "transformers/configuration_utils.py",
                "comp_name": "SpecificPreTrainedConfigType",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils.py#CONFIG_NAME",
                "filepath": "transformers/utils.py",
                "comp_name": "CONFIG_NAME",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/generic.py#is_timm_config_dict",
                "filepath": "transformers/utils/generic.py",
                "comp_name": "is_timm_config_dict",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_torch_greater_or_equal",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_torch_greater_or_equal",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "_is_torch_greater_or_equal_than_2_6",
                "Dependencies": [
                    "transformers/utils/import_utils.py#is_torch_greater_or_equal"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#_is_torch_xpu_available",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "_is_torch_xpu_available",
                "Dependencies": [
                    "transformers/utils.py#is_torch_xpu_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#find_packed_sequence_indices",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "find_packed_sequence_indices",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#sliding_window_overlay",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "sliding_window_overlay",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#sliding_window_causal_mask_function",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "sliding_window_causal_mask_function",
                "Dependencies": [
                    "transformers/masking_utils.py#and_masks",
                    "transformers/masking_utils.py#causal_mask_function",
                    "transformers/masking_utils.py#sliding_window_overlay"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#find_tied_parameters",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "find_tied_parameters",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                "filepath": "transformers/integrations/deepspeed.py",
                "comp_name": "is_deepspeed_zero3_enabled",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#check_and_set_device_map",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "check_and_set_device_map",
                "Dependencies": [
                    "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                    "transformers/utils.py#is_accelerate_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#init_on_device",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "init_on_device",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#init_empty_weights",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "init_empty_weights",
                "Dependencies": [
                    "transformers/integrations/accelerate.py#init_on_device"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/hub.py#DownloadKwargs",
                "filepath": "transformers/utils/hub.py",
                "comp_name": "DownloadKwargs",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/peft.py#maybe_load_adapters",
                "filepath": "transformers/integrations/peft.py",
                "comp_name": "maybe_load_adapters",
                "Dependencies": [
                    "transformers/utils/hub.py#DownloadKwargs"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#_torch_distributed_available",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "_torch_distributed_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#add_tensor_parallel_hooks_to_module",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "add_tensor_parallel_hooks_to_module",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#distribute_model",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "distribute_model",
                "Dependencies": [
                    "transformers/distributed.py#DistributedConfig.from_dict",
                    "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                    "transformers/integrations/tensor_parallel.py#_torch_distributed_available",
                    "transformers/integrations/tensor_parallel.py#add_tensor_parallel_hooks_to_module",
                    "transformers/utils.py#is_torch_greater_or_equal"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#convert_local_tensor_to_dtensor",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "convert_local_tensor_to_dtensor",
                "Dependencies": [
                    "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor",
                "filepath": "transformers/integrations/tensor_parallel.py",
                "comp_name": "replace_state_dict_local_with_dtensor",
                "Dependencies": [
                    "transformers/integrations/tensor_parallel.py#convert_local_tensor_to_dtensor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_flash_fn",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_flash_fn",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_flash_varlen_fn",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_flash_varlen_fn",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_pad_fn",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_pad_fn",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_process_flash_kwargs_fn",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_process_flash_kwargs_fn",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_unpad_fn",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_unpad_fn",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#get_parameter_device",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "get_parameter_device",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_end_ptr",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_end_ptr",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_find_disjoint",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_find_disjoint",
                "Dependencies": [
                    "transformers/modeling_utils.py#_end_ptr"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_find_identical",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_find_identical",
                "Dependencies": [
                    "transformers/modeling_utils.py#_end_ptr"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_find_missing_and_unexpected_keys",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_find_missing_and_unexpected_keys",
                "Dependencies": [
                    "transformers/integrations/accelerate.py#find_tied_parameters",
                    "transformers/quantizers.py#HfQuantizer"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#get_state_dict_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "get_state_dict_dtype",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/safetensors_conversion.py#auto_conversion",
                "filepath": "transformers/safetensors_conversion.py",
                "comp_name": "auto_conversion",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/hub.py#get_checkpoint_shard_files",
                "filepath": "transformers/utils/hub.py",
                "comp_name": "get_checkpoint_shard_files",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_get_resolved_checkpoint_files",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_get_resolved_checkpoint_files",
                "Dependencies": [
                    "transformers/modeling_utils.py#_add_variant",
                    "transformers/safetensors_conversion.py#auto_conversion",
                    "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                    "transformers/utils.py#SAFE_WEIGHTS_NAME",
                    "transformers/utils.py#WEIGHTS_INDEX_NAME",
                    "transformers/utils.py#WEIGHTS_NAME",
                    "transformers/utils.py#cached_file",
                    "transformers/utils.py#download_url",
                    "transformers/utils.py#has_file",
                    "transformers/utils.py#is_offline_mode",
                    "transformers/utils.py#is_remote_url",
                    "transformers/utils/hub.py#DownloadKwargs",
                    "transformers/utils/hub.py#get_checkpoint_shard_files"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_infer_parameter_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_infer_parameter_dtype",
                "Dependencies": [
                    "transformers/quantizers.py#HfQuantizer",
                    "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                    "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                    "transformers/utils/quantization_config.py#QuantizationMethod.MXFP4",
                    "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_is_dtensor_available",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_is_dtensor_available",
                "Dependencies": [
                    "transformers/modeling_utils.py#_torch_distributed_available",
                    "transformers/utils.py#is_torch_greater_or_equal"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_load_parameter_into_model",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_load_parameter_into_model",
                "Dependencies": [
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#XLA_DOWNCAST_BF16",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "XLA_DOWNCAST_BF16",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#XLA_USE_BF16",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "XLA_USE_BF16",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#get_parameter_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "get_parameter_dtype",
                "Dependencies": [
                    "transformers/modeling_utils.py#XLA_DOWNCAST_BF16",
                    "transformers/modeling_utils.py#XLA_USE_BF16",
                    "transformers/utils.py#is_torch_xla_available",
                    "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_load_state_dict_into_meta_model",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_load_state_dict_into_meta_model",
                "Dependencies": [
                    "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                    "transformers/modeling_utils.py#_infer_parameter_dtype",
                    "transformers/modeling_utils.py#_load_parameter_into_model",
                    "transformers/quantizers.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#str_to_torch_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "str_to_torch_dtype",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#load_state_dict",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "load_state_dict",
                "Dependencies": [
                    "transformers/modeling_utils.py#str_to_torch_dtype"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#TORCH_INIT_FUNCTIONS",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "TORCH_INIT_FUNCTIONS",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#no_init_weights",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "no_init_weights",
                "Dependencies": [
                    "transformers/modeling_utils.py#TORCH_INIT_FUNCTIONS",
                    "transformers/modeling_utils.py#_init_weights"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/pytorch_utils.py#_torch_distributed_available",
                "filepath": "transformers/pytorch_utils.py",
                "comp_name": "_torch_distributed_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/pytorch_utils.py#id_tensor_storage",
                "filepath": "transformers/pytorch_utils.py",
                "comp_name": "id_tensor_storage",
                "Dependencies": [
                    "transformers/pytorch_utils.py#_torch_distributed_available",
                    "transformers/utils.py#is_torch_greater_or_equal",
                    "transformers/utils.py#is_torch_xla_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#rotate_half",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "rotate_half",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#apply_rotary_pos_emb",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "apply_rotary_pos_emb",
                "Dependencies": [
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#rotate_half"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#repeat_kv",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "repeat_kv",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#eager_attention_forward",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "eager_attention_forward",
                "Dependencies": [
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#repeat_kv",
                    "transformers/processing_utils.py#Unpack",
                    "transformers/utils.py#TransformersKwargs"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/configuration_utils.py#get_configuration_file",
                "filepath": "transformers/configuration_utils.py",
                "comp_name": "get_configuration_file",
                "Dependencies": [
                    "transformers/__init__.py#__version__",
                    "transformers/utils.py#CONFIG_NAME"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#read_field",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "read_field",
                "Dependencies": [
                    "transformers/integrations.py#_gguf_parse_value"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#compute_module_sizes",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "compute_module_sizes",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#get_balanced_memory",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "get_balanced_memory",
                "Dependencies": [
                    "transformers/integrations/accelerate.py#compute_module_sizes"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#_get_device_map",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "_get_device_map",
                "Dependencies": [
                    "transformers/integrations/accelerate.py#find_tied_parameters",
                    "transformers/integrations/accelerate.py#get_balanced_memory",
                    "transformers/utils.py#is_torch_xpu_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/fsdp.py#is_fsdp_enabled",
                "filepath": "transformers/integrations/fsdp.py",
                "comp_name": "is_fsdp_enabled",
                "Dependencies": [
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils.py#strtobool"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/accelerate.py#accelerate_dispatch",
                "filepath": "transformers/integrations/accelerate.py",
                "comp_name": "accelerate_dispatch",
                "Dependencies": [
                    "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                    "transformers/integrations/fsdp.py#is_fsdp_enabled",
                    "transformers/utils/quantization_config.py#QuantizationMethod.FBGEMM_FP8",
                    "transformers/utils/quantization_config.py#QuantizationMethod.HQQ"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/flash_attention.py#get_target_dtype",
                "filepath": "transformers/integrations/flash_attention.py",
                "comp_name": "get_target_dtype",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#_set_aux_loss",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "_set_aux_loss",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#_set_aux_loss2",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "_set_aux_loss2",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#_set_aux_loss",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "_set_aux_loss",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_rt_detr.py#_set_aux_loss",
                "filepath": "transformers/loss/loss_rt_detr.py",
                "comp_name": "_set_aux_loss",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#fixed_cross_entropy",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "fixed_cross_entropy",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#ForCausalLMLoss",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "ForCausalLMLoss",
                "Dependencies": [
                    "transformers/loss/loss_utils.py#fixed_cross_entropy"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#ForMaskedLMLoss",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "ForMaskedLMLoss",
                "Dependencies": [
                    "transformers/loss/loss_utils.py#fixed_cross_entropy"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#ForQuestionAnsweringLoss",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "ForQuestionAnsweringLoss",
                "Dependencies": [
                    "transformers/loss/loss_utils.py#fixed_cross_entropy"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#ForSequenceClassificationLoss",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "ForSequenceClassificationLoss",
                "Dependencies": [
                    "transformers/loss/loss_utils.py#fixed_cross_entropy"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#ForTokenClassification",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "ForTokenClassification",
                "Dependencies": [
                    "transformers/loss/loss_utils.py#fixed_cross_entropy"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_hf_api_to_flash_mapping",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_hf_api_to_flash_mapping",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_process_flash_attention_kwargs",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_process_flash_attention_kwargs",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_lazy_define_process_function",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_lazy_define_process_function",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#_hf_api_to_flash_mapping",
                    "transformers/modeling_flash_attention_utils.py#_process_flash_attention_kwargs"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_pad_input",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_pad_input",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#ModuleUtilsMixin",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "ModuleUtilsMixin",
                "Dependencies": [
                    "transformers/modeling_utils.py#get_parameter_device",
                    "transformers/modeling_utils.py#get_parameter_dtype"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#load_shard_file",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "load_shard_file",
                "Dependencies": [
                    "transformers/modeling_utils.py#_load_state_dict_into_meta_model",
                    "transformers/modeling_utils.py#load_state_dict"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#load_shard_files_with_threadpool",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "load_shard_files_with_threadpool",
                "Dependencies": [
                    "transformers/modeling_utils.py#load_shard_file"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#ACCELERATE_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "ACCELERATE_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#AV_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "AV_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#BS4_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "BS4_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#CCL_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "CCL_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#CV2_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "CV2_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#CYTHON_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "CYTHON_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#DATASETS_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "DATASETS_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#DECORD_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "DECORD_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#DETECTRON2_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "DETECTRON2_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#ESSENTIA_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "ESSENTIA_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#FAISS_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "FAISS_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#FASTAPI_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "FASTAPI_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#FTFY_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "FTFY_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#G2P_EN_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "G2P_EN_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#JINJA_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "JINJA_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#LEVENSHTEIN_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "LEVENSHTEIN_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#LIBROSA_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "LIBROSA_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#MISTRAL_COMMON_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "MISTRAL_COMMON_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#NATTEN_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "NATTEN_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#NLTK_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "NLTK_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#OPENAI_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "OPENAI_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PANDAS_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PANDAS_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PEFT_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PEFT_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PHONEMIZER_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PHONEMIZER_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PRETTY_MIDI_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PRETTY_MIDI_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PROTOBUF_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PROTOBUF_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PYCTCDECODE_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PYCTCDECODE_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PYDANTIC_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PYDANTIC_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PYTESSERACT_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PYTESSERACT_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PYTORCH_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PYTORCH_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PYTORCH_QUANTIZATION_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PYTORCH_QUANTIZATION_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#RICH_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "RICH_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#RJIEBA_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "RJIEBA_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#SACREMOSES_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "SACREMOSES_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#SCIPY_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "SCIPY_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#SENTENCEPIECE_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "SENTENCEPIECE_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#SKLEARN_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "SKLEARN_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#SPEECH_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "SPEECH_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#TIMM_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "TIMM_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#TOKENIZERS_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "TOKENIZERS_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#TORCHAUDIO_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "TORCHAUDIO_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#TORCHCODEC_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "TORCHCODEC_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#TORCHVISION_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "TORCHVISION_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#UROMAN_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "UROMAN_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#UVICORN_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "UVICORN_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#VISION_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "VISION_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#YT_DLP_IMPORT_ERROR",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "YT_DLP_IMPORT_ERROR",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_datasets_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_datasets_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_detectron2_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_detectron2_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_ftfy_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_ftfy_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_g2p_en_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_g2p_en_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_natten_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_natten_available",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#VersionComparison",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "VersionComparison",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#split_package_version",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "split_package_version",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "GGUF_TO_TRANSFORMERS_MAPPING",
                "Dependencies": [
                    "transformers/integrations.py#GGUF_CONFIG_MAPPING",
                    "transformers/integrations.py#GGUF_TOKENIZER_MAPPING"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "GGUFTensor",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "TensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#GGUF_MIN_VERSION",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "GGUF_MIN_VERSION",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_check_received_keys",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_check_received_keys",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_is_packed_sequence",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_is_packed_sequence",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#fa_peft_integration_check",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "fa_peft_integration_check",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "sigmoid_focal_loss",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#dice_loss",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "dice_loss",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_grounding_dino.py#sigmoid_focal_loss",
                "filepath": "transformers/loss/loss_grounding_dino.py",
                "comp_name": "sigmoid_focal_loss",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_index_first_axis",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_index_first_axis",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_unpad_input",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_unpad_input",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#_index_first_axis"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_lazy_imports",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_lazy_imports",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#_pad_input",
                    "transformers/modeling_flash_attention_utils.py#_unpad_input",
                    "transformers/utils.py#is_flash_attn_2_available",
                    "transformers/utils.py#is_flash_attn_3_available",
                    "transformers/utils.py#is_torch_npu_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "lazy_import_flash_attention",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#_flash_fn",
                    "transformers/modeling_flash_attention_utils.py#_flash_varlen_fn",
                    "transformers/modeling_flash_attention_utils.py#_lazy_define_process_function",
                    "transformers/modeling_flash_attention_utils.py#_lazy_imports",
                    "transformers/modeling_flash_attention_utils.py#_pad_fn",
                    "transformers/modeling_flash_attention_utils.py#_process_flash_kwargs_fn",
                    "transformers/modeling_flash_attention_utils.py#_unpad_fn"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#QuantizationMethod",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "QuantizationMethod",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#AWQLinearVersion",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "AWQLinearVersion",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#AwqBackendPackingMethod",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "AwqBackendPackingMethod",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#ExllamaVersion",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "ExllamaVersion",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "QuantizationConfigMixin",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#ACCELERATE_MIN_VERSION",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "ACCELERATE_MIN_VERSION",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#PACKAGE_DISTRIBUTION_MAPPING",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "PACKAGE_DISTRIBUTION_MAPPING",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#_is_package_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "_is_package_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#PACKAGE_DISTRIBUTION_MAPPING"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#GGUF_SUPPORTED_ARCHITECTURES",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "GGUF_SUPPORTED_ARCHITECTURES",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#BloomTensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "BloomTensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#GPT2TensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "GPT2TensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#Gemma2TensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "Gemma2TensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#Lfm2TensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "Lfm2TensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#LlamaTensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "LlamaTensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#MambaTensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "MambaTensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#NemotronTensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "NemotronTensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#Qwen2MoeTensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "Qwen2MoeTensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#T5TensorProcessor",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "T5TensorProcessor",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#TENSOR_PROCESSORS",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "TENSOR_PROCESSORS",
                "Dependencies": [
                    "transformers/modeling_gguf_pytorch_utils.py#BloomTensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#GPT2TensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#Gemma2TensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#Lfm2TensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#LlamaTensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#MambaTensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#NemotronTensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#Qwen2MoeTensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#T5TensorProcessor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_gguf_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_gguf_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#GGUF_MIN_VERSION",
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#flash_attn_supports_top_left_mask",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "flash_attn_supports_top_left_mask",
                "Dependencies": [
                    "transformers/utils.py#is_flash_attn_2_available",
                    "transformers/utils.py#is_flash_attn_3_available",
                    "transformers/utils.py#is_flash_attn_greater_or_equal_2_10"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/flash_attention.py#_use_top_left_mask",
                "filepath": "transformers/integrations/flash_attention.py",
                "comp_name": "_use_top_left_mask",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#flash_attn_supports_top_left_mask"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#prepare_fa_kwargs_from_position_ids",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "prepare_fa_kwargs_from_position_ids",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_prepare_from_posids",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_prepare_from_posids",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#prepare_fa_kwargs_from_position_ids"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_get_unpad_data",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_get_unpad_data",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_upad_input",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_upad_input",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#_get_unpad_data",
                    "transformers/modeling_flash_attention_utils.py#_index_first_axis"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_flash_attention_utils.py#_flash_attention_forward",
                "filepath": "transformers/modeling_flash_attention_utils.py",
                "comp_name": "_flash_attention_forward",
                "Dependencies": [
                    "transformers/modeling_flash_attention_utils.py#_is_packed_sequence",
                    "transformers/modeling_flash_attention_utils.py#_prepare_from_posids",
                    "transformers/modeling_flash_attention_utils.py#_upad_input",
                    "transformers/modeling_flash_attention_utils.py#fa_peft_integration_check",
                    "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/flash_attention.py#flash_attention_forward",
                "filepath": "transformers/integrations/flash_attention.py",
                "comp_name": "flash_attention_forward",
                "Dependencies": [
                    "transformers/integrations/flash_attention.py#_use_top_left_mask",
                    "transformers/integrations/flash_attention.py#get_target_dtype",
                    "transformers/modeling_flash_attention_utils.py#_flash_attention_forward"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel",
                "filepath": "transformers/integrations/hub_kernels.py",
                "comp_name": "load_and_register_attn_kernel",
                "Dependencies": [
                    "transformers/integrations/flash_attention.py#flash_attention_forward",
                    "transformers/integrations/hub_kernels.py#is_kernel",
                    "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/image_transforms.py#_center_to_corners_format_numpy",
                "filepath": "transformers/image_transforms.py",
                "comp_name": "_center_to_corners_format_numpy",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/image_transforms.py#_center_to_corners_format_torch",
                "filepath": "transformers/image_transforms.py",
                "comp_name": "_center_to_corners_format_torch",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/image_transforms.py#center_to_corners_format",
                "filepath": "transformers/image_transforms.py",
                "comp_name": "center_to_corners_format",
                "Dependencies": [
                    "transformers/image_transforms.py#_center_to_corners_format_numpy",
                    "transformers/image_transforms.py#_center_to_corners_format_torch",
                    "transformers/utils.py#TensorType",
                    "transformers/utils.py#is_torch_tensor"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#NestedTensor",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "NestedTensor",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#_max_by_axis",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "_max_by_axis",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "nested_tensor_from_tensor_list",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#NestedTensor",
                    "transformers/loss/loss_for_object_detection.py#_max_by_axis"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#AqlmConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "AqlmConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#BitNetQuantConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "BitNetQuantConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#BitsAndBytesConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "BitsAndBytesConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#EetqConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "EetqConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#FPQuantConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "FPQuantConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#FineGrainedFP8Config",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "FineGrainedFP8Config",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#HiggsConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "HiggsConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#HqqConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "HqqConfig",
                "Dependencies": [
                    "transformers/utils.py#is_hqq_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#QuantoConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "QuantoConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#QuarkConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "QuarkConfig",
                "Dependencies": [
                    "transformers/utils.py#is_quark_available",
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#SpQRConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "SpQRConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#TorchAoConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "TorchAoConfig",
                "Dependencies": [
                    "transformers/utils.py#is_torchao_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#VptqLayerConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "VptqLayerConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#VptqConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "VptqConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod",
                    "transformers/utils/quantization_config.py#VptqLayerConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_quark.py#CHECKPOINT_KEYS",
                "filepath": "transformers/quantizers/quantizer_quark.py",
                "comp_name": "CHECKPOINT_KEYS",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_torchao.py#fuzzy_match_size",
                "filepath": "transformers/quantizers/quantizer_torchao.py",
                "comp_name": "fuzzy_match_size",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#AutoRoundConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "AutoRoundConfig",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#AwqConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "AwqConfig",
                "Dependencies": [
                    "transformers/utils.py#is_auto_awq_available",
                    "transformers/utils/quantization_config.py#AWQLinearVersion",
                    "transformers/utils/quantization_config.py#AwqBackendPackingMethod",
                    "transformers/utils/quantization_config.py#ExllamaVersion",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "CompressedTensorsConfig",
                "Dependencies": [
                    "transformers/utils.py#is_compressed_tensors_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#FbgemmFp8Config",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "FbgemmFp8Config",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_auto_gptq_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_auto_gptq_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#GPTQConfig",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "GPTQConfig",
                "Dependencies": [
                    "transformers/utils.py#is_gptqmodel_available",
                    "transformers/utils/import_utils.py#is_auto_gptq_available",
                    "transformers/utils/quantization_config.py#ExllamaVersion",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/quantization_config.py#Mxfp4Config",
                "filepath": "transformers/utils/quantization_config.py",
                "comp_name": "Mxfp4Config",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_accelerate_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_accelerate_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#ACCELERATE_MIN_VERSION",
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_av_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_av_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_bs4_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_bs4_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_ccl_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_ccl_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_cv2_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_cv2_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_cython_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_cython_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_decord_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_decord_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_essentia_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_essentia_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_faiss_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_faiss_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_fastapi_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_fastapi_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_jinja_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_jinja_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_levenshtein_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_levenshtein_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_librosa_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_librosa_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_mistral_common_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_mistral_common_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_nltk_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_nltk_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_openai_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_openai_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_pandas_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_pandas_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_peft_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_peft_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_phonemizer_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_phonemizer_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_pretty_midi_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_pretty_midi_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_protobuf_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_protobuf_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_pyctcdecode_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_pyctcdecode_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_pydantic_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_pydantic_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_pytesseract_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_pytesseract_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_pytorch_quantization_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_pytorch_quantization_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_rich_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_rich_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_rjieba_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_rjieba_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_sacremoses_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_sacremoses_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_scipy_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_scipy_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_sentencepiece_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_sentencepiece_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_sklearn_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_sklearn_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_timm_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_timm_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_tokenizers_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_tokenizers_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_torch_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_torch_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_torchaudio_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_torchaudio_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_torchcodec_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_torchcodec_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_torchvision_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_torchvision_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_uroman_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_uroman_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_uvicorn_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_uvicorn_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_vision_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_vision_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_yt_dlp_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_yt_dlp_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#_is_package_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#get_gguf_hf_weights_map",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "get_gguf_hf_weights_map",
                "Dependencies": [
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils/import_utils.py#is_gguf_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint",
                "filepath": "transformers/modeling_gguf_pytorch_utils.py",
                "comp_name": "load_gguf_checkpoint",
                "Dependencies": [
                    "transformers/integrations.py#_gguf_parse_value",
                    "transformers/modeling_gguf_pytorch_utils.py#GGUF_SUPPORTED_ARCHITECTURES",
                    "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING",
                    "transformers/modeling_gguf_pytorch_utils.py#TENSOR_PROCESSORS",
                    "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor",
                    "transformers/modeling_gguf_pytorch_utils.py#get_gguf_hf_weights_map",
                    "transformers/modeling_gguf_pytorch_utils.py#read_field",
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils/import_utils.py#is_gguf_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/configuration_utils.py#PreTrainedConfig",
                "filepath": "transformers/configuration_utils.py",
                "comp_name": "PreTrainedConfig",
                "Dependencies": [
                    "transformers/__init__.py#__version__",
                    "transformers/configuration_utils.py#SpecificPreTrainedConfigType",
                    "transformers/configuration_utils.py#get_configuration_file",
                    "transformers/dynamic_module_utils.py#custom_object_save",
                    "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint",
                    "transformers/utils.py#CONFIG_NAME",
                    "transformers/utils.py#PushToHubMixin",
                    "transformers/utils.py#cached_file",
                    "transformers/utils.py#download_url",
                    "transformers/utils.py#extract_commit_hash",
                    "transformers/utils.py#is_remote_url",
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils/generic.py#is_timm_config_dict"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/cache_utils.py#DynamicCache",
                "filepath": "transformers/cache_utils.py",
                "comp_name": "DynamicCache",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/cache_utils.py#DynamicLayer",
                    "transformers/cache_utils.py#DynamicSlidingWindowLayer",
                    "transformers/configuration_utils.py#PreTrainedConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#_preprocess_mask_arguments",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "_preprocess_mask_arguments",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/masking_utils.py#find_packed_sequence_indices"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#create_causal_mask",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "create_causal_mask",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                    "transformers/masking_utils.py#_is_torch_xpu_available",
                    "transformers/masking_utils.py#_preprocess_mask_arguments",
                    "transformers/masking_utils.py#and_masks",
                    "transformers/masking_utils.py#causal_mask_function",
                    "transformers/masking_utils.py#or_masks",
                    "transformers/masking_utils.py#packed_sequence_mask_function"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/masking_utils.py#create_sliding_window_causal_mask",
                "filepath": "transformers/masking_utils.py",
                "comp_name": "create_sliding_window_causal_mask",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                    "transformers/masking_utils.py#_preprocess_mask_arguments",
                    "transformers/masking_utils.py#and_masks",
                    "transformers/masking_utils.py#or_masks",
                    "transformers/masking_utils.py#packed_sequence_mask_function",
                    "transformers/masking_utils.py#sliding_window_causal_mask_function"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_validate_default_rope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_validate_default_rope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#_check_received_keys"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_validate_dynamic_scaling_rope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#_check_received_keys"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_validate_linear_scaling_rope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#_check_received_keys"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_validate_llama3_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_validate_llama3_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#_check_received_keys"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_validate_longrope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_validate_longrope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#_check_received_keys"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_validate_yarn_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_validate_yarn_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#_check_received_keys"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "ROPE_VALIDATION_FUNCTIONS",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#_validate_default_rope_parameters",
                    "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters",
                    "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters",
                    "transformers/modeling_rope_utils.py#_validate_llama3_parameters",
                    "transformers/modeling_rope_utils.py#_validate_longrope_parameters",
                    "transformers/modeling_rope_utils.py#_validate_yarn_parameters"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#rope_config_validation",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "rope_config_validation",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                "filepath": "transformers/models/qwen3_moe/configuration_qwen3_moe.py",
                "comp_name": "Qwen3MoeConfig",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#RopeParameters",
                    "transformers/modeling_rope_utils.py#rope_config_validation",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_dynamic_ntk_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_linear_scaling_rope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_llama3_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_longrope_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#_compute_yarn_parameters",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "_compute_yarn_parameters",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_rope_utils.py#standardize_rope_params"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "ROPE_INIT_FUNCTIONS",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                    "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                    "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                    "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                    "transformers/modeling_rope_utils.py#_compute_yarn_parameters"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_rope_utils.py#dynamic_rope_update",
                "filepath": "transformers/modeling_rope_utils.py",
                "comp_name": "dynamic_rope_update",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRotaryEmbedding",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "Qwen3MoeRotaryEmbedding",
                "Dependencies": [
                    "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                    "transformers/modeling_rope_utils.py#dynamic_rope_update",
                    "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#weighting_function",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "weighting_function",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#translate_gt",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "translate_gt",
                "Dependencies": [
                    "transformers/loss/loss_d_fine.py#weighting_function"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#bbox2distance",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "bbox2distance",
                "Dependencies": [
                    "transformers/loss/loss_d_fine.py#translate_gt"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#_upcast",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "_upcast",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#box_area",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "box_area",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#_upcast"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#box_iou",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "box_iou",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#box_area"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "generalized_box_iou",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#box_iou"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "HungarianMatcher",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#ImageLoss",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "ImageLoss",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_for_object_detection.py#dice_loss",
                    "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                    "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                    "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#ForObjectDetectionLoss",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "ForObjectDetectionLoss",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                    "transformers/loss/loss_for_object_detection.py#ImageLoss",
                    "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_for_object_detection.py#ForSegmentationLoss",
                "filepath": "transformers/loss/loss_for_object_detection.py",
                "comp_name": "ForSegmentationLoss",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                    "transformers/loss/loss_for_object_detection.py#ImageLoss",
                    "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_grounding_dino.py#GroundingDinoHungarianMatcher",
                "filepath": "transformers/loss/loss_grounding_dino.py",
                "comp_name": "GroundingDinoHungarianMatcher",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                    "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_grounding_dino.py#GroundingDinoImageLoss",
                "filepath": "transformers/loss/loss_grounding_dino.py",
                "comp_name": "GroundingDinoImageLoss",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#ImageLoss",
                    "transformers/loss/loss_grounding_dino.py#sigmoid_focal_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_grounding_dino.py#GroundingDinoForObjectDetectionLoss",
                "filepath": "transformers/loss/loss_grounding_dino.py",
                "comp_name": "GroundingDinoForObjectDetectionLoss",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#_set_aux_loss",
                    "transformers/loss/loss_grounding_dino.py#GroundingDinoHungarianMatcher",
                    "transformers/loss/loss_grounding_dino.py#GroundingDinoImageLoss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#_get_dtype",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "_get_dtype",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/modeling_utils.py#get_state_dict_dtype",
                    "transformers/modeling_utils.py#load_state_dict"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                "filepath": "transformers/quantizers/auto.py",
                "comp_name": "AUTO_QUANTIZATION_CONFIG_MAPPING",
                "Dependencies": [
                    "transformers/utils/quantization_config.py#AqlmConfig",
                    "transformers/utils/quantization_config.py#AutoRoundConfig",
                    "transformers/utils/quantization_config.py#AwqConfig",
                    "transformers/utils/quantization_config.py#BitNetQuantConfig",
                    "transformers/utils/quantization_config.py#BitsAndBytesConfig",
                    "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                    "transformers/utils/quantization_config.py#EetqConfig",
                    "transformers/utils/quantization_config.py#FPQuantConfig",
                    "transformers/utils/quantization_config.py#FbgemmFp8Config",
                    "transformers/utils/quantization_config.py#FineGrainedFP8Config",
                    "transformers/utils/quantization_config.py#GPTQConfig",
                    "transformers/utils/quantization_config.py#HiggsConfig",
                    "transformers/utils/quantization_config.py#HqqConfig",
                    "transformers/utils/quantization_config.py#Mxfp4Config",
                    "transformers/utils/quantization_config.py#QuantoConfig",
                    "transformers/utils/quantization_config.py#QuarkConfig",
                    "transformers/utils/quantization_config.py#SpQRConfig",
                    "transformers/utils/quantization_config.py#TorchAoConfig",
                    "transformers/utils/quantization_config.py#VptqConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/base.py#_assign_original_dtype",
                "filepath": "transformers/quantizers/base.py",
                "comp_name": "_assign_original_dtype",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_torchao.py#_quantization_type",
                "filepath": "transformers/quantizers/quantizer_torchao.py",
                "comp_name": "_quantization_type",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_torchao.py#_linear_extra_repr",
                "filepath": "transformers/quantizers/quantizer_torchao.py",
                "comp_name": "_linear_extra_repr",
                "Dependencies": [
                    "transformers/quantizers/quantizer_torchao.py#_quantization_type"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/auto.py#AutoQuantizationConfig",
                "filepath": "transformers/quantizers/auto.py",
                "comp_name": "AutoQuantizationConfig",
                "Dependencies": [
                    "transformers/models/auto/configuration_auto.py#AutoConfig.from_pretrained",
                    "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                    "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeAttention",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "Qwen3MoeAttention",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs",
                    "transformers/modeling_utils.py#ALL_ATTENTION_FUNCTIONS",
                    "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#apply_rotary_pos_emb",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#eager_attention_forward",
                    "transformers/processing_utils.py#Unpack"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#is_speech_available",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "is_speech_available",
                "Dependencies": [
                    "transformers/utils/import_utils.py#is_torchaudio_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "BACKENDS_MAPPING",
                "Dependencies": [
                    "transformers/utils/import_utils.py#ACCELERATE_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#AV_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#BS4_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#CCL_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#CV2_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#CYTHON_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#DATASETS_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#DECORD_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#DETECTRON2_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#ESSENTIA_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#FAISS_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#FASTAPI_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#FTFY_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#G2P_EN_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#JINJA_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#LEVENSHTEIN_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#LIBROSA_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#MISTRAL_COMMON_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#NATTEN_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#NLTK_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#OPENAI_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PANDAS_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PEFT_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PHONEMIZER_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PRETTY_MIDI_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PROTOBUF_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PYCTCDECODE_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PYDANTIC_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PYTESSERACT_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PYTORCH_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#PYTORCH_QUANTIZATION_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#RICH_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#RJIEBA_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#SACREMOSES_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#SCIPY_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#SENTENCEPIECE_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#SKLEARN_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#SPEECH_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#TIMM_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#TOKENIZERS_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#TORCHAUDIO_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#TORCHCODEC_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#TORCHVISION_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#UROMAN_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#UVICORN_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#VISION_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#YT_DLP_IMPORT_ERROR",
                    "transformers/utils/import_utils.py#is_accelerate_available",
                    "transformers/utils/import_utils.py#is_av_available",
                    "transformers/utils/import_utils.py#is_bs4_available",
                    "transformers/utils/import_utils.py#is_ccl_available",
                    "transformers/utils/import_utils.py#is_cv2_available",
                    "transformers/utils/import_utils.py#is_cython_available",
                    "transformers/utils/import_utils.py#is_datasets_available",
                    "transformers/utils/import_utils.py#is_decord_available",
                    "transformers/utils/import_utils.py#is_detectron2_available",
                    "transformers/utils/import_utils.py#is_essentia_available",
                    "transformers/utils/import_utils.py#is_faiss_available",
                    "transformers/utils/import_utils.py#is_fastapi_available",
                    "transformers/utils/import_utils.py#is_ftfy_available",
                    "transformers/utils/import_utils.py#is_g2p_en_available",
                    "transformers/utils/import_utils.py#is_jinja_available",
                    "transformers/utils/import_utils.py#is_levenshtein_available",
                    "transformers/utils/import_utils.py#is_librosa_available",
                    "transformers/utils/import_utils.py#is_mistral_common_available",
                    "transformers/utils/import_utils.py#is_natten_available",
                    "transformers/utils/import_utils.py#is_nltk_available",
                    "transformers/utils/import_utils.py#is_openai_available",
                    "transformers/utils/import_utils.py#is_pandas_available",
                    "transformers/utils/import_utils.py#is_peft_available",
                    "transformers/utils/import_utils.py#is_phonemizer_available",
                    "transformers/utils/import_utils.py#is_pretty_midi_available",
                    "transformers/utils/import_utils.py#is_protobuf_available",
                    "transformers/utils/import_utils.py#is_pyctcdecode_available",
                    "transformers/utils/import_utils.py#is_pydantic_available",
                    "transformers/utils/import_utils.py#is_pytesseract_available",
                    "transformers/utils/import_utils.py#is_pytorch_quantization_available",
                    "transformers/utils/import_utils.py#is_rich_available",
                    "transformers/utils/import_utils.py#is_rjieba_available",
                    "transformers/utils/import_utils.py#is_sacremoses_available",
                    "transformers/utils/import_utils.py#is_scipy_available",
                    "transformers/utils/import_utils.py#is_sentencepiece_available",
                    "transformers/utils/import_utils.py#is_sklearn_available",
                    "transformers/utils/import_utils.py#is_speech_available",
                    "transformers/utils/import_utils.py#is_timm_available",
                    "transformers/utils/import_utils.py#is_tokenizers_available",
                    "transformers/utils/import_utils.py#is_torch_available",
                    "transformers/utils/import_utils.py#is_torchaudio_available",
                    "transformers/utils/import_utils.py#is_torchcodec_available",
                    "transformers/utils/import_utils.py#is_torchvision_available",
                    "transformers/utils/import_utils.py#is_uroman_available",
                    "transformers/utils/import_utils.py#is_uvicorn_available",
                    "transformers/utils/import_utils.py#is_vision_available",
                    "transformers/utils/import_utils.py#is_yt_dlp_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#Backend",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "Backend",
                "Dependencies": [
                    "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                    "transformers/utils/import_utils.py#VersionComparison",
                    "transformers/utils/import_utils.py#_is_package_available",
                    "transformers/utils/import_utils.py#split_package_version"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/import_utils.py#requires",
                "filepath": "transformers/utils/import_utils.py",
                "comp_name": "requires",
                "Dependencies": [
                    "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                    "transformers/utils/import_utils.py#Backend"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/generic.py#OutputRecorder",
                "filepath": "transformers/utils/generic.py",
                "comp_name": "OutputRecorder",
                "Dependencies": [
                    "transformers/utils/import_utils.py#requires"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/utils/generic.py#check_model_inputs",
                "filepath": "transformers/utils/generic.py",
                "comp_name": "check_model_inputs",
                "Dependencies": [
                    "transformers/utils/generic.py#OutputRecorder",
                    "transformers/utils/generic.py#_CAN_RECORD_REGISTRY"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher",
                "filepath": "transformers/loss/loss_rt_detr.py",
                "comp_name": "RTDetrHungarianMatcher",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                    "transformers/utils.py#requires_backends"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_deformable_detr.py#DeformableDetrHungarianMatcher",
                "filepath": "transformers/loss/loss_deformable_detr.py",
                "comp_name": "DeformableDetrHungarianMatcher",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                    "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                "filepath": "transformers/loss/loss_deformable_detr.py",
                "comp_name": "DeformableDetrImageLoss",
                "Dependencies": [
                    "transformers/loss/loss_for_object_detection.py#ImageLoss",
                    "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_deformable_detr.py#DeformableDetrForObjectDetectionLoss",
                "filepath": "transformers/loss/loss_deformable_detr.py",
                "comp_name": "DeformableDetrForObjectDetectionLoss",
                "Dependencies": [
                    "transformers/loss/loss_deformable_detr.py#DeformableDetrHungarianMatcher",
                    "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                    "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_deformable_detr.py#DeformableDetrForSegmentationLoss",
                "filepath": "transformers/loss/loss_deformable_detr.py",
                "comp_name": "DeformableDetrForSegmentationLoss",
                "Dependencies": [
                    "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                    "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                    "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_rt_detr.py#RTDetrLoss",
                "filepath": "transformers/loss/loss_rt_detr.py",
                "comp_name": "RTDetrLoss",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_for_object_detection.py#box_iou",
                    "transformers/loss/loss_for_object_detection.py#dice_loss",
                    "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                    "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                    "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss",
                    "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_rt_detr.py#RTDetrForObjectDetectionLoss",
                "filepath": "transformers/loss/loss_rt_detr.py",
                "comp_name": "RTDetrForObjectDetectionLoss",
                "Dependencies": [
                    "transformers/loss/loss_rt_detr.py#RTDetrLoss",
                    "transformers/loss/loss_rt_detr.py#_set_aux_loss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/base.py#SequentialLlama4TextExperts",
                "filepath": "transformers/quantizers/base.py",
                "comp_name": "SequentialLlama4TextExperts",
                "Dependencies": [],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/base.py#MODULES_TO_PATCH_FOR_QUANTIZATION",
                "filepath": "transformers/quantizers/base.py",
                "comp_name": "MODULES_TO_PATCH_FOR_QUANTIZATION",
                "Dependencies": [
                    "transformers/quantizers/base.py#SequentialLlama4TextExperts",
                    "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                    "transformers/utils/quantization_config.py#QuantizationMethod.COMPRESSED_TENSORS"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/base.py#HfQuantizer",
                "filepath": "transformers/quantizers/base.py",
                "comp_name": "HfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#MODULES_TO_PATCH_FOR_QUANTIZATION",
                    "transformers/quantizers/base.py#_assign_original_dtype",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_aqlm.py#AqlmHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_aqlm.py",
                "comp_name": "AqlmHfQuantizer",
                "Dependencies": [
                    "transformers/integrations.py#replace_with_aqlm_linear",
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_aqlm_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_auto_round.py#AutoRoundQuantizer",
                "filepath": "transformers/quantizers/quantizer_auto_round.py",
                "comp_name": "AutoRoundQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_auto_round_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_awq.py#AwqQuantizer",
                "filepath": "transformers/quantizers/quantizer_awq.py",
                "comp_name": "AwqQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_auto_awq_available",
                    "transformers/utils/quantization_config.py#AWQLinearVersion.EXLLAMA",
                    "transformers/utils/quantization_config.py#AWQLinearVersion.GEMM",
                    "transformers/utils/quantization_config.py#AWQLinearVersion.IPEX"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_bitnet.py#BitNetHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_bitnet.py",
                "comp_name": "BitNetHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_bnb_4bit.py#Bnb4BitHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_bnb_4bit.py",
                "comp_name": "Bnb4BitHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_bitsandbytes_available",
                    "transformers/utils.py#is_torch_hpu_available",
                    "transformers/utils.py#is_torch_npu_available",
                    "transformers/utils.py#is_torch_xpu_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_bnb_8bit.py#Bnb8BitHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_bnb_8bit.py",
                "comp_name": "Bnb8BitHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_torch_xpu_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_compressed_tensors.py#CompressedTensorsHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_compressed_tensors.py",
                "comp_name": "CompressedTensorsHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_compressed_tensors_available",
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils/quantization_config.py#CompressedTensorsConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_eetq.py#EetqHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_eetq.py",
                "comp_name": "EetqHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_eetq_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_fbgemm_fp8.py#FbgemmFp8HfQuantizer",
                "filepath": "transformers/quantizers/quantizer_fbgemm_fp8.py",
                "comp_name": "FbgemmFp8HfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_fbgemm_gpu_available",
                    "transformers/utils.py#is_torch_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_finegrained_fp8.py#FineGrainedFP8HfQuantizer",
                "filepath": "transformers/quantizers/quantizer_finegrained_fp8.py",
                "comp_name": "FineGrainedFP8HfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils.py#is_torch_xpu_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_fp_quant.py#FPQuantHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_fp_quant.py",
                "comp_name": "FPQuantHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_fp_quant_available",
                    "transformers/utils.py#is_qutlass_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_gptq.py#GptqHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_gptq.py",
                "comp_name": "GptqHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_auto_gptq_available",
                    "transformers/utils.py#is_gptqmodel_available",
                    "transformers/utils.py#is_optimum_available",
                    "transformers/utils/quantization_config.py#GPTQConfig.from_dict",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_higgs.py#HiggsHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_higgs.py",
                "comp_name": "HiggsHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_flute_available",
                    "transformers/utils.py#is_hadamard_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_hqq.py#HqqHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_hqq.py",
                "comp_name": "HqqHfQuantizer",
                "Dependencies": [
                    "transformers/integrations.py#prepare_for_hqq_linear",
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_hqq_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_mxfp4.py#Mxfp4HfQuantizer",
                "filepath": "transformers/quantizers/quantizer_mxfp4.py",
                "comp_name": "Mxfp4HfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_kernels_available",
                    "transformers/utils.py#is_torch_available",
                    "transformers/utils.py#is_triton_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_quanto.py#QuantoHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_quanto.py",
                "comp_name": "QuantoHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_optimum_quanto_available",
                    "transformers/utils/quantization_config.py#QuantoConfig"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_quark.py#QuarkHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_quark.py",
                "comp_name": "QuarkHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizer_quark.py#CHECKPOINT_KEYS",
                    "transformers/utils.py#is_quark_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_spqr.py#SpQRHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_spqr.py",
                "comp_name": "SpQRHfQuantizer",
                "Dependencies": [
                    "transformers/integrations.py#replace_with_spqr_linear",
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_spqr_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_torchao.py#TorchAoHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_torchao.py",
                "comp_name": "TorchAoHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/quantizers/quantizer_torchao.py#_linear_extra_repr",
                    "transformers/quantizers/quantizer_torchao.py#fuzzy_match_size",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#is_torchao_available"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/quantizer_vptq.py#VptqHfQuantizer",
                "filepath": "transformers/quantizers/quantizer_vptq.py",
                "comp_name": "VptqHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/base.py#HfQuantizer",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_vptq_available",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING",
                "filepath": "transformers/quantizers/auto.py",
                "comp_name": "AUTO_QUANTIZER_MAPPING",
                "Dependencies": [
                    "transformers/quantizers/quantizer_aqlm.py#AqlmHfQuantizer",
                    "transformers/quantizers/quantizer_auto_round.py#AutoRoundQuantizer",
                    "transformers/quantizers/quantizer_awq.py#AwqQuantizer",
                    "transformers/quantizers/quantizer_bitnet.py#BitNetHfQuantizer",
                    "transformers/quantizers/quantizer_bnb_4bit.py#Bnb4BitHfQuantizer",
                    "transformers/quantizers/quantizer_bnb_8bit.py#Bnb8BitHfQuantizer",
                    "transformers/quantizers/quantizer_compressed_tensors.py#CompressedTensorsHfQuantizer",
                    "transformers/quantizers/quantizer_eetq.py#EetqHfQuantizer",
                    "transformers/quantizers/quantizer_fbgemm_fp8.py#FbgemmFp8HfQuantizer",
                    "transformers/quantizers/quantizer_finegrained_fp8.py#FineGrainedFP8HfQuantizer",
                    "transformers/quantizers/quantizer_fp_quant.py#FPQuantHfQuantizer",
                    "transformers/quantizers/quantizer_gptq.py#GptqHfQuantizer",
                    "transformers/quantizers/quantizer_higgs.py#HiggsHfQuantizer",
                    "transformers/quantizers/quantizer_hqq.py#HqqHfQuantizer",
                    "transformers/quantizers/quantizer_mxfp4.py#Mxfp4HfQuantizer",
                    "transformers/quantizers/quantizer_quanto.py#QuantoHfQuantizer",
                    "transformers/quantizers/quantizer_quark.py#QuarkHfQuantizer",
                    "transformers/quantizers/quantizer_spqr.py#SpQRHfQuantizer",
                    "transformers/quantizers/quantizer_torchao.py#TorchAoHfQuantizer",
                    "transformers/quantizers/quantizer_vptq.py#VptqHfQuantizer"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/auto.py#AutoHfQuantizer",
                "filepath": "transformers/quantizers/auto.py",
                "comp_name": "AutoHfQuantizer",
                "Dependencies": [
                    "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                    "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING",
                    "transformers/quantizers/auto.py#AutoQuantizationConfig",
                    "transformers/utils/quantization_config.py#AutoRoundConfig",
                    "transformers/utils/quantization_config.py#AutoRoundConfig.from_dict",
                    "transformers/utils/quantization_config.py#AwqConfig",
                    "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                    "transformers/utils/quantization_config.py#FbgemmFp8Config",
                    "transformers/utils/quantization_config.py#GPTQConfig",
                    "transformers/utils/quantization_config.py#Mxfp4Config",
                    "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                    "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/quantizers/auto.py#get_hf_quantizer",
                "filepath": "transformers/quantizers/auto.py",
                "comp_name": "get_hf_quantizer",
                "Dependencies": [
                    "transformers/quantizers/auto.py#AutoHfQuantizer"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#DFineLoss",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "DFineLoss",
                "Dependencies": [
                    "transformers/image_transforms.py#center_to_corners_format",
                    "transformers/loss/loss_d_fine.py#bbox2distance",
                    "transformers/loss/loss_for_object_detection.py#box_iou",
                    "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher",
                    "transformers/loss/loss_rt_detr.py#RTDetrLoss"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_d_fine.py#DFineForObjectDetectionLoss",
                "filepath": "transformers/loss/loss_d_fine.py",
                "comp_name": "DFineForObjectDetectionLoss",
                "Dependencies": [
                    "transformers/loss/loss_d_fine.py#DFineLoss",
                    "transformers/loss/loss_d_fine.py#_set_aux_loss",
                    "transformers/loss/loss_d_fine.py#_set_aux_loss2"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/loss/loss_utils.py#LOSS_MAPPING",
                "filepath": "transformers/loss/loss_utils.py",
                "comp_name": "LOSS_MAPPING",
                "Dependencies": [
                    "transformers/loss/loss_d_fine.py#DFineForObjectDetectionLoss",
                    "transformers/loss/loss_deformable_detr.py#DeformableDetrForObjectDetectionLoss",
                    "transformers/loss/loss_deformable_detr.py#DeformableDetrForSegmentationLoss",
                    "transformers/loss/loss_for_object_detection.py#ForObjectDetectionLoss",
                    "transformers/loss/loss_for_object_detection.py#ForSegmentationLoss",
                    "transformers/loss/loss_grounding_dino.py#GroundingDinoForObjectDetectionLoss",
                    "transformers/loss/loss_rt_detr.py#RTDetrForObjectDetectionLoss",
                    "transformers/loss/loss_utils.py#ForCausalLMLoss",
                    "transformers/loss/loss_utils.py#ForMaskedLMLoss",
                    "transformers/loss/loss_utils.py#ForQuestionAnsweringLoss",
                    "transformers/loss/loss_utils.py#ForSequenceClassificationLoss",
                    "transformers/loss/loss_utils.py#ForTokenClassification"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/modeling_utils.py#PreTrainedModel",
                "filepath": "transformers/modeling_utils.py",
                "comp_name": "PreTrainedModel",
                "Dependencies": [
                    "transformers/configuration_utils.py#PreTrainedConfig",
                    "transformers/distributed.py#DistributedConfig",
                    "transformers/dynamic_module_utils.py#custom_object_save",
                    "transformers/generation.py#CompileConfig",
                    "transformers/generation.py#GenerationConfig.from_model_config",
                    "transformers/integrations.py#PeftAdapterMixin",
                    "transformers/integrations.py#deepspeed_config",
                    "transformers/integrations.py#is_deepspeed_zero3_enabled",
                    "transformers/integrations.py#is_fsdp_enabled",
                    "transformers/integrations/accelerate.py#_get_device_map",
                    "transformers/integrations/accelerate.py#accelerate_disk_offload",
                    "transformers/integrations/accelerate.py#accelerate_dispatch",
                    "transformers/integrations/accelerate.py#check_and_set_device_map",
                    "transformers/integrations/accelerate.py#expand_device_map",
                    "transformers/integrations/accelerate.py#init_empty_weights",
                    "transformers/integrations/hub_kernels.py#is_kernel",
                    "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel",
                    "transformers/integrations/peft.py#maybe_load_adapters",
                    "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES",
                    "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES.keys",
                    "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                    "transformers/integrations/tensor_parallel.py#distribute_model",
                    "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism",
                    "transformers/integrations/tensor_parallel.py#repack_weights",
                    "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor",
                    "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                    "transformers/integrations/tensor_parallel.py#verify_tp_plan",
                    "transformers/loss/loss_utils.py#LOSS_MAPPING",
                    "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention",
                    "transformers/modeling_utils.py#EmbeddingAccessMixin",
                    "transformers/modeling_utils.py#ModuleUtilsMixin",
                    "transformers/modeling_utils.py#SpecificPreTrainedModelType",
                    "transformers/modeling_utils.py#VLMS",
                    "transformers/modeling_utils.py#_add_variant",
                    "transformers/modeling_utils.py#_find_disjoint",
                    "transformers/modeling_utils.py#_find_identical",
                    "transformers/modeling_utils.py#_find_mismatched_keys",
                    "transformers/modeling_utils.py#_find_missing_and_unexpected_keys",
                    "transformers/modeling_utils.py#_get_dtype",
                    "transformers/modeling_utils.py#_get_resolved_checkpoint_files",
                    "transformers/modeling_utils.py#_get_tied_weight_keys",
                    "transformers/modeling_utils.py#_infer_parameter_dtype",
                    "transformers/modeling_utils.py#_init_weights",
                    "transformers/modeling_utils.py#_is_ds_init_called",
                    "transformers/modeling_utils.py#_is_dtensor_available",
                    "transformers/modeling_utils.py#_is_quantized",
                    "transformers/modeling_utils.py#_load_parameter_into_model",
                    "transformers/modeling_utils.py#_torch_distributed_available",
                    "transformers/modeling_utils.py#get_parameter_dtype",
                    "transformers/modeling_utils.py#is_accelerator_device",
                    "transformers/modeling_utils.py#is_local_dist_rank_0",
                    "transformers/modeling_utils.py#load_shard_file",
                    "transformers/modeling_utils.py#load_shard_files_with_threadpool",
                    "transformers/modeling_utils.py#load_state_dict",
                    "transformers/modeling_utils.py#no_init_weights",
                    "transformers/modeling_utils.py#restore_default_dtype",
                    "transformers/modeling_utils.py#set_quantized_state",
                    "transformers/modeling_utils.py#set_zero3_state",
                    "transformers/modeling_utils.py#unwrap_model",
                    "transformers/pytorch_utils.py#id_tensor_storage",
                    "transformers/quantizers.py#HfQuantizer",
                    "transformers/quantizers/auto.py#get_hf_quantizer",
                    "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                    "transformers/utils.py#ADAPTER_SAFE_WEIGHTS_NAME",
                    "transformers/utils.py#ADAPTER_WEIGHTS_NAME",
                    "transformers/utils.py#ContextManagers",
                    "transformers/utils.py#DUMMY_INPUTS",
                    "transformers/utils.py#KernelConfig",
                    "transformers/utils.py#PushToHubMixin",
                    "transformers/utils.py#PushToHubMixin.push_to_hub",
                    "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                    "transformers/utils.py#SAFE_WEIGHTS_NAME",
                    "transformers/utils.py#WEIGHTS_INDEX_NAME",
                    "transformers/utils.py#WEIGHTS_NAME",
                    "transformers/utils.py#is_accelerate_available",
                    "transformers/utils.py#is_flash_attn_2_available",
                    "transformers/utils.py#is_flash_attn_3_available",
                    "transformers/utils.py#is_kernels_available",
                    "transformers/utils.py#is_offline_mode",
                    "transformers/utils.py#is_torch_flex_attn_available",
                    "transformers/utils.py#is_torch_mlu_available",
                    "transformers/utils.py#is_torch_npu_available",
                    "transformers/utils/generic.py#OutputRecorder",
                    "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
                    "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES",
                    "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal",
                    "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                    "transformers/utils/quantization_config.py#QuantizationMethod.GPTQ",
                    "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                    "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "Qwen3MoePreTrainedModel",
                "Dependencies": [
                    "transformers/modeling_utils.py#PreTrainedModel",
                    "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeAttention",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer",
                    "transformers/utils.py#auto_docstring",
                    "transformers/utils/generic.py#OutputRecorder"
                ],
                "JaxDependencies": {}
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "Qwen3MoeModel",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/cache_utils.py#DynamicCache",
                    "transformers/masking_utils.py#create_causal_mask",
                    "transformers/masking_utils.py#create_sliding_window_causal_mask",
                    "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                    "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRotaryEmbedding",
                    "transformers/processing_utils.py#Unpack",
                    "transformers/utils.py#TransformersKwargs",
                    "transformers/utils.py#auto_docstring",
                    "transformers/utils/generic.py#check_model_inputs"
                ],
                "JaxDependencies": {
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer": [
                        [
                            0.18296018242835999,
                            "src/MaxText/layers/qwen3.py#Qwen3MoeDecoderLayer",
                            "src/MaxText/layers/qwen3.py",
                            "class Qwen3MoeDecoderLayer(AttentionWithNorm):\n  \"\"\"Qwen3 Transformer decoder layer (MoE).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    super().__init__(config, mesh, model_mode, quant, rngs)\n    self.moe_block = RoutedMoE(\n        config=config,\n        num_experts=config.num_experts,\n        num_experts_per_tok=config.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=config.moe_mlp_dim,  # same as config.mlp_dim\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=quant,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    hidden_states, intermediate_inputs = self.apply_attention_with_norm(\n        inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode\n    )\n\n    mlp_lnx, load_balance_loss = self.moe_block(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    layer_output = intermediate_inputs + mlp_lnx\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output"
                        ]
                    ]
                }
            },
            {
                "comp_id": "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeForCausalLM",
                "filepath": "transformers/models/qwen3_moe/modeling_qwen3_moe.py",
                "comp_name": "Qwen3MoeForCausalLM",
                "Dependencies": [
                    "transformers/cache_utils.py#Cache",
                    "transformers/generation.py#GenerationMixin",
                    "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                    "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                    "transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func",
                    "transformers/processing_utils.py#Unpack",
                    "transformers/utils.py#TransformersKwargs",
                    "transformers/utils.py#auto_docstring",
                    "transformers/utils.py#can_return_tuple"
                ],
                "JaxDependencies": {}
            }
        ],
        "original_dependencies": {
            "Qwen3MoeForCausalLM": [
                "transformers/cache_utils.py#Cache",
                "transformers/generation.py#GenerationMixin",
                "transformers/modeling_outputs.py#MoeCausalLMOutputWithPast",
                "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeModel",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#load_balancing_loss_func",
                "transformers/processing_utils.py#Unpack",
                "transformers/utils.py#TransformersKwargs",
                "transformers/utils.py#auto_docstring",
                "transformers/utils.py#can_return_tuple"
            ],
            "Cache": [
                "transformers/cache_utils.py#CacheLayerMixin",
                "transformers/cache_utils.py#_is_torch_greater_or_equal_than_2_7"
            ],
            "MoeCausalLMOutputWithPast": [
                "transformers/cache_utils.py#Cache",
                "transformers/utils.py#ModelOutput"
            ],
            "MoeModelOutputWithPast": [
                "transformers/cache_utils.py#Cache",
                "transformers/utils.py#ModelOutput"
            ],
            "Qwen3MoeModel": [
                "transformers/cache_utils.py#Cache",
                "transformers/cache_utils.py#DynamicCache",
                "transformers/masking_utils.py#create_causal_mask",
                "transformers/masking_utils.py#create_sliding_window_causal_mask",
                "transformers/modeling_outputs.py#MoeModelOutputWithPast",
                "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoePreTrainedModel",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRotaryEmbedding",
                "transformers/processing_utils.py#Unpack",
                "transformers/utils.py#TransformersKwargs",
                "transformers/utils.py#auto_docstring",
                "transformers/utils/generic.py#check_model_inputs"
            ],
            "Qwen3MoePreTrainedModel": [
                "transformers/modeling_utils.py#PreTrainedModel",
                "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeAttention",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer",
                "transformers/utils.py#auto_docstring",
                "transformers/utils/generic.py#OutputRecorder"
            ],
            "_is_torch_greater_or_equal_than_2_7": [
                "transformers/utils.py#is_torch_greater_or_equal"
            ],
            "DynamicCache": [
                "transformers/cache_utils.py#Cache",
                "transformers/cache_utils.py#DynamicLayer",
                "transformers/cache_utils.py#DynamicSlidingWindowLayer",
                "transformers/configuration_utils.py#PreTrainedConfig"
            ],
            "create_causal_mask": [
                "transformers/cache_utils.py#Cache",
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                "transformers/masking_utils.py#_is_torch_xpu_available",
                "transformers/masking_utils.py#_preprocess_mask_arguments",
                "transformers/masking_utils.py#and_masks",
                "transformers/masking_utils.py#causal_mask_function",
                "transformers/masking_utils.py#or_masks",
                "transformers/masking_utils.py#packed_sequence_mask_function"
            ],
            "create_sliding_window_causal_mask": [
                "transformers/cache_utils.py#Cache",
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/masking_utils.py#_is_torch_greater_or_equal_than_2_6",
                "transformers/masking_utils.py#_preprocess_mask_arguments",
                "transformers/masking_utils.py#and_masks",
                "transformers/masking_utils.py#or_masks",
                "transformers/masking_utils.py#packed_sequence_mask_function",
                "transformers/masking_utils.py#sliding_window_causal_mask_function"
            ],
            "Qwen3MoeConfig": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#RopeParameters",
                "transformers/modeling_rope_utils.py#rope_config_validation",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "Qwen3MoeRMSNorm": [
                "transformers/integrations.py#use_kernel_forward_from_hub"
            ],
            "Qwen3MoeRotaryEmbedding": [
                "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS",
                "transformers/modeling_rope_utils.py#dynamic_rope_update",
                "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig"
            ],
            "check_model_inputs": [
                "transformers/utils/generic.py#OutputRecorder",
                "transformers/utils/generic.py#_CAN_RECORD_REGISTRY"
            ],
            "PreTrainedModel": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/distributed.py#DistributedConfig",
                "transformers/dynamic_module_utils.py#custom_object_save",
                "transformers/generation.py#CompileConfig",
                "transformers/generation.py#GenerationConfig.from_model_config",
                "transformers/integrations.py#PeftAdapterMixin",
                "transformers/integrations.py#deepspeed_config",
                "transformers/integrations.py#is_deepspeed_zero3_enabled",
                "transformers/integrations.py#is_fsdp_enabled",
                "transformers/integrations/accelerate.py#_get_device_map",
                "transformers/integrations/accelerate.py#accelerate_disk_offload",
                "transformers/integrations/accelerate.py#accelerate_dispatch",
                "transformers/integrations/accelerate.py#check_and_set_device_map",
                "transformers/integrations/accelerate.py#expand_device_map",
                "transformers/integrations/accelerate.py#init_empty_weights",
                "transformers/integrations/hub_kernels.py#is_kernel",
                "transformers/integrations/hub_kernels.py#load_and_register_attn_kernel",
                "transformers/integrations/peft.py#maybe_load_adapters",
                "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES",
                "transformers/integrations/tensor_parallel.py#ALL_PARALLEL_STYLES.keys",
                "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                "transformers/integrations/tensor_parallel.py#distribute_model",
                "transformers/integrations/tensor_parallel.py#initialize_tensor_parallelism",
                "transformers/integrations/tensor_parallel.py#repack_weights",
                "transformers/integrations/tensor_parallel.py#replace_state_dict_local_with_dtensor",
                "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                "transformers/integrations/tensor_parallel.py#verify_tp_plan",
                "transformers/loss/loss_utils.py#LOSS_MAPPING",
                "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention",
                "transformers/modeling_utils.py#EmbeddingAccessMixin",
                "transformers/modeling_utils.py#ModuleUtilsMixin",
                "transformers/modeling_utils.py#SpecificPreTrainedModelType",
                "transformers/modeling_utils.py#VLMS",
                "transformers/modeling_utils.py#_add_variant",
                "transformers/modeling_utils.py#_find_disjoint",
                "transformers/modeling_utils.py#_find_identical",
                "transformers/modeling_utils.py#_find_mismatched_keys",
                "transformers/modeling_utils.py#_find_missing_and_unexpected_keys",
                "transformers/modeling_utils.py#_get_dtype",
                "transformers/modeling_utils.py#_get_resolved_checkpoint_files",
                "transformers/modeling_utils.py#_get_tied_weight_keys",
                "transformers/modeling_utils.py#_infer_parameter_dtype",
                "transformers/modeling_utils.py#_init_weights",
                "transformers/modeling_utils.py#_is_ds_init_called",
                "transformers/modeling_utils.py#_is_dtensor_available",
                "transformers/modeling_utils.py#_is_quantized",
                "transformers/modeling_utils.py#_load_parameter_into_model",
                "transformers/modeling_utils.py#_torch_distributed_available",
                "transformers/modeling_utils.py#get_parameter_dtype",
                "transformers/modeling_utils.py#is_accelerator_device",
                "transformers/modeling_utils.py#is_local_dist_rank_0",
                "transformers/modeling_utils.py#load_shard_file",
                "transformers/modeling_utils.py#load_shard_files_with_threadpool",
                "transformers/modeling_utils.py#load_state_dict",
                "transformers/modeling_utils.py#no_init_weights",
                "transformers/modeling_utils.py#restore_default_dtype",
                "transformers/modeling_utils.py#set_quantized_state",
                "transformers/modeling_utils.py#set_zero3_state",
                "transformers/modeling_utils.py#unwrap_model",
                "transformers/pytorch_utils.py#id_tensor_storage",
                "transformers/quantizers.py#HfQuantizer",
                "transformers/quantizers/auto.py#get_hf_quantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#ADAPTER_SAFE_WEIGHTS_NAME",
                "transformers/utils.py#ADAPTER_WEIGHTS_NAME",
                "transformers/utils.py#ContextManagers",
                "transformers/utils.py#DUMMY_INPUTS",
                "transformers/utils.py#KernelConfig",
                "transformers/utils.py#PushToHubMixin",
                "transformers/utils.py#PushToHubMixin.push_to_hub",
                "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                "transformers/utils.py#SAFE_WEIGHTS_NAME",
                "transformers/utils.py#WEIGHTS_INDEX_NAME",
                "transformers/utils.py#WEIGHTS_NAME",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_flash_attn_2_available",
                "transformers/utils.py#is_flash_attn_3_available",
                "transformers/utils.py#is_kernels_available",
                "transformers/utils.py#is_offline_mode",
                "transformers/utils.py#is_torch_flex_attn_available",
                "transformers/utils.py#is_torch_mlu_available",
                "transformers/utils.py#is_torch_npu_available",
                "transformers/utils/generic.py#OutputRecorder",
                "transformers/utils/generic.py#_CAN_RECORD_REGISTRY",
                "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES",
                "transformers/utils/import_utils.py#is_huggingface_hub_greater_or_equal",
                "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                "transformers/utils/quantization_config.py#QuantizationMethod.GPTQ",
                "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
            ],
            "Qwen3MoeAttention": [
                "transformers/cache_utils.py#Cache",
                "transformers/modeling_flash_attention_utils.py#FlashAttentionKwargs",
                "transformers/modeling_utils.py#ALL_ATTENTION_FUNCTIONS",
                "transformers/models/qwen3_moe/configuration_qwen3_moe.py#Qwen3MoeConfig",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeRMSNorm",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#apply_rotary_pos_emb",
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#eager_attention_forward",
                "transformers/processing_utils.py#Unpack"
            ],
            "OutputRecorder": [
                "transformers/utils/import_utils.py#requires"
            ],
            "DynamicLayer": [
                "transformers/cache_utils.py#CacheLayerMixin"
            ],
            "DynamicSlidingWindowLayer": [
                "transformers/cache_utils.py#DynamicLayer"
            ],
            "PreTrainedConfig": [
                "transformers/__init__.py#__version__",
                "transformers/configuration_utils.py#SpecificPreTrainedConfigType",
                "transformers/configuration_utils.py#get_configuration_file",
                "transformers/dynamic_module_utils.py#custom_object_save",
                "transformers/modeling_gguf_pytorch_utils.py#load_gguf_checkpoint",
                "transformers/utils.py#CONFIG_NAME",
                "transformers/utils.py#PushToHubMixin",
                "transformers/utils.py#cached_file",
                "transformers/utils.py#download_url",
                "transformers/utils.py#extract_commit_hash",
                "transformers/utils.py#is_remote_url",
                "transformers/utils.py#is_torch_available",
                "transformers/utils/generic.py#is_timm_config_dict"
            ],
            "_is_torch_greater_or_equal_than_2_6": [
                "transformers/utils/import_utils.py#is_torch_greater_or_equal"
            ],
            "_is_torch_xpu_available": [
                "transformers/utils.py#is_torch_xpu_available"
            ],
            "_preprocess_mask_arguments": [
                "transformers/cache_utils.py#Cache",
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/masking_utils.py#find_packed_sequence_indices"
            ],
            "sliding_window_causal_mask_function": [
                "transformers/masking_utils.py#and_masks",
                "transformers/masking_utils.py#causal_mask_function",
                "transformers/masking_utils.py#sliding_window_overlay"
            ],
            "rope_config_validation": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#ROPE_VALIDATION_FUNCTIONS"
            ],
            "ROPE_INIT_FUNCTIONS": [
                "transformers/modeling_rope_utils.py#_compute_dynamic_ntk_parameters",
                "transformers/modeling_rope_utils.py#_compute_linear_scaling_rope_parameters",
                "transformers/modeling_rope_utils.py#_compute_llama3_parameters",
                "transformers/modeling_rope_utils.py#_compute_longrope_parameters",
                "transformers/modeling_rope_utils.py#_compute_yarn_parameters"
            ],
            "dynamic_rope_update": [
                "transformers/modeling_rope_utils.py#ROPE_INIT_FUNCTIONS"
            ],
            "custom_object_save": [],
            "_get_device_map": [
                "transformers/integrations/accelerate.py#find_tied_parameters",
                "transformers/integrations/accelerate.py#get_balanced_memory",
                "transformers/utils.py#is_torch_xpu_available"
            ],
            "accelerate_disk_offload": [],
            "accelerate_dispatch": [
                "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                "transformers/integrations/fsdp.py#is_fsdp_enabled",
                "transformers/utils/quantization_config.py#QuantizationMethod.FBGEMM_FP8",
                "transformers/utils/quantization_config.py#QuantizationMethod.HQQ"
            ],
            "check_and_set_device_map": [
                "transformers/integrations/deepspeed.py#is_deepspeed_zero3_enabled",
                "transformers/utils.py#is_accelerate_available"
            ],
            "init_empty_weights": [
                "transformers/integrations/accelerate.py#init_on_device"
            ],
            "load_and_register_attn_kernel": [
                "transformers/integrations/flash_attention.py#flash_attention_forward",
                "transformers/integrations/hub_kernels.py#is_kernel",
                "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention"
            ],
            "maybe_load_adapters": [
                "transformers/utils/hub.py#DownloadKwargs"
            ],
            "distribute_model": [
                "transformers/distributed.py#DistributedConfig.from_dict",
                "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan",
                "transformers/integrations/tensor_parallel.py#_torch_distributed_available",
                "transformers/integrations/tensor_parallel.py#add_tensor_parallel_hooks_to_module",
                "transformers/utils.py#is_torch_greater_or_equal"
            ],
            "initialize_tensor_parallelism": [
                "transformers/utils.py#is_torch_greater_or_equal"
            ],
            "replace_state_dict_local_with_dtensor": [
                "transformers/integrations/tensor_parallel.py#convert_local_tensor_to_dtensor"
            ],
            "shard_and_distribute_module": [
                "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan"
            ],
            "verify_tp_plan": [],
            "LOSS_MAPPING": [
                "transformers/loss/loss_d_fine.py#DFineForObjectDetectionLoss",
                "transformers/loss/loss_deformable_detr.py#DeformableDetrForObjectDetectionLoss",
                "transformers/loss/loss_deformable_detr.py#DeformableDetrForSegmentationLoss",
                "transformers/loss/loss_for_object_detection.py#ForObjectDetectionLoss",
                "transformers/loss/loss_for_object_detection.py#ForSegmentationLoss",
                "transformers/loss/loss_grounding_dino.py#GroundingDinoForObjectDetectionLoss",
                "transformers/loss/loss_rt_detr.py#RTDetrForObjectDetectionLoss",
                "transformers/loss/loss_utils.py#ForCausalLMLoss",
                "transformers/loss/loss_utils.py#ForMaskedLMLoss",
                "transformers/loss/loss_utils.py#ForQuestionAnsweringLoss",
                "transformers/loss/loss_utils.py#ForSequenceClassificationLoss",
                "transformers/loss/loss_utils.py#ForTokenClassification"
            ],
            "lazy_import_flash_attention": [
                "transformers/modeling_flash_attention_utils.py#_flash_fn",
                "transformers/modeling_flash_attention_utils.py#_flash_varlen_fn",
                "transformers/modeling_flash_attention_utils.py#_lazy_define_process_function",
                "transformers/modeling_flash_attention_utils.py#_lazy_imports",
                "transformers/modeling_flash_attention_utils.py#_pad_fn",
                "transformers/modeling_flash_attention_utils.py#_process_flash_kwargs_fn",
                "transformers/modeling_flash_attention_utils.py#_unpad_fn"
            ],
            "ModuleUtilsMixin": [
                "transformers/modeling_utils.py#get_parameter_device",
                "transformers/modeling_utils.py#get_parameter_dtype"
            ],
            "_find_disjoint": [
                "transformers/modeling_utils.py#_end_ptr"
            ],
            "_find_identical": [
                "transformers/modeling_utils.py#_end_ptr"
            ],
            "_find_mismatched_keys": [],
            "_find_missing_and_unexpected_keys": [
                "transformers/integrations/accelerate.py#find_tied_parameters",
                "transformers/quantizers.py#HfQuantizer"
            ],
            "_get_dtype": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_utils.py#get_state_dict_dtype",
                "transformers/modeling_utils.py#load_state_dict"
            ],
            "_get_resolved_checkpoint_files": [
                "transformers/modeling_utils.py#_add_variant",
                "transformers/safetensors_conversion.py#auto_conversion",
                "transformers/utils.py#SAFE_WEIGHTS_INDEX_NAME",
                "transformers/utils.py#SAFE_WEIGHTS_NAME",
                "transformers/utils.py#WEIGHTS_INDEX_NAME",
                "transformers/utils.py#WEIGHTS_NAME",
                "transformers/utils.py#cached_file",
                "transformers/utils.py#download_url",
                "transformers/utils.py#has_file",
                "transformers/utils.py#is_offline_mode",
                "transformers/utils.py#is_remote_url",
                "transformers/utils/hub.py#DownloadKwargs",
                "transformers/utils/hub.py#get_checkpoint_shard_files"
            ],
            "_infer_parameter_dtype": [
                "transformers/quantizers.py#HfQuantizer",
                "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                "transformers/utils/quantization_config.py#QuantizationMethod.HQQ",
                "transformers/utils/quantization_config.py#QuantizationMethod.MXFP4",
                "transformers/utils/quantization_config.py#QuantizationMethod.QUARK"
            ],
            "_is_dtensor_available": [
                "transformers/modeling_utils.py#_torch_distributed_available",
                "transformers/utils.py#is_torch_greater_or_equal"
            ],
            "_load_parameter_into_model": [
                "transformers/quantizers/quantizers_utils.py#get_module_from_name"
            ],
            "get_parameter_dtype": [
                "transformers/modeling_utils.py#XLA_DOWNCAST_BF16",
                "transformers/modeling_utils.py#XLA_USE_BF16",
                "transformers/utils.py#is_torch_xla_available",
                "transformers/utils/import_utils.py#ENV_VARS_TRUE_VALUES"
            ],
            "load_shard_file": [
                "transformers/modeling_utils.py#_load_state_dict_into_meta_model",
                "transformers/modeling_utils.py#load_state_dict"
            ],
            "load_shard_files_with_threadpool": [
                "transformers/modeling_utils.py#load_shard_file"
            ],
            "load_state_dict": [
                "transformers/modeling_utils.py#str_to_torch_dtype"
            ],
            "no_init_weights": [
                "transformers/modeling_utils.py#TORCH_INIT_FUNCTIONS",
                "transformers/modeling_utils.py#_init_weights"
            ],
            "set_quantized_state": [
                "transformers/modeling_utils.py#_is_quantized"
            ],
            "set_zero3_state": [
                "transformers/modeling_utils.py#_is_ds_init_called"
            ],
            "unwrap_model": [],
            "id_tensor_storage": [
                "transformers/pytorch_utils.py#_torch_distributed_available",
                "transformers/utils.py#is_torch_greater_or_equal",
                "transformers/utils.py#is_torch_xla_available"
            ],
            "get_hf_quantizer": [
                "transformers/quantizers/auto.py#AutoHfQuantizer"
            ],
            "is_huggingface_hub_greater_or_equal": [],
            "apply_rotary_pos_emb": [
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#rotate_half"
            ],
            "eager_attention_forward": [
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#repeat_kv",
                "transformers/processing_utils.py#Unpack",
                "transformers/utils.py#TransformersKwargs"
            ],
            "requires": [
                "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                "transformers/utils/import_utils.py#Backend"
            ],
            "get_configuration_file": [
                "transformers/__init__.py#__version__",
                "transformers/utils.py#CONFIG_NAME"
            ],
            "load_gguf_checkpoint": [
                "transformers/integrations.py#_gguf_parse_value",
                "transformers/modeling_gguf_pytorch_utils.py#GGUF_SUPPORTED_ARCHITECTURES",
                "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING",
                "transformers/modeling_gguf_pytorch_utils.py#TENSOR_PROCESSORS",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#get_gguf_hf_weights_map",
                "transformers/modeling_gguf_pytorch_utils.py#read_field",
                "transformers/utils.py#is_torch_available",
                "transformers/utils/import_utils.py#is_gguf_available"
            ],
            "is_torch_greater_or_equal": [],
            "ROPE_VALIDATION_FUNCTIONS": [
                "transformers/modeling_rope_utils.py#_validate_default_rope_parameters",
                "transformers/modeling_rope_utils.py#_validate_dynamic_scaling_rope_parameters",
                "transformers/modeling_rope_utils.py#_validate_linear_scaling_rope_parameters",
                "transformers/modeling_rope_utils.py#_validate_llama3_parameters",
                "transformers/modeling_rope_utils.py#_validate_longrope_parameters",
                "transformers/modeling_rope_utils.py#_validate_yarn_parameters"
            ],
            "_compute_dynamic_ntk_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_linear_scaling_rope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_llama3_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_longrope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "_compute_yarn_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#standardize_rope_params"
            ],
            "get_balanced_memory": [
                "transformers/integrations/accelerate.py#compute_module_sizes"
            ],
            "is_deepspeed_zero3_enabled": [],
            "is_fsdp_enabled": [
                "transformers/utils.py#is_torch_available",
                "transformers/utils.py#strtobool"
            ],
            "flash_attention_forward": [
                "transformers/integrations/flash_attention.py#_use_top_left_mask",
                "transformers/integrations/flash_attention.py#get_target_dtype",
                "transformers/modeling_flash_attention_utils.py#_flash_attention_forward"
            ],
            "convert_local_tensor_to_dtensor": [
                "transformers/integrations/tensor_parallel.py#_get_parameter_tp_plan"
            ],
            "DFineForObjectDetectionLoss": [
                "transformers/loss/loss_d_fine.py#DFineLoss",
                "transformers/loss/loss_d_fine.py#_set_aux_loss",
                "transformers/loss/loss_d_fine.py#_set_aux_loss2"
            ],
            "DeformableDetrForObjectDetectionLoss": [
                "transformers/loss/loss_deformable_detr.py#DeformableDetrHungarianMatcher",
                "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
            ],
            "DeformableDetrForSegmentationLoss": [
                "transformers/loss/loss_deformable_detr.py#DeformableDetrImageLoss",
                "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
            ],
            "ForObjectDetectionLoss": [
                "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                "transformers/loss/loss_for_object_detection.py#ImageLoss",
                "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
            ],
            "ForSegmentationLoss": [
                "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                "transformers/loss/loss_for_object_detection.py#ImageLoss",
                "transformers/loss/loss_for_object_detection.py#_set_aux_loss"
            ],
            "GroundingDinoForObjectDetectionLoss": [
                "transformers/loss/loss_for_object_detection.py#_set_aux_loss",
                "transformers/loss/loss_grounding_dino.py#GroundingDinoHungarianMatcher",
                "transformers/loss/loss_grounding_dino.py#GroundingDinoImageLoss"
            ],
            "RTDetrForObjectDetectionLoss": [
                "transformers/loss/loss_rt_detr.py#RTDetrLoss",
                "transformers/loss/loss_rt_detr.py#_set_aux_loss"
            ],
            "ForCausalLMLoss": [
                "transformers/loss/loss_utils.py#fixed_cross_entropy"
            ],
            "ForMaskedLMLoss": [
                "transformers/loss/loss_utils.py#fixed_cross_entropy"
            ],
            "ForQuestionAnsweringLoss": [
                "transformers/loss/loss_utils.py#fixed_cross_entropy"
            ],
            "ForSequenceClassificationLoss": [
                "transformers/loss/loss_utils.py#fixed_cross_entropy"
            ],
            "ForTokenClassification": [
                "transformers/loss/loss_utils.py#fixed_cross_entropy"
            ],
            "_lazy_define_process_function": [
                "transformers/modeling_flash_attention_utils.py#_hf_api_to_flash_mapping",
                "transformers/modeling_flash_attention_utils.py#_process_flash_attention_kwargs"
            ],
            "_lazy_imports": [
                "transformers/modeling_flash_attention_utils.py#_pad_input",
                "transformers/modeling_flash_attention_utils.py#_unpad_input",
                "transformers/utils.py#is_flash_attn_2_available",
                "transformers/utils.py#is_flash_attn_3_available",
                "transformers/utils.py#is_torch_npu_available"
            ],
            "auto_conversion": [],
            "get_checkpoint_shard_files": [],
            "_load_state_dict_into_meta_model": [
                "transformers/integrations/tensor_parallel.py#shard_and_distribute_module",
                "transformers/modeling_utils.py#_infer_parameter_dtype",
                "transformers/modeling_utils.py#_load_parameter_into_model",
                "transformers/quantizers.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name"
            ],
            "AutoHfQuantizer": [
                "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                "transformers/quantizers/auto.py#AUTO_QUANTIZER_MAPPING",
                "transformers/quantizers/auto.py#AutoQuantizationConfig",
                "transformers/utils/quantization_config.py#AutoRoundConfig",
                "transformers/utils/quantization_config.py#AutoRoundConfig.from_dict",
                "transformers/utils/quantization_config.py#AwqConfig",
                "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                "transformers/utils/quantization_config.py#FbgemmFp8Config",
                "transformers/utils/quantization_config.py#GPTQConfig",
                "transformers/utils/quantization_config.py#Mxfp4Config",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES"
            ],
            "BACKENDS_MAPPING": [
                "transformers/utils/import_utils.py#ACCELERATE_IMPORT_ERROR",
                "transformers/utils/import_utils.py#AV_IMPORT_ERROR",
                "transformers/utils/import_utils.py#BS4_IMPORT_ERROR",
                "transformers/utils/import_utils.py#CCL_IMPORT_ERROR",
                "transformers/utils/import_utils.py#CV2_IMPORT_ERROR",
                "transformers/utils/import_utils.py#CYTHON_IMPORT_ERROR",
                "transformers/utils/import_utils.py#DATASETS_IMPORT_ERROR",
                "transformers/utils/import_utils.py#DECORD_IMPORT_ERROR",
                "transformers/utils/import_utils.py#DETECTRON2_IMPORT_ERROR",
                "transformers/utils/import_utils.py#ESSENTIA_IMPORT_ERROR",
                "transformers/utils/import_utils.py#FAISS_IMPORT_ERROR",
                "transformers/utils/import_utils.py#FASTAPI_IMPORT_ERROR",
                "transformers/utils/import_utils.py#FTFY_IMPORT_ERROR",
                "transformers/utils/import_utils.py#G2P_EN_IMPORT_ERROR",
                "transformers/utils/import_utils.py#JINJA_IMPORT_ERROR",
                "transformers/utils/import_utils.py#LEVENSHTEIN_IMPORT_ERROR",
                "transformers/utils/import_utils.py#LIBROSA_IMPORT_ERROR",
                "transformers/utils/import_utils.py#MISTRAL_COMMON_IMPORT_ERROR",
                "transformers/utils/import_utils.py#NATTEN_IMPORT_ERROR",
                "transformers/utils/import_utils.py#NLTK_IMPORT_ERROR",
                "transformers/utils/import_utils.py#OPENAI_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PANDAS_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PEFT_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PHONEMIZER_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PRETTY_MIDI_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PROTOBUF_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PYCTCDECODE_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PYDANTIC_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PYTESSERACT_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PYTORCH_IMPORT_ERROR",
                "transformers/utils/import_utils.py#PYTORCH_QUANTIZATION_IMPORT_ERROR",
                "transformers/utils/import_utils.py#RICH_IMPORT_ERROR",
                "transformers/utils/import_utils.py#RJIEBA_IMPORT_ERROR",
                "transformers/utils/import_utils.py#SACREMOSES_IMPORT_ERROR",
                "transformers/utils/import_utils.py#SCIPY_IMPORT_ERROR",
                "transformers/utils/import_utils.py#SENTENCEPIECE_IMPORT_ERROR",
                "transformers/utils/import_utils.py#SKLEARN_IMPORT_ERROR",
                "transformers/utils/import_utils.py#SPEECH_IMPORT_ERROR",
                "transformers/utils/import_utils.py#TIMM_IMPORT_ERROR",
                "transformers/utils/import_utils.py#TOKENIZERS_IMPORT_ERROR",
                "transformers/utils/import_utils.py#TORCHAUDIO_IMPORT_ERROR",
                "transformers/utils/import_utils.py#TORCHCODEC_IMPORT_ERROR",
                "transformers/utils/import_utils.py#TORCHVISION_IMPORT_ERROR",
                "transformers/utils/import_utils.py#UROMAN_IMPORT_ERROR",
                "transformers/utils/import_utils.py#UVICORN_IMPORT_ERROR",
                "transformers/utils/import_utils.py#VISION_IMPORT_ERROR",
                "transformers/utils/import_utils.py#YT_DLP_IMPORT_ERROR",
                "transformers/utils/import_utils.py#is_accelerate_available",
                "transformers/utils/import_utils.py#is_av_available",
                "transformers/utils/import_utils.py#is_bs4_available",
                "transformers/utils/import_utils.py#is_ccl_available",
                "transformers/utils/import_utils.py#is_cv2_available",
                "transformers/utils/import_utils.py#is_cython_available",
                "transformers/utils/import_utils.py#is_datasets_available",
                "transformers/utils/import_utils.py#is_decord_available",
                "transformers/utils/import_utils.py#is_detectron2_available",
                "transformers/utils/import_utils.py#is_essentia_available",
                "transformers/utils/import_utils.py#is_faiss_available",
                "transformers/utils/import_utils.py#is_fastapi_available",
                "transformers/utils/import_utils.py#is_ftfy_available",
                "transformers/utils/import_utils.py#is_g2p_en_available",
                "transformers/utils/import_utils.py#is_jinja_available",
                "transformers/utils/import_utils.py#is_levenshtein_available",
                "transformers/utils/import_utils.py#is_librosa_available",
                "transformers/utils/import_utils.py#is_mistral_common_available",
                "transformers/utils/import_utils.py#is_natten_available",
                "transformers/utils/import_utils.py#is_nltk_available",
                "transformers/utils/import_utils.py#is_openai_available",
                "transformers/utils/import_utils.py#is_pandas_available",
                "transformers/utils/import_utils.py#is_peft_available",
                "transformers/utils/import_utils.py#is_phonemizer_available",
                "transformers/utils/import_utils.py#is_pretty_midi_available",
                "transformers/utils/import_utils.py#is_protobuf_available",
                "transformers/utils/import_utils.py#is_pyctcdecode_available",
                "transformers/utils/import_utils.py#is_pydantic_available",
                "transformers/utils/import_utils.py#is_pytesseract_available",
                "transformers/utils/import_utils.py#is_pytorch_quantization_available",
                "transformers/utils/import_utils.py#is_rich_available",
                "transformers/utils/import_utils.py#is_rjieba_available",
                "transformers/utils/import_utils.py#is_sacremoses_available",
                "transformers/utils/import_utils.py#is_scipy_available",
                "transformers/utils/import_utils.py#is_sentencepiece_available",
                "transformers/utils/import_utils.py#is_sklearn_available",
                "transformers/utils/import_utils.py#is_speech_available",
                "transformers/utils/import_utils.py#is_timm_available",
                "transformers/utils/import_utils.py#is_tokenizers_available",
                "transformers/utils/import_utils.py#is_torch_available",
                "transformers/utils/import_utils.py#is_torchaudio_available",
                "transformers/utils/import_utils.py#is_torchcodec_available",
                "transformers/utils/import_utils.py#is_torchvision_available",
                "transformers/utils/import_utils.py#is_uroman_available",
                "transformers/utils/import_utils.py#is_uvicorn_available",
                "transformers/utils/import_utils.py#is_vision_available",
                "transformers/utils/import_utils.py#is_yt_dlp_available"
            ],
            "Backend": [
                "transformers/utils/import_utils.py#BACKENDS_MAPPING",
                "transformers/utils/import_utils.py#VersionComparison",
                "transformers/utils/import_utils.py#_is_package_available",
                "transformers/utils/import_utils.py#split_package_version"
            ],
            "GGUF_SUPPORTED_ARCHITECTURES": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUF_TO_TRANSFORMERS_MAPPING"
            ],
            "GGUF_TO_TRANSFORMERS_MAPPING": [
                "transformers/integrations.py#GGUF_CONFIG_MAPPING",
                "transformers/integrations.py#GGUF_TOKENIZER_MAPPING"
            ],
            "TENSOR_PROCESSORS": [
                "transformers/modeling_gguf_pytorch_utils.py#BloomTensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#GPT2TensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#Gemma2TensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#Lfm2TensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#LlamaTensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#MambaTensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#NemotronTensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#Qwen2MoeTensorProcessor",
                "transformers/modeling_gguf_pytorch_utils.py#T5TensorProcessor"
            ],
            "TensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor"
            ],
            "get_gguf_hf_weights_map": [
                "transformers/utils.py#is_torch_available",
                "transformers/utils/import_utils.py#is_gguf_available"
            ],
            "read_field": [
                "transformers/integrations.py#_gguf_parse_value"
            ],
            "is_gguf_available": [
                "transformers/utils/import_utils.py#GGUF_MIN_VERSION",
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "_validate_default_rope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#_check_received_keys"
            ],
            "_validate_dynamic_scaling_rope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#_check_received_keys"
            ],
            "_validate_linear_scaling_rope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#_check_received_keys"
            ],
            "_validate_llama3_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#_check_received_keys"
            ],
            "_validate_longrope_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#_check_received_keys"
            ],
            "_validate_yarn_parameters": [
                "transformers/configuration_utils.py#PreTrainedConfig",
                "transformers/modeling_rope_utils.py#_check_received_keys"
            ],
            "_use_top_left_mask": [
                "transformers/modeling_flash_attention_utils.py#flash_attn_supports_top_left_mask"
            ],
            "_flash_attention_forward": [
                "transformers/modeling_flash_attention_utils.py#_is_packed_sequence",
                "transformers/modeling_flash_attention_utils.py#_prepare_from_posids",
                "transformers/modeling_flash_attention_utils.py#_upad_input",
                "transformers/modeling_flash_attention_utils.py#fa_peft_integration_check",
                "transformers/modeling_flash_attention_utils.py#lazy_import_flash_attention"
            ],
            "DFineLoss": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_d_fine.py#bbox2distance",
                "transformers/loss/loss_for_object_detection.py#box_iou",
                "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher",
                "transformers/loss/loss_rt_detr.py#RTDetrLoss"
            ],
            "DeformableDetrHungarianMatcher": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
            ],
            "DeformableDetrImageLoss": [
                "transformers/loss/loss_for_object_detection.py#ImageLoss",
                "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss"
            ],
            "HungarianMatcher": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
            ],
            "ImageLoss": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_for_object_detection.py#dice_loss",
                "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss"
            ],
            "GroundingDinoHungarianMatcher": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_for_object_detection.py#HungarianMatcher",
                "transformers/loss/loss_for_object_detection.py#generalized_box_iou"
            ],
            "GroundingDinoImageLoss": [
                "transformers/loss/loss_for_object_detection.py#ImageLoss",
                "transformers/loss/loss_grounding_dino.py#sigmoid_focal_loss"
            ],
            "RTDetrLoss": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_for_object_detection.py#box_iou",
                "transformers/loss/loss_for_object_detection.py#dice_loss",
                "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                "transformers/loss/loss_for_object_detection.py#nested_tensor_from_tensor_list",
                "transformers/loss/loss_for_object_detection.py#sigmoid_focal_loss",
                "transformers/loss/loss_rt_detr.py#RTDetrHungarianMatcher"
            ],
            "_unpad_input": [
                "transformers/modeling_flash_attention_utils.py#_index_first_axis"
            ],
            "AUTO_QUANTIZATION_CONFIG_MAPPING": [
                "transformers/utils/quantization_config.py#AqlmConfig",
                "transformers/utils/quantization_config.py#AutoRoundConfig",
                "transformers/utils/quantization_config.py#AwqConfig",
                "transformers/utils/quantization_config.py#BitNetQuantConfig",
                "transformers/utils/quantization_config.py#BitsAndBytesConfig",
                "transformers/utils/quantization_config.py#CompressedTensorsConfig",
                "transformers/utils/quantization_config.py#EetqConfig",
                "transformers/utils/quantization_config.py#FPQuantConfig",
                "transformers/utils/quantization_config.py#FbgemmFp8Config",
                "transformers/utils/quantization_config.py#FineGrainedFP8Config",
                "transformers/utils/quantization_config.py#GPTQConfig",
                "transformers/utils/quantization_config.py#HiggsConfig",
                "transformers/utils/quantization_config.py#HqqConfig",
                "transformers/utils/quantization_config.py#Mxfp4Config",
                "transformers/utils/quantization_config.py#QuantoConfig",
                "transformers/utils/quantization_config.py#QuarkConfig",
                "transformers/utils/quantization_config.py#SpQRConfig",
                "transformers/utils/quantization_config.py#TorchAoConfig",
                "transformers/utils/quantization_config.py#VptqConfig"
            ],
            "AUTO_QUANTIZER_MAPPING": [
                "transformers/quantizers/quantizer_aqlm.py#AqlmHfQuantizer",
                "transformers/quantizers/quantizer_auto_round.py#AutoRoundQuantizer",
                "transformers/quantizers/quantizer_awq.py#AwqQuantizer",
                "transformers/quantizers/quantizer_bitnet.py#BitNetHfQuantizer",
                "transformers/quantizers/quantizer_bnb_4bit.py#Bnb4BitHfQuantizer",
                "transformers/quantizers/quantizer_bnb_8bit.py#Bnb8BitHfQuantizer",
                "transformers/quantizers/quantizer_compressed_tensors.py#CompressedTensorsHfQuantizer",
                "transformers/quantizers/quantizer_eetq.py#EetqHfQuantizer",
                "transformers/quantizers/quantizer_fbgemm_fp8.py#FbgemmFp8HfQuantizer",
                "transformers/quantizers/quantizer_finegrained_fp8.py#FineGrainedFP8HfQuantizer",
                "transformers/quantizers/quantizer_fp_quant.py#FPQuantHfQuantizer",
                "transformers/quantizers/quantizer_gptq.py#GptqHfQuantizer",
                "transformers/quantizers/quantizer_higgs.py#HiggsHfQuantizer",
                "transformers/quantizers/quantizer_hqq.py#HqqHfQuantizer",
                "transformers/quantizers/quantizer_mxfp4.py#Mxfp4HfQuantizer",
                "transformers/quantizers/quantizer_quanto.py#QuantoHfQuantizer",
                "transformers/quantizers/quantizer_quark.py#QuarkHfQuantizer",
                "transformers/quantizers/quantizer_spqr.py#SpQRHfQuantizer",
                "transformers/quantizers/quantizer_torchao.py#TorchAoHfQuantizer",
                "transformers/quantizers/quantizer_vptq.py#VptqHfQuantizer"
            ],
            "AutoQuantizationConfig": [
                "transformers/models/auto/configuration_auto.py#AutoConfig.from_pretrained",
                "transformers/quantizers/auto.py#AUTO_QUANTIZATION_CONFIG_MAPPING",
                "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES"
            ],
            "AutoRoundConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "AwqConfig": [
                "transformers/utils.py#is_auto_awq_available",
                "transformers/utils/quantization_config.py#AWQLinearVersion",
                "transformers/utils/quantization_config.py#AwqBackendPackingMethod",
                "transformers/utils/quantization_config.py#ExllamaVersion",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "CompressedTensorsConfig": [
                "transformers/utils.py#is_compressed_tensors_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "FbgemmFp8Config": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "GPTQConfig": [
                "transformers/utils.py#is_gptqmodel_available",
                "transformers/utils/import_utils.py#is_auto_gptq_available",
                "transformers/utils/quantization_config.py#ExllamaVersion",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "Mxfp4Config": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "QuantizationConfigMixin": [
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "is_accelerate_available": [
                "transformers/utils/import_utils.py#ACCELERATE_MIN_VERSION",
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_av_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_bs4_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_ccl_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_cv2_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_cython_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_datasets_available": [],
            "is_decord_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_essentia_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_faiss_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_fastapi_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_ftfy_available": [],
            "is_g2p_en_available": [],
            "is_jinja_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_levenshtein_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_librosa_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_mistral_common_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_natten_available": [],
            "is_nltk_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_openai_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_pandas_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_peft_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_phonemizer_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_pretty_midi_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_protobuf_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_pyctcdecode_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_pydantic_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_pytesseract_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_pytorch_quantization_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_rich_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_rjieba_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_sacremoses_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_scipy_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_sentencepiece_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_sklearn_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_speech_available": [
                "transformers/utils/import_utils.py#is_torchaudio_available"
            ],
            "is_timm_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_tokenizers_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_torch_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_torchaudio_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_torchcodec_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_torchvision_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_uroman_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_uvicorn_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_vision_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "is_yt_dlp_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "_is_package_available": [
                "transformers/utils/import_utils.py#PACKAGE_DISTRIBUTION_MAPPING"
            ],
            "BloomTensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "GPT2TensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "Gemma2TensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "Lfm2TensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "LlamaTensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "MambaTensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "NemotronTensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "Qwen2MoeTensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "T5TensorProcessor": [
                "transformers/modeling_gguf_pytorch_utils.py#GGUFTensor",
                "transformers/modeling_gguf_pytorch_utils.py#TensorProcessor"
            ],
            "_check_received_keys": [],
            "flash_attn_supports_top_left_mask": [
                "transformers/utils.py#is_flash_attn_2_available",
                "transformers/utils.py#is_flash_attn_3_available",
                "transformers/utils.py#is_flash_attn_greater_or_equal_2_10"
            ],
            "_prepare_from_posids": [
                "transformers/modeling_flash_attention_utils.py#prepare_fa_kwargs_from_position_ids"
            ],
            "_upad_input": [
                "transformers/modeling_flash_attention_utils.py#_get_unpad_data",
                "transformers/modeling_flash_attention_utils.py#_index_first_axis"
            ],
            "fa_peft_integration_check": [],
            "center_to_corners_format": [
                "transformers/image_transforms.py#_center_to_corners_format_numpy",
                "transformers/image_transforms.py#_center_to_corners_format_torch",
                "transformers/utils.py#TensorType",
                "transformers/utils.py#is_torch_tensor"
            ],
            "bbox2distance": [
                "transformers/loss/loss_d_fine.py#translate_gt"
            ],
            "box_iou": [
                "transformers/loss/loss_for_object_detection.py#box_area"
            ],
            "RTDetrHungarianMatcher": [
                "transformers/image_transforms.py#center_to_corners_format",
                "transformers/loss/loss_for_object_detection.py#generalized_box_iou",
                "transformers/utils.py#requires_backends"
            ],
            "generalized_box_iou": [
                "transformers/loss/loss_for_object_detection.py#box_iou"
            ],
            "nested_tensor_from_tensor_list": [
                "transformers/loss/loss_for_object_detection.py#NestedTensor",
                "transformers/loss/loss_for_object_detection.py#_max_by_axis"
            ],
            "AqlmConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "BitNetQuantConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "BitsAndBytesConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "EetqConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "FPQuantConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "FineGrainedFP8Config": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "HiggsConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "HqqConfig": [
                "transformers/utils.py#is_hqq_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "QuantoConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "QuarkConfig": [
                "transformers/utils.py#is_quark_available",
                "transformers/utils.py#is_torch_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "SpQRConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "TorchAoConfig": [
                "transformers/utils.py#is_torchao_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod"
            ],
            "VptqConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin",
                "transformers/utils/quantization_config.py#QuantizationMethod",
                "transformers/utils/quantization_config.py#VptqLayerConfig"
            ],
            "AqlmHfQuantizer": [
                "transformers/integrations.py#replace_with_aqlm_linear",
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_aqlm_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "AutoRoundQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_auto_round_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "AwqQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_auto_awq_available",
                "transformers/utils/quantization_config.py#AWQLinearVersion.EXLLAMA",
                "transformers/utils/quantization_config.py#AWQLinearVersion.GEMM",
                "transformers/utils/quantization_config.py#AWQLinearVersion.IPEX"
            ],
            "BitNetHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer"
            ],
            "Bnb4BitHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_bitsandbytes_available",
                "transformers/utils.py#is_torch_hpu_available",
                "transformers/utils.py#is_torch_npu_available",
                "transformers/utils.py#is_torch_xpu_available"
            ],
            "Bnb8BitHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_torch_xpu_available"
            ],
            "CompressedTensorsHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_compressed_tensors_available",
                "transformers/utils.py#is_torch_available",
                "transformers/utils/quantization_config.py#CompressedTensorsConfig"
            ],
            "EetqHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_eetq_available"
            ],
            "FbgemmFp8HfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_fbgemm_gpu_available",
                "transformers/utils.py#is_torch_available"
            ],
            "FineGrainedFP8HfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_torch_available",
                "transformers/utils.py#is_torch_xpu_available"
            ],
            "FPQuantHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_fp_quant_available",
                "transformers/utils.py#is_qutlass_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "GptqHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_auto_gptq_available",
                "transformers/utils.py#is_gptqmodel_available",
                "transformers/utils.py#is_optimum_available",
                "transformers/utils/quantization_config.py#GPTQConfig.from_dict",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "HiggsHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_flute_available",
                "transformers/utils.py#is_hadamard_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "HqqHfQuantizer": [
                "transformers/integrations.py#prepare_for_hqq_linear",
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_hqq_available"
            ],
            "Mxfp4HfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_kernels_available",
                "transformers/utils.py#is_torch_available",
                "transformers/utils.py#is_triton_available"
            ],
            "QuantoHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_optimum_quanto_available",
                "transformers/utils/quantization_config.py#QuantoConfig"
            ],
            "QuarkHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizer_quark.py#CHECKPOINT_KEYS",
                "transformers/utils.py#is_quark_available"
            ],
            "SpQRHfQuantizer": [
                "transformers/integrations.py#replace_with_spqr_linear",
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_spqr_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "TorchAoHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/quantizers/quantizer_torchao.py#_linear_extra_repr",
                "transformers/quantizers/quantizer_torchao.py#fuzzy_match_size",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils.py#is_torchao_available"
            ],
            "VptqHfQuantizer": [
                "transformers/quantizers/base.py#HfQuantizer",
                "transformers/utils.py#is_accelerate_available",
                "transformers/utils.py#is_vptq_available",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "is_auto_gptq_available": [
                "transformers/utils/import_utils.py#_is_package_available"
            ],
            "translate_gt": [
                "transformers/loss/loss_d_fine.py#weighting_function"
            ],
            "box_area": [
                "transformers/loss/loss_for_object_detection.py#_upcast"
            ],
            "VptqLayerConfig": [
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "HfQuantizer": [
                "transformers/quantizers/base.py#MODULES_TO_PATCH_FOR_QUANTIZATION",
                "transformers/quantizers/base.py#_assign_original_dtype",
                "transformers/quantizers/quantizers_utils.py#get_module_from_name",
                "transformers/utils/quantization_config.py#QuantizationConfigMixin"
            ],
            "_linear_extra_repr": [
                "transformers/quantizers/quantizer_torchao.py#_quantization_type"
            ],
            "MODULES_TO_PATCH_FOR_QUANTIZATION": [
                "transformers/quantizers/base.py#SequentialLlama4TextExperts",
                "transformers/utils/quantization_config.py#QuantizationMethod.BITS_AND_BYTES",
                "transformers/utils/quantization_config.py#QuantizationMethod.COMPRESSED_TENSORS"
            ]
        },
        "jax_found_dependencies": {
            "Qwen3MoeModel": {
                "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer": [
                    [
                        0.18296018242835999,
                        "src/MaxText/layers/qwen3.py#Qwen3MoeDecoderLayer",
                        "src/MaxText/layers/qwen3.py",
                        "class Qwen3MoeDecoderLayer(AttentionWithNorm):\n  \"\"\"Qwen3 Transformer decoder layer (MoE).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      model_mode: str,\n      quant: None | Quant,\n      rngs: nnx.Rngs,\n  ):\n    super().__init__(config, mesh, model_mode, quant, rngs)\n    self.moe_block = RoutedMoE(\n        config=config,\n        num_experts=config.num_experts,\n        num_experts_per_tok=config.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=max_initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=config.moe_mlp_dim,  # same as config.mlp_dim\n        dtype=config.dtype,\n        weight_dtype=config.weight_dtype,\n        quant=quant,\n        rngs=rngs,\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    hidden_states, intermediate_inputs = self.apply_attention_with_norm(\n        inputs, decoder_segment_ids, decoder_positions, deterministic, model_mode\n    )\n\n    mlp_lnx, load_balance_loss = self.moe_block(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, self.activation_axis_names)\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    layer_output = intermediate_inputs + mlp_lnx\n    layer_output = nn.with_logical_constraint(layer_output, self.activation_axis_names)\n\n    if self.config.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output"
                    ]
                ]
            }
        },
        "jax_dependencies_list": [
            "transformers/models/qwen3_moe/modeling_qwen3_moe.py#Qwen3MoeDecoderLayer"
        ]
    },
    "metadata": {
        "q": "<class 'collections.deque'>",
        "processed_components": "<class 'set'>",
        "processed_count": "<class 'int'>",
        "file_analysis_cache": "<class 'dict'>",
        "files_to_convert": "<class 'list'>",
        "original_dependencies": "<class 'dict'>",
        "jax_found_dependencies": "<class 'collections.defaultdict'>",
        "jax_dependencies_list": "<class 'list'>"
    }
}