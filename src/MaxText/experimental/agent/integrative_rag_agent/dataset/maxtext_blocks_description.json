[
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3LayerNorm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3LayerNorm(nnx.Module):\n  \"\"\"GPT3 Layer normalization operating on the last axis of the input data.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.zeros,\n      use_bias: bool = True,\n      reductions_in_fp32: bool = False,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.use_bias = use_bias\n    self.reductions_in_fp32 = reductions_in_fp32\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    self.scale = nnx.Param(\n        self.scale_init(rngs.params(), (num_features,), self.weight_dtype),\n        sharding=self.kernel_axes,\n    )\n    if self.use_bias:\n      self.bias = nnx.Param(\n          initializers.default_bias_init(rngs.params(), (num_features,), self.weight_dtype), sharding=self.kernel_axes\n      )\n    else:\n      self.bias = None\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    if self.reductions_in_fp32:\n      x = jnp.asarray(x, jnp.float32)\n    mean = jnp.mean(x, axis=[-1], keepdims=True)\n    var = jnp.mean(jnp.square(x - mean), axis=[-1], keepdims=True)\n    normed_inputs = (x - mean) * lax.rsqrt(var + self.epsilon)\n    if self.reductions_in_fp32:\n      normed_inputs = normed_inputs.astype(self.dtype)\n\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"gpt3.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    output = normed_inputs * (scale + 1)\n\n    if self.bias is not None:\n      bias = self.bias.value\n      bias = jnp.asarray(bias, self.dtype)\n      output += bias\n    return output",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#gpt3_layer_norm",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "def gpt3_layer_norm(\n    *,\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.zeros,\n    use_bias: bool = True,\n    reductions_in_fp32: bool = False,\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Initializes the gpt3_layer_norm module.\n\n  Args:\n    num_features: the number of features.\n    epsilon: the epsilon for the layer norm.\n    dtype: the dtype of the computation (default: float32).\n    weight_dtype: the dtype of the weights (default: float32).\n    kernel_axes: logical axes for partitioning the kernel.\n    scale_init: initializer for the scale.\n    use_bias: whether to add bias in linear transformation.\n    reductions_in_fp32: whether to do reductions in fp32.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n\n  module = nnx_wrappers.to_linen(\n      Gpt3LayerNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      use_bias=use_bias,\n      reductions_in_fp32=reductions_in_fp32,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=initializers.variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3MultiHeadAttention",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3MultiHeadAttention(nn.Module):\n  \"\"\"Multi-head attention in gpt3.\n\n  Attributes:\n    num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n      should be divisible by the number of heads.\n    head_dim: dimension of each head.\n    max_target_length: maximum length of output\n    max_prefill_predict_length: size of the maximum prefill\n    mesh: device mesh\n    dtype: the dtype of the computation.\n    dropout_rate: dropout rate\n    kernel_init: initializer for the kernel of the Dense layers.\n    float32_qk_product: bool, if True then compute logits via float32 qk_product to avoid\n      numerical issues with bfloat16.\n    float32_logits: bool, if True then cast logits to float32 before softmax to avoid\n      numerical issues with bfloat16.\n    fused_qkv: whether to fuse query, key and value into one projection.\n    quant: Quant, stores quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n  \"\"\"\n\n  config: Config\n  num_heads: int\n  head_dim: int\n  max_target_length: int\n  max_prefill_predict_length: int\n  mesh: Mesh\n  attention_kernel: str\n  dtype: DType = jnp.float32\n  weight_dtype: DType = jnp.float32\n  dropout_rate: float = 0.0\n  kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\")\n  float32_qk_product: bool = False  # computes logits in float32 for stability.\n  float32_logits: bool = True  # cast logits in float32 for stability.\n  fused_qkv: bool = True\n  quant: None | Quant = None\n  kv_quant: None | KVQuant = None\n  use_bias: bool = True\n\n  input_axis_names: AxisNames = (BATCH, LENGTH, EMBED)\n  query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  key_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  value_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n  out_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV)\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(3, self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def projection(self, inputs: Array, proj_name: str) -> Array:\n    \"\"\"individual projection for one of q, k and v.\"\"\"\n    proj = dense_general(\n        inputs_shape=inputs.shape,\n        out_features_shape=(self.num_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=proj_name,\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(inputs)\n    return proj\n\n  def out_projection(self, output_dim: int, out: Array) -> Array:\n    \"\"\"output projection\"\"\"\n    out_proj = dense_general(\n        inputs_shape=out.shape,\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"heads\", \"kv\", \"embed\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        name=\"out\",\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n    )(out)\n    return out_proj\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs_q: Array,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n  ):\n    inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n    if self.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.projection(inputs_q, proj_name=\"query\")\n      key = self.projection(inputs_q, proj_name=\"key\")\n      value = self.projection(inputs_q, proj_name=\"value\")\n\n    depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n    query /= depth_scaling\n\n    # annotate with sharding constraint.\n    query = nn.with_logical_constraint(query, self.query_axis_names)\n    query = checkpoint_name(query, \"query_proj\")\n    key = nn.with_logical_constraint(key, self.key_axis_names)\n    key = checkpoint_name(key, \"key_proj\")\n    value = nn.with_logical_constraint(value, self.value_axis_names)\n    value = checkpoint_name(value, \"value_proj\")\n\n    attention_op = attention_op_as_linen(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_heads,\n        num_kv_heads=self.num_heads,\n        dtype=self.dtype,\n    )\n\n    out = attention_op(query, key, value, decoder_segment_ids, model_mode)\n\n    out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    # apply output projection,  output dim is set to the input dim.\n    out = self.out_projection(inputs_q.shape[-1], out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/gpt3.py#Gpt3DecoderLayer",
        "file_path": "src/MaxText/layers/gpt3.py",
        "code_block": "class Gpt3DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx = gpt3_layer_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n        reductions_in_fp32=False,\n        use_bias=True,\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    assert (\n        cfg.num_query_heads == cfg.num_kv_heads\n    ), f\"{cfg.num_query_heads=} should be the same as {cfg.num_kv_heads=} in gpt3\"\n    attention_layer = Gpt3MultiHeadAttention(\n        config=cfg,\n        num_heads=cfg.num_query_heads,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        mesh=mesh,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        fused_qkv=cfg.fused_qkv,\n        use_bias=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n    )\n\n    attention_lnx = attention_layer(\n        lnx, decoder_segment_ids=decoder_segment_ids, model_mode=model_mode, deterministic=deterministic\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attention_lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        use_bias=True,\n        use_pre_norm=True,\n        config=cfg,\n        quant=self.quant,\n    )(attention_lnx, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = attention_lnx + mlp_lnx\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#L2Norm",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class L2Norm(nnx.Module):\n  \"\"\"\n  Implementation of L2Norm in JAX.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n\n  eps: float = 1e-6\n  rngs: nnx.Rngs = None  # Not used in L2Norm but passed in by nnx.bridge.to_linen\n\n  def __call__(self, x):\n    return x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#l2_norm_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def l2_norm_as_linen(self, eps: float = 1e-6):\n  \"\"\"\n  Initializes the L2Norm module and returns it as a Linen module.\n\n  Args:\n    eps: float, epsilon used for numerical stability (default value should be ok for most cases).\n  \"\"\"\n  return nnx_wrappers.to_linen(L2Norm, eps=eps, metadata_fn=variable_to_logically_partitioned)",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#attention_as_linen",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "def attention_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an Attention as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Attention` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Attention,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/attentions.py#Attention",
        "file_path": "src/MaxText/layers/attentions.py",
        "code_block": "class Attention(nnx.Module):\n  \"\"\"Attention Module.\n\n  This module implements multi-headed attention as described in the\n  original Transformer paper. It projects the inputs into query, key, and\n  value vectors, applies the attention mechanism, and projects the results to\n  an output vector.\n\n  Attributes:\n    config: The model configuration.\n    num_query_heads: Number of query attention heads.\n    num_kv_heads: Number of key-value attention heads.\n    head_dim: The dimension of each attention head.\n    max_target_length: Maximum sequence length.\n    mesh: The device mesh.\n    attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n    inputs_q_shape: Query inputs shape for initialization, required by NNX.\n    inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n    dtype: The data type for computation.\n    weight_dtype: The data type for weights.\n    max_prefill_predict_length: Maximum length for prefill.\n    dropout_rate: The dropout rate.\n    kernel_init: Initializer for the kernel of the dense layers.\n    float32_qk_product: If True, compute query-key product in float32.\n    float32_logits: If True, cast logits to float32 before softmax.\n    quant: Quantization configuration.\n    kv_quant: KV cache quantization configuration.\n    attention_type: The type of attention (e.g., 'global', 'local_sliding').\n    attn_logits_soft_cap: Soft cap for attention logits.\n    ... and other configuration parameters.\n  \"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      base_kv_cache: bool = True,\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the Attention module.\n\n    Attributes:\n      config: The model configuration.\n      num_query_heads: Number of query attention heads.\n      num_kv_heads: Number of key-value attention heads.\n      head_dim: The dimension of each attention head.\n      max_target_length: Maximum sequence length.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use (e.g., 'dot_product', 'flash').\n      inputs_q_shape: Query inputs shape for initialization, required by NNX.\n      inputs_kv_shape: Key/value inputs shape for initialization, required by NNX.\n      dtype: The data type for computation.\n      weight_dtype: The data type for weights.\n      max_prefill_predict_length: Maximum length for prefill.\n      dropout_rate: The dropout rate.\n      kernel_init: Initializer for the kernel of the dense layers.\n      float32_qk_product: If True, compute query-key product in float32.\n      float32_logits: If True, cast logits to float32 before softmax.\n      quant: Quantization configuration.\n      kv_quant: KV cache quantization configuration.\n      attention_type: The type of attention (e.g., 'global', 'local_sliding').\n      attn_logits_soft_cap: Soft cap for attention logits.\n      sliding_window_size: The size of the sliding window for local attention.\n      use_ragged_attention: Whether to use ragged attention for decoding.\n      ragged_block_size: The block size for ragged attention.\n      use_qk_norm: Whether to apply normalization to query and key.\n      query_pre_attn_scalar: Scalar to apply to query before attention.\n      use_bias_in_projections: Whether to use bias in Q, K, V, and output projections.\n      temperature_tuning: Whether to use temperature tuning for attention.\n      temperature_tuning_scale: The scale for temperature tuning.\n      temperature_tuning_floor_scale: The floor scale for temperature tuning.\n      ... other configuration parameters.\n      is_nope_layer: Whether this is a \"NoPE\" (No Position-Embedding) layer.\n      is_vision: Whether this is a vision attention layer.\n      model_mode: The model's operational mode (e.g., 'train', 'prefill').\n      base_kv_cache: Whether to use base (non-MLA) kv cache, if KVCache is used\n      rngs: RNG state for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n\n    self.config = config\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.head_dim = head_dim\n    self.max_target_length = max_target_length\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.dropout_rate = dropout_rate\n    self.kernel_init = kernel_init\n    self.float32_qk_product = float32_qk_product\n    self.float32_logits = float32_logits\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n    self.use_qk_norm = use_qk_norm\n    self.query_pre_attn_scalar = query_pre_attn_scalar\n    self.use_bias_in_projections = use_bias_in_projections\n    self.temperature_tuning = temperature_tuning\n    self.temperature_tuning_scale = temperature_tuning_scale\n    self.temperature_tuning_floor_scale = temperature_tuning_floor_scale\n    self.prefill_query_axis_names = prefill_query_axis_names\n    self.prefill_key_axis_names = prefill_key_axis_names\n    self.prefill_value_axis_names = prefill_value_axis_names\n    self.query_axis_names = query_axis_names\n    self.key_axis_names = key_axis_names\n    self.value_axis_names = value_axis_names\n    self.ep_query_axis_names = ep_query_axis_names\n    self.ep_key_axis_names = ep_key_axis_names\n    self.ep_value_axis_names = ep_value_axis_names\n    self.input_axis_names = input_axis_names\n    self.ep_input_axis_names = ep_input_axis_names\n    self.out_axis_names = out_axis_names\n    self.ep_out_axis_names = ep_out_axis_names\n    self.prefill_input_axis_names = prefill_input_axis_names\n    self.decode_input_axis_names = decode_input_axis_names\n    self.prefill_out_axis_names = prefill_out_axis_names\n    self.decode_out_axis_names = decode_out_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.compute_axis_order = compute_axis_order\n    self.reshape_q = reshape_q\n    self.is_nope_layer = is_nope_layer\n    self.is_vision = is_vision\n    self.model_mode = model_mode\n    self.rngs = rngs\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.KVCache_0 = (\n        self.init_kv_caches(inputs_kv_shape=inputs_kv_shape)\n        if self.model_mode != MODEL_MODE_TRAIN and base_kv_cache\n        else None\n    )\n\n    self.rotary_embedding = self.init_rotary_embedding()\n\n    self.attention_op = AttentionOp(\n        config=self.config,\n        mesh=self.mesh,\n        attention_kernel=self.attention_kernel,\n        max_target_length=self.max_target_length,\n        max_prefill_predict_length=self.max_prefill_predict_length,\n        float32_qk_product=self.float32_qk_product,\n        float32_logits=self.float32_logits,\n        quant=self.quant,\n        kv_quant=self.kv_quant,\n        num_query_heads=self.num_query_heads,\n        num_kv_heads=self.num_kv_heads,\n        dropout_rate=self.dropout_rate,\n        dtype=self.dtype,\n        compute_axis_order=self.compute_axis_order,\n        reshape_q=self.reshape_q,\n        attention_type=self.attention_type,\n        attn_logits_soft_cap=self.attn_logits_soft_cap,\n        sliding_window_size=self.sliding_window_size,\n        chunk_attn_window_size=self.config.chunk_attn_window_size,\n        use_ragged_attention=self.use_ragged_attention,\n        ragged_block_size=self.ragged_block_size,\n        rngs=self.rngs,\n    )\n    # When paged attention is enabled, paged attention op is used for all model modes except TRAIN,\n    # which uses default attention op.\n    if self.config.attention == \"paged\":\n      self.paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=self.head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n    self._init_projections(inputs_q_shape, inputs_kv_shape)\n\n    if self.config.attention_sink:\n      self.sinks = nnx.Param(\n          default_bias_init(self.rngs.params(), (self.config.num_query_heads,), self.weight_dtype),\n          sharding=(None,),\n      )\n    else:\n      self.sinks = None\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      self.query_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.key_norm = RMSNorm(\n          num_features=self.head_dim,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n    else:\n      self.query_norm = None\n      self.key_norm = None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the query, key, value, and output projections.\"\"\"\n    if self.config.fused_qkv:\n      self.qkv_proj = self.init_qkv_w(inputs_shape=inputs_q_shape)\n    else:\n      self.query = self.init_query_w(inputs_q_shape=inputs_q_shape)\n      self.key = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n      self.value = self.init_kv_w(inputs_kv_shape=inputs_kv_shape)\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n  def init_query_w(self, inputs_q_shape: Tuple) -> nnx.Module:\n    \"\"\"Query projection initialization.\"\"\"\n\n    # NOTE: T5 does not explicitly rescale the attention logits by\n    #       1/sqrt(depth_kq)!  This is folded into the initializers of the\n    #       linear transformations, which is equivalent under Adafactor.\n    # We disable depth_scaling when using qk_norm or a query_pre_attn_scalar\n    # to avoid applying scaling twice.\n    if self.config.use_qk_norm or (self.query_pre_attn_scalar is not None and self.query_pre_attn_scalar != 1.0):\n      depth_scaling = 1.0\n    else:\n      depth_scaling = jnp.sqrt(self.head_dim).astype(self.dtype)\n\n    def query_init(*args):\n      # pylint: disable=no-value-for-parameter\n      return self.kernel_init(*args) / depth_scaling\n\n    kernel_axes = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"embed\", \"q_heads\", \"kv\")\n    )\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_q_shape),\n        out_features_shape=(self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=query_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def query_projection(self, inputs_q: Array) -> Array:\n    \"\"\"Query projection.\"\"\"\n\n    return self.query(inputs_q)\n\n  def init_kv_w(self, inputs_kv_shape: Tuple) -> nnx.Module:\n    \"\"\"Initializes the key or value projection.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A DenseGeneral module that performs the key or value projection.\n    \"\"\"\n    if self.num_kv_heads == -1:\n      raise ValueError(\"num_kv_heads is not defined.\")\n\n    if self.num_query_heads % self.num_kv_heads != 0:\n      raise ValueError(\"Invalid num_kv_heads for GQA.\")\n\n    kernel_axes = (\n        (None, None, None)\n        if self.config.ici_context_autoregressive_parallelism > 1\n        else (\"embed\", \"kv_heads\", \"kv_head_dim\")\n    )\n\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_kv_shape),\n        out_features_shape=(self.num_kv_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=kernel_axes,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def kv_projection(self, inputs_kv: Array, proj_name: str) -> nnx.Module:\n    \"\"\"Applies the key or value projection.\n\n    Args:\n      inputs_kv: The input tensor to project.\n      proj_name: The name of the projection (\"key\" or \"value\").\n\n    Returns:\n      The projected key or value tensor.\n\n    Raises:\n      ValueError: If `proj_name` is not one of the supported values\n        (\"key\", \"value\").\n\n    \"\"\"\n    if proj_name == \"key\":\n      return self.key(inputs_kv)\n    elif proj_name == \"value\":\n      return self.value(inputs_kv)\n    else:\n      raise ValueError(f\"proj_name must be 'key' or 'value', but got {proj_name}\")\n\n  def init_qkv_w(self, inputs_shape: Tuple) -> nnx.Module:\n    return DenseGeneral(\n        in_features_shape=self.convert_dense_general_inputs_shape(inputs_shape),\n        out_features_shape=(3, self.num_query_heads, self.head_dim),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"qkv\", \"heads\", \"kv\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def qkv_projection(self, inputs: Array, proj_name: str):\n    \"\"\"Fused QKV projection\"\"\"\n\n    qkv_proj = self.qkv_proj(inputs)\n    qkv_proj = checkpoint_name(qkv_proj, \"qkv_proj\")\n    query, key, value = qkv_proj[:, :, 0, ...], qkv_proj[:, :, 1, ...], qkv_proj[:, :, 2, ...]\n    return query, key, value\n\n  def init_out_w(self, output_dim: int) -> nnx.Module:\n    \"\"\"out projection\"\"\"\n    out_kernel_axis = (\n        (None, None, None) if self.config.ici_context_autoregressive_parallelism > 1 else (\"heads\", \"kv\", \"embed\")\n    )\n    return DenseGeneral(\n        in_features_shape=(self.num_query_heads, self.head_dim),\n        out_features_shape=output_dim,\n        axis=(-2, -1),\n        kernel_init=self.kernel_init,\n        kernel_axes=out_kernel_axis,  # trade speed with memory\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        use_bias=self.use_bias_in_projections,\n        rngs=self.rngs,\n    )\n\n  def out_projection(self, out: Array) -> Array:\n    \"\"\"out projection\"\"\"\n\n    return self.out(out)\n\n  def convert_dense_general_inputs_shape(\n      self,\n      inputs_shape: tuple[int, ...] | None = None,\n      axis: Union[Iterable[int], int] = -1,\n  ) -> Union[Iterable[int], int]:\n    axis = canonicalize_tuple(axis)\n    return tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n\n  def init_rotary_embedding(self):\n    \"\"\"Initializes the rotary embeddings, handling different model types.\n\n    Returns:\n      The rotary embedding module that will be used in the model.\n    \"\"\"\n    if self.config.attention_type == AttentionType.MLA.value:\n      # For MLA attention RoPE is applied to only `self.qk_rope_head_dim` portion the heads.\n      rope_embedding_dims = self.qk_rope_head_dim\n    else:\n      rope_embedding_dims = self.head_dim\n\n    rope_type = self.config.rope_type.lower()\n    rope_use_scale = self.config.rope_use_scale\n    if self.is_vision:\n      rotary_embedding = LlamaVisionRotaryEmbedding(\n          image_size=self.config.image_size_for_vit,\n          patch_size=self.config.patch_size_for_vit,\n          hidden_size=self.config.hidden_size_for_vit,\n          num_attention_heads=self.config.num_attention_heads_for_vit,\n          rope_theta=self.config.rope_theta_for_vit,\n          rngs=self.rngs,\n      )\n    elif self.config.model_name.startswith(\"llama3.1\") or rope_type.startswith(\"llama3.1\"):\n      rotary_embedding = LLaMARotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=self.config.rope_max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          use_scale=rope_use_scale,\n          rngs=self.rngs,\n      )\n    elif rope_type.startswith(\"yarn\"):\n      rotary_embedding = YarnRotaryEmbedding(\n          max_position_embeddings=self.config.max_position_embeddings,\n          original_max_position_embeddings=self.config.original_max_position_embeddings,\n          beta_fast=self.config.beta_fast,\n          beta_slow=self.config.beta_slow,\n          rope_theta=self.config.rope_max_timescale,\n          rope_factor=self.config.rope_factor,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          interleave=self.config.rope_interleave,\n          truncate=self.config.rope_truncate,\n          attention_scaling=self.config.rope_attention_scaling,\n          rngs=self.rngs,\n      )\n    else:\n      max_timescale = self.config.rope_max_timescale\n      # For local attention use local_rope_max_timescale if it's is positive\n      if self.attention_type == AttentionType.LOCAL_SLIDING and self.config.local_rope_max_timescale > 0:\n        max_timescale = self.config.local_rope_max_timescale\n\n      rope_linear_scaling_factor = self.config.rope_linear_scaling_factor\n      # In gemma3, linear scaling factor does not apply to local sliding layers.\n      if self.config.model_name.startswith(\"gemma3\") and self.attention_type == AttentionType.LOCAL_SLIDING:\n        rope_linear_scaling_factor = 1.0\n\n      rotary_embedding = RotaryEmbedding(\n          min_timescale=self.config.rope_min_timescale,\n          max_timescale=max_timescale,\n          embedding_dims=rope_embedding_dims,\n          fprop_dtype=self.dtype,\n          rope_linear_scaling_factor=rope_linear_scaling_factor,\n          rngs=self.rngs,\n      )\n    return rotary_embedding\n\n  def apply_rotary_embedding(self, inputs: Array, inputs_positions: Optional[Array | None] = None):\n    \"\"\"Applies rotary embeddings, handling different model types.\n\n    Args:\n      inputs: The input tensor to apply rotary embeddings to.\n      inputs_positions: The positions of the inputs.\n      name: A name for the embedding layer.\n\n    Returns:\n      The input tensor with rotary embeddings applied.\n    \"\"\"\n    return self.rotary_embedding(inputs, inputs_positions)\n\n  def init_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes KVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      A KVCache module instance.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in KVCache.\n    # However, KVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # KVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.KVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_heads=self.num_kv_heads,\n        value_heads=self.num_kv_heads,\n        key_head_size=self.head_dim,\n        value_head_size=self.head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        model_mode=self.model_mode,\n        rngs=self.rngs,\n    )\n\n  def update_kv_caches(self, key, value, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"Updates the KV caches for prefill and autoregressive modes.\n\n    This method uses a kvcache module to update and retrieve the key-value\n    caches based on the current operational mode.\n\n    Args:\n      key: The key tensor for the current attention computation.\n      value: The value tensor for the current attention computation.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, or None.\n      - The autoregressive key-value cache, or None.\n    \"\"\"\n    prefill_kv_cache, ar_kv_cache = self.KVCache_0(\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Any = None,\n  ):\n    \"\"\"Applies Attention on the input data.\n\n    Projects the inputs into multi-headed query, key, and value vectors,\n    applies dot-product attention, and project the results to an output vector.\n\n    This method handles three modes:\n    1.  **Training**: The KV cache is ignored.\n    2.  **Prefill**: The KV cache is filled with the key-value pairs from the input sequence.\n    3.  **Autoregressive Decoding**: The KV cache is used to provide context from previous steps.\n\n    In the cache initialization call, `inputs_q` has a shape [batch, length,\n    q_features] and `inputs_kv`: [batch, length, kv_features]. During the\n    incremental decoding stage, query, key and value all have the shape [batch,\n    1, qkv_features] corresponding to a single step.\n\n    Args:\n      inputs_q: Input queries of shape `[batch, q_length, q_features]`.\n      inputs_kv: Key/values of shape `[batch, kv_length, kv_features]`.\n      inputs_positions: Input positions for rotary embeddings.\n      decoder_segment_ids: Segment IDs for masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      deterministic: If True, disables dropout.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      output of shape `[batch, length, q_features]`.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.decode_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.decode_input_axis_names)\n\n    # apply projection.\n    if self.config.fused_qkv:\n      query, key, value = self.qkv_projection(inputs_q, proj_name=\"qkv_proj\")\n    else:\n      query = self.query_projection(inputs_q)\n      key = self.kv_projection(inputs_kv, proj_name=\"key\")\n      value = self.kv_projection(inputs_kv, proj_name=\"value\")\n\n    is_llama4_decoder_block = self.config.decoder_block == DecoderBlockType.LLAMA4\n    # NOTE: llama 4 does L2 normalization after RoPE\n    if self.use_qk_norm and not is_llama4_decoder_block:\n      query = self.query_norm(query)\n      key = self.key_norm(key)\n\n    # NOTE: is_nope_layer should be used in attention mask and also used in attention tuning\n    use_rope = not self.is_nope_layer\n    use_qk_norm = self.use_qk_norm and use_rope\n\n    if use_rope:\n      query = self.apply_rotary_embedding(query, inputs_positions=inputs_positions)\n      key = self.apply_rotary_embedding(key, inputs_positions=inputs_positions)\n\n    if use_qk_norm and is_llama4_decoder_block:\n      l2_norm = L2Norm(eps=self.config.normalization_layer_epsilon)\n      query = l2_norm(query)\n      key = l2_norm(key)\n\n    # apply query_pre_attn_scalar if it's present.\n    if self.query_pre_attn_scalar and self.query_pre_attn_scalar != 1.0:\n      query = query * self.query_pre_attn_scalar\n\n    if self.temperature_tuning and not use_rope:\n      attn_scales = (\n          jnp.log(jnp.floor((inputs_positions.astype(self.dtype) + 1.0) / self.temperature_tuning_floor_scale) + 1.0)\n          * self.temperature_tuning_scale\n          + 1.0\n      )\n      query = (query * attn_scales[:, :, jnp.newaxis, jnp.newaxis]).astype(self.dtype)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      query = nn.with_logical_constraint(query, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      key = nn.with_logical_constraint(key, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n      value = nn.with_logical_constraint(value, (DECODE_BATCH, DECODE_LENGTH, KV_HEAD, D_KV))\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    assert not self.config.quantize_kvcache or self.kv_quant\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      cached_values = [None, None]\n      if model_mode != MODEL_MODE_TRAIN:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      out = self.attention_op(\n          query, key, value, decoder_segment_ids, model_mode, cached_values, previous_chunk, bidirectional_mask, self.sinks\n      )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      out = nn.with_logical_constraint(out, self.prefill_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.decode_out_axis_names)\n    out = self.out_projection(out)\n    out = checkpoint_name(out, \"out_proj\")\n    return out",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinenPure",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinenPure(nn.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n  # pylint: enable=attribute-defined-outside-init\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    return nn.Module.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    module = self.clone(model_mode=model_mode)\n    return nn.Module.apply(module, *args, **kwargs)\n\n  def setup(self):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n    self.shared_embedding = embed_as_linen(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        name=\"token_embedder\",\n        config=cfg,\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n    self.decoder = Decoder(\n        config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode\n    )\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      self.mtp_block = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      enable_dropout=True,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method=None,\n  ):\n    \"\"\"Applies Transformer decoder-branch on encoded-input and target.\n\n    Args:\n      true_length: (Optional) Prompt length before padding\n      slot: (Optional) An integer representing the decode batch index selected\n        for this request.\n    \"\"\"\n\n    if decoder_segment_ids is not None and self.model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.shared_embedding,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n    )\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    if self.is_initializing() and self.config.mtp_num_layers > 0:\n      if decoder_target_tokens is None:\n        dummy_shape = decoder_input_tokens.shape\n        decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n        decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.shared_embedding,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n      )\n\n    return logits",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/models.py#transformer_as_linen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "def transformer_as_linen(\n    config: Config,\n    mesh: Mesh,\n    quant: Quant,\n    model_mode: str = MODEL_MODE_TRAIN,\n    *,\n    name: str | None = None,\n) -> nnx_wrappers.ToLinen | TransformerLinenPure:\n  if config.enable_nnx:\n    return TransformerLinen(\n        Transformer,\n        args=(),\n        kwargs=nn.FrozenDict({\n          \"mesh\": mesh,\n        \"config\": config,\n        \"quant\": quant,\n        \"model_mode\": model_mode,\n      }),\n      metadata_fn=initializers.variable_to_logically_partitioned,\n      name=name,\n  )\n  else:\n    return TransformerLinenPure(\n      config, mesh, quant, model_mode=model_mode, name=name\n    )",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/models.py#TransformerLinen",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class TransformerLinen(nnx_wrappers.ToLinen):\n  \"\"\"Transformer model as a linen module.\"\"\"\n\n  def init(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Initializes the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    return nnx_wrappers.ToLinen.init(module, *args, **kwargs)\n\n  def apply(self, *args, model_mode: str = MODEL_MODE_TRAIN, **kwargs):\n    \"\"\"Applies the model.\"\"\"\n    model_kwargs = self.kwargs.copy({\"model_mode\": model_mode})  # type: ignore[wrong-arg-types]\n    module = self.clone(kwargs=model_kwargs)\n    return nnx_wrappers.ToLinen.apply(module, *args, **kwargs)",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/models.py#Transformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class Transformer(nnx.Module):\n  \"\"\"An autoregressive transformer model.\"\"\"\n\n  # Make new attributes required, so that all Transformer dependencies (train, decode,\n  # compile, etc) will error instead of silently use defaults.\n  # pylint: disable=attribute-defined-outside-init\n  def __init__(self, config: Config, mesh: Mesh, quant: Quant, *, model_mode: str = MODEL_MODE_TRAIN, rngs: nnx.Rngs):\n    \"\"\"Initialize shared_embedding & decoder layers.\"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.quant = quant\n    self.model_mode = model_mode\n\n    cfg = self.config\n    mesh = self.mesh\n    self.token_embedder = Embed(\n        num_embeddings=cfg.vocab_size,\n        num_features=cfg.emb_dim,\n        dtype=cfg.dtype,\n        attend_dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n        embedding_init=nn.initializers.normal(stddev=1.0),\n        config=cfg,\n        rngs=rngs\n    )\n    self.vision_encoder = VisionEncoder(config=cfg, mesh=mesh) if cfg.use_multimodal else None\n\n    decoder_linen = Decoder(config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode)\n    self.decoder = nnx_wrappers.ToNNX(decoder_linen, rngs=rngs)\n\n    if self.model_mode == MODEL_MODE_PREFILL:\n      seq_len = cfg.max_prefill_predict_length\n    elif self.model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      seq_len = 1\n    else:\n      seq_len = cfg.max_target_length\n\n    batch_size = cfg.micro_batch_size_to_train_on\n    dummy_decoder_input_tokens = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n    dummy_decoder_positions = jnp.ones((batch_size, seq_len), dtype=jnp.int32)\n\n    self.decoder.lazy_init(\n      shared_embedding=self.token_embedder,\n      decoder_input_tokens=dummy_decoder_input_tokens,\n      decoder_positions=dummy_decoder_positions,\n    )\n\n    # If MTP is enabled via config, set up the MTP block.\n    if self.config.mtp_num_layers > 0:\n      # Get the list of layer blueprints for the current model.\n      layer_types = self.decoder.get_decoder_layers()\n      # For MTP, we use the DecoderLayer blueprint to ensure architectural consistency.\n      # By convention, this is the last layer in the list.\n      mtp_layer = layer_types[-1]\n      mtp_block_linen = MultiTokenPredictionBlock(\n          config=self.config, mesh=self.mesh, name=\"mtp_block\", transformer_layer_module=mtp_layer, decoder=self.decoder\n      )\n      self.mtp_block = nnx_wrappers.ToNNX(mtp_block_linen, rngs=rngs)\n\n      self.mtp_block.lazy_init(\n        shared_embedding=self.token_embedder,\n        main_hidden_state=jnp.ones((1, 1, self.config.emb_dim), dtype=self.config.dtype),\n        input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n        target_ids=jnp.ones((1, 1), dtype=jnp.int32),\n        target_mask=jnp.ones((1, 1), dtype=jnp.int32),\n        position_ids=jnp.ones((1, 1), dtype=jnp.int32),\n        decoder_segment_ids=jnp.ones((1, 1), dtype=jnp.int32),\n        deterministic=True,\n      )\n\n  def no_op(self, *args, **kwargs):\n    \"\"\"A no-op method to allow the model to be used in a lazy context.\"\"\"\n    return\n\n  def init_cache(self, cache_size: int, batch_size: int, dtype=jnp.float32):\n    return True\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      cache=None,\n      encoder_images: jax.Array | None = None,\n      enable_dropout=True,\n      previous_chunk=None,\n      true_length: int | None = None,\n      slot: int | None = None,\n      page_state: page_manager.PageState | None = None,\n      decoder_target_tokens: jax.Array | None = None,\n      decoder_target_mask: jax.Array | None = None,\n  ):\n    \"\"\"Applies Transformer decoder-branch on encoded-input and target.\n\n    Args:\n      true_length: (Optional) Prompt length before padding\n      slot: (Optional) An integer representing the decode batch index selected\n        for this request.\n    \"\"\"\n\n    if decoder_segment_ids is not None and self.model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      raise ValueError(\n          f\"During autoregressive decoding we assume the tokens are in the active sequence\"\n          f\" which is always {DECODING_ACTIVE_SEQUENCE_INDICATOR}.\"\n      )\n\n    bidirectional_mask = None\n    image_embeddings = None\n    if self.config.use_multimodal and encoder_images is not None:\n      image_embeddings = self.vision_encoder(input_images=encoder_images, deterministic=not enable_dropout)\n\n      if self.config.decoder_block == DecoderBlockType.GEMMA3:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.GEMMA_TOKEN_PLACEHOLDER\n      elif self.config.decoder_block == DecoderBlockType.LLAMA4:\n        bidirectional_mask = decoder_input_tokens == multimodal_utils.LLAMA4_PATCH_TOKEN\n\n    logits, hidden_state = self.decoder(\n        shared_embedding=self.token_embedder,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=not enable_dropout,\n        previous_chunk=previous_chunk,\n        slot=slot,\n        page_state=page_state,\n        bidirectional_mask=bidirectional_mask,\n        image_embeddings=image_embeddings,\n    )\n\n    # If we are initializing the model AND MTP is enabled, we must create\n    # dummy target tensors. This allows Flax to trace the MTPBlock and create\n    # all its necessary parameters, without requiring the main training pipeline\n    # to be aware of this initialization detail.\n    # if self.is_initializing() and self.config.mtp_num_layers > 0:\n    #   if decoder_target_tokens is None:\n    #     dummy_shape = decoder_input_tokens.shape\n    #     decoder_target_tokens = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_target_mask = jnp.ones(dummy_shape, dtype=jnp.int32)\n    #     decoder_segment_ids = jnp.ones(dummy_shape, dtype=jnp.int32)\n\n    # The Multi-Token Prediction (MTP) block functions as a \"side-car\" to the main\n    # model, active only during training. It computes an auxiliary loss based on\n    # predicting multiple future tokens, as described in the DeepSeek-V3 paper.\n    # To ensure architectural consistency, it uses two key components from the parent Transformer:\n    #   1. The same `DecoderLayer` blueprint for its internal transformer blocks.\n    #   2. The `shared_embedding` for both embedding future tokens and for its final\n    #      logit projection.\n    # Its only effect is to \"sow\" these losses; it does not alter the primary logits output.\n    if self.config.mtp_num_layers > 0:\n      self.mtp_block(\n          shared_embedding=self.token_embedder,\n          main_hidden_state=hidden_state,\n          input_ids=decoder_input_tokens,\n          target_ids=decoder_target_tokens,\n          target_mask=decoder_target_mask,\n          position_ids=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          deterministic=not enable_dropout,\n      )\n\n    return logits",
        "analysis": null
    },
    {
        "block_name": "src/MaxText/layers/models.py#ZeroOneTransformer",
        "file_path": "src/MaxText/layers/models.py",
        "code_block": "class ZeroOneTransformer(nn.Module):\n  \"\"\"\n  A wrapper for the base Transformer model designed to implement the Zero-1\n  FSDP optimization.\n\n  The goal of this optimization is to reduce communication overhead. In the standard\n  FSDP implementation, an all-gather operation on the model weights is performed twice\n  for each gradient accumulation microbatch (once for the forward pass, once for the backward pass).\n  This class changes that behavior. When enabled, it performs the all-gather operation\n  only *once* per full gradient accumulation step. It gathers the full weights into\n  memory, runs all the microbatch forward and backward passes, and then releases the\n  full weights. This trades higher peak memory usage for significantly reduced\n  network communication, which can improve training speed if sufficient memory is\n  available.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  # Possible model_mode values can be found in MaxText.common_types.\n  # We generally use MaxText.common_types.MODEL_MODE_TRAIN or\n  # MaxText.common_types.MODEL_MODE_PREFILL for initializations here.\n  # TODO: Make model_mode required after confirming no users are affected.\n  model_mode: str = MODEL_MODE_TRAIN  # May be different than the model_mode passed to __call__\n\n  def setup(self):\n    self.model = transformer_as_linen(self.config, self.mesh, self.quant, self.model_mode)\n\n  def __call__(\n      self,\n      decoder_input_tokens: jnp.ndarray,\n      decoder_positions: jnp.ndarray,\n      decoder_segment_ids=None,\n      encoder_images: None | jnp.ndarray = None,\n      enable_dropout=True,\n      previous_chunk=None,\n      true_length: None | int = None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      partition_spec=None,\n      decoder_target_tokens: None | jnp.ndarray = None,\n      decoder_target_mask: None | jnp.ndarray = None,\n      nnx_method: str | None = None,\n  ):\n    if self.is_initializing():\n      return self.model(\n          decoder_input_tokens=decoder_input_tokens,\n          decoder_positions=decoder_positions,\n          decoder_segment_ids=decoder_segment_ids,\n          encoder_images=encoder_images,\n          enable_dropout=enable_dropout,\n          previous_chunk=previous_chunk,\n          true_length=true_length,\n          slot=slot,\n          page_state=page_state,\n      )\n    all_model_weights = all_gather_over_fsdp(\n        self.model.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n    )\n\n    return self.model.apply(\n        all_model_weights,\n        decoder_input_tokens=decoder_input_tokens,\n        decoder_positions=decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        encoder_images=encoder_images,\n        enable_dropout=enable_dropout,\n        model_mode=self.model_mode,\n        previous_chunk=previous_chunk,\n        true_length=true_length,\n        slot=slot,\n        page_state=page_state,\n        mutable=False,\n        decoder_target_tokens=decoder_target_tokens,\n        decoder_target_mask=decoder_target_mask,\n        nnx_method=nnx_method,\n    )",
        "analysis": {
            "module_type": "zero_one_transformer",
            "purpose": "A wrapper for a base Transformer model that implements the Zero-1 FSDP optimization to reduce communication overhead by gathering all model weights once per gradient accumulation step.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes an underlying transformer model via `transformer_as_linen` in the `setup` method.",
                "During the forward pass (`__call__`), it checks if the model is being initialized.",
                "If not initializing, it performs an `all_gather_over_fsdp` operation to collect the full model weights from all devices.",
                "It then executes the forward pass of the underlying model using `self.model.apply` with the gathered weights."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext_utils.all_gather_over_fsdp",
                "transformer_as_linen",
                "Config",
                "Mesh",
                "Quant"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters and settings.",
                "mesh": "A JAX Mesh object defining the device topology for distributed training.",
                "quant": "A Quant object for quantization configuration.",
                "model_mode": "A string indicating the model's operational mode (e.g., 'train', 'prefill')."
            },
            "notes": [
                "This class trades higher peak memory usage for reduced network communication, which can improve training speed if sufficient memory is available.",
                "The core optimization logic is conditional and does not run during model initialization."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the underlying transformer model.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `transformer_as_linen` with the provided config, mesh, quant, and model_mode.",
                        "Assigns the created model instance to `self.model`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "transformer_as_linen"
                    ],
                    "notes": [
                        "This is a standard Flax method that is called automatically during model instantiation."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the transformer, applying the Zero-1 FSDP optimization.",
                    "input": {
                        "shape": "decoder_input_tokens: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Checks if the model is in the initialization phase via `self.is_initializing()`.",
                        "If initializing, calls the underlying `self.model` directly.",
                        "If not initializing, calls `all_gather_over_fsdp` to gather `self.model.variables`.",
                        "Calls `self.model.apply` with the gathered weights and all other input arguments to perform the forward pass."
                    ],
                    "output": {
                        "shape": "Typically logits of shape [batch_size, sequence_length, vocab_size]."
                    },
                    "dependencies": [
                        "all_gather_over_fsdp"
                    ],
                    "notes": [
                        "This method implements the core logic of the Zero-1 optimization by wrapping the standard forward pass with a weight gathering operation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/encoders.py#VisionEncoder",
        "file_path": "src/MaxText/layers/encoders.py",
        "code_block": "class VisionEncoder(nn.Module):\n  \"\"\"Vision encoder to encode images into soft tokens.\"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    self.vision_encoder_layer = self.get_vision_encoder_layers()\n\n  def get_vision_encoder_layers(self):\n    \"\"\"Get vision encoder layers specific to the model, classes of nn.Module type.\"\"\"\n    if self.config.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\"]:\n      from MaxText.layers import gemma3  # pylint: disable=import-outside-toplevel\n\n      return [gemma3.gemma3visionencoder_as_linen, gemma3.visionembedder_as_linen]\n    elif self.config.model_name in [\"llama4-17b-16e\", \"llama4-17b-128e\"]:\n      from MaxText.layers import llama4  # pylint: disable=import-outside-toplevel\n\n      return [llama4.Llama4VisionModel, llama4.Llama4MultiModalProjector]\n    else:\n      raise ValueError(f\"No VisionEncoder implemented for {self.config.model_name} yet\")\n\n  @nn.compact\n  def __call__(self, input_images, deterministic=False):\n    cfg = self.config\n    mesh = self.mesh\n    # vision encoder output, frozen params in many cases\n    embeddings = self.vision_encoder_layer[0](config=cfg, mesh=mesh)(input_images, deterministic=deterministic)\n    if cfg.freeze_vision_encoder_params:\n      embeddings = jax.lax.stop_gradient(embeddings)\n\n    if len(self.vision_encoder_layer) > 1:\n      # vision embedder / projection layer, not frozen in most cases, trained / finetuned together with main model\n      embeddings = self.vision_encoder_layer[1](config=cfg, mesh=mesh)(embeddings)\n    return embeddings",
        "analysis": {
            "module_type": "vision_encoder",
            "purpose": "A configurable Flax module that encodes input images into a sequence of embeddings (soft tokens) using a model-specific vision encoder and an optional projection layer.",
            "input": {
                "shape": "See __call__ method.",
                "dtype": "See __call__ method."
            },
            "processing_steps": [
                "In the setup phase, `get_vision_encoder_layers` is called to select the appropriate vision model layers based on `config.model_name`.",
                "The `__call__` method processes the input images through the selected vision encoder layer.",
                "Optionally stops gradients from flowing back into the vision encoder if `config.freeze_vision_encoder_params` is true.",
                "If a second layer (projector) is defined, it processes the embeddings from the first layer.",
                "Returns the final image embeddings."
            ],
            "output": {
                "shape": "See __call__ method."
            },
            "dependencies": [
                "flax.linen.Module",
                "jax",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.gemma3.gemma3visionencoder_as_linen",
                "MaxText.layers.gemma3.visionembedder_as_linen",
                "MaxText.layers.llama4.Llama4VisionModel",
                "MaxText.layers.llama4.Llama4MultiModalProjector"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters, including `model_name` to select the correct layers and `freeze_vision_encoder_params` to control gradient flow.",
                "mesh": "The JAX sharding mesh for distributed computation."
            },
            "notes": [
                "This class acts as a factory and wrapper, dynamically selecting and composing the appropriate vision model components based on the configuration.",
                "It supports a two-stage encoding process: a main vision encoder followed by a multimodal projector."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the `vision_encoder_layer` attribute by dynamically selecting the appropriate layer classes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `self.get_vision_encoder_layers()`.",
                        "Assigns the returned list of layer classes to `self.vision_encoder_layer`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.get_vision_encoder_layers"
                    ],
                    "notes": [
                        "This is a standard Flax lifecycle method called once during module initialization."
                    ]
                },
                "get_vision_encoder_layers": {
                    "purpose": "Selects and returns the vision encoder and projector layer classes based on the `model_name` in the config.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks `self.config.model_name` against known model families ('gemma3', 'llama4').",
                        "Performs a lazy import of the required module (`gemma3` or `llama4`).",
                        "Returns a list containing the appropriate vision encoder and projector layer classes.",
                        "Raises a ValueError if the `model_name` is not supported."
                    ],
                    "output": {
                        "shape": "A list of one or two nn.Module classes."
                    },
                    "dependencies": [
                        "MaxText.layers.gemma3",
                        "MaxText.layers.llama4"
                    ],
                    "notes": [
                        "The use of local, inside-function imports prevents loading large model-specific modules unless they are actually needed."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass, encoding a batch of images into embeddings.",
                    "input": {
                        "shape": "[batch_size, height, width, channels]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Passes `input_images` through the first layer (`self.vision_encoder_layer[0]`).",
                        "If `config.freeze_vision_encoder_params` is True, applies `jax.lax.stop_gradient` to the resulting embeddings.",
                        "If a second layer exists (`self.vision_encoder_layer[1]`), passes the embeddings through it.",
                        "Returns the final embeddings."
                    ],
                    "output": {
                        "shape": "[batch_size, num_tokens, embedding_dim]"
                    },
                    "dependencies": [
                        "jax.lax.stop_gradient"
                    ],
                    "notes": [
                        "The `@nn.compact` decorator allows the submodules (the vision layers) to be instantiated inline during the first call.",
                        "The `deterministic` flag is passed to the underlying vision layers, typically to control behaviors like dropout."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#get_attention_type",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "def get_attention_type(layer_id):\n  \"\"\"Get attention type based on layer ID.\"\"\"\n  layer_id %= len(GPT_OSS_ATTENTION_PATTERN)\n  return GPT_OSS_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "functionality": "Retrieves an attention type from a predefined cyclic pattern based on a given layer ID.",
            "usage": "Call this function with an integer `layer_id`. It returns `attentions.AttentionType.LOCAL_SLIDING` for even layer IDs and `attentions.AttentionType.GLOBAL` for odd layer IDs, based on the `GPT_OSS_ATTENTION_PATTERN` constant."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssDecoderLayer",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  attention_type: AttentionType\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"GptOssAttention\",\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_bias_in_projections=cfg.attention_bias,\n        attention_type=self.attention_type,\n        sliding_window_size=cfg.sliding_window_size,\n        query_pre_attn_scalar=(cfg.head_dim**-0.5),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=jnp.float32,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    mlp_lnx, load_balance_loss = moe.get_routed_moe(\n        name=\"GptOssMlp\",\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "This code defines a single transformer decoder layer for a GPT-style model. The layer consists of a self-attention block followed by a Mixture-of-Experts (MoE) feed-forward block. It uses RMSNorm for pre-normalization and includes residual connections after both the attention and MoE blocks.",
            "usage": "This class is intended to be instantiated as part of a larger neural network model. To use it, you create an instance with a configuration object, a JAX mesh, and an attention type. Then, you call the instance with input tensors (`inputs`, `decoder_segment_ids`, `decoder_positions`) to perform the forward pass for one layer. The output is a tensor of the same shape as the input, representing the processed hidden states."
        }
    },
    {
        "block_name": "src/MaxText/layers/gpt_oss.py#GptOssScannableBlock",
        "file_path": "src/MaxText/layers/gpt_oss.py",
        "code_block": "class GptOssScannableBlock(nn.Module):\n  \"\"\"A repeatable block of GPT OSS decoder layers.\n\n    This block applies multiple decoder layers sequentially, using the attention\n    pattern defined by GPT_OSS_ATTENTION_PATTERN. It's designed to be\n    used with `nn.scan` for efficient compilation.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    num_of_layers: int, number of decoder layers in the block\n    quant: Optional[Quant], quantization config\n  \"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: Optional[Quant] = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      attention_type = get_attention_type(layer_id)\n      layer = GptOssDecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          model_mode=model_mode,\n          name=f\"layers_{layer_id}\",\n          attention_type=attention_type,\n          quant=self.quant,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gpt_oss_scannable_block",
            "purpose": "Applies a sequence of GPT OSS decoder layers, designed to be used with `nn.scan` for efficient compilation.",
            "input": {
                "shape": "N/A (See __call__ method)",
                "dtype": "N/A (See __call__ method)"
            },
            "processing_steps": [
                "N/A (See __call__ method)"
            ],
            "output": {
                "shape": "N/A (See __call__ method)"
            },
            "dependencies": [
                "flax.linen.Module",
                "GptOssDecoderLayer",
                "get_attention_type"
            ],
            "parameters": {
                "config": "MaxText model configuration object, containing parameters like `inhomogeneous_layer_cycle_interval` and `scan_layers`.",
                "mesh": "JAX device mesh for sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'decode').",
                "quant": "Optional quantization configuration."
            },
            "notes": [
                "This module is designed to be a repeatable unit within a larger model, often used with `nn.scan` to improve compilation and execution efficiency.",
                "The number of layers within this block is determined by `config.inhomogeneous_layer_cycle_interval`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through a series of `GptOssDecoderLayer` instances.",
                    "input": {
                        "shape": "{'inputs': '[batch_size, sequence_length, hidden_dim]', 'decoder_segment_ids': '[batch_size, sequence_length]', 'decoder_positions': '[batch_size, sequence_length]'}",
                        "dtype": "Depends on `config.dtype` (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply logical constraints and a checkpoint name to the input tensor.",
                        "Iterate `config.inhomogeneous_layer_cycle_interval` times.",
                        "In each iteration, determine the attention type using `get_attention_type`.",
                        "Instantiate and apply a `GptOssDecoderLayer` to the intermediate output.",
                        "If `config.scan_layers` is true, unpack the layer's tuple output.",
                        "Return the final tensor, potentially in a tuple with `None` if `config.scan_layers` is true."
                    ],
                    "output": {
                        "shape": "Same as the input `inputs` shape: `[batch_size, sequence_length, hidden_dim]`. The output is a tuple `(tensor, None)` if `config.scan_layers` is true."
                    },
                    "dependencies": [
                        "GptOssDecoderLayer",
                        "get_attention_type",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "The return format changes based on the `config.scan_layers` flag. If True, it returns a tuple `(output, None)` to be compatible with `nn.scan`; otherwise, it returns the raw output tensor."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#RMSNorm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "class RMSNorm(nnx.Module):\n  \"\"\"RMS normalization.\"\"\"\n\n  def __init__(\n      self,\n      num_features: int,\n      epsilon: float = 1e-6,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      kernel_axes: tuple[None | str, ...] = (),\n      scale_init: Initializer = nn.initializers.ones,\n      parameter_memory_host_offload: bool = False,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.num_features = num_features\n    self.epsilon = epsilon\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.kernel_axes = kernel_axes\n    self.scale_init = scale_init\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n    self.scale = nnx.Param(\n        scale_init(rngs.params(), (num_features,), weight_dtype),\n        sharding=kernel_axes,\n    )\n\n  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Applies layer normalization on the input.\"\"\"\n    x = jnp.asarray(x, jnp.float32)\n    mean2 = jnp.mean(lax.square(x), axis=-1, keepdims=True)\n    y = jnp.asarray(x * lax.rsqrt(mean2 + self.epsilon), self.dtype)\n    scale = self.scale.value\n    # Move scale to device if parameter offloading is enabled\n    if self.parameter_memory_host_offload:\n      max_logging.log(\"normalizations.py: Moving scale parameter to device\")\n      scale = jax.device_put(scale, max_utils.device_space())\n\n    scale = jnp.asarray(scale, self.dtype)\n    return y * scale",
        "analysis": {
            "module_type": "rms_normalization",
            "purpose": "Applies Root Mean Square (RMS) normalization to an input tensor, normalizing activations by their root mean square and scaling them with a learnable parameter.",
            "input": {
                "shape": "The shape of the input tensor for the `__call__` method is `[..., num_features]`.",
                "dtype": "The data type of the input tensor for the `__call__` method is `jnp.ndarray` (e.g., float32, bfloat16)."
            },
            "processing_steps": [
                "Initializes a learnable `scale` parameter of shape `[num_features]`.",
                "In the forward pass, calculates the mean of the squared input values along the last dimension.",
                "Normalizes the input by dividing it by the square root of the calculated mean plus a small epsilon.",
                "Scales the normalized output by the learnable `scale` parameter."
            ],
            "output": {
                "shape": "The output tensor has the same shape as the input, `[..., num_features]`."
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy",
                "MaxText.max_logging",
                "MaxText.max_utils",
                "MaxText.layers.initializers.Initializer"
            ],
            "parameters": {
                "num_features": "The size of the feature dimension (the last axis) to be normalized.",
                "epsilon": "A small float added to the denominator for numerical stability.",
                "dtype": "The data type for the output and the `scale` parameter during computation.",
                "weight_dtype": "The data type for storing the `scale` parameter.",
                "kernel_axes": "A tuple defining the sharding for the `scale` parameter.",
                "scale_init": "An initializer function for the `scale` parameter.",
                "parameter_memory_host_offload": "If True, the `scale` parameter is stored on the host and moved to the device only during the forward pass."
            },
            "notes": [
                "This is a Flax NNX module (`nnx.Module`).",
                "The normalization is always performed over the last axis of the input tensor.",
                "Internal computations are done in `jnp.float32` for precision before casting to the specified output `dtype`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RMSNorm layer, setting up its configuration and creating the learnable `scale` parameter.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters (num_features, epsilon, etc.) as instance attributes.",
                        "Initialize the `scale` parameter as an `nnx.Param` using the provided `scale_init` function and `rngs`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx",
                        "flax.linen.initializers"
                    ],
                    "notes": [
                        "The `rngs` argument is required to provide a random key for parameter initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the RMS normalization transformation to the input tensor.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Cast input `x` to `jnp.float32`.",
                        "Compute the mean of the square of `x` along the last axis.",
                        "Normalize `x` by multiplying with the reciprocal square root of (mean + epsilon).",
                        "Cast the result to the module's `dtype`.",
                        "Retrieve the `scale` parameter value.",
                        "If `parameter_memory_host_offload` is True, move the `scale` parameter to the device.",
                        "Multiply the normalized tensor by the `scale` parameter."
                    ],
                    "output": {
                        "shape": "[..., num_features]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.lax",
                        "jax",
                        "MaxText.max_logging",
                        "MaxText.max_utils"
                    ],
                    "notes": [
                        "This method implements the core logic of the RMS normalization algorithm."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/normalizations.py#rms_norm",
        "file_path": "src/MaxText/layers/normalizations.py",
        "code_block": "def rms_norm(\n    num_features: int,\n    epsilon: float = 1e-6,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    kernel_axes: tuple[None | str, ...] = (),\n    scale_init: Initializer = nn.initializers.ones,\n    name: None | str = None,\n    parameter_memory_host_offload: bool = False,\n):\n  \"\"\"Creates a RMSNorm module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      RMSNorm,\n      num_features=num_features,\n      epsilon=epsilon,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      kernel_axes=kernel_axes,\n      scale_init=scale_init,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "rms_norm_factory",
            "purpose": "A factory function that creates and returns a Flax Linen-compatible RMSNorm module by wrapping an NNX RMSNorm class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RMSNorm` NNX module into a Flax Linen module.",
                "Passes all function arguments (num_features, epsilon, dtype, etc.) to the `RMSNorm` constructor via the wrapper.",
                "Specifies `variable_to_logically_partitioned` as the `metadata_fn` for parameter partitioning."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance. When called, this module expects an input of shape [..., num_features] and returns a tensor of the same shape."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RMSNorm",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "num_features": "The number of features in the input tensor to normalize over.",
                "epsilon": "A small float added to the denominator for numerical stability.",
                "dtype": "The data type of the computation and output.",
                "weight_dtype": "The data type of the learnable scale parameter.",
                "kernel_axes": "A tuple specifying the sharding annotations for the scale parameter.",
                "scale_init": "The initializer function for the learnable scale parameter.",
                "parameter_memory_host_offload": "A boolean indicating whether to offload parameters to the host memory."
            },
            "notes": [
                "This function acts as a bridge, allowing an `nnx.Module` (`RMSNorm`) to be used within a Flax Linen model structure."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#_maybe_move_embedding_to_device",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def _maybe_move_embedding_to_device(embedding_table: Array, config: Config) -> Array:\n  \"\"\"Moves embedding table to device if parameter offloading is enabled.\"\"\"\n  if config.parameter_memory_host_offload:\n    max_logging.log(\"embeddings.py: Moving embedding parameter to device\")\n    return jax.device_put(embedding_table, max_utils.device_space())\n  return embedding_table",
        "analysis": {
            "module_type": "conditional_device_placement",
            "purpose": "Conditionally moves an embedding table tensor to the accelerator device if parameter offloading is enabled in the configuration.",
            "input": {
                "shape": "[vocab_size, embedding_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the `config.parameter_memory_host_offload` flag.",
                "If the flag is true, log a message indicating the move.",
                "If the flag is true, call `jax.device_put` to move the `embedding_table` to the device specified by `max_utils.device_space()`.",
                "Return the original or the device-placed embedding table."
            ],
            "output": {
                "shape": "[vocab_size, embedding_dim]"
            },
            "dependencies": [
                "jax.device_put",
                "max_logging.log",
                "max_utils.device_space",
                "Config"
            ],
            "parameters": {
                "config.parameter_memory_host_offload": "A boolean flag that, when true, triggers the movement of the embedding table from host memory to the accelerator device."
            },
            "notes": [
                "This is a memory optimization technique used to offload large parameters to host CPU memory and only transfer them to the device when required for computation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#embed_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def embed_as_linen(\n    *,\n    num_embeddings: int,\n    num_features: int,\n    config: Config,\n    cast_input_dtype: None | DType = None,\n    dtype: DType = jnp.float32,\n    attend_dtype: None | DType = None,\n    embedding_init: Initializer = default_embed_init,\n    name: str | None = None,\n):\n  \"\"\"Initializes the Embed NNX module and returns it as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `Embed` module within\n  a Linen model. It wraps the `Embed` module using `nnx.bridge.to_linen`,\n  making it compatible with the Linen API.\n\n  Args:\n    num_embeddings: The number of embeddings.\n    num_features: The number of feature dimensions for each embedding.\n    config: The model configuration.\n    cast_input_dtype: The dtype to cast the input to, if any.\n    dtype: The dtype of the embedding vectors.\n    attend_dtype: The dtype for the `attend` method.\n    embedding_init: The initializer for the embedding matrix.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `Embed` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      Embed,\n      num_embeddings=num_embeddings,\n      num_features=num_features,\n      config=config,\n      cast_input_dtype=cast_input_dtype,\n      dtype=dtype,\n      attend_dtype=attend_dtype,\n      embedding_init=embedding_init,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "nnx_to_linen_embedding_bridge",
            "purpose": "Initializes the NNX-based `Embed` module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `Embed` NNX module.",
                "Passes configuration parameters such as `num_embeddings`, `num_features`, `config`, dtypes, and an initializer to the wrapper.",
                "Specifies `variable_to_logically_partitioned` as the `metadata_fn` for sharding."
            ],
            "output": {
                "shape": "A Flax Linen module instance that encapsulates the NNX `Embed` module."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Embed",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "num_embeddings": "The number of embeddings (vocabulary size).",
                "num_features": "The number of feature dimensions for each embedding.",
                "config": "The model configuration object.",
                "dtype": "The data type of the embedding vectors.",
                "embedding_init": "The initializer for the embedding matrix."
            },
            "notes": [
                "This function serves as a compatibility bridge to use an NNX-defined module within a Linen model.",
                "The actual embedding logic is handled by the `Embed` class, which this function instantiates and wraps."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#Embed",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class Embed(nnx.Module):\n  \"\"\"A parameterized function from integers [0, n) to d-dimensional vectors.\"\"\"\n\n  def __init__(\n      self,\n      num_embeddings: int,\n      num_features: int,\n      config: Config,\n      cast_input_dtype: None | DType = None,\n      dtype: DType = jnp.float32,\n      attend_dtype: None | DType = None,\n      embedding_init: Initializer = default_embed_init,\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the Embed module.\n\n    Args:\n      num_embeddings: The number of embeddings.\n      num_features: The number of feature dimensions for each embedding.\n      config: The model configuration.\n      cast_input_dtype: The dtype to cast the input to, if any.\n      dtype: The dtype of the embedding vectors.\n      attend_dtype: The dtype for the `attend` method.\n      embedding_init: The initializer for the embedding matrix.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.num_embeddings = num_embeddings\n    self.num_features = num_features\n    self.config = config\n    self.cast_input_dtype = cast_input_dtype\n    self.dtype = dtype\n    self.attend_dtype = attend_dtype\n\n    self.embedding = nnx.Param(\n        embedding_init(rngs.params(), (self.num_embeddings, self.num_features), self.config.weight_dtype),\n        sharding=(\"vocab\", \"embed\"),\n    )\n\n  def __call__(self, inputs: Array, model_mode: str = MODEL_MODE_TRAIN) -> Array:\n    \"\"\"Embeds the inputs along the last dimension.\n\n    Args:\n      inputs: input data, all dimensions are considered batch dimensions.\n\n    Returns:\n      Output which is embedded input data.  The output shape follows the input,\n      with an additional `num_features` dimension appended.\n    \"\"\"\n    cfg = self.config\n    if self.cast_input_dtype:\n      inputs = inputs.astype(self.cast_input_dtype)\n    if not jnp.issubdtype(inputs.dtype, jnp.integer):\n      raise ValueError(\"Input type must be an integer or unsigned integer.\")\n\n    embedding = _maybe_move_embedding_to_device(self.embedding.value, self.config)\n\n    if cfg.use_iota_embed:\n      iota = lax.iota(jnp.int32, self.num_embeddings)\n      one_hot = jnp.array(inputs[..., jnp.newaxis] == iota, dtype=self.dtype)\n      output = jnp.dot(one_hot, jnp.asarray(embedding, self.dtype))\n    else:\n      output = jnp.asarray(embedding, self.dtype)[inputs]\n\n    output_prefill_axis_names = (\"activation_embed_and_logits_batch\", \"prefill_activation_length\", \"activation_embed\")\n    output_default_axis_names = (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_embed\")\n\n    if model_mode == MODEL_MODE_PREFILL:\n      output = nn.with_logical_constraint(output, output_prefill_axis_names)\n    else:\n      output = nn.with_logical_constraint(output, output_default_axis_names)\n    return output\n\n  def attend(self, query: Array) -> Array:\n    \"\"\"Attend over the embedding using a query array.\n\n    Args:\n      query: array with last dimension equal the feature depth `num_features` of the\n        embedding.\n\n    Returns:\n      An array with final dim `num_embeddings` corresponding to the batched\n      inner-product of the array of query vectors against each embedding.\n      Commonly used for weight-sharing between embeddings and logit transform\n      in NLP models.\n    \"\"\"\n    embedding = self.embedding.value\n    attend_dtype = self.attend_dtype if self.attend_dtype is not None else self.dtype\n    return attend_on_embedding(query, embedding, attend_dtype, self.config)",
        "analysis": {
            "module_type": "embedding_layer",
            "purpose": "A parameterized module that maps integer indices from a vocabulary to dense vector representations.",
            "input": {
                "shape": "[batch_dims...]",
                "dtype": "integer"
            },
            "processing_steps": [
                "Optionally casts the input tensor to `cast_input_dtype`.",
                "Validates that the input is an integer type.",
                "Optionally moves the embedding table to the device via `_maybe_move_embedding_to_device`.",
                "Conditionally performs embedding lookup either via one-hot dot product (if `config.use_iota_embed` is true) or direct array indexing.",
                "Applies logical constraints for tensor sharding using `nn.with_logical_constraint`, with different constraints for 'prefill' mode versus other modes."
            ],
            "output": {
                "shape": "[batch_dims..., num_features]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.Rngs",
                "_maybe_move_embedding_to_device",
                "attend_on_embedding",
                "flax.linen.with_logical_constraint"
            ],
            "parameters": {
                "num_embeddings": "The size of the vocabulary (number of possible integer inputs).",
                "num_features": "The dimensionality of the output embedding vectors.",
                "config.use_iota_embed": "A boolean flag that determines the embedding implementation: `True` for one-hot dot product, `False` for direct indexing."
            },
            "notes": [
                "The module contains an `attend` method for computing logits, which facilitates weight sharing between input embeddings and the final output layer.",
                "The `rngs` argument in `__init__` is a temporary requirement for compatibility with `nnx.bridge.to_linen` and is not used by the module itself."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the embedding layer, creating the embedding parameter matrix.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration parameters like `num_embeddings`, `num_features`, and `config`.",
                        "Initializes the embedding matrix as an `nnx.Param` using the `embedding_init` function and `rngs`.",
                        "Assigns a sharding annotation `(\"vocab\", \"embed\")` to the embedding parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.Rngs",
                        "Initializer"
                    ],
                    "notes": [
                        "The `rngs` parameter is passed by `nnx.bridge.to_linen` and is planned for removal."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the embedding lookup, mapping integer inputs to their corresponding vector representations.",
                    "input": {
                        "shape": "[batch_dims...]",
                        "dtype": "integer"
                    },
                    "processing_steps": [
                        "Optionally casts the input tensor's dtype based on `self.cast_input_dtype`.",
                        "Validates that the input dtype is an integer type.",
                        "Calls `_maybe_move_embedding_to_device` to potentially move the embedding table to the active device.",
                        "If `config.use_iota_embed` is true, calculates embeddings using a one-hot vector and a dot product.",
                        "If `config.use_iota_embed` is false, performs a standard array indexing lookup.",
                        "Applies `nn.with_logical_constraint` with different axis names depending on the `model_mode` ('prefill' or other)."
                    ],
                    "output": {
                        "shape": "[batch_dims..., num_features]"
                    },
                    "dependencies": [
                        "_maybe_move_embedding_to_device",
                        "jax.lax.iota",
                        "jax.numpy.dot",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The implementation path (one-hot dot product vs. direct indexing) is determined by the `config.use_iota_embed` flag."
                    ]
                },
                "attend": {
                    "purpose": "Computes the dot product between a query tensor and the embedding matrix, typically for generating logits in language models.",
                    "input": {
                        "shape": "[..., num_features]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Retrieves the embedding table value.",
                        "Determines the computation dtype from `attend_dtype` or `self.dtype`.",
                        "Calls the `attend_on_embedding` function with the query, embedding table, and determined dtype."
                    ],
                    "output": {
                        "shape": "[..., num_embeddings]"
                    },
                    "dependencies": [
                        "attend_on_embedding"
                    ],
                    "notes": [
                        "This method is commonly used for weight-sharing between the input embedding layer and the final output logit transformation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#attend_on_embedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def attend_on_embedding(query: Array, embedding_table: Array, attend_dtype: DType, config: Config) -> Array:\n  \"\"\"Attend over an embedding table using a query array.\n\n  TODO: Remove this method when Embed bridge to Linen is no longer needed\n\n  Args:\n    query: An array with a last dimension equal to the feature depth of the embedding.\n    embedding_table: The embedding table to attend over.\n    attend_dtype: The data type for the attention computation.\n    config: The model configuration, used to check for parameter offloading.\n\n  Returns:\n    An array with a final dimension equal to `num_embeddings`, corresponding to the\n    batched inner-product of the query vectors against each embedding.\n  \"\"\"\n  embedding_table = _maybe_move_embedding_to_device(embedding_table, config)\n  return jnp.dot(query, jnp.asarray(embedding_table, jnp.bfloat16).T, preferred_element_type=attend_dtype)",
        "analysis": {
            "module_type": "embedding_attention",
            "purpose": "Calculates the batched inner-product of a query tensor against an embedding table, typically for generating logits in a language model's final layer.",
            "input": {
                "shape": "query: [..., feature_depth], embedding_table: [num_embeddings, feature_depth]",
                "dtype": "float"
            },
            "processing_steps": [
                "Optionally move the embedding_table to the device by calling `_maybe_move_embedding_to_device`.",
                "Cast the embedding_table to `jnp.bfloat16`.",
                "Transpose the casted embedding_table.",
                "Compute the dot product of the query and the transposed embedding_table using `jnp.dot`, with `attend_dtype` as the preferred computation type."
            ],
            "output": {
                "shape": "[..., num_embeddings]"
            },
            "dependencies": [
                "_maybe_move_embedding_to_device",
                "jax.numpy.dot",
                "jax.numpy.asarray",
                "Config"
            ],
            "parameters": {
                "config.parameter_memory_host_offload": "A boolean flag that, if true, triggers moving the embedding table from host memory to the device before computation."
            },
            "notes": [
                "The function is marked with a TODO for removal, indicating it serves as a temporary bridge for using an NNX Embed module within a Linen model.",
                "The embedding table is hardcoded to be cast to `jnp.bfloat16` before the dot product, regardless of its original dtype."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the RotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      RotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "rotary_embedding_factory",
            "purpose": "A factory function that initializes the NNX `RotaryEmbedding` module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RotaryEmbedding` NNX module into a Linen-compatible module.",
                "Forwards all provided arguments (`min_timescale`, `max_timescale`, `embedding_dims`, etc.) to the `RotaryEmbedding` constructor.",
                "Specifies `variable_to_logically_partitioned` as the metadata function for handling variable partitioning.",
                "Returns the newly created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance that wraps the `RotaryEmbedding` NNX module."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, which determines the periodicity of the added signal.",
                "max_timescale": "End of the geometric index, which determines the frequency of the added signal.",
                "embedding_dims": "Dimension of the embedding to be generated by the module.",
                "cast_as_fprop_dtype": "A boolean indicating whether the returned module should cast its output to `fprop_dtype`.",
                "fprop_dtype": "The data type for the forward pass computation in the returned module.",
                "name": "The name for the resulting Linen module."
            },
            "notes": [
                "This function serves as a bridge to allow an NNX-defined module (`RotaryEmbedding`) to be used within a Flax Linen-based model architecture."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#RotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class RotaryEmbedding(nnx.Module):\n  \"\"\"Rotary Position Embedding.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      # Not used in RotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rope_linear_scaling_factor: float = 1.0,\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the RotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    self.min_timescale = min_timescale\n    self.max_timescale = max_timescale\n    self.embedding_dims = embedding_dims\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.rope_linear_scaling_factor = rope_linear_scaling_factor\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def timescale(self):\n    \"\"\"Returns the timescale for the rotary embedding.\"\"\"\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n    if self.rope_linear_scaling_factor != 1.0:\n      timescale = timescale * self.rope_linear_scaling_factor\n    return timescale\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      inputs: jax.Array,\n      position: None | jax.Array = None,\n  ) -> jax.Array:\n    \"\"\"Generates a jax.Array of sinusoids with different frequencies.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. Since rotary position embeddings are applied to query and\n        keys after projection, it is assumed of shape [B, S, N, H].\n      position: Optional position jax.Array which denotes the position of each\n        token in the sequence. This only needs to be supplied when the sequence\n        is packed. It is of shape [B, S].\n\n    Returns:\n      a jax.Array of shape [B, S, N, H] which includes the inputs together with\n      the rotary position embedding incorporated in it.\n    \"\"\"\n    assert position is not None\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape\" \"[batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\n          \"The embedding dims of the rotary position embedding\" \"must match the hidden dimension of the inputs.\"\n      )\n\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n    sin = jnp.sin(sinusoid_inp).astype(inputs.dtype)\n    cos = jnp.cos(sinusoid_inp).astype(inputs.dtype)\n    first_half, second_half = jnp.split(inputs, 2, axis=-1)\n    first_part = first_half * cos - second_half * sin\n    second_part = second_half * cos + first_half * sin\n    if self.cast_as_fprop_dtype:\n      first_part = first_part.astype(self.fprop_dtype)\n      second_part = second_part.astype(self.fprop_dtype)\n    x_out = jnp.concatenate((first_part, second_part), axis=-1)\n    return x_out",
        "analysis": {
            "module_type": "rotary_position_embedding",
            "purpose": "Applies Rotary Position Embedding (RoPE) to an input tensor to encode positional information by rotating the embedding vectors.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes parameters for generating sinusoidal embeddings.",
                "When called, it computes sine and cosine values based on input positions.",
                "Splits the input tensor's feature dimension into two halves.",
                "Applies a rotational transformation to the two halves using the sine and cosine values.",
                "Concatenates the transformed halves to produce the output."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "jax.numpy"
            ],
            "parameters": {
                "min_timescale": "The starting value for the geometric progression of timescales, affecting the periodicity.",
                "max_timescale": "The ending value for the geometric progression of timescales, affecting the frequency.",
                "embedding_dims": "The dimension of the feature space to which the embedding is applied; must be a multiple of 2.",
                "cast_as_fprop_dtype": "A boolean indicating whether to cast the output to the forward propagation data type.",
                "fprop_dtype": "The data type for the forward propagation pass, e.g., jnp.bfloat16.",
                "rope_linear_scaling_factor": "A factor to linearly scale the timescale, adjusting the frequency range."
            },
            "notes": [
                "The `embedding_dims` parameter must be an even number, as the implementation splits the feature dimension in half.",
                "The `__call__` method requires an explicit `position` tensor, which is necessary for handling packed sequences where token positions are not sequential."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the RotaryEmbedding module with configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration values (timescales, dimensions, dtypes) to instance attributes.",
                        "Validates that `embedding_dims` is a multiple of 2."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `rngs` parameter is accepted for compatibility with `nnx.bridge.to_linen` but is not used by this module."
                    ]
                },
                "timescale": {
                    "purpose": "A property that computes and returns the timescale vector used for generating the sinusoidal frequencies.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculates a fraction array based on the embedding dimension.",
                        "Computes a timescale vector using a geometric progression from `min_timescale` to `max_timescale`.",
                        "Applies the `rope_linear_scaling_factor` if it is not 1.0."
                    ],
                    "output": {
                        "shape": "[embedding_dims // 2]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a `@property`, so it is accessed as an attribute (e.g., `self.timescale`) rather than called as a method."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the rotary position embedding to an input tensor.",
                    "input": {
                        "shape": "inputs: [B, S, N, H], position: [B, S]",
                        "dtype": "inputs: float, position: float or integer"
                    },
                    "processing_steps": [
                        "Asserts that the `position` tensor is provided.",
                        "Validates that the input tensor has a rank of 4.",
                        "Validates that `self.embedding_dims` matches the last dimension of the input tensor.",
                        "Calculates sine and cosine values based on the `position` tensor and `self.timescale`.",
                        "Splits the input tensor into two halves along the last dimension.",
                        "Applies the rotation: `x_out = [x_part1 * cos - x_part2 * sin, x_part2 * cos + x_part1 * sin]`.",
                        "Optionally casts the result to `self.fprop_dtype`.",
                        "Concatenates the two resulting parts along the last dimension."
                    ],
                    "output": {
                        "shape": "[B, S, N, H]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "The input tensor shape is assumed to be [batch, sequence, heads, dims]."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_rotary_embedding_as_linen(\n    *,\n    min_timescale: int,\n    max_timescale: int,\n    embedding_dims: int = 0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    use_scale: bool = True,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LLaMARotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    min_timescale: Start of the geometric index. Determines the periodicity of\n      the added signal.\n    max_timescale: End of the geometric index. Determines the frequency of the\n      added signal.\n    embedding_dims: Dimension of the embedding to be generated.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    use_scale: Whether to apply LLaMA3.1 scaling factor.\n    name: Name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LLaMARotaryEmbedding,\n      min_timescale=min_timescale,\n      max_timescale=max_timescale,\n      embedding_dims=embedding_dims,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      use_scale=use_scale,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "llama_rotary_embedding_factory",
            "purpose": "A factory function that initializes the LLaMARotaryEmbedding NNX module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to create a Linen-compatible module from the `LLaMARotaryEmbedding` class.",
                "Forwards all provided arguments (min_timescale, max_timescale, etc.) to the `LLaMARotaryEmbedding` constructor.",
                "Specifies `variable_to_logically_partitioned` as the `metadata_fn` for handling parameter sharding.",
                "Returns the newly created Linen module instance."
            ],
            "output": {
                "shape": "An instance of a Flax Linen module."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "LLaMARotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, determining the periodicity of the signal.",
                "max_timescale": "End of the geometric index, determining the frequency of the signal.",
                "embedding_dims": "Dimension of the embedding to be generated.",
                "cast_as_fprop_dtype": "Whether to cast the output of the returned module to the fprop dtype.",
                "fprop_dtype": "The data type for the forward pass computation in the returned module.",
                "use_scale": "Whether to apply the LLaMA3.1 scaling factor to the rotary frequencies."
            },
            "notes": [
                "This function does not process tensors itself but rather constructs and returns a layer.",
                "It acts as a bridge to allow an NNX-defined module to be used within a Linen model architecture."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LLaMARotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LLaMARotaryEmbedding(RotaryEmbedding):\n  \"\"\"LLaMA variant of ROPE.\"\"\"\n\n  def __init__(\n      self,\n      min_timescale: int,\n      max_timescale: int,\n      embedding_dims: int = 0,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      use_scale: bool = True,\n      # Not used in LLaMARotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the LLaMARotaryEmbedding module.\n\n    Args:\n      min_timescale: Start of the geometric index. Determines the periodicity of\n        the added signal.\n      max_timescale: End of the geometric index. Determines the frequency of the\n        added signal.\n      embedding_dims: Dimension of the embedding to be generated.\n      cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n      fprop_dtype: The dtype of the output.\n      use_scale: Whether to apply LLaMA3.1 scaling factor.\n      rngs: rng keys passed in by nnx.bridge.to_linen.\n    \"\"\"\n    super().__init__(\n        min_timescale=min_timescale,\n        max_timescale=max_timescale,\n        embedding_dims=embedding_dims,\n        cast_as_fprop_dtype=cast_as_fprop_dtype,\n        fprop_dtype=fprop_dtype,\n        rngs=rngs,\n    )\n\n    # LLaMA3.1 ROPE scaling, see the original pytorch implementation:\n    # https://github.com/meta-llama/llama-models/blob/301ca3a2b3b10e94ddcd1fdd2c57e52f812e1cac/models/llama3/reference_impl/model.py#L45C5-L45C18\n    self.use_scale = use_scale\n\n  @property\n  def timescale(self):\n    half_embedding_dim = self.embedding_dims // 2\n    fraction = 2 * jnp.arange(0, half_embedding_dim) / self.embedding_dims\n    fraction = jnp.repeat(fraction, 2)\n    timescale = self.min_timescale * (self.max_timescale / self.min_timescale) ** fraction\n\n    # Apply scaling factor if enabled\n    if self.use_scale:\n      timescale = 1.0 / jax.vmap(self._apply_scaling_factor)(1.0 / timescale)\n\n    # Expand timescale dimensions for broadcasting\n    return timescale[jnp.newaxis, jnp.newaxis, jnp.newaxis, :]\n\n  def _apply_scaling_factor(self, freq):\n    \"\"\"apply scaling factor to rotary position embedding.\"\"\"\n    scale_factor = 8\n    low_freq_factor = 1\n    high_freq_factor = 4\n    old_context_len = 8192  # original llama3 length\n\n    low_freq_wavelen = old_context_len / low_freq_factor\n    high_freq_wavelen = old_context_len / high_freq_factor\n    wavelen = 2 * jnp.pi / freq\n\n    def lower_wavelen(freq):\n      return freq\n\n    def bigger_or_equal_wavelen(freq):\n      def bigger_wavelen(freq):\n        return freq / scale_factor\n\n      def equal_wavelen(freq):\n        smooth = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)\n        return (1 - smooth) * freq / scale_factor + smooth * freq\n\n      bigger_wavelen_cond = wavelen > low_freq_wavelen\n      return jax.lax.cond(bigger_wavelen_cond, bigger_wavelen, equal_wavelen, freq)\n\n    lower_wavelen_cond = wavelen < high_freq_wavelen\n    return jax.lax.cond(lower_wavelen_cond, lower_wavelen, bigger_or_equal_wavelen, freq)\n\n  def __call__(self, inputs: jax.Array, position: None | jax.Array = None) -> jax.Array:\n    \"\"\"Applies LLaMA variant of rotary position embedding.\n\n    Args:\n      inputs: The input sequence on which to apply the Rotary position\n        embedding. It is assumed of shape [B, S, N, H].\n      position: Optional position array [B, S]. Only needed when the sequence\n        is packed.\n\n    Returns:\n      A jax.Array of shape [B, S, N, H] with rotary position embeddings applied.\n    \"\"\"\n    # Ensure input is 4D\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [B, S, N, H].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\")\n\n    # Shift the inputs left and right as per LLaMA's specific behavior\n    inputs_shifted_left = jnp.concatenate([inputs[..., 1:], inputs[..., :1]], axis=-1)\n    inputs_shifted_right = jnp.concatenate([inputs[..., -1:], inputs[..., :-1]], axis=-1)\n    inputs_shifted = jax.lax.select(\n        jnp.tile(\n            jnp.mod(jnp.arange(self.embedding_dims, dtype=jnp.int32), 2),\n            inputs.shape[:-1] + (1,),\n        ),\n        inputs_shifted_right,\n        inputs_shifted_left,\n    )\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.float32)[jnp.newaxis, :]\n\n    # Calculate sinusoidal input\n    position = position[:, :, jnp.newaxis, jnp.newaxis]\n    sinusoid_inp = position / self.timescale\n\n    sin = jnp.sin(sinusoid_inp)\n    cos = jnp.cos(sinusoid_inp)\n\n    # Apply alternating sign\n    sign = jnp.tile(jnp.array([-1, 1]), self.embedding_dims // 2)\n\n    # Combine original inputs with sinusoidal information\n    outputs = inputs * cos + inputs_shifted * sin * sign\n\n    if self.cast_as_fprop_dtype:\n      outputs = outputs.astype(self.fprop_dtype)\n\n    return outputs",
        "analysis": {
            "module_type": "llama_rotary_embedding",
            "purpose": "Implements the LLaMA-specific variant of Rotary Position Embedding (RoPE), which can optionally apply LLaMA3.1 frequency scaling.",
            "input": {
                "shape": "[B, S, N, H] (batch, sequence, heads, hidden_dim_per_head)",
                "dtype": "jax.Array"
            },
            "processing_steps": [
                "Validates that the input tensor is 4D and its last dimension matches `embedding_dims`.",
                "Creates a shifted version of the input tensor by concatenating slices.",
                "Generates a position tensor if one is not provided.",
                "Calculates sinusoidal inputs by dividing the positions by the module's timescale.",
                "Computes sine and cosine of the sinusoidal inputs.",
                "Creates an alternating sign tensor `[-1, 1, -1, 1, ...]`.",
                "Combines the original inputs, shifted inputs, sine, cosine, and sign tensors to produce the output.",
                "Optionally casts the output to a specified forward propagation data type."
            ],
            "output": {
                "shape": "[B, S, N, H]"
            },
            "dependencies": [
                "RotaryEmbedding",
                "jax",
                "jax.numpy",
                "jax.lax"
            ],
            "parameters": {
                "min_timescale": "Start of the geometric index, determining the periodicity of the signal.",
                "max_timescale": "End of the geometric index, determining the frequency of the signal.",
                "embedding_dims": "Dimension of the embedding to be generated, must match the input's last dimension.",
                "use_scale": "A boolean flag to determine whether to apply the LLaMA3.1 scaling factor to the frequencies."
            },
            "notes": [
                "This class inherits from `RotaryEmbedding` but overrides the `timescale` property and the `__call__` method to implement LLaMA-specific logic.",
                "Unlike the standard RoPE implementation which splits the hidden dimension, this version shifts the input features and combines them.",
                "The `rngs` parameter in `__init__` is a placeholder for compatibility with `nnx.bridge.to_linen` and is not used."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the LLaMARotaryEmbedding module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls the parent `RotaryEmbedding` constructor.",
                        "Stores the `use_scale` parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RotaryEmbedding"
                    ],
                    "notes": []
                },
                "timescale": {
                    "purpose": "Calculates the timescale for the rotary embeddings, with optional LLaMA3.1 scaling.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculates a base timescale using a geometric progression from `min_timescale` to `max_timescale`.",
                        "If `use_scale` is true, applies `_apply_scaling_factor` to the inverse of the timescale (frequencies) using `jax.vmap`.",
                        "Expands the dimensions of the final timescale tensor for broadcasting."
                    ],
                    "output": {
                        "shape": "[1, 1, 1, embedding_dims]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.vmap",
                        "self._apply_scaling_factor"
                    ],
                    "notes": [
                        "This is a `@property`, so it's accessed as an attribute, not called as a method."
                    ]
                },
                "_apply_scaling_factor": {
                    "purpose": "Applies a scaling factor to a single frequency value based on its corresponding wavelength, as per the LLaMA3.1 implementation.",
                    "input": {
                        "shape": "Scalar",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Calculates the wavelength from the input frequency.",
                        "Uses `jax.lax.cond` to apply different scaling logic based on whether the wavelength is within certain high and low frequency bounds.",
                        "For wavelengths between the bounds, a smooth scaling factor is applied."
                    ],
                    "output": {
                        "shape": "Scalar"
                    },
                    "dependencies": [
                        "jax.lax",
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a private helper method used by the `timescale` property."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the LLaMA variant of rotary position embedding to an input tensor.",
                    "input": {
                        "shape": "inputs: [B, S, N, H], position (optional): [B, S]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Validates input tensor shape.",
                        "Creates `inputs_shifted` by concatenating slices of the input tensor along the last axis.",
                        "Generates a position tensor if not provided.",
                        "Calculates `sinusoid_inp` by dividing position by `self.timescale`.",
                        "Computes `sin` and `cos` of `sinusoid_inp`.",
                        "Creates an alternating `sign` tensor.",
                        "Computes the output by combining `inputs`, `cos`, `inputs_shifted`, `sin`, and `sign`.",
                        "Optionally casts the output to `self.fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[B, S, N, H]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "jax.lax"
                    ],
                    "notes": [
                        "If the `position` argument is not provided, it defaults to `jnp.arange(sequence_length)`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#yarn_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def yarn_rotary_embedding_as_linen(\n    *,\n    embedding_dims: int,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    beta_fast: float = 32,\n    beta_slow: float = 1,\n    rope_theta: float = 10000.0,\n    rope_factor: float = 40,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n    interleave: bool = True,\n    truncate: bool = True,\n    attention_scaling: bool = False,\n):\n  \"\"\"Initializes the YarnRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_position_embeddings: The maximum number of positions.\n    original_max_position_embeddings: The original maximum number of positions.\n    beta_fast: The fast beta parameter for YaRN.\n    beta_slow: The slow beta parameter for YaRN.\n    rope_theta: The base for the rotary frequencies.\n    rope_factor: The scaling factor for RoPE.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    name: The name of the module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      YarnRotaryEmbedding,\n      embedding_dims=embedding_dims,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      beta_fast=beta_fast,\n      beta_slow=beta_slow,\n      rope_theta=rope_theta,\n      rope_factor=rope_factor,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      interleave=interleave,\n      truncate=truncate,\n      attention_scaling=attention_scaling,\n  )",
        "analysis": {
            "module_type": "yarn_rotary_embedding_factory",
            "purpose": "A factory function that initializes the NNX-based YarnRotaryEmbedding module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `YarnRotaryEmbedding` NNX module into a Linen module, passing along all configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance. When this instance is called, it preserves the input tensor shape of [batch, sequence, heads, dims]."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "YarnRotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_position_embeddings": "The maximum number of positions.",
                "original_max_position_embeddings": "The original maximum number of positions.",
                "beta_fast": "The fast beta parameter for YaRN.",
                "beta_slow": "The slow beta parameter for YaRN.",
                "rope_theta": "The base for the rotary frequencies.",
                "rope_factor": "The scaling factor for RoPE.",
                "cast_as_fprop_dtype": "Whether to cast the output to `fprop_dtype`.",
                "fprop_dtype": "The forward pass dtype.",
                "interleave": "Whether the complex representation is interleaved or concatenated.",
                "truncate": "Whether to floor the lower bound and ceil the upper bound for the correction range.",
                "attention_scaling": "Whether to scale the rotary embedding output."
            },
            "notes": [
                "This function serves as a bridge to allow using the NNX-defined `YarnRotaryEmbedding` module within a larger model built with the Linen API."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#YarnRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class YarnRotaryEmbedding(nnx.Module):\n  \"\"\"Yarn rotary embedding.\n\n  Based on https://arxiv.org/abs/2309.00071\n  This implementation uses DeepSeek-v3 PyTorch as reference\n  https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L294\n\n  Attributes:\n    embedding_dims: Dimension of the embedding to be generated.\n    max_position_embeddings: The maximum sequence length that will be encountered.\n    original_max_position_embeddings: The sequence length for which the base frequencies were defined.\n    beta_fast: Lower bound parameter for correction.\n    beta_slow: Upper bound parameter for correction.\n    rope_theta: The base theta value for the frequency computation.\n    rope_factor: Factor applied to adjust the frequencies.\n    cast_as_fprop_dtype: Whether to cast the output to `fprop_dtype`.\n    fprop_dtype: The forward pass dtype.\n    rope_interleave: Whether complex representation is interleaved or concatenated.\n    rope_truncate: Whether or not to floor lower bound and ceil upper bound for correction range.\n    rope_attention_scaling: Whether or not to scale the rotary embedding output.\n    rngs: rng keys passed in by nnx.bridge.to_linen.\n  \"\"\"\n\n  def __init__(\n      self,\n      embedding_dims: int,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      beta_fast: float = 32,\n      beta_slow: float = 1,\n      rope_theta: float = 10000.0,\n      rope_factor: float = 40,\n      cast_as_fprop_dtype: bool = True,\n      fprop_dtype: DType = jnp.bfloat16,\n      interleave=True,\n      truncate=True,\n      attention_scaling=False,\n      # Not used in YarnRotaryEmbedding but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the YarnRotaryEmbedding module.\"\"\"\n    self.embedding_dims = embedding_dims\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.beta_fast = beta_fast\n    self.beta_slow = beta_slow\n    self.rope_theta = rope_theta\n    self.rope_factor = rope_factor\n    self.cast_as_fprop_dtype = cast_as_fprop_dtype\n    self.fprop_dtype = fprop_dtype\n    self.interleave = interleave\n    self.truncate = truncate\n    self.attention_scaling = attention_scaling\n\n    if self.embedding_dims % 2:\n      raise ValueError(\"Embedding dim for rotary position embedding must be a multiple of 2.\")\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    half_dim = self.embedding_dims // 2\n    # Compute base frequencies for each (even-indexed) dimension.\n    # (Note: We use jnp.arange with float32 for precision.)\n    freqs = 1.0 / (self.rope_theta ** (2.0 * jnp.arange(0, half_dim, dtype=jnp.float32) / self.embedding_dims))\n\n    low, high = self._find_correction_range(\n        self.beta_fast,\n        self.beta_slow,\n        self.embedding_dims,\n        self.rope_theta,\n        self.original_max_position_embeddings,\n        self.truncate,\n    )\n    smooth = 1 - self._linear_ramp_factor(low, high, half_dim)\n    # The corrected frequency is a weighted mix of the scaled and base values.\n    freqs = freqs / self.rope_factor * (1 - smooth) + freqs * smooth\n\n    # Precompute frequencies for all positions by taking the outer product.\n    t = jnp.arange(self.max_position_embeddings, dtype=jnp.float32)  # shape [max_position_embeddings]\n    # This gives a [max_position_embeddings, half_dim] tensor with rows as time steps.\n    freqs = jnp.outer(t, freqs)\n\n    # Compute the complex \u201ccis\u201d values: exp(i * theta).\n    return jnp.exp(1j * freqs)  # shape [max_position_embeddings, half_dim]\n\n  def _find_correction_dim(self, num_rotations: float, dim: int, base: float, max_position_embeddings: int) -> float:\n    \"\"\"Compute the correction dimension for a given number of rotations.\"\"\"\n    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n\n  def _find_correction_range(\n      self, low_rot: float, high_rot: float, dim: int, base: float, max_position_embeddings: int, truncate: bool\n  ):\n    \"\"\"Computes the range of correction dimensions for rotary positional embeddings.\n\n    Args:\n        low_rot (float): Lower bound for the number of rotations.\n        high_rot (float): Upper bound for the number of rotations.\n        dim (int): Dimensionality of the embedding space.\n        base (float): Base value for the exponential computation.\n        max_position_embeddings (int): Maximum sequence length.\n        truncate (bool): Whether to floor lower bound and ceil upper bound.\n\n    Returns:\n        tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n    \"\"\"\n    low = self._find_correction_dim(low_rot, dim, base, max_position_embeddings)\n    high = self._find_correction_dim(high_rot, dim, base, max_position_embeddings)\n    if truncate:\n      low = math.floor(low)\n      high = math.ceil(high)\n    low = max(low, 0)\n    high = min(high, dim - 1)\n    return low, high\n\n  def _linear_ramp_factor(self, min_val: float, max_val: float, dim: int) -> Array:\n    \"\"\"Computes a linear ramp over the dimension.\n\n    Returns a jax.Array of shape (dim,) with values between 0 and 1.\n    \"\"\"\n    if min_val == max_val:\n      max_val += 0.001  # Avoid division by zero.\n    linear_func = (jnp.arange(dim, dtype=jnp.float32) - min_val) / (max_val - min_val)\n    return jnp.clip(linear_func, 0, 1)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies the rotary positional embedding using the precomputed complex frequencies.\n\n    Args:\n      inputs: jax.Array of shape [B, S, N, H]. (H must equal self.embedding_dims.)\n      position: jax.Array of shape [B, S] with integer positions (indexes into precomputed freqs).\n\n    Returns:\n      jax.Array of shape [B, S, N, H] with the rotary embedding applied.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\"Input is assumed to be a rank 4 tensor of shape [batch, sequence, heads, dims].\")\n    if self.embedding_dims != inputs.shape[3]:\n      raise ValueError(\"The embedding dims of the rotary position embedding must match the hidden dimension of the inputs.\")\n\n    # Determine positions if not provided\n    if position is None:\n      seq_length = inputs.shape[1]\n      position = jnp.arange(seq_length, dtype=jnp.int32)[jnp.newaxis, :]\n    else:\n      position = position.astype(jnp.int32)\n\n    # Lookup the precomputed frequencies using the position indices.\n    # self.freqs_cis has shape [max_position_embeddings, half_dim] so we use jnp.take along axis 0.\n    # After indexing, shape becomes [B, S, half_dim]; we then add an axis for the heads.\n    freqs = jnp.take(self.freqs_cis, position, axis=0)  # shape: [B, S, half_dim]\n    freqs = freqs[:, :, jnp.newaxis, :]  # shape: [B, S, 1, half_dim]\n\n    if self.interleave:\n      # Inputs with interleaved format [real1, img1, real2, img2, ...] at last dimension\n      # Convert the last dimension into a complex representation.\n      # First reshape so that each pair of numbers represents the real and imaginary parts.\n      B, S, N, H = inputs.shape\n      half_dim = H // 2\n      inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n      first_half, second_half = inputs_reshaped[..., 0], inputs_reshaped[..., 1]\n    else:\n      # Inputs with concatenated format [real1, real2, ..., img1, img2, ...] at last dimension\n      first_half, second_half = jnp.split(inputs, 2, axis=-1)\n\n    inputs_complex = first_half + 1j * second_half  # shape: [B, S, N, half_dim]\n    # Apply the rotary transformation via complex multiplication.\n    rotated = inputs_complex * freqs  # shape: [B, S, N, half_dim]\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    # [real1, real2, ..., img1, img2, ...]\n    output = jnp.concatenate([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n\n    if self.attention_scaling:\n      attention_scaling = 1.0 if self.rope_factor <= 1 else (0.1 * math.log(self.rope_factor) + 1.0)\n      output = output * attention_scaling\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n    return output",
        "analysis": {
            "module_type": "yarn_rotary_embedding",
            "purpose": "Implements the YaRN (Yet another RoPE extensioN) method to apply rotary positional embeddings, designed to extend the context window of language models.",
            "input": {
                "shape": "[batch_size, sequence_length, num_heads, hidden_dim]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Validates the shape of the input tensor.",
                "Determines token positions, generating them from the sequence length if not provided.",
                "Looks up pre-computed complex frequencies from the `freqs_cis` property using the token positions.",
                "Converts the input tensor's last dimension into a complex number representation.",
                "Applies the rotary transformation via complex multiplication with the looked-up frequencies.",
                "Converts the complex result back to a real tensor by concatenating its real and imaginary parts.",
                "Optionally applies an attention scaling factor based on `rope_factor`.",
                "Optionally casts the output tensor to the specified `fprop_dtype`."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, num_heads, hidden_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "jax.numpy"
            ],
            "parameters": {
                "embedding_dims": "The dimensionality of the embeddings to which RoPE is applied.",
                "max_position_embeddings": "The maximum sequence length the model will encounter.",
                "original_max_position_embeddings": "The original maximum sequence length the base RoPE frequencies were trained for.",
                "rope_factor": "The scaling factor applied to the RoPE frequencies for context window extension.",
                "beta_fast": "Lower bound parameter for the YaRN correction, controlling interpolation for high frequencies.",
                "beta_slow": "Upper bound parameter for the YaRN correction, controlling interpolation for low frequencies.",
                "interleave": "A boolean indicating if the real and imaginary parts of the input tensor are interleaved or concatenated.",
                "attention_scaling": "A boolean indicating whether to apply an additional scaling factor to the output."
            },
            "notes": [
                "This module pre-computes the complex sinusoidal frequencies for all possible positions up to `max_position_embeddings` in the `freqs_cis` property.",
                "The implementation is based on the YaRN paper (https://arxiv.org/abs/2309.00071) and references the DeepSeek-v3 implementation."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the YarnRotaryEmbedding module with configuration parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration parameters to instance attributes.",
                        "Validates that `embedding_dims` is a multiple of 2."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `rngs` parameter is accepted for compatibility with `nnx.bridge.to_linen` but is not used."
                    ]
                },
                "freqs_cis": {
                    "purpose": "Computes and caches the complex sinusoidal frequencies (`exp(i * theta)`) for all positions up to `max_position_embeddings`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Compute base RoPE frequencies.",
                        "Calculate the YaRN correction range using `_find_correction_range`.",
                        "Create a linear ramp factor using `_linear_ramp_factor`.",
                        "Interpolate between base and scaled frequencies to get corrected frequencies.",
                        "Compute the outer product of positions and corrected frequencies.",
                        "Return the complex exponential of the result."
                    ],
                    "output": {
                        "shape": "[max_position_embeddings, embedding_dims // 2]"
                    },
                    "dependencies": [
                        "_find_correction_range",
                        "_linear_ramp_factor",
                        "jnp.outer",
                        "jnp.exp"
                    ],
                    "notes": [
                        "This is a `@property`, so it's computed on first access and then cached."
                    ]
                },
                "_find_correction_dim": {
                    "purpose": "Helper function to compute the correction dimension for a given number of rotations, based on the YaRN formula.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "float, int"
                    },
                    "processing_steps": [
                        "Calculates the dimension using a logarithmic formula."
                    ],
                    "output": {
                        "shape": "Scalar"
                    },
                    "dependencies": [
                        "math.log"
                    ],
                    "notes": [
                        "This is an internal helper method for `_find_correction_range`."
                    ]
                },
                "_find_correction_range": {
                    "purpose": "Computes the start and end dimension indices for applying the YaRN correction.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "float, int, bool"
                    },
                    "processing_steps": [
                        "Call `_find_correction_dim` for the low and high rotation bounds.",
                        "Optionally floor and ceil the results based on the `truncate` parameter.",
                        "Clamp the range to valid dimension indices."
                    ],
                    "output": {
                        "shape": "A tuple of two integers (low, high)."
                    },
                    "dependencies": [
                        "_find_correction_dim"
                    ],
                    "notes": [
                        "This is an internal helper method for `freqs_cis`."
                    ]
                },
                "_linear_ramp_factor": {
                    "purpose": "Computes a linear ramp of values from 0 to 1 over a specified dimensional range.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "float, int"
                    },
                    "processing_steps": [
                        "Generate a sequence of numbers from 0 to `dim-1`.",
                        "Linearly scale the sequence to fit between `min_val` and `max_val`.",
                        "Clip the output values to be between 0 and 1."
                    ],
                    "output": {
                        "shape": "[dim,]"
                    },
                    "dependencies": [
                        "jnp.arange",
                        "jnp.clip"
                    ],
                    "notes": [
                        "This is an internal helper method for `freqs_cis`."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the pre-computed YaRN rotary positional embeddings to an input tensor.",
                    "input": {
                        "shape": "inputs: [B, S, N, H], position: [B, S] (optional)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Generate position indices if not provided.",
                        "Look up complex frequencies from `self.freqs_cis` using position indices.",
                        "Reshape the input tensor's last dimension into a complex representation based on the `interleave` flag.",
                        "Multiply the complex input by the complex frequencies.",
                        "Convert the result back to a real tensor by concatenating real and imaginary parts.",
                        "Optionally apply attention scaling.",
                        "Optionally cast to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[B, S, N, H]"
                    },
                    "dependencies": [
                        "self.freqs_cis",
                        "jnp.take",
                        "jnp.split",
                        "jnp.concatenate"
                    ],
                    "notes": [
                        "Handles both interleaved and concatenated input formats for the head dimension."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#positional_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def positional_embedding_as_linen(*, embedding_dims: int, max_wavelength: int = _MAX_WAVELENGTH):\n  \"\"\"Initializes the PositionalEmbedding module and returns it as a Linen module.\n\n  Args:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      PositionalEmbedding,\n      embedding_dims=embedding_dims,\n      max_wavelength=max_wavelength,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "positional_embedding_factory",
            "purpose": "Initializes the NNX `PositionalEmbedding` module and wraps it to be compatible with the Flax Linen API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `PositionalEmbedding` class.",
                "Passes `embedding_dims`, `max_wavelength`, and `metadata_fn` to the wrapper function."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "PositionalEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "This function serves as a factory to create a Linen-compatible version of the `PositionalEmbedding` NNX module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#PositionalEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class PositionalEmbedding(nnx.Module):\n  \"\"\"A layer that adds sinusoidal positional embeddings to the input.\n\n  Attributes:\n    embedding_dims: The dimension of the embeddings.\n    max_wavelength: The maximum wavelength for the sinusoidal positional embeddings.\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  \"\"\"\n\n  embedding_dims: int\n  max_wavelength: int = _MAX_WAVELENGTH\n\n  rngs: nnx.Rngs = None  # Not used in PositionalEmbedding but passed in by nnx.bridge.to_linen\n\n  def __call__(\n      self,  # pytype: disable=signature-mismatch  # overriding-parameter-count-checks\n      input_embedding: jax.Array,\n      position: jax.Array,\n  ) -> jax.Array:\n    num_timescales = self.embedding_dims // 2\n    log_timescale_increment = jnp.log(float(self.max_wavelength)) / jnp.maximum(\n        jnp.asarray(num_timescales, dtype=jnp.float32) - 1, 1\n    )\n    inv_timescales = jnp.exp(jnp.arange(num_timescales, dtype=jnp.float32) * -log_timescale_increment)\n    position = position[:, :, jnp.newaxis]\n    inv_timescales = inv_timescales[jnp.newaxis, jnp.newaxis, :]\n    scaled_time = position * inv_timescales\n    signal = jnp.concatenate([jnp.sin(scaled_time), jnp.cos(scaled_time)], axis=-1)\n    # signal = jnp.pad(signal, [[0, jnp.mod(self.embedding_dims, 2)]])\n    position_embedding = signal.astype(jnp.float32)\n    return input_embedding + position_embedding",
        "analysis": {
            "module_type": "positional_embedding",
            "purpose": "A layer that computes sinusoidal positional embeddings and adds them to an input tensor.",
            "input": {
                "shape": "N/A (See __call__ method)",
                "dtype": "N/A (See __call__ method)"
            },
            "processing_steps": [
                "N/A (See __call__ method)"
            ],
            "output": {
                "shape": "N/A (See __call__ method)"
            },
            "dependencies": [
                "nnx.Module",
                "jax.numpy"
            ],
            "parameters": {
                "embedding_dims": "The dimension of the embeddings.",
                "max_wavelength": "The maximum wavelength for the sinusoidal positional embeddings."
            },
            "notes": [
                "The `rngs` attribute is passed by `nnx.bridge.to_linen` but is not used in this module.",
                "This class implements the standard sinusoidal positional encoding formula from the 'Attention Is All You Need' paper."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Calculates sinusoidal positional embeddings based on input positions and adds them to the input token embeddings.",
                    "input": {
                        "shape": "{'input_embedding': '[batch_size, sequence_length, embedding_dims]', 'position': '[batch_size, sequence_length]'}",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Calculate inverse timescales based on `embedding_dims` and `max_wavelength`.",
                        "Expand dimensions of `position` and `inv_timescales` for broadcasting.",
                        "Compute `scaled_time` by multiplying `position` with `inv_timescales`.",
                        "Generate a `signal` by concatenating the sine and cosine of `scaled_time`.",
                        "Cast the signal to float32 to create the `position_embedding`.",
                        "Return the sum of `input_embedding` and `position_embedding`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dims]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "The shape of the output tensor is identical to the shape of the `input_embedding` tensor."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#llama_vision_rotary_embedding_as_linen",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "def llama_vision_rotary_embedding_as_linen(\n    *,\n    image_size: int,\n    patch_size: int,\n    hidden_size: int,\n    num_attention_heads: int,\n    rope_theta: float = 10000.0,\n    cast_as_fprop_dtype: bool = True,\n    fprop_dtype: DType = jnp.bfloat16,\n    name: str | None = None,\n):\n  \"\"\"Initializes the LlamaVisionRotaryEmbedding module and returns it as a Linen module.\n\n  Args:\n    image_size: The size of the input image.\n    patch_size: The size of the image patches.\n    hidden_size: The size of the hidden dimension.\n    num_attention_heads: The number of attention heads.\n    rope_theta: The base theta value for the frequency computation.\n    cast_as_fprop_dtype: Whether to cast the output to the fprop dtype.\n    fprop_dtype: The dtype of the output.\n    name: The name of the Linen module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      LlamaVisionRotaryEmbedding,\n      image_size=image_size,\n      patch_size=patch_size,\n      hidden_size=hidden_size,\n      num_attention_heads=num_attention_heads,\n      rope_theta=rope_theta,\n      cast_as_fprop_dtype=cast_as_fprop_dtype,\n      fprop_dtype=fprop_dtype,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n  )",
        "analysis": {
            "module_type": "llama_vision_rotary_embedding_factory",
            "purpose": "A factory function that initializes the LlamaVisionRotaryEmbedding NNX module and wraps it to be used as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `LlamaVisionRotaryEmbedding` NNX module into a Linen-compatible module, passing along all configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "LlamaVisionRotaryEmbedding",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "image_size": "The size of the input image.",
                "patch_size": "The size of the image patches.",
                "hidden_size": "The size of the hidden dimension.",
                "num_attention_heads": "The number of attention heads.",
                "rope_theta": "The base theta value for the frequency computation.",
                "fprop_dtype": "The data type for the forward pass computation in the returned module."
            },
            "notes": [
                "This function serves as a bridge to use an NNX-defined module within a Linen-based model architecture.",
                "The actual rotary embedding logic is contained within the `LlamaVisionRotaryEmbedding` class, which this function instantiates and wraps."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/embeddings.py#LlamaVisionRotaryEmbedding",
        "file_path": "src/MaxText/layers/embeddings.py",
        "code_block": "class LlamaVisionRotaryEmbedding(nnx.Module):\n  \"\"\"Rotary position embedding for Llama4 vision encoder.\n\n  Based on Pytorch Reference\n  https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n  This implementation follows the Llama4 vision encoder's rotary embedding approach,\n  which uses 2D coordinates (x, y) to generate rotary position embeddings.\n\n  Attributes:\n    image_size: int size of the input image\n    patch_size: int size of the image patches\n    hidden_size: int size of the hidden dimension\n    num_attention_heads: int number of attention heads\n    rope_theta: float = 10000.0 base theta value for the frequency computation\n    cast_as_fprop_dtype: bool = True whether to cast the output to the fprop dtype\n    fprop_dtype: DType = jnp.bfloat16 the dtype of the output\n    rngs: RNG state passed in by nnx.bridge.to_linen, not used in this module.\n  Returns:\n    jax.Array of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n    where vision rotary position embeddings are applied.\n  \"\"\"\n\n  image_size: int\n  patch_size: int\n  hidden_size: int\n  num_attention_heads: int\n  rope_theta: float = 10000.0\n  cast_as_fprop_dtype: bool = True\n  fprop_dtype: DType = jnp.bfloat16\n  # Not used in LlamaVisionRotaryEmbedding but passed in by nnx.bridge.to_linen.\n  # TODO: Remove when bridge no longer needed\n  rngs: nnx.Rngs = None\n\n  @property\n  def freqs_cis(self):\n    \"\"\"Frequencies for rotary embedding.\"\"\"\n    idx = self.image_size // self.patch_size\n    img_idx = jnp.arange(idx**2, dtype=jnp.int32).reshape(idx**2, 1)\n    img_idx = jnp.concatenate([img_idx, img_idx[:1]], axis=0)\n    img_idx = img_idx.at[-1, -1].set(-2)  # ID_CLS_TOKEN\n\n    # Get 2D coordinates\n    frequencies_x = img_idx % idx  # x coordinates\n    frequencies_y = img_idx // idx  # y coordinates\n\n    # Compute frequency dimensions\n    freq_dim = self.hidden_size // self.num_attention_heads // 2\n    rope_freq = 1.0 / (self.rope_theta ** (jnp.arange(0, freq_dim, 2)[: (freq_dim // 2)].astype(jnp.float32) / freq_dim))\n\n    # Compute frequencies for x and y coordinates\n    freqs_x = (frequencies_x + 1)[..., None] * rope_freq[None, None, :]\n    freqs_y = (frequencies_y + 1)[..., None] * rope_freq[None, None, :]\n\n    # Interleave x and y frequencies\n    freqs_x = jnp.repeat(freqs_x, 2, axis=-1)\n    freqs_y = jnp.repeat(freqs_y, 2, axis=-1)\n\n    # Combine frequencies\n    freqs = jnp.concatenate([freqs_x, freqs_y], axis=-1).astype(jnp.float32)\n    freqs = freqs[..., ::2]\n\n    # Mask out invalid positions\n    freqs = jnp.where(img_idx.reshape(-1, 1, 1) < 0, 0, freqs)\n    # Convert to complex representation\n    return jnp.exp(1j * freqs)\n\n  def __call__(self, inputs: Array, position: None | Array = None) -> Array:\n    \"\"\"Applies rotary embeddings to the input tensor for Llama4 vision encoder.\n\n    Args:\n      inputs: Input tensor of shape [batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]\n\n    Returns:\n      Tensor with rotary embeddings applied, maintaining the same shape as input.\n    \"\"\"\n    if len(inputs.shape) != 4:\n      raise ValueError(\n          \"\"\"Input is assumed to be a rank 4 tensor of shape [batch_size_times_tiles, num_patches_incl_cls, \n          num_heads, head_dim].\"\"\"\n      )\n\n    # Reshape inputs to complex representation\n    B, S, N, H = inputs.shape\n    half_dim = H // 2\n\n    # Convert the last dimension into a complex representation.\n    # First reshape so that each pair of numbers represents the real and imaginary parts.\n    inputs_reshaped = inputs.reshape(B, S, N, half_dim, 2)\n    inputs_complex = inputs_reshaped[..., 0] + 1j * inputs_reshaped[..., 1]\n\n    # Reshape freqs_ci for broadcasting\n    freqs_ci = self.freqs_cis[jnp.newaxis, :, :, :]\n\n    # Apply rotary transformation\n    rotated = inputs_complex * freqs_ci\n\n    # Convert the complex result back to a real tensor.\n    # Split the complex number into its real and imaginary parts.\n    rotated_real = jnp.stack([jnp.real(rotated), jnp.imag(rotated)], axis=-1)\n    output = rotated_real.reshape(B, S, N, H)\n\n    if self.cast_as_fprop_dtype:\n      output = output.astype(self.fprop_dtype)\n\n    return output",
        "analysis": {
            "module_type": "llama_vision_rotary_embedding",
            "purpose": "Implements rotary position embeddings for a vision encoder, generating embeddings based on 2D image patch coordinates.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Pre-computes complex frequency sinusoids (`freqs_cis`) based on 2D patch coordinates.",
                "In the forward pass (`__call__`), it converts the input tensor to a complex representation.",
                "Applies the rotary transformation by multiplying the complex input with the pre-computed frequencies.",
                "Converts the result back to a real tensor."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx",
                "jax.numpy"
            ],
            "parameters": {
                "image_size": "The size of the input image.",
                "patch_size": "The size of the image patches.",
                "hidden_size": "The size of the hidden dimension.",
                "num_attention_heads": "The number of attention heads.",
                "rope_theta": "The base theta value for the frequency computation.",
                "cast_as_fprop_dtype": "A boolean indicating whether to cast the output to the fprop dtype.",
                "fprop_dtype": "The data type for the output if casting is enabled."
            },
            "notes": [
                "This implementation is specific to the Llama4 vision encoder.",
                "It uses 2D coordinates of image patches to generate the rotary embeddings.",
                "Includes special handling for a CLS token by masking its position.",
                "The rotary embeddings are pre-computed in the `freqs_cis` property."
            ],
            "methods": {
                "freqs_cis": {
                    "purpose": "Pre-computes the complex rotary frequency sinusoids (cis) based on 2D grid coordinates of image patches.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the number of patches per side of the image.",
                        "Generate a 1D index for all patches and append a special index for a CLS token.",
                        "Convert the 1D index into 2D coordinates (x and y).",
                        "Compute the base frequency dimensions using `rope_theta`.",
                        "Calculate frequencies for x and y coordinates.",
                        "Interleave and combine x and y frequencies.",
                        "Mask out invalid positions (e.g., the CLS token) to produce zero frequencies.",
                        "Convert the final frequencies to a complex representation using `jnp.exp(1j * freqs)`."
                    ],
                    "output": {
                        "shape": "[num_patches_incl_cls, 1, head_dim // 2]"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "This is a property, not a method that takes explicit arguments.",
                        "The CLS token is assigned a special ID (-2) to facilitate masking."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the pre-computed rotary embeddings to an input tensor.",
                    "input": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate that the input tensor has 4 dimensions.",
                        "Reshape the last dimension of the input into a complex representation.",
                        "Retrieve the pre-computed complex frequencies from `self.freqs_cis`.",
                        "Broadcast and multiply the complex input with the complex frequencies to apply the rotation.",
                        "Convert the rotated complex tensor back to a real tensor by stacking its real and imaginary parts.",
                        "Reshape the output back to the original input shape.",
                        "Optionally cast the output tensor to `fprop_dtype`."
                    ],
                    "output": {
                        "shape": "[batch_size_times_tiles, num_patches_incl_cls, num_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.freqs_cis"
                    ],
                    "notes": [
                        "The `position` argument is present in the method signature but is not used in the implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations(\n    inputs: jax.Array,\n    sort_indices: jax.Array,\n    use_custom_vjp: bool,\n) -> jax.Array:\n  \"\"\"Sort activations by `sort_indices`.\n\n  If `use_custom_vjp=True`, then we use a custom backward pass that\n  reverses the sort order. Specifically, this unsort operation is simply a sort\n  with `jnp.argsort(sort_indices)` as the sort indices. This is only needed in\n  the case where the compiler generates a less efficient backward pass op.\n\n  Note that `use_custom_vjp=True` assumes that `sort_indices` is a permutation\n  of `jnp.arange(inputs.shape[0])`.\n\n  Args:\n    inputs: `(tokens, ...)`-shaped array of input activations to sort.\n    sort_indices: `(tokens,)`-shaped array containing the sort order.\n    use_custom_vjp: Whether to use the explicit backward pass.\n\n  Returns:\n    `(tokens, ...)`-shaped array of input activations sorted by `sort_indices`.\n  \"\"\"\n  assert inputs.shape[0] == sort_indices.shape[0]\n\n  with jax.named_scope(\"sort_activations\"):\n    if use_custom_vjp:\n      return _sort_activations_custom(inputs, sort_indices)\n    return inputs[sort_indices, ...]",
        "analysis": {
            "functionality": "Sorts an input tensor along its first dimension based on provided indices. It can optionally use a custom backward pass (VJP) for potentially better performance during training.",
            "usage": "Call this function with an input tensor `inputs` of shape `(tokens, ...)`, a 1D tensor `sort_indices` of shape `(tokens,)`, and a boolean `use_custom_vjp`. It returns a new tensor of the same shape as `inputs`, with its first dimension sorted according to `sort_indices`."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom(inputs: jax.Array, sort_indices: jax.Array) -> jax.Array:\n  \"\"\"Sort functions with custom vjp.\"\"\"\n  return inputs[sort_indices, ...]",
        "analysis": {
            "module_type": "custom_vjp_sorting_forward_pass",
            "purpose": "Sorts the first dimension of an input tensor according to a given set of indices, serving as the forward pass for a custom JAX gradient.",
            "input": {
                "shape": "inputs: [num_tokens, ...], sort_indices: [num_tokens]",
                "dtype": "jax.Array"
            },
            "processing_steps": [
                "Performs advanced indexing on the `inputs` tensor using `sort_indices` to reorder its first dimension."
            ],
            "output": {
                "shape": "[num_tokens, ...]"
            },
            "dependencies": [
                "jax.custom_vjp",
                "_sort_activations_custom_fwd",
                "_sort_activations_custom_bwd"
            ],
            "parameters": {},
            "notes": [
                "This function is decorated with `@jax.custom_vjp` in the full file context, defining a custom backward pass (`_sort_activations_custom_bwd`) that performs an unsorting operation.",
                "The `...` (ellipsis) indicates that all dimensions other than the first are preserved."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_fwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_fwd(inputs: jax.Array, sort_indices: jax.Array\n) -> tuple[jax.Array, jax.Array]:\n  \"\"\"Forward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  return _sort_activations_custom(inputs, sort_indices), sort_indices",
        "analysis": {
            "module_type": "custom_vjp_forward_pass",
            "purpose": "Performs the forward pass for a custom JAX VJP, sorting an input array and returning the sorted array along with the sorting indices for the backward pass.",
            "input": {
                "shape": "inputs: [N, ...], sort_indices: [N]",
                "dtype": "inputs: jax.Array (typically float), sort_indices: jax.Array (typically integer)"
            },
            "processing_steps": [
                "Calls `_sort_activations_custom` to sort the `inputs` array using `sort_indices`.",
                "Returns a tuple containing the sorted array and the original `sort_indices` which will be used as residuals in the backward pass."
            ],
            "output": {
                "shape": "A tuple containing two arrays: (sorted_inputs: [N, ...], sort_indices: [N])"
            },
            "dependencies": [
                "_sort_activations_custom"
            ],
            "parameters": {},
            "notes": [
                "This function serves as the forward pass (`fwd`) for the `jax.custom_vjp` applied to the `_sort_activations_custom` function.",
                "The returned `sort_indices` are used as residuals by the corresponding backward pass function `_sort_activations_custom_bwd`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#_sort_activations_custom_bwd",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def _sort_activations_custom_bwd(residuals: jax.Array, grads: jax.Array\n) -> tuple[jax.Array, None]:\n  \"\"\"Backward pass of the custom vjp for `_sort_activations()`.\"\"\"\n  sort_indices = residuals\n  return _sort_activations_custom(grads, jnp.argsort(sort_indices)), None",
        "analysis": {
            "module_type": "custom_vjp_backward_pass",
            "purpose": "Implements the backward pass for the `_sort_activations_custom` JAX custom VJP, unsorting the gradients to match the original input order.",
            "input": {
                "shape": "residuals (sort_indices): [num_tokens], grads: [num_tokens, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assign the `residuals` (the `sort_indices` from the forward pass) to a local variable.",
                "Compute the 'unsort' indices by applying `jnp.argsort` to the `sort_indices`.",
                "Apply the 'unsort' permutation to the incoming gradients (`grads`) by calling `_sort_activations_custom`.",
                "Return the permuted gradients and `None` (as the gradient for `sort_indices`)."
            ],
            "output": {
                "shape": "A tuple containing the unsorted gradients with shape [num_tokens, ...] and None."
            },
            "dependencies": [
                "_sort_activations_custom",
                "jax.numpy.argsort"
            ],
            "parameters": {},
            "notes": [
                "This function is registered as the backward pass for the `_sort_activations_custom` function using `defvjp`.",
                "It effectively reverses the permutation applied in the forward pass to correctly propagate gradients.",
                "The gradient with respect to the `sort_indices` input of the forward function is defined as `None`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#random_routing",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def random_routing(rng_key, gate_logits, num_experts_per_tok):\n  \"\"\"Performs random routing of tokens to experts.\n\n  Args:\n    rng_key: A JAX PRNGKey for randomness.\n    gate_logits: A JAX array of shape (batch_size, sequence_length, num_experts)\n      representing the logits for each expert.\n    num_experts_per_tok: The number of experts to select for each token.\n\n  Returns:\n    A tuple containing:\n      - top_k_indices: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the indices of the selected experts for each\n                       token.\n      - top_k_weights: JAX array of shape (batch_size, sequence_length,\n      num_experts_per_tok)\n                       representing the weights for the selected experts.\n  \"\"\"\n  bs, seq_len, num_experts = gate_logits.shape\n  indices = jnp.arange(num_experts).repeat(bs * seq_len)\n  selected_num = bs * seq_len * num_experts_per_tok\n  top_k_indices = jax.random.choice(rng_key, indices, shape=(selected_num,)).reshape(bs, seq_len, num_experts_per_tok)\n  top_k_weights = jnp.take_along_axis(gate_logits, top_k_indices, axis=-1)\n  return top_k_weights, top_k_indices",
        "analysis": {
            "module_type": "random_routing",
            "purpose": "Performs random routing of tokens to experts by selecting a specified number of experts for each token, irrespective of their gate logit values.",
            "input": {
                "shape": "gate_logits: [batch_size, sequence_length, num_experts]",
                "dtype": "float (JAX array)"
            },
            "processing_steps": [
                "Get shape dimensions from the input `gate_logits`.",
                "Create a flat array of expert indices from 0 to `num_experts-1`, repeated for each token in the batch.",
                "Use `jax.random.choice` to randomly select `num_experts_per_tok` expert indices for each token.",
                "Reshape the randomly selected indices to `[batch_size, sequence_length, num_experts_per_tok]`.",
                "Use `jnp.take_along_axis` to gather the original logits corresponding to the selected indices, treating them as weights.",
                "Return the gathered weights and the selected indices."
            ],
            "output": {
                "shape": "A tuple of (top_k_weights, top_k_indices), where both tensors have the shape [batch_size, sequence_length, num_experts_per_tok]."
            },
            "dependencies": [
                "jax.random.choice",
                "jax.numpy.arange",
                "jax.numpy.repeat",
                "jax.numpy.take_along_axis"
            ],
            "parameters": {
                "num_experts_per_tok": "The number of experts to select for each token."
            },
            "notes": [
                "The selection of experts is purely random and does not depend on the values in `gate_logits`.",
                "The returned `top_k_weights` are the original logit values of the randomly chosen experts.",
                "This function requires a JAX PRNGKey (`rng_key`) for reproducible randomness."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#GateLogit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class GateLogit(nnx.Module):\n  \"\"\"A layer used to compute gate logits, allowing to return the pre bias values for DeepSeek routing.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Union[Iterable[int], int],\n      out_features_shape: Union[Iterable[int], int],\n      model_name: str,\n      rngs: nnx.Rngs,\n      axis: Union[Iterable[int], int] = -1,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: Tuple[Optional[str], ...] = (),\n      use_bias: bool = False,\n      score_func: str = \"\",\n      quant: Optional[quantizations.AqtQuantization] = None,\n      matmul_precision: str = \"default\",\n  ):\n    \"\"\"Initializes the GateLogit module.\n\n    Attributes:\n      in_features_shape: The shape of the input features.\n      out_features_shape: The shape of the output features, typically the number of experts.\n      model_name: The name of the model.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      axis: The axis or axes over transformation is applied.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      use_bias: Whether to add learnable bias in gate logit scores. When enabled,\n        this bias aids expert load balancing (like in DeepSeek V3), and is not\n        part of the loss calculation.\n      score_func: Scoring function for output normalization before applying bias.\n      quant: The quantization configuration. If None, no quantization is applied.\n      matmul_precision: The precision level for the matrix multiplication.\n    \"\"\"\n    self.in_features_shape = linears.canonicalize_tuple(in_features_shape)\n    self.out_features_shape = linears.canonicalize_tuple(out_features_shape)\n    self.model_name = model_name\n    self.axis = linears.canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.use_bias = use_bias\n    self.score_func = score_func\n    self.quant = quant\n    self.matmul_precision = matmul_precision\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: jax.Array, _initializing: bool = False) -> Tuple[jax.Array, Optional[jax.Array]]:\n\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = linears.normalize_axes(self.axis, inputs.ndim)\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n    kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(norm_axis)))\n    output = linears._compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n    pre_bias_logits = None\n\n    if self.score_func:\n      output = linears._convert_to_activation_function(self.score_func)(output)\n      if self.model_name.startswith(\"deepseek3\"):\n        pre_bias_logits = output\n\n    if self.use_bias:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output, pre_bias_logits",
        "analysis": {
            "module_type": "gate_logit_layer",
            "purpose": "A linear layer to compute gate logits for Mixture-of-Experts (MoE) routing, with an option to return pre-bias values for specific model architectures like DeepSeek.",
            "input": {
                "shape": "N/A (Class constructor)",
                "dtype": "N/A (Class constructor)"
            },
            "processing_steps": [
                "Canonicalizes input/output feature shapes and axis.",
                "Initializes a kernel parameter `self.kernel` of shape `in_features_shape + out_features_shape`.",
                "Optionally initializes a bias parameter `self.bias` if `use_bias` is True.",
                "If quantization is enabled, initializes a quantized dot-general operator and performs a dummy forward pass for initialization."
            ],
            "output": {
                "shape": "N/A (Class constructor)"
            },
            "dependencies": [
                "flax.nnx",
                "jax.numpy",
                "MaxText.layers.linears",
                "MaxText.layers.quantizations",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features.",
                "out_features_shape": "The shape of the output features, typically the number of experts.",
                "model_name": "The name of the model, used for special conditional logic (e.g., 'deepseek3').",
                "use_bias": "A boolean indicating whether to add a learnable bias to the output logits.",
                "score_func": "An optional scoring function (e.g., 'sigmoid') applied to the output before adding bias.",
                "quant": "An optional configuration object for applying quantization to the matrix multiplication."
            },
            "notes": [
                "This module is implemented using Flax's NNX API.",
                "It has specific logic to handle DeepSeek-style routing by returning pre-bias logits when `model_name` starts with 'deepseek3'."
            ],
            "methods": {
                "quant_dot_general": {
                    "purpose": "A property to retrieve the quantized dot-general operator if it exists.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if a quantized operator was initialized.",
                        "Returns the operator instance using `getattr` or returns None."
                    ],
                    "output": {
                        "shape": "Returns an `nnx_wrappers.ToNNX` object or `None`."
                    },
                    "dependencies": [
                        "MaxText.layers.nnx_wrappers.ToNNX"
                    ],
                    "notes": [
                        "This is a read-only property for accessing the dynamically named quantization module."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass to compute the gate logits.",
                    "input": {
                        "shape": "[..., *in_features_shape] (e.g., [batch_size, sequence_length, hidden_dim])",
                        "dtype": "The data type specified during initialization (e.g., float32)."
                    },
                    "processing_steps": [
                        "Casts the input tensor to the expected dtype.",
                        "Retrieves the kernel weights.",
                        "Computes a dot product between the input and the kernel using `linears._compute_dot_general_nnx`.",
                        "Initializes `pre_bias_logits` to None.",
                        "If `score_func` is specified, applies the corresponding activation function.",
                        "If `model_name` is 'deepseek3', captures the result after the score function as `pre_bias_logits`.",
                        "If `use_bias` is True, adds the bias term to the output.",
                        "Returns the final output logits and the optional `pre_bias_logits`."
                    ],
                    "output": {
                        "shape": "A tuple containing (output_logits, pre_bias_logits). The `output_logits` shape is [..., *out_features_shape] (e.g., [batch_size, sequence_length, num_experts]). `pre_bias_logits` is either an array of the same shape or None."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "MaxText.layers.linears",
                        "MaxText.layers.quantizations"
                    ],
                    "notes": [
                        "The second element of the returned tuple, `pre_bias_logits`, is only non-None for specific model configurations."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedMoE(nnx.Module):\n  \"\"\"Implements a routed MoE block.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      num_experts: int,\n      num_experts_per_tok: int,\n      mesh: jax.sharding.Mesh,\n      kernel_init: attentions.NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      intermediate_dim: int = 2048,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"Initializes the RoutedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      num_experts: Number of experts.\n      num_experts_per_tok: Number of experts for each token.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      intermediate_dim: Intermediate dimension of MoE.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.num_experts = num_experts\n    self.num_experts_per_tok = num_experts_per_tok\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.intermediate_dim = intermediate_dim\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n\n    if self.config.fsdp_shard_on_exp:\n    # special sharding for dsv3\n      self.wi_kernel_axes = (\"embed_no_exp\", None, \"mlp\")\n      self.wo_kernel_axes = (\"embed_no_exp\", \"mlp\", None)\n    else:\n      self.wi_kernel_axes = (\"exp\", \"embed_no_exp\", \"mlp\")\n      self.wo_kernel_axes = (\"exp\", \"mlp\", \"embed_no_exp\")\n\n    self.gate = GateLogit(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.num_experts,\n        model_name=self.config.model_name,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        kernel_init=self.kernel_init,\n        kernel_axes=self.kernel_axes,\n        use_bias=self.config.routed_bias,\n        score_func=self.config.routed_score_func,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # pylint: disable=protected-access\n    self.activation_fn = linears._convert_to_activation_function(\n        self.config.mlp_activations[0]\n    )\n\n    kernel_in_axis = np.arange(1)\n    kernel_out_axis = np.arange(1, 2)\n\n    if quantizations.in_serve_mode(self.quant):\n      # During aqt convert state we delete kernel weight from params to save\n      # memory. Instead they are retrieved from the tensors stored in the 'aqt'\n      # collection.\n      self.wi_0 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wi_1 = jnp.zeros((num_experts, self.config.emb_dim, intermediate_dim))\n      self.wo = jnp.zeros((num_experts, intermediate_dim, self.config.emb_dim))\n    else:\n      self.wi_0 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wi_1 = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (num_experts, self.config.emb_dim, intermediate_dim),\n              weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wi_kernel_axes,\n      )\n      self.wo = nnx.Param(\n          self.kernel_init(\n              self.rngs.params(),\n              (self.num_experts, self.intermediate_dim, self.config.emb_dim),\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.wo_kernel_axes,\n      )\n\n    if self.config.mlp_bias:\n      wi_bias_axes = (\"exp\", \"activation_mlp\")\n      wo_bias_axes = (\"exp\", \"activation_embed\")\n      wi_bias_shape = (self.num_experts, self.intermediate_dim)\n      wo_bias_shape = (self.num_experts, self.config.emb_dim)\n      self.wi_0_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wi_1_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wi_bias_shape, self.weight_dtype),\n          sharding=wi_bias_axes,\n      )\n      self.wo_bias = nnx.Param(\n          default_bias_init(self.rngs.params(), wo_bias_shape, self.weight_dtype),\n          sharding=wo_bias_axes,\n      )\n    else:\n      self.wi_0_bias = None\n      self.wi_1_bias = None\n      self.wo_bias = None\n\n  def get_expert_parallelism_size(self):\n    return self.mesh.shape[\"expert\"]\n\n  def get_tensor_parallelism_size(self):\n    return self.mesh.shape[\"tensor\"]\n\n  def get_tensor_transpose_parallelism_size(self):\n    return self.mesh.shape[\"tensor_transpose\"]\n\n  def get_context_autoregressive_parallelism_size(self):\n    return self.mesh.shape[\"context_autoregressive\"]\n\n  def get_topk(self, gate_logits, pre_bias_logits, rngs=None):\n    \"\"\"get topk.\"\"\"\n    # shape of top_k_weights & top_k_indices:\n    # (batch, sequence, num_experts_per_tok).\n    if self.config.use_random_routing:\n      if rngs is None:\n        raise ValueError(\"The random key cannot be None for random routing.\")\n      # Re-use the 'dropout' RNG stream to ensure random routing\n      rng = rngs.dropout()\n      top_k_weights, top_k_indices = random_routing(rng, gate_logits, self.num_experts_per_tok)\n      return top_k_weights, top_k_indices\n\n    if self.config.model_name.startswith(\"deepseek3\"):\n      top_k_weights, top_k_indices = self.deepseek_routing(gate_logits, pre_bias_logits)\n    else:\n      top_k_weights, top_k_indices = jax.lax.top_k(gate_logits, self.num_experts_per_tok)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.DEEPSEEK:\n      top_k_weights = self.deepseek_scale_weights(top_k_weights)\n    elif self.config.decoder_block != ctypes.DecoderBlockType.LLAMA4:\n      top_k_weights = jax.nn.softmax(top_k_weights.astype(jnp.float32), axis=-1).astype(self.dtype)\n\n    # This is the Qwen3-specific normalization of router weights.\n    if self.config.norm_topk_prob:\n      top_k_weights /= top_k_weights.sum(axis=-1, keepdims=True)\n\n    return top_k_weights, top_k_indices\n\n    \n  def deepseek_scale_weights(self, weights):\n    \"\"\"Scales weights according to DeepSeek's v3 reference implementation.\"\"\"\n    # https://github.com/deepseek-ai/DeepSeek-V3/blob/2f7b80eecebf3d1c84da5a0d465f6639ea175012/inference/model.py#L592-L594.\n    if self.config.routed_score_func == \"sigmoid\":\n      weights /= weights.sum(-1, keepdims=True)\n    weights *= self.config.routed_scaling_factor\n    return weights\n\n  def expert_group_mask(self, gate_logits: jax.Array) -> jax.Array:\n    \"\"\"Returns a mask that selects only the top-k groups of experts.\n\n    Groups of experts are selected based on the sum of the top-2 expert scores\n    for each group.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n\n    Returns:\n      Array of shape `(batch, seq, num_experts)` that is 1 for experts in the\n      top-k groups and 0 elsewhere.\n    \"\"\"\n    # Find top groups based on each group's top-2 expert scores, where\n    # `scores_grouped.shape =\n    # (batch * seq, n_routing_groups, experts_per_group)`.\n    scores_grouped = jnp.reshape(\n        gate_logits,\n        gate_logits.shape[:-1] + (self.config.n_routing_groups, -1),\n    )\n    top2_in_group_vals, _ = jax.lax.top_k(scores_grouped, k=2)\n    group_scores = jnp.sum(jnp.astype(top2_in_group_vals, jnp.float32), axis=-1)\n    _, group_idx = jax.lax.top_k(group_scores, k=self.config.topk_routing_group)\n\n    # Mask selected groups so that only those experts are considered.\n    group_mask = jax.nn.one_hot(group_idx, num_classes=self.config.n_routing_groups, dtype=jnp.float32)\n    group_mask = jnp.sum(group_mask, axis=-2)\n\n    # Apply masks and get top-k indices.\n    score_mask_expanded = jnp.broadcast_to(\n        group_mask[..., None],\n        group_mask.shape + (self.num_experts // self.config.n_routing_groups,),\n    )\n    return jnp.reshape(\n        score_mask_expanded,\n        score_mask_expanded.shape[:-2] + (self.num_experts,),\n    )\n\n  def deepseek_routing(self, gate_logits: jax.Array, pre_bias_logits: jax.Array) -> tuple[jax.Array, jax.Array]:\n    \"\"\"DeepSeek routing logit.\n\n    If the configuration does not specify routing groups (`n_routing_groups` is\n    -1), we use a standard top-k routing mechanism. Otherwise, we force all\n    selected experts to be from the a subset of the highest rated expert groups.\n\n    The selection process uses post_bias logits, while the return weights use\n    pre_bias logits.\n\n    Args:\n      gate_logits: Array of shape `(batch, seq, num_experts)`.\n      pre_bias_logits: Array of shape `(batch, seq,num_experts)`.\n\n    Returns:\n      - top_k_weights: `(batch, seq, num_experts_per_tok)` array of weight values for\n        each selected expert.\n      - top_k_indices: `(batch, seq, num_experts_per_tok)` array of indices\n        identifying the selected experts for each token.\n    \"\"\"\n    expert_mask = 1 if self.config.n_routing_groups == -1 else self.expert_group_mask(gate_logits)\n    _, top_k_indices = jax.lax.top_k(\n        jnp.where(expert_mask > 0, gate_logits, -jnp.inf),\n        k=self.num_experts_per_tok,\n    )\n    top_k_weights = jnp.take_along_axis(pre_bias_logits, top_k_indices, axis=-1)\n    return top_k_weights, top_k_indices\n\n  def apply_ffn_activation(self, layer_w0, layer_w1):\n    \"\"\"Applies FFN activation function.\"\"\"\n    with jax.named_scope(\"ffn_act\"):\n      if self.config.decoder_block == ctypes.DecoderBlockType.GPT_OSS:\n        layer_w0 = jnp.clip(layer_w0, a_min=None, a_max=self.config.mlp_activations_limit)\n        layer_w1 = jnp.clip(layer_w1, a_min=-self.config.mlp_activations_limit, a_max=self.config.mlp_activations_limit)\n        layer_act = self.activation_fn(layer_w0 * 1.702)\n        glu = jnp.multiply(layer_w0, layer_act)\n        intermediate_layer = jnp.multiply(glu, (layer_w1 + 1))\n      else:\n        layer_act = self.activation_fn(layer_w0)\n        intermediate_layer = jnp.multiply(layer_act, layer_w1)\n      return intermediate_layer.astype(self.dtype)\n\n  def permute(self, inputs, gate_logits, pre_bias_logits, use_custom_sort_vjp=True, rngs=None, roll_to_expert_id=None):\n    \"\"\"Permute tokens to group by expert to fit gmm call.\"\"\"\n    # reshape inputs (batch, sequence, emb) to (batch * sequence, emb)\n    inputs_shape = inputs.shape\n    bsz_times_seq_len = inputs_shape[0] * inputs_shape[1]\n    inputs_2d = jnp.reshape(inputs, (bsz_times_seq_len, inputs_shape[2]))\n    weights, selected_experts = self.get_topk(gate_logits, pre_bias_logits, rngs)\n\n    if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n      # weights will be of shape (batch_size, seq_len, num_experts_per_tok)\n      router_scores = jax.nn.sigmoid(weights.astype(jnp.float32))  # weights are top_k_weights here\n      # Squeeze router_scores to (batch_size * seq_len, num_experts_per_tok)\n      inputs_2d = inputs_2d * router_scores.reshape(bsz_times_seq_len, -1)\n\n    flatten_selected_experts = jnp.ravel(selected_experts)\n    if roll_to_expert_id is not None:\n      flatten_selected_experts = (flatten_selected_experts - roll_to_expert_id) % self.num_experts\n    sorted_selected_experts = jnp.argsort(flatten_selected_experts)\n    # sort inputs for number of selected experts\n    replicated_inputs_2d = jnp.repeat(inputs_2d, self.num_experts_per_tok, axis=0)\n    sorted_inputs = _sort_activations(replicated_inputs_2d, sorted_selected_experts, use_custom_sort_vjp).astype(self.dtype)\n    group_size = jnp.bincount(flatten_selected_experts, length=self.num_experts)\n    # Return the experts for each sorted input.\n    expert_indices = jnp.arange(self.num_experts)\n    sorted_experts = jnp.repeat(\n        expert_indices,\n        repeats=group_size,\n        total_repeat_length=flatten_selected_experts.shape[0],\n    )\n    return (\n        sorted_inputs,\n        sorted_selected_experts,\n        weights,\n        group_size,\n        sorted_experts,\n    )\n\n  def unpermute(\n      self,\n      intermediate,\n      sorted_selected_experts,\n      weights,\n      batch_size,\n      sequence_length,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Unpermute tokens to original order and combine weights.\"\"\"\n\n    unsort_intermediate = _sort_activations(\n        intermediate,\n        jnp.argsort(sorted_selected_experts),\n        use_custom_sort_vjp,\n    )\n    reshaped_weights = jnp.reshape(weights, (-1, self.num_experts_per_tok))\n    reshaped_intermediate = jnp.reshape(\n        unsort_intermediate,\n        (reshaped_weights.shape[0], self.num_experts_per_tok, -1),\n    )\n    with jax.named_scope(\"weight_sum\"):\n      matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n      if self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4:\n        # For Llama4, combine using weights of 1 for selected experts\n        reshaped_weights = jnp.ones_like(reshaped_weights)\n      output = jnp.einsum(\n          \"BKE,BK -> BE\",\n          reshaped_intermediate.astype(jnp.float32),\n          reshaped_weights.astype(jnp.float32),\n          precision=matmul_precision,\n      )\n    return output.reshape(batch_size, sequence_length, -1).astype(self.dtype)\n\n  @staticmethod\n  def local_permute(\n      inputs,\n      global_group_sizes,\n      local_expert_size,\n      shard_index,\n      is_offset=False,\n      global_sorted_experts=None,\n      use_custom_sort_vjp=True,\n  ):\n    \"\"\"Permutes tokens locally within an expert shard.\n\n    This function prepares the input tokens for processing by the experts\n    located\n    on the current shard. It groups the tokens by their assigned local expert\n    index (0 to local_expert_size - 1).\n\n    Args:\n      inputs: The input data (tokens) assigned to the experts on this shard.\n        Shape `[tokens, emb_dim]`.\n      global_group_sizes: The count of tokens assignments for each global expert\n        across all the batch shards. Shape `[num_batch_shards, num_experts].\n      local_expert_size: The number of experts handled by the current shard.\n      shard_index: The index of the current expert shard (0 to\n        num_expert_parallelism - 1).\n      is_offset: If True, assumes `inputs` are pre-sorted by global expert ID\n        and selects the slice relevant to this shard's assigned experts. If\n        False, assumes that `inputs` corresponding to the shard's experts start\n        from the beginning of the tensor but need to be permuted by expert ID.\n      global_sorted_experts: Global expert IDs for the `inputs` used when\n        `is_offset` is True. Shape `[total_tokens_for_this_shard]`.\n\n    Returns:\n      A tuple containing:\n        sorted_inputs: Input data permuted local expert ID.\n        sorted_indices: Indices used to permute the inputs.\n        local_group_size: Number of tokens assigned to each local expert on this\n          shard.\n        sorted_experts_ids: expert ID corresponding to each token of the permuted\n        inputs.\n    \"\"\"\n\n    # Slice the count of local expert IDs in each batch shard.\n    # all_shard_local_sizes.shape: [expert_shard, local_expert_size]\n    all_shard_local_sizes = jax.lax.dynamic_slice_in_dim(\n        global_group_sizes,\n        shard_index * local_expert_size,\n        local_expert_size,\n        axis=1,\n    )\n    local_sizes = all_shard_local_sizes.reshape(-1)\n\n    # Total count of the local expert IDs is the sum of the counts across all\n    # batch shards, since all batch shards will send their contributions to the\n    # current expert shard.\n    local_group_size = jnp.sum(all_shard_local_sizes, axis=0)\n\n    # In this case, the data that needs to be processed by the local shard\n    # does not start from row 0 but actually starts at\n    # (jnp.concatenate((jnp.array([0]),\n    #  jnp.cumsum(local_group_sizes[:-1]))[shard_id]).\n    # This happens if batches (`inputs`) are replicated across expert shards and\n    # pre-sorted by global Expert ID (via permute()).\n    if is_offset:\n      divided_assignments = jnp.floor_divide(global_sorted_experts, local_expert_size)\n      expert_indices = jnp.where(\n          divided_assignments == shard_index,\n          jnp.mod(global_sorted_experts, local_expert_size),\n          local_expert_size,\n      )\n\n    # In this case the `input` data has been received from the batch shards and\n    # needs to be reorganized in order of local Expert IDs.\n    else:\n      base_indices = jnp.mod(jnp.arange(local_sizes.shape[0]), local_expert_size)\n      expert_indices = jnp.repeat(base_indices, local_sizes, total_repeat_length=inputs.shape[0])\n\n    sorted_indices = jnp.argsort(expert_indices)\n    sorted_inputs = _sort_activations(inputs, sorted_indices, use_custom_sort_vjp)\n    sorted_experts_ids = expert_indices[sorted_indices]\n    return (\n        sorted_inputs,\n        sorted_indices,\n        local_group_size,\n        sorted_experts_ids,\n    )\n\n  @staticmethod\n  def get_all_to_all_params(\n      all_shards_group_sizes,\n      shard_id,\n      num_expert_parallelism,\n      is_batch_sharded=True,\n  ):\n    \"\"\"Generates input offsets, send sizes, output offsets, and receive sizes used for ragged_all_to_all.\"\"\"\n\n    class TransformStrategy(enum.Enum):\n      INPUT_OFFSET = enum.auto()\n      SEND_SIZE = enum.auto()\n      OUTPUT_OFFSET = enum.auto()\n      RECV_SIZE = enum.auto()\n\n    def transform_array(input_array, shard_id, strategy, is_batch_sharded):\n      \"\"\"Transforms the input array based on the specified strategy.\"\"\"\n      # Prepares it for the usage with `ragged_all_to_all` API. The\n      # transformation determines how data is sent and received between shards.\n      if is_batch_sharded:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # Index of input array for the send\n          local_array = input_array[shard_id]\n          return jnp.concatenate((jnp.array([0]), jnp.cumsum(local_array)[:-1]))\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # Size of input array for the send\n          return input_array[shard_id]\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # Received index in the target output\n          zero_row = jnp.zeros((1,) + input_array.shape[1:], dtype=input_array.dtype)\n          array_with_zeros = jnp.concatenate((zero_row, input_array), axis=0)\n          cumulated_array = jnp.cumsum(array_with_zeros, axis=0, dtype=input_array.dtype)\n          return cumulated_array[shard_id]\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array[:, shard_id]\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n      # If the batch is unsharded then we send the same data slice to all other\n      # shards. We also assume each shard will have the local processed inputs\n      # sorted to start from index 0. Finally, len(input_array.shape) == 1 since\n      # there is only one batch shard.\n      else:\n        if strategy == TransformStrategy.INPUT_OFFSET:\n          # The data on each shard always starts at 0.\n          return jnp.zeros(num_expert_parallelism, dtype=input_array.dtype)\n        elif strategy == TransformStrategy.SEND_SIZE:\n          # The send amount is always the amount of data the current expert\n          # shard needs to process.\n          return jnp.repeat(input_array[shard_id], num_expert_parallelism)\n        elif strategy == TransformStrategy.OUTPUT_OFFSET:\n          # The offset in each shard will just be the start of the group which\n          # that shard is responsible for.\n          output_offset = jnp.concatenate((jnp.array([0]), jnp.cumsum(input_array[:-1])))[shard_id]\n          return jnp.repeat(output_offset, num_expert_parallelism)\n        # The amount that each shard receives from all other shards is\n        # equivalent to the group sizes (aka input_array).\n        elif strategy == TransformStrategy.RECV_SIZE:\n          # Received size in the target output\n          return input_array\n        else:\n          raise ValueError(f\"Unknown transform array strategy: {strategy}\")\n\n    input_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.INPUT_OFFSET,\n        is_batch_sharded,\n    )\n    send_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.SEND_SIZE,\n        is_batch_sharded,\n    )\n    output_offsets = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.OUTPUT_OFFSET,\n        is_batch_sharded,\n    )\n    recv_sizes = transform_array(\n        all_shards_group_sizes,\n        shard_id,\n        TransformStrategy.RECV_SIZE,\n        is_batch_sharded,\n    )\n    return input_offsets, send_sizes, output_offsets, recv_sizes\n\n  def transform_bias(self, experts_index, *biases):\n    \"\"\"Selects bias values for a variable number of bias tensors based on chosen experts.\"\"\"\n    return tuple(bias[experts_index] for bias in biases)\n\n  def sparse_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ):\n    \"\"\"Perform sparse matrix multiplication of inputs and Experts.\"\"\"\n\n    def gmm(inputs, kernel, group_sizes, expert_assignments):\n      tile_size = (\n          self.config.tile_batch_seq,\n          self.config.tile_activation_dim,\n          self.config.tile_weight_dim,\n      )\n      pad_length = self.config.tile_batch_seq\n      hs_shape = inputs.shape\n      # pad length is the 1st dimension of tiling size in gmm call\n      if inputs.shape[0] != expert_assignments.shape[0]:\n        raise ValueError(\"The number of input tokens must match the number of expert\" \" assignments!\")\n      padding_amount = 0\n      if hs_shape[0] % pad_length:\n        padding_amount = pad_length - hs_shape[0] % pad_length\n        inputs = jax.lax.pad(inputs, jnp.array(0.0, dtype=inputs.dtype), [(0, padding_amount, 0), (0, 0, 0)])\n\n      inputs = inputs.astype(self.dtype)\n      kernel = kernel.astype(self.dtype)\n\n      lhs_quantize_dtype, rhs_quantize_dtype = None, None\n      if self.quant is not None:\n        quant_dg = self.quant.quant_dg\n        lhs_quantize_dtype = quant_dg.fwd.dg_quantizer.lhs.numerics.get_dtype()\n        rhs_quantize_dtype = quant_dg.fwd.dg_quantizer.rhs.numerics.get_dtype()\n      if self.config.use_qwix_quantization:\n        quantization_rule = qpl.get_current_rule(\"dot_general\")\n        if quantization_rule is not None:\n          lhs_quantize_dtype = quantization_rule.act_qtype\n          rhs_quantize_dtype = quantization_rule.weight_qtype\n      m, k, n = inputs.shape[0], inputs.shape[1], kernel.shape[2]\n      tiling = (\n          min(tile_size[0], m),\n          min(tile_size[1], k),\n          min(tile_size[2], n),\n      )\n      if self.config.megablox:\n        output = mblx.gmm(\n            lhs=inputs,\n            rhs=kernel,\n            group_sizes=group_sizes,\n            preferred_element_type=self.dtype,\n            tiling=tiling,\n            lhs_quantize_dtype=lhs_quantize_dtype,\n            rhs_quantize_dtype=rhs_quantize_dtype,\n            use_qwix_quantization=self.config.use_qwix_quantization,\n        )\n      else:\n        rhs_inputs = kernel\n        if isinstance(kernel, aqt.QTensor):\n          if kernel.bias or kernel.sparsity_mask or len(kernel.scale) > 1:\n            raise ValueError(\"Unsupported usecase for ragged_dot with quantized kernel.\")\n          rhs_inputs = kernel.qvalue\n        with set_xla_metadata(ragged_dot_tiling=\",\".join([str(t) for t in tiling])):\n          output = jax.lax.ragged_dot(\n              lhs=inputs,\n              rhs=rhs_inputs,\n              group_sizes=group_sizes,\n              preferred_element_type=self.dtype,\n          )\n        if isinstance(kernel, aqt.QTensor):\n          # Multiply outputs by the kernely scale\n          scales = jnp.take(kernel.scale[0].squeeze(), indices=expert_assignments, axis=0)\n          if padding_amount > 0:\n            scales = jax.lax.pad(\n                scales,\n                jnp.array(0.0, dtype=scales.dtype),\n                [(0, padding_amount, 0), (0, 0, 0)],\n            )\n          output *= scales\n      if padding_amount > 0:\n        output = output[: hs_shape[0]]\n      return output\n\n    # Currently, we support data, tensor, and expert parallelism with Megablox.\n    # We all gather the input activations over tensor parallelism to follow\n    # https://parsa.epfl.ch/course-info/cs723/papers/Megatron.pdf.\n\n    # Check if the batch should be sharded by expert and whether the batch_size\n    # supports this. For example, for interleaved inference, prefill always has\n    # batch_size=1 while decode can have batch_size > 1.\n    try:\n      is_batch_sharded_by_expert = (\n          \"expert\"\n          in tuple(\n              filter(\n                  lambda tup: tup[0] == \"activation_batch\",\n                  self.config.logical_axis_rules,\n              )\n          )[\n              0\n          ][1]\n      )\n    except:  # pylint: disable=bare-except\n      is_batch_sharded_by_expert = False\n    if is_batch_sharded_by_expert and inputs.shape[0] > 1:\n      batch_logical_axis = \"activation_batch\"\n    else:\n      batch_logical_axis = \"activation_batch_no_exp\"\n\n    if self.get_tensor_transpose_parallelism_size() > 1:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", None))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n    else:\n      input_partition_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n      w0_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      w1_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_mlp\"))\n      wo_bias_pspec = nn.logical_to_mesh_axes((\"exp\", \"activation_embed\"))\n\n    gate_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      pre_bias_logits_pspec = nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", None))\n    else:\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits_pspec = None\n\n    # w0, w1, wo needs to be un sharded on fsdp / fsdp_transpose axis, so use\n    # mlp_no_fsdp axis\n    if self.config.fsdp_shard_on_exp:\n    # special sharding for dsv3 to remove overhead between gmm/AG\n      w0_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", None, \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"embed_tensor_transpose\", \"mlp_no_fsdp\", None))\n    else:\n      w0_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_pspec = nn.logical_to_mesh_axes((\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_pspec = nn.logical_to_mesh_axes((\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n    if isinstance(w0_kernel, aqt.QTensor):\n      w0_pspec = aqt.partition_spec(w0_pspec, (1,), w0_kernel.dtype, use_bias=False)\n    if isinstance(w1_kernel, aqt.QTensor):\n      w1_pspec = aqt.partition_spec(w1_pspec, (1,), w1_kernel.dtype, use_bias=False)\n    if isinstance(wo_kernel, aqt.QTensor):\n      wo_pspec = aqt.partition_spec(wo_pspec, (1,), wo_kernel.dtype, use_bias=False)\n\n    @functools.partial(\n        shard_map.shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            input_partition_pspec,\n            gate_logits_pspec,\n            pre_bias_logits_pspec,\n            w0_pspec,\n            w1_pspec,\n            wo_pspec,\n            w0_bias_pspec,\n            w1_bias_pspec,\n            wo_bias_pspec,\n            None,\n        ),\n        out_specs=(nn.logical_to_mesh_axes((batch_logical_axis, \"activation_norm_length\", \"activation_embed\"))),\n        check_rep=False,\n    )\n    def wrapper(x, logits, pre_bias_logits, w0, w1, wo, w0_bias, w1_bias, wo_bias, rngs):\n      batch_size, sequence_length, _ = x.shape\n      expert_axis_name = \"expert\"\n      expert_shard_id = jax.lax.axis_index(expert_axis_name)\n      num_expert_parallelism = self.get_expert_parallelism_size()\n      if self.config.use_ring_of_experts:\n        # The ring-of-experts strategy first duplicates the inputs to all\n        # expert shards, and then routes within each shard.\n\n        # Duplicate inputs to all expert shards.\n        x, logits, pre_bias_logits = tuple(\n            jax.lax.all_gather(z, axis_name=expert_axis_name, tiled=True)\n            for z in (x, logits, pre_bias_logits)\n        )\n\n        # \"Route\" tokens within each shard.\n        num_experts_per_shard = self.config.num_experts // num_expert_parallelism\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x, logits, pre_bias_logits, self.config.use_custom_sort_vjp,\n            roll_to_expert_id=num_experts_per_shard * expert_shard_id)\n\n        # Filter down to the group sizes that apply to only the experts in the\n        # current shard.\n        group_sizes = group_sizes[:num_experts_per_shard]\n        mask = jnp.arange(x.shape[0]) < jnp.sum(group_sizes)\n        x = jnp.where(mask[:, None], x, 0)\n      else:\n        x, sorted_selected_experts, weights, group_sizes, selected_experts = self.permute(\n            x, logits, pre_bias_logits, self.config.use_custom_sort_vjp, rngs)\n\n        if num_expert_parallelism > 1:\n          batch_axis = \"expert\" if is_batch_sharded_by_expert else \"data\"\n          # get group sizes for all shards\n          local_expert_size = self.config.num_experts // num_expert_parallelism\n          reshaped_group_sizes = jnp.sum(group_sizes.reshape(-1, local_expert_size), axis=1)\n          global_group_sizes = group_sizes\n          if is_batch_sharded_by_expert:\n            all_shards_group_sizes = jax.lax.all_gather(reshaped_group_sizes, axis_name=batch_axis)\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                all_shards_group_sizes,\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n\n            # TODO(ranran): For better performance, we could update output buffer to a smaller\n            # size to replace self.get_expert_parallelism_size() for efficiency,\n            # Or we could apply capacity_factor for excessive experts.\n            # Note: Reducing buffer increase the risk of token dropping under unbalanced distribution.\n\n            # In the worst case, all of the global input data is assigned to each expert in the current shard.\n            # This would result in num_expert_shards * input_size * experts_per_shard assignments. However, if\n            # experts_per_shard > num_experts_per_tok we cannot assign more than num_experts_per_tok to all of the inputs.\n            max_local_experts_per_tok = min(local_expert_size, self.config.num_experts_per_tok)\n            buffer_size = int(\n                num_expert_parallelism\n                * self.config.per_device_batch_size\n                * self.config.max_target_length\n                * max_local_experts_per_tok\n            )\n            output_shape = jnp.zeros((buffer_size, self.config.emb_dim), dtype=x.dtype)\n\n            x = jax.lax.ragged_all_to_all(\n                x,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n            global_group_sizes = jax.lax.all_gather(group_sizes, axis_name=expert_axis_name)\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes,\n                local_expert_size,\n                shard_index=expert_shard_id,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n          else:\n            x, local_sorted_indices, group_sizes, selected_experts = RoutedMoE.local_permute(\n                x,\n                global_group_sizes[None, :],\n                local_expert_size,\n                shard_index=expert_shard_id,\n                is_offset=True,\n                global_sorted_experts=selected_experts,\n                use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n            )\n\n      if self.config.mlp_bias:\n        w0_bias, w1_bias, wo_bias = self.transform_bias(selected_experts, w0_bias, w1_bias, wo_bias)\n\n      gmm_fn = functools.partial(\n          gmm,\n          group_sizes=group_sizes,\n          expert_assignments=selected_experts,\n      )\n      layer_w0 = gmm_fn(x, w0)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w0 = jax.lax.psum(layer_w0, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w0 = layer_w0 + w0_bias\n      layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n\n      layer_w1 = gmm_fn(x, w1)\n      if self.get_tensor_transpose_parallelism_size() > 1:\n        layer_w1 = jax.lax.psum(layer_w1, \"tensor_transpose\")\n      if self.config.mlp_bias:\n        layer_w1 = layer_w1 + w1_bias\n      layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      intermediate_layer = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      intermediate_output = gmm_fn(intermediate_layer, wo)\n      if self.get_tensor_parallelism_size() > 1:\n        intermediate_output = jax.lax.psum_scatter(intermediate_output, \"tensor\", scatter_dimension=1, tiled=True)\n      if self.config.mlp_bias:\n        intermediate_output = intermediate_output + wo_bias\n      intermediate_output = adc.checkpoint_name(intermediate_output, \"mlpwo\")\n\n      if self.config.use_ring_of_experts:\n        # Set the outputs of tokens which were not processed to 0.\n        mask = jnp.arange(intermediate_output.shape[0]) < jnp.sum(group_sizes)\n        intermediate_output = jnp.where(mask[:, None], intermediate_output, 0)\n\n        # Unsort and deduplicate the outputs locally.\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n        # Sum up the partial outputs across the expert shards.\n        output = jnp.reshape(output, (-1, sequence_length, self.config.emb_dim))\n        output = jax.lax.psum_scatter(\n            output, expert_axis_name, scatter_dimension=0, tiled=True)\n\n      else:\n        if num_expert_parallelism > 1:\n          original_inputs_first_dim = batch_size * sequence_length * self.config.num_experts_per_tok\n          if sorted_selected_experts.shape[0] != original_inputs_first_dim:\n            raise ValueError(\"original_inputs_first_dim does not match the original tensor\" \" shape!\")\n          output_shape = jnp.zeros(\n              (\n                  original_inputs_first_dim,\n                  self.config.emb_dim // self.get_tensor_parallelism_size(),\n              ),\n              dtype=intermediate_output.dtype,\n          )\n          if is_batch_sharded_by_expert:\n            # locally unpermute back to the original order\n            local_output = _sort_activations(\n                intermediate_output,\n                jnp.argsort(local_sorted_indices),  # pylint: disable=undefined-variable\n                self.config.use_custom_sort_vjp,\n            )\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                jnp.transpose(all_shards_group_sizes),  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                local_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n          else:\n            # If bach is replicated across EP shards then each shard should send\n            # 0..local_shard_size data to the other shards and receive the\n            # local_shard data from all of the other shards using\n            # ragged_all_to_all.\n            input_offsets, send_sizes, output_offsets, recv_sizes = RoutedMoE.get_all_to_all_params(\n                reshaped_group_sizes,  # pylint: disable=undefined-variable\n                expert_shard_id,\n                num_expert_parallelism,\n                is_batch_sharded=False,\n            )\n            intermediate_output = jax.lax.ragged_all_to_all(\n                intermediate_output,\n                output_shape,\n                input_offsets,\n                send_sizes,\n                output_offsets,\n                recv_sizes,\n                axis_name=expert_axis_name,\n            )\n\n        output = self.unpermute(\n            intermediate_output,\n            sorted_selected_experts,\n            weights,\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            use_custom_sort_vjp=self.config.use_custom_sort_vjp,\n        )\n\n      return output, None\n\n    if self.config.moe_fsdp_use_two_stage_all_gather:\n      # Unshard on fsdp axis\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp\"))\n\n      # Unshard on fsdp_transpose axis\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp\", \"embed_tensor_transpose\"))\n\n      # Make sure XLA does not optimize by combining above All-Gather to unshard\n      # on FSDP axis and the subsequent unshard on fsdp_transpose axis\n      w0_kernel = jax.lax.optimization_barrier(w0_kernel)\n      w1_kernel = jax.lax.optimization_barrier(w1_kernel)\n      wo_kernel = jax.lax.optimization_barrier(wo_kernel)\n\n      # Unshard on both fsdp and fsdp_transpose transpose\n      w0_kernel = nn.with_logical_constraint(w0_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      w1_kernel = nn.with_logical_constraint(w1_kernel, (\"exp\", \"embed_tensor_transpose\", \"mlp_no_fsdp\"))\n      wo_kernel = nn.with_logical_constraint(wo_kernel, (\"exp\", \"mlp_no_fsdp\", \"embed_tensor_transpose\"))\n\n    return wrapper(inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias, self.rngs)\n\n  def reshape_and_update_weights(self, weights, indices):\n    \"\"\"reshape and update weights.\"\"\"\n    # input of weights and indices: (batch_size, seq_len, num_experts_per_tok)\n    # output of updated weights: (batch_size, seq_len, num_experts)\n    update_weights = jnp.zeros((weights.shape[0], weights.shape[1], self.num_experts), dtype=self.dtype)\n    index_update = (\n        jnp.arange(weights.shape[0])[:, None, None],\n        jnp.arange(weights.shape[1])[:, None],\n        indices,\n    )\n    update_weights = update_weights.at[index_update].set(weights)\n    return update_weights\n\n  def get_context_partition_and_sub_seq(self, seq_len):\n    cp = self.get_context_autoregressive_parallelism_size()\n    if seq_len % cp != 0:\n      cp = 1\n    sub_seq = seq_len // cp\n    return cp, sub_seq\n\n  def generate_masks_subgroup(self, top_k_indices, softmax_probs):\n    \"\"\"Subgroup mask generation for inference only.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    # Break sequence into subsequences (groups) of tokens, and route only within\n    # each group.\n    top_k_indices = jnp.reshape(top_k_indices, (batch_size, cp, sub_seq, top_k_indices.shape[2]))\n\n    tokens_per_batch = sub_seq * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, cp, sub_seq * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=2)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=3)\n\n    # reshape & update weights\n    softmax_probs = jnp.reshape(\n        softmax_probs,\n        ((batch_size, cp, sub_seq, self.num_experts)),\n    )\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, cp, sub_seq, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=3) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    # ici_context_parallelism\n    dispatch_mask = jnp.reshape(\n        dispatch_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n    combine_mask = jnp.reshape(\n        combine_mask,\n        (batch_size, cp, sub_seq, self.num_experts, expert_capacity_per_batch),\n    )\n\n    return dispatch_mask, combine_mask\n\n  def generate_masks(self, top_k_indices, softmax_probs):\n    \"\"\"Generate masks.\"\"\"\n    # calculate\n    # expert_capacity = (tokens_per_batch / num_experts) * capacity_factor\n    batch_size, seq_len, _ = top_k_indices.shape\n\n    tokens_per_batch = seq_len * self.num_experts_per_tok\n    # this is to avoid expert_capacity_per_batch = 0\n    expert_capacity_per_batch = int(\n        max(\n            math.ceil(tokens_per_batch / self.num_experts) * self.config.capacity_factor,\n            self.config.capacity_factor,\n        )\n    )\n    max_logging.log(\"Applying potential token dropping with a batch expert_capacity of\" f\" {expert_capacity_per_batch}\")\n\n    # calculate expert mask and drop tokens if needed\n    # shape of output expert mask: (batch, sequence, num_experts_per_tok)\n    #\n    # A small example:\n    # give num_experts=4 & num_experts_per_tok=2, and two tokens are routed to\n    # expert [0, 1] & [1, 3],\n    # then expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 1, 0, 0],[0, 0, 0, 1]]]],\n    # after cumsum, expert_token_count becomes\n    # [[[[1, 0, 0, 0],[1, 1, 0, 0]], [[1, 2, 0, 0],[1, 2, 0, 1]]]],\n    # if we set expert_capacity=1,\n    # trunc_expert_mask becomes\n    # [[[[1, 0, 0, 0],[0, 1, 0, 0]], [[0, 0, 0, 0],[0, 0, 0, 1]]]],\n    # so the 2nd token for expert #1 ([0, 1] & [1, 3]) is dropped, output of\n    # updated_expert_mask is [[[1, 1],[0, 1]]].\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    expert_mask_fused = jnp.reshape(\n        expert_mask,\n        (batch_size, seq_len * self.num_experts_per_tok, self.num_experts),\n    )\n    expert_mask_fused = nn.with_logical_constraint(expert_mask_fused, (\"activation_batch\", None, None))\n    expert_token_count_fused = jnp.cumsum(expert_mask_fused, axis=1)\n    expert_token_count = jnp.reshape(\n        expert_token_count_fused,\n        ((batch_size, seq_len, self.num_experts_per_tok, self.num_experts)),\n    )\n    expert_token_count = nn.with_logical_constraint(\n        expert_token_count,\n        (\"activation_batch\", \"activation_norm_length\", None, None),\n    )\n    trunc_expert_mask = expert_mask * jnp.less_equal(expert_token_count, expert_capacity_per_batch)\n    combined_expert_mask = jnp.sum(trunc_expert_mask, axis=2)\n\n    softmax_probs *= combined_expert_mask\n\n    # calculate token position in expert capacity dimension\n    expert_token_position_fused = expert_mask_fused * expert_token_count_fused\n    expert_token_position = jnp.reshape(\n        expert_token_position_fused,\n        (batch_size, seq_len, self.num_experts_per_tok, self.num_experts),\n    )\n    combined_expert_token_position = jnp.sum(expert_token_position, axis=2) * combined_expert_mask\n    expert_token_position_in_capacity = jax.nn.one_hot(\n        combined_expert_token_position,\n        num_classes=expert_capacity_per_batch + 1,\n        dtype=jnp.int32,\n    )\n\n    # shape of combine_mask is\n    # (batch_size, seq_len, num_experts, expert_capacity_per_batch + 1),\n    # and cut 0-dimension which is always 0\n    combine_mask = softmax_probs[..., None] * expert_token_position_in_capacity\n    combine_mask = combine_mask[..., 1:]\n    dispatch_mask = combine_mask.astype(bool)\n\n    return dispatch_mask, combine_mask\n\n  # See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details.\n  def load_balance_loss(self, top_k_indices, logits) -> jax.Array:\n    \"\"\"Compute the load balance loss.\"\"\"\n    expert_mask = jax.nn.one_hot(top_k_indices, num_classes=self.num_experts, dtype=jnp.int32)\n    summed_expert_mask = jnp.sum(expert_mask, axis=2)\n    # Get fraction of tokens dispatched to each expert\n    density = jnp.mean(summed_expert_mask, axis=1)\n    # get fraction of probability allocated to each expert\n    density_prob = jnp.mean(logits, axis=1)\n    loss = jnp.mean(density * density_prob) * (self.num_experts**2) * self.config.load_balance_loss_weight\n    return loss\n\n  def get_einsum(\n      self,\n      rhs_mesh_axes: Tuple[Optional[str], ...] = (),\n      einsum_name: str | None = None,\n  ):\n    \"\"\"Get the Einstein summation.\"\"\"\n\n    # the check is to prevent aqteinsum as einsum op for dispatch and combine\n    # einsums in ase when capacity_factor > 0\n    # this is necessary to load pre-quantized weights in case of inference\n    if self.config.model_call_mode == \"inference\" and einsum_name in (\n        DISPATCH,\n        COMBINE,\n    ):\n      return jnp.einsum\n\n    if self.quant:\n\n      def aqt_einsum(*args, **kwargs):  # pylint: disable=unused-argument\n        # simply skip kwargs, since aqt einsum doesn't support any kwargs\n        # like precision\n        is_aqt = not isinstance(self.quant, quantizations.Fp8Quantization)\n        kw = {\"mesh_axes\": rhs_mesh_axes} if is_aqt else {\"dtype\": self.dtype}\n        return self.quant.einsum(**kw)(*args)  # pytype: disable=attribute-error\n\n      einsum_op = aqt_einsum\n    else:\n      einsum_op = jnp.einsum\n    return einsum_op\n\n  def maybe_all_gather_kernel_weight_in_expert_parallelism(\n      self, kernel: jax.Array, kernel_axes: Tuple[Optional[str], ...]\n  ):\n    \"\"\"All-gather kernel weight in expert parallelism if needed.\"\"\"\n    if self.get_expert_parallelism_size() > 1:\n      # This will trigger all-gather using weight_dtype\n      # relax it unless really necessary in expert parallelism only\n      # Otherwise compiler will handle communication automatically\n      # esp. with int8 quantization, kernel will be all-gathered in int8 instead\n      # of weight_dtype\n      kernel = nn.with_logical_constraint(kernel, kernel_axes)\n    return kernel\n\n  def dense_matmul(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[jax.Array, Optional[jax.Array]]:\n    \"\"\"Dense matrix multiplication.\"\"\"\n    # gate_logits: batch, length, expert\n    gate_logits = nn.with_logical_constraint(gate_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    if self.config.model_name.startswith(\"deepseek3\"):\n      # pre_bias_logits is None for non-DeepSeek v3 models\n      pre_bias_logits = nn.with_logical_constraint(pre_bias_logits, (\"activation_batch\", \"activation_norm_length\", None))\n    top_k_weights, top_k_indices = self.get_topk(gate_logits, pre_bias_logits, self.rngs)\n    is_llama4_decoder_layer = self.config.decoder_block == ctypes.DecoderBlockType.LLAMA4\n    if is_llama4_decoder_layer:\n      router_scores = jax.nn.sigmoid(top_k_weights.astype(jnp.float32)).astype(self.dtype)\n      inputs = inputs * router_scores\n    else:\n      weights = self.reshape_and_update_weights(top_k_weights, top_k_indices)\n    matmul_precision = jax.lax.Precision(self.config.matmul_precision)\n\n    if self.config.model_call_mode != \"inference\":\n      softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n      loss = self.load_balance_loss(top_k_indices, softmax_probs)\n    else:\n      loss = None\n    batch_size = inputs.shape[0]\n    seq_len = inputs.shape[1]\n\n    cp, sub_seq = self.get_context_partition_and_sub_seq(seq_len)\n\n    if self.config.capacity_factor > 0:\n      # token dropping if needed\n      if self.config.model_call_mode != \"inference\":\n        # TODO(b/425930949): remove this pylint by refactoring the logic here.\n        dispatch_mask, combine_mask = self.generate_masks(\n            top_k_indices, weights  # pylint: disable=undefined-variable,possibly-used-before-assignment\n        )\n        mask_axes = (\"activation_batch\", \"activation_norm_length\", None, None)\n        dispatch_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_embed\",\n        )\n        mlp_axis = (\n            \"activation_exp\",\n            \"activation_batch_no_exp\",\n            None,\n            \"activation_mlp\",\n        )\n        dispatch_eimsum = \"BSM,BSEC -> EBCM\"\n        mlp_up_einsum = \"EBCM,EMH -> EBCH\"\n        mlp_down_einsum = \"EBCH,EHM -> EBCM\"\n        output_einsum = \"EBCM,BSEC -> BSM\"\n      else:\n        # TODO(b/425930507): Try replacing `softmax_probs` with padded weights\n        # and verify with decode acc tests.\n        softmax_probs = jax.nn.softmax(gate_logits.astype(jnp.float32), axis=-1).astype(self.dtype)\n        dispatch_mask, combine_mask = self.generate_masks_subgroup(top_k_indices, softmax_probs)\n        if self.get_context_autoregressive_parallelism_size() > 0 and cp == 1:\n          mask_axes = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_norm_length\",\n              \"activation_batch\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        else:\n          mask_axes = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              None,\n              None,\n          )\n          input_axis = (\n              \"activation_batch\",\n              \"activation_norm_length\",\n              None,\n              \"activation_embed\",\n          )\n          dispatch_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_embed\",\n          )\n          mlp_axis = (\n              \"activation_exp\",\n              \"activation_batch_no_exp\",\n              None,\n              None,\n              \"activation_mlp\",\n          )\n        dispatch_eimsum = \"BNSM,BNSEC -> EBNCM\"\n        mlp_up_einsum = \"EBNCM,EMH -> EBNCH\"\n        mlp_down_einsum = \"EBNCH,EHM -> EBNCM\"\n        output_einsum = \"EBNCM,BNSEC -> BNSM\"\n\n        inputs = jnp.reshape(inputs, (batch_size, cp, sub_seq, inputs.shape[2]))\n        inputs = nn.with_logical_constraint(inputs, input_axis)\n\n      dispatch_mask = nn.with_logical_constraint(dispatch_mask, mask_axes)\n      combine_mask = nn.with_logical_constraint(combine_mask, mask_axes)\n\n      with jax.named_scope(\"dispatch\"):\n        # only cp during prefill\n        dispatch = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=DISPATCH)(\n            dispatch_eimsum, inputs, dispatch_mask, precision=matmul_precision\n        )\n        if cp > 1:\n          dispatch = nn.with_logical_constraint(\n              dispatch,\n              (\n                  None,\n                  \"activation_batch_no_exp\",\n                  \"activation_norm_length\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        dispatch = nn.with_logical_constraint(\n            dispatch,\n            dispatch_axis,\n        )\n      with jax.named_scope(\"wi_0\"):\n        w0_kernel_axes = (\"exp\", None, \"mlp\")\n        w0_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w0_kernel, w0_kernel_axes)\n        layer_w0 = self.get_einsum(rhs_mesh_axes=w0_kernel_axes)(\n            mlp_up_einsum, dispatch, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w0_bias = w0_bias[:, None, None, :]\n          layer_w0 = layer_w0 + w0_bias\n\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = nn.with_logical_constraint(\n            layer_w0,\n            mlp_axis,\n        )\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        w1_kernel_axes = (\"exp\", None, \"mlp\")\n        w1_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(w1_kernel, w1_kernel_axes)\n        layer_w1 = self.get_einsum(rhs_mesh_axes=w1_kernel_axes)(\n            mlp_up_einsum, dispatch, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          w1_bias = w1_bias[:, None, None, :]\n          layer_w1 = layer_w1 + w1_bias\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = nn.with_logical_constraint(\n            layer_w1,\n            mlp_axis,\n        )\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n      with jax.named_scope(\"wo\"):\n        wo_kernel_axes = (\"exp\", \"mlp\", None)\n        wo_kernel = self.maybe_all_gather_kernel_weight_in_expert_parallelism(wo_kernel, wo_kernel_axes)\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=wo_kernel_axes)(\n            mlp_down_einsum,\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          wo_bias = wo_bias[:, None, None, :]\n          intermediate_layer = intermediate_layer + wo_bias\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        if self.config.model_call_mode != \"inference\":\n          intermediate_layer = nn.with_logical_constraint(\n              intermediate_layer,\n              (\n                  \"activation_exp\",\n                  \"activation_batch_no_exp\",\n                  None,\n                  \"activation_embed\",\n              ),\n          )\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"combine\"):\n        # Matmul & element wise operation\n        output = self.get_einsum(rhs_mesh_axes=mask_axes, einsum_name=COMBINE)(\n            output_einsum,\n            intermediate_layer,\n            combine_mask,\n            precision=matmul_precision,\n        )\n        if output.ndim == 4:\n          output = jnp.reshape(\n              output,\n              (\n                  output.shape[0],\n                  output.shape[1] * output.shape[2],\n                  output.shape[3],\n              ),\n          )\n      return output, loss\n    else:\n      inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n      with jax.named_scope(\"wi_0\"):\n        layer_w0 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w0_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w0 = layer_w0 + w0_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w0 = layer_w0.astype(jnp.float32)\n        layer_w0 = adc.checkpoint_name(layer_w0, \"mlpwi_0\")\n      with jax.named_scope(\"wi_1\"):\n        layer_w1 = self.get_einsum(rhs_mesh_axes=self.wi_kernel_axes)(\n            \"BSM,EMH -> BSEH\", inputs, w1_kernel, precision=matmul_precision\n        )\n        if self.config.mlp_bias:\n          layer_w1 = layer_w1 + w1_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          layer_w1 = layer_w1.astype(jnp.float32)\n        layer_w1 = adc.checkpoint_name(layer_w1, \"mlpwi_1\")\n      layer_multiply = self.apply_ffn_activation(layer_w0, layer_w1)\n\n      with jax.named_scope(\"wo\"):\n        intermediate_layer = self.get_einsum(rhs_mesh_axes=self.wo_kernel_axes)(\n            \"BSEH,EHM -> BSEM\",\n            layer_multiply,\n            wo_kernel,\n            precision=matmul_precision,\n        )\n        if self.config.mlp_bias:\n          intermediate_layer = intermediate_layer + wo_bias[None, None, :, :]\n        if self.config.activations_in_float32:\n          intermediate_layer = intermediate_layer.astype(jnp.float32)\n        intermediate_layer = adc.checkpoint_name(intermediate_layer, \"mlpwo\")\n      with jax.named_scope(\"w_sum\"):\n        if is_llama4_decoder_layer:\n          weights = self.reshape_and_update_weights(jnp.ones_like(top_k_weights), top_k_indices)\n        # cast to f32 for sum up in einsum op\n        output = jnp.einsum(\n            \"BSEM,BSE -> BSM\",\n            intermediate_layer.astype(jnp.float32),\n            weights.astype(jnp.float32),  # pylint: disable=undefined-variable,possibly-used-before-assignment\n            precision=matmul_precision,\n        ).astype(self.dtype)\n      return output, None\n\n  def retrieve_quantized_weight(\n      self,\n      inputs,\n      gate_logits,\n      pre_bias_logits,\n      w0_kernel,\n      w1_kernel,\n      wo_kernel,\n      w0_bias,\n      w1_bias,\n      wo_bias,\n  ) -> tuple[aqt.QTensor, aqt.QTensor, aqt.QTensor]:\n    \"\"\"Retrieve quantized weights.\"\"\"\n    # This is called only during tracing. This is to invoke creation of\n    # quantized tensor inside AqtEinsum.  After jit, this will become no-op and\n    # will not affect performance.\n    _ = self.dense_matmul(inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias)\n\n    w0_kernel = self.variables[\"aqt\"][\"AqtEinsum_0\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    w1_kernel = self.variables[\"aqt\"][\"AqtEinsum_1\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n    wo_kernel = self.variables[\"aqt\"][\"AqtEinsum_2\"][\"AqtDotGeneral_0\"][\"qrhs\"][\"frozen\"]\n\n    w0_kernel = max_utils.unbox_logicallypartioned(w0_kernel)\n    w1_kernel = max_utils.unbox_logicallypartioned(w1_kernel)\n    wo_kernel = max_utils.unbox_logicallypartioned(wo_kernel)\n    return w0_kernel, w1_kernel, wo_kernel\n\n  def __call__(self, inputs: jax.Array) -> tuple[jax.Array, Optional[jax.Array]]:\n    cfg = self.config\n    inputs = inputs.astype(cfg.dtype)\n    gate_logits, pre_bias_logits = self.gate(inputs)\n\n    w0_kernel = jnp.asarray(self.wi_0[...], self.dtype)\n    w1_kernel = jnp.asarray(self.wi_1[...], self.dtype)\n    wo_kernel = jnp.asarray(self.wo[...], self.dtype)\n\n    if cfg.mlp_bias:\n      w0_bias = jnp.asarray(self.wi_0_bias[...], self.dtype)\n      w1_bias = jnp.asarray(self.wi_1_bias[...], self.dtype)\n      wo_bias = jnp.asarray(self.wo_bias[...], self.dtype)\n    else:\n      w0_bias, w1_bias, wo_bias = None, None, None\n\n    if cfg.sparse_matmul:\n      if quantizations.in_serve_mode(self.quant):\n        w0_kernel, w1_kernel, wo_kernel = self.retrieve_quantized_weight(\n            inputs,\n            gate_logits,\n            pre_bias_logits,\n            w0_kernel,\n            w1_kernel,\n            wo_kernel,\n            w0_bias,\n            w1_bias,\n            wo_bias,\n        )\n      return self.sparse_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )\n    else:\n      return self.dense_matmul(\n          inputs, gate_logits, pre_bias_logits, w0_kernel, w1_kernel, wo_kernel, w0_bias, w1_bias, wo_bias\n      )",
        "analysis": {
            "functionality": "The `RoutedMoE` class implements a Mixture of Experts (MoE) layer. It dynamically routes each input token to a subset of 'expert' networks (which are simple MLPs) based on a learned gating mechanism. The class supports two main operational modes: a 'sparse' mode that permutes tokens to group them by expert for efficient computation (often using custom kernels like Megablox), and a 'dense' mode that uses `einsum` operations to achieve the same result. It is designed for large-scale distributed training, incorporating logic for various parallelism strategies (expert, tensor, data) and optional quantization.",
            "usage": "To use this class, instantiate it with a configuration object (`ctypes.Config`), the number of experts, the number of experts per token, a JAX mesh for parallelism, and other parameters. The instance is then called like a function with an input tensor of shape `[batch_size, sequence_length, embedding_dim]`. It returns a tuple containing the output tensor of the same shape and an optional load-balancing loss (during training). The behavior (e.g., sparse vs. dense computation, routing logic) is controlled by the provided configuration object."
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#RoutedAndSharedMoE",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "class RoutedAndSharedMoE(nnx.Module):\n  \"\"\"Implements a block which combines shared and routed experts.\"\"\"\n\n  def __init__(\n      self,\n      config: ctypes.Config,\n      mesh: jax.sharding.Mesh,\n      kernel_init: NdInitializer,\n      kernel_axes: Tuple[Optional[str], ...],\n      rngs: nnx.Rngs,\n      weight_dtype: ctypes.DType = jnp.float32,\n      dtype: ctypes.DType = jnp.float32,\n      quant: Optional[quantizations.AqtQuantization] = None,\n  ):\n    \"\"\"nitializes the RoutedAndSharedMoE module.\n\n    Attributes:\n      config: The main config setting.\n      mesh: Mesh, device mesh.\n      kernel_init: The initializer function for the kernel weight matrix.\n      kernel_axes: A tuple of logical axis names for partitioning the kernel.\n      rngs: An `nnx.Rngs` object used for initializing parameters.\n      weight_dtype: The data type of the kernel weights.\n      dtype: The data type for the computation.\n      quant: The quantization configuration. If None, no quantization is applied.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.quant = quant\n    self.rngs = rngs\n    # NOTE: the name MoeBlock_0 is to ensure reverse compatibility with\n    # existing checkpoints for routed experts.\n    self.MoeBlock_0 = RoutedMoE(\n        config=self.config,\n        num_experts=self.config.num_experts,\n        num_experts_per_tok=self.config.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=self.config.moe_mlp_dim,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n    self.shared_experts = linears.MlpBlock(\n        in_features=self.config.emb_dim,\n        intermediate_dim=self.config.shared_experts * self.config.moe_mlp_dim,\n        activations=self.config.mlp_activations,\n        intermediate_dropout_rate=self.config.dropout_rate,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        config=self.config,\n        quant=self.quant,\n        rngs=self.rngs,\n    )\n\n  @property\n  def routed_moe(self):\n    return self.MoeBlock_0\n\n  def __call__(self, inputs: jax.Array) -> jax.Array:\n    routed_experts, _ = self.routed_moe(inputs)\n    shared_experts = self.shared_experts(inputs)\n    return routed_experts + shared_experts",
        "analysis": {
            "module_type": "routed_and_shared_mixture_of_experts",
            "purpose": "Implements a Mixture of Experts (MoE) block that combines the outputs of a routed expert layer and a shared MLP layer.",
            "input": {
                "shape": "[batch_size, sequence_length, emb_dim]",
                "dtype": "Determined by the 'dtype' parameter in the constructor, typically float32 or bfloat16."
            },
            "processing_steps": [
                "The `__call__` method processes the input through both the routed and shared expert blocks and sums their outputs."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, emb_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "RoutedMoE",
                "linears.MlpBlock",
                "ctypes.Config"
            ],
            "parameters": {
                "config": "The main configuration object containing settings like `num_experts`, `moe_mlp_dim`, `shared_experts`, `emb_dim`, etc.",
                "mesh": "The JAX device mesh used for model parallelism and sharding.",
                "quant": "Optional quantization configuration for the expert layers."
            },
            "notes": [
                "The final output is the element-wise sum of the results from the routed experts and the shared experts.",
                "The routed expert module is named `MoeBlock_0` to ensure reverse compatibility with existing checkpoints."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by creating a `RoutedMoE` instance for the routed experts and an `MlpBlock` for the shared experts.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters.",
                        "Instantiate `RoutedMoE` as `self.MoeBlock_0`.",
                        "Instantiate `linears.MlpBlock` as `self.shared_experts`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "RoutedMoE",
                        "linears.MlpBlock",
                        "nd_dense_init"
                    ],
                    "notes": [
                        "The intermediate dimension for the shared MLP is calculated as `config.shared_experts * config.moe_mlp_dim`."
                    ]
                },
                "routed_moe": {
                    "purpose": "A property that provides access to the internal `RoutedMoE` instance.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Return `self.MoeBlock_0`."
                    ],
                    "output": {
                        "shape": "An instance of the `RoutedMoE` class."
                    },
                    "dependencies": [],
                    "notes": [
                        "This is a read-only property."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass by processing the input through both routed and shared paths and summing the results.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Pass inputs through the `self.routed_moe` module.",
                        "Pass inputs through the `self.shared_experts` module.",
                        "Return the element-wise sum of the two outputs."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, emb_dim]"
                    },
                    "dependencies": [
                        "self.routed_moe",
                        "self.shared_experts"
                    ],
                    "notes": [
                        "The `self.routed_moe` call returns a tuple `(output, loss)`, but only the output tensor is used in this method."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_gate_logit",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_gate_logit(\n    inputs_shape: tuple[int, ...],\n    out_features_shape: Union[Iterable[int], int],\n    model_name: str,\n    axis: Union[Iterable[int], int] = -1,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: Tuple[Optional[str], ...] = (),\n    use_bias: bool = False,\n    score_func: str = \"\",\n    quant: Optional[quantizations.AqtQuantization] = None,\n    matmul_precision: str = \"default\",\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a GateLogit Linen module.\"\"\"\n\n  axis = linears.canonicalize_tuple(axis)\n  in_features_shape = tuple(inputs_shape[ax] for ax in linears.normalize_axes(axis, len(inputs_shape)))\n\n  module = nnx_wrappers.to_linen(\n      GateLogit,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      model_name=model_name,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      use_bias=use_bias,\n      score_func=score_func,\n      quant=quant,\n      matmul_precision=matmul_precision,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "gate_logit_factory",
            "purpose": "A factory function that creates and returns a Flax Linen module by wrapping the `GateLogit` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Canonicalizes the `axis` parameter using `linears.canonicalize_tuple`.",
                "Calculates `in_features_shape` for the `GateLogit` module from the provided `inputs_shape` and normalized `axis`.",
                "Wraps the `GateLogit` NNX module into a Flax Linen module using `nnx_wrappers.to_linen`, passing along all configuration parameters.",
                "Returns the newly created Linen module."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "GateLogit",
                "nnx_wrappers.to_linen",
                "linears.canonicalize_tuple",
                "linears.normalize_axes",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "inputs_shape": "The shape of the expected input tensor for the created module.",
                "out_features_shape": "The shape of the output features for the created module, typically the number of experts.",
                "model_name": "The name of the model, used for specific routing logic (e.g., 'deepseek3').",
                "axis": "The axis or axes over which the linear transformation is applied.",
                "use_bias": "A boolean indicating whether to add a learnable bias to the gate logit scores.",
                "score_func": "Scoring function for output normalization before applying bias (e.g., 'sigmoid').",
                "quant": "The quantization configuration object, if any.",
                "matmul_precision": "The precision level for the matrix multiplication."
            },
            "notes": [
                "This function acts as a bridge between the NNX-based `GateLogit` implementation and the broader Flax Linen-based model architecture.",
                "The `abstract_init` parameter for `to_linen` is set to `False`, meaning the module's parameters are initialized upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_moe(\n    config: ctypes.Config,\n    num_experts: int,\n    num_experts_per_tok: int,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    intermediate_dim: int = 2048,\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedMoE,\n      config=config,\n      num_experts=num_experts,\n      num_experts_per_tok=num_experts_per_tok,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      intermediate_dim=intermediate_dim,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "routed_moe_factory",
            "purpose": "A factory function that creates a Flax Linen module for a Routed Mixture-of-Experts (MoE) layer by wrapping the NNX `RoutedMoE` class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `RoutedMoE` NNX module into a Flax Linen module, passing all the function's arguments to the `RoutedMoE` constructor."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is not applicable."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RoutedMoE",
                "ctypes.Config",
                "jax.sharding.Mesh",
                "initializers.NdInitializer",
                "quantizations.AqtQuantization",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main configuration object containing model settings like embedding dimension, MLP activations, etc.",
                "num_experts": "The total number of experts in the MoE layer.",
                "num_experts_per_tok": "The number of experts to route each token to.",
                "mesh": "The JAX device mesh used for sharding parameters and activations.",
                "kernel_init": "The initializer function for the kernel weight matrices.",
                "kernel_axes": "A tuple of logical axis names for partitioning the kernel.",
                "intermediate_dim": "The intermediate dimension of the feed-forward network within each expert.",
                "weight_dtype": "The data type for the kernel weights.",
                "dtype": "The data type for the computation.",
                "quant": "Optional quantization configuration for the module."
            },
            "notes": [
                "This function serves as a bridge to make the NNX-defined `RoutedMoE` class compatible with a Flax Linen model architecture.",
                "The `metadata_fn` is set to `variable_to_logically_partitioned` to handle parameter sharding.",
                "The `abstract_init` argument is set to `False`, meaning the module is initialized immediately upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/moe.py#get_routed_and_shared_moe",
        "file_path": "src/MaxText/layers/moe.py",
        "code_block": "def get_routed_and_shared_moe(\n    config: ctypes.Config,\n    mesh: jax.sharding.Mesh,\n    kernel_init: NdInitializer,\n    kernel_axes: Tuple[Optional[str], ...],\n    weight_dtype: ctypes.DType = jnp.float32,\n    dtype: ctypes.DType = jnp.float32,\n    quant: Optional[quantizations.AqtQuantization] = None,\n    name: Optional[str] = None,\n):\n  \"\"\"Creates a RoutedAndSharedMoE Linen module.\"\"\"\n\n  module = nnx_wrappers.to_linen(\n      RoutedAndSharedMoE,\n      config=config,\n      mesh=mesh,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      quant=quant,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "routed_and_shared_moe_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible version of the `RoutedAndSharedMoE` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to wrap the `RoutedAndSharedMoE` NNX module.",
                "Passes the provided configuration arguments (`config`, `mesh`, `kernel_init`, etc.) to the `RoutedAndSharedMoE` constructor via the wrapper.",
                "Returns the created Flax Linen module."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "RoutedAndSharedMoE",
                "initializers.variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "A `ctypes.Config` object containing model and training configurations.",
                "mesh": "A `jax.sharding.Mesh` object for device parallelism.",
                "kernel_init": "An `NdInitializer` for initializing kernel weights.",
                "kernel_axes": "A tuple of logical axis names for partitioning the kernel.",
                "weight_dtype": "The data type for the weights.",
                "dtype": "The data type for computations.",
                "quant": "An optional `AqtQuantization` configuration for quantization."
            },
            "notes": [
                "This function acts as a factory, abstracting the conversion from an NNX module to a Flax Linen module.",
                "The returned object is a Flax Linen module, ready to be used within a larger Linen model architecture.",
                "The `abstract_init=False` argument ensures that the module's parameters are initialized immediately upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/mistral.py#MistralDecoderLayer",
        "file_path": "src/MaxText/layers/mistral.py",
        "code_block": "class MistralDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    mlp_lnx = mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mistral_decoder_layer",
            "purpose": "Implements a single decoder layer for a Mistral-style transformer model, consisting of a self-attention block and a feed-forward MLP block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes class attributes: config, mesh, model_mode, and quant."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "MaxText.layers.models.Config",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dimensions, dropout rates, and activation functions.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "model_mode": "A string indicating the operational mode of the model (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This class encapsulates the standard transformer decoder layer architecture with pre-normalization (RMSNorm), a self-attention mechanism, and an MLP block, each followed by a residual connection."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through one complete decoder layer, applying self-attention and a feed-forward network.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "Determined by config.dtype (e.g., float32, bfloat16)."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Pass the normalized tensor through a self-attention layer.",
                        "Add the output of the attention layer to the original input (first residual connection).",
                        "Apply post-attention RMS normalization to the result of the first residual connection.",
                        "Pass the normalized tensor through an MLP block.",
                        "Add the output of the MLP block to the result of the first residual connection (second residual connection).",
                        "Apply dropout to the final output.",
                        "Optionally record internal activation metrics.",
                        "Return the final layer output."
                    ],
                    "output": {
                        "shape": "Returns a tensor of shape [batch_size, sequence_length, hidden_dim]. If config.scan_layers is true, returns a tuple (output_tensor, None)."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "flax.linen.Dropout",
                        "jax.numpy"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is active.",
                        "The `model_mode` parameter is passed to the attention layer to handle different decoding phases like prefill and autoregressive generation.",
                        "The output shape is identical to the input shape, allowing for stacking multiple layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#DecoderLayer",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class DecoderLayer(nn.Module):\n  \"\"\"\n  Transformer decoder layer that attends to the encoder.\n  This is the core, reusable building block for both the main model's\n  decoder stack and the auxiliary MTP layers.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_length\", \"activation_embed\")\n\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    else:\n      inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    if model_mode == MODEL_MODE_PREFILL:\n      lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n    else:\n      lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=self.config,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        model_mode=self.model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=self.model_mode,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n    else:\n      attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n\n    # MLP block.\n    mlp_lnx = linears.mlp_block(\n        in_features=lnx.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        model_mode=self.model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(lnx, deterministic=deterministic)\n    if model_mode == MODEL_MODE_PREFILL:\n      mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n    else:\n      mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    next_layer_addition = mlp_lnx + attention_lnx\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out + inputs\n    if model_mode == MODEL_MODE_PREFILL:\n      layer_output = nn.with_logical_constraint(\n          layer_output,\n          logical_axis_names,\n      )\n    else:\n      layer_output = nn.with_logical_constraint(\n          layer_output,\n          logical_axis_names,\n      )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    return layer_output, None if cfg.scan_layers else layer_output",
        "analysis": {
            "module_type": "transformer_decoder_layer",
            "purpose": "Implements a single, reusable transformer decoder layer, consisting of a self-attention mechanism and a feed-forward MLP block with residual connections and normalization.",
            "input": {
                "shape": "N/A (Handled by the '__call__' method)",
                "dtype": "N/A (Handled by the '__call__' method)"
            },
            "processing_steps": [
                "The '__call__' method executes the forward pass of the layer."
            ],
            "output": {
                "shape": "N/A (Handled by the '__call__' method)"
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "MaxText.common_types.Config",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dimensions, dropout rates, and data types.",
                "mesh": "The JAX mesh used for distributed computation and sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'prefill', 'autoregressive'), which affects tensor sharding annotations.",
                "quant": "Optional quantization configuration for the layer."
            },
            "notes": [
                "This class is a core building block for the decoder stack.",
                "It uses a pre-normalization architecture (RMSNorm is applied before the self-attention and MLP blocks)."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the decoder layer, applying self-attention and an MLP block.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "cfg.dtype"
                    },
                    "processing_steps": [
                        "Apply logical constraints to the input tensor based on `model_mode`.",
                        "Apply RMS normalization to the input tensor.",
                        "Compute self-attention on the normalized input using `attention_as_linen`.",
                        "Process the normalized input through an `mlp_block`.",
                        "Add the outputs of the attention and MLP blocks.",
                        "Apply dropout to the combined output.",
                        "Add the original input tensor as a residual connection to get the final layer output.",
                        "Optionally record internal activation metrics if `cfg.record_internal_nn_metrics` is true.",
                        "Return the final output tensor and a carry for potential scanning operations."
                    ],
                    "output": {
                        "shape": "A tuple `(output, carry)`. `output` has shape [batch_size, sequence_length, embedding_dim]. `carry` is `None` if `cfg.scan_layers` is true, otherwise it is a copy of `output`."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "linears.mlp_block",
                        "nn.Dropout",
                        "nn.with_logical_constraint",
                        "checkpoint_name"
                    ],
                    "notes": [
                        "The `model_mode` argument determines the logical sharding constraints applied to intermediate tensors.",
                        "The return signature is designed to be compatible with `flax.linen.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#SequentialBlockDecoderLayers",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class SequentialBlockDecoderLayers(nn.Module):\n  \"\"\"Sequential unscanned series of decoder layers.\"\"\"\n\n  decoder_layer: Any\n  num_decoder_layers: int\n  config: Config\n  mesh: Mesh\n  quant: Quant\n  model_mode: str\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic: bool,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ) -> jnp.ndarray:\n    for lyr in range(self.num_decoder_layers):\n      inputs = self.decoder_layer(\n          config=self.config, mesh=self.mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=self.model_mode\n      )(\n          inputs,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          slot=slot,\n          page_state=page_state,\n      )\n      if self.config.scan_layers:\n        inputs = inputs[0]  #  When scan_layers is True the decoder layers return (outputs, None).\n    if self.config.scan_layers:\n      return inputs, None  # pytype: disable=bad-return-type\n    else:\n      return inputs",
        "analysis": {
            "module_type": "sequential_decoder_block",
            "purpose": "Applies a specified decoder layer sequentially a configured number of times using a standard Python for-loop, as an alternative to `flax.linen.scan`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The `__call__` method iterates from 0 to `num_decoder_layers`.",
                "In each iteration, it instantiates and applies the `decoder_layer` to the input tensor.",
                "The input tensor is updated with the output of the previous layer for the next iteration."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module as nn",
                "jax.numpy as jnp",
                "page_manager.PageState"
            ],
            "parameters": {
                "decoder_layer": "The nn.Module class for the decoder layer that will be repeatedly applied.",
                "num_decoder_layers": "The total number of times the decoder layer is applied.",
                "config": "The main configuration object, which contains settings like `scan_layers`.",
                "mesh": "The JAX device mesh for model parallelism.",
                "quant": "The quantization configuration object.",
                "model_mode": "A string indicating the model's operational mode (e.g., 'train', 'prefill')."
            },
            "notes": [
                "This module is typically used when `config.scan_layers` is False or within a pipeline stage.",
                "It is designed to handle the output format of both scanned and unscanned layers by checking `config.scan_layers` to correctly process the layer's return value."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass by sequentially applying the decoder layer for `num_decoder_layers` iterations.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Loop from `lyr` in `range(self.num_decoder_layers)`.",
                        "Instantiate `self.decoder_layer` with a unique name for each layer (e.g., 'layers_0').",
                        "Call the instantiated layer with `inputs` and other arguments (`decoder_segment_ids`, `decoder_positions`, etc.).",
                        "Update `inputs` with the result of the layer call.",
                        "If `self.config.scan_layers` is True, unwrap the tuple output from the layer to get the tensor for the next iteration.",
                        "After the loop, return the final `inputs`, formatted as a tuple `(inputs, None)` if `config.scan_layers` is True, or as a tensor otherwise."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is False, returns a tensor of shape [batch_size, sequence_length, hidden_dim]. If True, returns a tuple `(tensor, None)` where the tensor has the same shape."
                    },
                    "dependencies": [
                        "self.decoder_layer"
                    ],
                    "notes": [
                        "This method forwards arguments such as `decoder_segment_ids`, `decoder_positions`, `deterministic`, `model_mode`, `slot`, and `page_state` to each underlying decoder layer instance."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/decoders.py#Decoder",
        "file_path": "src/MaxText/layers/decoders.py",
        "code_block": "class Decoder(nn.Module):\n  \"\"\"A stack of decoder layers as a part of an encoder-decoder architecture.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  quant: None | Quant = None\n  model_mode: str = MODEL_MODE_TRAIN\n\n  def setup(self):\n    \"\"\"Initialize decoder layer.\"\"\"\n    self.decoder_layer = self.get_decoder_layers()\n    self.norm_layer = self.get_norm_layer(num_features=self.config.emb_dim)\n    if self.config.using_pipeline_parallelism:\n      pipeline_stage_module = self.get_pipeline_stage_module(self.decoder_layer)\n      remat_policy = self.get_remat_policy()\n      self.pipeline_module = pipeline.Pipeline(\n          config=self.config, mesh=self.mesh, layers=pipeline_stage_module, remat_policy=remat_policy\n      )\n\n  def minimal_policy(self, with_context=False):\n    \"\"\"Helper for creating minimal checkpoint policies.\"\"\"\n    names = [\n        \"query_proj\",\n        \"value_proj\",\n        \"key_proj\",\n        \"qkv_proj\",\n        \"out_proj\",\n        \"mlpwi_0\",\n        \"mlpwi_1\",\n        \"mlpwi\",\n        \"mlpwo\",\n    ]\n    if with_context:\n      names.append(\"context\")\n    return jax.checkpoint_policies.save_only_these_names(*names)\n\n  def get_remat_policy(self):\n    \"\"\"Get remat policy\"\"\"\n    policy = None\n    cfg = self.config\n    if cfg.remat_policy != \"none\":\n      if cfg.remat_policy in (\"minimal_with_context\", \"minimal_flash\"):\n        # save all\n        if cfg.remat_policy == \"minimal_flash\":\n            max_logging.log(\"WARNING: 'minimal_flash' will be deprecated soon, please use 'minimal_with_context' instead.\")\n        policy = self.minimal_policy(with_context=True)\n      elif cfg.remat_policy == \"minimal\":\n        # save all except context\n        policy = self.minimal_policy()\n      elif cfg.remat_policy == \"save_dot_with_context_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"context\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlpwi\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n            \"mlpwo\",\n        )\n      elif cfg.remat_policy == \"save_dot_except_mlp\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n            \"out_proj\",\n        )\n      elif cfg.remat_policy == \"save_qkv_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"query_proj\",\n            \"value_proj\",\n            \"key_proj\",\n            \"qkv_proj\",\n        )\n      elif cfg.remat_policy == \"qkv_proj_offloaded\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\"query_proj\", \"value_proj\", \"key_proj\"],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"minimal_offloaded\":\n        # offload all except context\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=[],\n            names_which_can_be_offloaded=[\n                \"query_proj\",\n                \"value_proj\",\n                \"key_proj\",\n                \"qkv_proj\",\n                \"out_proj\",\n                \"mlpwi_0\",\n                \"mlpwi_1\",\n                \"mlpwi\",\n                \"mlpwo\",\n            ],\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"custom\":\n        policy = jax.checkpoint_policies.save_and_offload_only_these_names(\n            names_which_can_be_saved=cfg.tensors_on_device,\n            names_which_can_be_offloaded=cfg.tensors_to_offload,\n            offload_src=\"device\",\n            offload_dst=\"pinned_host\",\n        )\n      elif cfg.remat_policy == \"save_out_proj\":\n        policy = jax.checkpoint_policies.save_only_these_names(\n            \"out_proj\",\n        )\n      else:\n        assert cfg.remat_policy == \"full\", \"Remat policy needs to be on list of remat policies\"\n        policy = None\n    return policy\n\n  def get_decoder_layers(self):\n    \"\"\"Retrieves a list of decoder layer classes based on the `decoder_block` config.\n\n    Returns:\n        A list containing one or more `nn.Module` classes for the decoder.\n    \"\"\"\n    match self.config.decoder_block:\n      case DecoderBlockType.DEFAULT:\n        return [DecoderLayer]\n      case DecoderBlockType.LLAMA2:\n        return [llama2.LlamaDecoderLayer]\n      case DecoderBlockType.MISTRAL:\n        # TODO(ranran): update to Mistral with sliding window attention\n        return [mistral.MistralDecoderLayer]\n      case DecoderBlockType.MIXTRAL:\n        return [mixtral.MixtralDecoderLayer]\n      case DecoderBlockType.DEEPSEEK:\n        return [deepseek.DeepSeekDenseLayer, deepseek.DeepSeekMoELayer]\n      case DecoderBlockType.GEMMA:\n        return [gemma.GemmaDecoderLayer]\n      case DecoderBlockType.GEMMA2:\n        return [gemma2.Gemma2DecoderLayer]\n      case DecoderBlockType.GEMMA3:\n        return [gemma3.Gemma3DecoderLayer]\n      case DecoderBlockType.GPT3:\n        return [gpt3.Gpt3DecoderLayer]\n      case DecoderBlockType.GPT_OSS:\n        return [gpt_oss.GptOssScannableBlock] if self.config.scan_layers else [gpt_oss.GptOssDecoderLayer]\n      case DecoderBlockType.QWEN3:\n        return [qwen3.Qwen3DecoderLayer]\n      case DecoderBlockType.QWEN3_MOE:\n        return [qwen3.Qwen3MoeDecoderLayer]\n      case DecoderBlockType.SIMPLE:\n        return [simple_layer.SimpleDecoderLayer]\n      case DecoderBlockType.SIMPLE_MLP:\n        return [simple_layer.SimpleMlpDecoderLayer]\n      case DecoderBlockType.LLAMA4:\n        return [llama4.Llama4ScannableBlock] if self.config.scan_layers else [llama4.Llama4DecoderLayer]\n      case _:\n        # Default case to handle any unknown decoder block types.\n        raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def set_remat_policy(self, block_layers, policy):\n    \"\"\"Set remat policy\"\"\"\n    RemattedBlockLayers = []\n    for block_layer in block_layers:\n      if self.config.parameter_memory_host_offload:\n        # Define parameter movement with mesh-based sharding\n        def move_to_device(variables):\n          \"\"\"Move parameters to device with proper sharding.\"\"\"\n\n          def map_fn(path, value):\n            max_logging.log(f\"models.py: Moving parameter {path} to device\")\n            return jax.device_put(value, max_utils.device_space())\n\n          return jax.tree_util.tree_map_with_path(map_fn, variables)\n\n        # Transform layer class before remat\n        block_layer = nn.map_variables(block_layer, [\"params\"], move_to_device, mutable=True)\n\n      # Apply remat policy to layer\n      layer = nn.remat(\n          block_layer,\n          prevent_cse=not self.config.scan_layers,\n          policy=policy,\n          static_argnums=(4, 5),  # Deterministic and model mode are static arguments.\n      )\n      RemattedBlockLayers.append(layer)\n    return RemattedBlockLayers\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer (return type inherits from nn.Module)\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.QWEN3_MOE,\n        DecoderBlockType.GPT_OSS,\n        DecoderBlockType.SIMPLE,\n        DecoderBlockType.SIMPLE_MLP,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(rms_norm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      return functools.partial(gpt3.gpt3_layer_norm, num_features=num_features, reductions_in_fp32=False, use_bias=True)\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def scan_decoder_layers(self, cfg, decoder_layer, length, metadata_axis_name, mesh, in_axes_tuple, **kwargs):\n    \"\"\"scan decoder layers, calls `flax.linen.transforms.scan`\"\"\"\n    initializing = self.is_mutable_collection(\"params\")\n    params_spec = cfg.param_scan_axis if initializing else ScanIn(cfg.param_scan_axis)\n    cache_spec = 0\n    scan_fn = nn.scan(\n        decoder_layer,\n        variable_axes={\n            \"params\": params_spec,\n            \"cache\": cache_spec,\n            \"intermediates\": 0,\n            \"aqt\": 0,\n            \"_overwrite_with_gradient\": 0,\n        },\n        split_rngs={\n            \"params\": True,\n            \"dropout\": cfg.enable_dropout,\n        },\n        in_axes=in_axes_tuple,\n        length=length,\n        metadata_params={nn.PARTITION_NAME: metadata_axis_name},\n    )\n    return scan_fn(config=cfg, mesh=mesh, name=metadata_axis_name, quant=self.quant, model_mode=self.model_mode, **kwargs)\n\n  def get_pipeline_stage_module(self, decoder_blocks):\n    \"\"\"get pipeline stage module\"\"\"\n\n    def get_layer_to_pipeline(blocks, cfg):\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        return blocks[1]  # return the sparse block\n      else:\n        return blocks[0]\n\n    cfg = self.config\n    base_stage = get_layer_to_pipeline(decoder_blocks, cfg)\n    if cfg.set_remat_policy_on_layers_per_stage:\n      policy = self.get_remat_policy()\n      base_stage = self.set_remat_policy([base_stage], policy)[0]\n    if cfg.num_layers_per_pipeline_stage == 1:\n      stage_module = base_stage(config=cfg, mesh=self.mesh, quant=self.quant, model_mode=self.model_mode)\n    elif cfg.scan_layers_per_stage:\n      stage_module = self.scan_decoder_layers(\n          cfg,\n          base_stage,\n          cfg.num_layers_per_pipeline_stage,\n          \"layers_per_stage\",\n          self.mesh,\n          in_axes_tuple=(nn.broadcast,) * 4,\n      )\n    else:\n      stage_module = SequentialBlockDecoderLayers(\n          decoder_layer=base_stage,\n          num_decoder_layers=cfg.num_layers_per_pipeline_stage,\n          config=cfg,\n          mesh=self.mesh,\n          quant=self.quant,\n          model_mode=self.model_mode,\n      )\n    return stage_module\n\n  @nn.compact\n  def _apply_embedding(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      deterministic,\n      image_embeddings=None,\n      bidirectional_mask=None,\n  ):\n    \"\"\"Applies token and positional embeddings to the input tokens.\"\"\"\n    cfg = self.config\n\n    y = shared_embedding(decoder_input_tokens.astype(\"int32\"), model_mode=self.model_mode)\n\n    # Merge the image embeddings with the text embeddings for multimodal models\n    if image_embeddings is not None and cfg.use_multimodal:\n      if cfg.model_name in [\"gemma3-4b\", \"gemma3-12b\", \"gemma3-27b\", \"llama4-17b-16e\", \"llama4-17b-128e\"]:\n        y = multimodal_utils.merge_mm_embeddings(\n            text_embeddings=y,\n            vision_embeddings=image_embeddings,\n            mask=bidirectional_mask,\n        )\n      # TODO(hengtaoguo): Add support for other multimodal models such as Llama4, refactor if needed\n      else:\n        raise ValueError(f\"Unsupported model_name for multimodal: {cfg.model_name}\")\n\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n    y = y.astype(cfg.dtype)\n\n    if cfg.use_untrainable_positional_embedding:\n      y = positional_embedding_as_linen(embedding_dims=cfg.base_emb_dim)(y, decoder_positions)\n\n    if cfg.trainable_position_size > 0:\n      y += embed_as_linen(\n          num_embeddings=cfg.trainable_position_size,\n          num_features=cfg.emb_dim,\n          dtype=cfg.dtype,\n          embedding_init=nn.initializers.normal(stddev=1.0),\n          name=\"position_embedder\",\n          config=cfg,\n      )(decoder_positions, model_mode=self.model_mode)\n    return y\n\n  @nn.compact\n  def _apply_output_head(self, shared_embedding: nn.Module | nnx.Module, y, deterministic):\n    \"\"\"Applies final normalization and projects hidden states to logits.\"\"\"\n\n    cfg = self.config\n    y = self.get_norm_layer(num_features=y.shape[-1])(\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"decoder_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n    )(y)\n    y = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(y, deterministic=deterministic)\n\n    # [batch, length, emb_dim] -> [batch, length, vocab_size]\n    if cfg.logits_via_embedding:\n      # Use the transpose of embedding matrix for logit transform.\n      if isinstance(shared_embedding, nnx.Module):\n        embedding_table = shared_embedding.embedding.value\n      else:\n        embedding_table = shared_embedding.variables[\"params\"][\"embedding\"]\n      if isinstance(embedding_table, nn.spmd.LogicallyPartitioned):\n        embedding_table = embedding_table.unbox()\n      attend_dtype = jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype\n      logits = attend_on_embedding(y, embedding_table, attend_dtype, self.config)\n\n      if self.config.normalize_embedding_logits:\n        # Correctly normalize pre-softmax logits for this shared case.\n        logits = logits / jnp.sqrt(y.shape[-1])\n      if cfg.final_logits_soft_cap:\n        logits = logits / cfg.final_logits_soft_cap\n        logits = jnp.tanh(logits) * cfg.final_logits_soft_cap\n    else:\n      logits = linears.dense_general(\n          inputs_shape=y.shape,\n          out_features_shape=cfg.vocab_size,\n          weight_dtype=cfg.weight_dtype,\n          dtype=jnp.float32 if cfg.logits_dot_in_fp32 else cfg.dtype,  # for logit training stability\n          kernel_axes=(\"embed\", \"vocab\"),\n          name=\"logits_dense\",\n          matmul_precision=self.config.matmul_precision,\n          parameter_memory_host_offload=cfg.parameter_memory_host_offload,\n      )(\n          y\n      )  # We do not quantize the logits matmul.\n    if self.model_mode in (MODEL_MODE_PREFILL, MODEL_MODE_AUTOREGRESSIVE):\n      logits = nn.with_logical_constraint(logits, (None, None, \"activation_vocab\"))\n    else:\n      logits = nn.with_logical_constraint(\n          logits, (\"activation_embed_and_logits_batch\", \"activation_length_no_exp\", \"activation_vocab\")\n      )\n\n    if self.config.cast_logits_to_fp32:\n      logits = logits.astype(jnp.float32)\n\n    return logits\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding: nn.Module | nnx.Module,\n      decoder_input_tokens,\n      decoder_positions,\n      decoder_segment_ids=None,\n      deterministic=False,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      bidirectional_mask: None | Any = None,\n      image_embeddings: None | jnp.ndarray = None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    assert decoder_input_tokens.ndim == 2  # [batch, len]\n\n    # [batch, length] -> [batch, length, emb_dim]\n    y = self._apply_embedding(\n        shared_embedding,\n        decoder_input_tokens,\n        decoder_positions,\n        deterministic,\n        image_embeddings,\n        bidirectional_mask\n    )\n\n    policy = self.get_remat_policy()\n    RemattedBlockLayers = self.set_remat_policy(self.decoder_layer, policy)\n    # scan does not support kwargs in layer call, passing broadcast_args as positional arg\n    broadcast_args = (\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        self.model_mode,\n    )\n    if cfg.using_pipeline_parallelism:\n      if cfg.pipeline_fsdp_ag_once:\n        partition_spec = self.pipeline_module.get_weight_sharding(\n            y, decoder_segment_ids, decoder_positions, deterministic, self.model_mode\n        )\n      else:\n        partition_spec = None  # This partition spec is only used for the fsdp_ag_once feature.\n      if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n        assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n        dense_layer = RemattedBlockLayers[0]\n        moe_layer = RemattedBlockLayers[1]\n        num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n        num_moe_layers_outside_pp = num_moe_layers - self.config.pipeline_parallel_layers\n        logical_axis_rules_pp_as_dp = maxtext_utils.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n        # We chose not to pipeline the dense layers, only sparse for SPMD.\n        with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          )(y, *broadcast_args)\n          if num_moe_layers_outside_pp > 0:\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                moe_layer,\n                num_moe_layers_outside_pp,\n                \"moe_layers\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n            )(y, *broadcast_args)\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n      else:  # Not DeepSeek\n        y = self.pipeline_module(y, *broadcast_args, partition_spec=partition_spec)\n        remaining_layers = self.config.num_decoder_layers - self.config.pipeline_parallel_layers\n        if remaining_layers > 0:\n          logical_axis_rules_pp_as_dp = maxtext_utils.logical_axis_rules_pp_act_as_dp(self.config.logical_axis_rules)\n          with self.mesh, nn.partitioning.axis_rules(logical_axis_rules_pp_as_dp):\n            y, _ = self.scan_decoder_layers(\n                cfg,\n                RemattedBlockLayers[0],\n                remaining_layers,\n                \"layers_outside_pipeline\",\n                mesh,\n                in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n            )(y, *broadcast_args)\n    else:\n      if cfg.scan_layers:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Scanned layers must have a length of 2 using deepseek.\"\n          layer_call_kwargs = {\n              \"page_state\": page_state,\n              \"previous_chunk\": previous_chunk,\n              \"slot\": slot,\n          }\n          dense_layer = RemattedBlockLayers[0]\n          dense_layer.__call__ = functools.partial(dense_layer.__call__, **layer_call_kwargs)\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              dense_layer,\n              cfg.first_num_dense_layers,\n              \"dense_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          )(y, *broadcast_args)\n          moe_layer = RemattedBlockLayers[1]\n          moe_layer.__call__ = functools.partial(moe_layer.__call__, **layer_call_kwargs)\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              moe_layer,\n              num_moe_layers,\n              \"moe_layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          )(y, *broadcast_args)\n        elif cfg.decoder_block == DecoderBlockType.GEMMA3:\n          y = self._apply_gemma3_scanned_blocks(\n              y,\n              decoder_segment_ids,\n              decoder_positions,\n              deterministic,\n              bidirectional_mask,\n              previous_chunk,\n              page_state,\n              slot,\n          )\n        else:\n          RemattedBlockLayer = RemattedBlockLayers[0]\n          scan_length = int(cfg.num_decoder_layers / cfg.inhomogeneous_layer_cycle_interval)\n          layer_kwargs = {}\n          if cfg.decoder_block == DecoderBlockType.LLAMA4:\n            layer_kwargs = {\n                \"nope_layer_interval\": self.config.nope_layer_interval,\n                \"interleave_moe_layer_step\": self.config.interleave_moe_layer_step,\n            }\n            broadcast_args += (bidirectional_mask,)\n          y, _ = self.scan_decoder_layers(\n              cfg,\n              RemattedBlockLayer,\n              scan_length,\n              \"layers\",\n              mesh,\n              in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n              **layer_kwargs,\n          )(y, *broadcast_args)\n      else:\n        if cfg.decoder_block == DecoderBlockType.DEEPSEEK:\n          assert len(RemattedBlockLayers) == 2, \"Unscanned layers must have a length of 2 using deepseek.\"\n          dense_layer = RemattedBlockLayers[0]\n          moe_layer = RemattedBlockLayers[1]\n\n          layers = [dense_layer, moe_layer]\n          layer_prefixes = [\"dense_layers\", \"moe_layers\"]\n          num_moe_layers = cfg.num_decoder_layers - cfg.first_num_dense_layers\n          num_layers_list = [cfg.first_num_dense_layers, num_moe_layers]\n          # Iterate over the two layer groups (dense and MoE) and apply layer transformation\n          for layer, num_layers, layer_prefix in zip(layers, num_layers_list, layer_prefixes):\n            for index in range(num_layers):\n              y = layer(config=cfg, mesh=mesh, name=f\"{layer_prefix}_{index}\", quant=self.quant, model_mode=self.model_mode)(\n                  y,\n                  decoder_segment_ids,\n                  decoder_positions,\n                  deterministic,\n                  self.model_mode,\n                  previous_chunk=previous_chunk,\n                  page_state=page_state,\n                  slot=slot,\n              )\n        else:\n          for lyr in range(cfg.num_decoder_layers):\n            RemattedBlockLayer = RemattedBlockLayers[0]\n            layer_kwargs = {}\n            layer_call_kwargs = {}\n            if cfg.decoder_block == DecoderBlockType.GEMMA3:\n              # Gemma3 uses both global and sliding window attention depending on the layer index.\n              layer_kwargs = {\"attention_type\": gemma3.get_attention_type(layer_id=lyr)}\n              layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n            if cfg.decoder_block == DecoderBlockType.LLAMA4:\n              layer_kwargs = {\n                  \"is_nope_layer\": llama4.determine_is_nope_layer(lyr, self.config.nope_layer_interval),\n                  \"is_moe_layer\": llama4.determine_is_moe_layer(lyr, self.config.interleave_moe_layer_step),\n              }\n              layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n            if cfg.decoder_block == DecoderBlockType.GPT_OSS:\n              layer_kwargs = {\"attention_type\": gpt_oss.get_attention_type(layer_id=lyr)}\n            layer = RemattedBlockLayer(\n                config=cfg, mesh=mesh, name=f\"layers_{lyr}\", quant=self.quant, model_mode=self.model_mode, **layer_kwargs\n            )\n            y = layer(\n                y,\n                decoder_segment_ids,\n                decoder_positions,\n                deterministic,\n                self.model_mode,\n                previous_chunk=previous_chunk,\n                page_state=page_state,\n                slot=slot,\n                **layer_call_kwargs,\n            )\n\n    assert isinstance(y, jax.Array)\n\n    # After the final transformer layer, `y` holds the raw, un-normalized hidden state.\n    hidden_state = y\n\n    logits = self._apply_output_head(shared_embedding, hidden_state, deterministic)\n\n    # The API of the Decoder is now a tuple, providing both the main output\n    # and the raw hidden state needed for auxiliary tasks.\n    return logits, hidden_state\n\n  def _apply_gemma3_scanned_blocks(\n      self,\n      y,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      bidirectional_mask,\n      previous_chunk,\n      page_state,\n      slot,\n  ):\n    \"\"\"Applies Gemma3 scanned decoder blocks, handling main scan and remainders.\"\"\"\n\n    cfg = self.config\n    mesh = self.mesh\n\n    # Define the repeating pattern length and calculate how many full blocks to scan\n    attention_pattern_length = len(gemma3.GEMMA3_ATTENTION_PATTERN)\n    scan_length = cfg.num_decoder_layers // attention_pattern_length\n\n    policy = self.get_remat_policy()\n    RemattedGemma3Block = self.set_remat_policy([gemma3.Gemma3ScannableBlock], policy)[0]\n\n    layer_call_kwargs = {\"bidirectional_mask\": bidirectional_mask}\n    layer_kwargs = {\"num_of_layers\": attention_pattern_length}\n\n    # Apply the main scan over the full blocks\n    if scan_length > 0:\n      broadcast_args = (\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          self.model_mode,\n      )\n      y, _ = self.scan_decoder_layers(\n          cfg,\n          RemattedGemma3Block,\n          scan_length,\n          \"layers\",\n          mesh,\n          in_axes_tuple=(nn.broadcast,) * len(broadcast_args),\n          **layer_kwargs,\n      )(y, *broadcast_args, **layer_call_kwargs)\n\n    # Apply any remaining layers that did not fit into a full scanned block\n    num_remaining_layers = cfg.num_decoder_layers % attention_pattern_length\n    if num_remaining_layers > 0:\n      # We name the remainder block with a 'remainder' suffix to avoid parameter name collisions\n      rem_layer_kwargs = {\"num_of_layers\": num_remaining_layers}\n      layer = RemattedGemma3Block(\n          config=cfg, mesh=mesh, quant=self.quant, model_mode=self.model_mode, name=\"layers_remainder\", **rem_layer_kwargs\n      )\n      y, _ = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          self.model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          **layer_call_kwargs,\n      )\n    return y",
        "analysis": {
            "module_type": "transformer_decoder",
            "purpose": "A highly configurable stack of transformer decoder layers that processes input token embeddings to produce output logits and the final hidden state.",
            "input": {
                "shape": "The primary input `decoder_input_tokens` has shape [batch_size, sequence_length]. This is transformed by an embedding layer to [batch_size, sequence_length, embedding_dim] before being processed by the decoder layers.",
                "dtype": "int32 for tokens, then cfg.dtype (e.g., float32 or bfloat16) for embeddings."
            },
            "processing_steps": [
                "Apply token and positional embeddings to input tokens using `_apply_embedding`, optionally merging multimodal embeddings.",
                "Select and apply a rematerialization (gradient checkpointing) policy based on the configuration.",
                "Process the embeddings through a stack of decoder layers. The execution strategy (pipeline parallelism, layer scanning via `nn.scan`, or sequential iteration) is determined by the configuration.",
                "Apply a final normalization layer to the output of the last decoder layer.",
                "Project the normalized hidden state to vocabulary-sized logits using `_apply_output_head`."
            ],
            "output": {
                "shape": "A tuple of (logits, hidden_state), where logits have shape [batch_size, sequence_length, vocab_size] and hidden_state has shape [batch_size, sequence_length, embedding_dim]."
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.sharding.Mesh",
                "Config",
                "Quant",
                "pipeline.Pipeline",
                "Various decoder layer implementations (e.g., DecoderLayer, llama2.LlamaDecoderLayer, gemma3.Gemma3DecoderLayer)",
                "Various normalization layers (e.g., rms_norm, gpt3.gpt3_layer_norm)"
            ],
            "parameters": {
                "config.decoder_block": "Specifies the type of decoder layer to use (e.g., LLAMA2, GEMMA3, MISTRAL), determining the model's architecture.",
                "config.num_decoder_layers": "The total number of decoder layers in the stack.",
                "config.remat_policy": "Defines the gradient checkpointing strategy to save memory (e.g., 'full', 'minimal').",
                "config.scan_layers": "A boolean that, if true, uses `flax.linen.scan` to execute the decoder layers, which can improve compilation and execution time.",
                "config.using_pipeline_parallelism": "A boolean that, if true, enables pipeline parallelism to distribute layers across multiple devices.",
                "config.logits_via_embedding": "A boolean that, if true, ties the weights of the output projection layer with the token embedding matrix."
            },
            "notes": [
                "This module is highly flexible and supports a wide range of transformer architectures and parallelism configurations.",
                "The `__call__` method contains complex control flow to handle different execution paths based on the model configuration (e.g., pipeline vs. not, scan vs. not, different model types like DeepSeek or Gemma3).",
                "It supports multimodal inputs by merging text and image embeddings at the input stage."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the decoder layer type, normalization layer, and the pipeline module if pipeline parallelism is enabled.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `get_decoder_layers` to determine the layer class.",
                        "Call `get_norm_layer` to determine the final normalization function.",
                        "If `config.using_pipeline_parallelism` is true, initialize `self.pipeline_module`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.get_decoder_layers",
                        "self.get_norm_layer",
                        "self.get_pipeline_stage_module",
                        "pipeline.Pipeline"
                    ],
                    "notes": [
                        "This is a standard Flax setup method that is called once during module instantiation."
                    ]
                },
                "minimal_policy": {
                    "purpose": "A helper method to create a minimal JAX checkpointing policy that saves only specified activation tensors.",
                    "input": {
                        "shape": "A boolean `with_context`.",
                        "dtype": "bool"
                    },
                    "processing_steps": [
                        "Define a list of tensor names to save.",
                        "Optionally add 'context' to the list.",
                        "Call `jax.checkpoint_policies.save_only_these_names` with the list."
                    ],
                    "output": {
                        "shape": "A JAX checkpoint policy object."
                    },
                    "dependencies": [
                        "jax.checkpoint_policies.save_only_these_names"
                    ],
                    "notes": [
                        "This is a utility function used by `get_remat_policy`."
                    ]
                },
                "get_remat_policy": {
                    "purpose": "Selects and returns a JAX checkpointing (rematerialization) policy based on the `config.remat_policy` string.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Read `self.config.remat_policy`.",
                        "Use a series of conditional statements to select the appropriate policy from `jax.checkpoint_policies` or create one using `self.minimal_policy`."
                    ],
                    "output": {
                        "shape": "A JAX checkpoint policy object or None."
                    },
                    "dependencies": [
                        "self.minimal_policy",
                        "jax.checkpoint_policies"
                    ],
                    "notes": [
                        "Supports numerous predefined policies, including those that offload activations to host memory."
                    ]
                },
                "get_decoder_layers": {
                    "purpose": "Retrieves a list of decoder layer classes based on the `config.decoder_block` setting.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Use a `match/case` statement on `self.config.decoder_block` to return the corresponding layer class(es)."
                    ],
                    "output": {
                        "shape": "A list of `nn.Module` classes."
                    },
                    "dependencies": [
                        "DecoderBlockType",
                        "deepseek.DeepSeekDenseLayer",
                        "gemma3.Gemma3DecoderLayer",
                        "llama2.LlamaDecoderLayer",
                        "etc."
                    ],
                    "notes": [
                        "For some architectures like DeepSeek, this method returns two different layer types (dense and MoE)."
                    ]
                },
                "set_remat_policy": {
                    "purpose": "Wraps decoder layer classes with `nn.remat` to apply a specified gradient checkpointing policy.",
                    "input": {
                        "shape": "A list of `nn.Module` classes and a JAX checkpoint policy object.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through the input list of layer classes.",
                        "Optionally wrap the layer to handle parameter offloading if `config.parameter_memory_host_offload` is true.",
                        "Wrap the layer class with `nn.remat`, passing the policy."
                    ],
                    "output": {
                        "shape": "A list of rematerialized (wrapped) `nn.Module` classes."
                    },
                    "dependencies": [
                        "flax.linen.remat",
                        "flax.linen.map_variables"
                    ],
                    "notes": []
                },
                "get_norm_layer": {
                    "purpose": "Selects and returns the appropriate normalization function (e.g., RMSNorm) based on the `config.decoder_block`.",
                    "input": {
                        "shape": "An integer `num_features`.",
                        "dtype": "int"
                    },
                    "processing_steps": [
                        "Use conditional statements on `self.config.decoder_block` to select a normalization function.",
                        "Return a partially applied function with `num_features` bound."
                    ],
                    "output": {
                        "shape": "A callable normalization function."
                    },
                    "dependencies": [
                        "functools.partial",
                        "rms_norm",
                        "gpt3.gpt3_layer_norm"
                    ],
                    "notes": []
                },
                "scan_decoder_layers": {
                    "purpose": "A wrapper around `flax.linen.scan` to efficiently apply decoder layers over the layer axis.",
                    "input": {
                        "shape": "Takes the config, decoder_layer class, length (number of layers to scan), and other `nn.scan` arguments.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Configure `variable_axes` and `split_rngs` for transformer layers.",
                        "Instantiate `nn.scan` with the provided decoder layer class and configuration.",
                        "Call the resulting scan function."
                    ],
                    "output": {
                        "shape": "The output of the scanned layers."
                    },
                    "dependencies": [
                        "flax.linen.scan",
                        "flax.linen.partitioning.ScanIn"
                    ],
                    "notes": [
                        "This is a key optimization for models where `config.scan_layers` is enabled."
                    ]
                },
                "get_pipeline_stage_module": {
                    "purpose": "Constructs the module for a single stage of a pipeline-parallel model.",
                    "input": {
                        "shape": "A list of decoder block classes.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Select the base layer for the pipeline stage.",
                        "Optionally apply a rematerialization policy.",
                        "Construct the stage module, which can be a single layer, a scanned set of layers, or a sequential block of layers, depending on the configuration."
                    ],
                    "output": {
                        "shape": "An `nn.Module` instance representing one pipeline stage."
                    },
                    "dependencies": [
                        "self.set_remat_policy",
                        "self.scan_decoder_layers",
                        "SequentialBlockDecoderLayers"
                    ],
                    "notes": []
                },
                "_apply_embedding": {
                    "purpose": "Applies token and positional embeddings to the input tokens, and handles merging of multimodal embeddings.",
                    "input": {
                        "shape": "`decoder_input_tokens`: [batch_size, sequence_length]",
                        "dtype": "`decoder_input_tokens`: int32"
                    },
                    "processing_steps": [
                        "Call the shared embedding module on the input tokens.",
                        "If multimodal, merge image embeddings with text embeddings.",
                        "Apply dropout.",
                        "Add untrainable and/or trainable positional embeddings."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "nn.Dropout",
                        "multimodal_utils.merge_mm_embeddings",
                        "positional_embedding_as_linen",
                        "embed_as_linen"
                    ],
                    "notes": [
                        "This is a private helper method for the main `__call__`."
                    ]
                },
                "_apply_output_head": {
                    "purpose": "Applies final normalization and projects the final hidden states to vocabulary logits.",
                    "input": {
                        "shape": "`y`: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "cfg.dtype"
                    },
                    "processing_steps": [
                        "Apply the final normalization layer (e.g., RMSNorm).",
                        "Apply dropout.",
                        "Project hidden states to logits, either via a dot product with the embedding matrix or a separate dense layer.",
                        "Optionally apply soft capping to logits.",
                        "Optionally cast logits to float32."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, vocab_size]"
                    },
                    "dependencies": [
                        "self.get_norm_layer",
                        "nn.Dropout",
                        "attend_on_embedding",
                        "linears.dense_general"
                    ],
                    "notes": [
                        "This is a private helper method for the main `__call__`."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the main forward pass of the decoder, processing token embeddings through the stack of layers to produce logits.",
                    "input": {
                        "shape": "`decoder_input_tokens`: [batch_size, sequence_length]",
                        "dtype": "`decoder_input_tokens`: int32"
                    },
                    "processing_steps": [
                        "Call `_apply_embedding` to get initial embeddings.",
                        "Determine and apply the rematerialization policy to the decoder layers.",
                        "Execute the stack of decoder layers using the configured strategy (pipeline, scan, or loop).",
                        "Call `_apply_output_head` on the final hidden state to compute logits."
                    ],
                    "output": {
                        "shape": "A tuple of (logits, hidden_state), with shapes [batch, len, vocab_size] and [batch, len, emb_dim] respectively."
                    },
                    "dependencies": [
                        "self._apply_embedding",
                        "self.get_remat_policy",
                        "self.set_remat_policy",
                        "self.pipeline_module",
                        "self.scan_decoder_layers",
                        "self._apply_output_head",
                        "self._apply_gemma3_scanned_blocks"
                    ],
                    "notes": [
                        "This is the primary method for the module and contains extensive control flow to handle various model architectures and parallelism strategies."
                    ]
                },
                "_apply_gemma3_scanned_blocks": {
                    "purpose": "A specialized method to apply Gemma3 decoder blocks using `nn.scan`, handling its repeating attention pattern and any remaining layers.",
                    "input": {
                        "shape": "`y`: [batch_size, sequence_length, embedding_dim]",
                        "dtype": "cfg.dtype"
                    },
                    "processing_steps": [
                        "Calculate the number of full repeating blocks to be scanned.",
                        "Call `scan_decoder_layers` with `gemma3.Gemma3ScannableBlock` for the main repeating part.",
                        "Instantiate and call a separate `gemma3.Gemma3ScannableBlock` for any remaining layers that don't form a full block."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, embedding_dim]"
                    },
                    "dependencies": [
                        "gemma3.Gemma3ScannableBlock",
                        "self.scan_decoder_layers"
                    ],
                    "notes": [
                        "This logic is specific to the Gemma3 architecture and is called from the main `__call__` method when `decoder_block` is `GEMMA3` and `scan_layers` is true."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleDecoderLayer(nn.Module):\n  \"\"\"Decoder layer consisting of a single [embed, embed] weight matrix.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | quantizations.AqtQuantization = None\n\n  def setup(self):\n    self.weight_mat = self.param(\n        \"weights\",\n        nn.with_logical_partitioning(nn.initializers.lecun_normal(), (\"embed\", \"mlp\")),\n        (self.config.emb_dim, self.config.emb_dim),\n    )\n\n  def __call__(\n      self, inputs: jnp.ndarray, positions, segmentation, deterministic, model_mode, previous_chunk=None, page_state=None\n  ):\n    if self.config.scan_layers:\n      return inputs @ self.weight_mat.astype(inputs.dtype), None\n    else:\n      return inputs @ self.weight_mat.astype(inputs.dtype)",
        "analysis": {
            "module_type": "simple_decoder_layer",
            "purpose": "A simple decoder layer for testing purposes that applies a single linear transformation (matrix multiplication) to its input.",
            "input": {
                "shape": "[batch_size, sequence_length, config.emb_dim]",
                "dtype": "jnp.ndarray (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Initializes a weight matrix of shape [emb_dim, emb_dim] during setup.",
                "Performs a matrix multiplication between the input tensor and the weight matrix."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, config.emb_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "jax.numpy",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.emb_dim": "The embedding dimension, which determines the size of the weight matrix.",
                "config.scan_layers": "A boolean flag that changes the return signature to be compatible with `flax.linen.scan`."
            },
            "notes": [
                "This module is likely used for testing or debugging, providing a minimal implementation of a decoder layer API.",
                "The class has attributes `mesh`, `model_mode`, and `quant` which are part of the standard layer interface but are not used in this simple implementation."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes and registers the single weight matrix for the linear transformation.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes a parameter 'weights' of shape (config.emb_dim, config.emb_dim) using a lecun_normal initializer.",
                        "Applies logical partitioning ('embed', 'mlp') to the weight matrix for model parallelism.",
                        "Assigns the created parameter to `self.weight_mat`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.Module.param",
                        "flax.linen.initializers.lecun_normal",
                        "flax.linen.with_logical_partitioning"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during model initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the linear transformation to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, config.emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Casts the layer's weight matrix to the dtype of the input tensor.",
                        "Performs a matrix multiplication between the inputs and the weight matrix.",
                        "Conditionally wraps the output in a tuple based on the `config.scan_layers` flag."
                    ],
                    "output": {
                        "shape": "Returns a tensor of shape [batch_size, sequence_length, config.emb_dim]. If `config.scan_layers` is true, it returns a tuple: (tensor, None)."
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "Accepts several unused arguments (`positions`, `segmentation`, `deterministic`, `model_mode`, `previous_chunk`, `page_state`) to conform to a standard decoder layer API.",
                        "The return value's structure depends on the `config.scan_layers` flag to support `flax.linen.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/simple_layer.py#SimpleMlpDecoderLayer",
        "file_path": "src/MaxText/layers/simple_layer.py",
        "code_block": "class SimpleMlpDecoderLayer(nn.Module):\n  \"\"\"Decoder layer consisting of [embed,mlp] followed by an [mlp,embed] matmul.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | quantizations.AqtQuantization = None\n\n  def setup(self):\n    self.ff_1 = self.param(\n        \"ff_1\",\n        nn.with_logical_partitioning(nn.initializers.lecun_normal(), (\"embed\", \"mlp\")),\n        (self.config.emb_dim, self.config.mlp_dim),\n    )\n    self.ff_2 = self.param(\n        \"ff_2\",\n        nn.with_logical_partitioning(nn.initializers.lecun_normal(), (\"mlp\", \"embed\")),\n        (self.config.mlp_dim, self.config.emb_dim),\n    )\n\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      positions,\n      segmentation,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=0,\n  ):\n    intermediate = inputs @ self.ff_1.astype(inputs.dtype)\n    output = intermediate @ self.ff_2.astype(inputs.dtype)\n    if self.config.scan_layers:\n      return output, None\n    else:\n      return output",
        "analysis": {
            "module_type": "simple_mlp_decoder_layer",
            "purpose": "A simple two-layer feed-forward network (MLP) without activation functions, used as a decoder layer primarily for testing and debugging.",
            "input": {
                "shape": "[batch_size, sequence_length, emb_dim]",
                "dtype": "jnp.ndarray (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Applies a matrix multiplication with the first feed-forward weight (ff_1) to project from `emb_dim` to `mlp_dim`.",
                "Applies a matrix multiplication with the second feed-forward weight (ff_2) to project from `mlp_dim` back to `emb_dim`."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, emb_dim]"
            },
            "dependencies": [
                "flax.linen.nn",
                "jax.numpy",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.emb_dim": "The embedding dimension of the input and output tensors.",
                "config.mlp_dim": "The intermediate dimension of the MLP.",
                "config.scan_layers": "A boolean flag that, if True, causes the `__call__` method to return a tuple `(output, None)` for compatibility with `nn.scan`."
            },
            "notes": [
                "This layer implements a purely linear transformation as it lacks any non-linear activation functions.",
                "The `quant` attribute is defined for API compatibility but is not used in this implementation.",
                "The class is intended for simple model configurations, often for testing or debugging purposes."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the two weight matrices for the MLP layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize the first weight matrix `ff_1` with shape (emb_dim, mlp_dim) and logical partitioning ('embed', 'mlp').",
                        "Initialize the second weight matrix `ff_2` with shape (mlp_dim, emb_dim) and logical partitioning ('mlp', 'embed')."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nn.Module.param",
                        "nn.with_logical_partitioning",
                        "nn.initializers.lecun_normal"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during model initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the two-layer MLP on the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, emb_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Perform matrix multiplication: `inputs @ self.ff_1`.",
                        "Perform matrix multiplication on the intermediate result: `intermediate @ self.ff_2`.",
                        "If `config.scan_layers` is True, return `(output, None)`.",
                        "Otherwise, return `output`."
                    ],
                    "output": {
                        "shape": "Returns a tensor of shape [batch_size, sequence_length, emb_dim]. If `config.scan_layers` is True, it returns a tuple `(tensor, None)`."
                    },
                    "dependencies": [],
                    "notes": [
                        "The arguments `positions`, `segmentation`, `deterministic`, `model_mode`, `previous_chunk`, `page_state`, and `slot` are accepted for API compatibility with other decoder layers but are not used in this simple implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#roll_and_mask",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def roll_and_mask(x: jnp.ndarray, shift: int = -1) -> jnp.ndarray:\n  \"\"\"\n  Performs a leftward roll on the sequence axis (axis=1) and masks the\n  newly created invalid positions at the end of the sequence.\n  Assumes input `x` has a batch dimension at axis 0 and sequence at axis 1.\n\n  Args:\n    x: The input array of shape [batch, seq_len, ...].\n    shift: The number of positions to shift left.\n\n  Returns:\n    The rolled array of the same shape as x.\n  \"\"\"\n  # If shift is 0, it's a no-op. Return the original array.\n  if shift == 0:\n    return x\n\n  # to set the last `abs(shift)` elements of the sequence to zero.\n  return jnp.roll(x, shift, axis=1).at[:, shift:, ...].set(0)",
        "analysis": {
            "module_type": "sequence_roll_and_mask",
            "purpose": "Performs a leftward circular shift on the sequence axis (axis=1) of a tensor and masks the wrapped-around elements at the end by setting them to zero.",
            "input": {
                "shape": "[batch, seq_len, ...]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Check if the `shift` amount is 0; if so, return the original input array `x`.",
                "Perform a circular roll on the input array `x` along the sequence axis (axis=1) by `shift` positions using `jnp.roll`.",
                "Set the last `abs(shift)` elements along the sequence axis to 0 to mask the values that wrapped around from the beginning."
            ],
            "output": {
                "shape": "Same as input: [batch, seq_len, ...]"
            },
            "dependencies": [
                "jax.numpy.roll"
            ],
            "parameters": {
                "x": "The input array of shape [batch, seq_len, ...].",
                "shift": "The number of positions to shift left. A negative value indicates a left shift. Defaults to -1."
            },
            "notes": [
                "The function is a no-op if `shift` is 0.",
                "It assumes the input tensor has a batch dimension at axis 0 and a sequence dimension at axis 1."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionLayer",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionLayer(nn.Module):\n  \"\"\"\n  Implements Multi-Token Prediction (MTP) step:\n      1. Normalization of previous hidden state and target token embedding.\n      2. Concatenation and Projection of normalized features.\n      3. Processing through a Transformer Decoder Layer.\n\n      Equation Representation (Conceptual):\n          norm_h = RMSNorm(h_prev)\n          norm_e = RMSNorm(e_target)\n          h_proj = W_p(concat(norm_h, norm_e))\n          h_next = TransformerLayer(h_proj, pos_ids, segment_ids, ...)\n\n      It takes the previous hidden state and target embedding as input and outputs the\n      processed hidden state from its internal transformer block.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  layer_number: int\n  transformer_layer_module: Type[DecoderLayer] = DecoderLayer\n\n  @nn.compact\n  def __call__(\n      self,\n      prev_hidden_state: jnp.ndarray,\n      target_token_embedding: jnp.ndarray,\n      position_ids: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> jnp.ndarray:\n    \"\"\"\n    Applies the MTP combination, projection, and internal transformer processing.\n\n    Args:\n        prev_hidden_state: Hidden state from the previous step/layer.\n                           Shape: [batch, seq_len, hidden_size]\n        target_token_embedding: Embedding of the target token. In the context of MTP,\n                                this often refers to a token at a position relative\n                                to the current step, where the offset is determined\n                                by the layer number `k` (i.e., token t+k).\n                                Shape: [batch, seq_len, embed_dim]\n        position_ids: Original position IDs for the sequence.\n                      Shape: [batch, seq_len]\n        decoder_segment_ids: Original segment IDs for the sequence (for attention mask).\n                             Shape: [batch, seq_len]\n        deterministic: If true, disable dropout.\n        model_mode: The current operational mode (train, eval, decode).\n\n    Returns:\n        next_hidden_state: The hidden state produced by this MTP step's internal transformer.\n                           Shape: [batch, seq_len, hidden_size]\n    \"\"\"\n    cfg = self.config\n    mesh = self.mesh\n    k = self.layer_number\n\n    # --- 1. Normalize Hidden State and Embedding ---\n    embedding_norm_layer = rms_norm(\n        num_features=target_token_embedding.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_embedding_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n    embedding_norm = embedding_norm_layer(target_token_embedding)\n\n    hidden_state_norm_layer = rms_norm(\n        num_features=prev_hidden_state.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=f\"mtp_{k}_hidden_state_norm\",\n        epsilon=cfg.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n    )\n\n    hidden_state_norm = hidden_state_norm_layer(prev_hidden_state)\n\n    # --- 2. Concatenate Normalized Representations ---\n    # Shape: [B, S, 2*H]\n    concatenated_features = jnp.concatenate([embedding_norm, hidden_state_norm], axis=-1)\n\n    # --- 3. Project Concatenated Features ---\n    # Projects from 2*H back down to H\n    projection_layer = dense_general(\n        inputs_shape=concatenated_features.shape,\n        out_features_shape=cfg.base_emb_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        use_bias=False,\n        kernel_axes=(\"concat_embed\", \"embed\"),\n        name=f\"mtp_{k}_projection\",\n    )\n    # Shape: [B, S, H]\n    projected_features = projection_layer(concatenated_features)\n\n    # --- 4. Pass through MTP Transformer Block ---\n    output = self.transformer_layer_module(\n        config=cfg, mesh=mesh, model_mode=model_mode, name=f\"mtp_{k}_transformer_layer\"\n    )(\n        inputs=projected_features,\n        decoder_segment_ids=decoder_segment_ids,\n        decoder_positions=position_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    if isinstance(output, tuple):\n      # Handles the scan=True case, where the output is a tuple.\n      next_hidden_state = output[0]\n    else:\n      # Handles the scan=False case, where the output is a single tensor.\n      next_hidden_state = output\n\n    # Shape: [B, S, H]\n    # --- Return Processed Hidden State ---\n    return next_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_layer",
            "purpose": "Implements a single Multi-Token Prediction (MTP) step by normalizing and combining a previous hidden state with a target token embedding, then processing the result through an internal transformer decoder layer.",
            "input": {
                "shape": "N/A (Inputs are provided to the __call__ method).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes RMSNorm layers for the hidden state and target embedding.",
                "Initializes a dense projection layer.",
                "Initializes an internal transformer decoder layer (`DecoderLayer`)."
            ],
            "output": {
                "shape": "N/A (Outputs are produced by the __call__ method)."
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.linears.dense_general",
                "MaxText.layers.decoders.DecoderLayer"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like dtype, embedding dimensions, and normalization epsilon.",
                "mesh": "The JAX device mesh for model parallelism.",
                "layer_number": "An integer `k` identifying the MTP layer's position in the sequence, used for naming internal sub-modules.",
                "transformer_layer_module": "The class type for the transformer decoder layer to be used internally, which defaults to `DecoderLayer`."
            },
            "notes": [
                "This layer is designed to be a building block within a larger `MultiTokenPredictionBlock`.",
                "The `layer_number` is crucial for creating uniquely named parameters for each MTP step in a sequence."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies the MTP combination, projection, and internal transformer processing to generate the next hidden state.",
                    "input": {
                        "shape": "prev_hidden_state: [batch, seq_len, hidden_size], target_token_embedding: [batch, seq_len, embed_dim], position_ids: [batch, seq_len], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Normalize `prev_hidden_state` using an RMSNorm layer.",
                        "Normalize `target_token_embedding` using a separate RMSNorm layer.",
                        "Concatenate the two normalized tensors along the feature dimension.",
                        "Project the concatenated features back to the model's hidden dimension using a dense layer.",
                        "Pass the projected features through the internal transformer decoder layer.",
                        "Extract the final hidden state from the transformer layer's output.",
                        "Return the processed hidden state."
                    ],
                    "output": {
                        "shape": "[batch, seq_len, hidden_size]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "jnp.concatenate",
                        "dense_general",
                        "self.transformer_layer_module"
                    ],
                    "notes": [
                        "The method handles cases where the internal transformer layer returns a tuple (e.g., when using `nn.scan`) by extracting the first element as the hidden state."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#MultiTokenPredictionBlock",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "class MultiTokenPredictionBlock(nn.Module):\n  \"\"\"Orchestrates the MTP process by running a sequence of MTP layers.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  transformer_layer_module: Type[DecoderLayer]\n  decoder: Decoder\n\n  @nn.compact\n  def __call__(\n      self,\n      shared_embedding,\n      main_hidden_state,\n      input_ids,\n      target_ids,\n      target_mask,\n      position_ids,\n      decoder_segment_ids,\n      deterministic,\n  ):\n    cfg = self.config\n    # The initial hidden state for the MTP chain is the raw output from the main model.\n    mtp_hidden_state = main_hidden_state\n\n    # These variables are updated sequentially in each loop iteration,\n    # moving the prediction window one token to the right each time.\n    rolled_input_ids = input_ids\n    rolled_target_ids = target_ids\n    rolled_target_mask = target_mask\n    rolled_position_id = position_ids\n\n    # Range chosen to align with the naming convention of the paper\n    for k in range(1, cfg.mtp_num_layers + 1):\n      # Sequentially roll all tensors to prepare data for predicting the k-th future token.\n      rolled_input_ids = roll_and_mask(rolled_input_ids)\n      rolled_target_ids = roll_and_mask(rolled_target_ids)\n      rolled_target_mask = roll_and_mask(rolled_target_mask)\n      rolled_position_id = roll_and_mask(rolled_position_id)\n\n      # Embed the k-th future input tokens using the shared embedding module\n      target_token_embedding = self.decoder._apply_embedding(\n        shared_embedding,\n        rolled_input_ids,\n        rolled_position_id,\n        deterministic,\n        self.decoder.model_mode\n      )\n\n      # Instantiate and apply the MTP layer for this step\n      mtp_layer = MultiTokenPredictionLayer(\n          config=cfg,\n          mesh=self.mesh,\n          layer_number=k,\n          name=f\"mtp_layer_{k}\",\n          transformer_layer_module=self.transformer_layer_module,\n      )\n\n      next_mtp_hidden_state = mtp_layer(\n          mtp_hidden_state, target_token_embedding, position_ids, decoder_segment_ids, deterministic, self.decoder.model_mode\n      )\n\n      # Project to logits using the shared embedding transpose\n      mtp_logits = self.decoder._apply_output_head(shared_embedding, next_mtp_hidden_state, deterministic)\n\n      # Calculate cross-entropy loss for this specific layer's prediction\n      mtp_xent, _ = max_utils.cross_entropy_with_logits(\n          mtp_logits, jax.nn.one_hot(rolled_target_ids, cfg.vocab_size), 0.0\n      )\n      mtp_xent_masked = mtp_xent * rolled_target_mask\n\n      # This logic doesn't run during model initialization to avoid unwated population of the mutable collections.\n      if not self.is_initializing():\n        # For evaluation, save the top prediction and a valid token mask.\n        # This is only active for the target layer during an eval run.\n        if cfg.mtp_eval_target_module == k and self.is_mutable_collection(\"mtp_acceptance\"):\n          mtp_top_1_pred = jnp.argmax(mtp_logits, axis=-1)\n          self.sow(\"mtp_acceptance\", \"mtp_preds\", mtp_top_1_pred)\n          self.sow(\"mtp_acceptance\", \"mtp_mask\", rolled_target_mask)\n\n        # For training, save the loss components for this MTP head.\n        # This is only active during a training run.\n        if self.is_mutable_collection(\"mtp_losses\"):\n          self.sow(\"mtp_losses\", \"losses\", jnp.sum(mtp_xent_masked))\n          self.sow(\"mtp_losses\", \"weights\", jnp.sum(rolled_target_mask))\n\n      # The output of this layer is the input for the next, maintaining the causal chain.\n      mtp_hidden_state = next_mtp_hidden_state",
        "analysis": {
            "module_type": "multi_token_prediction_block",
            "purpose": "Orchestrates a sequence of Multi-Token Prediction (MTP) layers to predict multiple future tokens, calculating and storing their respective losses and evaluation metrics.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes an MTP hidden state from the main model's output.",
                "Iteratively applies a series of `MultiTokenPredictionLayer` modules.",
                "In each iteration, it shifts input data, generates embeddings, processes through an MTP layer, computes logits, and calculates loss.",
                "Sows losses and evaluation metrics into mutable collections for later aggregation."
            ],
            "output": {
                "shape": "N/A (output is via side-effects)"
            },
            "dependencies": [
                "Decoder",
                "DecoderLayer",
                "MultiTokenPredictionLayer",
                "roll_and_mask",
                "max_utils.cross_entropy_with_logits"
            ],
            "parameters": {
                "mtp_num_layers": "The number of MTP layers to apply sequentially, determining how many future tokens are predicted.",
                "mtp_eval_target_module": "The specific MTP layer index (k) to use for generating evaluation metrics during non-training runs."
            },
            "notes": [
                "This module does not return a tensor. Instead, it uses Flax's `sow` mechanism to store losses and evaluation metrics in mutable collections named 'mtp_losses' and 'mtp_acceptance'."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass, iteratively applying MTP layers to predict a sequence of future tokens and record their losses/metrics.",
                    "input": {
                        "shape": "shared_embedding: [vocab_size, hidden_dim], main_hidden_state: [batch, seq, hidden_dim], input_ids/target_ids/target_mask/position_ids/decoder_segment_ids: [batch, seq]",
                        "dtype": "float32 for embeddings/states, int32 for ids/masks"
                    },
                    "processing_steps": [
                        "Initialize the MTP hidden state with the main model's final hidden state.",
                        "Iterate from k=1 to `config.mtp_num_layers`.",
                        "In each iteration, roll input tensors leftward using `roll_and_mask` to align for predicting the k-th future token.",
                        "Embed the rolled input tokens using the shared embedding layer via `decoder._apply_embedding`.",
                        "Instantiate and apply a `MultiTokenPredictionLayer` to produce the next hidden state.",
                        "Project the next hidden state to logits using the shared output head via `decoder._apply_output_head`.",
                        "Calculate the cross-entropy loss for the current prediction step.",
                        "Conditionally sow the calculated loss (for training) or prediction metrics (for evaluation) into mutable collections using `self.sow`.",
                        "Update the MTP hidden state with the output of the current layer for the next iteration."
                    ],
                    "output": {
                        "shape": "N/A (no return value)"
                    },
                    "dependencies": [
                        "roll_and_mask",
                        "Decoder._apply_embedding",
                        "MultiTokenPredictionLayer",
                        "Decoder._apply_output_head",
                        "max_utils.cross_entropy_with_logits",
                        "self.sow"
                    ],
                    "notes": [
                        "The primary output is through side effects via `self.sow`, populating 'mtp_losses' and 'mtp_acceptance' collections.",
                        "The final hidden state from the last MTP layer is discarded."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_loss",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_loss(intermediate_outputs, config):\n  \"\"\"Calculates the Multi Token Prediction loss from intermediate outputs.\"\"\"\n  losses_path = (\"mtp_losses\", \"mtp_block\", \"losses\")\n  weights_path = (\"mtp_losses\", \"mtp_block\", \"weights\")\n\n  mtp_losses = maxtext_utils.get_nested_value(intermediate_outputs, losses_path, default=())\n  mtp_weights = maxtext_utils.get_nested_value(intermediate_outputs, weights_path, default=())\n\n  if not mtp_losses:  # MTP heads did not run\n    return 0.0\n\n  sum_of_all_mtp_losses = jnp.sum(jnp.array(mtp_losses))\n  sum_of_all_mtp_weights = jnp.sum(jnp.array(mtp_weights))\n\n  avg_mtp_loss = sum_of_all_mtp_losses / (sum_of_all_mtp_weights + EPS)\n  scaled_mtp_loss = avg_mtp_loss * config.mtp_loss_scaling_factor\n  return scaled_mtp_loss",
        "analysis": {
            "module_type": "mtp_loss_calculation",
            "purpose": "Calculates the scaled, weighted average loss for all Multi-Token Prediction (MTP) heads based on intermediate outputs from a model run.",
            "input": {
                "shape": "intermediate_outputs: A nested dictionary containing lists of scalar losses and weights. config: A configuration object.",
                "dtype": "float32 for numerical values within the dictionary."
            },
            "processing_steps": [
                "Extract lists of MTP losses and weights from the `intermediate_outputs` dictionary using `maxtext_utils.get_nested_value`.",
                "Check if the extracted losses list is empty; if so, return 0.0.",
                "Sum all collected MTP losses and weights.",
                "Calculate the average MTP loss by dividing the sum of losses by the sum of weights, adding a small epsilon (EPS) for numerical stability.",
                "Scale the average loss by the `mtp_loss_scaling_factor` from the configuration object.",
                "Return the final scaled MTP loss."
            ],
            "output": {
                "shape": "Scalar (a single float value)."
            },
            "dependencies": [
                "maxtext_utils.get_nested_value",
                "jax.numpy.sum",
                "jax.numpy.array",
                "MaxText.globals.EPS"
            ],
            "parameters": {
                "config.mtp_loss_scaling_factor": "A scalar factor used to scale the final calculated average MTP loss."
            },
            "notes": [
                "This function is designed to work with Flax models that use the 'sow' feature to collect intermediate values during the forward pass, specifically from the `MultiTokenPredictionBlock`.",
                "It gracefully handles cases where MTP heads did not run by checking if the `mtp_losses` list is empty and returning zero.",
                "The `EPS` constant is used to prevent division by zero if the sum of weights is zero."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/multi_token_prediction.py#calculate_mtp_acceptance_rate",
        "file_path": "src/MaxText/layers/multi_token_prediction.py",
        "code_block": "def calculate_mtp_acceptance_rate(intermediate_outputs, config):\n  \"\"\"Calculates the MTP acceptance rate from intermediate outputs.\"\"\"\n\n  sown_data = maxtext_utils.get_nested_value(intermediate_outputs, (\"mtp_acceptance\", \"mtp_block\"), {})\n  mtp_preds = maxtext_utils.get_nested_value(sown_data, (\"mtp_preds\",), [None])[0]\n  valid_mask = maxtext_utils.get_nested_value(sown_data, (\"mtp_mask\",), [None])[0]\n\n  # These values are only \"sown\" (saved) during an evaluation run and only for the specific\n  # MTP layer specified by `config.mtp_eval_target_module`. This check handles cases\n  # where the required data is absent (e.g., during a training step) and prevents errors.\n  if mtp_preds is None or valid_mask is None:\n    return 0.0\n\n  # Get the main model's greedy predictions from the logits.\n  main_model_preds = jnp.argmax(intermediate_outputs[\"logits\"], axis=-1)\n\n  # Roll the main model's predictions to align them in time with the MTP head's target.\n  rolled_main_preds = main_model_preds\n  for _ in range(config.mtp_eval_target_module):\n    rolled_main_preds = roll_and_mask(rolled_main_preds)\n\n  # Compare the aligned predictions. The `valid_mask` ensures that the comparison\n  # only happens on valid tokens, ignoring the placeholder values introduced at the\n  # end of the sequence by the `roll_and_mask` operation.\n  correct_predictions = jnp.sum((mtp_preds == rolled_main_preds) * valid_mask)\n  total_valid_tokens = jnp.sum(valid_mask)\n\n  # Return acceptance rate as a percentage\n  return (correct_predictions / (total_valid_tokens + EPS)) * 100",
        "analysis": {
            "module_type": "metric_calculation_utility",
            "purpose": "Calculates the Multi-Token Prediction (MTP) acceptance rate, which measures the percentage of times a specific MTP head's prediction matches the main model's greedy prediction for the same future token.",
            "input": {
                "shape": "intermediate_outputs: A dictionary containing model outputs, including 'logits' of shape [batch_size, sequence_length, vocab_size] and sown MTP data. config: A configuration object.",
                "dtype": "float32 for logits, int32 for predictions and masks."
            },
            "processing_steps": [
                "Extract MTP predictions (`mtp_preds`) and the corresponding validity mask (`valid_mask`) from the `intermediate_outputs` dictionary using `maxtext_utils.get_nested_value`.",
                "If MTP data is not found (e.g., during a training step), return 0.0.",
                "Calculate the main model's greedy predictions by applying `jnp.argmax` to the final logits.",
                "Time-align the main model's predictions with the MTP head's predictions by repeatedly applying the `roll_and_mask` function, controlled by `config.mtp_eval_target_module`.",
                "Compare the aligned main model predictions with the MTP predictions, using the validity mask to exclude invalid tokens.",
                "Calculate the number of correct predictions and the total number of valid tokens using `jnp.sum`.",
                "Compute the final acceptance rate as (correct_predictions / total_valid_tokens) * 100."
            ],
            "output": {
                "shape": "A scalar float representing the acceptance rate percentage."
            },
            "dependencies": [
                "maxtext_utils.get_nested_value",
                "jnp.argmax",
                "jnp.sum",
                "roll_and_mask"
            ],
            "parameters": {
                "config.mtp_eval_target_module": "Specifies which MTP layer's predictions to use for the acceptance rate calculation and determines the necessary time-shift for alignment."
            },
            "notes": [
                "This function is intended for use during evaluation, as the necessary MTP prediction data is only 'sown' (collected) in that mode for a specific target layer.",
                "The acceptance rate is a measure of consistency between an MTP head and the main model, not a measure of correctness against ground-truth labels.",
                "The `roll_and_mask` operation is crucial for aligning the main model's prediction for token `t+1` with the MTP head's prediction for token `t+k`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma2.py#Gemma2DecoderLayer",
        "file_path": "src/MaxText/layers/gemma2.py",
        "code_block": "class Gemma2DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    if model_mode == MODEL_MODE_PREFILL:\n      activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    inputs = nn.with_logical_constraint(inputs, activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm_local\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    lnx = nn.with_logical_constraint(lnx, activation_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention_local\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        attention_type=AttentionType.LOCAL_SLIDING,\n        sliding_window_size=cfg.sliding_window_size,\n        attn_logits_soft_cap=cfg.attn_logits_soft_cap,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = rms_norm(\n          num_features=attention_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_self_attention_norm_local\",\n          kernel_axes=(\"norm\",),\n      )(attention_lnx)\n    attention_lnx = nn.with_logical_constraint(attention_lnx, activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm_local\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp_local\",\n        model_mode=model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = rms_norm(\n          num_features=mlp_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_ffw_norm_local\",\n          kernel_axes=(\"norm\",),\n      )(mlp_lnx)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)\n\n    ### global part\n    inputs = nn.with_logical_constraint(layer_output, activation_axis_names)\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm_global\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n    lnx = nn.with_logical_constraint(lnx, activation_axis_names)\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention_global\",\n        float32_qk_product=True,\n        float32_logits=True,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        attention_type=AttentionType.GLOBAL,\n        attn_logits_soft_cap=cfg.attn_logits_soft_cap,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = rms_norm(\n          num_features=attention_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_self_attention_norm_global\",\n          kernel_axes=(\"norm\",),\n      )(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, activation_axis_names)\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm_global\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp_global\",\n        model_mode=model_mode,\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = rms_norm(\n          num_features=mlp_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_ffw_norm_global\",\n          kernel_axes=(\"norm\",),\n      )(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma2_decoder_layer",
            "purpose": "Implements a single decoder layer for the Gemma2 transformer model, which sequentially applies a local (sliding window) attention block and a global attention block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes class attributes: config, mesh, model_mode, and quant."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "Configuration object containing model hyperparameters like dtype, dimensions, dropout rates, and attention types.",
                "mesh": "JAX sharding mesh for distributed computation.",
                "model_mode": "Specifies the operational mode, e.g., 'prefill' or 'decode'.",
                "quant": "Optional quantization configuration for the layer."
            },
            "notes": [
                "This layer is unique to the Gemma2 architecture, featuring a two-stage attention process within a single decoder layer: first local, then global.",
                "Each stage (local and global) consists of a self-attention mechanism followed by a feed-forward network (MLP), with residual connections and layer normalization."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through one complete Gemma2 decoder layer, including both local and global attention mechanisms.",
                    "input": {
                        "shape": "[batch, length, emb_dim]",
                        "dtype": "Depends on config.dtype"
                    },
                    "processing_steps": [
                        "Apply a 'local' block: RMS Norm -> Local Sliding Window Attention -> Optional Post-Attn Norm -> Residual Connection.",
                        "Apply a 'local' MLP sub-block: RMS Norm -> MLP -> Optional Post-FFW Norm -> Residual Connection -> Dropout.",
                        "Take the output of the local block as input for the global block.",
                        "Apply a 'global' block: RMS Norm -> Global Attention -> Optional Post-Attn Norm -> Residual Connection.",
                        "Apply a 'global' MLP sub-block: RMS Norm -> MLP -> Optional Post-FFW Norm -> Residual Connection -> Dropout.",
                        "Optionally record intermediate activation metrics if `cfg.record_internal_nn_metrics` is enabled.",
                        "Return the final processed tensor."
                    ],
                    "output": {
                        "shape": "[batch, length, emb_dim]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "nn.Dropout",
                        "jnp"
                    ],
                    "notes": [
                        "The logical axis names for tensor constraints are chosen based on the `model_mode` ('prefill' or other).",
                        "If `cfg.scan_layers` is true, the method returns a tuple `(layer_output, None)` for compatibility with `flax.linen.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/mixtral.py#MixtralDecoderLayer",
        "file_path": "src/MaxText/layers/mixtral.py",
        "code_block": "class MixtralDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: models.Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx, load_balance_loss = moe.get_routed_moe(\n        name=\"MoeBlock_0\",\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "mixtral_decoder_layer",
            "purpose": "Implements a single decoder layer for a Mixtral-style transformer model, including self-attention and a Mixture-of-Experts (MoE) feed-forward block.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "float32 or bfloat16 (determined by config.dtype)"
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input tensor.",
                "Perform self-attention using the `attention_as_linen` module.",
                "Add the attention output to the original input (first residual connection).",
                "Apply post-attention RMS normalization.",
                "Process the normalized tensor through a Mixture-of-Experts (MoE) block (`moe.get_routed_moe`).",
                "Add the MoE output to the output of the first residual connection (second residual connection).",
                "Apply dropout to the final output.",
                "Optionally record intermediate metrics and load balancing loss.",
                "Return the final layer output."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.layers.models.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.moe.get_routed_moe",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "Configuration object (`models.Config`) containing model hyperparameters like dimensions, dropout rates, and number of experts.",
                "mesh": "JAX sharding mesh for distributed computation.",
                "model_mode": "String indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "Optional quantization configuration object."
            },
            "notes": [
                "This layer uses two residual connections: one after the self-attention block and another after the MoE block.",
                "It utilizes a Mixture-of-Experts (MoE) block instead of a standard MLP/FFN layer.",
                "The class can optionally record intermediate values like MoE load balancing loss and activation statistics using `sow`.",
                "The return signature changes based on `cfg.scan_layers`, returning `(layer_output, None)` if True, otherwise just `layer_output`."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies a complete Mixtral decoder layer transformation, including self-attention and a Mixture-of-Experts block, to the input sequence.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "float32 or bfloat16 (determined by config.dtype)"
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization (`rms_norm`) to `inputs`.",
                        "Pass the normalized tensor through a self-attention layer (`attention_as_linen`).",
                        "Add the attention output to the original `inputs` in a residual connection.",
                        "Apply post-attention RMS normalization (`rms_norm`) to the result of the first residual connection.",
                        "Process the result through a Mixture-of-Experts (MoE) block (`moe.get_routed_moe`).",
                        "Add the MoE output to the result of the first residual connection in a second residual connection.",
                        "Apply dropout (`nn.Dropout`) to the final result.",
                        "Optionally log MoE load balancing loss and other metrics via `self.sow`.",
                        "Return the final processed tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "moe.get_routed_moe",
                        "nn.Dropout",
                        "jnp.mean",
                        "jnp.std"
                    ],
                    "notes": [
                        "The method handles different operational modes (`model_mode`) which are passed down to the attention layer.",
                        "It takes `decoder_segment_ids` and `decoder_positions` to correctly mask the self-attention mechanism.",
                        "The `deterministic` flag controls whether dropout is applied.",
                        "The return value depends on the `cfg.scan_layers` configuration flag."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#self_attention_with_norm",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "def self_attention_with_norm(\n    inputs: jnp.ndarray,\n    cfg: Config,\n    mesh: Mesh,\n    quant: None | Quant,\n    decoder_segment_ids: None | jnp.ndarray,\n    decoder_positions: None | jnp.ndarray,\n    deterministic: bool,\n    model_mode: str,\n):\n  \"\"\"A helper function for self-attention block with normalization.\"\"\"\n\n  inputs_checkpoint = checkpoint_name(inputs, \"decoder_layer_input\")\n\n  # Corresponds to Qwen3's `input_layernorm`\n  lnx = rms_norm(\n      num_features=inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"pre_self_attention_layer_norm\",\n      epsilon=cfg.normalization_layer_epsilon,\n      kernel_axes=(\"norm\",),\n  )(inputs_checkpoint)\n  lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_length\", \"activation_embed\"))\n\n  # Self-attention block\n  attention_layer = attentions.attention_as_linen(\n      config=cfg,\n      num_query_heads=cfg.num_query_heads,\n      num_kv_heads=cfg.num_kv_heads,\n      head_dim=cfg.head_dim,\n      max_target_length=cfg.max_target_length,\n      max_prefill_predict_length=cfg.max_prefill_predict_length,\n      attention_kernel=cfg.attention,\n      inputs_q_shape=lnx.shape,\n      inputs_kv_shape=lnx.shape,\n      mesh=mesh,\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      dropout_rate=cfg.dropout_rate,\n      name=\"self_attention\",\n      quant=quant,\n      kv_quant=quantizations.configure_kv_quant(cfg),\n      use_qk_norm=cfg.use_qk_norm,\n      query_pre_attn_scalar=(cfg.head_dim**-0.5),  # Qwen3 specific scaling\n      model_mode=model_mode,\n  )\n\n  attention_output = attention_layer(\n      lnx,  # inputs_q\n      lnx,  # inputs_kv\n      decoder_positions,\n      decoder_segment_ids=decoder_segment_ids,\n      deterministic=deterministic,\n      model_mode=model_mode,\n  )\n  attention_output = nn.with_logical_constraint(\n      attention_output, (\"activation_batch\", \"activation_length\", \"activation_embed\")\n  )\n\n  # Residual connection after attention\n  residual_after_attention = inputs_checkpoint + attention_output\n\n  # Post Attention LayerNorm (corresponds to Qwen3's `post_attention_layernorm`)\n  hidden_states = rms_norm(\n      num_features=residual_after_attention.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"post_self_attention_layer_norm\",\n      epsilon=cfg.normalization_layer_epsilon,\n      kernel_axes=(\"norm\",),\n  )(residual_after_attention)\n  hidden_states = nn.with_logical_constraint(hidden_states, (\"activation_batch\", \"activation_length\", \"activation_embed\"))\n\n  return hidden_states, residual_after_attention",
        "analysis": {
            "functionality": "This function implements a self-attention block with pre and post RMS normalization, a core component of a Transformer decoder layer. It applies RMS normalization to the input, computes self-attention, adds the result back to the original input via a residual connection, and then applies a final RMS normalization.",
            "usage": "This function is a helper used within a larger decoder layer. It takes an input tensor of shape `[batch_size, sequence_length, hidden_dim]`, along with configuration, sharding mesh, and optional quantization/positional information. It returns a tuple containing two tensors: the final normalized hidden states and the intermediate result after the first residual connection, both with the same shape as the input."
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3DecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3DecoderLayer(nn.Module):\n  \"\"\"Qwen3 Transformer decoder layer (dense).\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n\n    hidden_states, residual_after_attention = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n    )\n\n    # Dense MLP block\n    mlp_output = linears.mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n\n    # Final residual connection\n    layer_output = residual_after_attention + mlp_output\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_length\", \"activation_embed\"),\n    )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "qwen3_decoder_layer",
            "purpose": "Implements a single dense decoder layer for a Qwen3-style Transformer, combining a self-attention block and a feed-forward MLP block with specific normalization and residual connection patterns.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "Performs self-attention with pre and post-RMS normalization via the `self_attention_with_norm` helper function, which also computes the first residual connection.",
                "Applies a dense MLP block to the normalized output of the attention sub-layer.",
                "Adds the MLP output to the un-normalized output of the attention block in a final residual connection.",
                "Returns the layer output, with a signature that can be adapted for use with `flax.linen.scan`."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "self_attention_with_norm",
                "MaxText.layers.linears.mlp_block"
            ],
            "parameters": {
                "config": "The model configuration object containing hyperparameters like mlp_dim, dropout_rate, dtype, etc.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "An optional quantization configuration object."
            },
            "notes": [
                "This is the 'dense' version of the decoder layer, as opposed to the Mixture-of-Experts (MoE) version.",
                "The final residual connection adds the MLP output to the result of the *first* residual connection (after attention but before the post-attention norm), which is a specific architectural choice of the Qwen3 model."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the dense Qwen3 decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Calls `self_attention_with_norm` to compute the self-attention output (`hidden_states`) and the first residual connection (`residual_after_attention`).",
                        "Passes the `hidden_states` through a dense `linears.mlp_block` to get `mlp_output`.",
                        "Computes the final output by adding `mlp_output` to `residual_after_attention`.",
                        "Applies a logical constraint for tensor sharding.",
                        "Conditionally returns the output tensor based on `cfg.scan_layers` to support `flax.linen.scan`."
                    ],
                    "output": {
                        "shape": "Returns `[batch_size, sequence_length, hidden_dim]` if `cfg.scan_layers` is False, otherwise returns a tuple `([batch_size, sequence_length, hidden_dim], None)`."
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "linears.mlp_block"
                    ],
                    "notes": [
                        "The arguments `previous_chunk`, `page_state`, and `slot` are passed down to the attention layer for inference but are not directly used in this method's logic."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/qwen3.py#Qwen3MoeDecoderLayer",
        "file_path": "src/MaxText/layers/qwen3.py",
        "code_block": "class Qwen3MoeDecoderLayer(nn.Module):\n  \"\"\"Qwen3 Transformer decoder layer (MoE).\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      decoder_segment_ids: None | jnp.ndarray,\n      decoder_positions: None | jnp.ndarray,\n      deterministic: bool,\n      model_mode: str,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n\n    hidden_states, residual_after_attention = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n    )\n\n    # Mixture of Experts block\n    mlp_output, load_balance_loss = moe.get_routed_moe(\n        config=cfg,\n        num_experts=cfg.num_experts,\n        num_experts_per_tok=cfg.num_experts_per_tok,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        intermediate_dim=cfg.moe_mlp_dim,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"moe_block\",\n        quant=self.quant,\n    )(hidden_states)\n\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    mlp_output = nn.with_logical_constraint(mlp_output, (\"activation_batch\", \"activation_length\", \"activation_embed\"))\n\n    # Final residual connection\n    layer_output = residual_after_attention + mlp_output\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_length\", \"activation_embed\"),\n    )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "The `Qwen3MoeDecoderLayer` class implements a single transformer decoder layer for a Qwen3 model, which uses a Mixture of Experts (MoE) block for its feed-forward network instead of a standard dense MLP.",
            "usage": "This class is used as a building block within a larger transformer model. To use it, instantiate the class with a configuration object, a JAX mesh, and a model mode. Then, call the instance with an input tensor of shape `[batch_size, sequence_length, hidden_dim]`, along with other optional arguments for attention masking and operational mode (e.g., training vs. inference). The layer processes the input through a self-attention block and an MoE block, returning an output tensor of the same shape. It also computes and records a `load_balance_loss` specific to the MoE architecture."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Quantization:\n  \"\"\"Base class for quantization configurations\"\"\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Placeholder for dot_general implementation in subclasses.\"\"\"\n\n  def einsum(self, dtype: DType = jnp.float32):\n    \"\"\"Placeholder for einsum implementation in subclasses.\"\"\"",
        "analysis": {
            "functionality": "The `Quantization` class serves as a base class for defining quantization configurations. It establishes a common interface with placeholder methods (`dot_general_cls` and `einsum`) that are intended to be implemented by its subclasses to provide specific quantization logic for tensor operations.",
            "usage": "This class is not used directly. Instead, it is subclassed to create specific quantization schemes (e.g., `AqtQuantization`, `Fp8Quantization`). Subclasses must override the `dot_general_cls` and `einsum` methods to return the appropriate quantized operation handlers."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_tiling_fn",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _tiling_fn(lhs, rhs, dimension_numbers, tile_size):\n  \"\"\"apply tiling function\"\"\"\n  del lhs, rhs\n\n  (lhs_ca, rhs_ca), _ = dimension_numbers\n  ret = tiled_dot_general.Cfg(\n      lhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n      rhs=tiled_dot_general.TensorTiling(contraction_axes=[], remaining_axes=[]),\n  )\n\n  for lhs_idx, rhs_idx in zip(lhs_ca, rhs_ca):\n    ret.lhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=lhs_idx, tile_size=tile_size, tile_count=None))\n    ret.rhs.contraction_axes.append(tiled_dot_general.AxisTiling(axis=rhs_idx, tile_size=tile_size, tile_count=None))\n\n  return ret",
        "analysis": {
            "functionality": "This function creates a configuration object for tiled dot-general operations. It specifies that the contraction axes of the input tensors (`lhs`, `rhs`) should be tiled with a given `tile_size`.",
            "usage": "This function is designed to be used as a callback (`tiling_fn`) for AQT's dot-general operations. It takes `lhs` and `rhs` tensors (which are ignored), `dimension_numbers` (which specify the contraction axes), and a `tile_size`. It returns a `tiled_dot_general.Cfg` object that defines the tiling strategy for the contraction axes."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_rhs_axis_metadata_wrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _rhs_axis_metadata_wrapper(\n    x: jnp.ndarray,\n    tile_map,\n    no_sharding_axis: Sequence[int],\n    mesh_axes: Tuple[str, ...],\n    is_tiled: bool,\n    replicate_scale: bool = False,\n):\n  \"\"\"right-hand-side axis metadata wrapper\"\"\"\n  if replicate_scale:\n    # Temporarily using the shape to identify the scale.\n    # TODO: remove the replication once the 2d sharding quantization\n    # works as expected.\n    if len(x.shape) == 1:\n      return nn.with_logical_partitioning((lambda: x), tuple(None for _ in mesh_axes))()\n\n  mesh_axes = list(mesh_axes)\n  if is_tiled:\n    # tile_map is a mapping between original rank and a list of new, tiled rank.\n    if len(mesh_axes) < len(tile_map):\n      mesh_axes = [None] * (len(tile_map) - len(mesh_axes)) + mesh_axes\n    new_mesh_axes = [None] * len(x.shape)\n    for orig_rank, new_rank in tile_map.items():\n      assert new_rank\n      assert len(new_rank) <= 2\n      new_mesh_axes[new_rank[-1]] = mesh_axes[orig_rank]\n    mesh_axes = new_mesh_axes\n\n  if mesh_axes is not None and len(mesh_axes) > 0:\n    for no_shard_idx in no_sharding_axis:\n      if no_shard_idx < len(mesh_axes):\n        mesh_axes[no_shard_idx] = None\n\n  return nn.with_logical_partitioning((lambda: x), mesh_axes)()",
        "analysis": {
            "functionality": "This function is a wrapper that applies JAX sharding annotations (logical partitioning) to a right-hand-side (RHS) tensor of a matrix multiplication. It adjusts the sharding based on whether the tensor is tiled, which axes should not be sharded, and a special case for replicating 1D scale tensors.",
            "usage": "Call this function with a JAX tensor `x` and its sharding configuration. It is used internally by the AQT quantization library to correctly annotate tensors for distributed computation. \n\nInputs:\n- `x`: The `jnp.ndarray` to be annotated.\n- `tile_map`: A dictionary mapping original tensor axis indices to new tiled axis indices.\n- `no_sharding_axis`: A sequence of integer axis indices that should not be sharded.\n- `mesh_axes`: A tuple of strings representing the initial sharding annotations for the tensor's axes.\n- `is_tiled`: A boolean indicating if the tensor is tiled.\n- `replicate_scale`: An optional boolean to fully replicate 1D tensors (scales).\n\nOutput:\nThe input tensor `x` with the appropriate logical partitioning annotations applied."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#AqtQuantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class AqtQuantization:\n  \"\"\"Configures AQT quantization github.com/google/aqt.\"\"\"\n\n  quant_dg: aqt_config.DotGeneral\n  quant_mode: aqt_flax.QuantMode = aqt_flax.QuantMode.TRAIN\n  replicate_scale: bool = False\n\n  def _get_mixed_precision_cfg(self):\n    \"\"\"get configuration for mixed precision\"\"\"\n    quant_dg = None\n    is_tiled = False\n    tiling_fn = None\n    # pylint: disable=protected-access\n    module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    tile_size = -1\n    for layer_name_re, layer_quant_dg in self.quant_dg.items():\n      if re.fullmatch(layer_name_re, module_path):\n        quant_dg, tile_size = layer_quant_dg\n    if quant_dg is None:\n      quant_dg, tile_size = self.quant_dg[DEFAULT]\n    if tile_size != -1:\n      is_tiled = True\n      tiling_fn = functools.partial(_tiling_fn, tile_size=tile_size)\n    return quant_dg, is_tiled, tiling_fn\n\n  def _get_rhs_axis_metadata_wrapper(\n      self, mesh_axes: Tuple[str, ...] = (), is_tiled: bool = False, replicate_scale: bool = False\n  ):\n    if self.quant_mode == aqt_flax.QuantMode.CONVERT:\n      return None\n    return functools.partial(\n        _rhs_axis_metadata_wrapper, mesh_axes=mesh_axes, is_tiled=is_tiled, replicate_scale=replicate_scale\n    )\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    # module_path = \"/\".join(nn.module._context.module_stack[-1].path)\n    # print(f\"quant_dg: {quant_dg}, is_tiled: {is_tiled}, module_path: {module_path}\")\n    aqt_dg_cls = functools.partial(\n        aqt_flax.AqtDotGeneral,\n        quant_dg,\n        rhs_quant_mode=self.quant_mode,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n        rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n        use_legacy_freezer=False,\n        tiling_fn=tiling_fn,\n    )\n    return aqt_dg_cls\n\n  def einsum(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns einsum configured with aqt params.\"\"\"\n    if isinstance(self.quant_dg, dict):\n      quant_dg, is_tiled, tiling_fn = self._get_mixed_precision_cfg()\n    else:\n      quant_dg, is_tiled, tiling_fn = self.quant_dg, False, None\n\n    rhs_axis_metadata_wrapper = self._get_rhs_axis_metadata_wrapper(\n        mesh_axes, is_tiled, replicate_scale=self.replicate_scale\n    )\n    aqt_einsum = functools.partial(\n        aqt_flax.AqtEinsum(\n            cfg=quant_dg,\n            rhs_quant_mode=self.quant_mode,\n            lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n            rhs_freeze_mode=aqt_flax.FreezerMode.CALIBRATION_AND_VALUE,\n            rhs_axis_metadata_wrapper=rhs_axis_metadata_wrapper,\n            use_legacy_freezer=False,\n            tiling_fn=tiling_fn,\n        )\n    )\n    return aqt_einsum",
        "analysis": {
            "module_type": "aqt_quantization_config",
            "purpose": "Configures and provides AQT (Algorithm Quantization Toolkit) implementations for dot_general and einsum operations.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "aqt.jax.v2.flax.aqt_flax",
                "functools",
                "re",
                "flax.linen.module"
            ],
            "parameters": {
                "quant_dg": "An `aqt_config.DotGeneral` object or a dictionary mapping layer name regex to `(aqt_config.DotGeneral, tile_size)` for mixed-precision quantization.",
                "quant_mode": "The quantization mode (TRAIN, SERVE, or CONVERT) from `aqt_flax.QuantMode`.",
                "replicate_scale": "A boolean indicating whether to replicate the quantization scale across devices."
            },
            "notes": [
                "This class acts as a factory for creating quantized `dot_general` and `einsum` functions based on its configuration.",
                "It supports mixed-precision quantization by matching the current module's path against regular expressions defined in the `quant_dg` dictionary."
            ],
            "methods": {
                "_get_mixed_precision_cfg": {
                    "purpose": "Determines the appropriate AQT configuration (`quant_dg`), tiling status, and tiling function for the current module based on its path.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the current module's path from the Flax module context.",
                        "Iterate through the `self.quant_dg` dictionary to find a configuration matching the module path using `re.fullmatch`.",
                        "If no match is found, use the default configuration.",
                        "If a `tile_size` is specified, set `is_tiled` to True and create a partial `_tiling_fn`.",
                        "Return the selected `quant_dg`, `is_tiled` flag, and `tiling_fn`."
                    ],
                    "output": {
                        "shape": "Returns a tuple: (aqt_config.DotGeneral, bool, callable or None)."
                    },
                    "dependencies": [
                        "flax.linen.module",
                        "re",
                        "functools.partial",
                        "_tiling_fn"
                    ],
                    "notes": [
                        "This is a private helper method used by `dot_general_cls` and `einsum` to handle mixed-precision setups."
                    ]
                },
                "_get_rhs_axis_metadata_wrapper": {
                    "purpose": "Creates a wrapper function for handling right-hand-side tensor axis metadata, which is used for sharding in AQT.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `quant_mode` is CONVERT. If so, return None.",
                        "Return a `functools.partial` of the `_rhs_axis_metadata_wrapper` function, pre-filled with `mesh_axes`, `is_tiled`, and `replicate_scale`."
                    ],
                    "output": {
                        "shape": "Returns a callable (the partial function) or None."
                    },
                    "dependencies": [
                        "functools.partial",
                        "_rhs_axis_metadata_wrapper",
                        "aqt_flax.QuantMode"
                    ],
                    "notes": [
                        "The returned wrapper is passed to AQT's `dot_general` or `einsum` to manage sharding annotations for quantization scales and other metadata."
                    ]
                },
                "dot_general_cls": {
                    "purpose": "Returns a partially configured `aqt_flax.AqtDotGeneral` class ready to be used as a replacement for `jax.lax.dot_general`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the quantization config (`quant_dg`), tiling status, and tiling function by calling `_get_mixed_precision_cfg` if `self.quant_dg` is a dict, otherwise use `self.quant_dg` directly.",
                        "Get the RHS axis metadata wrapper by calling `_get_rhs_axis_metadata_wrapper`.",
                        "Create a `functools.partial` of `aqt_flax.AqtDotGeneral` with the determined configuration.",
                        "Return the partially configured class."
                    ],
                    "output": {
                        "shape": "Returns a callable (the partially configured `AqtDotGeneral` class)."
                    },
                    "dependencies": [
                        "_get_mixed_precision_cfg",
                        "_get_rhs_axis_metadata_wrapper",
                        "functools.partial",
                        "aqt_flax.AqtDotGeneral"
                    ],
                    "notes": [
                        "The returned class can be passed to Flax layers (e.g., `Dense`) via the `dot_general_cls` argument to apply AQT quantization."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a partially configured `aqt_flax.AqtEinsum` instance ready to be used as a replacement for `jnp.einsum`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the quantization config (`quant_dg`), tiling status, and tiling function by calling `_get_mixed_precision_cfg` if `self.quant_dg` is a dict, otherwise use `self.quant_dg` directly.",
                        "Get the RHS axis metadata wrapper by calling `_get_rhs_axis_metadata_wrapper`.",
                        "Create a `functools.partial` of an `aqt_flax.AqtEinsum` instance initialized with the determined configuration.",
                        "Return the partially configured instance."
                    ],
                    "output": {
                        "shape": "Returns a callable (the partially configured `AqtEinsum` instance)."
                    },
                    "dependencies": [
                        "_get_mixed_precision_cfg",
                        "_get_rhs_axis_metadata_wrapper",
                        "functools.partial",
                        "aqt_flax.AqtEinsum"
                    ],
                    "notes": [
                        "The returned callable can be used in place of `jnp.einsum` to apply AQT quantization."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Quantization(Quantization):\n  \"\"\"Configures Fp8 quantization for NVIDIA GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.Fp8DirectDotGeneralOp\n\n  def einsum(self, dtype: DType = jnp.float32):\n    return _Fp8EinsumWrapper(dtype=dtype)",
        "analysis": {
            "module_type": "fp8_quantization_config",
            "purpose": "A configuration class that provides Fp8 quantization operations (dot_general and einsum) tailored for NVIDIA GPUs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "This class acts as a factory or provider for specific Fp8 operation modules and does not perform tensor processing itself."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization",
                "flax.linen.Fp8DirectDotGeneralOp",
                "_Fp8EinsumWrapper"
            ],
            "parameters": {
                "quant_mode": "The quantization mode, hardcoded to 'train'."
            },
            "notes": [
                "This class inherits from the base `Quantization` class and provides a concrete implementation for NVIDIA's Fp8 support."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the Flax Fp8 dot_general operation class for NVIDIA GPUs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the `nn.Fp8DirectDotGeneralOp` class."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.Fp8DirectDotGeneralOp"
                    ],
                    "notes": [
                        "The method signature includes `mesh_axes` for compatibility with the base class, but it is not used in this implementation."
                    ]
                },
                "einsum": {
                    "purpose": "Returns a wrapper module that performs Fp8 einsum operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiates and returns `_Fp8EinsumWrapper` with the specified computation `dtype`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_Fp8EinsumWrapper"
                    ],
                    "notes": [
                        "The returned wrapper handles casting the right-hand-side tensor to the desired computation dtype before the Fp8 einsum operation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_Fp8EinsumWrapper",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class _Fp8EinsumWrapper(nn.Module):\n  \"\"\"Wrapper for nn.Fp8Einsum to handle computation dtype.\"\"\"\n\n  dtype: DType\n\n  @nn.compact\n  def __call__(self, eqn, lhs, rhs, **kwargs):\n    # nn.Fp8Einsum determines compute dtype from rhs.\n    # We cast rhs to the desired computation dtype.\n    # nn.Fp8Einsum will then cast lhs to the same dtype.\n    rhs = rhs.astype(self.dtype)\n    return nn.Fp8Einsum(name=\"fp8_einsum\")(eqn, lhs, rhs, **kwargs)",
        "analysis": {
            "module_type": "fp8_einsum_wrapper",
            "purpose": "A Flax module that wraps `nn.Fp8Einsum` to explicitly control the computation data type by casting the right-hand side operand.",
            "input": {
                "shape": "N/A (Handled by the __call__ method)",
                "dtype": "N/A (Handled by the __call__ method)"
            },
            "processing_steps": [
                "The module's logic is entirely contained within its `__call__` method."
            ],
            "output": {
                "shape": "N/A (Handled by the __call__ method)"
            },
            "dependencies": [
                "flax.linen.Module",
                "flax.linen.Fp8Einsum"
            ],
            "parameters": {
                "dtype": "The desired computation data type for the einsum operation."
            },
            "notes": [
                "This wrapper is necessary because `nn.Fp8Einsum` infers the computation dtype from the right-hand side (rhs) tensor. By casting `rhs` first, this wrapper forces a specific computation dtype."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs an FP8 einsum operation with a specified computation data type.",
                    "input": {
                        "shape": "eqn: string, lhs: tensor, rhs: tensor. Tensor shapes must be compatible according to the einsum equation `eqn`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Cast the `rhs` tensor to the `dtype` specified during the wrapper's initialization.",
                        "Call `nn.Fp8Einsum` with the equation string, `lhs`, the casted `rhs`, and any additional keyword arguments."
                    ],
                    "output": {
                        "shape": "The output shape is determined by the einsum equation string `eqn`."
                    },
                    "dependencies": [
                        "flax.linen.Fp8Einsum"
                    ],
                    "notes": [
                        "The underlying `nn.Fp8Einsum` module will automatically cast the `lhs` tensor to match the dtype of the (now casted) `rhs` tensor."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#Fp8Einsum",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class Fp8Einsum(nn.Module):\n  \"\"\"An fp8 einsum op.\n\n  Attributes:\n    amax_history_length: size of the amax history.\n    e4m3_dtype: e4m3 variants, e.g., e4m3fn, e4m3fnuz.\n    e5m2_dtype: e5m2 variants, e.g., e5m2, e5m2fnuz.\n    dtype: computation dtype.\n  \"\"\"\n\n  amax_history_length: int = 1024\n  e4m3_dtype: DType = jnp.float8_e4m3fn\n  e5m2_dtype: DType = jnp.float8_e5m2\n  dtype: DType = jnp.float32\n\n  def setup(self) -> None:\n    \"\"\"init with input_amax_history, kernel_amax_history, output_grad_amax_history,\n    input_scale, kernel_scale, output_grad_scale\"\"\"\n    scale_args = (\n        flax_initializers.ones_init(),\n        jax.random.PRNGKey(0),\n        (1,),\n        jnp.float32,\n    )\n    amax_history_args = (\n        flax_initializers.zeros_init(),\n        jax.random.PRNGKey(0),\n        (self.amax_history_length,),\n        jnp.float32,\n    )\n\n    OVERWRITE_WITH_GRADIENT = \"_overwrite_with_gradient\"\n    self.input_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"input_amax_history\", *amax_history_args)\n    self.kernel_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_amax_history\", *amax_history_args)\n    self.output_grad_amax_history = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_amax_history\", *amax_history_args)\n\n    self.input_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"input_scale\", *scale_args)\n    self.kernel_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"kernel_scale\", *scale_args)\n    self.output_grad_scale = self.variable(OVERWRITE_WITH_GRADIENT, \"output_grad_scale\", *scale_args)\n\n  def __call__(self, eqn, *args, **kwargs):\n    assert len(args) == 2\n    x = args[0]\n    k = args[1]\n\n    comp_dtype = self.dtype\n    k = jnp.asarray(k, comp_dtype)\n    x = jnp.asarray(x, comp_dtype)\n\n    x_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, x, self.input_scale.value, self.input_amax_history.value)\n    k_qdq = fp8_ops.in_qdq(comp_dtype, self.e4m3_dtype, k, self.kernel_scale.value, self.kernel_amax_history.value)\n\n    y_qdq = jnp.einsum(eqn, x_qdq, k_qdq, _dot_general=fp8_ops.dot_general_with_precision)\n\n    y = fp8_ops.out_qdq(\n        comp_dtype,\n        self.e5m2_dtype,\n        y_qdq,\n        self.output_grad_scale.value,\n        self.output_grad_amax_history.value,\n    )\n    return y",
        "analysis": {
            "module_type": "fp8_einsum",
            "purpose": "Performs an Einstein summation (einsum) operation using 8-bit floating-point (FP8) precision, managing the necessary scaling factors and amax history for quantization.",
            "input": {
                "shape": "The `__call__` method takes an einsum equation string and two tensors with shapes compatible with that equation.",
                "dtype": "Tensors are cast to the computation `dtype` (e.g., float32) internally."
            },
            "processing_steps": [
                "Initializes scaling factors and amax history buffers for inputs, kernel, and output gradients in the `setup` method.",
                "In the `__call__` method, casts input tensors to the computation data type.",
                "Quantizes and de-quantizes (QDQ) the two input tensors to an FP8 format (e4m3) using `fp8_ops.in_qdq`.",
                "Performs the einsum operation on the QDQ inputs using `jnp.einsum` with a specialized `dot_general` for FP8.",
                "Performs an output QDQ step on the result, preparing it for the backward pass, using `fp8_ops.out_qdq`.",
                "Returns the final computed tensor."
            ],
            "output": {
                "shape": "The output shape is determined by the provided einsum equation string and the input tensor shapes."
            },
            "dependencies": [
                "flax.linen.nn.Module",
                "jax.numpy",
                "flax.linen.fp8_ops",
                "flax.linen.initializers"
            ],
            "parameters": {
                "amax_history_length": "The size of the buffer to store the history of absolute maximum values for dynamic scaling.",
                "e4m3_dtype": "The FP8 data type (E4M3 variant) used for quantizing the input tensors.",
                "e5m2_dtype": "The FP8 data type (E5M2 variant) used for the output dequantization step, typically for gradients.",
                "dtype": "The data type used for the intermediate computation (e.g., float32 or bfloat16)."
            },
            "notes": [
                "This module is stateful, maintaining scaling factors and amax histories as Flax variables.",
                "The variables are stored in the `_overwrite_with_gradient` collection, indicating they are updated during training."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes and registers Flax variables for amax histories and scaling factors required for FP8 quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define arguments for initializing scale variables to ones.",
                        "Define arguments for initializing amax history variables to zeros.",
                        "Create and register `input_amax_history`, `kernel_amax_history`, and `output_grad_amax_history` variables.",
                        "Create and register `input_scale`, `kernel_scale`, and `output_grad_scale` variables."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.initializers.ones_init",
                        "flax.linen.initializers.zeros_init",
                        "jax.random.PRNGKey"
                    ],
                    "notes": [
                        "This method is called automatically by Flax during module initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the FP8 einsum operation by quantizing inputs, performing the computation, and dequantizing the output.",
                    "input": {
                        "shape": "Takes an equation string `eqn` and two tensors `x` and `k` as positional arguments, with shapes appropriate for the equation.",
                        "dtype": "Any JAX-compatible numeric type."
                    },
                    "processing_steps": [
                        "Assert that exactly two tensor arguments are provided.",
                        "Cast input tensors `x` and `k` to the computation `dtype`.",
                        "Apply input quantization-dequantization (QDQ) to `x` using `fp8_ops.in_qdq`.",
                        "Apply input QDQ to `k` using `fp8_ops.in_qdq`.",
                        "Compute `jnp.einsum` on the QDQ results, using `fp8_ops.dot_general_with_precision` as the backend.",
                        "Apply output QDQ to the result using `fp8_ops.out_qdq`.",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "Determined by the einsum equation `eqn` and input shapes."
                    },
                    "dependencies": [
                        "jax.numpy.einsum",
                        "flax.linen.fp8_ops.in_qdq",
                        "flax.linen.fp8_ops.out_qdq",
                        "flax.linen.fp8_ops.dot_general_with_precision"
                    ],
                    "notes": [
                        "The input tensors are quantized to `e4m3_dtype`, while the output dequantization uses `e5m2_dtype`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Quantization(Quantization):\n  \"\"\"Configures NANOO Fp8 quantization for AMD MI300/MI325 GPUs\"\"\"\n\n  quant_mode = \"train\"\n\n  def dot_general_cls(self, mesh_axes: Tuple[str, ...] = ()):\n    \"\"\"Returns dot_general configured with aqt params.\"\"\"\n    return nn.NANOOFp8DotGeneralOp",
        "analysis": {
            "module_type": "nanoo_fp8_quantization_config",
            "purpose": "Provides a configuration for NANOO Fp8 quantization, specifically designed for AMD MI300/MI325 GPUs.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "This class is instantiated to provide quantization configuration.",
                "The 'dot_general_cls' method is called to retrieve the appropriate dot-general operation module."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Quantization",
                "flax.linen.NANOOFp8DotGeneralOp"
            ],
            "parameters": {
                "quant_mode": "A class attribute hardcoded to 'train', indicating the quantization mode."
            },
            "notes": [
                "This class inherits from the base 'Quantization' class.",
                "It is part of a factory pattern, selected by the 'configure_quantization' function when config.quantization is 'nanoo_fp8'."
            ],
            "methods": {
                "dot_general_cls": {
                    "purpose": "Returns the Flax Linen module class to be used for NANOO Fp8 quantized dot-general operations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Returns the nn.NANOOFp8DotGeneralOp class."
                    ],
                    "output": {
                        "shape": "The method returns a class type (nn.Module), not a tensor. The shape is not applicable."
                    },
                    "dependencies": [
                        "flax.linen.NANOOFp8DotGeneralOp"
                    ],
                    "notes": [
                        "The 'mesh_axes' argument is accepted to maintain a consistent API with other Quantization subclasses but is not used in this implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_int8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_int8_quant_config(config):\n  drhs_bits = None\n  drhs_accumulator_dtype = None\n  drhs_local_aqt = None\n  if config.quantization_local_shard_count != 0:\n    drhs_bits = 8\n    drhs_accumulator_dtype = jnp.int32\n    drhs_local_aqt = aqt_config.LocalAqt(contraction_axis_shard_count=config.quantization_local_shard_count)\n  return aqt_config.config_v3(\n      fwd_bits=8,\n      dlhs_bits=8,\n      drhs_bits=drhs_bits,\n      rng_type=\"jax.uniform\",\n      dlhs_local_aqt=None,\n      drhs_local_aqt=drhs_local_aqt,\n      fwd_accumulator_dtype=jnp.int32,\n      dlhs_accumulator_dtype=jnp.int32,\n      drhs_accumulator_dtype=drhs_accumulator_dtype,\n  )",
        "analysis": {
            "functionality": "This function creates and returns an AQT (Algorithm Quantization Toolkit) version 3 configuration for 8-bit integer quantization. It conditionally configures quantization for the right-hand side of the backward pass (drhs) based on the `quantization_local_shard_count` setting.",
            "usage": "Call this function with a configuration object (`config`) that has a `quantization_local_shard_count` attribute. It returns an `aqt_config.DotGeneral` object suitable for configuring 8-bit integer quantization in AQT-enabled layers."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#ConstantBoundConfig",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class ConstantBoundConfig:\n  fwd_lhs_bound: float | None = None\n  fwd_rhs_bound: float | None = None\n  dlhs_lhs_bound: float | None = None\n  dlhs_rhs_bound: float | None = None\n  drhs_lhs_bound: float | None = None\n  drhs_rhs_bound: float | None = None",
        "analysis": {
            "module_type": "quantization_configuration_dataclass",
            "purpose": "A frozen dataclass that holds optional constant calibration bounds for different stages of an AQT (Accurate Quantized Training) dot-product operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes an immutable object to store constant bound values."
            ],
            "output": {
                "shape": "An instance of the ConstantBoundConfig class."
            },
            "dependencies": [
                "dataclasses.dataclass",
                "_build_const_scale_config"
            ],
            "parameters": {
                "fwd_lhs_bound": "The constant calibration bound for the left-hand side tensor in the forward pass.",
                "fwd_rhs_bound": "The constant calibration bound for the right-hand side tensor in the forward pass.",
                "dlhs_lhs_bound": "The constant calibration bound for the left-hand side tensor in the backward pass gradient calculation for the left-hand side input (dlhs).",
                "dlhs_rhs_bound": "The constant calibration bound for the right-hand side tensor in the backward pass gradient calculation for the left-hand side input (dlhs).",
                "drhs_lhs_bound": "The constant calibration bound for the left-hand side tensor in the backward pass gradient calculation for the right-hand side input (drhs).",
                "drhs_rhs_bound": "The constant calibration bound for the right-hand side tensor in the backward pass gradient calculation for the right-hand side input (drhs)."
            },
            "notes": [
                "The `@dataclass(frozen=True)` decorator makes instances of this class immutable.",
                "This configuration is used to enable static quantization by providing fixed scaling factors, as seen in its usage within the `_build_const_scale_config` function.",
                "All parameters are optional (float | None) and default to None, allowing for partial or no constant bound specification."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_const_scale_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_const_scale_config(\n    aqt_dg: aqt_config.DotGeneral,\n    cst_bound_config: ConstantBoundConfig,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a constant scale config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    cst_bound_config: The constant bound config.\n\n  Returns:\n    The AQT dot general config with constant scale config.\n  \"\"\"\n  if cst_bound_config.fwd_lhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_lhs_bound\n    )\n  if cst_bound_config.fwd_rhs_bound is not None:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.fwd_rhs_bound\n    )\n  if cst_bound_config.dlhs_lhs_bound:\n    aqt_dg.dlhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_lhs_bound\n    )\n\n  if cst_bound_config.dlhs_rhs_bound is not None:\n    aqt_dg.dlhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.dlhs_rhs_bound\n    )\n\n  if cst_bound_config.drhs_lhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.lhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_lhs_bound\n    )\n\n  if cst_bound_config.drhs_rhs_bound is not None:\n    aqt_dg.drhs.dg_quantizer.rhs.calibration = functools.partial(\n        calibration.ConstantCalibration, bound=cst_bound_config.drhs_rhs_bound\n    )\n\n  return aqt_dg",
        "analysis": {
            "functionality": "This function updates an AQT (Automatic Quantization Toolkit) DotGeneral configuration object with constant calibration bounds. It iterates through a `ConstantBoundConfig` object and, for each non-None bound, it sets the corresponding calibration method in the `aqt_dg` configuration to `calibration.ConstantCalibration` with the specified bound.",
            "usage": "Call this function to apply static quantization scales to an existing AQT configuration. Provide an `aqt_config.DotGeneral` object and a `ConstantBoundConfig` object. The function will modify the `aqt_dg` object in place and return it."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#PerTensorScales",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class PerTensorScales:\n  fwd_lhs: bool = False\n  fwd_rhs: bool = False\n  dlhs_lhs: bool = False\n  dlhs_rhs: bool = False\n  drhs_lhs: bool = False\n  drhs_rhs: bool = False",
        "analysis": {
            "module_type": "quantization_configuration_dataclass",
            "purpose": "A dataclass that holds boolean flags to specify whether per-tensor scaling should be applied to the left-hand side (lhs) and right-hand side (rhs) operands during the forward (fwd), gradient w.r.t. lhs (dlhs), and gradient w.r.t. rhs (drhs) passes of a quantized dot-general operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes boolean attributes to False by default upon instantiation."
            ],
            "output": {
                "shape": "An instance of the PerTensorScales class."
            },
            "dependencies": [
                "dataclasses.dataclass",
                "_build_per_tensor_config"
            ],
            "parameters": {
                "fwd_lhs": "If True, use per-tensor scaling for the left-hand side operand in the forward pass.",
                "fwd_rhs": "If True, use per-tensor scaling for the right-hand side operand in the forward pass.",
                "dlhs_lhs": "If True, use per-tensor scaling for the left-hand side operand in the dLHS backward pass.",
                "dlhs_rhs": "If True, use per-tensor scaling for the right-hand side operand in the dLHS backward pass.",
                "drhs_lhs": "If True, use per-tensor scaling for the left-hand side operand in the dRHS backward pass.",
                "drhs_rhs": "If True, use per-tensor scaling for the right-hand side operand in the dRHS backward pass."
            },
            "notes": [
                "This class is used by the `_build_per_tensor_config` function to modify an `aqt_config.DotGeneral` object.",
                "Setting a flag to True results in setting the corresponding 'calib_shared_axes' attribute to 'per_tensor' in the AQT configuration."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_build_per_tensor_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _build_per_tensor_config(\n    aqt_dg: aqt_config.DotGeneral,\n    per_tensor_scales: PerTensorScales,\n) -> aqt_config.DotGeneral:\n  \"\"\"Build a per tensor config for AQT dot general.\n\n  Args:\n    aqt_dg: The AQT dot general config.\n    per_tensor_scales: The per tensor scales config.\n\n  Returns:\n    The AQT dot general config with per tensor config.\n  \"\"\"\n  if per_tensor_scales.fwd_lhs:\n    aqt_dg.fwd.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.fwd_rhs:\n    aqt_dg.fwd.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_lhs:\n    aqt_dg.dlhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.dlhs_rhs:\n    aqt_dg.dlhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_lhs:\n    aqt_dg.drhs.dg_quantizer.lhs.calib_shared_axes = \"per_tensor\"\n  if per_tensor_scales.drhs_rhs:\n    aqt_dg.drhs.dg_quantizer.rhs.calib_shared_axes = \"per_tensor\"\n  return aqt_dg",
        "analysis": {
            "module_type": "aqt_config_modifier",
            "purpose": "Modifies an AQT DotGeneral configuration object to enable per-tensor quantization scaling for specified forward and backward passes.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the forward pass left-hand side (fwd_lhs) based on the `per_tensor_scales` config.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the forward pass right-hand side (fwd_rhs) based on the `per_tensor_scales` config.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass dlhs left-hand side (dlhs_lhs) based on the `per_tensor_scales` config.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass dlhs right-hand side (dlhs_rhs) based on the `per_tensor_scales` config.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass drhs left-hand side (drhs_lhs) based on the `per_tensor_scales` config.",
                "Conditionally set `calib_shared_axes` to 'per_tensor' for the backward pass drhs right-hand side (drhs_rhs) based on the `per_tensor_scales` config.",
                "Return the modified AQT DotGeneral configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.DotGeneral",
                "PerTensorScales"
            ],
            "parameters": {
                "aqt_dg": "The AQT dot general configuration object to be modified.",
                "per_tensor_scales": "A dataclass object specifying which dot product inputs (lhs/rhs) in which passes (fwd/dlhs/drhs) should use per-tensor scaling."
            },
            "notes": [
                "This is a helper function used to configure AQT quantization.",
                "Setting `calib_shared_axes` to 'per_tensor' instructs AQT to compute a single scaling factor for the entire tensor, as opposed to per-channel or other per-axis scaling strategies.",
                "The function modifies the input `aqt_dg` object in place and also returns it."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_default_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_default_config(config):\n  \"\"\"Get aqt for 8-bit floating point quantization configuration.\"\"\"\n  aqt_dg = aqt_config.config_v4(\n      fwd_bits=\"e4m3\",\n      dlhs_bits=\"e5m2\",\n      drhs_bits=\"e5m2\",\n      use_dummy_static_bound=False,\n      fwd_accumulator_dtype=jnp.bfloat16,\n      dlhs_accumulator_dtype=jnp.bfloat16,\n      drhs_accumulator_dtype=jnp.bfloat16,\n      dlhs_use_fwd_quant=False,\n      drhs_use_fwd_quant=False,\n  )\n  constant_bound_config = None\n\n  if len(config.constant_bound_config) == 6:\n    fwd_lhs_bound, fwd_rhs_bound, dlhs_lhs_bound, dlhs_rhs_bound, drhs_lhs_bound, drhs_rhs_bound = (\n        config.constant_bound_config\n    )\n    constant_bound_config = ConstantBoundConfig(\n        fwd_lhs_bound=fwd_lhs_bound,\n        fwd_rhs_bound=fwd_rhs_bound,\n        dlhs_lhs_bound=dlhs_lhs_bound,\n        dlhs_rhs_bound=dlhs_rhs_bound,\n        drhs_lhs_bound=drhs_lhs_bound,\n        drhs_rhs_bound=drhs_rhs_bound,\n    )\n    aqt_dg = _build_const_scale_config(aqt_dg, constant_bound_config)\n\n  aqt_config.set_stochastic_rounding(\n      aqt_dg,\n      vjp_lhs_stochastic_rounding=False,\n      vjp_rhs_stochastic_rounding=False,\n      implementation=\"jax.uniform\",\n  )\n\n  per_tensor_scales = PerTensorScales(\n      fwd_lhs=True,\n      fwd_rhs=True,\n      dlhs_lhs=True,\n      dlhs_rhs=True,\n      drhs_lhs=True,\n      drhs_rhs=True,\n  )\n  return _build_per_tensor_config(aqt_dg, per_tensor_scales)",
        "analysis": {
            "functionality": "This function generates a default AQT (AQuantized Training) configuration for 8-bit floating-point (FP8) quantization. It sets up the quantization parameters for forward and backward passes, with options for static scaling via constant bounds or dynamic scaling by default.",
            "usage": "Call this function with a configuration object (`config`) to get an `aqt_config.DotGeneral` object. The input `config` object can optionally contain a `constant_bound_config` attribute (a sequence of 6 floats) to enable static scaling; otherwise, dynamic scaling is used. The returned object is used to configure AQT-enabled layers."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_fp8_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_fp8_quant_config(config):\n  \"\"\"get aqt for 8-bit floating point quantization configuration\"\"\"\n  cfg = aqt_config.config_v4(fwd_bits=\"e4m3\", dlhs_bits=None, drhs_bits=None, fwd_accumulator_dtype=jnp.bfloat16)\n  return cfg",
        "analysis": {
            "module_type": "quantization_config_helper",
            "purpose": "Creates and returns an AQT configuration for 8-bit floating-point (e4m3) forward-pass-only quantization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `aqt_config.config_v4` with hardcoded parameters to create a quantization configuration.",
                "Sets forward pass bits (`fwd_bits`) to 'e4m3'.",
                "Disables backward pass quantization by setting `dlhs_bits` and `drhs_bits` to None.",
                "Sets the forward pass accumulator data type (`fwd_accumulator_dtype`) to `jnp.bfloat16`.",
                "Returns the created configuration object."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.config.config_v4",
                "jax.numpy.bfloat16"
            ],
            "parameters": {
                "config": "A configuration object, which is passed as an argument but not used within the function."
            },
            "notes": [
                "This function generates a configuration specifically for inference or forward-pass-only scenarios, as the backward pass quantization is explicitly disabled.",
                "The input `config` parameter is not utilized in the function's logic."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_dot_general_make",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _dot_general_make(quant_cfg):\n  \"\"\"Create quantization configs for input matrices to a matmul\"\"\"\n  lhs_bits = quant_cfg[_A_BITS]\n  lhs_scale = quant_cfg[_A_SCALE]\n  rhs_bits = quant_cfg[_W_BITS]\n  rhs_scale = quant_cfg[_W_SCALE]\n  aqt_dg = aqt_config.dot_general_make(lhs_bits=lhs_bits, rhs_bits=rhs_bits)\n  if lhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.lhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=lhs_scale)\n  if rhs_scale < 1.0:\n    aqt_dg.fwd.dg_quantizer.rhs.calibration = functools.partial(calibration.AbsMaxCalibration, scale=rhs_scale)\n  return aqt_dg",
        "analysis": {
            "functionality": "This function creates and configures an AQT (Accurate Quantized Training) `DotGeneral` object for a matrix multiplication operation. It sets the bit precision for the left-hand side (activations) and right-hand side (weights) and conditionally applies a specific calibration method (`AbsMaxCalibration`) if a clipping scale less than 1.0 is provided.",
            "usage": "Call this function with a dictionary `quant_cfg` containing keys for activation bits ('a_bits'), activation scale ('a_scale'), weight bits ('w_bits'), and weight scale ('w_scale'). It returns a configured `aqt_config.DotGeneral` object that can be used to quantize a matrix multiplication."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_default_mp_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_default_mp_config(default=None):\n  default_config = {_W_BITS: None, _A_BITS: None, _W_SCALE: 1.0, _A_SCALE: 1.0, _TILE_SIZE: -1}\n  if default:\n    default_config.update(default)\n  return default_config",
        "analysis": {
            "module_type": "configuration_helper",
            "purpose": "Creates a default configuration dictionary for mixed-precision quantization and optionally updates it with user-provided values.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize a dictionary `default_config` with predefined keys and default values.",
                "Check if the optional `default` argument (a dictionary) is provided.",
                "If `default` is provided, update `default_config` with its key-value pairs.",
                "Return the final `default_config` dictionary."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_W_BITS",
                "_A_BITS",
                "_W_SCALE",
                "_A_SCALE",
                "_TILE_SIZE"
            ],
            "parameters": {
                "default": "An optional dictionary to override or add to the default configuration values. Defaults to None."
            },
            "notes": [
                "This function is a helper used to establish a base configuration for mixed-precision quantization layers.",
                "The default configuration sets bits to None, scales to 1.0, and tile size to -1."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_mixed_precision_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_mixed_precision_quant_config(mixed_precision_config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  ret_config = {}\n  default_mp_config = _get_default_mp_config(default=mixed_precision_config.get(DEFAULT, None))\n  for layer_name_re, layer_quantization_config in mixed_precision_config.items():\n    # Make a copy of default_mp_config to avoid updating original dict\n    quant_config = default_mp_config.copy()\n    # print(f\"Mixed precision config: processing\n    # {layer_name_re} - {layer_quantization_config}, default config - {quant_config}\")\n    if layer_name_re != DEFAULT:\n      for k in quant_config:\n        quant_config[k] = layer_quantization_config.get(k, default_mp_config[k])\n    ret_config[layer_name_re] = [_dot_general_make(quant_config), quant_config[\"tile_size\"]]\n  return ret_config",
        "analysis": {
            "module_type": "mixed_precision_quantization_configurator",
            "purpose": "Processes a user-defined mixed-precision configuration dictionary to generate AQT quantization parameters for different model layers.",
            "input": {
                "shape": "N/A",
                "dtype": "A dictionary where keys are layer name regular expressions (strings) and values are dictionaries of quantization parameters (e.g., 'w_bits', 'a_bits', 'tile_size')."
            },
            "processing_steps": [
                "Initialize an empty dictionary `ret_config`.",
                "Get a default configuration by calling `_get_default_mp_config`, using the '__default__' entry from the input config if it exists.",
                "Iterate through each layer's configuration in the input `mixed_precision_config`.",
                "For each layer, create a copy of the default configuration.",
                "If the layer is not the default, override the copied configuration with the layer-specific settings.",
                "Generate an AQT `DotGeneral` configuration for the layer by calling `_dot_general_make`.",
                "Store the generated AQT config and the `tile_size` in `ret_config` keyed by the layer name regex.",
                "Return the populated `ret_config` dictionary."
            ],
            "output": {
                "shape": "A dictionary where keys are layer name regular expressions and values are a list containing an AQT `DotGeneral` config object and an integer `tile_size`."
            },
            "dependencies": [
                "_get_default_mp_config",
                "_dot_general_make",
                "DEFAULT"
            ],
            "parameters": {
                "mixed_precision_config": "A dictionary mapping layer name regular expressions to their specific quantization settings. A special '__default__' key can be used to set global defaults."
            },
            "notes": [
                "The function supports layer-specific quantization settings by matching layer names against regular expressions provided as keys in the input dictionary.",
                "It uses a copy of the default configuration for each layer to ensure that layer-specific overrides do not affect the defaults for other layers."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_quant_config",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_quant_config(config):\n  \"\"\"Set quantization params based on user configuration.\"\"\"\n  if not config.quantization or config.quantization == \"\":\n    return None\n  if config.quantization == \"int8\":\n    return _get_int8_quant_config(config)\n  if config.quantization == \"intmp\":\n    assert config.quant_cfg_path, \"Must specify quant_cfg for mixed precision quantization\"\n    with open(config.quant_cfg_path, \"rt\", encoding=\"utf8\") as config_file:\n      mixed_precision_config = json.load(config_file)\n    return _get_mixed_precision_quant_config(mixed_precision_config)\n  if config.quantization == \"fp8\":\n    return \"fp8\"\n  if config.quantization == \"nanoo_fp8\":\n    return \"nanoo_fp8\"\n  if config.quantization == \"aqt_fp8\":\n    return _get_aqt_fp8_quant_config(config)\n  if config.quantization == \"aqt_fp8_full\":\n    return _get_aqt_fp8_default_config(config)\n\n  raise ValueError(f\"Invalid value configured for quantization {config.quantization}.\")",
        "analysis": {
            "functionality": "This function acts as a factory to generate a specific quantization configuration based on a provided configuration object. It selects the appropriate quantization method (e.g., int8, fp8, mixed-precision) based on the `config.quantization` string and calls helper functions to create the detailed configuration.",
            "usage": "Call this function with a configuration object (e.g., `MaxText.common_types.Config`). The function reads the `quantization` and `quant_cfg_path` attributes from the config object and returns the corresponding quantization settings. The return type varies depending on the selected method and can be `None`, a string identifier, or a complex configuration object."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_convert_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_convert_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.CONVERT)",
        "analysis": {
            "module_type": "quantization_mode_checker",
            "purpose": "Checks if the provided quantization configuration object is in CONVERT mode.",
            "input": {
                "shape": "N/A",
                "dtype": "AqtQuantization object or similar"
            },
            "processing_steps": [
                "Check if the input 'quant' object is truthy (not None).",
                "Check if the 'quant_mode' attribute of the 'quant' object is equal to aqt_flax.QuantMode.CONVERT.",
                "Return the boolean result of the logical AND of the two conditions."
            ],
            "output": {
                "shape": "boolean"
            },
            "dependencies": [
                "aqt_flax.QuantMode"
            ],
            "parameters": {
                "quant": "The quantization configuration object, which may be None or an object with a 'quant_mode' attribute."
            },
            "notes": [
                "This is a utility function to simplify checking the quantization status within other modules.",
                "The function short-circuits and returns False if the 'quant' object is None."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#in_serve_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def in_serve_mode(quant):\n  return quant and (quant.quant_mode == aqt_flax.QuantMode.SERVE)",
        "analysis": {
            "functionality": "Checks if the provided quantization configuration object is in 'serve' mode.",
            "usage": "Call with a quantization configuration object. Returns true if the object exists and its 'quant_mode' attribute is set to aqt_flax.QuantMode.SERVE, otherwise returns false."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quant_mode",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quant_mode(quant_mode_str: str = \"train\"):\n  \"\"\"Set quant mode.\"\"\"\n  if quant_mode_str == \"train\":\n    return aqt_flax.QuantMode.TRAIN\n  elif quant_mode_str == \"serve\":\n    return aqt_flax.QuantMode.SERVE\n  elif quant_mode_str == \"convert\":\n    return aqt_flax.QuantMode.CONVERT\n  else:\n    raise ValueError(f\"Invalid quantization mode {quant_mode_str}.\")\n  return None",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Converts a string representation of a quantization mode into the corresponding aqt_flax.QuantMode enum value.",
            "input": {
                "shape": "N/A",
                "dtype": "str"
            },
            "processing_steps": [
                "Check if the input string `quant_mode_str` matches 'train', 'serve', or 'convert'.",
                "Return the corresponding `aqt_flax.QuantMode` enum value (TRAIN, SERVE, or CONVERT).",
                "If the string does not match any valid mode, raise a ValueError."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "aqt.jax.v2.flax.aqt_flax"
            ],
            "parameters": {
                "quant_mode_str": "A string specifying the quantization mode. Valid options are 'train', 'serve', and 'convert'. Defaults to 'train'."
            },
            "notes": [
                "The function enforces a strict set of allowed quantization modes.",
                "The final `return None` statement is unreachable because the `else` block always raises an exception."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_quantization",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_quantization(config: Config, quant_mode_str: str = \"train\"):\n  \"\"\"Configure quantization based on user config and quant mode.\"\"\"\n  if config.use_qwix_quantization:\n    return None\n  quant_cfg = _get_quant_config(config)\n  if quant_cfg:\n    if quant_cfg == \"fp8\":\n      return Fp8Quantization()\n    elif quant_cfg == \"nanoo_fp8\":\n      return NANOOFp8Quantization()\n    quant_mode = get_quant_mode(quant_mode_str)\n    replicate_scale = config.replicate_quant_scale if config.replicate_quant_scale else False\n    return AqtQuantization(quant_dg=quant_cfg, quant_mode=quant_mode, replicate_scale=replicate_scale)\n  return None",
        "analysis": {
            "functionality": "This function acts as a factory to create and configure a specific quantization object based on the provided configuration and mode. It can return an AQT, FP8, or NANOO FP8 quantization object, or None if quantization is disabled.",
            "usage": "Call this function with a `Config` object and an optional quantization mode string (e.g., 'train', 'serve', 'convert'). It returns a quantization configuration object or `None`. The returned object is then used to configure quantization-aware operations within the model."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#match_aqt_and_unquantized_param",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def match_aqt_and_unquantized_param(aqt_params, params):\n  \"\"\"match aqt and unquantized params\"\"\"\n  aqt_param_flat, aqt_tree_def = jax.tree_util.tree_flatten_with_path(\n      aqt_params, is_leaf=lambda x: isinstance(x, aqt_tensor.QTensor)\n  )\n  param_tree_flat, _ = jax.tree_util.tree_flatten_with_path(params)\n  aqt_paths = []\n  # Original path of quantized AQT param path.\n  param_paths = []\n\n  for aqt_k, _ in aqt_param_flat:\n    index = None\n    for index, (k, _) in enumerate(param_tree_flat):\n      path_depth = len(k)\n      # every quantized parameter has AQT.. as the leaf node\n      # AqtDotGeneral and AqtEinsum replace leaf node.\n      # Therefore, leaf node should be ignored for path matching\n      # Note: Aqt only operates on kernels so don't pop bias parameters.\n      # Ref: https://github.com/AI-Hypercomputer/maxtext/compare/main...quantize_r1\n      if k[: path_depth - 1] == aqt_k[: path_depth - 1] and k[-1].key != \"bias\":\n        aqt_paths.append(aqt_k)\n        param_paths.append(k)\n        break\n    assert index is not None\n    # since the parameter is already added, we can delete it.\n    param_tree_flat.pop(index)\n  return jax.tree_util.tree_unflatten(aqt_tree_def, param_paths)",
        "analysis": {
            "module_type": "parameter_path_matching_utility",
            "purpose": "Matches paths of quantized AQT parameters with their corresponding unquantized parameter paths in two separate PyTrees.",
            "input": {
                "shape": "aqt_params: PyTree containing aqt_tensor.QTensor leaves, params: PyTree of unquantized parameters.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Flatten the `aqt_params` PyTree into a list of (path, value) pairs, defining `aqt_tensor.QTensor` instances as leaves.",
                "Flatten the `params` PyTree into a list of (path, value) pairs.",
                "Iterate through each path from the flattened `aqt_params`.",
                "For each AQT path, search for a corresponding path in the flattened `params`.",
                "A match is determined if the paths are identical up to the penultimate element and the final key of the unquantized path is not 'bias'.",
                "Store the matched unquantized parameter path.",
                "Remove the matched unquantized parameter from the search list to prevent redundant matching.",
                "Reconstruct a PyTree with the original structure of `aqt_params`, where leaves are the matched unquantized parameter paths."
            ],
            "output": {
                "shape": "A PyTree with the same structure as `aqt_params`, where each leaf is the JAX path tuple of the corresponding unquantized parameter."
            },
            "dependencies": [
                "jax.tree_util.tree_flatten_with_path",
                "jax.tree_util.tree_unflatten",
                "aqt.jax.v2.aqt_tensor.QTensor"
            ],
            "parameters": {
                "N/A": "This function does not take parameters from a configuration object."
            },
            "notes": [
                "The core logic relies on the assumption that AQT layers replace the final leaf node (e.g., 'kernel') of a parameter path, so matching is done on the parent path.",
                "It explicitly excludes parameters named 'bias' from the matching process, assuming they are not quantized.",
                "An assertion ensures that every quantized AQT parameter finds a corresponding unquantized parameter."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#_get_aqt_key_paths",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def _get_aqt_key_paths(aqt_vars, params):\n  \"\"\"Generate a list of paths which have aqt state\"\"\"\n  aqt_to_unquantized_key_path = match_aqt_and_unquantized_param(aqt_vars, params)\n  aqt_key_paths, _ = jax.tree_util.tree_flatten(aqt_to_unquantized_key_path, is_leaf=lambda x: isinstance(x, tuple))\n  return list(aqt_key_paths)",
        "analysis": {
            "module_type": "helper_function",
            "purpose": "Generates a list of key paths for parameters that have AQT (AQuantized Training) state.",
            "input": {
                "shape": "aqt_vars: PyTree of AQT variables. params: PyTree of model parameters.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `match_aqt_and_unquantized_param` to create a PyTree mapping AQT variables to their corresponding unquantized parameter paths.",
                "Uses `jax.tree_util.tree_flatten` to extract all the path tuples from the resulting PyTree.",
                "Converts the flattened paths into a list and returns it."
            ],
            "output": {
                "shape": "A Python list of tuples, where each tuple represents a key path to a quantized parameter."
            },
            "dependencies": [
                "match_aqt_and_unquantized_param",
                "jax.tree_util.tree_flatten"
            ],
            "parameters": {
                "aqt_vars": "A PyTree (e.g., nested dictionary) containing AQT state variables.",
                "params": "A PyTree containing the model's unquantized parameters."
            },
            "notes": [
                "This function is a utility used to identify which parameters have been quantized, as demonstrated by its usage in the `remove_quantized_params` function.",
                "The `is_leaf` argument in `tree_flatten` is specifically set to treat path tuples as the leaves to be collected."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#remove_quantized_params",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def remove_quantized_params(params, aqt_vars):\n  \"\"\"Remove param values with aqt tensors to Null to optimize memory.\"\"\"\n  quantized_param_paths = _get_aqt_key_paths(aqt_vars, params)\n  tree_flat, tree_struct = tree_flatten_with_path(params)\n  for i, (k, v) in enumerate(tree_flat):\n    if k in quantized_param_paths:\n      v = {}\n    tree_flat[i] = v\n  return tree_unflatten(tree_struct, tree_flat)",
        "analysis": {
            "functionality": "This function identifies parameters within a PyTree that have corresponding AQT quantization variables and replaces their values with empty dictionaries to optimize memory.",
            "usage": "Call this function with a PyTree of model parameters (`params`) and a PyTree of AQT variables (`aqt_vars`). It returns a new PyTree of the same structure as `params`, but with quantized parameter values replaced by `{}`. This is typically used to reduce the size of a checkpoint by removing redundant unquantized weights."
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#configure_kv_quant",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def configure_kv_quant(config):\n  return None if not config.quantize_kvcache else KVQuant(config)",
        "analysis": {
            "module_type": "kv_cache_quantization_configurator",
            "purpose": "Conditionally creates a KVQuant object for Key-Value cache quantization based on a configuration flag.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the boolean value of `config.quantize_kvcache`.",
                "If true, instantiate and return a `KVQuant` object, passing the `config` to its constructor.",
                "If false, return `None`."
            ],
            "output": {
                "shape": "An instance of the KVQuant class or None."
            },
            "dependencies": [
                "MaxText.inference.kvcache.KVQuant",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantize_kvcache": "A boolean flag that determines whether to enable Key-Value cache quantization."
            },
            "notes": [
                "This function acts as a factory for the KV cache quantization handler.",
                "Returning `None` is the mechanism used to disable this feature throughout the application."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NvidaFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NvidaFp8Provider(qwix.QtProvider):\n  \"\"\"Wraps nn.Fp8DirectDotGeneralOp with Qwix's provider interface.\"\"\"\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.Fp8DirectDotGeneralOp(name=op_id)(*args, **kwargs)\n\n  def einsum(self, *args, **kwargs):\n    rule, op_id = self._get_current_rule_and_op_id(\"einsum\")\n    if rule is None:\n      return jnp.einsum(*args, **kwargs)\n    return nn.Fp8Einsum(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "fp8_quantization_provider",
            "purpose": "Provides a conditional wrapper for JAX operations, replacing them with NVIDIA-specific FP8 implementations (`nn.Fp8DirectDotGeneralOp`, `nn.Fp8Einsum`) when a quantization rule from the Qwix framework is active.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax",
                "jax.numpy",
                "flax.linen as nn"
            ],
            "parameters": {
                "rules": "A list of quantization rules passed to the parent `qwix.QtProvider` constructor, which determine when to apply FP8 operations."
            },
            "notes": [
                "This class inherits from `qwix.QtProvider` and implements its interface to enable FP8 quantization for specific operations.",
                "The decision to use an FP8 kernel or the default JAX primitive is made dynamically based on the current module's path and the configured quantization rules."
            ],
            "methods": {
                "dot_general": {
                    "purpose": "Executes an FP8 dot-general operation if a quantization rule is active for the current context, otherwise falls back to the standard JAX implementation.",
                    "input": {
                        "shape": "Accepts the same arguments as `jax.lax.dot_general`, typically `(lhs, rhs, dimension_numbers, ...)`.",
                        "dtype": "Typically a floating-point type like float32 or bfloat16."
                    },
                    "processing_steps": [
                        "Call `self._get_current_rule_and_op_id(\"dot_general\")` to check for an active quantization rule.",
                        "If a rule is found, instantiate and call `nn.Fp8DirectDotGeneralOp` with the provided arguments.",
                        "If no rule is found, call `jax.lax.dot_general` with the provided arguments."
                    ],
                    "output": {
                        "shape": "The shape resulting from the dot-general operation."
                    },
                    "dependencies": [
                        "jax.lax.dot_general",
                        "nn.Fp8DirectDotGeneralOp"
                    ],
                    "notes": []
                },
                "einsum": {
                    "purpose": "Executes an FP8 einsum operation if a quantization rule is active for the current context, otherwise falls back to the standard JAX implementation.",
                    "input": {
                        "shape": "Accepts the same arguments as `jnp.einsum`, typically `(equation, operand1, operand2, ...)`.",
                        "dtype": "Typically a floating-point type like float32 or bfloat16."
                    },
                    "processing_steps": [
                        "Call `self._get_current_rule_and_op_id(\"einsum\")` to check for an active quantization rule.",
                        "If a rule is found, instantiate and call `nn.Fp8Einsum` with the provided arguments.",
                        "If no rule is found, call `jnp.einsum` with the provided arguments."
                    ],
                    "output": {
                        "shape": "The shape resulting from the einsum operation, determined by the equation string."
                    },
                    "dependencies": [
                        "jnp.einsum",
                        "nn.Fp8Einsum"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#NANOOFp8Provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "class NANOOFp8Provider(qwix.QtProvider):\n\n  def dot_general(self, *args, **kwargs):\n    # Here we only check if the rule is None or not.\n    rule, op_id = self._get_current_rule_and_op_id(\"dot_general\")\n    if rule is None:\n      return jax.lax.dot_general(*args, **kwargs)\n    return nn.NANOOFp8DotGeneralOp(name=op_id)(*args, **kwargs)",
        "analysis": {
            "module_type": "fp8_quantization_provider",
            "purpose": "Provides a hardware-specific implementation for FP8 dot-general operations on AMD NANOO GPUs by conditionally replacing the standard JAX operation with a quantized version.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "N/A"
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtProvider",
                "jax.lax",
                "flax.linen.nn"
            ],
            "parameters": {},
            "notes": [
                "This class is part of the Qwix quantization framework and is intended to be used to quantize models for AMD MI300/MI325 GPUs.",
                "It inherits from `qwix.QtProvider` and overrides specific JAX operations based on quantization rules."
            ],
            "methods": {
                "dot_general": {
                    "purpose": "Intercepts dot_general calls and replaces them with `nn.NANOOFp8DotGeneralOp` if a quantization rule applies, otherwise falls back to the standard `jax.lax.dot_general`.",
                    "input": {
                        "shape": "Accepts standard arguments for `jax.lax.dot_general`, including input tensors `lhs` and `rhs`.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieve the current quantization rule and operation ID for 'dot_general' using `_get_current_rule_and_op_id`.",
                        "If no rule is found, execute the standard `jax.lax.dot_general`.",
                        "If a rule is found, execute the specialized `nn.NANOOFp8DotGeneralOp`."
                    ],
                    "output": {
                        "shape": "The shape of the resulting tensor from the dot-general operation."
                    },
                    "dependencies": [
                        "jax.lax.dot_general",
                        "flax.linen.nn.NANOOFp8DotGeneralOp"
                    ],
                    "notes": [
                        "The decision to use the quantized operation is determined by the quantization rules configured in the provider, not by the method's arguments."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_quantization_rule",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_quantization_rule(config: Config):\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.int8,\n          act_qtype=jnp.int8,\n          bwd_qtype=jnp.int8,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_full\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e5m2,\n          bwd_use_original_residuals=True,\n          disable_channelwise_axes=True, # per_tensor calibration\n          weight_calibration_method=config.quantization_calibration_method,\n          act_calibration_method=config.quantization_calibration_method,\n          bwd_calibration_method=config.quantization_calibration_method,\n          op_names=(\"dot_general\",),\n          additional_qt_config={\n            \"dlhs_lhs_qtype\": jnp.float8_e5m2,\n            \"dlhs_rhs_qtype\": jnp.float8_e4m3fn,\n            \"drhs_lhs_qtype\": jnp.float8_e4m3fn,\n            \"drhs_rhs_qtype\": jnp.float8_e5m2,\n          },\n      )\n    case \"fp8_gpu\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"fp8_nanoo\":\n      return qwix.QtRule(\n          module_path=\"decoder/.*layers.*\",\n          weight_qtype=jnp.float8_e4m3fn,\n          act_qtype=jnp.float8_e4m3fn,\n          bwd_qtype=jnp.float8_e4m3fn,\n          bwd_weight_grad_tile_size=1 / config.quantization_local_shard_count,\n          op_names=(\"dot_general\",),\n      )\n    case \"\":\n      return None",
        "analysis": {
            "module_type": "quantization_rule_factory",
            "purpose": "Creates and returns a Qwix quantization rule object based on the quantization type specified in the configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "MaxText.common_types.Config"
            },
            "processing_steps": [
                "Read the `quantization` string from the input `config` object.",
                "Use a match-case statement to select the appropriate quantization recipe ('int8', 'fp8', 'fp8_full', 'fp8_gpu', 'fp8_nanoo').",
                "Instantiate a `qwix.QtRule` object with parameters specific to the selected recipe.",
                "Return the created `qwix.QtRule` object, or `None` if `config.quantization` is an empty string."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "qwix.QtRule",
                "jax.numpy",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantization": "A string that determines which quantization configuration to create. Can be 'int8', 'fp8', 'fp8_full', 'fp8_gpu', 'fp8_nanoo', or ''.",
                "config.quantization_local_shard_count": "Used to calculate the `bwd_weight_grad_tile_size` for sharding in the backward pass.",
                "config.quantization_calibration_method": "Specifies the calibration method for weights and activations in the 'fp8_full' configuration."
            },
            "notes": [
                "This function acts as a factory to centralize the creation of different quantization configurations for the Qwix library.",
                "The returned rule is configured to apply only to `dot_general` operations within modules matching the path 'decoder/.*layers.*'.",
                "If `config.quantization` is an empty string, the function returns `None`, effectively disabling this quantization rule.",
                "The 'fp8_full' case provides a more detailed configuration, including different dtypes for the backward pass and per-tensor calibration."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#get_qt_provider",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def get_qt_provider(config):\n  \"\"\"Get quantization rules based on the config.\"\"\"\n  match config.quantization:\n    case \"int8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_full\":\n      return qwix.QtProvider([get_quantization_rule(config)])\n    case \"fp8_gpu\":\n      return NvidaFp8Provider([get_quantization_rule(config)])\n    case \"fp8_nanoo\":\n      return NANOOFp8Provider([get_quantization_rule(config)])\n  return None",
        "analysis": {
            "module_type": "quantization_provider_factory",
            "purpose": "Creates and returns a specific quantization provider instance based on the quantization type specified in the configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "Config object"
            },
            "processing_steps": [
                "Read the `quantization` attribute from the input `config` object.",
                "Use a `match` statement to select the appropriate quantization provider based on the `config.quantization` string.",
                "Call `get_quantization_rule(config)` to obtain the specific quantization rules.",
                "Instantiate a provider class (`qwix.QtProvider`, `NvidaFp8Provider`, or `NANOOFp8Provider`) with the generated rule.",
                "Return the provider instance if a match is found, otherwise return `None`."
            ],
            "output": {
                "shape": "A quantization provider object (e.g., qwix.QtProvider, NvidaFp8Provider) or None."
            },
            "dependencies": [
                "get_quantization_rule",
                "qwix.QtProvider",
                "NvidaFp8Provider",
                "NANOOFp8Provider"
            ],
            "parameters": {
                "config.quantization": "A string that specifies the type of quantization to use. Expected values include 'int8', 'fp8', 'fp8_full', 'fp8_gpu', 'fp8_nanoo'."
            },
            "notes": [
                "This function acts as a factory for creating quantization providers.",
                "The choice of provider class depends on the target hardware implied by the quantization string (e.g., 'fp8_gpu' for NVIDIA, 'fp8_nanoo' for AMD).",
                "If `config.quantization` does not match any of the cases, the function returns `None`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/quantizations.py#maybe_quantize_model",
        "file_path": "src/MaxText/layers/quantizations.py",
        "code_block": "def maybe_quantize_model(model, config):\n  \"\"\"Quantize the model if quantization is enabled.\"\"\"\n  if config.use_qwix_quantization:\n    quantization_provider = get_qt_provider(config)\n    if quantization_provider:\n      model = qwix.quantize_model(model, quantization_provider)\n  return model",
        "analysis": {
            "functionality": "This function conditionally applies quantization to a given model based on the provided configuration. If the `use_qwix_quantization` flag in the config is set, it obtains a quantization provider and uses the `qwix` library to quantize the model.",
            "usage": "Call this function with a model object and a configuration object. It returns the quantized model if quantization is enabled in the config, otherwise it returns the original model. Example: `quantized_model = maybe_quantize_model(my_model, my_config)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#is_vanilla_variable",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def is_vanilla_variable(vs: variablelib.VariableState) -> bool:\n  \"\"\"A variables state is vanilla if its metadata is essentially blank.\n\n  Returns False only if it has non-empty hooks or any non-built-in attribute.\n  \"\"\"\n  for key, value in vs.get_metadata().items():\n    if key.endswith(\"_hooks\"):\n      if value != ():\n        return False\n    else:\n      return False\n  return True",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Checks if a `VariableState` object is 'vanilla', meaning its metadata is essentially blank, containing only empty hooks and no other custom attributes.",
            "input": {
                "shape": "N/A",
                "dtype": "flax.nnx.variablelib.VariableState"
            },
            "processing_steps": [
                "Retrieves the metadata dictionary from the input VariableState object using `vs.get_metadata()`.",
                "Iterates through the key-value pairs of the metadata.",
                "If a key ends with '_hooks', it checks if the associated value is an empty tuple. If not, it returns False.",
                "If a key does not end with '_hooks', it immediately returns False, indicating a non-built-in attribute.",
                "If the loop completes without returning False, it returns True."
            ],
            "output": {
                "shape": "Scalar boolean"
            },
            "dependencies": [
                "flax.nnx.variablelib"
            ],
            "parameters": {},
            "notes": [
                "The function defines a 'vanilla' variable state as one that has no metadata other than potentially empty `_hooks` attributes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_var(vs: variablelib.VariableState) -> meta.AxisMetadata:\n  metadata = vs.get_metadata()\n  if \"linen_meta_type\" in metadata:\n    linen_type = metadata[\"linen_meta_type\"]\n    if hasattr(linen_type, \"from_nnx_metadata\"):\n      return linen_type.from_nnx_metadata({\"value\": vs.value, **metadata})\n    return linen_type(vs.value, **metadata)\n  if is_vanilla_variable(vs):\n    return vs.value\n  return nnx.bridge.NNXMeta(vs.type, vs.value, metadata)",
        "analysis": {
            "module_type": "nnx_to_linen_variable_converter",
            "purpose": "Converts an NNX `VariableState` object into a representation suitable for a Linen variable dictionary, which could be a raw value or a Linen metadata object.",
            "input": {
                "shape": "Shape of the value within the input `VariableState` object.",
                "dtype": "Data type of the value within the input `VariableState` object."
            },
            "processing_steps": [
                "Extract metadata from the input `VariableState`.",
                "Check if 'linen_meta_type' exists in the metadata to reconstruct a specific Linen metadata type, calling `from_nnx_metadata` if available.",
                "If no specific type is found, check if the variable is a 'vanilla' variable using `is_vanilla_variable` and return the raw value if true.",
                "If the variable is not vanilla, wrap its type, value, and metadata in an `nnx.bridge.NNXMeta` object."
            ],
            "output": {
                "shape": "Same as the shape of the value within the input `VariableState`."
            },
            "dependencies": [
                "flax.nnx.variablelib.VariableState",
                "flax.core.meta.AxisMetadata",
                "is_vanilla_variable",
                "flax.nnx.bridge.NNXMeta"
            ],
            "parameters": {},
            "notes": [
                "The function's return type is conditional and can be a raw value (e.g., JAX array), a Linen `meta.AxisMetadata` subclass, or an `nnx.bridge.NNXMeta` object.",
                "This function is a core component of the interoperability layer between Flax NNX and Flax Linen."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#get_col_name",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def get_col_name(keypath: tp.Sequence[Any]) -> str:\n  \"\"\"Given the keypath of a Flax variable type, return its Linen collection name.\"\"\"\n  # Infer variable type from the leaf's path, which contains its Linen collection name\n  assert isinstance(keypath[0], jax.tree_util.DictKey)\n  return str(keypath[0].key)",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Given the keypath of a Flax variable type from a tree traversal, this function extracts and returns its Linen collection name.",
            "input": {
                "shape": "[jax.tree_util.DictKey, ...]",
                "dtype": "tp.Sequence[Any]"
            },
            "processing_steps": [
                "Assert that the first element of the input `keypath` is a `jax.tree_util.DictKey`.",
                "Access the `key` attribute of the first element.",
                "Convert the key to a string and return it."
            ],
            "output": {
                "shape": "scalar (string)"
            },
            "dependencies": [
                "jax.tree_util.DictKey"
            ],
            "parameters": {},
            "notes": [
                "The function assumes the `keypath` is generated from a JAX tree utility function like `tree_map_with_path` applied to a Linen variable dictionary.",
                "The Linen collection name is expected to be the first key in the path."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_nnx_var",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_nnx_var(col: str, x: meta.AxisMetadata | Any) -> variablelib.Variable:\n  \"\"\"Convert a Linen variable to an NNX variable.\"\"\"\n  vtype = variablelib.variable_type_from_name(col, allow_register=True)\n  if isinstance(x, nnx.bridge.NNXMeta):\n    assert vtype == x.var_type, f\"Type stored in NNXMeta {x.var_type} != type inferred from collection name {vtype}\"\n    return x.to_nnx_variable()\n  if isinstance(x, meta.AxisMetadata):\n    x_metadata = vars(x)\n    if hasattr(x, \"to_nnx_metadata\"):\n      x_metadata = x.to_nnx_metadata()\n    assert hasattr(x, \"value\")\n    return vtype(**x_metadata, linen_meta_type=type(x))\n  return vtype(x)",
        "analysis": {
            "functionality": "Converts a single Linen-style variable into its corresponding NNX Variable object, preserving metadata.",
            "usage": "Call this function with a Linen collection name (e.g., 'params') and a Linen variable value. The function determines the correct NNX variable type based on the collection name and handles different input types, including raw values, `meta.AxisMetadata`, and `nnx.bridge.NNXMeta`, returning an `nnx.variablelib.Variable` instance."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_recursive_merge",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _recursive_merge(dict1, dict2):\n  \"\"\"Recursively merge two dicts.\"\"\"\n  flat_map = nnx.traversals.flatten_mapping(dict1)\n  flat_map |= nnx.traversals.flatten_mapping(dict2)\n  return nnx.traversals.unflatten_mapping(flat_map)",
        "analysis": {
            "module_type": "dictionary_utility",
            "purpose": "Recursively merges two dictionaries, with values from the second dictionary overwriting conflicting values from the first.",
            "input": {
                "shape": "N/A",
                "dtype": "dict"
            },
            "processing_steps": [
                "Flatten the first dictionary `dict1` into a path-value map using `nnx.traversals.flatten_mapping`.",
                "Flatten the second dictionary `dict2` and merge it into the first flattened map using the in-place union operator (`|=`), which overwrites existing keys.",
                "Unflatten the merged map back into a nested dictionary structure using `nnx.traversals.unflatten_mapping`.",
                "Return the newly created merged dictionary."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "nnx.traversals.flatten_mapping",
                "nnx.traversals.unflatten_mapping"
            ],
            "parameters": {
                "dict1": "The first dictionary to be merged.",
                "dict2": "The second dictionary, whose key-value pairs will be merged into the first. Values from `dict2` will overwrite values in `dict1` in case of a key collision."
            },
            "notes": [
                "This function effectively performs a deep merge.",
                "The merge is not performed in-place on the original input dictionaries; a new dictionary is returned."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_vars_to_nnx_attrs",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_vars_to_nnx_attrs(variables: tp.Mapping[str, Any]) -> dict[str, Any]:\n  \"\"\"Convert a dict of Linen-style variables to NNX variables.\"\"\"\n  nnx_vars = jax.tree_util.tree_map_with_path(\n      lambda kp, x: to_nnx_var(get_col_name(kp), x),\n      variables,\n      is_leaf=lambda x: not isinstance(x, dict),\n  )\n\n  flat_paths: dict[tuple, tp.Any] = {}\n\n  for col_name, col_variables in nnx_vars.items():  # pylint: disable=unused-variable\n    for path, variable in nnx.traversals.flatten_mapping(col_variables).items():\n      if path in flat_paths:\n        raise ValueError(\n            f\"Found duplicate variable path {path} with variables \"\n            f\"{flat_paths[path]} and {variable}. \"\n            \"This is not allowed in NNX.\"\n        )\n      flat_paths[path] = variable\n\n  nnx_vars = nnx.traversals.unflatten_mapping(flat_paths)\n  return nnx_vars",
        "analysis": {
            "module_type": "linen_to_nnx_variable_converter",
            "purpose": "Converts a dictionary of Flax Linen-style variables into a dictionary of Flax NNX-style attributes.",
            "input": {
                "shape": "A nested dictionary mapping collection names (str) to variable structures (e.g., {'params': {'kernel': Array, 'bias': Array}}).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Recursively traverse the input `variables` dictionary using `jax.tree_util.tree_map_with_path`.",
                "For each leaf variable, convert it from a Linen-style variable to an NNX Variable object using the `to_nnx_var` helper function.",
                "Flatten all the converted NNX variables from different collections into a single dictionary mapping from a path tuple to the variable object.",
                "Check for duplicate variable paths across different collections, raising a ValueError if any are found.",
                "Unflatten the combined dictionary back into a nested dictionary structure representing NNX attributes.",
                "Return the final nested dictionary of NNX attributes."
            ],
            "output": {
                "shape": "A nested dictionary of NNX attributes, where the top-level keys are attribute names and values can be nested dictionaries or NNX Variable objects."
            },
            "dependencies": [
                "jax.tree_util.tree_map_with_path",
                "nnx.traversals.flatten_mapping",
                "nnx.traversals.unflatten_mapping",
                "to_nnx_var",
                "get_col_name"
            ],
            "parameters": {
                "variables": "A mapping (dictionary) where keys are Linen collection names (e.g., 'params') and values are nested dictionaries of variables."
            },
            "notes": [
                "This function is a key part of the interoperability bridge between Flax Linen and Flax NNX.",
                "It enforces a critical NNX constraint: variable paths must be unique across all collections, preventing situations where, for example, `params.dense.kernel` and `state.dense.kernel` would map to the same attribute path in an NNX module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#nnx_attrs_to_linen_vars",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def nnx_attrs_to_linen_vars(nnx_attrs: dict) -> dict:\n  \"\"\"Convert a dict of NNX variables (or variable states) to Linen-style variables.\"\"\"\n  linen_structured = {}\n  for kp, v in nnx.traversals.flatten_mapping(nnx_attrs).items():\n    if isinstance(v, variablelib.Variable):\n      col_name = variablelib.variable_name_from_type(type(v))\n      v = to_linen_var(v.to_state())\n    elif isinstance(v, variablelib.VariableState):\n      col_name = variablelib.variable_name_from_type(v.type)\n      v = to_linen_var(v)\n    else:\n      raise ValueError(f\"Cannot infer collection name from value: {v}\")\n    linen_structured[(col_name, *kp)] = v\n  variables = nnx.traversals.unflatten_mapping(linen_structured)\n  return variables",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Converts a dictionary of Flax NNX attributes (containing Variable or VariableState objects) into a nested dictionary of Flax Linen-style variables.",
            "input": {
                "shape": "N/A",
                "dtype": "dict"
            },
            "processing_steps": [
                "Flatten the input dictionary of NNX attributes into key paths and values using `nnx.traversals.flatten_mapping`.",
                "Iterate through the flattened key-path and value pairs.",
                "For each value, check if it's an `nnx.Variable` or `nnx.VariableState` to determine its collection name.",
                "Convert the NNX variable/state to a Linen-compatible value using the `to_linen_var` helper function.",
                "Create a new key path for the Linen structure by prepending the collection name to the original key path.",
                "Store the converted value and the new key path in a temporary dictionary.",
                "Unflatten the temporary dictionary to restore the nested structure, now organized by Linen collections, using `nnx.traversals.unflatten_mapping`."
            ],
            "output": {
                "shape": "A nested dictionary where top-level keys are Linen collection names (e.g., 'params', 'batch_stats') and values are the corresponding variable trees."
            },
            "dependencies": [
                "nnx.traversals.flatten_mapping",
                "nnx.traversals.unflatten_mapping",
                "flax.nnx.variablelib.Variable",
                "flax.nnx.variablelib.VariableState",
                "flax.nnx.variablelib.variable_name_from_type",
                "to_linen_var"
            ],
            "parameters": {},
            "notes": [
                "This function is the inverse of `linen_vars_to_nnx_attrs`.",
                "It handles both `nnx.Variable` instances (by converting them to `VariableState` first) and `nnx.VariableState` instances directly.",
                "It raises a `ValueError` if it encounters a value in the flattened dictionary that is not a `Variable` or `VariableState`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_set_initializing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _set_initializing(module: Module, initializing: bool):\n  for _, value in graph.iter_graph(module):\n    if isinstance(value, Pytree):\n      value._object__state._initializing = initializing",
        "analysis": {
            "module_type": "graph_state_modifier",
            "purpose": "Recursively traverses an NNX Module's graph and sets the `_initializing` flag on all Pytree sub-objects.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through all nodes in the graph of the input `module` using `graph.iter_graph`.",
                "For each node, check if it is an instance of `Pytree`.",
                "If it is a `Pytree`, set its protected `_object__state._initializing` attribute to the input `initializing` boolean value."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.graph.iter_graph",
                "flax.nnx.module.Module",
                "flax.nnx.Pytree"
            ],
            "parameters": {
                "module": "The NNX Module to traverse and modify.",
                "initializing": "A boolean flag indicating whether the module is in an initializing state."
            },
            "notes": [
                "This function performs an in-place modification of the `Pytree` objects within the module.",
                "It accesses a protected attribute `_object__state`, indicating it's an internal utility function.",
                "It is used by the `lazy_init` function to manage state during the lazy initialization process."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#lazy_init",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def lazy_init(fn: Module | tp.Callable[..., tp.Any], *args, **kwargs):\n  \"\"\"To run through an arbitrary nnx.Module method and initialize all its needed state.\n\n  Here used to trigger initialization of all `LinenToNNX` module variables.\"\"\"\n  if isinstance(fn, Module):\n    module = fn\n    assert callable(fn)\n  else:\n    if not (hasattr(fn, \"__self__\") and isinstance(fn.__self__, Module)):\n      raise ValueError(f\"{fn = } needs to be a method of an NNX Module.\")\n    module = fn.__self__\n  _set_initializing(module, True)\n  try:\n    _ = fn(*args, **kwargs)\n  finally:\n    _set_initializing(module, False)\n  return fn",
        "analysis": {
            "functionality": "Performs a dry run of an `nnx.Module` method to trigger the initialization of its state. It sets an internal `_initializing` flag to `True` before calling the method and resets it to `False` afterward, ensuring that modules like `LinenToNNX` can correctly create their variables.",
            "usage": "Call `lazy_init` with an `nnx.Module` instance or one of its methods, along with sample input arguments required for a forward pass. The function will execute the method, trigger state initialization, and return the original module/method. Example: `initialized_model = lazy_init(model, sample_input)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#current_linen_module",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def current_linen_module() -> linen.Module | None:\n  \"\"\"Get the current Linen module from the Linen context.\"\"\"\n  if linen.module._context.module_stack:  # pylint: disable=W0212\n    return linen.module._context.module_stack[-1]  # pylint: disable=W0212\n  return None",
        "analysis": {
            "module_type": "context_utility",
            "purpose": "Retrieves the currently active Flax Linen module from the global Linen context stack.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the internal `linen.module._context.module_stack` is non-empty.",
                "If the stack is non-empty, return the top module (the last element).",
                "If the stack is empty, return None."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen"
            ],
            "parameters": {},
            "notes": [
                "The function returns either a `linen.Module` instance or `None`.",
                "It accesses internal, protected attributes of the `flax.linen` module (`_context.module_stack`), as indicated by the `pylint: disable=W0212` comments.",
                "This function is part of the interoperability bridge between Flax NNX and Flax Linen."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToNNX",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToNNX(Module):\n  \"\"\"A wrapper to turn any Linen module into an NNX module.\n\n  The result NNX module can be used standalone with all NNX APIs, or as a submodule of\n  another NNX module.\n\n  Since Linen module initialization requires a sample input, you need to call `lazy_init`\n  with an argument to initialize the variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> linen_module = nn.Dense(features=64)\n    >>> x = jax.numpy.ones((1, 32))\n    >>> # Like Linen init(), initialize with a sample input\n    >>> model = nnx.bridge.ToNNX(linen_module, rngs=nnx.Rngs(0)).lazy_init(x)\n    >>> # Like Linen apply(), but using NNX's direct call method\n    >>> y = model(x)\n    >>> model.kernel.shape\n    (32, 64)\n\n  Args:\n    module: The Linen Module instance.\n    rngs: The `nnx.Rngs` instance being passed to any NNX module.\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  def __init__(\n      self,\n      module: linen.Module,\n      rngs: Rngs | jax.Array | None = None,\n  ):\n    self.to_nnx__module = module\n\n    self.to_nnx__rngs: Rngs | None\n    if isinstance(rngs, jax.Array):\n      self.to_nnx__rngs = Rngs(params=rngs)\n    elif isinstance(rngs, nnx.Rngs):\n      self.to_nnx__rngs = rngs.fork() if hasattr(type(rngs), \"fork\") else nnx.clone(rngs)  # type: ignore\n    else:\n      self.to_nnx__rngs = rngs\n\n  def lazy_init(self, *args, **kwargs):\n    \"\"\"A shortcut of calling `nnx.bridge.lazy_init()` upon this module.\"\"\"\n    return lazy_init(self, *args, **kwargs)\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    maybe_method = getattr(type(self.to_nnx__module), name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def __call__(\n    self,\n    *args: Any,\n    rngs: Rngs | jax.Array | None = None,\n    method: tp.Callable[..., Any] | str | None = None,\n    mutable: tp.Any = None,\n    **kwargs: Any,\n  ) -> Any:\n    # Shape-based lazy init of the flax variables\n    if rngs is None:\n      rngs = self.to_nnx__rngs\n    if isinstance(rngs, nnx.Rngs):\n      _rngs = {name: stream() for name, stream in rngs.items()}\n    elif isinstance(rngs, jax.Array):\n      _rngs = {\"params\": rngs}\n    else:\n      _rngs = {}\n    # rename default to params\n    if \"params\" not in _rngs and \"default\" in _rngs:\n      _rngs[\"params\"] = _rngs.pop(\"default\")\n    if self._object__state.initializing:\n      out, updates = self.to_nnx__module.init_with_output(_rngs, *args, method=method, **kwargs)\n    else:\n      nnx_attrs = {\n          k: v\n          for k, v in vars(self).items()\n          if not k.startswith(\"to_nnx__\") and not k.startswith(\"_pytree__\") and not k.startswith(\"_object__\")\n      }\n      variables = nnx_attrs_to_linen_vars(nnx_attrs)\n\n      # Get `mutable` from top level bridge.Module context if any\n      if mutable is not None:\n        pass\n      elif (m := bdg_module.current_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      elif (m := current_linen_module()) is not None:\n        assert m.scope is not None\n        mutable = m.scope.mutable\n      else:\n        mutable = False\n\n      out = self.to_nnx__module.apply(\n        variables, *args, rngs=_rngs, method=method, mutable=mutable, **kwargs\n      )\n\n      # Split out the updates if `mutable` is passed into the Flax module\n      if mutable is not False:\n        out, updates = out\n      else:\n        updates = None\n\n    # Split out the updates if `mutable` is passed into the Flax module\n    if updates:\n      nnx_attrs = linen_vars_to_nnx_attrs(updates)\n      # nnx.update(self, nnx_attrs)\n      # TODO(cgarciae): ideally we just do an update but currently dictionaries don't allow\n      # insertion of new keys, we need to enable this in NNX to simplify the code bellow\n      # to the simple nnx.update(self, nnx_attrs) above.\n      for attr_name, value in nnx_attrs.items():\n        if hasattr(self, attr_name) and isinstance(value, dict):\n          original_value = getattr(self, attr_name)\n          new_values = _recursive_merge(original_value, value)\n          setattr(self, attr_name, nnx.data(new_values))\n        else:\n          setattr(self, attr_name, nnx.data(value))\n\n    return out",
        "analysis": {
            "module_type": "linen_to_nnx_wrapper",
            "purpose": "A wrapper class to convert a Flax Linen module into an NNX module, making it compatible with the NNX API.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The user instantiates the class with a `flax.linen.Module` instance and optional `nnx.Rngs`.",
                "The user must call the `lazy_init` method with a sample input tensor to initialize the wrapped Linen module's parameters.",
                "The user can then call the instance like a function to perform a forward pass, which internally calls the Linen module's `apply` method.",
                "Specific methods of the wrapped Linen module can be called directly on the wrapper instance (e.g., `model.encode(x)`)."
            ],
            "output": {
                "shape": "An NNX module instance that behaves like the original Linen module."
            },
            "dependencies": [
                "flax.linen.Module",
                "flax.nnx.Module",
                "flax.nnx.Rngs",
                "lazy_init",
                "nnx_attrs_to_linen_vars",
                "linen_vars_to_nnx_attrs"
            ],
            "parameters": {
                "module": "The `flax.linen.Module` instance to be wrapped.",
                "rngs": "An optional `nnx.Rngs` instance, `jax.Array`, or `None` for random number generation."
            },
            "notes": [
                "This class acts as a bridge, allowing stateful, object-oriented NNX modules to contain and manage traditional functional Linen modules.",
                "Initialization is a mandatory and explicit step that must be performed using the `lazy_init` method with sample input data.",
                "The `__getattr__` method enables calling specific methods of the wrapped Linen module by dynamically creating partial functions that forward to `__call__`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the wrapper by storing the Linen module and processing the provided RNGs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the provided `linen.Module` in `self.to_nnx__module`.",
                        "Processes the `rngs` argument, converting it to an `nnx.Rngs` object if necessary and storing it in `self.to_nnx__rngs`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.Module",
                        "flax.nnx.Rngs",
                        "jax.Array"
                    ],
                    "notes": [
                        "Internal attributes are prefixed with `to_nnx__` to avoid name collisions with attributes of the wrapped module."
                    ]
                },
                "lazy_init": {
                    "purpose": "A shortcut method to initialize the wrapped Linen module's variables using a sample input.",
                    "input": {
                        "shape": "Arbitrary `*args` and `**kwargs` representing a sample input for the wrapped module.",
                        "dtype": "Arbitrary"
                    },
                    "processing_steps": [
                        "Calls the external `lazy_init` function on `self`, which sets an 'initializing' flag and runs a forward pass."
                    ],
                    "output": {
                        "shape": "Returns the initialized module instance (`self`)."
                    },
                    "dependencies": [
                        "lazy_init"
                    ],
                    "notes": [
                        "This is a required step before the module can be used for inference or training."
                    ]
                },
                "__getattr__": {
                    "purpose": "Dynamically provides access to the methods of the wrapped Linen module.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if an attribute exists on the parent `nnx.Module`.",
                        "If not, it checks if an attribute with the given name exists and is callable on the wrapped Linen module's class.",
                        "If it is a callable method, it returns a `partial` function that is pre-configured to call `self.__call__` with the specified method.",
                        "Otherwise, it falls back to the default attribute access."
                    ],
                    "output": {
                        "shape": "A callable partial function or the requested attribute."
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "This mechanism allows seamless calling of different methods on the wrapped module, e.g., `model.encode(x)` instead of `model(x, method='encode')`."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the wrapped Linen module, handling both initialization and application (forward pass) logic.",
                    "input": {
                        "shape": "Arbitrary `*args` and `**kwargs` to be passed to the Linen module's method.",
                        "dtype": "Arbitrary"
                    },
                    "processing_steps": [
                        "Prepare RNGs for the Linen call.",
                        "If the module is initializing, call `to_nnx__module.init_with_output`.",
                        "If the module is not initializing, convert its NNX state to Linen variables using `nnx_attrs_to_linen_vars` and call `to_nnx__module.apply`.",
                        "If the call produced state updates (e.g., from batch norm), convert the updated Linen variables back to NNX attributes using `linen_vars_to_nnx_attrs`.",
                        "Update the module's state with the new attributes.",
                        "Return the output of the Linen module call."
                    ],
                    "output": {
                        "shape": "The output tensor(s) from the wrapped Linen module's method."
                    },
                    "dependencies": [
                        "linen.Module.init_with_output",
                        "linen.Module.apply",
                        "nnx_attrs_to_linen_vars",
                        "linen_vars_to_nnx_attrs",
                        "_recursive_merge"
                    ],
                    "notes": [
                        "This is the core method that bridges the two APIs.",
                        "It uses the internal `_object__state.initializing` flag to differentiate between the initialization path and the regular application path."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#linen_rngs_dict",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def linen_rngs_dict(linen_module: linen.Module, add_default: bool = False):\n  \"\"\"Given a module, split out one of its every active RNG key collections.\"\"\"\n  assert linen_module.scope is not None, \"linen_rngs_dict() must be called inside a Linen module.\"\n  rngs: dict[str, tp.Any] = {name: linen_module.make_rng(name) for name in linen_module.scope.rngs.keys()}\n  if add_default and \"default\" not in rngs:\n    rngs[\"default\"] = 0\n  return rngs",
        "analysis": {
            "module_type": "rng_utility_function",
            "purpose": "Extracts and splits a new set of RNG keys from a Flax Linen module's active RNG streams into a dictionary.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Assert that the provided `linen_module` has an active `scope`.",
                "Create a dictionary by iterating through the names of the RNG streams in `linen_module.scope.rngs`.",
                "For each RNG stream name, call `linen_module.make_rng(name)` to generate a new JAX PRNGKey.",
                "If `add_default` is True and the 'default' key is not in the created dictionary, add it with a value of 0.",
                "Return the dictionary of RNG names to JAX PRNGKeys."
            ],
            "output": {
                "shape": "A dictionary mapping RNG stream names (str) to JAX PRNGKeys (jax.Array) or an integer placeholder."
            },
            "dependencies": [
                "flax.linen.Module"
            ],
            "parameters": {
                "linen_module": "The Flax Linen module instance from which to generate new RNG keys.",
                "add_default": "A boolean flag that, if True, ensures a 'default' key exists in the output dictionary, adding it with value 0 if not present."
            },
            "notes": [
                "This function must be called from within a Linen module's context (e.g., inside `__call__` or `setup`), as it relies on `linen_module.scope` being available.",
                "The purpose is to prepare a dictionary of RNGs suitable for passing to other Linen or NNX functions that expect an `rngs` argument."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_get_module_method",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _get_module_method(module, method: tp.Callable[..., Any] | str | None):\n  \"\"\"Get a callable method from the module, or raise TypeError.\"\"\"\n  if method is None:\n    method = \"__call__\"\n\n  if isinstance(method, str):\n    attribute_name = method\n    method = getattr(type(module), attribute_name)\n    if not callable(method):\n      class_name = type(module).__name__\n      raise TypeError(f\"'{class_name}.{attribute_name}' must be a callable, got {type(method)}.\")\n  if not callable(method):\n    class_name = type(module).__name__\n    raise TypeError(f\"'{method}' must be a callable, got {type(method)}.\")\n\n  return method",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Retrieves a callable method from a module given a method name or a direct callable, raising a TypeError if the method is not callable.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "If the 'method' argument is None, default it to the string '__call__'.",
                "If 'method' is a string, retrieve the corresponding attribute from the module's class using 'getattr'.",
                "Verify that the retrieved attribute is callable, raising a TypeError if it is not.",
                "Verify that the final 'method' object is callable, raising a TypeError if it is not.",
                "Return the validated callable method."
            ],
            "output": {
                "shape": "A callable function/method."
            },
            "dependencies": [],
            "parameters": {
                "module": "The module instance from which to retrieve the method.",
                "method": "The method to retrieve, which can be a callable, a string name, or None."
            },
            "notes": [
                "If the 'method' argument is None, it defaults to retrieving the '__call__' method.",
                "The function specifically looks up the method on the module's class (`type(module)`) rather than the instance itself.",
                "It raises a TypeError if the specified method name does not correspond to a callable attribute on the module's class."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_fix_for_qwix_quantization",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def _fix_for_qwix_quantization(module: Module):\n  \"\"\"Process the nnx module to make it compatible with QWIX quantization.\n\n  Normally Qwix only works with pure Linen modules or pure NNX modules. When\n  NNX modules are called inside Linen modules, Qwix will have issues to\n    * detect the correct module path when a Jax op (e.g. einsum) is called.\n    * detect the input types (whether it's a weight) of the Jax op.\n\n  This function will fix those issues.\n\n  Args:\n    module: The NNX module to be processed.\n  \"\"\"\n\n  # Wrap the __call__ function of the nnx modules to make sure the linen module\n  # path is updated correctly.\n  def wrap(call_fn, name: str):\n    def wrapped(*args, **kwargs):\n      if not linen.module._context.module_stack:  # pylint: disable=W0212\n        return call_fn(*args, **kwargs)\n      nn_module = linen.module._context.module_stack[-1]  # pylint: disable=W0212\n      old_path = nn_module.path\n      # We modify the path of the current nn module in place. This is a little\n      # bit hacky but should be good as a temporary solution.\n      nn_module.scope.path += (name,)\n      try:\n        return call_fn(*args, **kwargs)\n      finally:\n        nn_module.scope.path = old_path\n\n    return wrapped\n\n  for path, node in nnx.iter_graph(module):\n    # Only enable it on non-root nnx modules.\n    if path and isinstance(node, nnx.Module):\n      node.__class__ = type(\n          node.__class__.__name__,\n          (node.__class__,),\n          {\n              \"__call__\": wrap(node.__class__.__call__, str(path[-1])),\n          },\n      )\n\n  # Set the correct weight names. We call QtProvider.process_model_inputs here\n  # to avoid using Qwix internal APIs.\n  qwix.QtProvider.process_model_inputs(None, module, None, None)",
        "analysis": {
            "module_type": "qwix_quantization_fix",
            "purpose": "Processes an NNX module in-place to make it compatible with QWIX quantization, especially when used within a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "flax.nnx.Module"
            },
            "processing_steps": [
                "Define a `wrap` function to temporarily modify the current Linen module's scope path during a call.",
                "Iterate through all submodules of the input `module` using `nnx.iter_graph`.",
                "For each non-root submodule, replace its `__call__` method with the `wrap` function to correctly update the Linen module path context.",
                "Call `qwix.QtProvider.process_model_inputs` on the modified module to set correct weight names for quantization."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen",
                "flax.nnx",
                "qwix.QtProvider"
            ],
            "parameters": {
                "module": "The NNX module to be processed."
            },
            "notes": [
                "This function modifies the input module in-place.",
                "It addresses issues where Qwix fails to detect the correct module path and input types for Jax ops when NNX modules are nested inside Linen modules.",
                "The implementation is described as 'hacky' in the code comments, as it directly manipulates the private context of `flax.linen.module` and dynamically re-assigns the `__class__` of submodules."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#ToLinen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class ToLinen(linen.Module):\n  \"\"\"A wrapper to turn any NNX module into a Linen module.\n\n  The result Linen module can be used standalone with all Linen APIs, or as a\n  submodule of\n  another Linen module.\n\n  Since NNX modules are stateful and owns the state, we only create it once\n  during init\n  time, and will track its state and static data as separate variables.\n\n  Example::\n\n    >>> from flax import linen as nn, nnx\n    >>> import jax\n    >>> model = nnx.bridge.ToLinen(nnx.Linear, args=(32, 64))\n    >>> x = jax.numpy.ones((1, 32))\n    >>> y, variables = model.init_with_output(jax.random.key(0), x)\n    >>> y.shape\n    (1, 64)\n    >>> variables['params']['kernel'].shape\n    (32, 64)\n    >>> # The static GraphDef of the underlying NNX module\n    >>> variables.keys()\n    dict_keys(['params'])\n\n  Args:\n    nnx_class: The NNX Module class (not instance!).\n    args: The arguments that normally would be passed in to create the NNX\n      module.\n    kwargs: The keyword arguments that normally would be passed in to create the\n      NNX module.\n    skip_rng: True if this NNX module doesn't need `rngs` arg during\n      initialization (not common).\n\n  Returns:\n    A stateful NNX module that behaves the same as the wrapped Linen module.\n  \"\"\"\n\n  nnx_class: tp.Callable[..., Module]\n  args: tp.Sequence = ()\n  kwargs: tp.Mapping[str, tp.Any] = FrozenDict({})\n  skip_rng: bool = False\n  metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var\n\n  @linen.compact\n  def __call__(\n    self, *args, nnx_method: tp.Callable[..., Any] | str | None = None, **kwargs\n  ):\n    def _module_kwargs():\n      maybe_add_default = not self.is_initializing()\n      module_kwargs = dict(self.kwargs)\n      if not self.skip_rng:\n        module_kwargs[\"rngs\"] = nnx.Rngs(**linen_rngs_dict(self, add_default=maybe_add_default))\n      return module_kwargs\n\n    # init codepath\n    if self.is_initializing():\n      module = self.nnx_class(*self.args, **_module_kwargs())\n      # TODO: add lazy_init here in case there's an `ToNNX` submodule under `module`.\n      # update linen variables before call module to save initial state\n      self._update_variables(module)\n      _fix_for_qwix_quantization(module)\n      method_fn = _get_module_method(module, nnx_method)\n      out = method_fn(module, *args, **kwargs)\n      return out\n\n    # create the nnx module\n    module = self.nnx_class(*self.args, **_module_kwargs())\n\n    # update nnx module from linen variables\n    def maybe_unbox(x):\n      if isinstance(x, meta.AxisMetadata):\n        return x.unbox()\n      return x\n\n    states = jtu.tree_map(\n        maybe_unbox,\n        list(core.unfreeze(self.variables).values()),  # type: ignore[wrong-arg-types]\n        is_leaf=lambda x: isinstance(x, meta.AxisMetadata),\n    )\n    if not states:\n      states = ({},)\n\n    new_state = nnx.merge_state(*states)\n    new_state_flat = nnx.traversals.flatten_mapping(new_state)\n    current_state_flat = nnx.traversals.flatten_mapping(nnx.state(module))\n    unknown_state_flat = {path: v for path, v in new_state_flat.items() if path not in current_state_flat}\n\n    if unknown_state_flat:\n      paths_str = \"\"\n      for path, _ in unknown_state_flat.items():\n        paths_str += f\"\\n  - {'/'.join(map(str, path))}\"\n\n      warnings.warn(f\"Found unknown module paths in incoming state:{paths_str}\")\n\n    nnx.update(module, new_state)\n\n    _fix_for_qwix_quantization(module)\n    method_fn = _get_module_method(module, nnx_method)\n    out = method_fn(module, *args, **kwargs)\n    self._update_variables(module)\n    return out\n\n  def __getattr__(self, name: str):\n    if hasattr(super(), name):\n      return super().__getattribute__(name)\n    if name in self.kwargs:\n      return self.kwargs[name]\n    maybe_method = getattr(self.nnx_class, name, None)\n    if callable(maybe_method):\n      method = partial(self.__call__, nnx_method=maybe_method)\n      method.__self__ = self\n      return method\n    return super().__getattribute__(name)\n\n  def _update_variables(self, module):\n    \"\"\"Store the NNX module's graph def and state inside Linen module variables.\"\"\"\n    state = nnx.state(module, nnx.Not(nnx.RngState))\n\n    collection_flat_state: dict[str, list[tuple[tuple[str, ...], tp.Any]]] = {}\n\n    # group state by collection\n    for path, leaf in nnx.to_flat_state(state):\n      type_ = leaf.type if isinstance(leaf, nnx.VariableState) else type(leaf)\n      collection = variablelib.variable_name_from_type(type_, allow_register=True)\n      if collection not in collection_flat_state:\n        collection_flat_state[collection] = []\n      collection_flat_state[collection].append((path, leaf))\n\n    # update linen variables\n    for collection, flat_state in collection_flat_state.items():\n      if self.is_mutable_collection(collection):\n\n        def _to_linen_var(x):\n          if isinstance(x, nnx.VariableState):\n            if self.metadata_fn is not None:\n              return self.metadata_fn(x)  # pylint: disable=too-many-function-args\n            else:\n              return x.value\n          return x\n\n        collection_state = nnx.traversals.unflatten_mapping(flat_state)\n        collection_state = jax.tree.map(\n            _to_linen_var,\n            collection_state,\n            is_leaf=lambda x: isinstance(x, nnx.VariableState),\n        )\n        for k, v in collection_state.items():\n          self.put_variable(collection, k, v)",
        "analysis": {
            "module_type": "nnx_to_linen_wrapper",
            "purpose": "A wrapper to convert a Flax NNX module class into a Flax Linen module instance, enabling its use within Linen APIs and models.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a given NNX module class with specified arguments and keyword arguments.",
                "Manages the state of the stateful NNX module by storing it within Linen's variable collections.",
                "When called, it reconstructs the NNX module, loads the state from Linen variables, executes a specified method, and then updates the Linen variables with the new state."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "flax.nnx.Module",
                "flax.nnx.Rngs",
                "flax.nnx.state",
                "flax.nnx.update",
                "flax.nnx.merge_state",
                "flax.nnx.variablelib",
                "_get_module_method",
                "_fix_for_qwix_quantization",
                "linen_rngs_dict",
                "to_linen_var"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class (not instance!) to be wrapped.",
                "args": "The positional arguments that would normally be passed to create the NNX module.",
                "kwargs": "The keyword arguments that would normally be passed to create the NNX module.",
                "skip_rng": "If True, this NNX module doesn't receive the `rngs` argument during initialization.",
                "metadata_fn": "A function to convert an NNX VariableState to a Linen variable."
            },
            "notes": [
                "The wrapper distinguishes between an initialization path and a regular application path using `self.is_initializing()`.",
                "State synchronization is key: on application, state flows from Linen variables to a new NNX instance, and after execution, the updated state flows back to the Linen variables.",
                "Implements `__getattr__` to allow calling methods of the wrapped NNX module directly on the wrapper instance (e.g., `model.encode(x)`)."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes a method of the wrapped NNX module, handling state synchronization between the Linen and NNX worlds.",
                    "input": {
                        "shape": "Variable, depends on the wrapped NNX module's method.",
                        "dtype": "Variable, depends on the wrapped NNX module's method."
                    },
                    "processing_steps": [
                        "Determine if the module is in its initialization phase using `self.is_initializing()`.",
                        "If initializing, instantiate the NNX module, save its initial state to Linen variables via `_update_variables`, and execute the target method.",
                        "If not initializing, instantiate the NNX module, load state from Linen variables into it, execute the target method, and save the updated state back to Linen variables via `_update_variables`."
                    ],
                    "output": {
                        "shape": "Variable, depends on the output of the wrapped NNX module's method."
                    },
                    "dependencies": [
                        "self.is_initializing",
                        "self._update_variables",
                        "_fix_for_qwix_quantization",
                        "_get_module_method",
                        "nnx.merge_state",
                        "nnx.update",
                        "nnx.state",
                        "linen_rngs_dict"
                    ],
                    "notes": [
                        "The `nnx_method` argument specifies which method of the wrapped NNX module to call; defaults to `__call__` if not provided.",
                        "Handles RNGs by creating an `nnx.Rngs` object from the Linen scope's RNGs."
                    ]
                },
                "__getattr__": {
                    "purpose": "Forwards attribute access to the wrapped NNX class, allowing methods of the NNX module to be called directly on the Linen wrapper instance.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the attribute exists on the parent class or in `self.kwargs`.",
                        "Check if the attribute is a callable method on the wrapped `nnx_class`.",
                        "If it is a callable method, return a partial function of `self.__call__` with the `nnx_method` argument pre-filled.",
                        "Otherwise, fall back to the default `__getattribute__`."
                    ],
                    "output": {
                        "shape": "Returns an attribute, a value, or a partially applied method. Shape is N/A."
                    },
                    "dependencies": [
                        "functools.partial"
                    ],
                    "notes": [
                        "This enables a more natural API, e.g., `model.encode(x)` instead of `model(x, nnx_method='encode')`."
                    ]
                },
                "_update_variables": {
                    "purpose": "Extracts the state from the wrapped NNX module and stores it in the appropriate Linen variable collections.",
                    "input": {
                        "shape": "Takes an NNX Module instance as input.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Get the state of the NNX module using `nnx.state`, excluding `RngState`.",
                        "Flatten the state and group items by their corresponding Linen collection name (e.g., 'params').",
                        "For each mutable collection, convert NNX `VariableState` objects to Linen-compatible variables using `self.metadata_fn`.",
                        "Store the resulting key-value pairs in the Linen module's variables using `self.put_variable`."
                    ],
                    "output": {
                        "shape": "N/A (modifies the Linen module's state in-place)."
                    },
                    "dependencies": [
                        "nnx.state",
                        "nnx.to_flat_state",
                        "flax.nnx.variablelib.variable_name_from_type",
                        "nnx.traversals.unflatten_mapping",
                        "jax.tree.map",
                        "self.put_variable",
                        "self.is_mutable_collection"
                    ],
                    "notes": [
                        "This method is the core mechanism for synchronizing state from the NNX module back to the parent Linen module."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#_Missing",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "class _Missing:\n  ...",
        "analysis": {
            "functionality": "The `_Missing` class is a simple, empty class used to create a unique sentinel object. This object serves as a default value for function arguments to distinguish between a user-provided value (which could be `None`) and an argument that was not provided at all.",
            "usage": "Instantiate the class to create a sentinel value, e.g., `_MISSING = _Missing()`. Use this instance as a default value in a function signature, e.g., `def my_func(arg=_MISSING):`. Inside the function, you can check `if arg is _MISSING:` to determine if the caller supplied a value for `arg`."
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen(\n    nnx_class: tp.Callable[..., Module],\n    *args,\n    metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    name: str | None = None,\n    skip_rng: bool = False,\n    abstract_init: bool = True,\n    **kwargs,\n):\n  \"\"\"Shortcut of `nnx.bridge.ToLinen` if user is not changing any of its default fields.\"\"\"\n  return ToLinen(\n      nnx_class,\n      args=args,\n      kwargs=FrozenDict(kwargs),\n      metadata_fn=metadata_fn,\n      skip_rng=skip_rng,\n      name=name,\n  )",
        "analysis": {
            "module_type": "linen_wrapper_factory",
            "purpose": "A convenience function that creates and returns an instance of the `ToLinen` wrapper module, which turns an NNX module into a Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Accepts an NNX Module class (`nnx_class`) and its constructor arguments (`*args`, `**kwargs`).",
                "Accepts optional configuration parameters for the wrapper (`metadata_fn`, `name`, `skip_rng`).",
                "Instantiates the `ToLinen` class, passing the provided arguments.",
                "Wraps the keyword arguments (`kwargs`) in a `flax.core.FrozenDict` before passing them to the `ToLinen` constructor.",
                "Returns the created `ToLinen` instance."
            ],
            "output": {
                "shape": "An instance of the `flax.nnx.bridge.ToLinen` module."
            },
            "dependencies": [
                "flax.nnx.bridge.ToLinen",
                "flax.core.FrozenDict"
            ],
            "parameters": {
                "nnx_class": "The NNX Module class to be wrapped into a Linen module.",
                "args": "Positional arguments for the `nnx_class` constructor.",
                "kwargs": "Keyword arguments for the `nnx_class` constructor.",
                "metadata_fn": "A function to convert an NNX `VariableState` to a Linen-compatible variable format.",
                "skip_rng": "If True, the NNX module's initialization does not require an `rngs` argument.",
                "name": "The name for the resulting Linen module.",
                "abstract_init": "A boolean parameter that is accepted but not used within the function's body."
            },
            "notes": [
                "This function serves as a simplified constructor or factory for the `ToLinen` class, intended for cases where default wrapper fields are not being changed."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/nnx_wrappers.py#to_linen_class",
        "file_path": "src/MaxText/layers/nnx_wrappers.py",
        "code_block": "def to_linen_class(\n    base_nnx_class: type[M],\n    base_metadata_fn: tp.Callable[[variablelib.VariableState], tp.Any] | None = to_linen_var,\n    base_skip_rng: bool = False,\n    **partial_kwargs: tp.Any,\n) -> type[ToLinen]:\n  \"\"\"Dynamically wraps an NNX module class into a Flax Linen module class.\"\"\"\n\n  class ToLinenPartial(ToLinen):\n    \"\"\"A dynamically created Linen Module that wraps a specific NNX Module.\n\n    This class is not meant to be used directly. Instead, it is created and\n    returned by the `to_linen_class` function. It acts as a \"partially applied\"\n    version of the `ToLinen` wrapper, where the NNX module to be wrapped and\n    its default arguments are pre-configured.\n\n    When you instantiate this class, it behaves like a standard Linen module.\n    The arguments you provide during instantiation can override the defaults\n    that were set when this class was created by `to_linen_class`.\n\n    For example:\n      >>> from flax import linen as nn, nnx\n      >>> from MaxText.layers import linears\n      >>> # Create a specialized Linen wrapper for linears.DenseGeneral\n      >>> LinenDenseGeneral = to_linen_class(linears.DenseGeneral)\n      >>> # Now, LinenDenseGeneral can be used like a regular Linen module\n      >>> class MyModel(nn.Module):\n      ...   def setup(self):\n      ...     # Instantiate the wrapped linears.DenseGeneral with its arguments\n      ...     self.dense = LinenDenseGeneral(\n      ...         in_features_shape=10, out_features_shape=5\n      ...     )\n      ...   def __call__(self, x):\n      ...     return self.dense(x)\n\n    Attributes:\n      (The attributes are dynamically set by the `ToLinen` parent class based\n       on the arguments provided during instantiation.)\n    \"\"\"\n\n    def __init_subclass__(cls, **kwargs):\n      super().__init_subclass__(**kwargs)\n\n      def __init__(\n          self,\n          args=None,\n          kwargs=None,\n          nnx_class=None,\n          skip_rng=None,\n          metadata_fn=None,\n          name=_MISSING,\n          parent=_MISSING,\n          **other_kwargs,\n      ):\n        linen_kwargs = {}\n        if not isinstance(parent, _Missing):\n          linen_kwargs[\"parent\"] = parent\n        if not isinstance(name, _Missing):\n          linen_kwargs[\"name\"] = name\n        ToLinen.__init__(\n            self,\n            nnx_class=nnx_class or base_nnx_class,\n            args=args or (),\n            metadata_fn=metadata_fn or base_metadata_fn,\n            skip_rng=skip_rng or base_skip_rng,\n            kwargs=FrozenDict({**partial_kwargs, **(kwargs or {}), **other_kwargs}),\n            **linen_kwargs,\n        )\n\n      cls.__init__ = __init__\n\n  return ToLinenPartial",
        "analysis": {
            "module_type": "nnx_to_linen_class_factory",
            "purpose": "A factory function that dynamically creates a Flax Linen module class which wraps a specified NNX module class.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define an inner class `ToLinenPartial` that inherits from `ToLinen`.",
                "Dynamically create and assign an `__init__` method to the `ToLinenPartial` class.",
                "The new `__init__` method gathers arguments passed during instantiation and merges them with the pre-configured arguments from the factory function (`partial_kwargs`).",
                "The `__init__` method calls the parent `ToLinen.__init__` with the combined arguments, effectively initializing the wrapper.",
                "Return the `ToLinenPartial` class type."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "ToLinen",
                "flax.core.FrozenDict",
                "to_linen_var"
            ],
            "parameters": {
                "base_nnx_class": "The NNX Module class (not an instance) to be wrapped into a Linen module.",
                "base_metadata_fn": "A function to convert an NNX VariableState to Linen metadata.",
                "base_skip_rng": "A boolean indicating if the NNX module's initialization skips requiring an `rngs` argument.",
                "partial_kwargs": "A dictionary of default keyword arguments to be passed to the `base_nnx_class` constructor when the returned Linen class is instantiated."
            },
            "notes": [
                "This function returns a class type (`type[ToLinen]`), not an instance of a class.",
                "The returned class acts as a 'partially applied' version of the `ToLinen` wrapper, pre-configured with the specified NNX module and its default arguments.",
                "The returned class can be instantiated like a regular Linen module, and any arguments provided at instantiation time will override the defaults set by this factory."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_mla.py#mla_as_linen",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "def mla_as_linen(\n    *,\n    config: Config,\n    num_query_heads: int,\n    num_kv_heads: int,\n    head_dim: int,\n    max_target_length: int,\n    mesh: Mesh,\n    attention_kernel: str,\n    inputs_q_shape: Tuple,\n    inputs_kv_shape: Tuple,\n    dtype: DType = jnp.float32,\n    weight_dtype: DType = jnp.float32,\n    max_prefill_predict_length: int = -1,\n    dropout_rate: float = 0.0,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n    float32_qk_product: bool = False,  # computes logits in float32 for stability.\n    float32_logits: bool = False,  # cast logits in float32 for stability.\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n    use_qk_norm: bool = False,\n    query_pre_attn_scalar: float | None = None,\n    use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n    # Temperature tuning parameters used for Llama4\n    temperature_tuning: bool = False,\n    temperature_tuning_scale: float = 0.1,\n    temperature_tuning_floor_scale: float = 8192.0,\n    # Shard the query activation as the same as the key and value.\n    # TODO: Find a better sharding axis name.\n    # TODO: Further break down the Training and Inference axes for the q, k, v.\n    prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n    query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n    ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n    input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n    ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n    out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n    ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n    prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n    decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n    prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n    decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    reshape_q: bool = False,\n    is_nope_layer: bool = False,\n    is_vision: bool = False,\n    model_mode: str = MODEL_MODE_TRAIN,\n    q_lora_rank: int = 0,\n    kv_lora_rank: int = 512,\n    qk_nope_head_dim: int = 128,\n    qk_rope_head_dim: int = 64,\n    v_head_dim: int = 128,\n    max_position_embeddings: int = 4096 * 4,\n    original_max_position_embeddings: int = 4096,\n    mscale: float = 1.0,  # scaling factor for softmax\n    rope_factor: float = 40.0,  # rotary embedding factor\n    name: str | None = None,\n):\n  \"\"\"A factory function to create an MLA as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `MLA` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MLA,\n      config=config,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      head_dim=head_dim,\n      max_target_length=max_target_length,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      inputs_q_shape=inputs_q_shape,\n      inputs_kv_shape=inputs_kv_shape,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      max_prefill_predict_length=max_prefill_predict_length,\n      dropout_rate=dropout_rate,\n      kernel_init=kernel_init,\n      float32_qk_product=float32_qk_product,\n      float32_logits=float32_logits,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      use_qk_norm=use_qk_norm,\n      query_pre_attn_scalar=query_pre_attn_scalar,\n      use_bias_in_projections=use_bias_in_projections,\n      temperature_tuning=temperature_tuning,\n      temperature_tuning_scale=temperature_tuning_scale,\n      temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n      prefill_query_axis_names=prefill_query_axis_names,\n      prefill_key_axis_names=prefill_key_axis_names,\n      prefill_value_axis_names=prefill_value_axis_names,\n      query_axis_names=query_axis_names,\n      key_axis_names=key_axis_names,\n      value_axis_names=value_axis_names,\n      ep_query_axis_names=ep_query_axis_names,\n      ep_key_axis_names=ep_key_axis_names,\n      ep_value_axis_names=ep_value_axis_names,\n      input_axis_names=input_axis_names,\n      ep_input_axis_names=ep_input_axis_names,\n      out_axis_names=out_axis_names,\n      ep_out_axis_names=ep_out_axis_names,\n      prefill_input_axis_names=prefill_input_axis_names,\n      decode_input_axis_names=decode_input_axis_names,\n      prefill_out_axis_names=prefill_out_axis_names,\n      decode_out_axis_names=decode_out_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      compute_axis_order=compute_axis_order,\n      reshape_q=reshape_q,\n      is_nope_layer=is_nope_layer,\n      is_vision=is_vision,\n      model_mode=model_mode,\n      q_lora_rank=q_lora_rank,\n      kv_lora_rank=kv_lora_rank,\n      qk_nope_head_dim=qk_nope_head_dim,\n      qk_rope_head_dim=qk_rope_head_dim,\n      v_head_dim=v_head_dim,\n      max_position_embeddings=max_position_embeddings,\n      original_max_position_embeddings=original_max_position_embeddings,\n      mscale=mscale,\n      rope_factor=rope_factor,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "mla_linen_factory",
            "purpose": "A factory function that creates a Flax Linen module by wrapping the NNX-based Multi-Head Latent Attention (MLA) class, making it compatible with a Linen-based model.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Accepts numerous configuration parameters for the MLA layer.",
                "Calls `nnx_wrappers.to_linen` with the `MLA` class and the provided parameters.",
                "Passes `variable_to_logically_partitioned` as the `metadata_fn` for parameter sharding."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance that wraps the MLA functionality."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MLA",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "A Config object containing model-wide settings.",
                "num_query_heads": "The number of attention heads for queries.",
                "num_kv_heads": "The number of attention heads for keys and values.",
                "head_dim": "The dimension of each attention head.",
                "mesh": "The JAX device mesh for distributed computation.",
                "attention_kernel": "The name of the specific attention kernel to use (e.g., 'dot_product').",
                "inputs_q_shape": "The shape of the query input tensor, used for initialization.",
                "inputs_kv_shape": "The shape of the key/value input tensor, used for initialization.",
                "kv_lora_rank": "The rank for the LoRA projection applied to keys and values.",
                "q_lora_rank": "The rank for the LoRA projection applied to queries (if non-zero)."
            },
            "notes": [
                "This function serves as a compatibility bridge between the NNX and Linen programming models.",
                "It forwards a large number of configuration parameters directly to the `MLA` class constructor.",
                "The returned object is a stateful Linen module, ready to be used within a larger Linen model definition."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_mla.py#MLA",
        "file_path": "src/MaxText/layers/attention_mla.py",
        "code_block": "class MLA(Attention):\n  \"\"\"Multi-Head Latent Attention (MLA) layer.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      num_query_heads: int,\n      num_kv_heads: int,\n      head_dim: int,\n      max_target_length: int,\n      mesh: Mesh,\n      attention_kernel: str,\n      inputs_q_shape: Tuple,\n      inputs_kv_shape: Tuple,\n      dtype: DType = jnp.float32,\n      weight_dtype: DType = jnp.float32,\n      max_prefill_predict_length: int = -1,\n      dropout_rate: float = 0.0,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"normal\"),\n      float32_qk_product: bool = False,  # computes logits in float32 for stability.\n      float32_logits: bool = False,  # cast logits in float32 for stability.\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.MLA,  # Default to MLA attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      use_qk_norm: bool = False,\n      query_pre_attn_scalar: float | None = None,\n      use_bias_in_projections: bool = False,  # Set to True will enable bias in q, k, v, o projections\n      # Temperature tuning parameters used for Llama4\n      temperature_tuning: bool = False,\n      temperature_tuning_scale: float = 0.1,\n      temperature_tuning_floor_scale: float = 8192.0,\n      # Shard the query activation as the same as the key and value.\n      # TODO: Find a better sharding axis name.\n      # TODO: Further break down the Training and Inference axes for the q, k, v.\n      prefill_query_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_key_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      prefill_value_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, KV_HEAD, KV_HEAD_DIM),\n      query_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      key_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      value_axis_names: AxisNames = (KV_BATCH, LENGTH_NO_EXP, KV_HEAD, KV_HEAD_DIM),\n      ep_query_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_key_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      ep_value_axis_names: AxisNames = (KV_BATCH_NO_EXP, LENGTH, KV_HEAD, KV_HEAD_DIM),\n      input_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, EMBED),\n      ep_input_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, EMBED),\n      out_axis_names: AxisNames = (BATCH, LENGTH_NO_EXP, HEAD, D_KV),\n      ep_out_axis_names: AxisNames = (BATCH_NO_EXP, LENGTH, HEAD, D_KV),\n      prefill_input_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, EMBED),\n      decode_input_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, EMBED),\n      prefill_out_axis_names: AxisNames = (PREFILL_KV_BATCH, PREFILL_LENGTH, HEAD, D_KV),\n      decode_out_axis_names: AxisNames = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      reshape_q: bool = False,\n      is_nope_layer: bool = False,\n      is_vision: bool = False,\n      model_mode: str = MODEL_MODE_TRAIN,\n      q_lora_rank: int = 0,\n      kv_lora_rank: int = 512,\n      qk_nope_head_dim: int = 128,\n      qk_rope_head_dim: int = 64,\n      v_head_dim: int = 128,\n      max_position_embeddings: int = 4096 * 4,\n      original_max_position_embeddings: int = 4096,\n      mscale: float = 1.0,  # scaling factor for softmax\n      rope_factor: float = 40.0,  # rotary embedding factor\n      name: str | None = None,\n      rngs: Optional[nnx.Rngs] = None,\n  ):\n    \"\"\"Initializes the MLA module.\n\n    Args:\n      config: The model configuration.\n      ... and other configuration parameters for MLA attention.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    base_kv_cache = config.attention != \"paged\" and config.mla_naive_kvcache\n\n    # Setting these before call to super because a field is used in super\n    self.q_lora_rank = q_lora_rank\n    self.kv_lora_rank = kv_lora_rank\n    self.qk_nope_head_dim = qk_nope_head_dim\n    self.qk_rope_head_dim = qk_rope_head_dim\n    self.v_head_dim = v_head_dim\n    self.max_position_embeddings = max_position_embeddings\n    self.original_max_position_embeddings = original_max_position_embeddings\n    self.mscale = mscale\n    self.rope_factor = rope_factor\n\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n\n    super().__init__(\n        config=config,\n        num_query_heads=num_query_heads,\n        num_kv_heads=num_kv_heads,\n        head_dim=head_dim,\n        max_target_length=max_target_length,\n        mesh=mesh,\n        attention_kernel=attention_kernel,\n        inputs_q_shape=inputs_q_shape,\n        inputs_kv_shape=inputs_kv_shape,\n        dtype=dtype,\n        weight_dtype=weight_dtype,\n        max_prefill_predict_length=max_prefill_predict_length,\n        dropout_rate=dropout_rate,\n        kernel_init=kernel_init,\n        float32_qk_product=float32_qk_product,\n        float32_logits=float32_logits,\n        quant=quant,\n        kv_quant=kv_quant,\n        attention_type=attention_type,\n        attn_logits_soft_cap=attn_logits_soft_cap,\n        sliding_window_size=sliding_window_size,\n        use_ragged_attention=use_ragged_attention,\n        ragged_block_size=ragged_block_size,\n        use_qk_norm=use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        use_bias_in_projections=use_bias_in_projections,\n        temperature_tuning=temperature_tuning,\n        temperature_tuning_scale=temperature_tuning_scale,\n        temperature_tuning_floor_scale=temperature_tuning_floor_scale,\n        prefill_query_axis_names=prefill_query_axis_names,\n        prefill_key_axis_names=prefill_key_axis_names,\n        prefill_value_axis_names=prefill_value_axis_names,\n        query_axis_names=query_axis_names,\n        key_axis_names=key_axis_names,\n        value_axis_names=value_axis_names,\n        ep_query_axis_names=ep_query_axis_names,\n        ep_key_axis_names=ep_key_axis_names,\n        ep_value_axis_names=ep_value_axis_names,\n        input_axis_names=input_axis_names,\n        ep_input_axis_names=ep_input_axis_names,\n        out_axis_names=out_axis_names,\n        ep_out_axis_names=ep_out_axis_names,\n        prefill_input_axis_names=prefill_input_axis_names,\n        decode_input_axis_names=decode_input_axis_names,\n        prefill_out_axis_names=prefill_out_axis_names,\n        decode_out_axis_names=decode_out_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        compute_axis_order=compute_axis_order,\n        reshape_q=reshape_q,\n        is_nope_layer=is_nope_layer,\n        is_vision=is_vision,\n        model_mode=model_mode,\n        base_kv_cache=base_kv_cache,\n        rngs=rngs,\n    )\n\n    # Module attribute names must match names previously passed to Linen for checkpointing\n    self.MlaKVCache_0 = self.init_mla_kv_caches(inputs_kv_shape) if model_mode != MODEL_MODE_TRAIN else None\n\n  def _init_projections(self, inputs_q_shape: Tuple, inputs_kv_shape: Tuple) -> None:\n    \"\"\"Initializes the MLA-specific projections.\"\"\"\n    # Assert required configuration parameters for MLA attention.\n    assert (\n        self.config.attention_type == AttentionType.MLA.value\n    ), f\"MLA requires MLA attention type {AttentionType.MLA.value}\"\n    assert self.kv_lora_rank > 0, \"KV LoRA rank must be > 0\"\n    assert self.qk_nope_head_dim > 0, \"QK NoPe head dim must be > 0\"\n    assert self.qk_rope_head_dim > 0, \"QK RoPE head dim must be > 0\"\n    assert self.v_head_dim > 0, \"V head dim must be > 0\"\n    assert self.num_query_heads == self.num_kv_heads, \"MLA requires equal number of query and kv heads\"\n    assert not self.config.fused_qkv, \"Fused QKV is not supported for MLA\"\n\n    if self.q_lora_rank == 0:\n      # Standard Q projection (without LoRA).\n      self.query = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n    else:\n      # LoRA path for Q.\n      self.wq_a = DenseGeneral(\n          in_features_shape=self.config.emb_dim,\n          out_features_shape=self.q_lora_rank,\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"q_lora_up_proj\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n      self.q_norm = RMSNorm(\n          num_features=self.q_lora_rank,\n          dtype=self.config.dtype,\n          weight_dtype=self.config.weight_dtype,\n          epsilon=self.config.normalization_layer_epsilon,\n          kernel_axes=(\"norm\",),\n          rngs=self.rngs,\n      )\n      self.wq_b = DenseGeneral(\n          in_features_shape=self.q_lora_rank,\n          out_features_shape=(self.num_query_heads, self.qk_head_dim),\n          axis=-1,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"q_lora\", \"q_heads\", \"kv\"),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          quant=self.quant,\n          matmul_precision=self.config.matmul_precision,\n          rngs=self.rngs,\n      )\n\n    # KV LoRA path.\n    self.wkv_a = DenseGeneral(\n        in_features_shape=self.config.emb_dim,\n        out_features_shape=self.kv_lora_rank + self.qk_rope_head_dim,\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"embed\", \"kv_lora_up_proj\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.kv_norm = RMSNorm(\n        num_features=self.kv_lora_rank,\n        dtype=self.config.dtype,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.wkv_b = DenseGeneral(\n        in_features_shape=self.kv_lora_rank,\n        out_features_shape=(\n            self.num_query_heads,\n            (self.qk_nope_head_dim + self.v_head_dim),\n        ),\n        axis=-1,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"kv_lora\", \"kv_heads\", \"kv_head_dim\"),\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        quant=self.quant,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n    # Set softmax scaling.\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    self.out = self.init_out_w(output_dim=inputs_q_shape[-1])\n\n    # Setup paged attention op\n    if self.config.attention == \"paged\":\n      # Set head_dim to the max of qk_head_dim and v_head_dim. The current paged\n      # attention kernel requires the head_dim to be the same for q, k, v.\n      head_dim = max(self.qk_head_dim, self.v_head_dim)\n      # Align head_dim to the pagedattn_head_dim_alignment if specified.\n      if self.config.pagedattn_head_dim_alignment > 0:\n        alignment = self.config.pagedattn_head_dim_alignment\n        head_dim = (head_dim + alignment - 1) // alignment * alignment\n      self.ds_paged_attention_op = paged_attention.PagedAttentionOp(\n          mesh=self.mesh,\n          num_pages=self.config.pagedattn_num_pages,\n          tokens_per_page=self.config.pagedattn_tokens_per_page,\n          max_pages_per_slot=(self.config.max_target_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          max_pages_per_prefill=(self.config.max_prefill_predict_length + self.config.pagedattn_tokens_per_page - 1)\n          // self.config.pagedattn_tokens_per_page,\n          pages_per_compute_block=self.config.pagedattn_pages_per_compute_block,\n          num_kv_heads=self.num_kv_heads,\n          kv_head_dim_size=head_dim,\n          dtype=self.dtype,\n          attn_logits_soft_cap=self.attn_logits_soft_cap,\n          rngs=self.rngs,\n      )\n\n  def mla_query_projection(self, inputs_q: Array, inputs_positions: Array, model_mode) -> Array:\n    \"\"\"Query projection for MLA, e.g. includes LoRA if q_lora_rank > 0.\"\"\"\n    # Set softmax scaling.\n    self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n    self.softmax_scale = self.qk_head_dim**-0.5\n    if self.max_position_embeddings > self.original_max_position_embeddings:\n      mscale = 0.1 * self.mscale * math.log(self.rope_factor) + 1.0\n      self.softmax_scale = self.softmax_scale * mscale * mscale\n\n    if self.q_lora_rank == 0:\n      q = self.query(inputs_q)\n    else:\n      # LoRA path\n      low_rank_q = self.wq_a(inputs_q)  # [B, L, q_lora_rank]\n      low_rank_q = self.q_norm(low_rank_q)  # RMSNorm on low rank\n      q = self.wq_b(low_rank_q)  # [B, L, n_heads * qk_head_dim]\n\n    # Split into non-positional and rotary parts.\n    q_nope, q_pe = jnp.split(q, [self.qk_nope_head_dim], axis=-1)\n    q_pe = self.apply_rotary_embedding(q_pe, inputs_positions=inputs_positions)\n    # Query projection is scaled by self.softmax_scale to be consistent MaxText implementation.\n    # DeepSeek v3 was doing it in attention score computation.\n    query = jnp.concatenate([q_nope, q_pe], axis=-1) * self.softmax_scale\n\n    if model_mode == MODEL_MODE_PREFILL:\n      query = nn.with_logical_constraint(query, self.prefill_query_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      query = nn.with_logical_constraint(query, self.ep_query_axis_names)\n    else:\n      query = nn.with_logical_constraint(query, self.query_axis_names)\n    return query\n\n  def mla_get_key_value(self, low_rank_main, key_rope, model_mode):\n    \"\"\"get (key,value) pair from mla\"\"\"\n    kv_out = self.wkv_b(low_rank_main)\n\n    # Split kv_out into key_nope and value parts.\n    key_nope, value = jnp.split(kv_out, [self.qk_nope_head_dim], axis=-1)\n    key_rope = jnp.broadcast_to(key_rope, (key_nope.shape[0], key_nope.shape[1], self.num_query_heads, key_rope.shape[3]))\n\n    key = jnp.concatenate([key_nope, key_rope], axis=-1)\n\n    if model_mode == MODEL_MODE_PREFILL:\n      key = nn.with_logical_constraint(key, self.prefill_key_axis_names)\n      value = nn.with_logical_constraint(value, self.prefill_value_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      key = nn.with_logical_constraint(key, self.ep_key_axis_names)\n      value = nn.with_logical_constraint(value, self.ep_value_axis_names)\n    else:\n      key = nn.with_logical_constraint(key, self.key_axis_names)\n      value = nn.with_logical_constraint(value, self.value_axis_names)\n    return key, value\n\n  def init_mla_kv_caches(self, inputs_kv_shape: Tuple):\n    \"\"\"Initializes MlaKVCache.\n\n    Args:\n      inputs_kv_shape: Key/value inputs shape for initialization.\n\n    Returns:\n      An MlaKVCache module instance.\n\n    Raises:\n      ValueError: If the configuration is invalid.\n\n    \"\"\"\n    batch_size, _, _ = inputs_kv_shape\n    # During initialization, seq_len of inputs_kv is max_target_length,\n    # which is not always correct for some functions in MlaKVCache.\n    # However, MlaKVCache internal cache shapes are based on max_prefill_length\n    # and max_target_length, not the passed seq_len.\n    # We can use a placeholder value. The correct fix might involve refactoring\n    # MlaKVCache.\n    placeholder_seq_len = 1\n\n    return kvcache.MlaKVCache(\n        max_prefill_length=self.max_prefill_predict_length,\n        max_target_length=self.max_target_length,\n        batch=batch_size,\n        key_seq_len=placeholder_seq_len,\n        value_seq_len=placeholder_seq_len,\n        key_head_size=self.kv_lora_rank,\n        value_head_size=self.qk_rope_head_dim,\n        dtype=self.dtype,\n        kv_quant=self.kv_quant,\n        prefill_cache_axis_order=self.prefill_cache_axis_order,\n        ar_cache_axis_order=self.ar_cache_axis_order,\n        model_mode=self.model_mode,\n        use_chunked_prefill=self.config.use_chunked_prefill,\n        rngs=self.rngs,\n    )\n\n  def update_mla_kv_caches(self, low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk=None):\n    \"\"\"Updates the MLA (Multi-Head Latent Attention) KV caches.\n\n    This method is specific to the MLA attention mechanism. It calls the\n    `mla_kv_cache_as_linen` module to update and retrieve the caches, which\n    store latent representations (`low_rank_main`) and RoPE-applied keys\n    (`key_rope`). It then reconstructs the full key and value tensors from\n    the cached components.\n\n    Args:\n      low_rank_main: The main latent component of the key.\n      key_rope: The RoPE-applied component of the key.\n      decoder_segment_ids: Segment IDs for decoder masking.\n      model_mode: The operational mode ('train', 'prefill', 'autoregressive').\n      previous_chunk: Information about previously processed chunks, for\n        chunked prefill.\n\n    Returns:\n      A list containing two elements:\n      - The prefill key-value cache, reconstructed from the MLA cache, or None.\n      - The autoregressive key-value cache, reconstructed from the MLA cache, or None.\n    \"\"\"\n\n    prefill_mla_cache, ar_mla_cache = self.MlaKVCache_0(\n        key_latent=low_rank_main,\n        key_rope=key_rope,\n        decoder_segment_ids=decoder_segment_ids,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n    )\n\n    if prefill_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids = prefill_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      prefill_kv_cache = key, value, decoder_segment_ids\n    else:\n      prefill_kv_cache = None\n\n    if ar_mla_cache:\n      low_rank_main, key_rope, decoder_segment_ids, lengths = ar_mla_cache\n      key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n      ar_kv_cache = key, value, decoder_segment_ids, lengths\n    else:\n      ar_kv_cache = None\n    return [prefill_kv_cache, ar_kv_cache]\n\n  def mla_kv_projection(self, inputs: Array, inputs_positions: Array, decoder_segment_ids, model_mode, previous_chunk):\n    \"\"\"MLA key/value projection with integrated rotary embedding.\"\"\"\n    low_rank = self.wkv_a(inputs)\n    low_rank_main, low_rank_rope = jnp.split(low_rank, [self.kv_lora_rank], axis=-1)\n    low_rank_main = self.kv_norm(low_rank_main)\n\n    # Apply rotary embedding to key_rope.\n    key_rope = jnp.expand_dims(low_rank_rope, axis=2)\n    key_rope = self.apply_rotary_embedding(key_rope, inputs_positions=inputs_positions)\n\n    key, value = self.mla_get_key_value(low_rank_main, key_rope, model_mode)\n    cached_values = [None, None]\n    if self.config.attention != \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      if self.config.mla_naive_kvcache:\n        cached_values = self.update_kv_caches(key, value, decoder_segment_ids, model_mode, previous_chunk)\n      else:\n        cached_values = self.update_mla_kv_caches(low_rank_main, key_rope, decoder_segment_ids, model_mode, previous_chunk)\n\n    return key, value, cached_values\n\n  def __call__(\n      self,\n      inputs_q: Array,\n      inputs_kv: Array,\n      inputs_positions: Array | None = None,\n      decoder_segment_ids: Array | None = None,\n      *,\n      model_mode: str = MODEL_MODE_TRAIN,\n      deterministic: bool = False,\n      previous_chunk: Any = None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n      bidirectional_mask: Optional[Any] = None,\n  ) -> Array:\n    \"\"\"Forward pass for MLA, reusing `AttentionOp` for the actual attention.\n\n    Args:\n      inputs_q: Query input [batch, q_length, embed_dim].\n      inputs_kv: KV input   [batch, kv_length, embed_dim].\n      inputs_positions: Positions for rotary embeddings or similar.\n      decoder_segment_ids: Segment IDs for masking, if any.\n      model_mode: \"train\", \"prefill\", or \"autoregressive\".\n      deterministic: Disables dropout if set to True.\n      previous_chunk: Information about previously processed chunks for chunked prefill.\n      slot: The batch slot index for paged attention.\n      page_state: The current state of the paged attention manager.\n      bidirectional_mask: A mask for bidirectional attention, used in multimodal models.\n\n    Returns:\n      A tensor of shape [batch, length, embed_dim] containing the\n      MLA-attended outputs.\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.prefill_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.prefill_input_axis_names)\n    elif model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.ep_input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.ep_input_axis_names)\n    else:\n      inputs_q = nn.with_logical_constraint(inputs_q, self.input_axis_names)\n      inputs_kv = nn.with_logical_constraint(inputs_kv, self.input_axis_names)\n\n    query = self.mla_query_projection(inputs_q, inputs_positions, model_mode)\n    key, value, cached_values = self.mla_kv_projection(\n        inputs_kv, inputs_positions, decoder_segment_ids, model_mode, previous_chunk\n    )\n\n    query = checkpoint_name(query, \"query_proj\")\n    key = checkpoint_name(key, \"key_proj\")\n    value = checkpoint_name(value, \"value_proj\")\n\n    if self.config.attention == \"paged\" and model_mode != MODEL_MODE_TRAIN:\n      unnormalized_out, _, exp_sum = self.ds_paged_attention_op(\n          query, key, value, decoder_segment_ids, model_mode, previous_chunk, slot=slot, page_state=page_state\n      )\n      unnormalized_out = unnormalized_out[..., : self.v_head_dim]\n      out = unnormalized_out / (exp_sum + 1e-9) if exp_sum is not None else unnormalized_out\n    else:\n      out = self.attention_op(query, key, value, decoder_segment_ids, model_mode, cached_values)\n\n    if model_mode == MODEL_MODE_TRAIN and self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      out = nn.with_logical_constraint(out, self.ep_out_axis_names)\n    else:\n      out = nn.with_logical_constraint(out, self.out_axis_names)\n\n    out = self.out_projection(out)\n    return out",
        "analysis": {
            "module_type": "multi_head_latent_attention",
            "purpose": "Implements the Multi-Head Latent Attention (MLA) mechanism, a specialized attention layer that uses LoRA-like projections and splits query/key heads into positional (RoPE) and non-positional (NoPe) components.",
            "input": {
                "shape": "[batch_size, sequence_length, embed_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Apply logical constraints to input tensors based on the model's operational mode (e.g., train, prefill).",
                "Compute query projection using `mla_query_projection`, which includes optional LoRA, splitting for NoPe/RoPE, and applying rotary embeddings.",
                "Compute key and value projections using `mla_kv_projection`, which also handles updating the specialized MLA KV cache with latent representations.",
                "Apply JAX checkpointing to the projected query, key, and value tensors.",
                "Compute the attention output using either a paged attention kernel (`ds_paged_attention_op`) or a standard attention kernel (`attention_op`).",
                "Apply the final linear output projection to the attention result."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, embed_dim]"
            },
            "dependencies": [
                "maxtext.layers.attentions.Attention",
                "maxtext.layers.linears.DenseGeneral",
                "maxtext.layers.normalizations.RMSNorm",
                "maxtext.inference.kvcache.MlaKVCache",
                "maxtext.inference.paged_attention.PagedAttentionOp"
            ],
            "parameters": {
                "q_lora_rank": "The rank of the LoRA-like projection for the query. If 0, a standard dense projection is used.",
                "kv_lora_rank": "The rank of the LoRA-like projection for the key and value.",
                "qk_nope_head_dim": "The dimension of the non-positional (NoPe) part of the query and key heads.",
                "qk_rope_head_dim": "The dimension of the rotary positional embedding (RoPE) part of the query and key heads.",
                "v_head_dim": "The dimension of the value heads.",
                "attention_kernel": "The name of the kernel used to compute the attention scores (e.g., 'dot_product')."
            },
            "notes": [
                "This class inherits from the base `Attention` class and overrides its projection and caching logic.",
                "MLA uses a unique projection scheme where keys and values are derived from a shared low-rank latent representation.",
                "For inference, it can use a specialized `MlaKVCache` that stores these latent representations instead of the full key/value tensors.",
                "The query and key are split into two parts: one that receives rotary positional embeddings (RoPE) and one that does not (NoPe)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLA module, setting up parameters, calling the superclass constructor, and initializing the `MlaKVCache` for inference modes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set MLA-specific parameters like LoRA ranks and head dimensions.",
                        "Calculate the total query/key head dimension (`qk_head_dim`).",
                        "Call the `super().__init__` method to initialize the base `Attention` class.",
                        "Initialize `MlaKVCache_0` if `model_mode` is not 'train'."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Attention",
                        "kvcache.MlaKVCache"
                    ],
                    "notes": [
                        "Some instance attributes are set before calling `super().__init__` as they are used during the parent class's initialization."
                    ]
                },
                "_init_projections": {
                    "purpose": "Initializes the MLA-specific weight matrices (projections) for query, key, and value, including LoRA layers, normalization layers, and the output projection.",
                    "input": {
                        "shape": "inputs_q_shape: Tuple, inputs_kv_shape: Tuple",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert that the configuration is valid for MLA.",
                        "Initialize query projection, either as a standard `DenseGeneral` or a LoRA path (`wq_a`, `q_norm`, `wq_b`).",
                        "Initialize key/value projection using a LoRA path (`wkv_a`, `kv_norm`, `wkv_b`).",
                        "Calculate and set the `softmax_scale` for attention score scaling.",
                        "Initialize the output projection layer.",
                        "Initialize the paged attention operator if configured."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "RMSNorm",
                        "paged_attention.PagedAttentionOp"
                    ],
                    "notes": [
                        "This method overrides the projection initialization from the base `Attention` class to implement the specific MLA architecture."
                    ]
                },
                "mla_query_projection": {
                    "purpose": "Computes the query projection for MLA, including an optional LoRA path, splitting for NoPe/RoPE, applying rotary embeddings, and scaling.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, embed_dim], inputs_positions: [batch, q_length]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Calculate the softmax scaling factor.",
                        "Project `inputs_q` using either a standard dense layer or a LoRA path.",
                        "Split the projection into non-positional (NoPe) and positional (RoPE) parts.",
                        "Apply rotary embeddings to the RoPE part.",
                        "Concatenate the NoPe and RoPE parts.",
                        "Scale the final query tensor by `softmax_scale`.",
                        "Apply logical constraints for sharding."
                    ],
                    "output": {
                        "shape": "[batch, q_length, num_query_heads, qk_head_dim]"
                    },
                    "dependencies": [
                        "jnp.split",
                        "jnp.concatenate",
                        "self.apply_rotary_embedding"
                    ],
                    "notes": [
                        "The softmax scaling is applied directly to the query projection, which differs from some standard attention implementations."
                    ]
                },
                "mla_get_key_value": {
                    "purpose": "Reconstructs the full key and value tensors from the latent components (`low_rank_main` and `key_rope`).",
                    "input": {
                        "shape": "low_rank_main: [B, L, num_heads, kv_lora_rank], key_rope: [B, L, 1, qk_rope_head_dim]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Project `low_rank_main` using the second KV LoRA weight (`wkv_b`).",
                        "Split the result into the non-positional key part (`key_nope`) and the full `value`.",
                        "Broadcast `key_rope` to match the shape of `key_nope`.",
                        "Concatenate `key_nope` and `key_rope` to form the final `key`.",
                        "Apply logical constraints for sharding."
                    ],
                    "output": {
                        "shape": "A tuple of (key, value) tensors."
                    },
                    "dependencies": [
                        "jnp.split",
                        "jnp.broadcast_to",
                        "jnp.concatenate"
                    ],
                    "notes": [
                        "This is a helper method central to the MLA architecture, separating the latent representation from the final key/value tensors."
                    ]
                },
                "init_mla_kv_caches": {
                    "purpose": "Initializes the specialized `MlaKVCache` module used for efficient inference.",
                    "input": {
                        "shape": "inputs_kv_shape: (batch_size, seq_len, embed_dim)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extract the batch size from the input shape.",
                        "Instantiate `kvcache.MlaKVCache` with the necessary configuration parameters."
                    ],
                    "output": {
                        "shape": "An MlaKVCache module instance."
                    },
                    "dependencies": [
                        "kvcache.MlaKVCache"
                    ],
                    "notes": [
                        "A placeholder sequence length is used during initialization, as the internal cache shapes are determined by other configuration parameters like `max_target_length`."
                    ]
                },
                "update_mla_kv_caches": {
                    "purpose": "Updates the MLA KV caches with new latent components and reconstructs the full key/value tensors from the cached history.",
                    "input": {
                        "shape": "Dynamic shapes based on prefill/autoregressive mode.",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Call the `MlaKVCache_0` module with the current latent components (`low_rank_main`, `key_rope`).",
                        "Receive updated latent components for both prefill and autoregressive caches.",
                        "If a prefill cache is returned, reconstruct the full prefill key and value using `mla_get_key_value`.",
                        "If an autoregressive cache is returned, reconstruct the full autoregressive key and value using `mla_get_key_value`.",
                        "Return the reconstructed caches."
                    ],
                    "output": {
                        "shape": "A list containing the reconstructed prefill and autoregressive KV caches, or None."
                    },
                    "dependencies": [
                        "self.MlaKVCache_0",
                        "self.mla_get_key_value"
                    ],
                    "notes": [
                        "This method abstracts the interaction with the specialized latent cache during inference."
                    ]
                },
                "mla_kv_projection": {
                    "purpose": "Computes the key and value projections for MLA, including applying rotary embeddings and updating the KV cache.",
                    "input": {
                        "shape": "inputs: [batch, kv_length, embed_dim], inputs_positions: [batch, kv_length]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Apply the first dense projection (`wkv_a`) to get a low-rank representation.",
                        "Split the result into `low_rank_main` and `low_rank_rope`.",
                        "Apply RMS normalization to `low_rank_main`.",
                        "Apply rotary embeddings to `low_rank_rope` to get `key_rope`.",
                        "Reconstruct the full `key` and `value` using `mla_get_key_value`.",
                        "If in an inference mode, update the KV cache using `update_mla_kv_caches`."
                    ],
                    "output": {
                        "shape": "A tuple containing (key, value, cached_values)."
                    },
                    "dependencies": [
                        "self.wkv_a",
                        "self.kv_norm",
                        "self.apply_rotary_embedding",
                        "self.mla_get_key_value",
                        "self.update_mla_kv_caches"
                    ],
                    "notes": [
                        "This method orchestrates the entire key/value generation pipeline for MLA."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the main forward pass for the MLA layer, orchestrating projection, caching, and attention computation.",
                    "input": {
                        "shape": "inputs_q: [batch, q_length, embed_dim], inputs_kv: [batch, kv_length, embed_dim]",
                        "dtype": "jnp.float32"
                    },
                    "processing_steps": [
                        "Apply logical constraints to inputs.",
                        "Compute query projection via `mla_query_projection`.",
                        "Compute key/value projections and update caches via `mla_kv_projection`.",
                        "Checkpoint the projected tensors.",
                        "Compute attention scores and output using either a paged attention or standard attention operator.",
                        "Apply logical constraints to the output.",
                        "Apply the final output projection."
                    ],
                    "output": {
                        "shape": "[batch, length, embed_dim]"
                    },
                    "dependencies": [
                        "self.mla_query_projection",
                        "self.mla_kv_projection",
                        "self.attention_op",
                        "self.ds_paged_attention_op",
                        "self.out_projection"
                    ],
                    "notes": [
                        "This method handles different execution paths for training, prefill, and autoregressive decoding, including support for paged attention."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/pipeline.py#Pipeline",
        "file_path": "src/MaxText/layers/pipeline.py",
        "code_block": "class Pipeline(nn.Module):\n  \"\"\"Module that implements pipelining across stages.\n\n  This module will loop over microbatches and execute the main body with a vmap for both the inputs and weights.\n  This will produce a pipeline pattern if the stage dimension is sharded.\n\n  Supports circular pipelines, and multiple layers per stage are used when a module that executes multiple layers\n  is passed as the layers input.\n\n  Attributes:\n    config: Importantly contains num_pipeline_microbatches, num_pipeline_repeats.\n    layers: A module instance that each stage can execute. It can either be a single layer such as a\n      LlamaDecoderLayer instance or scanned/looped set of decoder layers to execute multiple layers per stage.\n    mesh:  The device mesh of the system.\n    remat_policy: Remat policy to use for the loop iterations\n  \"\"\"\n\n  config: Config\n  layers: nn.Module  # The name of this property (layers) is reflected in the state pytree and thus also checkpoints.\n  mesh: Mesh\n  remat_policy: Any = None\n\n  def setup(self):\n    self.num_stages = self.config.ici_pipeline_parallelism * self.config.dcn_pipeline_parallelism\n    self.forwarding_delay = 2 if self.config.pipeline_delay_activation_forwarding else 1\n    self.pipeline_microbatch_size = self.config.micro_batch_size_to_train_on // self.config.num_pipeline_microbatches\n    microbatches_per_stage = self.config.num_pipeline_microbatches // self.num_stages\n    self.microbatches_per_stage = microbatches_per_stage\n    self.use_circ_storage = self.need_circ_storage()\n\n  def need_circ_storage(self):\n    return (\n        self.config.num_pipeline_repeats > 1\n        and self.config.num_pipeline_microbatches > self.num_stages * self.forwarding_delay\n    )\n\n  def iterations_to_complete_first_microbatch_one_repeat(self):\n    # Return the number of iterations it takes for microbatch 0 to finish a repeat\n    return self.forwarding_delay * (self.num_stages - 1)\n\n  def iterations_to_complete_first_microbatch(self):\n    # Return the number of iterations it takes for microbatch 0 to finish the last stage of the last repeat\n    return (\n        self.config.num_pipeline_microbatches * (self.config.num_pipeline_repeats - 1)\n        + self.iterations_to_complete_first_microbatch_one_repeat()\n    )\n\n  def init_states(self, inputs):\n    \"\"\"Initialize components of state: state_io, shift, circular_storage and circular_storage_mover\n    Assumes input has already been reshaped into microbatches: [num_micro_batches, micro_batch_size, sequence, embed]\n\n    Returns a dictionary with properties\n      shift: zeros shape [num_stages, micro_size, sequence, embed]\n      prev_outputs: same shape as shift, only used when pipeline_delay_activation_forwarding is set to true, else None\n      state_io: reshaped inputs [num_stages, microbatches/stages, micro_size, sequence, embed]\n      circ_storage: zeros [num_stages, microbatches, micro_size, sequence, embed] when needed, else None\n      circ_storage_mover: zeros[num_stages, micro_size, sequence, embed] when needed, else None\n      loop_iteration: scalar set initially to 0.\n    \"\"\"\n\n    # Shift is used to rotate the output of each pipeline into the input of the next\n    # shift has shape [num_stages, micro_size, sequence, embed]\n    shift = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n    shift = nn.with_logical_constraint(\n        shift,\n        (\"activation_stage\", \"activation_batch\", \"activation_length\", \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # Prev outputs has the same shape of the output (and shift)\n    if self.config.pipeline_delay_activation_forwarding:\n      prev_outputs = jnp.zeros((self.num_stages,) + inputs.shape[1:], dtype=inputs.dtype)\n      prev_outputs = nn.with_logical_constraint(\n          prev_outputs,\n          (\"activation_stage\", \"activation_batch\", \"activation_length\", \"activation_embed\"),\n          rules=self.config.logical_axis_rules,\n          mesh=self.mesh,\n      )\n    else:\n      prev_outputs = None\n\n    # state_io (state input output) at first holds all of the input batches, but also will hold the outputs\n    #   as the pipeline runs/finishes\n    # state_io has shape [num_stages, microbatches/stages, micro_size, sequence, embed]\n    state_io = jnp.reshape(inputs, (self.num_stages, self.microbatches_per_stage) + inputs.shape[1:])\n    # We shard the pipeline_microbatch_size axis by data/fsdp, not num_microbatches since those are looped over.\n    state_io = nn.with_logical_constraint(\n        state_io,\n        (\"activation_stage\", None, \"activation_batch\", \"activation_length\", \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n\n    # circ_storage is used to hold the final pipeline stage outputs before it is used for the next repeat. It is only\n    # needed when num_microbatches > num_stages, else instead the final stage will immediately pass to the first without\n    # additional storage.\n    # circ_storage has shape [num_stages, microbatches, micro_size, sequence, embed].\n    # Note that this shape is a factor of num_stages larger than necessary - each stage holds the global batch, but only\n    # stage 0 holds the real activations (since it will use them), the rest hold dummy ones. This amount of storage\n    # [global_batch, sequence, embed] is fine as long as there is some amount of additional sharding axes, e.g. FSDP,\n    # TP, DP (e.g. there are many devices that shard stage 0)\n    # We may look into alternatives using less storage if this becomes an issue (ideas in b/347603101).\n    if self.use_circ_storage:\n      circ_storage = jnp.zeros((self.num_stages,) + inputs.shape, dtype=inputs.dtype)\n    else:\n      circ_storage = None\n\n    # circ_storage_mover is used to push the microbatches from the pipeline into circ_storage with one buffer iteration\n    # of delay circ_storage_mover shape is same as shift: [num_stages, micro_size, sequence, embed]\n    if self.use_circ_storage:\n      circ_storage_mover = shift\n    else:\n      circ_storage_mover = None\n\n    init_loop_state = {\n        \"state_io\": state_io,\n        \"shift\": shift,\n        \"circ_storage\": circ_storage,\n        \"circ_storage_mover\": circ_storage_mover,\n        \"loop_iteration\": 0,\n        \"prev_outputs\": prev_outputs,\n    }\n    return init_loop_state\n\n  def get_iteration_inputs(self, loop_iteration, state_io, circ_storage, shift):\n    \"\"\"\n    Construct stages_in: the global array that is operated on for this iteration, shape same as\n    shift=[stages, micro_size, sequence, embed]\n    This is almost a rotated version of the last outputs, except for the first stage which must grab a new batch from\n    state_io or an old one from circ_storage\n    \"\"\"\n\n    # Setup potential input from state_io, which has a rotating microbatch index (size of microbatches_per_stage)\n    state_io_batch_idx = loop_iteration % self.microbatches_per_stage\n    state_io_slice = state_io[:, state_io_batch_idx]\n\n    if self.use_circ_storage:\n      # Setup potential input from circ_storage, which also has a rotating index for microbatch,\n      # size of num_microbatches\n      circ_storage_batch_idx = loop_iteration % self.config.num_pipeline_microbatches\n      circular_stage_in = circ_storage[:, circ_storage_batch_idx]\n    else:\n      # The last stage immediately flows into the first stage, use this rotated shift instead of circular storage\n      circular_stage_in = shift\n\n    # For early loop iterations we grab a new input for stage 0 from the state_io. Once each microbatch has left\n    # state_io we instead grab from the last stage's output (possibly buffered when num_microbatches > num_stages, e.g.\n    # from circ_storage).\n    first_stage_in = jnp.where(loop_iteration < self.config.num_pipeline_microbatches, state_io_slice, circular_stage_in)\n\n    # Note that first_stage_in may correspond to bubble computation during the last few iterations.\n    # However, these bubble computation results remain in the shift buffer (do not make it back to state_io) and are\n    # thus discarded / not returned.\n    # The final returned output is stored in the state_io, which has the appropriate total size of num_microbatches. The\n    # state_io will not contain bubble results at the end of the last iteration.\n\n    def select_state_or_input(first_stage_in, shift):\n      # Selects input for stage 0, shift for other stages\n      return jnp.where(jax.lax.broadcasted_iota(\"int32\", shift.shape, 0) == 0, first_stage_in, shift)\n\n    # Selects input (from stream_io) for stage 0, other stages get from shift (the rotated previous output)\n    stages_in = select_state_or_input(first_stage_in, shift)\n    stages_in = nn.with_logical_constraint(\n        stages_in,\n        (\"activation_stage\", \"activation_batch\", \"activation_length\", \"activation_embed\"),\n        rules=self.config.logical_axis_rules,\n        mesh=self.mesh,\n    )\n    return stages_in\n\n  def shard_dim_by_stages(self, x, dim: int):\n    # Shards a dimension by stages. Currently, the sharding of other dimensions are left up the compiler, alternatively\n    # we may want to copy over the sharding from the other input axes.\n    dims_mapping = [jax.sharding.PartitionSpec.UNCONSTRAINED] * x.ndim\n    dims_mapping[dim] = \"stage\"\n    dims_mapping = tuple(dims_mapping)\n    sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(*dims_mapping))\n    return jax.lax.with_sharding_constraint(x, sharding)\n\n  def get_microbatch_and_repeat_ids(self, loop_iteration):\n    \"\"\"Gets the microbatch_ids and repeat_ids for all stages on this loop_iteration. Works for both circular and\n    non-circular\"\"\"\n    # Stage 0 has processed one microbatch every loop_iter, but Stage 1 is 1 behind due to bubble, etc for other stages\n    microbatches_processed = jnp.maximum(loop_iteration - self.forwarding_delay * jnp.arange(self.num_stages), 0)\n    microbatch_ids = microbatches_processed % self.config.num_pipeline_microbatches\n    repeat_ids = microbatches_processed // self.config.num_pipeline_microbatches\n    return microbatch_ids, repeat_ids\n\n  def vmap_parallel_gather(self, weights, repeat_ids, repeat_dim_in_weights, stages_dim_in_weights):\n    \"\"\"Use vmap to implement a sharded parallel gather.\n    Parallel gather means each stage has its own weights, and gets one slice from it.\n    Args:\n      weights: Per-stage data to be gathered from.\n      repeat_ids: Integer tensor of shape [num_stages], the repeats of the stages.\n      repeat_dim_in_weights: The dimension in weights where repeat_ids are applied. The output will not\n        have this dimension.\n      stages_dim_in_weights: The dimension in weights that represents parallel stages.\n    Returns:\n      The per-stage gathered values. The shape is weights.shape but with repeat_dim_in_weights\n        removed.\n    \"\"\"\n\n    def _gather_one(x, repeat_id):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, repeat_id, 1, repeat_dim_in_weights), repeat_dim_in_weights)\n\n    gathered_weights_stage_dim = 0\n    repeat_ids = self.shard_dim_by_stages(repeat_ids, 0)\n    weights = self.shard_dim_by_stages(weights, stages_dim_in_weights)\n    stage_weights = jax.vmap(_gather_one, in_axes=(stages_dim_in_weights, 0), out_axes=gathered_weights_stage_dim)(\n        weights, repeat_ids\n    )\n    stage_weights = self.shard_dim_by_stages(stage_weights, gathered_weights_stage_dim)\n    return stage_weights\n\n  def vmap_gather(self, xs, ids, ids_dim):\n    \"\"\"Use vmap to implement a stage-wise sharded gather.\n\n    The stages share the same input, but they have different offsets.\n\n    Args:\n      xs: Data shared by all stages, to be gathered from.\n      ids: Integer tensor of shape [num_stages], the offsets of the stages.\n      ids_dim: The dimension in xs where ids are applied. In the output, this\n        dimension will be [num_stages], since each stage gets one slice.\n\n    Returns:\n      The per-stage gathered values. The shape is xs.shape but with ids_dim size\n        replaced with [num_stages].\n    \"\"\"\n\n    def _gather_one(x, i):\n      return jnp.squeeze(jax.lax.dynamic_slice_in_dim(x, i, 1, ids_dim), ids_dim)\n\n    ids = self.shard_dim_by_stages(ids, 0)\n    outs = jax.vmap(_gather_one, in_axes=(None, 0), out_axes=ids_dim)(xs, ids)\n    return self.shard_dim_by_stages(outs, 0)\n\n  def get_new_loop_state(self, output, loop_state):\n    \"\"\"\n    Update the various buffers given the output of the most recent iteration\n    * state_io: rotates left/up by 1 (the whole created in the last slot is filled with the most recent pipeline output)\n       * Pushing inputs up from top of state_io into first stage of shift\n       * Pulling outputs up from last stage of shift into bottom of state_io\n    * shift: rotate output (or prev_outputs if using delay) right/down by 1 - we imagine the pipeline moves to\n               right/down\n    * circ_storage: pushes circ_storage_mover (the output of the previous iteration) into rotating index of circ_storage\n    * circ_storage_mover: assigned to rotated output and pushed into circ_storage on the next iteration\n    * prev_outputs: is set to the current output\n    \"\"\"\n\n    old_state_io = loop_state[\"state_io\"]\n    old_circ_storage = loop_state[\"circ_storage\"]\n    old_circ_storage_mover = loop_state[\"circ_storage_mover\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n    old_prev_outputs = loop_state[\"prev_outputs\"]\n\n    def _rotate_right(arr):\n      # Use lax.slice to avoid generating a gather.\n      last = jax.lax.slice_in_dim(arr, self.num_stages - 1, self.num_stages, axis=0)\n      except_last = jax.lax.slice_in_dim(arr, 0, self.num_stages - 1, axis=0)\n      return jnp.concatenate([last, except_last], axis=0)\n\n    def _shift_right(arr):\n      padding = [[1, 0]] + [[0, 0]] * (arr.ndim - 1)\n      # Use lax.slice to guarantee the gradient is a pad.\n      return jax.lax.slice(jnp.pad(arr, padding), [0] * arr.ndim, arr.shape)\n\n    # Shift either rotates or shifts depending on if the last stage immediately must send to first or not\n    # For non-circular pipelines, the last stage does not need to send to first\n    # For circular pipelines with #micro = #stages, last stage immediately sends to first\n    # For circular pipelines with #micro > stages (circ_storage), last stage sends to circ storage\n    def _update_shift(output_in):\n      if self.config.num_pipeline_repeats == 1 or self.use_circ_storage:\n        return _shift_right(output_in)  # last stage does not have to send to first immediately\n      else:\n        return _rotate_right(output_in)  # last stage must immediately send to first\n\n    if self.config.pipeline_delay_activation_forwarding:\n      new_shift = _update_shift(old_prev_outputs)\n      new_prev_outputs = output\n    else:\n      new_shift = _update_shift(output)\n      new_prev_outputs = None\n\n    if self.use_circ_storage:\n      # Insert the circ_storage_mover into new_circ_storage at a microbatch-rotating index.\n      # circ_storage_mover still points to the output of PREVIOUS iteration, which should aid in allowing overlapped\n      # compute/async transfers\n      def _rotate_right_and_update(circ_storage_mover_in, circ_storage_in):\n        rotated = _rotate_right(circ_storage_mover_in)\n        rotated = jnp.expand_dims(rotated, 1)\n        # We rotate the pushing index into circ storage, and ensure that microbatch 0 lands in index 0\n        offset = (\n            loop_iteration - self.iterations_to_complete_first_microbatch_one_repeat() - 1\n        ) % self.config.num_pipeline_microbatches  # Note extra -1 b/c grabbing from the\n        # previous output - using circ_storage_mover before it is updated\n        return jax.lax.dynamic_update_slice_in_dim(circ_storage_in, rotated, offset, axis=1)\n\n      new_circ_storage = _rotate_right_and_update(old_circ_storage_mover, old_circ_storage)\n      new_circ_storage_mover = output\n    else:\n      new_circ_storage = None\n      new_circ_storage_mover = None\n\n    # Rotate stream_io left/up by 1 on rotating micro/stage index (stream_buf_idx), replacing the last/bottom with the\n    # last stage output\n    stream_buf_idx = loop_iteration % self.microbatches_per_stage\n    stream_slice = old_state_io[:, stream_buf_idx]\n\n    def _update_state_io(state_in, stream_slice, output):\n      # Shift the current slice to the left, then fill the last stage with the final output.\n      padding = [[0, 1]] + [[0, 0]] * (stream_slice.ndim - 1)\n      stream_slice = jax.lax.slice_in_dim(jnp.pad(stream_slice, padding), 1, stream_slice.shape[0] + 1, axis=0)\n      stream_slice = jnp.where(\n          jax.lax.broadcasted_iota(\"int32\", stream_slice.shape, 0) == self.num_stages - 1, output, stream_slice\n      )\n      stream_slice = jnp.expand_dims(stream_slice, 1)\n      return jax.lax.dynamic_update_slice_in_dim(state_in, stream_slice, stream_buf_idx, axis=1)\n\n    new_state = _update_state_io(old_state_io, stream_slice, output)\n\n    new_loop_state = {\n        \"state_io\": new_state,\n        \"shift\": new_shift,\n        \"circ_storage\": new_circ_storage,\n        \"circ_storage_mover\": new_circ_storage_mover,\n        \"loop_iteration\": loop_iteration + 1,\n        \"prev_outputs\": new_prev_outputs,\n    }\n    return new_loop_state\n\n  def permute_output_micro_per_stage_dim(self, output):\n    # The first real output (microbatch 0) takes a certain amount of loop iterations to finish and be pushed to\n    # state_io - it will land on a different index of state_io depending on the number of iterations.\n    microbatch_0_idx = self.iterations_to_complete_first_microbatch() % self.microbatches_per_stage\n    permutation = (\n        np.arange(self.microbatches_per_stage) + microbatch_0_idx\n    ) % self.microbatches_per_stage  # permute so the value in land_idx is moved into idx 0, and (land_idx + 1) appear\n    # in idx 1, etc\n    output = output[:, permutation]\n    return output\n\n  def get_current_stage_weights(self, pipeline_weights, loop_iteration):\n    \"\"\"\n    Gets the current weights used for one iteration. Outputs a pytree whose arrays have leading dimension of stages, e.g.\n    {'mlp': 'wo': [stages, mlp, embed]}. Stage 0 will use the 0th index of this pytree, Stage 1 the 1st index, etc.\n    For non-circular pipelines, this simply returns all weights - every weight is used in every iteraiton. However\n    for circular pipelines each stage grabs only the weights corresponding to the current repeat.\n    \"\"\"\n    if self.config.num_pipeline_repeats > 1:\n      return self.get_current_repeat_from_stages(pipeline_weights, loop_iteration)\n    else:\n      return pipeline_weights\n\n  def get_current_repeat_from_stages(self, weights, loop_iteration):\n    \"\"\"get current repeat from stages\"\"\"\n    _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    def gather_weights_for_stages_in(weights):\n      return jax.tree.map(\n          functools.partial(\n              self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n          ),\n          weights,\n      )\n\n    circular_metadata_params = {\n        nn.PARTITION_NAME: \"circular_repeats\",\n        \"sub_weight_split_dims_mapping\": (None,),\n        \"is_initializing\": self.is_initializing(),\n        \"x_times\": self.config.num_pipeline_repeats,\n        \"optimizer_dims_mapping\": None,\n    }\n    weights = meta.remove_axis(\n        weights, 0, circular_metadata_params\n    )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one circular\n    # entry per stage.\n    weights = gather_weights_for_stages_in(weights)\n    return weights\n\n  def get_vmap_func_for_init(self):\n    \"\"\"This vmap func is used to initialize the weights only on init.\"\"\"\n\n    def func_to_vmap(body_instance, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      return body_instance(stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0, \"_overwrite_with_gradient\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def get_main_vmap_func_for_iterations(self):\n    \"\"\"\n    Returns main stage function vmapped by number of stages.\n    This becomes a vmap over a single layer instance if body_instance is a single layer,\n    else a set of layers if body_instance is a set of layers.\n    \"\"\"\n\n    def func_to_vmap(\n        body_instance, weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode\n    ):\n      \"\"\"nn.vmap requires either a nn.module class or a function whose first argument is a nn.module instance.\"\"\"\n      weights = meta.remove_axis(\n          weights, 0, {\n              nn.PARTITION_NAME: \"layers\",\n              \"sub_weight_split_dims_mapping\": (None,),\n              \"is_initializing\": self.is_initializing(),\n              \"x_times\": self.num_stages,\n          }\n      )\n      return body_instance.apply(weights, stages_inputs, stages_segment_ids, stages_positions, deterministic, model_mode)\n\n    vmap_func = nn.vmap(\n        func_to_vmap,\n        in_axes=(0, 0, 0, 0, None, None),\n        spmd_axis_name=\"stage\",\n        variable_axes={\"params\": 0},\n        split_rngs={\"params\": self.is_initializing(), \"dropout\": self.config.enable_dropout},\n        metadata_params={\n            nn.PARTITION_NAME: \"layers\",\n            \"sub_weight_split_dims_mapping\": (None),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.num_stages,\n        },\n    )\n    return vmap_func\n\n  def run_one_iteration(\n      self, loop_state, pipeline_weights, positions, segment_ids, deterministic, model_mode, decoder_layer_instance\n  ):\n    \"\"\"Run one loop iteration - gets weights and inputs for each stage, run the stages in parallel,\n    and update the loop state.\"\"\"\n    state_io = loop_state[\"state_io\"]\n    shift = loop_state[\"shift\"]\n    circ_storage = loop_state[\"circ_storage\"]\n    loop_iteration = loop_state[\"loop_iteration\"]\n\n    microbatch_ids, _ = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n    stages_inputs = self.get_iteration_inputs(loop_iteration, state_io, circ_storage, shift)\n    # We checkpoint stages_inputs since we are grabbing only one slice of the state_io, don't need to save the entire\n    # buffer.\n    stages_inputs = jax.ad_checkpoint.checkpoint_name(stages_inputs, \"iteration_input\")\n    stages_positions = self.vmap_gather(positions, microbatch_ids, 0) if positions is not None else None\n    stages_segment_ids = self.vmap_gather(segment_ids, microbatch_ids, 0) if segment_ids is not None else None\n\n    vmap_func = self.get_main_vmap_func_for_iterations()\n\n    if self.config.num_pipeline_repeats > 1:\n      _, repeat_ids = self.get_microbatch_and_repeat_ids(loop_iteration)\n\n      def prepare_vars_for_main_vmap(weights):\n        def gather_weights_for_stages_in(weights):\n          return jax.tree.map(\n              functools.partial(\n                  self.vmap_parallel_gather, repeat_ids=repeat_ids, repeat_dim_in_weights=0, stages_dim_in_weights=1\n              ),\n              weights,\n          )\n\n        circular_metadata_params = {\n            nn.PARTITION_NAME: \"circular_repeats\",\n            \"sub_weight_split_dims_mapping\": (None,),\n            \"is_initializing\": self.is_initializing(),\n            \"x_times\": self.config.num_pipeline_repeats,\n            \"optimizer_dims_mapping\": None,\n        }\n        weights = meta.remove_axis(\n            weights, 0, circular_metadata_params\n        )  # Remove the circular metadata axis, this axis will be removed when passed to the main vmap, only one\n        # circular entry per stage.\n        weights = gather_weights_for_stages_in(weights)\n        return weights\n\n      vmap_func = nn.map_variables(\n          vmap_func,\n          mapped_collections=[\"params\", \"_overwrite_with_gradient\", \"non_trainable\", \"summaries\", \"intermediates\"],\n          mutable=True,\n          trans_in_fn=prepare_vars_for_main_vmap,\n      )\n\n    stage_weights = self.get_current_stage_weights(pipeline_weights, loop_iteration)\n    stages_output = vmap_func(\n        decoder_layer_instance,\n        stage_weights,\n        stages_inputs,\n        stages_segment_ids,\n        stages_positions,\n        deterministic,\n        model_mode,\n    )\n    if self.config.scan_layers:\n      stages_output = stages_output[0]\n\n    new_state = self.get_new_loop_state(stages_output, loop_state)\n    return new_state\n\n  def get_pipeline_remat_policy(self):\n    \"\"\"Returns the pipeline remat policy for this pipeline.\"\"\"\n    # We ensure that the decoder layer inputs are saved, although we leave it to a custom\n    # policy if they should be saved to device or offloaded.\n    if self.config.remat_policy == \"custom\":\n      return self.remat_policy\n\n    save_input_policy = jax.checkpoint_policies.save_only_these_names(\"iteration_input\", \"decoder_layer_input\")\n    if self.remat_policy is not None:\n      remat_policy = jax.checkpoint_policies.save_from_both_policies(self.remat_policy, save_input_policy)\n    else:\n      remat_policy = save_input_policy\n    return remat_policy\n\n  def get_weight_sharding(self, *init_args):\n    \"\"\"get weight sharding function for this pipeline.\"\"\"\n    # Returns a partition spec of all weights. Requires passing in arguments to init.\n    key = jax.random.PRNGKey(0)\n    keys = {\"params\": key, \"dropout\": key, \"aqt\": key}\n    weights = self.init(keys, *init_args)\n\n    def get_partition_spec(pytree):\n      def _is_leaf(x):\n        return isinstance(x, nn.spmd.LogicallyPartitioned)\n\n      def get_partition_spec_leaf(leaf):\n        return leaf.get_partition_spec()\n\n      partition_spec_tree = jax.tree.map(get_partition_spec_leaf, pytree, is_leaf=_is_leaf)\n      return partition_spec_tree\n\n    partition_spec_with_extra_layer = get_partition_spec(weights)\n    partition_spec = {\"params\": partition_spec_with_extra_layer[\"params\"][\"layers\"]}\n    return partition_spec\n\n  def get_physical_spec_no_fsdp(self, full_logical):\n    \"\"\"\n    Get physical spec without fsdp.\n\n    TODO: Remove the expert sharding on attention weights as well, since those act like fsdp.\n\n    Args:\n      full_logical: original logical partition specs of all weights\n\n    Returns:\n      Modified physical spec with \"fsdp\" and \"fsdp_transpose\" removed\n    \"\"\"\n\n    def remove_fsdp_sharding(sharding_tree):\n      def _remove_fsdp_from_partition_spec(named_sharding):\n        if isinstance(named_sharding, jax.sharding.NamedSharding):\n          new_spec = []\n          for axis in named_sharding.spec:\n            if axis is None:\n              new_spec.append(None)\n            elif isinstance(axis, str):\n              if axis not in (\"fsdp\", \"fsdp_transpose\"):\n                new_spec.append(axis)\n              else:\n                new_spec.append(None)\n            elif isinstance(axis, (list, tuple)):\n              new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n              new_spec.append(tuple(new_axis))\n            else:\n              raise ValueError(f\"Unsupported axis type: {type(axis)}\")\n          return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n        return named_sharding\n\n      return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n    physical = nn.logical_to_mesh_sharding(full_logical, mesh=self.mesh, rules=self.config.logical_axis_rules)\n    physical_no_fsdp = remove_fsdp_sharding(physical)\n    return physical_no_fsdp\n\n  def all_gather_over_fsdp(self, sharding_info):\n    physical_constraint_no_fsdp = self.get_physical_spec_no_fsdp(sharding_info)\n    return jax.lax.with_sharding_constraint(self.layers.variables, physical_constraint_no_fsdp)\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs: jnp.ndarray,\n      segment_ids: jnp.ndarray,\n      positions: jnp.ndarray,\n      deterministic: bool,\n      model_mode=MODEL_MODE_TRAIN,\n      partition_spec=None,  # Pytree of sharding specifications of the weights (aka self.layers.variables)\n  ) -> jnp.ndarray:\n    \"\"\"The main method that maps the series of decoder layer inputs to final layer outputs.\n    Has the same signature of a single decoder layer, and expects the same shapes, e.g. the inputs should have shape\n    [global_batch], and internally this will be reshapped into microbatches.\n    \"\"\"\n    # Reshape inputs of [global_batch, ...] to [microbatches, pipeline_microbatch_sizes, ...]\n    inputs = inputs.reshape(\n        (\n            self.config.num_pipeline_microbatches,\n            self.pipeline_microbatch_size,\n            self.config.max_target_length,\n            self.config.emb_dim,\n        )\n    )\n    example_inputs = jax.lax.broadcast(inputs[0], [self.num_stages])  # dummy inputs fed to initialize the module\n    # weights.\n    ag_sharding = jax.sharding.NamedSharding(self.mesh, jax.sharding.PartitionSpec(None, None))\n    if positions is not None:\n      # AG positions\n      positions = jax.lax.with_sharding_constraint(positions, ag_sharding)\n\n      positions = positions.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_position = jax.lax.broadcast(positions[0], [self.num_stages])\n      position_idx = 0\n    else:\n      example_position = None\n      position_idx = None\n    if segment_ids is not None:\n      segment_ids = jax.lax.with_sharding_constraint(segment_ids, ag_sharding)\n      segment_ids = segment_ids.reshape(\n          (self.config.num_pipeline_microbatches, self.pipeline_microbatch_size, self.config.max_target_length)\n      )\n      example_segmentation = jax.lax.broadcast(segment_ids[0], [self.num_stages])\n      segment_idx = 0\n    else:\n      example_segmentation = None\n      segment_idx = None\n\n    loop_state = self.init_states(inputs)\n\n    # Each microbatch should go through each stage (with repeats) - so there is num_micro * (num_stages * repeats)\n    # compute to perform\n    # Each iteration is vmapped by num_stages, so the number of iterations should be\n    # num_micro * num_stages * repeats / num_stages = num_micro * repeats\n    # However due to the pipeline bubble some iterations process less than num_stages microbatches. It takes\n    # num_micro * repeat iterations for the last microbatch to start the final repeat, then an additional\n    # num_stages - 1 to finish the final repeat.\n    # Thus the total iterations is num_micro * repeat + num_stages - 1, & we may consider the num_stages - 1 as bubble.\n    # The bubble doubles when we use forwarding delay.\n    bubble_iterations = self.forwarding_delay * (self.num_stages - 1)\n    real_iterations = self.config.num_pipeline_microbatches * self.config.num_pipeline_repeats\n    total_iterations = real_iterations + bubble_iterations\n\n    if self.is_initializing():\n      vmap_func = self.get_vmap_func_for_init()\n\n      if self.config.num_pipeline_repeats > 1:\n        # To shard the weights on initialization for the circular pipeline we create weights of\n        # shape [num_repeat, num_stages, ...] (e.g. [num_repeat, num_stages, embed, mlp]) and shard the num_stages axis.\n        # We wrap the main stage vmap with a num_repeat vmap to generate this axis only for parameter initialization.\n        vmap_func = nn.vmap(\n            vmap_func,\n            in_axes=(0, segment_idx, position_idx, None, None),\n            variable_axes={\n                \"params\": 0,\n                \"_overwrite_with_gradient\": 0,\n                \"non_trainable\": 0,\n                \"hyper_params\": 0,\n            },\n            split_rngs={\"params\": True, \"dropout\": self.config.enable_dropout},\n            metadata_params={\n                nn.PARTITION_NAME: \"circular_repeats\",\n                \"sub_weight_split_dims_mapping\": (None,),\n                \"is_initializing\": True,\n                \"x_times\": self.config.num_pipeline_repeats,\n                \"optimizer_dims_mapping\": None,\n            },\n        )\n\n        example_inputs = jax.lax.broadcast(example_inputs, [self.config.num_pipeline_repeats])\n        example_segmentation = (\n            jax.lax.broadcast(example_segmentation, [self.config.num_pipeline_repeats])\n            if example_segmentation is not None\n            else None\n        )\n        example_position = (\n            jax.lax.broadcast(example_position, [self.config.num_pipeline_repeats])\n            if example_position is not None\n            else None\n        )\n      # We only need to run one set of stages to initialize the variables, instead of looping over all microbatches for\n      # the full total_iterations.\n      stage_outputs = vmap_func(\n          self.layers, example_inputs, example_segmentation, example_position, deterministic, model_mode\n      )\n      if self.config.scan_layers:\n        stage_outputs = stage_outputs[0]\n\n      # We return something of the correct shape (global_batch, sequence, embed) by reshaping a single stages output\n      # which has shape [pipeline_microbatch_size, sequence, embed]\n      if self.config.num_pipeline_repeats > 1:\n        stage_outputs = stage_outputs[0]  # Remove extra dimension created for the circular vmap\n      broadcasted_stage_outpus = jax.lax.broadcast(\n          stage_outputs[0], [self.config.micro_batch_size_to_train_on // self.pipeline_microbatch_size]\n      )\n      return jnp.reshape(\n          broadcasted_stage_outpus,\n          [self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim],\n      )\n\n    if self.config.pipeline_fsdp_ag_once:\n      all_pipeline_weights = all_gather_over_fsdp(\n          self.layers.variables, partition_spec, mesh=self.mesh, logical_axis_rules=self.config.logical_axis_rules\n      )\n    else:\n      all_pipeline_weights = self.layers.variables\n\n    def run_iteration_scannable(model, loop_state, xs):\n      # flax transforms like nn.scan and nn.remat can only be applied to nn.module classes or nn.module instances, so we\n      # explicitly wrap the run_one_iteration in this method - the 1st argument model (`self`) is a nn.module instance.\n      return (\n          model.run_one_iteration(\n              loop_state, all_pipeline_weights, positions, segment_ids, deterministic, model_mode, model.layers\n          ),\n          None,\n      )\n\n    if self.config.set_remat_policy_on_pipeline_iterations:\n      run_iteration_scannable = nn.remat(\n          run_iteration_scannable,\n          prevent_cse=not self.config.scan_pipeline_iterations,  # prevent_cse not used with scan\n          policy=self.get_pipeline_remat_policy(),\n      )\n\n    # The scan cannot be used on init since it broadcasts the weights, which aren't yet initialized.\n    if self.config.scan_pipeline_iterations:\n      variable_carry = []\n      variable_broadcast = [\n          \"params\",\n          \"_overwrite_with_gradient\",\n      ]  # All loop iterations need the weights for the full pipeline.\n      if self.is_mutable_collection(\"non_trainable\"):\n        variable_carry.append(\"non_trainable\")\n      else:\n        variable_broadcast.append(\"non_trainable\")\n      run_all_iterations_scanned = nn.scan(\n          run_iteration_scannable,\n          variable_axes={\n              \"summaries\": 0,\n              \"aux_loss\": 0,\n              \"intermediates\": 0,\n              \"hyper_params\": 0,\n          },\n          variable_broadcast=variable_broadcast,\n          variable_carry=variable_carry,\n          # Dropout/aqt keys will be split for each iteration.\n          split_rngs={\"random\": True},\n          length=total_iterations,\n      )\n      loop_state, _ = run_all_iterations_scanned(self, loop_state, None)\n    else:\n      for _ in range(total_iterations):\n        loop_state, _ = run_iteration_scannable(self, loop_state, None)\n\n    # The final output is located in the input/output array, however the output microbatches may be permuted relative to\n    # the input\n    final_output = self.permute_output_micro_per_stage_dim(loop_state[\"state_io\"])\n\n    # reshape outputs to match input shape of total batch instead of microbatches [batch, sequence, embed]\n    final_output = jnp.reshape(\n        final_output, (self.config.micro_batch_size_to_train_on, self.config.max_target_length, self.config.emb_dim)\n    )\n\n    return final_output",
        "analysis": {
            "module_type": "pipeline_parallelism_module",
            "purpose": "Implements pipeline parallelism by wrapping a layer module, managing microbatching, and executing stages in parallel across devices, with support for circular pipelining.",
            "input": {
                "shape": "[micro_batch_size_to_train_on, max_target_length, emb_dim]",
                "dtype": "jnp.ndarray"
            },
            "processing_steps": [
                "The __call__ method reshapes inputs into microbatches.",
                "Initializes pipeline state buffers (state_io, shift, etc.) by calling self.init_states.",
                "Calculates the total number of iterations required, including pipeline bubbles.",
                "If initializing, it runs a vmapped version of the layer once to establish parameter shapes and returns.",
                "If not initializing, it iterates for the total number of iterations.",
                "In each iteration, it calls self.run_one_iteration, which may be wrapped in nn.scan or nn.remat.",
                "After the loop, it permutes the output microbatches to their correct order using self.permute_output_micro_per_stage_dim.",
                "Reshapes the final output tensor back to the global batch shape."
            ],
            "output": {
                "shape": "[micro_batch_size_to_train_on, max_target_length, emb_dim]"
            },
            "dependencies": [
                "flax.linen as nn",
                "jax",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.maxtext_utils.all_gather_over_fsdp"
            ],
            "parameters": {
                "config": "A configuration object containing parameters like num_pipeline_microbatches, num_pipeline_repeats, and parallelism settings.",
                "layers": "An nn.Module instance (e.g., a single or scanned set of decoder layers) that each pipeline stage executes.",
                "mesh": "The JAX device mesh used for sharding.",
                "remat_policy": "The rematerialization policy to apply to the pipeline loop iterations."
            },
            "notes": [
                "This module uses `nn.vmap` to execute the `layers` module in parallel across pipeline stages, which are sharded across devices.",
                "It manages complex state buffers (`state_io`, `shift`, `circ_storage`) to pass activations between stages and across loop iterations.",
                "Supports circular pipelining, where the output of the last stage can be fed back to the first, enabling more virtual layers than physical stages.",
                "Contains distinct logic paths for weight initialization versus the main forward/backward pass."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes class attributes based on the config, such as the number of stages and microbatch sizes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate `num_stages` from config parallelism settings.",
                        "Set `forwarding_delay` based on config.",
                        "Calculate `pipeline_microbatch_size` and `microbatches_per_stage`.",
                        "Determine if circular storage is needed by calling `need_circ_storage`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self.need_circ_storage"
                    ],
                    "notes": [
                        "This is a standard Flax setup method called once during module instantiation."
                    ]
                },
                "init_states": {
                    "purpose": "Initializes the state buffers (state_io, shift, circular_storage) required for the pipeline loop.",
                    "input": {
                        "shape": "[num_micro_batches, micro_batch_size, sequence, embed]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Create a zero-initialized `shift` tensor to pass activations between stages.",
                        "Optionally create `prev_outputs` tensor if delayed activation forwarding is enabled.",
                        "Reshape the input tensor into the `state_io` buffer.",
                        "Optionally create `circ_storage` and `circ_storage_mover` buffers if circular storage is used.",
                        "Apply logical sharding constraints to all created tensors.",
                        "Return a dictionary containing all initialized state tensors and the loop iteration counter."
                    ],
                    "output": {
                        "shape": "A dictionary of tensors representing the initial pipeline state."
                    },
                    "dependencies": [
                        "jnp",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "The input tensor is assumed to have already been reshaped to include the `num_micro_batches` dimension."
                    ]
                },
                "get_iteration_inputs": {
                    "purpose": "Constructs the input tensor for all stages for a single pipeline iteration by selecting from the various state buffers.",
                    "input": {
                        "shape": "loop_iteration (scalar), state_io (tensor), circ_storage (tensor), shift (tensor)",
                        "dtype": "int, jnp.ndarray"
                    },
                    "processing_steps": [
                        "Slice the current microbatch from `state_io`.",
                        "Determine the input for the first stage, which can come from `state_io`, `circ_storage`, or the rotated `shift` buffer depending on the iteration number and configuration.",
                        "Combine the first stage's input with the `shift` buffer, which provides inputs for all other stages.",
                        "Apply logical sharding constraints."
                    ],
                    "output": {
                        "shape": "[num_stages, micro_size, sequence, embed]"
                    },
                    "dependencies": [
                        "jnp",
                        "jax.lax"
                    ],
                    "notes": [
                        "This method implements the core logic for data flow between stages at each time step."
                    ]
                },
                "get_new_loop_state": {
                    "purpose": "Updates all pipeline state buffers after one iteration based on the stage outputs.",
                    "input": {
                        "shape": "output: [num_stages, micro_size, sequence, embed], loop_state: dictionary of state tensors",
                        "dtype": "jnp.ndarray, dict"
                    },
                    "processing_steps": [
                        "Update the `shift` buffer by rotating or shifting the previous iteration's outputs.",
                        "Update `circ_storage` and `circ_storage_mover` if circular pipelining is active.",
                        "Update the `state_io` buffer by shifting its contents and inserting the output from the last pipeline stage.",
                        "Increment the `loop_iteration` counter.",
                        "Return the new state dictionary."
                    ],
                    "output": {
                        "shape": "A dictionary containing the updated state tensors."
                    },
                    "dependencies": [
                        "jax.lax",
                        "jnp"
                    ],
                    "notes": [
                        "This method effectively 'advances' the pipeline by one step, moving all activations to their next position."
                    ]
                },
                "run_one_iteration": {
                    "purpose": "Executes a single step of the pipeline computation for all stages in parallel.",
                    "input": {
                        "shape": "loop_state (dict), pipeline_weights (pytree), positions (tensor), segment_ids (tensor), etc.",
                        "dtype": "dict, pytree, jnp.ndarray"
                    },
                    "processing_steps": [
                        "Get inputs for all stages using `get_iteration_inputs`.",
                        "Gather corresponding `positions` and `segment_ids` for each stage's microbatch.",
                        "Get the appropriate weights for each stage using `get_current_stage_weights`.",
                        "Execute the vmapped layer function to compute all stage outputs in parallel.",
                        "Update the loop state with the new outputs using `get_new_loop_state`."
                    ],
                    "output": {
                        "shape": "The updated `loop_state` dictionary."
                    },
                    "dependencies": [
                        "self.get_iteration_inputs",
                        "self.get_current_stage_weights",
                        "self.get_new_loop_state",
                        "nn.vmap"
                    ],
                    "notes": [
                        "This is the core computational function called inside the main pipeline loop."
                    ]
                },
                "__call__": {
                    "purpose": "The main entry point that orchestrates the entire pipeline execution from input to final output.",
                    "input": {
                        "shape": "inputs: [global_batch, sequence, embed], segment_ids: [global_batch, sequence], positions: [global_batch, sequence]",
                        "dtype": "jnp.ndarray"
                    },
                    "processing_steps": [
                        "Reshape inputs to include a microbatch dimension.",
                        "Initialize the pipeline state using `init_states`.",
                        "If initializing, run the vmapped layer once to create weights and return.",
                        "If not initializing, loop for a calculated number of total iterations, calling `run_one_iteration` in each step.",
                        "Permute the microbatches in the final `state_io` buffer to match the original input order.",
                        "Reshape the output back to the global batch shape."
                    ],
                    "output": {
                        "shape": "[global_batch, sequence, embed]"
                    },
                    "dependencies": [
                        "self.init_states",
                        "self.run_one_iteration",
                        "self.permute_output_micro_per_stage_dim",
                        "nn.scan",
                        "nn.remat"
                    ],
                    "notes": [
                        "This method handles the high-level control flow for both initialization and execution of the pipeline."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_attention_type",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_attention_type(layer_id):\n  layer_id %= len(GEMMA3_ATTENTION_PATTERN)\n  return GEMMA3_ATTENTION_PATTERN[layer_id]",
        "analysis": {
            "module_type": "attention_type_selector",
            "purpose": "Determines the attention type for a given layer ID based on a predefined repeating pattern.",
            "input": {
                "shape": "Scalar",
                "dtype": "int"
            },
            "processing_steps": [
                "Calculate the layer ID modulo the length of the `GEMMA3_ATTENTION_PATTERN` tuple.",
                "Use the result as an index to retrieve the attention type from the `GEMMA3_ATTENTION_PATTERN` tuple."
            ],
            "output": {
                "shape": "Scalar (AttentionType enum member)"
            },
            "dependencies": [
                "GEMMA3_ATTENTION_PATTERN",
                "AttentionType"
            ],
            "parameters": {
                "layer_id": "The integer index of the transformer layer."
            },
            "notes": [
                "This function implements a cyclic pattern of attention types.",
                "The pattern is defined by the `GEMMA3_ATTENTION_PATTERN` constant, which consists of 5 `LOCAL_SLIDING` attentions followed by 1 `GLOBAL` attention."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#get_query_pre_attn_scalar",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def get_query_pre_attn_scalar(config) -> float:\n  \"\"\"Returns the scalar to multiply the query by before attention.\"\"\"\n  if config.model_name in [\"gemma3-4b\", \"gemma3-12b\"]:\n    return config.head_dim**-0.5\n  elif config.model_name == \"gemma3-27b\":\n    return (config.base_emb_dim // config.base_num_query_heads) ** -0.5\n  else:\n    raise ValueError(f\"Unsupported model name: {config.model_name}\")",
        "analysis": {
            "module_type": "attention_scalar_calculator",
            "purpose": "Calculates a model-specific scalar value used to scale the query tensor before the attention mechanism.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check the `config.model_name` attribute.",
                "If the model is 'gemma3-4b' or 'gemma3-12b', calculate the inverse square root of `config.head_dim`.",
                "If the model is 'gemma3-27b', calculate the inverse square root of `config.base_emb_dim` divided by `config.base_num_query_heads`.",
                "Raise a ValueError for any other model name."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model-specific parameters like `model_name`, `head_dim`, `base_emb_dim`, and `base_num_query_heads`."
            },
            "notes": [
                "The function implements different scaling logic based on the specific Gemma3 model variant.",
                "This scalar is a form of normalization, typically 1/sqrt(d_k), to stabilize the attention mechanism."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3DecoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  attention_type: AttentionType = AttentionType.LOCAL_SLIDING\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state=None,\n      slot=None,\n      bidirectional_mask=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    query_pre_attn_scalar = get_query_pre_attn_scalar(cfg)\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        attention_type=self.attention_type,\n        sliding_window_size=cfg.sliding_window_size,\n        attn_logits_soft_cap=cfg.attn_logits_soft_cap,\n        use_qk_norm=True,  # Gemma 3 models use query, key normalizations\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        bidirectional_mask=bidirectional_mask,\n    )\n    if cfg.use_post_attn_norm:\n      attention_lnx = rms_norm(\n          num_features=attention_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_self_attention_norm\",\n          kernel_axes=(\"norm\",),\n      )(attention_lnx)\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n    residual = attention_lnx\n\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n\n    if cfg.use_post_ffw_norm:\n      mlp_lnx = rms_norm(\n          num_features=mlp_lnx.shape[-1],\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"post_ffw_norm\",\n          kernel_axes=(\"norm\",),\n      )(mlp_lnx)\n\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    next_layer_addition = mlp_lnx + residual\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "gemma3_decoder_layer",
            "purpose": "Implements a single decoder layer for a Gemma 3 transformer model, including self-attention and a feed-forward MLP block.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "The `__call__` method executes the forward pass of the decoder layer."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.linears.mlp_block",
                "gemma3.get_query_pre_attn_scalar"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters like dimensions, dropout rates, and feature flags.",
                "mesh": "The JAX device mesh for model parallelism and sharding.",
                "model_mode": "A string indicating the operational mode (e.g., 'train', 'prefill', 'autoregressive').",
                "quant": "Optional quantization configuration for the layer.",
                "attention_type": "The type of attention mechanism to use, such as LOCAL_SLIDING or GLOBAL."
            },
            "notes": [
                "This class is a Flax Linen Module designed to be a building block within a larger transformer model.",
                "It follows a pre-normalization architecture where normalization is applied before the attention and MLP blocks.",
                "The layer includes specific features for Gemma 3, such as query/key normalization and a query pre-attention scalar."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through one complete transformer decoder layer, including self-attention, residual connections, and an MLP block.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "Configurable via `config.dtype` (e.g., float32)."
                    },
                    "processing_steps": [
                        "Apply pre-attention RMS normalization to the input tensor.",
                        "Calculate a query pre-attention scalar.",
                        "Instantiate and apply a self-attention mechanism to the normalized input.",
                        "Conditionally apply post-attention RMS normalization based on `config.use_post_attn_norm`.",
                        "Add the original input to the attention output (first residual connection).",
                        "Apply pre-MLP RMS normalization to the result of the first residual connection.",
                        "Apply a feed-forward MLP block.",
                        "Conditionally apply post-MLP RMS normalization based on `config.use_post_ffw_norm`.",
                        "Add the output of the first residual connection to the MLP output (second residual connection).",
                        "Apply dropout to the final result.",
                        "Optionally record internal activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the final tensor, potentially in a tuple format if `config.scan_layers` is true."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]. If `config.scan_layers` is True, returns a tuple `(output_tensor, None)`."
                    },
                    "dependencies": [
                        "rms_norm",
                        "get_query_pre_attn_scalar",
                        "attention_as_linen",
                        "mlp_block",
                        "flax.linen.Dropout"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is applied.",
                        "The `model_mode` parameter can alter the behavior of sub-modules like attention for different decoding phases."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3ScannableBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3ScannableBlock(nn.Module):\n  \"\"\"A repeatable block of Gemma3 decoder layers.\n\n    This block applies multiple decoder layers sequentially, using the attention\n    pattern defined by GEMMA3_ATTENTION_PATTERN. It's designed to be\n    used with `nn.scan` for efficient compilation.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    quant: None | Quant, quantization config\n    num_of_layers: int, number of decoder layers in the block\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  num_of_layers: int = 1\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot=None,\n      page_state=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(self.num_of_layers):\n      attention_type = get_attention_type(layer_id)\n      layer = Gemma3DecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          model_mode=model_mode,\n          name=f\"layers_{layer_id}\",\n          quant=self.quant,\n          attention_type=attention_type,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          bidirectional_mask=bidirectional_mask,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "gemma3_scannable_block",
            "purpose": "Applies a sequence of Gemma3 decoder layers, designed to be efficiently compiled with `nn.scan` by grouping multiple layers into a single block.",
            "input": {
                "shape": "[batch_size, sequence_length, embedding_dim]",
                "dtype": "float32"
            },
            "processing_steps": [
                "Apply a logical constraint to the input tensor for sharding.",
                "Apply a gradient checkpoint name to the input tensor.",
                "Iterate `num_of_layers` times.",
                "In each iteration, determine the attention type for the current layer using `get_attention_type`.",
                "Instantiate and call a `Gemma3DecoderLayer` with the determined attention type and current hidden state.",
                "If `config.scan_layers` is true, unpack the output tuple from the `Gemma3DecoderLayer`.",
                "Return the final hidden state."
            ],
            "output": {
                "shape": "If `config.scan_layers` is false, returns a tensor of shape [batch_size, sequence_length, embedding_dim]. If true, returns a tuple `(tensor, None)` where the tensor has the same shape."
            },
            "dependencies": [
                "flax.linen.Module",
                "Gemma3DecoderLayer",
                "get_attention_type",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "num_of_layers": "The number of `Gemma3DecoderLayer` instances to apply sequentially within this block.",
                "config.scan_layers": "A boolean flag that alters the internal logic and return signature to make the block compatible with `nn.scan`."
            },
            "notes": [
                "This module is specifically designed to be used within a `flax.linen.scan` transformation for improved compilation and memory efficiency when stacking many decoder layers.",
                "The attention mechanism (e.g., local sliding or global) for each internal `Gemma3DecoderLayer` is determined cyclically based on the `GEMMA3_ATTENTION_PATTERN` constant."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Processes an input tensor through a series of `Gemma3DecoderLayer` instances.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, embedding_dim]. Additional arguments include decoder_segment_ids, decoder_positions, etc.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply a logical constraint to the input tensor for sharding.",
                        "Apply a gradient checkpoint name to the input tensor.",
                        "Loop `num_of_layers` times, processing the data through a `Gemma3DecoderLayer` in each iteration.",
                        "Determine the attention type for each layer dynamically using `get_attention_type`.",
                        "If `config.scan_layers` is true, unpack the layer's output tuple.",
                        "Return the final processed tensor, potentially in a tuple with `None` if `config.scan_layers` is true."
                    ],
                    "output": {
                        "shape": "If `config.scan_layers` is false, returns a tensor of shape [batch_size, sequence_length, embedding_dim]. If true, returns a tuple `(tensor, None)` where the tensor has the same shape."
                    },
                    "dependencies": [
                        "Gemma3DecoderLayer",
                        "get_attention_type"
                    ],
                    "notes": [
                        "The return signature changes based on the `config.scan_layers` flag to support `nn.scan`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#_posemb_sincos_2d",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def _posemb_sincos_2d(\n    h: int,\n    w: int,\n    *,\n    width: int,\n    temperature: float = 10_000.0,\n    precision: str = \"default\",\n    dtype: jnp.dtype = jnp.float32,\n):\n  \"\"\"Follows the MoCo v3 logic.\"\"\"\n  y, x = jnp.mgrid[:h, :w]  # pylint: disable=unpacking-non-sequence\n\n  assert width % 4 == 0, \"Width must be mult of 4 for sincos posemb\"\n  omega = jnp.arange(width // 4) / (width // 4 - 1)\n  omega = 1.0 / (temperature**omega)\n  y = jnp.einsum(\"m,d->md\", y.flatten(), omega, precision=precision)\n  x = jnp.einsum(\"m,d->md\", x.flatten(), omega, precision=precision)\n  pe = jnp.concatenate([jnp.sin(x), jnp.cos(x), jnp.sin(y), jnp.cos(y)], axis=1)\n  return jnp.asarray(pe, dtype)[None, :, :]",
        "analysis": {
            "module_type": "sinusoidal_positional_embedding_2d",
            "purpose": "Generates a 2D sinusoidal positional embedding for a grid of a given height and width, following the logic from MoCo v3.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create 2D coordinate grids `y` and `x` of shape `[h, w]` using `jnp.mgrid`.",
                "Assert that the embedding `width` is a multiple of 4.",
                "Calculate frequency values `omega` based on `width` and `temperature`.",
                "Apply frequencies to the flattened `y` coordinates using `jnp.einsum`.",
                "Apply frequencies to the flattened `x` coordinates using `jnp.einsum`.",
                "Concatenate the sine of x, cosine of x, sine of y, and cosine of y along the feature axis.",
                "Cast the result to the specified `dtype` and add a leading batch dimension."
            ],
            "output": {
                "shape": "[1, h * w, width]"
            },
            "dependencies": [
                "jax.numpy"
            ],
            "parameters": {
                "h": "The height of the input grid.",
                "w": "The width of the input grid.",
                "width": "The dimensionality of the positional embedding. Must be a multiple of 4.",
                "temperature": "A scaling factor for the frequency calculation.",
                "precision": "The precision for the `jnp.einsum` operations.",
                "dtype": "The desired data type for the output tensor."
            },
            "notes": [
                "This is a standalone utility function, not a class or module.",
                "The implementation is noted to follow the MoCo v3 logic."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#MlpBlockViT",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class MlpBlockViT(nnx.Module):\n  \"\"\"NNX version of Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.block_id = block_id\n    self.rngs = rngs\n\n    self.Dense_0 = DenseGeneral(\n        in_features_shape=self.config.hidden_size_for_vit,\n        out_features_shape=self.config.intermediate_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(rate=self.config.dropout_rate)\n    self.Dense_1 = DenseGeneral(\n        in_features_shape=self.config.intermediate_size_for_vit,\n        out_features_shape=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        use_bias=True,\n        matmul_precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    \"\"\"Applies the Transformer MlpBlock module.\"\"\"\n    x = self.Dense_0(x)\n    x = nnx.gelu(x)\n    x = self.Dropout_0(x, deterministic=deterministic)\n    x = self.Dense_1(x)\n    return x",
        "analysis": {
            "module_type": "vision_transformer_mlp_block",
            "purpose": "Implements a multi-layer perceptron (MLP) or feed-forward block for a Vision Transformer, using the Flax NNX API.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                "dtype": "Matches config.dtype_mm"
            },
            "processing_steps": [
                "Initializes a DenseGeneral layer to project from hidden_size_for_vit to intermediate_size_for_vit.",
                "Initializes a Dropout layer.",
                "Initializes a second DenseGeneral layer to project from intermediate_size_for_vit back to hidden_size_for_vit."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
            },
            "dependencies": [
                "flax.nnx",
                "MaxText.layers.linears.DenseGeneral",
                "jax"
            ],
            "parameters": {
                "hidden_size_for_vit": "The input and output feature dimension of the MLP block.",
                "intermediate_size_for_vit": "The size of the hidden layer within the MLP block.",
                "dropout_rate": "The dropout rate applied after the GELU activation.",
                "dtype_mm": "The data type for matrix multiplication operations.",
                "matmul_precision": "The precision setting for matrix multiplication."
            },
            "notes": [
                "This is an NNX (Flax's new API) implementation of a standard Transformer feed-forward network."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MLP block, setting up two dense layers and a dropout layer based on the provided configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes the first DenseGeneral layer (`Dense_0`).",
                        "Initializes the Dropout layer (`Dropout_0`).",
                        "Initializes the second DenseGeneral layer (`Dense_1`)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxText.common_types.Config",
                        "flax.nnx.Rngs",
                        "MaxText.layers.linears.DenseGeneral",
                        "flax.nnx.Dropout"
                    ],
                    "notes": [
                        "The layers are configured using parameters from the `Config` object, such as hidden size, intermediate size, and dropout rate."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the MLP transformation to an input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Applies the first dense layer (`Dense_0`).",
                        "Applies the GELU activation function (`nnx.gelu`).",
                        "Applies dropout (`Dropout_0`), controlled by the `deterministic` flag.",
                        "Applies the second dense layer (`Dense_1`)."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "jax.Array",
                        "flax.nnx.gelu"
                    ],
                    "notes": [
                        "The `deterministic` flag controls whether dropout is active. It should be `False` during training and `True` during inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder1DBlock",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder1DBlock(nnx.Module):\n  \"\"\"Single transformer encoder block (MHSA + MLP).\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      block_id: int,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.block_id = block_id\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n    self.seq_len = (self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2\n\n    self.LayerNorm_0 = nnx.LayerNorm(num_features=self.config.hidden_size_for_vit, rngs=self.rngs)\n    self.MultiHeadDotProductAttention_0 = Attention(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=self.seq_len,\n        mesh=self.mesh,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        inputs_kv_shape=(self.config.per_device_batch_size, self.seq_len, self.config.hidden_size_for_vit),\n        dropout_rate=0,\n        is_nope_layer=True,\n        use_bias_in_projections=True,\n        attention_type=AttentionType.FULL,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / (self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit) ** 0.5,\n        model_mode=\"train\",\n        is_vision=True,\n        rngs=self.rngs,\n    )\n    self.LayerNorm_1 = nnx.LayerNorm(num_features=self.config.hidden_size_for_vit, rngs=self.rngs)\n    self.MlpBlockViT_0 = MlpBlockViT(\n        block_id=self.block_id,\n        config=self.config,\n        rngs=self.rngs,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, deterministic: bool = False) -> jax.Array:\n    y = self.LayerNorm_0(x)\n\n    y = self.MultiHeadDotProductAttention_0(inputs_q=y, inputs_kv=y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n\n    y = self.LayerNorm_1(x)\n    y = self.MlpBlockViT_0(y, deterministic=deterministic)\n    y = self.Dropout_0(y, deterministic=deterministic)\n    x = x + y\n    return x",
        "analysis": {
            "module_type": "transformer_encoder_block",
            "purpose": "A single transformer encoder block for a Vision Transformer (ViT), consisting of a multi-head self-attention (MHSA) layer followed by a feed-forward MLP, with residual connections and layer normalization.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "jax.Array (typically float32 or bfloat16)"
            },
            "processing_steps": [
                "Apply pre-attention layer normalization to the input.",
                "Pass the normalized input through a multi-head self-attention layer.",
                "Apply dropout to the attention output.",
                "Add the original input to the attention output (first residual connection).",
                "Apply pre-MLP layer normalization.",
                "Pass the result through an MLP block.",
                "Apply dropout to the MLP output.",
                "Add the result from the first residual connection (second residual connection).",
                "Return the final tensor."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.LayerNorm",
                "Attention",
                "MlpBlockViT",
                "nnx.Dropout",
                "Config",
                "Mesh"
            ],
            "parameters": {
                "config": "A configuration object containing hyperparameters like hidden size, number of attention heads, dropout rate, etc.",
                "mesh": "The JAX device mesh used for model sharding.",
                "block_id": "The integer index of this block within the encoder stack.",
                "rngs": "An nnx.Rngs object for managing random number generation for initialization and dropout."
            },
            "notes": [
                "This module implements a standard pre-LayerNorm (pre-LN) transformer block architecture.",
                "The sequence length (`seq_len`) is calculated as `(config.image_size_for_vit // config.patch_size_for_vit) ** 2`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the transformer encoder block, setting up the layer normalization, multi-head attention, MLP, and dropout sub-modules.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store `block_id`, `config`, `mesh`, and `rngs`.",
                        "Calculate `seq_len` based on image and patch sizes from the config.",
                        "Initialize the first `nnx.LayerNorm`.",
                        "Initialize the `Attention` layer for multi-head self-attention.",
                        "Initialize the second `nnx.LayerNorm`.",
                        "Initialize the `MlpBlockViT` layer.",
                        "Initialize the `nnx.Dropout` layer."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "nnx.LayerNorm",
                        "Attention",
                        "MlpBlockViT",
                        "nnx.Dropout"
                    ],
                    "notes": [
                        "The attention layer is configured for vision tasks (`is_vision=True`) and uses full dot-product attention."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the transformer encoder block.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_dim]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Apply `LayerNorm_0` to the input `x`.",
                        "Apply `MultiHeadDotProductAttention_0` to the normalized input.",
                        "Apply `Dropout_0` to the attention output.",
                        "Add the result to the original input `x` (first residual connection).",
                        "Apply `LayerNorm_1` to the result of the first residual connection.",
                        "Apply `MlpBlockViT_0` to the second normalized result.",
                        "Apply `Dropout_0` to the MLP output.",
                        "Add the result to the output of the first residual connection (second residual connection).",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self.LayerNorm_0",
                        "self.MultiHeadDotProductAttention_0",
                        "self.Dropout_0",
                        "self.LayerNorm_1",
                        "self.MlpBlockViT_0"
                    ],
                    "notes": [
                        "The `deterministic` boolean argument controls the behavior of the dropout layers; if `True`, dropout is disabled."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Encoder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Encoder(nnx.Module):\n  \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      layer_name = f\"encoderblock_{lyr}\"\n      layer = Encoder1DBlock(\n          block_id=lyr,\n          config=self.config,\n          mesh=self.mesh,\n          rngs=self.rngs,\n      )\n      setattr(self, layer_name, layer)\n    self.encoder_norm = nnx.LayerNorm(num_features=self.config.hidden_size_for_vit, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, deterministic: bool = True) -> jax.Array:\n    # TODO(aireenmei, hengtaoguo): add if-scan branch to enable scan support for vision encoder\n    for lyr in range(self.config.num_hidden_layers_for_vit):\n      x = getattr(self, f\"encoderblock_{lyr}\")(x, deterministic=deterministic)\n    x = self.encoder_norm(x)\n    return x",
        "analysis": {
            "module_type": "vision_transformer_encoder",
            "purpose": "A stack of Transformer encoder blocks that processes a sequence of image patch embeddings.",
            "input": {
                "shape": "[batch_size, num_patches, hidden_size_for_vit]",
                "dtype": "jax.Array"
            },
            "processing_steps": [
                "Iteratively process the input tensor through `num_hidden_layers_for_vit` `Encoder1DBlock` layers using a for-loop.",
                "Apply a final `LayerNorm` to the output of the last block."
            ],
            "output": {
                "shape": "[batch_size, num_patches, hidden_size_for_vit]"
            },
            "dependencies": [
                "Encoder1DBlock",
                "nnx.LayerNorm",
                "nnx.Module",
                "Config",
                "jax.sharding.Mesh",
                "nnx.Rngs"
            ],
            "parameters": {
                "config.num_hidden_layers_for_vit": "The number of `Encoder1DBlock` layers to stack.",
                "config.hidden_size_for_vit": "The dimensionality of the embeddings and hidden states."
            },
            "notes": [
                "The encoder blocks are created dynamically in the constructor and accessed via `getattr` in the forward pass.",
                "A TODO comment indicates that a `jax.lax.scan` implementation is planned for better performance, but the current version uses a Python for-loop."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the encoder by creating and storing a stack of `Encoder1DBlock` layers and a final `LayerNorm`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate `config.num_hidden_layers_for_vit` times, creating an `Encoder1DBlock` instance in each iteration.",
                        "Store each `Encoder1DBlock` as a named attribute (e.g., 'encoderblock_0').",
                        "Initialize a final `nnx.LayerNorm` layer."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Encoder1DBlock",
                        "nnx.LayerNorm"
                    ],
                    "notes": [
                        "Layers are dynamically named and set as attributes using `setattr`."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass by sequentially applying the encoder blocks and the final normalization.",
                    "input": {
                        "shape": "[batch_size, num_patches, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Sequentially pass the input `x` through each `Encoder1DBlock` layer in a for-loop.",
                        "Apply `self.encoder_norm` to the result.",
                        "Return the final tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, hidden_size_for_vit]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `deterministic` flag is passed to the underlying `Encoder1DBlock` layers to control dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Einsum",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Einsum(nnx.Module):\n  \"\"\"Einsum is a convenience module for parameterized tensor multiplication.\"\"\"\n\n  def __init__(\n      self,\n      shape: tuple[int, ...],\n      initializer: nnx.initializers.Initializer = nnx.initializers.normal(),\n      dtype: jnp.dtype | None = None,\n      precision: str = \"default\",\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.precision = precision\n    self.w = nnx.Param(initializer(rngs.params(), shape, dtype))\n\n  def __call__(self, eqn: str, x: jax.Array) -> jax.Array:\n    return jnp.einsum(eqn, x, self.w, precision=self.precision)",
        "analysis": {
            "module_type": "parameterized_einsum",
            "purpose": "A convenience module that encapsulates a learnable weight parameter to perform a parameterized tensor multiplication using `jnp.einsum`.",
            "input": {
                "shape": "The `__call__` method takes an input tensor `x` whose shape is dependent on the `einsum` equation string.",
                "dtype": "jax.Array numeric dtype (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Initializes a learnable weight parameter `w` with a specified shape.",
                "In the forward pass, it performs a `jnp.einsum` operation between an input tensor `x` and the internal weight `w`."
            ],
            "output": {
                "shape": "The output shape is determined by the `einsum` equation string `eqn`."
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Param",
                "nnx.initializers.Initializer",
                "jnp.einsum"
            ],
            "parameters": {
                "shape": "The shape of the learnable weight tensor `w`.",
                "initializer": "The initializer function for the weight tensor `w`.",
                "dtype": "The data type for the weight tensor `w`.",
                "precision": "The precision for the `jnp.einsum` computation (e.g., 'default')."
            },
            "notes": [
                "This module simplifies creating layers that are fundamentally an `einsum` operation with a learnable weight, abstracting away the parameter creation logic."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the module by creating the learnable weight parameter `w`.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the `precision` value.",
                        "Initializes the `self.w` attribute as an `nnx.Param` using the provided shape, initializer, dtype, and rngs."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.Initializer"
                    ],
                    "notes": [
                        "Requires an `nnx.Rngs` object for stateful parameter initialization."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the parameterized `einsum` operation between the input tensor and the module's internal weight.",
                    "input": {
                        "shape": "Variable, depends on the `eqn` string.",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Calls `jnp.einsum` with the provided equation string, input tensor `x`, and the internal weight `self.w`."
                    ],
                    "output": {
                        "shape": "Variable, determined by the `einsum` equation string `eqn`."
                    },
                    "dependencies": [
                        "jnp.einsum"
                    ],
                    "notes": [
                        "The first argument `eqn` is a string that defines the tensor contraction, e.g., '...i,io->...o' for a dense layer."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionEmbedder",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionEmbedder(nnx.Module):\n  \"\"\"Projects image embeddings to the embedding space of the text encoder.\"\"\"\n\n  def __init__(self, config: Config, mesh: Mesh, *, rngs: nnx.Rngs):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.mm_soft_embedding_norm = RMSNorm(\n        num_features=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n        weight_dtype=self.config.weight_dtype,\n        epsilon=self.config.normalization_layer_epsilon,\n        kernel_axes=(\"norm\",),\n        rngs=self.rngs,\n    )\n    self.mm_input_projection = Einsum(shape=(self.config.hidden_size_for_vit, self.config.emb_dim), precision=self.config.matmul_precision, rngs=self.rngs)\n\n  def __call__(self, x: jax.Array, eqn: str = \"...tm,md->...td\") -> jax.Array:\n    x = self.mm_soft_embedding_norm(x)\n    x = self.mm_input_projection(eqn, x)\n    return x",
        "analysis": {
            "module_type": "vision_embedder",
            "purpose": "Projects image embeddings from the vision model's hidden space to the text model's embedding space by applying normalization and a linear projection.",
            "input": {
                "shape": "[batch_size, num_tokens, hidden_size_for_vit]",
                "dtype": "config.dtype_mm"
            },
            "processing_steps": [
                "Initializes an RMSNorm layer (`mm_soft_embedding_norm`) for normalization.",
                "Initializes an Einsum layer (`mm_input_projection`) for linear projection."
            ],
            "output": {
                "shape": "[batch_size, num_tokens, emb_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "RMSNorm",
                "Einsum",
                "Config",
                "Mesh"
            ],
            "parameters": {
                "hidden_size_for_vit": "The feature dimension of the input vision embeddings.",
                "emb_dim": "The target feature dimension of the text encoder's embedding space.",
                "dtype_mm": "The data type for the module's computations.",
                "weight_dtype": "The data type for the module's weights.",
                "normalization_layer_epsilon": "The epsilon value for the RMSNorm layer.",
                "matmul_precision": "The precision for the matrix multiplication in the Einsum layer."
            },
            "notes": [
                "This module acts as a bridge between the vision encoder and the text decoder in a multi-modal model."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the VisionEmbedder module, setting up the normalization and projection layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiate `RMSNorm` as `self.mm_soft_embedding_norm`.",
                        "Instantiate `Einsum` as `self.mm_input_projection` with a weight shape of (config.hidden_size_for_vit, config.emb_dim)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "Mesh",
                        "nnx.Rngs",
                        "RMSNorm",
                        "Einsum"
                    ],
                    "notes": [
                        "The `rngs` argument is required for initializing the parameters of the sub-modules."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass, normalizing and projecting the input image embeddings.",
                    "input": {
                        "shape": "[..., num_tokens, hidden_size_for_vit]",
                        "dtype": "jax.Array"
                    },
                    "processing_steps": [
                        "Apply `self.mm_soft_embedding_norm` to the input tensor `x`.",
                        "Apply `self.mm_input_projection` to the normalized tensor using the provided einsum equation."
                    ],
                    "output": {
                        "shape": "[..., num_tokens, emb_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The `eqn` argument defaults to '...tm,md->...td', which performs a matrix multiplication on the last two dimensions of the input tensor."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#visionembedder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def visionembedder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a VisionEmbedder module.\"\"\"\n  return nnx_wrappers.to_linen(\n      VisionEmbedder,\n      config,\n      mesh=mesh,\n      name=\"VisionEmbedder_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "vision_embedder_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible version of the `VisionEmbedder` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Wraps the `VisionEmbedder` NNX module using `nnx_wrappers.to_linen`.",
                "Passes the configuration, mesh, a fixed name 'VisionEmbedder_0', and a partitioning metadata function to the wrapper."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "VisionEmbedder",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main configuration object (`Config`) containing model and training parameters.",
                "mesh": "The JAX device mesh (`Mesh`) used for sharding and parallelism."
            },
            "notes": [
                "The created module is responsible for projecting image embeddings from the vision encoder's hidden space to the text model's embedding space.",
                "The `abstract_init=False` argument ensures the module is initialized with concrete shapes.",
                "The `metadata_fn` is used to apply logical partitioning rules to the module's parameters."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#VisionExit",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class VisionExit(nnx.Module):\n  \"\"\"The vision exit layer.\n\n  Possibly downsample the soft tokens to a required output length.\n\n  Attributes:\n    output_length: The embed will be spatially avg-pooled to this output length.\n  \"\"\"\n\n  def __init__(self, output_length: int = 256, *, rngs: nnx.Rngs):\n    self.output_length = output_length\n    self.rngs = rngs\n\n  def __call__(self, x):\n    cur_length = x.shape[1]\n    if cur_length == self.output_length:\n      return x\n    cur_width = int(cur_length**0.5)\n    assert cur_width**2 == cur_length\n    output_width = int(self.output_length**0.5)\n    assert output_width**2 == self.output_length, f\"Cannot pool {x.shape=} to {self.output_length}=!\"\n    batch_size = x.shape[0]\n    embed_dim = x.shape[-1]\n    x = jnp.reshape(x, (batch_size, cur_width, cur_width, embed_dim))\n    assert not cur_width % output_width, f\"{cur_width=} {output_width=}\"\n    window = cur_width // output_width\n    window_shape = (window, window)\n    x = nnx.avg_pool(x, window_shape=window_shape, strides=window_shape)\n    batch_size, height, width, embed_dim = x.shape\n    return jnp.reshape(x, (batch_size, height * width, embed_dim))",
        "analysis": {
            "module_type": "vision_exit_layer",
            "purpose": "Downsamples a sequence of vision tokens to a specified target length using spatial average pooling, treating the sequence as a 2D grid.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the layer with a target `output_length`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.numpy",
                "flax.nnx"
            ],
            "parameters": {
                "output_length": "The target sequence length for the output tokens. Both input and output lengths must be perfect squares."
            },
            "notes": [
                "This module assumes the input sequence of tokens can be reshaped into a square 2D grid.",
                "The `rngs` parameter is accepted in the constructor but not used in the forward pass."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the VisionExit layer with a target output length.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores the `output_length` attribute.",
                        "Stores the `rngs` attribute."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Rngs"
                    ],
                    "notes": [
                        "The `rngs` parameter is stored but not actively used within the module's logic."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the downsampling operation on an input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, embed_dim]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Check if the input sequence length already matches `output_length`; if so, return the input directly.",
                        "Calculate the width of the input and output grids by taking the square root of their respective lengths.",
                        "Reshape the input tensor from [batch_size, sequence_length, embed_dim] to [batch_size, cur_width, cur_width, embed_dim].",
                        "Calculate the pooling window size required to downsample from the current width to the output width.",
                        "Apply `nnx.avg_pool` with the calculated window shape and strides.",
                        "Reshape the pooled tensor from [batch_size, height, width, embed_dim] back to [batch_size, output_length, embed_dim].",
                        "Return the downsampled tensor."
                    ],
                    "output": {
                        "shape": "[batch_size, output_length, embed_dim]"
                    },
                    "dependencies": [
                        "jax.numpy.reshape",
                        "flax.nnx.avg_pool"
                    ],
                    "notes": [
                        "Asserts that both the input sequence length and the target `output_length` are perfect squares.",
                        "Asserts that the input grid width is perfectly divisible by the output grid width."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#vision_exit_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def vision_exit_as_linen(x: jax.Array, output_length: int) -> jax.Array:\n  \"\"\"A wrapper to use VisionExit as a function.\"\"\"\n  return nnx.bridge.to_linen(VisionExit, output_length=output_length)(x)",
        "analysis": {
            "module_type": "functional_vision_exit",
            "purpose": "A wrapper function that instantiates and applies the `VisionExit` nnx.Module in a stateless, Flax Linen-compatible manner to downsample vision tokens.",
            "input": {
                "shape": "[batch_size, sequence_length, embed_dim]",
                "dtype": "float32"
            },
            "processing_steps": [
                "Convert the `VisionExit` nnx.Module into a Flax Linen-compatible function using `nnx.bridge.to_linen`.",
                "Pass the `output_length` parameter to the `VisionExit` constructor during conversion.",
                "Call the resulting Linen function with the input tensor `x`."
            ],
            "output": {
                "shape": "[batch_size, output_length, embed_dim]"
            },
            "dependencies": [
                "nnx.bridge.to_linen",
                "VisionExit"
            ],
            "parameters": {
                "output_length": "The target sequence length for the vision tokens after average pooling."
            },
            "notes": [
                "This function serves as a bridge to use a stateful `nnx.Module` (`VisionExit`) as a simple, stateless function within a Flax Linen model.",
                "The implementation relies on the `VisionExit` module, which expects the input `sequence_length` to be a perfect square and its square root to be divisible by the square root of `output_length`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#Gemma3VisionEncoderLayer",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "class Gemma3VisionEncoderLayer(nnx.Module):\n  \"\"\"gemma 3 vision encoder layer\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      *,\n      rngs: nnx.Rngs,\n  ):\n    self.config = config\n    self.mesh = mesh\n    self.rngs = rngs\n\n    self.embedding = nnx.Conv(\n        in_features=self.config.num_channels_for_vit,\n        out_features=self.config.hidden_size_for_vit,\n        kernel_size=(self.config.patch_size_for_vit, self.config.patch_size_for_vit),\n        strides=self.config.conv_stride_for_vit,\n        padding=\"VALID\",\n        precision=self.config.matmul_precision,\n        rngs=self.rngs,\n    )\n    self.pos_embedding = self._get_posemb(\n        self.config.posemb_type_for_vit,\n        seqshape=(\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n            self.config.image_size_for_vit // self.config.patch_size_for_vit,\n        ),\n        width=self.config.hidden_size_for_vit,\n        dtype=self.config.dtype_mm,\n    )\n    self.Dropout_0 = nnx.Dropout(self.config.dropout_rate, rngs=self.rngs)\n    self.Transformer = Encoder(\n        config=self.config,\n        mesh=self.mesh,\n        rngs=self.rngs,\n    )\n    self.VisionExit = VisionExit(output_length=256, rngs=self.rngs)\n\n  def _get_posemb(\n      self,\n      typ: str,\n      *,\n      seqshape: tuple[int, int],\n      width: int,\n      dtype: jnp.dtype = jnp.float32,\n  ):\n    \"\"\"Returns the position embedding.\"\"\"\n    if typ == \"learn\":\n      shape = (1, seqshape[0] * seqshape[1], width)\n      initializer = nnx.initializers.normal(stddev=1 / (width**0.5))\n      return nnx.Param(initializer(self.rngs.params(), shape, dtype))\n    elif typ == \"sincos2d\":\n      return _posemb_sincos_2d(*seqshape, width=width, dtype=dtype, precision=self.config.matmul_precision)\n    else:\n      raise ValueError(f\"Unknown posemb type: {typ}\")\n\n  def __call__(self, inputs, deterministic, train=False):\n    \"\"\"ViT model that transforms image inputs to image embeddings.\n    Args:\n      inputs: jnp.array shaped [B, N, H, W, C], e.g. [4, 1, 896, 896, 3]\n    Returns:\n      jnp.array for image embeddings, shaped [B, N, P, D], e.g. [4, 1, 256, 1152]\n    \"\"\"\n    # currently only supports N=1, the inputs shape is [B, H, W, C]\n    if len(inputs.shape) == 4:\n      inputs = inputs[:, None, :]\n    b, n, h, w, c = inputs.shape\n    x = jnp.reshape(inputs, [b * n, h, w, c])\n    # Gemma3 uses conv2d with stride 14 and kernel size 14 to extract patches.\n    x = self.embedding(x)\n    bn, h, w, c = x.shape\n    x = jnp.reshape(x, [bn, h * w, c])\n\n    x = self.pos_embedding + x\n    x = self.Dropout_0(x)\n\n    # Transformer encoder to extract image features.\n    x = self.Transformer(x, deterministic=deterministic)\n\n    # Gemma3 use a vision exit layer to downsample the soft tokens to a required output length.\n    x = self.VisionExit(x)\n    bn, l, c = x.shape\n    x = jnp.reshape(x, [b, n, l, c])\n    return x",
        "analysis": {
            "module_type": "gemma3_vision_encoder_layer",
            "purpose": "A Vision Transformer (ViT) model that transforms image inputs into a sequence of image embeddings.",
            "input": {
                "shape": "[batch_size, num_images, height, width, channels] or [batch_size, height, width, channels]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Optionally reshapes 4D input to 5D by adding a 'num_images' dimension.",
                "Reshapes the input to merge batch and 'num_images' dimensions.",
                "Applies a convolutional layer (`self.embedding`) to extract image patches.",
                "Reshapes the patched output into a sequence of tokens.",
                "Adds positional embeddings (`self.pos_embedding`).",
                "Applies dropout (`self.Dropout_0`).",
                "Processes the sequence through a transformer encoder (`self.Transformer`).",
                "Downsamples the output sequence to a fixed length using `self.VisionExit`.",
                "Reshapes the final output to separate the batch and 'num_images' dimensions."
            ],
            "output": {
                "shape": "[batch_size, num_images, output_length, hidden_dim]"
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Conv",
                "nnx.Dropout",
                "Encoder",
                "VisionExit",
                "_posemb_sincos_2d"
            ],
            "parameters": {
                "config": "The model configuration object containing hyperparameters for the vision transformer, such as hidden size, patch size, and dropout rate.",
                "mesh": "The JAX device mesh used for model parallelism and data sharding.",
                "rngs": "The NNX random number generators used for parameter initialization and dropout."
            },
            "notes": [
                "The class is designed to handle image inputs and produce embeddings suitable for a multi-modal model.",
                "It supports both learnable and fixed (sincos2d) positional embeddings, configured via `config.posemb_type_for_vit`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Gemma3VisionEncoderLayer, setting up the patch embedding, positional embedding, transformer encoder, and vision exit layers.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes an `nnx.Conv` layer for patch embedding.",
                        "Calls `_get_posemb` to create the positional embedding.",
                        "Initializes an `nnx.Dropout` layer.",
                        "Initializes the `Encoder` module which contains multiple transformer blocks.",
                        "Initializes the `VisionExit` module to downsample the output sequence."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Conv",
                        "nnx.Dropout",
                        "Encoder",
                        "VisionExit",
                        "self._get_posemb"
                    ],
                    "notes": [
                        "All submodules are configured using parameters from the `config` object."
                    ]
                },
                "_get_posemb": {
                    "purpose": "Creates and returns the positional embedding based on the specified type ('learn' or 'sincos2d').",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If type is 'learn', creates a learnable `nnx.Param` with a normal initializer.",
                        "If type is 'sincos2d', calls `_posemb_sincos_2d` to generate fixed sinusoidal embeddings.",
                        "Raises a ValueError for any other type."
                    ],
                    "output": {
                        "shape": "[1, seqshape[0] * seqshape[1], width]"
                    },
                    "dependencies": [
                        "nnx.Param",
                        "nnx.initializers.normal",
                        "_posemb_sincos_2d"
                    ],
                    "notes": [
                        "This is a helper method called only during the initialization of the class."
                    ]
                },
                "__call__": {
                    "purpose": "Processes a batch of images through the Vision Transformer to produce image embeddings.",
                    "input": {
                        "shape": "[B, N, H, W, C] or [B, H, W, C]",
                        "dtype": "jnp.Array"
                    },
                    "processing_steps": [
                        "Optionally adds a 'num_images' dimension if the input is 4D.",
                        "Reshapes input to merge batch and 'num_images' dimensions.",
                        "Applies convolutional embedding (`self.embedding`).",
                        "Reshapes output into a sequence of patches.",
                        "Adds positional embedding (`self.pos_embedding`).",
                        "Applies dropout (`self.Dropout_0`).",
                        "Passes the sequence through the transformer encoder (`self.Transformer`).",
                        "Downsamples the output sequence using `self.VisionExit`.",
                        "Reshapes the final sequence to split the batch and 'num_images' dimensions."
                    ],
                    "output": {
                        "shape": "[B, N, P, D]"
                    },
                    "dependencies": [
                        "jnp.reshape",
                        "self.embedding",
                        "self.pos_embedding",
                        "self.Dropout_0",
                        "self.Transformer",
                        "self.VisionExit"
                    ],
                    "notes": [
                        "The `deterministic` flag controls the behavior of dropout layers.",
                        "The `train` argument is accepted but not used within the method's logic."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma3.py#gemma3visionencoder_as_linen",
        "file_path": "src/MaxText/layers/gemma3.py",
        "code_block": "def gemma3visionencoder_as_linen(\n    config: Config,\n    mesh: Mesh,\n):\n  \"\"\"Creates a Gemma3VisionEncoder module.\"\"\"\n  module = nnx_wrappers.to_linen(\n      Gemma3VisionEncoderLayer,\n      config=config,\n      mesh=mesh,\n      name=\"Gemma3VisionEncoderLayer_0\",\n      abstract_init=False,\n      metadata_fn=variable_to_logically_partitioned,\n  )\n  return module",
        "analysis": {
            "module_type": "vision_encoder_factory",
            "purpose": "A factory function that creates a Flax Linen-compatible version of the `Gemma3VisionEncoderLayer` NNX module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `Gemma3VisionEncoderLayer` NNX module into a Flax Linen module.",
                "Passes the configuration, mesh, a fixed name ('Gemma3VisionEncoderLayer_0'), and a partitioning function (`variable_to_logically_partitioned`) to the wrapper."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "Gemma3VisionEncoderLayer",
                "variable_to_logically_partitioned",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "An instance of the Config class containing model and training configurations.",
                "mesh": "A JAX sharding Mesh object for distributed computation."
            },
            "notes": [
                "This function serves as a bridge between the NNX module definition (`Gemma3VisionEncoderLayer`) and the Flax Linen framework used in the wider model.",
                "The `abstract_init=False` argument indicates that the module is initialized concretely upon creation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/gemma.py#GemmaDecoderLayer",
        "file_path": "src/MaxText/layers/gemma.py",
        "code_block": "class GemmaDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_manager=None,\n      page_state=None,\n      slot=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    # inputs: embedded inputs to the decoder with shape [batch, length, emb_dim]\n    lnx = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_norm\",\n        kernel_axes=(\"norm\",),\n    )(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        use_ragged_attention=cfg.use_ragged_attention,\n        ragged_block_size=cfg.ragged_block_size,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    attention_lnx += inputs\n    residual = attention_lnx\n    attn_output = rms_norm(\n        num_features=attention_lnx.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_ffw_norm\",\n        kernel_axes=(\"norm\",),\n    )(attention_lnx)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=attn_output.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(attn_output, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    next_layer_addition = mlp_lnx + residual\n\n    next_layer_addition_dropped_out = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(\n        next_layer_addition, deterministic=deterministic\n    )\n\n    layer_output = next_layer_addition_dropped_out\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "The `GemmaDecoderLayer` class implements a single layer of a Gemma-style Transformer decoder. It processes an input tensor through a self-attention mechanism followed by a multi-layer perceptron (MLP) block, incorporating residual connections and layer normalization.",
            "usage": "Instantiate the class with a configuration object (`Config`), a JAX device mesh (`Mesh`), a model mode string, and an optional quantization configuration. Call the instance with an input tensor of shape `[batch, length, emb_dim]`, along with segment IDs, positions, and a deterministic flag. The layer returns a tensor of the same shape, representing the processed output."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama2.py#LlamaDecoderLayer",
        "file_path": "src/MaxText/layers/llama2.py",
        "code_block": "class LlamaDecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer that attends to the encoder.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    if model_mode == MODEL_MODE_PREFILL:\n      activation_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      activation_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n    inputs = nn.with_logical_constraint(inputs, activation_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, activation_axis_names)\n\n    # Self-attention block\n    attention_layer = attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        use_ragged_attention=cfg.use_ragged_attention,\n        ragged_block_size=cfg.ragged_block_size,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n    )\n\n    attention_lnx = nn.with_logical_constraint(attention_lnx, activation_axis_names)\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(hidden_states, activation_axis_names)\n\n    # MLP block.\n    mlp_lnx = mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n        model_mode=model_mode,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, activation_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(layer_output, activation_axis_names)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "module_type": "llama_decoder_layer",
            "purpose": "Implements a single decoder layer of a Llama-style transformer, including self-attention and a feed-forward MLP block with residual connections.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Determined by config (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input.",
                "Perform self-attention on the normalized input.",
                "Add the attention output to the original input (first residual connection).",
                "Apply post-attention RMS normalization.",
                "Process the result through an MLP block.",
                "Add the MLP output to the result of the first residual connection (second residual connection).",
                "Apply dropout.",
                "Optionally record internal activation metrics.",
                "Return the final output tensor."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attentions.attention_as_linen",
                "MaxText.layers.linears.mlp_block",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like dimensions, dropout rates, and layer types.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "A string indicating the operational mode, e.g., 'prefill' or 'autoregressive'.",
                "quant": "An optional Quantization object for applying quantization-aware training."
            },
            "notes": [
                "This layer uses a pre-normalization architecture (RMSNorm before attention and MLP).",
                "It supports different operational modes ('prefill', 'autoregressive') which affect internal logic like logical axis naming.",
                "The return signature of the `__call__` method changes based on the `config.scan_layers` flag."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "Determined by config"
                    },
                    "processing_steps": [
                        "Select logical activation axis names based on `model_mode`.",
                        "Apply pre-attention RMS normalization to `inputs`.",
                        "Instantiate and call the self-attention layer (`attention_as_linen`).",
                        "Add the attention output to the original `inputs` (residual connection).",
                        "Apply post-attention RMS normalization to the intermediate result.",
                        "Instantiate and call the `mlp_block`.",
                        "Add the MLP output to the intermediate result (second residual connection).",
                        "Apply dropout to the final layer output.",
                        "Optionally log activation metrics if `config.record_internal_nn_metrics` is true.",
                        "Return the layer output, potentially as a tuple with `None` if `config.scan_layers` is true."
                    ],
                    "output": {
                        "shape": "If config.scan_layers is false: [batch_size, sequence_length, hidden_dim]. If true: ([batch_size, sequence_length, hidden_dim], None)."
                    },
                    "dependencies": [
                        "rms_norm",
                        "attention_as_linen",
                        "mlp_block",
                        "nn.with_logical_constraint",
                        "checkpoint_name",
                        "nn.Dropout"
                    ],
                    "notes": [
                        "Accepts optional arguments `slot`, `page_state`, and `previous_chunk` for handling paged attention during inference.",
                        "The `deterministic` argument controls whether dropout is applied."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#self_attention_with_norm",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def self_attention_with_norm(\n    inputs,\n    cfg,\n    mesh,\n    quant,\n    decoder_segment_ids,\n    decoder_positions,\n    deterministic,\n    model_mode,\n    previous_chunk=None,\n    page_state: None | page_manager.PageState = None,\n    slot: None | int = None,\n):\n  \"\"\"self-attention with normalization\"\"\"\n  # Normalization\n  lnx_rms = rms_norm(\n      num_features=inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"pre_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )\n  lnx = lnx_rms(inputs)\n  if model_mode == MODEL_MODE_PREFILL:\n    logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n  else:\n    logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n\n  lnx = nn.with_logical_constraint(lnx, logical_axis_names)\n\n  attention_layer = attention_mla.mla_as_linen(\n      config=cfg,\n      num_query_heads=cfg.num_query_heads,\n      num_kv_heads=cfg.num_kv_heads,\n      head_dim=cfg.head_dim,\n      max_target_length=cfg.max_target_length,\n      max_prefill_predict_length=cfg.max_prefill_predict_length,\n      attention_kernel=cfg.attention,\n      attention_type=cfg.attention_type,\n      inputs_q_shape=lnx.shape,\n      inputs_kv_shape=lnx.shape,\n      mesh=mesh,\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      dropout_rate=cfg.dropout_rate,\n      name=\"self_attention\",\n      quant=quant,\n      kv_quant=quantizations.configure_kv_quant(cfg),\n      q_lora_rank=cfg.q_lora_rank,\n      kv_lora_rank=cfg.kv_lora_rank,\n      qk_nope_head_dim=cfg.qk_nope_head_dim,\n      qk_rope_head_dim=cfg.qk_rope_head_dim,\n      v_head_dim=cfg.v_head_dim,\n      max_position_embeddings=cfg.max_position_embeddings,\n      original_max_position_embeddings=cfg.original_max_position_embeddings,\n      mscale=cfg.mscale,\n      rope_factor=cfg.rope_factor,\n      model_mode=model_mode,\n  )\n\n  attention_lnx = attention_layer(\n      lnx,\n      lnx,\n      decoder_positions,\n      decoder_segment_ids=decoder_segment_ids,\n      deterministic=deterministic,\n      model_mode=model_mode,\n      previous_chunk=previous_chunk,\n      page_state=page_state,\n      slot=slot,\n  )\n\n  attention_lnx = nn.with_logical_constraint(attention_lnx, logical_axis_names)\n  intermediate_inputs = inputs + attention_lnx\n\n  # Normalization\n  hidden_states = rms_norm(\n      num_features=intermediate_inputs.shape[-1],\n      dtype=cfg.dtype,\n      weight_dtype=cfg.weight_dtype,\n      name=\"post_self_attention_layer_norm\",\n      kernel_axes=(\"norm\",),\n      epsilon=cfg.normalization_layer_epsilon,\n  )(intermediate_inputs)\n  hidden_states = nn.with_logical_constraint(hidden_states, logical_axis_names)\n  return hidden_states, intermediate_inputs",
        "analysis": {
            "module_type": "self_attention_with_normalization",
            "purpose": "Performs self-attention on an input tensor, applying RMS normalization before and after the attention block, with a residual connection.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "cfg.dtype (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Apply pre-attention RMS normalization to the input tensor.",
                "Apply a logical axis constraint to the normalized tensor.",
                "Initialize and execute a multi-head latent attention layer (`attention_mla.mla_as_linen`).",
                "Add the attention output to the original input tensor via a residual connection, creating `intermediate_inputs`.",
                "Apply post-attention RMS normalization to the `intermediate_inputs` to produce `hidden_states`.",
                "Apply a logical axis constraint to the final `hidden_states`.",
                "Return both the final `hidden_states` and the `intermediate_inputs`."
            ],
            "output": {
                "shape": "A tuple of two tensors, both with shape [batch_size, sequence_length, hidden_dim]."
            },
            "dependencies": [
                "MaxText.layers.normalizations.rms_norm",
                "MaxText.layers.attention_mla.mla_as_linen",
                "flax.linen.with_logical_constraint",
                "MaxText.inference.page_manager.PageState"
            ],
            "parameters": {
                "cfg": "A configuration object containing model hyperparameters like dtype, head dimensions, attention type, etc.",
                "mesh": "The JAX sharding mesh for distributed computation.",
                "quant": "Configuration for quantization.",
                "model_mode": "A string indicating the operational mode (e.g., 'prefill'), which affects logical axis naming.",
                "deterministic": "A boolean flag to control dropout behavior.",
                "page_state": "An optional state object for paged attention during inference.",
                "slot": "An optional integer indicating the current slot in the paged attention cache."
            },
            "notes": [
                "This function implements a pre-normalization and post-normalization attention block.",
                "The function returns two tensors: the final normalized output (`hidden_states`) and the output of the first residual connection (`intermediate_inputs`), which is typically passed to a subsequent MLP block.",
                "It uses `nn.with_logical_constraint` to enforce sharding annotations for distributed training and inference."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#post_process",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "def post_process(cfg, layer_output, sow):\n  \"\"\"postprocessing.\"\"\"\n  if cfg.record_internal_nn_metrics:\n    sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n    sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n    sow(\n        \"intermediates\",\n        \"activation_fraction_zero\",\n        jnp.sum(layer_output == 0) / jnp.size(layer_output),\n    )\n\n  if cfg.scan_layers:\n    return layer_output, None\n  else:\n    return layer_output",
        "analysis": {
            "module_type": "layer_post_processing",
            "purpose": "Optionally records activation statistics (mean, stdev, fraction of zeros) and formats the layer output for compatibility with or without `flax.linen.scan`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `cfg.record_internal_nn_metrics` is true.",
                "If true, use the `sow` function to record the mean, standard deviation, and fraction of zero activations in `layer_output`.",
                "Check if `cfg.scan_layers` is true.",
                "If `scan_layers` is true, return a tuple of `(layer_output, None)`.",
                "Otherwise, return `layer_output`."
            ],
            "output": {
                "shape": "Returns the input `layer_output` tensor, either directly or as the first element of a tuple `(layer_output, None)`."
            },
            "dependencies": [
                "jax.numpy"
            ],
            "parameters": {
                "cfg.record_internal_nn_metrics": "A boolean flag to enable or disable the recording of activation statistics.",
                "cfg.scan_layers": "A boolean flag that changes the return signature to be compatible with `flax.linen.scan`."
            },
            "notes": [
                "The `sow` argument is a function, typically from a Flax Module, used to store intermediate values.",
                "The return value's structure is conditional on the `cfg.scan_layers` parameter."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekDenseLayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekDenseLayer(nn.Module):\n  \"\"\"DeepSeek-style dense layer with Multi-Head Latent Attention.\"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        cfg,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n    mlp_lnx = linears.mlp_block(\n        in_features=hidden_states.shape[-1],\n        intermediate_dim=cfg.mlp_dim,\n        activations=cfg.mlp_activations,\n        intermediate_dropout_rate=cfg.dropout_rate,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"mlp\",\n        config=cfg,\n        quant=self.quant,\n    )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "module_type": "deepseek_dense_layer",
            "purpose": "Implements a single decoder layer for a DeepSeek-style transformer, consisting of a self-attention block followed by a dense MLP block, with residual connections.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "float32 or bfloat16 (determined by config.dtype)"
            },
            "processing_steps": [
                "Apply logical constraints to the input tensor based on the model_mode.",
                "Call `self_attention_with_norm` which performs layer normalization, multi-head latent attention, and a residual connection.",
                "Pass the result through an `mlp_block`.",
                "Add a second residual connection, adding the output of the MLP to the output of the attention block.",
                "Apply dropout to the result.",
                "Apply final logical constraints.",
                "Return the output after passing through the `post_process` function."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_dim]"
            },
            "dependencies": [
                "flax.linen.Module",
                "self_attention_with_norm",
                "linears.mlp_block",
                "post_process",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh",
                "MaxText.layers.quantizations.AqtQuantization"
            ],
            "parameters": {
                "config": "A Config object containing model hyperparameters like mlp_dim, mlp_activations, dropout_rate, dtype, etc.",
                "mesh": "The JAX device mesh for model parallelism.",
                "model_mode": "Specifies the operational mode, e.g., 'prefill' or 'decode', which affects tensor sharding annotations.",
                "quant": "Optional quantization configuration for the layer."
            },
            "notes": [
                "This layer follows a pre-normalization (Pre-LN) architecture.",
                "The logical axis names for tensor sharding annotations change depending on whether `model_mode` is 'prefill' or not.",
                "The layer uses two residual connections: one after the attention block and another after the MLP block."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Executes the forward pass of the DeepSeek dense decoder layer.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim]",
                        "dtype": "float32 or bfloat16 (determined by config.dtype)"
                    },
                    "processing_steps": [
                        "Apply logical constraints to the input tensor using `nn.with_logical_constraint`.",
                        "Apply a JAX checkpoint name to the input for debugging and profiling.",
                        "Call `self_attention_with_norm` to get `hidden_states` and `intermediate_inputs`.",
                        "Instantiate and call `linears.mlp_block` on `hidden_states`.",
                        "Apply logical constraints to the MLP output.",
                        "Add the MLP output to `intermediate_inputs` for the second residual connection.",
                        "Apply dropout.",
                        "Apply final logical constraints to the layer output.",
                        "Call `post_process` on the final output."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_dim]"
                    },
                    "dependencies": [
                        "self_attention_with_norm",
                        "linears.mlp_block",
                        "post_process",
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint",
                        "flax.linen.Dropout"
                    ],
                    "notes": [
                        "Accepts additional arguments for managing decoding state, such as `decoder_segment_ids`, `decoder_positions`, `previous_chunk`, `page_state`, and `slot`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/deepseek.py#DeepSeekMoELayer",
        "file_path": "src/MaxText/layers/deepseek.py",
        "code_block": "class DeepSeekMoELayer(nn.Module):\n  \"\"\"DeepSeek-style MoE layer with Multi-Head Latent Attention.\n  Supports dropless and dropping base on configs.\n  Uses a bias in routing instead of load balancing loss.\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      previous_chunk=None,\n      page_state: None | page_manager.PageState = None,\n      slot: None | int = None,\n  ):\n    cfg = self.config\n    if model_mode == MODEL_MODE_PREFILL:\n      logical_axis_names = (\"activation_batch\", \"prefill_activation_norm_length\", \"activation_embed\")\n    else:\n      logical_axis_names = (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    inputs = nn.with_logical_constraint(inputs, logical_axis_names)\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n\n    hidden_states, intermediate_inputs = self_attention_with_norm(\n        inputs,\n        self.config,\n        self.mesh,\n        self.quant,\n        decoder_segment_ids,\n        decoder_positions,\n        deterministic,\n        model_mode,\n        previous_chunk,\n        page_state,\n        slot,\n    )\n\n    # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n    # The `name` represents the weight name in JAX/checkpoints and so the class name\n    # is just for readability.\n    mlp_lnx = moe.get_routed_and_shared_moe(\n        name=\"DeepSeekMoeBlock_0\",\n        config=cfg,\n        mesh=self.mesh,\n        kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n        kernel_axes=(\"embed\", None),\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        quant=self.quant,\n    )(hidden_states)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, logical_axis_names)\n\n    layer_output = mlp_lnx + intermediate_inputs\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        logical_axis_names,\n    )\n    return post_process(cfg, layer_output, self.sow)",
        "analysis": {
            "functionality": "The `DeepSeekMoELayer` class implements a single decoder layer for a DeepSeek-style transformer model. It processes an input tensor through a self-attention mechanism followed by a Mixture-of-Experts (MoE) block, incorporating residual connections and normalization.",
            "usage": "Instantiate the class with a configuration object (`Config`), a JAX device mesh (`Mesh`), a model mode string, and an optional quantization configuration. Call the instance with an input tensor of shape `[batch_size, sequence_length, embed_dim]` along with other parameters like `decoder_segment_ids`, `decoder_positions`, and a `deterministic` flag. The layer returns an output tensor of the same shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4UnfoldConvolution",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4UnfoldConvolution(nn.Module):\n  \"\"\"implementation of Llama4UnfoldConvolution for Llama4 Multi modal model.\n\n  This module extracts patches from input images and projects them to hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    \"\"\"\n    Initialize Llama4UnfoldConvolution\n    \"\"\"\n    cfg = self.config\n    # Linear projection layer using dense_general.\n    # patches sent to dense_general with shape:\n    # [batch_size, num_patches, num_channels * patch_size * patch_size]\n    self.linear = linears.dense_general(\n        in_features_shape=(cfg.num_channels_for_vit * cfg.patch_size_for_vit * cfg.patch_size_for_vit),\n        out_features_shape=cfg.hidden_size_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_unfold_linear\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n\n  def __call__(self, inputs: Array) -> Array:\n    \"\"\"Extract patches and project to hidden dimension.\n\n    Args:\n      inputs: Input tensor of shape [batch_size, channels, img, img]\n\n    Returns:\n      Tensor of shape [batch_size, num_patches*num_patches, hidden_size]\n    \"\"\"\n    cfg = self.config\n    # Extract patches using conv_general_dilated_patches\n    batch_size, num_channels, img, _ = inputs.shape\n    num_patches = (img // cfg.patch_size_for_vit) ** 2\n\n    # Extract patches using conv_general_dilated_patches\n    patches = lax.conv_general_dilated_patches(\n        inputs,\n        filter_shape=[cfg.patch_size_for_vit, cfg.patch_size_for_vit],\n        window_strides=[cfg.patch_size_for_vit, cfg.patch_size_for_vit],\n        padding=\"VALID\",\n        dimension_numbers=(\"NCHW\", \"HWIO\", \"NCHW\"),\n    )\n\n    # reshape patches to [batch_size, num_patches, num_channels * patch_size * patch_size]\n    patches = patches.reshape(batch_size, num_channels * cfg.patch_size_for_vit * cfg.patch_size_for_vit, num_patches)\n    # After transpose, patches shape:\n    # [batch_size, num_patches, num_channels * patch_size * patch_size]\n    patches = patches.transpose(0, 2, 1)\n\n    # Project patches to hidden dimension using dense_general\n    hidden_states = self.linear(patches)\n\n    return hidden_states",
        "analysis": {
            "module_type": "unfold_convolution_projection",
            "purpose": "Extracts non-overlapping patches from input images and projects them into a hidden dimension using a linear layer.",
            "input": {
                "shape": "[batch_size, channels, img_height, img_width]",
                "dtype": "Inferred from config.dtype_mm"
            },
            "processing_steps": [
                "Extracts patches from the input tensor using `lax.conv_general_dilated_patches`.",
                "Reshapes and transposes the patches to the shape [batch_size, num_patches, flattened_patch_dim].",
                "Applies a linear projection to the patches to map them to the target hidden dimension."
            ],
            "output": {
                "shape": "[batch_size, num_patches, hidden_size_for_vit]"
            },
            "dependencies": [
                "flax.linen.nn.Module",
                "jax.lax.conv_general_dilated_patches",
                "MaxText.layers.linears.dense_general",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.patch_size_for_vit": "The height and width of each square patch to be extracted.",
                "config.num_channels_for_vit": "The number of channels in the input image.",
                "config.hidden_size_for_vit": "The dimensionality of the output feature space after projection.",
                "config.dtype_mm": "The data type for the multi-modal computations.",
                "config.matmul_precision": "The precision for the matrix multiplication in the linear layer."
            },
            "notes": [
                "This module effectively performs a convolution with a stride equal to the patch size, followed by a reshape and a linear transformation.",
                "The linear projection layer does not use a bias."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the linear projection layer used to transform the flattened image patches.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes a `linears.dense_general` layer with input features corresponding to the flattened patch size and output features corresponding to `config.hidden_size_for_vit`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxText.layers.linears.dense_general"
                    ],
                    "notes": [
                        "The `in_features_shape` for the linear layer is calculated as `num_channels * patch_size * patch_size`."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the patch extraction and projection process on an input image tensor.",
                    "input": {
                        "shape": "[batch_size, channels, img, img]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calculates the number of patches based on image and patch size.",
                        "Extracts patches using `lax.conv_general_dilated_patches` with 'VALID' padding.",
                        "Reshapes the extracted patches into a 3D tensor.",
                        "Transposes the tensor to the shape [batch_size, num_patches, features].",
                        "Applies the pre-configured linear layer (`self.linear`) to project the patches."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "jax.lax.conv_general_dilated_patches",
                        "self.linear"
                    ],
                    "notes": [
                        "The patch extraction is non-overlapping because `window_strides` is set to be equal to `filter_shape`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#pixel_shuffle",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def pixel_shuffle(input_tensor: Array, shuffle_ratio: float) -> Array:\n  \"\"\"Apply pixel shuffle operation to the input tensor.\"\"\"\n  batch_size, num_patches, channels = input_tensor.shape\n  patch_size = int(math.sqrt(num_patches))\n\n  # Reshape to [batch_size, patch_size, patch_size, channels]\n  input_tensor = input_tensor.reshape(batch_size, patch_size, patch_size, -1)\n  batch_size, height, width, channels = input_tensor.shape\n\n  # Reshape to [batch_size, height, width * shuffle_ratio, channels / shuffle_ratio]\n  reshaped_tensor = input_tensor.reshape(batch_size, height, int(width * shuffle_ratio), int(channels / shuffle_ratio))\n\n  # Transpose to [batch_size, width * shuffle_ratio, height, channels / shuffle_ratio]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape to [batch_size, height * shuffle_ratio, width * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.reshape(\n      batch_size, int(height * shuffle_ratio), int(width * shuffle_ratio), int(channels / (shuffle_ratio**2))\n  )\n\n  # Transpose to [batch_size, width * shuffle_ratio, height * shuffle_ratio, channels / (shuffle_ratio^2)]\n  reshaped_tensor = reshaped_tensor.transpose(0, 2, 1, 3)\n\n  # Reshape back to [batch_size, num_patches, channels]\n  output_tensor = reshaped_tensor.reshape(batch_size, -1, reshaped_tensor.shape[-1])\n  return output_tensor",
        "analysis": {
            "functionality": "This function performs a pixel shuffle operation on a 3D tensor. It rearranges the tensor's elements by moving data from the channel dimension to the spatial dimensions, effectively increasing spatial resolution while decreasing channel depth.",
            "usage": "Call this function with a 3D tensor of shape `[batch_size, num_patches, channels]` and a `shuffle_ratio`. The `num_patches` dimension is treated as a square grid (`patch_size` x `patch_size`). The function returns a tensor of shape `[batch_size, num_patches, channels / (shuffle_ratio**2)]`. The input `num_patches` must be a perfect square, and `channels` must be divisible by `shuffle_ratio` squared."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP(nn.Module):\n  \"\"\"MLP block for Llama4EncoderLayer.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    cfg = self.config\n    self.fc1 = linears.dense_general(\n        in_features_shape=cfg.hidden_size_for_vit,\n        out_features_shape=cfg.intermediate_size_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_encoder_layer_mlp_fc1\",\n        use_bias=True,\n        matmul_precision=cfg.matmul_precision,\n    )\n    self.fc2 = linears.dense_general(\n        in_features_shape=cfg.intermediate_size_for_vit,\n        out_features_shape=cfg.hidden_size_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_encoder_layer_mlp_fc2\",\n        use_bias=True,\n        matmul_precision=cfg.matmul_precision,\n    )\n\n  def __call__(self, hidden_states: Array) -> Array:\n    \"\"\"Apply MLP transformation to hidden states.\n\n    Args:\n      hidden_states: Input tensor\n      deterministic: If True, disables dropout during inference\n    \"\"\"\n    hidden_states = self.fc1(hidden_states)\n    hidden_states = nn.gelu(hidden_states, approximate=False)\n\n    hidden_states = self.fc2(hidden_states)\n\n    return hidden_states",
        "analysis": {
            "module_type": "llama_4_vision_mlp",
            "purpose": "A standard two-layer Multi-Layer Perceptron (MLP) block used within the Llama4 Vision Encoder Layer.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                "dtype": "Corresponds to config.dtype_mm"
            },
            "processing_steps": [
                "Apply the first dense layer (fc1) to expand the hidden dimension.",
                "Apply a GELU activation function.",
                "Apply the second dense layer (fc2) to project back to the original hidden dimension."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.linears.dense_general",
                "flax.linen.gelu",
                "Config"
            ],
            "parameters": {
                "hidden_size_for_vit": "The input and output feature dimension of the MLP.",
                "intermediate_size_for_vit": "The size of the intermediate hidden layer in the MLP.",
                "dtype_mm": "The data type for the matrix multiplications in the dense layers.",
                "matmul_precision": "The precision setting for the matrix multiplications."
            },
            "notes": [
                "This module implements a standard feed-forward network as found in Transformer architectures."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the two dense linear layers (fc1 and fc2) of the MLP.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes `self.fc1` as a dense_general layer mapping `hidden_size_for_vit` to `intermediate_size_for_vit`.",
                        "Initializes `self.fc2` as a dense_general layer mapping `intermediate_size_for_vit` back to `hidden_size_for_vit`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.dense_general"
                    ],
                    "notes": [
                        "Both linear layers are configured to use a bias."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the MLP transformation to the input hidden states.",
                    "input": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Pass input through `self.fc1`.",
                        "Apply GELU activation with `approximate=False`.",
                        "Pass result through `self.fc2`."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "self.fc1",
                        "self.fc2",
                        "nn.gelu"
                    ],
                    "notes": [
                        "The docstring includes a `deterministic` argument which is not used in the method's implementation."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionMLP2",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionMLP2(nn.Module):\n  \"\"\"MLP block for Llama4VisionPixelShuffleMLP.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    \"\"\"\n    Initialize Llama4VisionMLP2\n    \"\"\"\n    cfg = self.config\n    self.fc1 = linears.dense_general(\n        in_features_shape=cfg.intermediate_size_for_vit,\n        out_features_shape=cfg.projector_input_dim_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_pixel_shuffle_mlp_fc1\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n    self.fc2 = linears.dense_general(\n        in_features_shape=cfg.projector_input_dim_for_vit,\n        out_features_shape=cfg.projector_output_dim_for_vit,\n        dtype=cfg.dtype_mm,\n        name=\"vit_pixel_shuffle_mlp_fc2\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n    self.dropout = nn.Dropout(rate=cfg.projector_dropout_for_vit)\n\n  def __call__(self, hidden_states: Array, deterministic: bool = False) -> Array:\n    \"\"\"Apply MLP transformation to hidden states.\n\n    Args:\n      hidden_states: Input tensor\n      deterministic: If True, disables dropout during inference\n    \"\"\"\n    # First linear layer with GELU activation\n    hidden_states = self.fc1(hidden_states)\n    hidden_states = nn.gelu(hidden_states, approximate=False)\n\n    # Apply dropout\n    # in pytorch it's using default Dropout Rate of 0.5\n    hidden_states = self.dropout(hidden_states, deterministic=deterministic)\n\n    # Second linear layer with GELU activation\n    hidden_states = self.fc2(hidden_states)\n    hidden_states = nn.gelu(hidden_states, approximate=False)\n\n    return hidden_states",
        "analysis": {
            "module_type": "multi_layer_perceptron",
            "purpose": "A two-layer Multi-Layer Perceptron (MLP) with GELU activations and dropout, designed for processing vision features within the Llama4VisionPixelShuffleMLP module.",
            "input": {
                "shape": "[batch_size, sequence_length, config.intermediate_size_for_vit]",
                "dtype": "config.dtype_mm"
            },
            "processing_steps": [
                "Pass input through the first dense layer (fc1).",
                "Apply a non-approximate GELU activation.",
                "Apply dropout.",
                "Pass the result through the second dense layer (fc2).",
                "Apply a final non-approximate GELU activation."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, config.projector_output_dim_for_vit]"
            },
            "dependencies": [
                "flax.linen as nn",
                "MaxText.layers.linears"
            ],
            "parameters": {
                "intermediate_size_for_vit": "The input feature dimension for the first linear layer.",
                "projector_input_dim_for_vit": "The output feature dimension of the first linear layer and input to the second.",
                "projector_output_dim_for_vit": "The final output feature dimension of the MLP.",
                "projector_dropout_for_vit": "The dropout rate applied between the two linear layers.",
                "dtype_mm": "The data type for the linear layers.",
                "matmul_precision": "The precision for matrix multiplication in the linear layers."
            },
            "notes": [
                "This module is a component of `Llama4VisionPixelShuffleMLP`.",
                "Both linear layers are created without a bias term (`use_bias=False`).",
                "The GELU activation used is non-approximate (`approximate=False`)."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the two dense layers (fc1, fc2) and the dropout layer based on the model configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initialize the first dense layer (`fc1`) using `linears.dense_general`.",
                        "Initialize the second dense layer (`fc2`) using `linears.dense_general`.",
                        "Initialize the dropout layer (`dropout`) with a rate from the config."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "linears.dense_general",
                        "nn.Dropout"
                    ],
                    "notes": [
                        "The layers are configured with dimensions, data types, and precision specified in the `config` object."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the two-layer MLP transformation with dropout to the input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, config.intermediate_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Apply the first linear transformation (`fc1`) to `hidden_states`.",
                        "Apply a non-approximate GELU activation function.",
                        "Apply the dropout layer, controlled by the `deterministic` flag.",
                        "Apply the second linear transformation (`fc2`).",
                        "Apply a second non-approximate GELU activation function."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, config.projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "nn.gelu"
                    ],
                    "notes": [
                        "Dropout is disabled when `deterministic` is True, which is typical during inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionPixelShuffleMLP",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionPixelShuffleMLP(nn.Module):\n  \"\"\"Implementation of Llama4VisionPixelShuffleMLP for Llama4 Multi modal model.\n\n  This module applies pixel shuffle operation and MLP to encoded patches.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n\n  def setup(self):\n    cfg = self.config\n    self.pixel_shuffle_ratio = cfg.pixel_shuffle_ratio_for_vit\n    self.pixel_shuffle_mlp = Llama4VisionMLP2(cfg)\n\n  def __call__(self, encoded_patches: Array, deterministic: bool = False) -> Array:\n    \"\"\"Apply pixel shuffle and MLP to encoded patches.\n\n    Args:\n      encoded_patches: Input tensor of shape [batch_size, num_patches, hidden_size]\n      deterministic: If True, disables dropout during inference\n\n    Returns:\n      Tensor of shape [batch_size, num_patches, hidden_size]\n    \"\"\"\n    # Apply pixel shuffle operation\n    encoded_patches = pixel_shuffle(encoded_patches, self.pixel_shuffle_ratio)\n\n    # Apply MLP transformation\n    result = self.pixel_shuffle_mlp(encoded_patches, deterministic=deterministic)\n\n    return result",
        "analysis": {
            "module_type": "vision_pixel_shuffle_mlp",
            "purpose": "Applies a pixel shuffle operation followed by an MLP transformation to encoded vision patches.",
            "input": {
                "shape": "[batch_size, num_patches, hidden_size]",
                "dtype": "Array"
            },
            "processing_steps": [
                "Applies a pixel shuffle operation to the input `encoded_patches` using the configured `pixel_shuffle_ratio`.",
                "Passes the shuffled patches through a two-layer MLP (`Llama4VisionMLP2`) for feature transformation."
            ],
            "output": {
                "shape": "[batch_size, new_num_patches, projector_output_dim_for_vit]"
            },
            "dependencies": [
                "nn.Module",
                "pixel_shuffle",
                "Llama4VisionMLP2"
            ],
            "parameters": {
                "pixel_shuffle_ratio_for_vit": "The ratio used in the pixel shuffle operation to rearrange patch data."
            },
            "notes": [
                "The output shape's patch dimension (`new_num_patches`) and feature dimension are altered by the pixel shuffle and MLP, respectively.",
                "This module combines a spatial data rearrangement with a feature projection."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the pixel shuffle ratio and the MLP submodule from the configuration.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Retrieves `pixel_shuffle_ratio_for_vit` from the config and assigns it to `self.pixel_shuffle_ratio`.",
                        "Instantiates the `Llama4VisionMLP2` module and assigns it to `self.pixel_shuffle_mlp`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Llama4VisionMLP2"
                    ],
                    "notes": [
                        "This is a standard Flax setup method that prepares the module's parameters and submodules before execution."
                    ]
                },
                "__call__": {
                    "purpose": "Executes the forward pass by applying the pixel shuffle operation and then the MLP.",
                    "input": {
                        "shape": "[batch_size, num_patches, hidden_size]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Calls the `pixel_shuffle` function on the input `encoded_patches`.",
                        "Passes the result through the `self.pixel_shuffle_mlp` module."
                    ],
                    "output": {
                        "shape": "[batch_size, new_num_patches, projector_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "pixel_shuffle",
                        "Llama4VisionMLP2"
                    ],
                    "notes": [
                        "The `deterministic` flag is passed to the MLP submodule to control its dropout behavior."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4MultiModalProjector",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4MultiModalProjector(nn.Module):\n  \"\"\"Implementation of Llama4MultiModalProjector for Llama4 Multi modal model.\n\n  This module projects vision features to text hidden dimension.\n\n  Attributes:\n    config: Config containing model parameters\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    cfg = self.config\n    self.linear = linears.dense_general(\n        in_features_shape=cfg.vision_output_dim_for_vit,\n        out_features_shape=cfg.base_emb_dim,\n        dtype=cfg.dtype_mm,\n        name=\"vit_multi_modal_projector\",\n        use_bias=False,\n        matmul_precision=cfg.matmul_precision,\n    )\n\n  def __call__(self, image_features: Array) -> Array:\n    \"\"\"Project image features to text hidden dimension.\n\n    Args:\n      image_features: Input tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_output_dim]\n\n    Returns:\n      Tensor of shape [batch_size, num_patches, (pixel_shuffle_ratio**2), vision_hidden_size]\n    \"\"\"\n    b, t, c, d = image_features.shape\n    image_features = image_features.reshape(b * t, c, d)\n    hidden_states = self.linear(image_features)\n    _, c, d = hidden_states.shape\n    hidden_states = hidden_states.reshape(b, t, c, d)\n    return hidden_states",
        "analysis": {
            "module_type": "multi_modal_projector",
            "purpose": "Projects vision features from the vision model's output dimension to the text model's hidden dimension using a linear transformation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a dense linear layer in the `setup` method.",
                "The `__call__` method reshapes the input image features, applies the linear projection, and then reshapes the result back to the original batch and patch dimensions."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.linen.Module",
                "maxtext.layers.linears.dense_general",
                "Config",
                "Mesh"
            ],
            "parameters": {
                "config.vision_output_dim_for_vit": "The input feature dimension from the vision encoder.",
                "config.base_emb_dim": "The target output feature dimension, corresponding to the text model's embedding dimension.",
                "config.dtype_mm": "The data type for the linear projection layer.",
                "config.matmul_precision": "The precision for the matrix multiplication in the linear layer."
            },
            "notes": [
                "This module acts as a bridge between the vision and language components of a multi-modal model.",
                "The linear projection is configured without a bias."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the linear projection layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Initializes a `dense_general` linear layer with parameters from the `config` object."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "maxtext.layers.linears.dense_general"
                    ],
                    "notes": [
                        "The layer is named 'vit_multi_modal_projector' and does not use a bias."
                    ]
                },
                "__call__": {
                    "purpose": "Projects the input image features into the text embedding space.",
                    "input": {
                        "shape": "[batch_size, num_patches, channels, vision_output_dim]",
                        "dtype": "Array (float type inferred from config.dtype_mm)"
                    },
                    "processing_steps": [
                        "Reshape the input tensor from [b, t, c, d] to [b*t, c, d].",
                        "Apply the linear projection layer (`self.linear`).",
                        "Reshape the output tensor back to [b, t, c, d_out]."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches, channels, base_emb_dim]"
                    },
                    "dependencies": [],
                    "notes": [
                        "The reshaping operation allows for efficient application of the linear layer across the batch and patch dimensions."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_nope_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_nope_layer(layer_id: int, nope_layer_interval: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` should use RoPE or not (NoPE).\n\n  Args:\n    layer_id: The index of the layer.\n    nope_layer_interval: The interval at which layers should use NoPE.\n\n  Returns:\n    True if the layer should use NoPE, False otherwise.\n  \"\"\"\n  return nope_layer_interval is not None and nope_layer_interval > 0 and (layer_id + 1) % nope_layer_interval == 0",
        "analysis": {
            "functionality": "Checks if a given layer, identified by its index, should be a 'NoPE' (No Position Embedding) layer based on a specified interval. It returns true if the 1-based layer index is a multiple of the interval.",
            "usage": "Input the 0-indexed `layer_id` (integer) and the `nope_layer_interval` (integer). The function returns a boolean: `True` if the layer is a NoPE layer, `False` otherwise. The interval must be a positive integer for the check to succeed."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#determine_is_moe_layer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "def determine_is_moe_layer(layer_id: int, interleave_moe_layer_step: int) -> bool:\n  \"\"\"\n  Determines whether the given layer at `layer_id` is MoE layer.\n\n  This function implements a striding pattern. For example:\n  - If moe_layer_stride is 1, all layers are MoE layers.\n  - If moe_layer_stride is 2, layers with index 1, 3, 5, ... are MoE layers.\n\n  Args:\n    layer_id: The 0-based index of the layer being checked.\n    interleave_moe_layer_step: The interval or stride for placing MoE layers.\n\n  Returns:\n    True if the layer is MoE layer, False otherwise.\n  \"\"\"\n  return (\n      interleave_moe_layer_step is not None\n      and interleave_moe_layer_step > 0\n      and (layer_id + 1) % interleave_moe_layer_step == 0\n  )",
        "analysis": {
            "module_type": "layer_type_determiner",
            "purpose": "Determines if a given layer, identified by its index, should be a Mixture-of-Experts (MoE) layer based on a specified stride or interval.",
            "input": {
                "shape": "N/A",
                "dtype": "int"
            },
            "processing_steps": [
                "Check if `interleave_moe_layer_step` is not None and is greater than 0.",
                "Check if `(layer_id + 1)` is perfectly divisible by `interleave_moe_layer_step` using the modulo operator.",
                "Return `True` if both conditions are met, otherwise return `False`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "layer_id": "The 0-based index of the layer being checked.",
                "interleave_moe_layer_step": "The interval or stride for placing MoE layers. If None or not positive, MoE is disabled for all layers."
            },
            "notes": [
                "This function implements a striding pattern. For example, if `interleave_moe_layer_step` is 2, layers with indices 1, 3, 5, etc., will be designated as MoE layers.",
                "The logic uses `layer_id + 1`, effectively treating the layer counting as 1-based for the modulo operation."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4DecoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4DecoderLayer(nn.Module):\n  \"\"\"Transformer decoder layer for Llama4.\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    quant: None | Quant, quantization config\n    is_nope_layer: bool, whether to use RoPE or not on this layer\n    is_moe_layer: bool, whether this layer operates as a MoE layer\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  is_nope_layer: bool = False\n  is_moe_layer: bool = False\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      bidirectional_mask=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n    cfg = self.config\n    mesh = self.mesh\n\n    assert cfg.num_experts >= 1, \"Expected the Llama4 config to have `num_experts > 1`.\"\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    lnx_rms = rms_norm(\n        num_features=inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"pre_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )\n    lnx = lnx_rms(inputs)\n\n    lnx = nn.with_logical_constraint(lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    # Instead of scaling the query values in the checkpoint conversion (`llama_or_mistral_ckpt`)\n    # we'll do it dynamically in the forward pass of Attention\n    query_pre_attn_scalar = cfg.head_dim**-0.5\n\n    # Self-attention block\n    attention_layer = attentions.attention_as_linen(\n        config=cfg,\n        num_query_heads=cfg.num_query_heads,\n        num_kv_heads=cfg.num_kv_heads,\n        head_dim=cfg.head_dim,\n        max_target_length=cfg.max_target_length,\n        max_prefill_predict_length=cfg.max_prefill_predict_length,\n        attention_kernel=cfg.attention,\n        inputs_q_shape=lnx.shape,\n        inputs_kv_shape=lnx.shape,\n        mesh=mesh,\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        dropout_rate=cfg.dropout_rate,\n        name=\"self_attention\",\n        float32_qk_product=cfg.float32_qk_product,\n        float32_logits=cfg.float32_logits,\n        quant=self.quant,\n        kv_quant=quantizations.configure_kv_quant(cfg),\n        prefill_cache_axis_order=tuple(map(int, cfg.prefill_cache_axis_order.split(\",\"))),\n        ar_cache_axis_order=tuple(map(int, cfg.ar_cache_axis_order.split(\",\"))),\n        compute_axis_order=tuple(map(int, cfg.compute_axis_order.split(\",\"))),\n        reshape_q=cfg.reshape_q,\n        use_ragged_attention=cfg.use_ragged_attention,\n        ragged_block_size=cfg.ragged_block_size,\n        is_nope_layer=self.is_nope_layer,\n        use_qk_norm=cfg.use_qk_norm,\n        query_pre_attn_scalar=query_pre_attn_scalar,\n        temperature_tuning=cfg.temperature_tuning,\n        temperature_tuning_scale=0.1,\n        temperature_tuning_floor_scale=8192.0,\n        # note: chunk_attn_window_size is set in the config\n        attention_type=AttentionType.GLOBAL if self.is_nope_layer else AttentionType.CHUNK,\n        model_mode=model_mode,\n    )\n\n    attention_lnx = attention_layer(\n        lnx,\n        lnx,\n        decoder_positions,\n        decoder_segment_ids=decoder_segment_ids,\n        deterministic=deterministic,\n        model_mode=model_mode,\n        slot=slot,\n        page_state=page_state,\n        previous_chunk=previous_chunk,\n        bidirectional_mask=bidirectional_mask,\n    )\n\n    attention_lnx = nn.with_logical_constraint(\n        attention_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n    intermediate_inputs = inputs + attention_lnx\n\n    # Fully Connected\n    hidden_states = rms_norm(\n        num_features=intermediate_inputs.shape[-1],\n        dtype=cfg.dtype,\n        weight_dtype=cfg.weight_dtype,\n        name=\"post_self_attention_layer_norm\",\n        kernel_axes=(\"norm\",),\n        epsilon=cfg.normalization_layer_epsilon,\n    )(intermediate_inputs)\n    hidden_states = nn.with_logical_constraint(\n        hidden_states, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\")\n    )\n\n    load_balance_loss = None\n    if self.is_moe_layer:\n      # NOTE: the naming mismatch here is to ensure reverse compatibility with existing checkpoints.\n      # The `name` represents the weight name in JAX/checkpoints and so the class name\n      # is just for readability.\n      mlp_lnx = moe.get_routed_and_shared_moe(\n          name=\"Llama4MoEBlock_0\",\n          config=cfg,\n          mesh=self.mesh,\n          kernel_init=initializers.nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n          kernel_axes=(\"embed\", None),\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          quant=self.quant,\n      )(hidden_states)\n    else:\n      mlp_lnx = mlp_block(\n          in_features=hidden_states.shape[-1],\n          intermediate_dim=cfg.mlp_dim,\n          activations=cfg.mlp_activations,\n          intermediate_dropout_rate=cfg.dropout_rate,\n          dtype=cfg.dtype,\n          weight_dtype=cfg.weight_dtype,\n          name=\"mlp\",\n          config=cfg,\n          quant=self.quant,\n      )(hidden_states, deterministic=deterministic)\n    mlp_lnx = nn.with_logical_constraint(mlp_lnx, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n\n    layer_output = mlp_lnx + intermediate_inputs\n\n    layer_output = nn.Dropout(rate=cfg.dropout_rate, broadcast_dims=(-2,))(layer_output, deterministic=deterministic)\n\n    layer_output = nn.with_logical_constraint(\n        layer_output,\n        (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"),\n    )\n\n    # NOTE: this is only needed for dropping MoE\n    if load_balance_loss is not None:\n      self.sow(\"intermediates\", \"moe_lb_loss\", load_balance_loss)\n\n    if cfg.record_internal_nn_metrics:\n      self.sow(\"intermediates\", \"activation_mean\", jnp.mean(layer_output))\n      self.sow(\"intermediates\", \"activation_stdev\", jnp.std(layer_output))\n      self.sow(\n          \"intermediates\",\n          \"activation_fraction_zero\",\n          jnp.sum(layer_output == 0) / jnp.size(layer_output),\n      )\n\n    if cfg.scan_layers:\n      return layer_output, None\n    else:\n      return layer_output",
        "analysis": {
            "functionality": "The `Llama4DecoderLayer` class implements a single transformer decoder layer for the Llama4 model. It performs self-attention followed by a feed-forward network (either a standard MLP or a Mixture-of-Experts block), with residual connections and RMS normalization. This layer is a fundamental building block of the Llama4 language model.",
            "usage": "This class is intended to be used as a layer within a larger transformer model. To use it, instantiate the class with a configuration object (`Config`), a JAX device mesh (`Mesh`), and optional flags for quantization, NoPE (No Position Embedding), and MoE (Mixture of Experts). The forward pass is executed by calling the instance with an input tensor of shape `[batch_size, sequence_length, hidden_dim]`, along with segment IDs, positions, and other control flags. The layer returns an output tensor of the same shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4ScannableBlock",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4ScannableBlock(nn.Module):\n  '''\n  A repeatable block given nope_layer_interval and interleave_moe_layer_step\n\n  Attributes:\n    config: Config, MaxText model config\n    mesh: Mesh, JAX device mesh (used for sharding)\n    quant: None | Quant, quantization config\n    nope_layer_interval: int, the interval at which layers should use NoPE.\n    interleave_moe_layer_step: int, the interval or stride for placing MoE layers.\n  \"\"\"\n  '''\n\n  config: Config\n  mesh: Mesh\n  model_mode: str\n  quant: None | Quant = None\n  nope_layer_interval: int = 1\n  interleave_moe_layer_step: int = 1\n\n  @nn.compact\n  def __call__(\n      self,\n      inputs,\n      decoder_segment_ids,\n      decoder_positions,\n      deterministic,\n      model_mode,\n      bidirectional_mask=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n      previous_chunk=None,\n  ):\n\n    cfg = self.config\n    mesh = self.mesh\n\n    inputs = nn.with_logical_constraint(inputs, (\"activation_batch\", \"activation_norm_length\", \"activation_embed\"))\n    inputs = checkpoint_name(inputs, \"decoder_layer_input\")\n    y = inputs\n    for layer_id in range(cfg.inhomogeneous_layer_cycle_interval):\n      nope_layer = determine_is_nope_layer(layer_id, self.nope_layer_interval)\n      moe_layer = determine_is_moe_layer(layer_id, self.interleave_moe_layer_step)\n      layer = Llama4DecoderLayer(\n          config=cfg,\n          mesh=mesh,\n          name=f\"layers_{layer_id}\",\n          quant=self.quant,\n          model_mode=model_mode,\n          is_nope_layer=nope_layer,\n          is_moe_layer=moe_layer,\n      )\n      y = layer(\n          y,\n          decoder_segment_ids,\n          decoder_positions,\n          deterministic,\n          model_mode,\n          previous_chunk=previous_chunk,\n          page_state=page_state,\n          slot=slot,\n          bidirectional_mask=bidirectional_mask,\n      )\n      if cfg.scan_layers:\n        y = y[0]\n    if cfg.scan_layers:\n      return y, None\n    else:\n      return y",
        "analysis": {
            "module_type": "llama_scannable_block",
            "purpose": "A container module that sequentially applies a configurable number of Llama4DecoderLayer instances, supporting heterogeneous layer configurations (NoPE, MoE) and compatibility with Flax's scan transformation.",
            "input": {
                "shape": "[batch_size, sequence_length, hidden_dim]",
                "dtype": "Depends on config.dtype (e.g., float32, bfloat16)"
            },
            "processing_steps": [
                "Initializes class attributes such as config, mesh, model_mode, quant, nope_layer_interval, and interleave_moe_layer_step."
            ],
            "output": {
                "shape": "Returns a tensor of shape [batch_size, sequence_length, hidden_dim]. If config.scan_layers is true, it returns a tuple: (tensor, None)."
            },
            "dependencies": [
                "flax.linen.Module",
                "Llama4DecoderLayer",
                "determine_is_nope_layer",
                "determine_is_moe_layer"
            ],
            "parameters": {
                "inhomogeneous_layer_cycle_interval": "The number of decoder layers to apply within this block.",
                "nope_layer_interval": "The interval at which layers should disable RoPE (NoPE).",
                "interleave_moe_layer_step": "The interval or stride for placing Mixture-of-Experts (MoE) layers.",
                "scan_layers": "A boolean flag that, if true, makes the module's call signature compatible with `flax.linen.scan`."
            },
            "notes": [
                "This module acts as a building block for constructing the full decoder stack, especially when using `scan` for memory efficiency.",
                "It dynamically configures each internal `Llama4DecoderLayer` as a standard, NoPE, or MoE layer based on its index within the block."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Applies a sequence of `Llama4DecoderLayer` instances to the input tensor.",
                    "input": {
                        "shape": "inputs: [batch_size, sequence_length, hidden_dim], decoder_segment_ids: [batch_size, sequence_length], decoder_positions: [batch_size, sequence_length]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Apply a logical constraint and a checkpoint name to the input tensor.",
                        "Initialize the output tensor `y` with the input tensor.",
                        "Iterate `config.inhomogeneous_layer_cycle_interval` times.",
                        "In each iteration, determine if the layer is a NoPE layer using `determine_is_nope_layer`.",
                        "In each iteration, determine if the layer is an MoE layer using `determine_is_moe_layer`.",
                        "Instantiate and apply a `Llama4DecoderLayer` with the determined configuration to the current tensor `y`.",
                        "If `config.scan_layers` is true, unpack the output tuple from the layer.",
                        "Return the final tensor `y`, potentially wrapped in a tuple for scan compatibility."
                    ],
                    "output": {
                        "shape": "Returns `[batch_size, sequence_length, hidden_dim]` if `config.scan_layers` is false, otherwise returns a tuple `([batch_size, sequence_length, hidden_dim], None)`."
                    },
                    "dependencies": [
                        "Llama4DecoderLayer",
                        "determine_is_nope_layer",
                        "determine_is_moe_layer",
                        "jax.ad_checkpoint.checkpoint_name"
                    ],
                    "notes": [
                        "The return signature depends on the `config.scan_layers` flag to support `flax.linen.scan`.",
                        "It forwards multiple arguments like `decoder_segment_ids`, `page_state`, etc., to the internal `Llama4DecoderLayer`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoderLayer",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoderLayer(nn.Module):\n  \"\"\"Transformer encoder layer for Llama4 vision model.\"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  @nn.compact\n  def __call__(\n      self,\n      hidden_states: Array,\n      deterministic: bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the vision encoder layer.\n\n    Args:\n      hidden_states: Input hidden states\n      deterministic: Whether to use deterministic mode\n\n    Returns:\n      Output hidden states\n    \"\"\"\n    # Self Attention\n    residual = hidden_states\n\n    # Input layer norm\n    hidden_states = nn.LayerNorm(name=\"input_layer_norm\", epsilon=1e-5)(hidden_states)\n\n    # Self attention\n    attention_layer = attentions.attention_as_linen(\n        config=self.config,\n        num_query_heads=self.config.num_attention_heads_for_vit,\n        num_kv_heads=self.config.num_attention_heads_for_vit,\n        head_dim=self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit,\n        max_target_length=(self.config.image_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1,\n        attention_kernel=\"dot_product\",\n        inputs_q_shape=hidden_states.shape,\n        inputs_kv_shape=hidden_states.shape,\n        float32_qk_product=self.config.float32_qk_product,\n        float32_logits=self.config.float32_logits,\n        mesh=self.mesh,\n        dropout_rate=0,\n        name=\"self_attention_vision\",\n        attention_type=AttentionType.FULL,\n        is_nope_layer=False,\n        use_bias_in_projections=True,\n        is_vision=True,\n        use_qk_norm=False,\n        query_pre_attn_scalar=1 / math.sqrt(self.config.hidden_size_for_vit // self.config.num_attention_heads_for_vit),\n        # The vision encoder processes an image in a single forward pass to produce\n        # embeddings. It doesn't have the concept of \"prefill\" and \"autoregressive\"\n        # steps that a text decoder has. Therefore, it doesn't need a KV cache for\n        # its self-attention mechanism.\n        model_mode=MODEL_MODE_TRAIN,\n    )\n\n    hidden_states = attention_layer(\n        inputs_q=hidden_states,\n        inputs_kv=hidden_states,\n        deterministic=deterministic,\n    )\n\n    hidden_states = residual + hidden_states\n\n    residual = hidden_states\n\n    # Post attention layer norm\n    hidden_states = nn.LayerNorm(name=\"post_attention_layer_norm\", epsilon=1e-5)(hidden_states)\n\n    # MLP\n    mlp = Llama4VisionMLP(self.config)\n    hidden_states = mlp(hidden_states)\n\n    hidden_states = residual + hidden_states\n\n    return hidden_states",
        "analysis": {
            "functionality": "The `Llama4VisionEncoderLayer` class implements a single transformer encoder layer for a vision model. It processes input embeddings through a self-attention mechanism and a multi-layer perceptron (MLP), using pre-layer normalization and residual connections, which is a standard transformer block architecture.",
            "usage": "This class is intended to be used as a building block within a larger vision encoder. To use it, instantiate the class with a configuration object (`Config`) and a JAX mesh (`Mesh`). Then, call the instance with an input tensor of shape `[batch_size, num_patches + 1, hidden_dim]` representing image patch embeddings plus a class token. The `deterministic` flag should be set to `True` during inference to disable dropout. The layer returns an output tensor of the same shape."
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionEncoder",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionEncoder(nn.Module):\n  \"\"\"Transformer encoder consisting of multiple Llama4VisionEncoderLayer layers.\n\n  This encoder is based on the PyTorch reference implementation and uses multiple\n  encoder layers to process vision input.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  @nn.compact\n  def __call__(\n      self,\n      hidden_states: Array,\n      deterministic: bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the vision encoder.\n\n    Args:\n      hidden_states: Input hidden states\n      deterministic: Whether to use deterministic mode (disables dropout)\n\n    Returns:\n      Final hidden states\n    \"\"\"\n    cfg = self.config\n\n    # Iterate through encoder layers (non-scan version)\n    for layer_idx in range(cfg.num_hidden_layers_for_vit):\n\n      # TODO\uff1a add scan version\n      layer = Llama4VisionEncoderLayer(config=cfg, mesh=self.mesh, name=f\"layers_{layer_idx}\")\n\n      hidden_states = layer(\n          hidden_states=hidden_states,\n          deterministic=deterministic,\n      )\n\n    return hidden_states",
        "analysis": {
            "module_type": "vision_transformer_encoder",
            "purpose": "A transformer encoder that processes vision embeddings by sequentially applying multiple Llama4VisionEncoderLayer layers.",
            "input": {
                "shape": "[batch_size, num_patches + 1, hidden_size_for_vit]",
                "dtype": "Array"
            },
            "processing_steps": [
                "Iterates a configured number of times (cfg.num_hidden_layers_for_vit).",
                "In each iteration, instantiates and calls a Llama4VisionEncoderLayer.",
                "Passes the output of one layer as the input to the next."
            ],
            "output": {
                "shape": "[batch_size, num_patches + 1, hidden_size_for_vit]"
            },
            "dependencies": [
                "flax.linen.nn",
                "Llama4VisionEncoderLayer"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters like `num_hidden_layers_for_vit`.",
                "mesh": "The JAX device mesh used for model sharding."
            },
            "notes": [
                "The current implementation uses a standard Python for-loop for iteration, with a TODO note to add a more JAX-idiomatic `scan` version.",
                "This encoder is based on a PyTorch reference implementation."
            ],
            "methods": {
                "__call__": {
                    "purpose": "Performs the forward pass of the vision encoder, processing input hidden states through all encoder layers.",
                    "input": {
                        "shape": "[batch_size, num_patches + 1, hidden_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Iterate from layer 0 to `cfg.num_hidden_layers_for_vit - 1`.",
                        "Create an instance of `Llama4VisionEncoderLayer` for the current layer index.",
                        "Call the layer with the current `hidden_states` and the `deterministic` flag.",
                        "Update `hidden_states` with the output from the layer.",
                        "Return the final `hidden_states` after the loop."
                    ],
                    "output": {
                        "shape": "[batch_size, num_patches + 1, hidden_size_for_vit]"
                    },
                    "dependencies": [
                        "Llama4VisionEncoderLayer"
                    ],
                    "notes": [
                        "The `deterministic` parameter controls whether dropout is enabled within the sub-layers."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/llama4.py#Llama4VisionModel",
        "file_path": "src/MaxText/layers/llama4.py",
        "code_block": "class Llama4VisionModel(nn.Module):\n  \"\"\"Llama4 vision model for processing image inputs.\n\n  This model extracts patches from input image tiles and processes them\n  through Llama4VisionEncoder and other vision-specific layers.\n\n  Attributes:\n    config: Config containing model parameters\n    mesh: Mesh, JAX device mesh (used for sharding)\n  \"\"\"\n\n  config: Config\n  mesh: Mesh\n\n  def setup(self):\n    self.scale = self.config.hidden_size_for_vit**-0.5\n    self.num_patches = (self.config.tile_size_for_vit // self.config.patch_size_for_vit) ** 2 + 1\n    self.class_embedding = self.param(\n        \"class_embedding\",\n        nn.initializers.normal(stddev=self.scale, dtype=self.config.dtype_mm),\n        (self.config.hidden_size_for_vit,),\n    )\n    self.positional_embedding_vlm = self.param(\n        \"positional_embedding_vlm\",\n        nn.initializers.normal(stddev=self.scale, dtype=self.config.dtype_mm),\n        (self.num_patches, self.config.hidden_size_for_vit),\n    )\n\n  @nn.compact\n  def __call__(\n      self,\n      pixel_values: Array,\n      output_attentions: None | bool = None,\n      output_hidden_states: None | bool = None,\n      return_dict: None | bool = None,\n      deterministic: None | bool = False,\n  ) -> Array:\n    \"\"\"Forward pass of the Llama4 vision model.\n\n    Args:\n      inputs: Input tensor of shape [batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]\n      deterministic: Whether to use deterministic mode (disables dropout)\n\n    Returns:\n      Final hidden states from the vision encoder of shape [batch_size, num_tiles, num_patches, vision_output_dim_for_vit]\n    \"\"\"\n    cfg = self.config\n    mesh = self.mesh\n\n    b, t, c, h, w = pixel_values.shape\n    pixel_values = jnp.reshape(pixel_values, [b * t, c, h, w])\n\n    # Unfold convolution to extract patches\n    hidden_states = Llama4UnfoldConvolution(config=cfg)(pixel_values)\n\n    # Add class embedding to the beginning of the sequence\n    class_embedding_expanded = jnp.expand_dims(jnp.expand_dims(self.class_embedding, axis=0), axis=0)\n    class_embedding = jnp.broadcast_to(class_embedding_expanded, (hidden_states.shape[0], 1, cfg.hidden_size_for_vit))\n    hidden_states = jnp.concatenate([class_embedding, hidden_states], axis=1)\n\n    # Add positional embedding\n    hidden_states += self.positional_embedding_vlm\n\n    # Transformation layers\n    hidden_states = nn.LayerNorm(name=\"layernorm_pre\")(hidden_states)\n    hidden_states = Llama4VisionEncoder(config=cfg, mesh=mesh)(hidden_states)\n    hidden_states = nn.LayerNorm(name=\"layernorm_post\")(hidden_states)\n    hidden_states = hidden_states[:, :-1, :]\n\n    hidden_states = Llama4VisionPixelShuffleMLP(config=cfg)(hidden_states)\n\n    # Reshape hidden states\n    _, patch_num, patch_dim = hidden_states.shape\n    hidden_states = jnp.reshape(hidden_states, [b, t, patch_num, patch_dim])\n\n    return hidden_states",
        "analysis": {
            "module_type": "llama4_vision_model",
            "purpose": "A vision model that extracts patches from input image tiles, processes them through a vision encoder, and applies further transformations to generate image features.",
            "input": {
                "shape": "[batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                "dtype": "float (configurable via config.dtype_mm)"
            },
            "processing_steps": [
                "Reshapes the input pixel values by collapsing the batch and tile dimensions.",
                "Extracts patches from the reshaped images using `Llama4UnfoldConvolution`.",
                "Prepends a learnable class embedding to the sequence of patches.",
                "Adds a learnable positional embedding to the sequence.",
                "Applies pre-encoder layer normalization.",
                "Processes the sequence through the `Llama4VisionEncoder`.",
                "Applies post-encoder layer normalization.",
                "Removes the output corresponding to the class token.",
                "Transforms the features using `Llama4VisionPixelShuffleMLP`.",
                "Reshapes the final hidden states to reintroduce the tile dimension."
            ],
            "output": {
                "shape": "[batch_size, num_tiles, num_patches, vision_output_dim_for_vit]"
            },
            "dependencies": [
                "flax.linen.nn",
                "jax.numpy",
                "Llama4UnfoldConvolution",
                "Llama4VisionEncoder",
                "Llama4VisionPixelShuffleMLP"
            ],
            "parameters": {
                "config": "A configuration object containing model parameters such as hidden sizes, patch sizes, tile sizes, and data types.",
                "mesh": "A JAX device mesh used for model sharding and parallel computation."
            },
            "notes": [
                "This model is designed to process images that are divided into multiple tiles.",
                "A learnable class embedding is prepended to the patch sequence before the encoder, and its corresponding output is removed after the encoder."
            ],
            "methods": {
                "setup": {
                    "purpose": "Initializes the model's learnable parameters, including the class embedding and positional embeddings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calculate the standard deviation (`scale`) for weight initialization.",
                        "Calculate the number of patches per tile, including one for the class embedding.",
                        "Initialize the `class_embedding` parameter.",
                        "Initialize the `positional_embedding_vlm` parameter."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.linen.nn.initializers.normal"
                    ],
                    "notes": [
                        "The number of positional embeddings is determined by the number of patches plus one for the class token."
                    ]
                },
                "__call__": {
                    "purpose": "Performs the forward pass of the vision model, transforming raw pixel values into encoded image features.",
                    "input": {
                        "shape": "[batch_size, num_tiles, num_channels_for_vit, tile_size_for_vit, tile_size_for_vit]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Reshape the input tensor from [b, t, c, h, w] to [b*t, c, h, w].",
                        "Extract image patches using `Llama4UnfoldConvolution`.",
                        "Broadcast and concatenate a class embedding to the beginning of the patch sequence.",
                        "Add the positional embedding to the combined sequence.",
                        "Apply a pre-encoder Layer Normalization (`layernorm_pre`).",
                        "Process the sequence through the `Llama4VisionEncoder`.",
                        "Apply a post-encoder Layer Normalization (`layernorm_post`).",
                        "Remove the output corresponding to the class token from the sequence.",
                        "Apply the `Llama4VisionPixelShuffleMLP` transformation.",
                        "Reshape the final tensor back to [b, t, patch_num, patch_dim]."
                    ],
                    "output": {
                        "shape": "[batch_size, num_tiles, num_patches, vision_output_dim_for_vit]"
                    },
                    "dependencies": [
                        "jax.numpy",
                        "Llama4UnfoldConvolution",
                        "Llama4VisionEncoder",
                        "flax.linen.nn.LayerNorm",
                        "Llama4VisionPixelShuffleMLP"
                    ],
                    "notes": [
                        "The `deterministic` argument controls dropout behavior in sub-modules.",
                        "The model processes all tiles for all batch items simultaneously by collapsing the batch and tile dimensions before the main processing pipeline."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_convert_to_activation_function",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _convert_to_activation_function(fn_or_string: str | Callable[..., Any]) -> Callable[..., Any]:\n  \"\"\"Convert a string to an activation function.\"\"\"\n  if fn_or_string == \"linear\":\n    return lambda x: x\n  elif isinstance(fn_or_string, str):\n    return getattr(nn, fn_or_string)\n  elif callable(fn_or_string):\n    return fn_or_string\n  else:\n    raise ValueError(\n        f\"\"\"Don't know how to convert {fn_or_string}\n                         to an activation function\"\"\"\n    )",
        "analysis": {
            "functionality": "This function converts a given string or callable into a corresponding activation function. It handles 'linear' as a special case, retrieves standard activation functions from `flax.linen.nn` by name, and passes through callables directly.",
            "usage": "Call this function with a string representing an activation function name (e.g., 'relu', 'gelu', 'linear') or an existing callable function. It will return a callable activation function. For example, `activation_fn = _convert_to_activation_function('relu')`."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#normalize_axes",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def normalize_axes(axes: Iterable[int], ndim: int) -> tuple[int, ...]:\n  # A tuple by convention. len(axes_tuple) then also gives the rank efficiently.\n  return tuple(ax if ax >= 0 else ndim + ax for ax in axes)",
        "analysis": {
            "functionality": "Converts an iterable of axis indices, which may include negative values, into a tuple of their positive equivalents based on the total number of dimensions.",
            "usage": "Call this function with an iterable of integer axes and the total number of dimensions (`ndim`) of the array. It returns a tuple of normalized, non-negative axis indices. For example, `normalize_axes(axes=[-1, 1], ndim=4)` returns `(3, 1)`."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#canonicalize_tuple",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def canonicalize_tuple(x):\n  if isinstance(x, Iterable):\n    return tuple(x)\n  else:\n    return (x,)",
        "analysis": {
            "module_type": "tuple_canonicalization",
            "purpose": "Ensures an input value is a tuple by converting iterables to tuples or wrapping single, non-iterable elements in a tuple.",
            "input": {
                "shape": "N/A",
                "dtype": "Any"
            },
            "processing_steps": [
                "Check if the input `x` is an instance of `Iterable`.",
                "If it is an `Iterable`, convert it to a tuple.",
                "If it is not an `Iterable`, wrap it in a single-element tuple."
            ],
            "output": {
                "shape": "A tuple of variable length."
            },
            "dependencies": [
                "typing.Iterable"
            ],
            "parameters": {},
            "notes": [
                "This is a utility function to standardize inputs that can be either a single value or a sequence of values into a consistent tuple format."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general(inputs, kernel, kernel_axes, axis, contract_ind, matmul_precision, quant):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant:\n    dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n    dot_general = dot_general_cls()\n    return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "module_type": "generalized_dot_product",
            "purpose": "Computes a `lax.dot_general` operation, with an optional path for quantization.",
            "input": {
                "shape": "inputs: [..., axis_dim, ...], kernel: [..., contract_ind_dim, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Set the matmul precision using `lax.Precision`.",
                "Check if a quantization object `quant` is provided.",
                "If `quant` is provided, instantiate and call its `dot_general_cls` for a quantized matrix multiplication.",
                "Otherwise, call the standard `lax.dot_general` with the specified precision.",
                "Return the result of the dot_general operation."
            ],
            "output": {
                "shape": "The shape resulting from the dot_general operation on the non-contracting dimensions of the input tensors."
            },
            "dependencies": [
                "jax.lax.dot_general",
                "jax.lax.Precision",
                "quantizations.AqtQuantization"
            ],
            "parameters": {
                "inputs": "The first input tensor for the dot product.",
                "kernel": "The second input tensor (e.g., weight matrix) for the dot product.",
                "kernel_axes": "Logical axes for partitioning the kernel, used for quantization.",
                "axis": "The contracting dimension(s) of the `inputs` tensor.",
                "contract_ind": "The contracting dimension(s) of the `kernel` tensor.",
                "matmul_precision": "A string specifying the precision for the matrix multiplication (e.g., 'default').",
                "quant": "An optional quantization configuration object. If provided, a quantized version of dot_general is used."
            },
            "notes": [
                "This is a helper function that abstracts away the choice between a standard and a quantized dot product.",
                "The dimension mapping `((axis, contract_ind), ((), ()))` indicates a contraction between specified axes of the two inputs, with no batch dimensions involved in the dot_general definition itself."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#_compute_dot_general_nnx",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def _compute_dot_general_nnx(\n    inputs, kernel, axis, contract_ind, matmul_precision, quant_dot_general: nnx_wrappers.ToNNX | None, initializing: bool\n):\n  \"\"\"Computes a dot_general operation that may be quantized.\"\"\"\n  dot_general = lax.dot_general\n  matmul_precision = lax.Precision(matmul_precision)\n  if quant_dot_general is not None:\n    if initializing:\n      quant_dot_general.lazy_init(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None)\n    return quant_dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=None, mutable=[\"aqt\"])\n  return dot_general(inputs, kernel, ((axis, contract_ind), ((), ())), precision=matmul_precision)",
        "analysis": {
            "functionality": "Computes a generalized dot product (`dot_general`) between two tensors, with an optional path for applying quantization.",
            "usage": "This is a helper function. Call it with input tensors (`inputs`, `kernel`), contraction axes (`axis`, `contract_ind`), and precision settings. If a `quant_dot_general` module is provided, it performs a quantized dot product; otherwise, it uses `jax.lax.dot_general`. The `initializing` flag is used to trigger lazy initialization for the quantization module."
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#DenseGeneral",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class DenseGeneral(nnx.Module):\n  \"\"\"A linear transformation with flexible axes.\"\"\"\n\n  def __init__(\n      self,\n      in_features_shape: Iterable[int] | int,\n      out_features_shape: Iterable[int] | int,\n      axis: Iterable[int] | int = -1,\n      weight_dtype: DType = jnp.float32,\n      dtype: DType = jnp.float32,\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      kernel_axes: tuple[None | str, ...] = (),\n      quant: None | Quant = None,\n      use_bias: bool = False,\n      matmul_precision: str = \"default\",\n      parameter_memory_host_offload: bool = False,\n      *,  # Following arguments are keyword-only\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the DenseGeneral module.\n\n    Args:\n      in_features_shape: tuple with numbers of input features for axes specified in\n        'axis'.\n      out_features_shape: tuple with numbers of output features.\n      axis: tuple with axes to apply the transformation on.\n      weight_dtype: the dtype of the weights (default: float32).\n      dtype: the dtype of the computation (default: float32).\n      kernel_init: initializer function for the weight matrix.\n      kernel_axes: logical axes for partitioning the kernel.\n      quant: quantization config, defaults to None implying no quantization.\n      use_bias: whether to add bias in linear transformation.\n      matmul_precision: Precision for matrix multiplication.\n      parameter_memory_host_offload: Determines whether to offload params to host\n      rngs: RNG state for initialization in nnx.\n    \"\"\"\n    self.in_features_shape = canonicalize_tuple(in_features_shape)\n    self.out_features_shape = canonicalize_tuple(out_features_shape)\n    self.axis = canonicalize_tuple(axis)\n    self.weight_dtype = weight_dtype\n    self.dtype = dtype\n    self.kernel_init = kernel_init\n    self.kernel_axes = kernel_axes\n    self.quant = quant\n    self.use_bias = use_bias\n    self.matmul_precision = matmul_precision\n    self.parameter_memory_host_offload = parameter_memory_host_offload\n\n    # Parameter initialization\n    kernel_shape = self.in_features_shape + self.out_features_shape\n    kernel_in_axis = np.arange(len(self.axis))\n    kernel_out_axis = np.arange(len(self.axis), len(self.axis) + len(self.out_features_shape))\n\n    if not quantizations.in_serve_mode(self.quant):\n      self.kernel = nnx.Param(\n          self.kernel_init(\n              rngs.params(),\n              kernel_shape,\n              self.weight_dtype,\n              kernel_in_axis,\n              kernel_out_axis,\n          ),\n          sharding=self.kernel_axes,\n      )\n\n    if self.use_bias:\n      bias_axes = self.kernel_axes[-len(self.out_features_shape) :]\n      bias_shape = kernel_shape[-len(self.out_features_shape) :]\n      self.bias = nnx.Param(\n          default_bias_init(rngs.params(), bias_shape, self.weight_dtype),\n          sharding=bias_axes,\n      )\n    else:\n      self.bias = None\n\n    if quant:\n      dot_general_cls = quant.dot_general_cls(mesh_axes=kernel_axes)\n      dot_general_linen = dot_general_cls()\n      quant_dot_general = nnx_wrappers.ToNNX(dot_general_linen, rngs=rngs)\n      self._quant_dot_general_name = f\"{type(dot_general_linen).__name__}_0\"\n      setattr(self, self._quant_dot_general_name, quant_dot_general)\n      dummy_inputs = jnp.zeros((1, *self.in_features_shape), dtype=self.dtype)\n      self(dummy_inputs, _initializing=True)\n    else:\n      self._quant_dot_general_name = None\n\n  @property\n  def quant_dot_general(self) -> nnx_wrappers.ToNNX | None:\n    if self._quant_dot_general_name is None:\n      return None\n    return getattr(self, self._quant_dot_general_name)\n\n  def __call__(self, inputs: Array, _initializing: bool = False) -> Array:\n    \"\"\"Applies a linear transformation to the inputs along multiple dimensions.\n\n    Args:\n      inputs: The nd-array to be transformed.\n\n    Returns:\n      The transformed input.\n    \"\"\"\n    inputs = jnp.asarray(inputs, self.dtype)\n    norm_axis = normalize_axes(self.axis, inputs.ndim)\n\n    for i, ax in enumerate(norm_axis):\n      if inputs.shape[ax] != self.in_features_shape[i]:\n        raise ValueError(\n            f\"Input dimension {inputs.shape[ax]} at axis {ax} \"\n            f\"does not match expected input feature size {self.in_features_shape[i]}\"\n        )\n\n    if quantizations.in_serve_mode(self.quant):\n      kernel_shape = self.in_features_shape + self.out_features_shape\n      kernel = jnp.zeros(kernel_shape, dtype=self.dtype)\n    else:\n      kernel = self.kernel[...]\n      # Move logit_dense kernel to device if parameter offloading is enabled\n      if self.parameter_memory_host_offload:\n        max_logging.log(\"linear.py: Moving parameter logits_dense kernel to device\")\n        kernel = jax.device_put(kernel, max_utils.device_space())\n      kernel = jnp.asarray(kernel, self.dtype)\n\n    contract_ind = tuple(range(0, len(self.axis)))\n    output = _compute_dot_general_nnx(\n        inputs,\n        kernel,\n        norm_axis,\n        contract_ind,\n        self.matmul_precision,\n        self.quant_dot_general,\n        _initializing,\n    )\n\n    if self.bias is not None:\n      bias = jnp.asarray(self.bias[...], self.dtype)\n      output += bias\n    return output",
        "analysis": {
            "module_type": "dense_general",
            "purpose": "A linear transformation with flexible axes, allowing matrix multiplication over specified dimensions of an input tensor.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes kernel (weight) and optional bias parameters during construction.",
                "When called, it performs a `dot_general` operation between the input tensor and the kernel along specified axes.",
                "Optionally adds a bias term to the result."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.numpy",
                "MaxText.layers.nnx_wrappers",
                "MaxText.layers.quantizations",
                "MaxText.layers.initializers"
            ],
            "parameters": {
                "in_features_shape": "The shape of the input features for the axes specified in 'axis'.",
                "out_features_shape": "The desired shape of the output features.",
                "axis": "The axis or axes of the input tensor to apply the transformation on.",
                "quant": "An optional quantization configuration object. If provided, the matrix multiplication will be quantized.",
                "use_bias": "A boolean indicating whether to add a bias term to the output.",
                "matmul_precision": "The precision for the matrix multiplication operation (e.g., 'default', 'high').",
                "parameter_memory_host_offload": "A boolean to determine whether to offload parameters to host memory."
            },
            "notes": [
                "This is a generalized version of a standard dense (fully-connected) layer.",
                "It uses `flax.nnx` for module and parameter management.",
                "If quantization is enabled, it performs a dummy forward pass during initialization to set up the quantized operator."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the DenseGeneral module, setting up parameters like the kernel and optional bias, and preparing for potential quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Canonicalizes `in_features_shape`, `out_features_shape`, and `axis` arguments to tuples.",
                        "Stores configuration parameters as attributes.",
                        "Calculates the required kernel shape from input and output feature shapes.",
                        "Initializes the kernel `nnx.Param` using the provided `kernel_init` function, unless in serving mode with quantization.",
                        "Initializes the bias `nnx.Param` if `use_bias` is True.",
                        "If `quant` is provided, sets up the quantized dot-general operator and performs a dummy forward pass to initialize it."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "canonicalize_tuple",
                        "nd_dense_init",
                        "default_bias_init",
                        "quantizations",
                        "nnx_wrappers.ToNNX",
                        "nnx.Param",
                        "nnx.Rngs"
                    ],
                    "notes": [
                        "The constructor performs a dummy forward pass if quantization is enabled to trigger lazy initialization of the quantized operator."
                    ]
                },
                "quant_dot_general": {
                    "purpose": "A property to access the quantized dot_general operator if it exists.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if a quantized operator was configured during initialization.",
                        "If so, retrieves the operator using `getattr`.",
                        "Returns the operator or None."
                    ],
                    "output": {
                        "shape": "An `nnx_wrappers.ToNNX` object or `None`."
                    },
                    "dependencies": [],
                    "notes": [
                        "This is a read-only property."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the linear transformation to the input tensor along multiple dimensions.",
                    "input": {
                        "shape": "An N-dimensional array where the dimensions specified in `axis` match `in_features_shape`.",
                        "dtype": "Any JAX array dtype."
                    },
                    "processing_steps": [
                        "Casts the input tensor to the module's computation `dtype`.",
                        "Normalizes the `axis` parameter to handle negative indices.",
                        "Validates that the input tensor's shape along the specified axes matches the expected `in_features_shape`.",
                        "Retrieves the kernel, potentially moving it from host to device if offloading is enabled.",
                        "Computes the dot product using `_compute_dot_general_nnx`, which handles both standard and quantized multiplication.",
                        "Adds the bias term to the output if `use_bias` is enabled.",
                        "Returns the transformed array."
                    ],
                    "output": {
                        "shape": "The shape of the input tensor with the dimensions specified in `axis` replaced by `out_features_shape`."
                    },
                    "dependencies": [
                        "jnp.asarray",
                        "normalize_axes",
                        "quantizations.in_serve_mode",
                        "jax.device_put",
                        "_compute_dot_general_nnx"
                    ],
                    "notes": [
                        "The `_initializing` argument is used internally to handle the dummy forward pass for quantization setup."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#dense_general",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def dense_general(\n    *,\n    inputs_shape: tuple[int, ...] | None = None,\n    in_features_shape: tuple[int, ...] | int | None = None,\n    out_features_shape: Iterable[int] | int,\n    axis: Iterable[int] | int = -1,\n    weight_dtype: DType = jnp.float32,\n    dtype: DType = jnp.float32,\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    kernel_axes: tuple[None | str, ...] = (),\n    quant: None | Quant = None,\n    use_bias: bool = False,\n    matmul_precision: str = \"default\",\n    parameter_memory_host_offload: bool = False,\n    name: None | str = None,\n):\n  \"\"\"Creates a DenseGeneral Linen module using nnx.bridge.to_linen.\n\n  Args:\n    inputs_shape: tuple with the shape of the inputs\n    in_features_shape: tuple with numbers of input features for axes specified in\n      'axis'.\n    out_features_shape: tuple with numbers of output features.\n    axis: tuple with axes to apply the transformation on.\n    weight_dtype: the dtype of the weights (default: float32).\n    dtype: the dtype of the computation (default: float32).\n    kernel_init: initializer function for the weight matrix.\n    kernel_axes: logical axes for partitioning the kernel.\n    quant: quantization config, defaults to None implying no quantization.\n    use_bias: whether to add bias in linear transformation.\n    matmul_precision: Precision for matrix multiplication.\n    parameter_memory_host_offload: Determines whether to offload params to host\n    name: name passed to the ToLinen Module\n  \"\"\"\n  if not (inputs_shape is not None) ^ (in_features_shape is not None):\n    raise ValueError(\"Exactly one of inputs_shape or in_features must be specified.\")\n\n  if inputs_shape is not None:\n    axis = canonicalize_tuple(axis)\n    in_features_shape = tuple(inputs_shape[ax] for ax in normalize_axes(axis, len(inputs_shape)))\n  else:\n    assert in_features_shape is not None\n  module = nnx_wrappers.to_linen(\n      DenseGeneral,\n      in_features_shape=in_features_shape,\n      out_features_shape=out_features_shape,\n      axis=axis,\n      weight_dtype=weight_dtype,\n      dtype=dtype,\n      kernel_init=kernel_init,\n      kernel_axes=kernel_axes,\n      quant=quant,\n      use_bias=use_bias,\n      matmul_precision=matmul_precision,\n      parameter_memory_host_offload=parameter_memory_host_offload,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "dense_general_factory",
            "purpose": "A factory function that creates a Flax Linen `DenseGeneral` module by wrapping an NNX `DenseGeneral` module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Validate that exactly one of `inputs_shape` or `in_features_shape` is provided.",
                "If `inputs_shape` is provided, compute `in_features_shape` from it based on the specified `axis`.",
                "Call `nnx_wrappers.to_linen` to wrap the `DenseGeneral` NNX module.",
                "Pass all configuration parameters to the `to_linen` wrapper.",
                "Return the created Linen module."
            ],
            "output": {
                "shape": "Returns a `flax.linen.Module` instance. The shape of the module's output tensor depends on its input and the `out_features_shape` parameter."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "DenseGeneral",
                "canonicalize_tuple",
                "normalize_axes",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "inputs_shape": "The shape of the input tensor, used to infer `in_features_shape`.",
                "in_features_shape": "The number of input features for the axes specified in `axis`.",
                "out_features_shape": "The number of output features.",
                "axis": "The axis or axes of the input tensor to apply the linear transformation on.",
                "quant": "Quantization configuration object; if None, no quantization is applied.",
                "use_bias": "A boolean indicating whether to add a bias term to the output.",
                "kernel_axes": "A tuple specifying the logical axes for partitioning the kernel weight matrix.",
                "parameter_memory_host_offload": "A boolean to determine whether to offload parameters to host memory."
            },
            "notes": [
                "This function does not perform a computation itself but rather constructs and returns a neural network layer (a Linen module).",
                "It requires either `inputs_shape` or `in_features_shape` to be specified, but not both."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#Dropout",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class Dropout(nnx.Dropout):\n  \"\"\"Forked nnx.Dropout that is easier to use with bridge\"\"\"\n  def __init__( # pylint: disable=super-init-not-called\n    self,\n    rate: float,\n    *,\n    broadcast_dims: Sequence[int] = (),\n    deterministic: bool = False,\n    rng_collection: str = 'dropout',\n    rngs: nnx.Rngs| None = None,\n  ):\n    self.rate = rate\n    self.broadcast_dims = broadcast_dims\n    self.deterministic = deterministic\n    self.rng_collection = rng_collection\n\n    if isinstance(rngs, nnx.Rngs):\n      self.rngs = rngs.fork() if hasattr(type(rngs), 'fork') else rngs\n    else:\n      raise TypeError(\n        f'rngs must be a Rngs, RngStream or None, but got {type(rngs)}.'\n      )",
        "analysis": {
            "module_type": "dropout_layer",
            "purpose": "A modified version of `nnx.Dropout` that regularizes a network by randomly setting a fraction of input units to 0 during training, specifically forked to be easier to use with `nnx.bridge`.",
            "input": {
                "shape": "[batch_size, ..., feature_dim] (inferred from parent class `nnx.Dropout`)",
                "dtype": "Any float dtype (e.g., float32)"
            },
            "processing_steps": [
                "The `__call__` method is inherited from `flax.nnx.Dropout`.",
                "If not deterministic, a random mask is generated using the provided RNG stream.",
                "The mask is applied to the input tensor, setting some elements to zero.",
                "The remaining elements are scaled by `1 / (1 - rate)`.",
                "The modified tensor is returned."
            ],
            "output": {
                "shape": "Same as input shape."
            },
            "dependencies": [
                "flax.nnx.Dropout",
                "flax.nnx.Rngs"
            ],
            "parameters": {
                "rate": "The fraction of input units to drop.",
                "broadcast_dims": "Dimensions along which the dropout mask is broadcast.",
                "deterministic": "If True, dropout is disabled and the module acts as an identity function.",
                "rng_collection": "The name of the RNG stream to use for generating the dropout mask.",
                "rngs": "An `nnx.Rngs` object used for random number generation."
            },
            "notes": [
                "This class inherits from `nnx.Dropout` but overrides the `__init__` method and intentionally does not call the parent's constructor.",
                "The primary modification is in the `__init__` method's handling of the `rngs` object, which is forked to ensure proper state management when used with tools like `nnx.bridge.to_linen`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the Dropout layer's configuration and RNG state.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assigns configuration parameters (`rate`, `broadcast_dims`, `deterministic`, `rng_collection`) to instance attributes.",
                        "Checks if `rngs` is an instance of `nnx.Rngs`.",
                        "Forks the provided `rngs` object if it has a `fork` method; otherwise, uses it directly.",
                        "Raises a `TypeError` if `rngs` is not of the expected type."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "flax.nnx.Rngs"
                    ],
                    "notes": [
                        "The `pylint: disable=super-init-not-called` comment indicates that the parent class constructor is intentionally not called."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#MlpBlock",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "class MlpBlock(nnx.Module):\n  \"\"\"Transformer MLP / feed-forward block.\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      in_features: int,\n      intermediate_dim: int = 2048,\n      activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n      kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n      intermediate_dropout_rate: float = 0.1,\n      dtype: Any = jnp.float32,\n      weight_dtype: Any = jnp.float32,\n      use_bias: bool = False,\n      use_pre_norm: bool = False,\n      quant: None | Quant = None,\n      model_mode: None | str = None,\n      *,\n      rngs: nnx.Rngs,\n  ) -> None:\n    \"\"\"A MlpBlock module.\n\n    Args:\n      config: Config object containing model parameters.\n      in_features: Number of input features.\n      intermediate_dim: Shared dimension of hidden layers.\n      activations: Type of activations for each layer.  Each element is either\n        'linear', a string function name in flax.linen, or a function.\n      kernel_init: Kernel function, passed to the dense layers.\n      deterministic: Whether the dropout layers should be deterministic.\n      intermediate_dropout_rate: Dropout rate used after the intermediate layers.\n      dtype: computation data type for the dense layer.\n      weight_dtype: weight data type for the dense layer.\n      use_bias: whether to add bias in all feedforward layers.\n      use_pre_norm: whether to add pre layer norm in mlp layers.\n      quant: Optional quantization config, no quantization if None.\n    \"\"\"\n    self.config = config\n    self.in_features = in_features\n    self.intermediate_dim = intermediate_dim\n    self.activations = activations\n    self.kernel_init = kernel_init\n    self.intermediate_dropout_rate = intermediate_dropout_rate\n    self.dtype = dtype\n    self.weight_dtype = weight_dtype\n    self.use_bias = use_bias\n    self.use_pre_norm = use_pre_norm\n    self.quant = quant\n    self.model_mode = model_mode\n\n    if self.use_pre_norm:\n      self.mlp_layer_norm = self.get_norm_layer(num_features=in_features)(\n          dtype=config.dtype,\n          weight_dtype=config.weight_dtype,\n          kernel_axes=(\"norm\",),\n          epsilon=config.normalization_layer_epsilon,\n          rngs=rngs,\n      )\n    else:\n      self.mlp_layer_norm = None\n\n    if config.fused_mlp:\n      self.wi = DenseGeneral(\n          in_features_shape=in_features,\n          out_features_shape=(len(self.activations), self.intermediate_dim),\n          dtype=self.dtype,\n          weight_dtype=self.weight_dtype,\n          kernel_init=self.kernel_init,\n          kernel_axes=(\"embed\", \"num_activations\", \"mlp\"),\n          quant=self.quant,\n          use_bias=self.use_bias,\n          matmul_precision=self.config.matmul_precision,\n          rngs=rngs,\n      )\n    else:\n      for idx in range(len(self.activations)):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = DenseGeneral(\n            in_features_shape=in_features,\n            out_features_shape=self.intermediate_dim,\n            dtype=self.dtype,\n            weight_dtype=self.weight_dtype,\n            kernel_init=self.kernel_init,\n            kernel_axes=(\"embed\", \"mlp\"),\n            quant=self.quant,\n            use_bias=self.use_bias,\n            matmul_precision=self.config.matmul_precision,\n            rngs=rngs,\n        )\n        setattr(self, dense_name, module)\n    self.dropout = Dropout(rate=self.intermediate_dropout_rate, broadcast_dims=(-2,), rngs=rngs)\n    self.wo = DenseGeneral(\n        in_features_shape=self.intermediate_dim,\n        out_features_shape=in_features,\n        dtype=self.dtype,\n        weight_dtype=self.weight_dtype,\n        kernel_init=self.kernel_init,\n        kernel_axes=(\"mlp\", \"embed\"),\n        quant=self.quant,\n        use_bias=self.use_bias,\n        matmul_precision=self.config.matmul_precision,\n        rngs=rngs,\n    )\n\n  def get_norm_layer(self, num_features: int):\n    \"\"\"get normalization layer.\"\"\"\n    if self.config.decoder_block in (\n        DecoderBlockType.DEFAULT,\n        DecoderBlockType.LLAMA2,\n        DecoderBlockType.MISTRAL,\n        DecoderBlockType.MIXTRAL,\n        DecoderBlockType.GEMMA,\n        DecoderBlockType.GEMMA2,\n        DecoderBlockType.GEMMA3,\n        DecoderBlockType.QWEN3,\n        DecoderBlockType.DEEPSEEK,\n        DecoderBlockType.LLAMA4,\n    ):\n      return functools.partial(normalizations.RMSNorm, num_features=num_features)\n    elif self.config.decoder_block == DecoderBlockType.GPT3:\n      from MaxText.layers import gpt3  # pylint: disable=import-outside-toplevel\n\n      return functools.partial(\n          gpt3.Gpt3LayerNorm, num_features=num_features, reductions_in_fp32=False, use_bias=self.use_bias\n      )\n    else:\n      raise ValueError(f\"Incorrect decoder_block name {self.config.decoder_block.value=}\")\n\n  def __call__(self, inputs, decode: bool = False, deterministic: bool = False):\n    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n    cfg = self.config\n\n    if self.mlp_layer_norm is not None:\n      inputs = self.mlp_layer_norm(inputs)\n\n    # Iterate over specified MLP input activation functions.\n    # e.g. ('relu',) or ('gelu', 'linear') for gated-gelu.\n    activations = []\n    if cfg.fused_mlp:\n      x = self.wi(inputs)\n      x = checkpoint_name(x, \"mlpwi\")\n      for idx, act_fn in enumerate(self.activations):\n        y = _convert_to_activation_function(act_fn)(x[:, :, idx, ...])\n        activations.append(y)\n    else:\n      for idx, act_fn in enumerate(self.activations):\n        dense_name = \"wi\" if len(self.activations) == 1 else f\"wi_{idx}\"\n        module = getattr(self, dense_name)\n        x = module(inputs)\n        x = checkpoint_name(x, \"mlp\" + dense_name)\n        if cfg.activations_in_float32:\n          x = x.astype(jnp.float32)\n        x = _convert_to_activation_function(act_fn)(x)\n        activations.append(x)\n\n    # Take elementwise product of above intermediate activations.\n    x = functools.reduce(operator.mul, activations).astype(self.dtype)\n    # Apply dropout and final dense output projection.\n    x = self.dropout(x, deterministic=deterministic)  # Broadcast along length.\n    if self.model_mode == MODEL_MODE_PREFILL:\n      x = nn.with_logical_constraint(x, (\"activation_batch\", \"prefill_activation_length\", \"activation_mlp\"))\n    else:\n      x = nn.with_logical_constraint(x, (\"activation_batch\", \"activation_length\", \"activation_mlp\"))\n    output = self.wo(x)\n\n    output = checkpoint_name(output, \"mlpwo\")\n    return output",
        "analysis": {
            "module_type": "transformer_mlp_block",
            "purpose": "Implements a standard or gated feed-forward network (MLP) block for a Transformer model, including optional pre-normalization and dropout.",
            "input": {
                "shape": "[batch_size, sequence_length, in_features]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Conditionally applies pre-layer normalization to the input tensor if `use_pre_norm` is true.",
                "Applies one or more input dense layers (`wi`) to project the input to an intermediate dimension.",
                "Applies specified activation functions to the intermediate representations.",
                "Element-wise multiplies the activated tensors (to support gated MLPs like SwiGLU).",
                "Applies dropout to the result.",
                "Applies a final output dense layer (`wo`) to project the tensor back to the input dimension.",
                "Returns the final output tensor."
            ],
            "output": {
                "shape": "[batch_size, sequence_length, in_features]"
            },
            "dependencies": [
                "nnx.Module",
                "DenseGeneral",
                "Dropout",
                "normalizations.RMSNorm",
                "gpt3.Gpt3LayerNorm",
                "_convert_to_activation_function",
                "jax.ad_checkpoint.checkpoint_name"
            ],
            "parameters": {
                "in_features": "The number of input features (embedding dimension).",
                "intermediate_dim": "The dimensionality of the hidden layer in the MLP.",
                "activations": "A sequence of activation functions to apply. Multiple activations enable gated MLP variants.",
                "intermediate_dropout_rate": "The dropout rate applied after the activation functions.",
                "use_pre_norm": "A boolean indicating whether to apply layer normalization before the MLP transformations.",
                "fused_mlp": "A boolean from the config that determines whether to use a single, larger dense layer for multiple activations or separate dense layers."
            },
            "notes": [
                "This module can function as a standard MLP or a gated MLP (e.g., SwiGLU) depending on the `activations` provided.",
                "The implementation of the input projection layers (`wi`) differs based on the `config.fused_mlp` flag for performance optimization.",
                "The specific type of normalization layer used is determined by the `config.decoder_block` setting."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MlpBlock, creating the necessary dense layers, dropout layer, and optional normalization layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration parameters.",
                        "Conditionally creates a layer normalization module (`mlp_layer_norm`) if `use_pre_norm` is true.",
                        "Based on `config.fused_mlp`, creates either a single fused input projection layer (`wi`) or multiple separate ones.",
                        "Creates a `Dropout` layer.",
                        "Creates the output projection `DenseGeneral` layer (`wo`)."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "DenseGeneral",
                        "Dropout",
                        "get_norm_layer"
                    ],
                    "notes": [
                        "The structure of the input projection layers is determined by the `config.fused_mlp` flag."
                    ]
                },
                "get_norm_layer": {
                    "purpose": "Returns a partially applied constructor for the appropriate normalization layer based on the model's configuration.",
                    "input": {
                        "shape": "num_features (int)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks the `self.config.decoder_block` value.",
                        "Returns a partially applied `normalizations.RMSNorm` or `gpt3.Gpt3LayerNorm` based on the configuration.",
                        "Raises a ValueError if the decoder block type is not supported."
                    ],
                    "output": {
                        "shape": "A partially applied constructor for a normalization layer."
                    },
                    "dependencies": [
                        "functools.partial",
                        "normalizations.RMSNorm",
                        "gpt3.Gpt3LayerNorm",
                        "DecoderBlockType"
                    ],
                    "notes": [
                        "This method allows the MLP block to adapt to the normalization scheme of different model architectures like LLaMA, GPT-3, etc."
                    ]
                },
                "__call__": {
                    "purpose": "Applies the forward pass of the MLP block to an input tensor.",
                    "input": {
                        "shape": "[batch_size, sequence_length, in_features]",
                        "dtype": "jnp.float32 (or as configured by self.dtype)"
                    },
                    "processing_steps": [
                        "Optionally applies pre-layer normalization.",
                        "Applies the input dense layer(s) (`wi`).",
                        "Applies activation function(s) to the intermediate tensor(s).",
                        "Element-wise multiplies the results of the activations.",
                        "Applies dropout.",
                        "Applies logical constraints for tensor parallelism.",
                        "Applies the output dense layer (`wo`)."
                    ],
                    "output": {
                        "shape": "[batch_size, sequence_length, in_features]"
                    },
                    "dependencies": [
                        "_convert_to_activation_function",
                        "functools.reduce",
                        "operator.mul",
                        "jax.ad_checkpoint.checkpoint_name",
                        "flax.linen.with_logical_constraint"
                    ],
                    "notes": [
                        "The `deterministic` flag controls the behavior of the dropout layer, disabling it during inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/linears.py#mlp_block",
        "file_path": "src/MaxText/layers/linears.py",
        "code_block": "def mlp_block(\n    *,\n    config: Config,\n    in_features: int,\n    intermediate_dim: int = 2048,\n    activations: Sequence[str | Callable[..., Any]] = (\"relu\",),\n    kernel_init: NdInitializer = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\"),\n    intermediate_dropout_rate: float = 0.1,\n    dtype: Any = jnp.float32,\n    weight_dtype: Any = jnp.float32,\n    use_bias: bool = False,\n    use_pre_norm: bool = False,\n    quant: None | Quant = None,\n    model_mode: None | str = None,\n    name: None | str = None,\n):\n  \"\"\"Creates a MlpBlock Linen module using nnx.bridge.to_linen.\"\"\"\n  module = nnx_wrappers.to_linen(\n      MlpBlock,\n      config=config,\n      in_features=in_features,\n      intermediate_dim=intermediate_dim,\n      activations=activations,\n      kernel_init=kernel_init,\n      intermediate_dropout_rate=intermediate_dropout_rate,\n      dtype=dtype,\n      weight_dtype=weight_dtype,\n      use_bias=use_bias,\n      use_pre_norm=use_pre_norm,\n      quant=quant,\n      model_mode=model_mode,\n      name=name,\n      metadata_fn=variable_to_logically_partitioned,\n      abstract_init=False,\n  )\n  return module",
        "analysis": {
            "module_type": "mlp_block_factory",
            "purpose": "A factory function that creates a Flax Linen module by wrapping the NNX `MlpBlock` class using `nnx_wrappers.to_linen`.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `MlpBlock` NNX module into a Flax Linen module.",
                "Passes all its configuration parameters (config, in_features, intermediate_dim, etc.) to the `MlpBlock` constructor through the wrapper.",
                "Returns the created Linen module instance."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlpBlock",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "Config object containing model parameters.",
                "in_features": "Number of input features for the MLP block.",
                "intermediate_dim": "The shared dimension of the MLP's hidden layers.",
                "activations": "A sequence of activation functions to be applied.",
                "intermediate_dropout_rate": "Dropout rate applied after the intermediate layers.",
                "quant": "Optional quantization configuration.",
                "use_pre_norm": "A boolean indicating whether to add a pre-layernorm to the MLP block."
            },
            "notes": [
                "This function acts as a bridge between the NNX module definition (`MlpBlock`) and the Flax Linen API.",
                "All arguments to this function are keyword-only."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#nd_dense_init",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def nd_dense_init(scale, mode, distribution):\n  \"\"\"Initializer with in_axis, out_axis set at call time.\"\"\"\n\n  def init_fn(key, shape, dtype, in_axis, out_axis):\n    fn = jax.nn.initializers.variance_scaling(scale, mode, distribution, in_axis, out_axis)\n    return fn(key, shape, dtype)\n\n  return init_fn",
        "analysis": {
            "module_type": "weight_initializer_factory",
            "purpose": "A higher-order function that creates a JAX variance scaling initializer where the input and output axes (`in_axis`, `out_axis`) are specified at call time.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Accepts `scale`, `mode`, and `distribution` parameters for variance scaling.",
                "Defines and returns an inner function `init_fn`.",
                "The returned `init_fn` accepts `key`, `shape`, `dtype`, `in_axis`, and `out_axis`.",
                "Inside `init_fn`, it creates a `jax.nn.initializers.variance_scaling` initializer with all the specified parameters.",
                "The JAX initializer is called with `key`, `shape`, and `dtype` to generate and return the initialized array."
            ],
            "output": {
                "shape": "Returns a callable function `init_fn` with the signature `(key, shape, dtype, in_axis, out_axis) -> Array`."
            },
            "dependencies": [
                "jax.nn.initializers.variance_scaling"
            ],
            "parameters": {
                "scale": "The scaling factor for the variance.",
                "mode": "A string specifying the mode for variance scaling (e.g., 'fan_in', 'fan_out').",
                "distribution": "A string specifying the distribution to sample from (e.g., 'normal', 'uniform')."
            },
            "notes": [
                "This function acts as a factory, deferring the specification of `in_axis` and `out_axis` to the moment the returned initializer is actually used. This provides flexibility for initializing n-dimensional tensors."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/initializers.py#variable_to_logically_partitioned",
        "file_path": "src/MaxText/layers/initializers.py",
        "code_block": "def variable_to_logically_partitioned(variable: nnx.VariableState):\n  if isinstance(variable.value, aqt_tensor.QTensor):\n    return variable.value\n\n  if variable.type.__name__ == \"_overwrite_with_gradient\":\n    return variable.value\n\n  metadata = variable.get_metadata()\n  if \"sharding\" in metadata or \"sharding_names\" in metadata:\n    if \"sharding_names\" in metadata:\n      sharding_names = metadata[\"sharding_names\"]\n    else:\n      sharding_names = metadata[\"sharding\"]\n    return nn.LogicallyPartitioned(  # type: ignore[wrong-keyword-args]\n        variable.value,\n        sharding_names,  # type: ignore[arg-type]\n        mesh=metadata.get(\"mesh\"),\n        rules=metadata.get(\"rules\"),\n    )\n  else:\n    return variable.value",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Conditionally wraps an `nnx.VariableState`'s value in an `nn.LogicallyPartitioned` object if sharding metadata is present, otherwise returns the value unchanged.",
            "input": {
                "shape": "N/A",
                "dtype": "nnx.VariableState"
            },
            "processing_steps": [
                "Check if the variable's value is an `aqt_tensor.QTensor` and return it if true.",
                "Check if the variable's type name is `_overwrite_with_gradient` and return its value if true.",
                "Retrieve metadata from the variable using `get_metadata()`.",
                "Check if 'sharding' or 'sharding_names' keys exist in the metadata.",
                "If sharding metadata exists, wrap `variable.value` in `nn.LogicallyPartitioned` using the sharding names, mesh, and rules from the metadata.",
                "If no sharding metadata is found, return the original `variable.value`."
            ],
            "output": {
                "shape": "The shape of the input `variable.value`."
            },
            "dependencies": [
                "flax.nnx.VariableState",
                "aqt.jax.v2.aqt_tensor.QTensor",
                "flax.linen.LogicallyPartitioned"
            ],
            "parameters": {},
            "notes": [
                "The function is designed to apply sharding annotations to model variables based on their metadata.",
                "It handles special cases for AQT QTensors and variables intended for gradient overwriting by leaving them unmodified.",
                "If both 'sharding' and 'sharding_names' are present in metadata, 'sharding_names' is used."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_compute_axis_order",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_compute_axis_order(s: AxisIdxes) -> None:\n  valid_compute_axis_order = ((0, 1, 2, 3), (0, 2, 1, 3))\n  if s not in valid_compute_axis_order:  # currently supported compute_axis_order\n    raise ValueError(\"Invalid compute_axis_order was passed. Valid options \", valid_compute_axis_order)",
        "analysis": {
            "functionality": "Validates that the provided compute axis order tuple is one of the supported options, raising a ValueError if it is not.",
            "usage": "Call this function with a tuple of four integers representing the axis order. The function will either return None if the order is valid or raise a ValueError if it is invalid. Example: validate_compute_axis_order((0, 1, 2, 3))"
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#apply_mask_to_logits",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def apply_mask_to_logits(logits: Array, mask: Array):\n  \"\"\"Applies a floating-point mask to a set of logits.\n\n  The mask is represented as a tensor with some dtype where 0 represents true and values\n  below a large negative number (here set to\n  get_large_negative_number(logits.dtype) / 2) represent false. Applying the mask\n  leaves the logits alone in the true case and replaces them by\n  get_large_negative_number(logits.dtype) in the false case. Previously, this was\n  done by adding the logits to the mask; however, this leads to a bad fusion\n  decision in the compiler that saves the values in memory rather than\n  just the predicate. This implementation avoids that problem.\n\n  from https://github.com/google/praxis/blob/4712a6b9ee13e224b86e235ff55f7c6bab9fbab3/praxis/py_utils.py#L706\n\n  Args:\n    logits: A JTensor of logit values.\n    mask: A JTensor of mask values with the encoding described in the\n      function documentation.\n\n  Returns:\n    Masked logits.\n  \"\"\"\n  return jnp.where((mask >= DEFAULT_MASK_VALUE * 0.5), logits, DEFAULT_MASK_VALUE)",
        "analysis": {
            "module_type": "logit_masking",
            "purpose": "Applies a floating-point attention mask to a tensor of logits, replacing masked-out positions with a large negative value.",
            "input": {
                "shape": "Logits: [..., sequence_length, key_length], Mask: A tensor broadcastable to the logits shape.",
                "dtype": "float32 or bfloat16"
            },
            "processing_steps": [
                "For each element, check if the mask value is greater than or equal to `DEFAULT_MASK_VALUE * 0.5`.",
                "Use `jnp.where` to return the original logit value if the condition is true, or `DEFAULT_MASK_VALUE` if it is false."
            ],
            "output": {
                "shape": "Same as the broadcasted shape of the input logits and mask tensors."
            },
            "dependencies": [
                "jax.numpy.where",
                "MaxText.common_types.DEFAULT_MASK_VALUE"
            ],
            "parameters": {},
            "notes": [
                "The mask is expected to be a floating-point tensor where 0.0 represents an unmasked position and a large negative number represents a masked position.",
                "This implementation uses `jnp.where` instead of addition to avoid a specific compiler fusion issue that could increase memory usage."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#validate_flash_attention_with_sinks_on_gpu",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def validate_flash_attention_with_sinks_on_gpu(sinks: Array | None) -> None:\n  \"\"\"Helper function to check for sinks with flash attention on GPU.\"\"\"\n  if sinks is not None:\n    raise ValueError(\"The flash attention with sinks is not supported on GPU yet.\")",
        "analysis": {
            "functionality": "Checks if the 'sinks' argument is provided when using flash attention on a GPU and raises an error if it is, as this combination is currently unsupported.",
            "usage": "This function is called to validate configuration before executing a GPU-based flash attention operation. If the `sinks` argument is not `None`, it raises a `ValueError`. Input: `sinks` (Array | None). Output: `None` or raises `ValueError`."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#ChunkedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class ChunkedCausalMask(splash_attention_mask._ComputableMask):  # pylint: disable=protected-access\n  \"\"\"Lazy chunked causal mask.\n\n  Attention is causal within each chunk (0, K), (K, 2K), (2K, 3K), ... tokens attend to each other but not across chunks.\n  Llama4 models use interleaved chunk attention along with global attention.\n\n  This mask class inherits from splash_attention_mask._ComputableMask and is designed to be used with Splash Attention.\n  It allows the mask logic to be computed on-the-fly or fused into the attention kernel, avoiding the memory cost of\n  materializing the full (sequence_length, sequence_length) boolean mask array, which can be prohibitive for long sequences.\n\n  Attributes:\n    chunk_size: The size of each attention chunk.\n  \"\"\"\n\n  chunk_size: int\n\n  def __init__(\n      self,\n      shape: tuple[int, int],\n      chunk_size: int,\n      shard_count: int = 1,\n  ):\n    if chunk_size <= 0:\n      raise ValueError(\"chunk_size must be positive\")\n    self.chunk_size = chunk_size\n\n    # Define the mask function for chunk attention\n    def chunked_causal_mask_function(q_ids, kv_ids):\n      \"\"\"Computes the mask logic for the given slice indices.\"\"\"\n      if q_ids.size == 0 or kv_ids.size == 0:\n        return np.empty((q_ids.shape[0], kv_ids.shape[1]), dtype=np.bool_)\n\n      # Condition 1: Same chunk\n      q_chunk = q_ids // self.chunk_size\n      kv_chunk = kv_ids // self.chunk_size\n      same_chunk = q_chunk == kv_chunk\n\n      # Condition 2: Causal\n      causal = q_ids >= kv_ids\n\n      return same_chunk & causal\n\n    # Initialize the parent ComputableMask with this function\n    super().__init__(\n        shape=shape,\n        mask_function=chunked_causal_mask_function,\n        shard_count=shard_count,\n    )\n\n  # Implement equality and hashing based on relevant attributes\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n    # Compare shape, chunk_size, and the underlying q_sequence array\n    return (\n        self.shape == other.shape\n        and self.chunk_size == other.chunk_size\n        and np.array_equal(self.q_sequence, other.q_sequence)\n    )\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.chunk_size,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "chunked_causal_mask",
            "purpose": "Implements a lazy, on-the-fly computable mask for chunked causal attention, where attention is causal within fixed-size chunks but disallowed across them.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes the parent `_ComputableMask` with a custom `mask_function`.",
                "The `mask_function` computes the mask by checking two conditions: if the query and key indices are in the same chunk, and if the query index is greater than or equal to the key index (causality)."
            ],
            "output": {
                "shape": "An object that can be used by Splash Attention to compute a boolean mask of the shape provided during initialization (e.g., [q_sequence_length, kv_sequence_length])."
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "numpy"
            ],
            "parameters": {
                "chunk_size": "The size of each attention chunk within which causal attention is applied.",
                "shape": "A tuple (q_sequence_length, kv_sequence_length) defining the full dimensions of the conceptual attention mask."
            },
            "notes": [
                "This is a 'lazy' mask designed for Splash Attention, avoiding the materialization of the full boolean mask matrix in memory.",
                "The mask logic enforces that a token can only attend to tokens within the same chunk and to previous tokens within that chunk."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the lazy mask by defining and passing a chunked causal mask function to its parent class.",
                    "input": {
                        "shape": "shape: tuple[int, int], chunk_size: int, shard_count: int",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate that chunk_size is positive.",
                        "Define an inner `chunked_causal_mask_function` that calculates if query/key indices are in the same chunk and satisfy causality.",
                        "Call the parent `_ComputableMask` constructor with the shape and the custom mask function."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "splash_attention_mask._ComputableMask.__init__",
                        "numpy"
                    ],
                    "notes": [
                        "The core logic is encapsulated within the `chunked_causal_mask_function` which is passed to the superclass for on-the-fly computation."
                    ]
                },
                "__eq__": {
                    "purpose": "Defines the equality condition for two ChunkedCausalMask objects.",
                    "input": {
                        "shape": "other: object",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the other object is of the same type.",
                        "Compare `shape`, `chunk_size`, and the `q_sequence` array between self and other."
                    ],
                    "output": {
                        "shape": "A boolean value."
                    },
                    "dependencies": [
                        "numpy.array_equal"
                    ],
                    "notes": [
                        "Equality depends on the configuration parameters and the inherited `q_sequence` array."
                    ]
                },
                "__hash__": {
                    "purpose": "Computes a hash value for the mask object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a tuple of the object's type, shape, chunk_size, and the byte representation of the `q_sequence` array.",
                        "Compute the hash of the tuple."
                    ],
                    "output": {
                        "shape": "An integer hash value."
                    },
                    "dependencies": [],
                    "notes": [
                        "Allows instances of this class to be used as keys in dictionaries or elements in sets."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_generate_chunk_attention_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _generate_chunk_attention_mask(mask_shape: tuple[int, int], chunk_size: int, q_offset: int = 0) -> jax.Array:\n  \"\"\"Generates an explicit boolean mask for chunked causal attention.\n\n  This function computes the full boolean mask array where True indicates\n  attention is allowed based on chunked causal rules (tokens attend only\n  within the same chunk, and causally within that chunk).\n\n  Args:\n    mask_shape: The desired shape of the mask (q_seq_len, kv_seq_len).\n    chunk_size: The size of the attention chunks.\n\n  Returns:\n    A boolean mask of shape `mask_shape` where True indicates attention is\n    allowed according to chunked causal rules, and False otherwise.\n\n  Raises:\n    ValueError: If chunk_window_size is None or not positive.\n  \"\"\"\n\n  row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0) + q_offset\n  col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n  if chunk_size <= 0:\n    raise ValueError(\"chunk_size must be positive\")\n\n  # chunk mask calculation\n  same_chunk = (row_ids // chunk_size) == (col_ids // chunk_size)\n  chunk_mask = same_chunk & (row_ids >= col_ids)\n  return chunk_mask",
        "analysis": {
            "module_type": "chunked_causal_attention_mask_generator",
            "purpose": "Generates a full boolean attention mask that enforces chunked causal attention, where tokens can only attend to previous tokens within the same chunk.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Generate row and column indices (`row_ids`, `col_ids`) for the target mask shape using `jax.lax.broadcasted_iota`.",
                "Apply the `q_offset` to the `row_ids`.",
                "Validate that `chunk_size` is a positive integer, raising a ValueError otherwise.",
                "Create a `same_chunk` boolean mask by checking if `row_ids // chunk_size` equals `col_ids // chunk_size`.",
                "Create a causal mask by checking if `row_ids >= col_ids`.",
                "Combine the `same_chunk` mask and the causal mask using a logical AND operation.",
                "Return the resulting boolean mask."
            ],
            "output": {
                "shape": "[q_seq_len, kv_seq_len] (as specified by the `mask_shape` input)."
            },
            "dependencies": [
                "jax.lax.broadcasted_iota",
                "jax.numpy"
            ],
            "parameters": {
                "mask_shape": "A tuple `(q_seq_len, kv_seq_len)` specifying the dimensions of the output mask.",
                "chunk_size": "The integer size of each attention chunk.",
                "q_offset": "An integer offset added to the query (row) indices, useful for chunked prefill scenarios."
            },
            "notes": [
                "The function returns a dense boolean array, materializing the full mask in memory.",
                "The mask logic ensures that attention is both causal (`row_ids >= col_ids`) and restricted to tokens within the same chunk.",
                "`True` values in the output mask indicate that attention is allowed, while `False` values indicate it is prohibited."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_block_mask_indices",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_block_mask_indices(bidirectional_mask):\n  \"\"\"Creates block mask identifying segments based on a bidirectional mask.\n\n  Args:\n    bidirectional_mask: boolean mask, e.g. [011110011010].\n\n  Returns:\n    block mask for segments, e.g. [011110022030].\n  \"\"\"\n  # Left pad 0.\n  padded_mask = jnp.pad(bidirectional_mask, [(0, 0), (1, 0)], constant_values=0)\n  boundary = padded_mask[..., 1:] > padded_mask[..., :-1]\n  numbered_boundary = jnp.cumsum(boundary, axis=-1)\n  return bidirectional_mask * numbered_boundary",
        "analysis": {
            "functionality": "This function takes a 2D boolean or integer mask and identifies contiguous non-zero segments along the last dimension. It then assigns a unique, incrementing integer ID to each segment, effectively converting a mask like `[0, 1, 1, 0, 1, 1]` into `[0, 1, 1, 0, 2, 2]`.",
            "usage": "This is a helper function used to create indexed masks for block-wise attention mechanisms. To use it, pass a JAX numpy array of shape `[batch_size, sequence_length]` representing the bidirectional mask. The function returns a JAX numpy array of the same shape with integer segment IDs."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#_make_bidirectional_block_mask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def _make_bidirectional_block_mask(bidirectional_mask):\n  \"\"\"Creates bidirectional block mask from bidirectional_mask, where True corresponds to image tokens.\n  bidirectional_mask shape: [B, L]\n  bidirectional_block_mask shape: [B, L, L]\n  Examples:\n  bidirectional_mask = [[0, 1, 1, 1, 0, 0]]\n  bidirectional_block_mask = [[\n      [False, False, False, False, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False,  True,  True,  True, False, False],\n      [False, False, False, False, False, False],\n      [False, False, False, False, False, False],\n  ]]\n  \"\"\"\n  q_block_indices = _make_block_mask_indices(bidirectional_mask)\n  kv_block_indices = q_block_indices\n  bidirectional_block_mask = (kv_block_indices[:, None, :] == q_block_indices[..., None]) & (\n      q_block_indices[..., None] > 0\n  )\n  return bidirectional_block_mask",
        "analysis": {
            "functionality": "Creates a 2D bidirectional block attention mask from a 1D sequence mask. This allows tokens within contiguous segments (marked as '1' or 'True' in the input) to attend to each other, while preventing attention between different segments or to non-segment tokens.",
            "usage": "This function is typically used to generate attention masks for multimodal models, where certain parts of a sequence (e.g., image tokens) should have full self-attention within their block but be isolated from other parts (e.g., text tokens). Call the function with a 2D tensor `bidirectional_mask` of shape `[batch_size, sequence_length]`. It returns a 3D boolean tensor of shape `[batch_size, sequence_length, sequence_length]`."
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#attention_op_as_linen",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "def attention_op_as_linen(\n    *,\n    config: Config,\n    mesh: Mesh,\n    attention_kernel: str,\n    max_target_length: int,\n    num_query_heads: int,\n    num_kv_heads: int,\n    float32_qk_product: bool = False,\n    max_prefill_predict_length: int = -1,\n    float32_logits: bool = False,\n    flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n    flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n    flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n    flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n    flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n    ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n    compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    reshape_q: bool = False,\n    dropout_rate: float = 0.0,\n    dtype: DType = jnp.float32,\n    quant: Optional[Quant] = None,\n    kv_quant: Optional[KVQuant] = None,\n    attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n    attn_logits_soft_cap: float | None = None,\n    sliding_window_size: int | None = None,\n    chunk_attn_window_size: int | None = None,\n    use_ragged_attention: bool = False,\n    ragged_block_size: int = 256,\n):\n  \"\"\"A factory function to create an AttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `AttentionOp` within a\n  Linen model.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      AttentionOp,\n      config=config,\n      mesh=mesh,\n      attention_kernel=attention_kernel,\n      max_target_length=max_target_length,\n      num_query_heads=num_query_heads,\n      num_kv_heads=num_kv_heads,\n      float32_qk_product=float32_qk_product,\n      max_prefill_predict_length=max_prefill_predict_length,\n      float32_logits=float32_logits,\n      flash_axis_names_q=flash_axis_names_q,\n      flash_axis_names_q_ep=flash_axis_names_q_ep,\n      flash_axis_names_kv=flash_axis_names_kv,\n      flash_axis_names_kv_ep=flash_axis_names_kv_ep,\n      flash_axis_names_splash_kernel=flash_axis_names_splash_kernel,\n      flash_axis_names_splash_kernel_ep=flash_axis_names_splash_kernel_ep,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      ragged_qkv_axis_names=ragged_qkv_axis_names,\n      ragged_lengths_names=ragged_lengths_names,\n      compute_axis_order=compute_axis_order,\n      key_axis_order=key_axis_order,\n      reshape_q=reshape_q,\n      dropout_rate=dropout_rate,\n      dtype=dtype,\n      quant=quant,\n      kv_quant=kv_quant,\n      attention_type=attention_type,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      sliding_window_size=sliding_window_size,\n      chunk_attn_window_size=chunk_attn_window_size,\n      use_ragged_attention=use_ragged_attention,\n      ragged_block_size=ragged_block_size,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "attention_op_factory",
            "purpose": "A factory function that wraps the NNX-based `AttentionOp` class to create a Flax Linen-compatible module, serving as a bridge between the two frameworks.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `AttentionOp` NNX module into a Linen module.",
                "Passes all its configuration arguments to the `AttentionOp` constructor via the wrapper.",
                "Specifies `variable_to_logically_partitioned` as the `metadata_fn` for parameter partitioning."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "AttentionOp",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "config": "The main configuration object for the model.",
                "mesh": "The JAX device mesh for distributed computation.",
                "attention_kernel": "A string specifying which attention implementation to use (e.g., 'flash', 'dot_product', 'autoselected').",
                "num_query_heads": "The number of attention heads for the query.",
                "num_kv_heads": "The number of attention heads for key and value, enabling Grouped-Query Attention (GQA).",
                "attention_type": "The type of attention pattern to apply (e.g., GLOBAL, LOCAL_SLIDING, CHUNK).",
                "use_ragged_attention": "A boolean flag to enable ragged attention for autoregressive decoding.",
                "sliding_window_size": "The window size for local sliding window attention.",
                "chunk_attn_window_size": "The window size for chunked attention."
            },
            "notes": [
                "This function acts as a compatibility bridge, allowing an NNX module to be used within a Flax Linen model.",
                "It takes a large number of configuration parameters that are forwarded to the underlying `AttentionOp` module."
            ]
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#AttentionOp",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class AttentionOp(nnx.Module):\n  \"\"\"Attention operation\"\"\"\n\n  def __init__(\n      self,\n      config: Config,\n      mesh: Mesh,\n      attention_kernel: str,\n      max_target_length: int,\n      num_query_heads: int,\n      num_kv_heads: int,\n      float32_qk_product: bool = False,\n      max_prefill_predict_length: int = -1,\n      float32_logits: bool = False,\n      flash_axis_names_q: AxisNames = (BATCH, HEAD, LENGTH_NO_EXP, D_KV),\n      flash_axis_names_q_ep: AxisNames = (BATCH_NO_EXP, HEAD, LENGTH, D_KV),\n      flash_axis_names_kv: AxisNames = (BATCH, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_kv_ep: AxisNames = (BATCH_NO_EXP, HEAD, KV_LENGTH, D_KV),\n      flash_axis_names_splash_kernel: AxisNames = (HEAD, LENGTH_NO_EXP),\n      flash_axis_names_splash_kernel_ep: AxisNames = (HEAD, LENGTH),\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      ragged_qkv_axis_names: AxisNames = (CACHE_BATCH, CACHE_HEADS, CACHE_SEQUENCE, CACHE_KV),\n      ragged_lengths_names: AxisNames = (CACHE_BATCH,),\n      compute_axis_order: AxisIdxes = (0, 1, 2, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      reshape_q: bool = False,\n      dropout_rate: float = 0.0,\n      dtype: DType = jnp.float32,\n      quant: Optional[Quant] = None,\n      kv_quant: Optional[KVQuant] = None,\n      attention_type: AttentionType = AttentionType.GLOBAL,  # Default to global attention\n      attn_logits_soft_cap: float | None = None,\n      sliding_window_size: int | None = None,\n      chunk_attn_window_size: int | None = None,\n      use_ragged_attention: bool = False,\n      ragged_block_size: int = 256,\n      rngs: nnx.Rngs | None = None,\n  ):\n    \"\"\"Initializes the AttentionOp module.\n\n    Args:\n      config: The configuration for the model.\n      mesh: The device mesh.\n      attention_kernel: The attention kernel to use.\n      max_target_length: The maximum target length.\n      num_query_heads: The number of query heads.\n      num_kv_heads: The number of key/value heads.\n      float32_qk_product: Whether to compute qk_product in float32.\n      max_prefill_predict_length: The maximum prefill predict length.\n      float32_logits: Whether to compute logits in float32.\n      flash_axis_names_kv: The logical axis names for the KV cache in flash attention.\n      flash_axis_names_q: The logical axis names for the query in flash attention.\n      flash_axis_names_splash_kernel: The logical axis names for the splash attention kernel.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      ragged_qkv_axis_names: The logical axis names for ragged QKV tensors.\n      ragged_lengths_names: The logical axis names for ragged lengths.\n      compute_axis_order: The order of axes for computation.\n      key_axis_order: The order of axes for the key.\n      ... and other configuration parameters.\n      rngs: The random number generators for initialization, passed by the nnx.to_linen wrapper.\n    \"\"\"\n    self.config = config\n    self.mesh = mesh\n    self.attention_kernel = attention_kernel\n    self.max_target_length = max_target_length\n    self.num_query_heads = num_query_heads\n    self.num_kv_heads = num_kv_heads\n    self.float32_qk_product = float32_qk_product\n    self.max_prefill_predict_length = max_prefill_predict_length\n    self.float32_logits = float32_logits\n    self.flash_axis_names_q = flash_axis_names_q\n    self.flash_axis_names_q_ep = flash_axis_names_q_ep\n    self.flash_axis_names_kv = flash_axis_names_kv\n    self.flash_axis_names_kv_ep = flash_axis_names_kv_ep\n    self.flash_axis_names_splash_kernel = flash_axis_names_splash_kernel\n    self.flash_axis_names_splash_kernel_ep = flash_axis_names_splash_kernel_ep\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.ragged_qkv_axis_names = ragged_qkv_axis_names\n    self.ragged_lengths_names = ragged_lengths_names\n    self.compute_axis_order = compute_axis_order\n    self.key_axis_order = key_axis_order\n    self.reshape_q = reshape_q\n    self.dropout_rate = dropout_rate\n    self.dtype = dtype\n    self.quant = quant\n    self.kv_quant = kv_quant\n    self.attention_type = attention_type\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.sliding_window_size = sliding_window_size\n    self.chunk_attn_window_size = chunk_attn_window_size\n    self.use_ragged_attention = use_ragged_attention\n    self.ragged_block_size = ragged_block_size\n\n    def maybe_create_nnx(einsum, *args):\n      if isinstance(einsum, nn.Module):\n        return nnx_wrappers.ToNNX(einsum, rngs=rngs).lazy_init(*args)\n      return einsum\n\n    # qk_product\n    if self.kv_quant:\n      # Dummy inputs for lazy initialization\n      b = 1\n      t_prefill = self.max_prefill_predict_length\n      t_ar = 1  # Autoregressive mode has a query length of 1\n      n = self.num_query_heads\n      n_kv = self.num_kv_heads\n      d = self.config.head_dim\n      g = n // n_kv\n      s_prefill = self.max_prefill_predict_length\n      s_ar = self.max_target_length\n\n      # Dummy query/key/value shapes as before...\n      dummy_query_prefill = jnp.zeros((b, t_prefill, n_kv, g, d), dtype=self.dtype)\n      dummy_key_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_query_ar = jnp.zeros((b, t_ar, n_kv, g, d), dtype=self.dtype)\n      dummy_key_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      dummy_attn_weights_prefill = jnp.zeros((b, n_kv, g, t_prefill, s_prefill), dtype=jnp.float32)\n      dummy_value_prefill = jnp.zeros((b, s_prefill, n_kv, d), dtype=self.dtype)\n      dummy_attn_weights_ar = jnp.zeros((b, n_kv, g, t_ar, s_ar), dtype=jnp.float32)\n      dummy_value_ar = jnp.zeros((b, s_ar, n_kv, d), dtype=self.dtype)\n\n      # Prefill AqtEinsum instances\n      self.AqtEinsum_0 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_prefill, dummy_key_prefill\n      )\n      self.AqtEinsum_1 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_prefill,\n          dummy_value_prefill,\n      )\n      # Autoregressive AqtEinsum instances\n      self.AqtEinsum_2 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor(), \"btkgd,bskd->bkgts\", dummy_query_ar, dummy_key_ar\n      )\n      self.AqtEinsum_3 = maybe_create_nnx(\n          self.kv_quant.einsum_fn_with_rhs_qtensor_and_dequant(),\n          \"bkgts,bskd->btkgd\",\n          dummy_attn_weights_ar,\n          dummy_value_ar,\n      )\n    else:\n      self.AqtEinsum_0 = jnp.einsum\n      self.AqtEinsum_1 = jnp.einsum\n      self.AqtEinsum_2 = jnp.einsum\n      self.AqtEinsum_3 = jnp.einsum\n\n  def check_attention_inputs(self, query: Array, key: Array | KVTensor, value: Array | KVTensor) -> None:\n    \"\"\"Check attention inputs.\"\"\"\n\n    assert key.ndim == value.ndim, f\"k (dim {key.ndim}), v (dim {value.ndim}) must have same rank.\"\n    assert query.shape[:-3] == key.shape[:-3] == value.shape[:-3], \"q, k, v batch dims must match.\"\n    assert key.shape[-2] == value.shape[-2], \"k, v num_kv_heads must match.\"\n    assert key.shape[-3] == value.shape[-3], \"k, v lengths must match.\"\n    assert query.shape[-1] == key.shape[-1], \"q, k depths must match.\"\n\n  def generate_attention_mask(\n      self,\n      query,\n      key,\n      decoder_segment_ids: Array | None,\n      model_mode: str,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n  ) -> Array | None:\n    \"\"\"Generates a combined attention mask for Transformer models.\n\n    This function constructs an attention mask by potentially combining\n    several types of masks based on the input parameters and model\n    configuration. The generated mask dictates which query-key pairs are\n    allowed to attend to each other.\n\n    The masking logic can enforce:\n    1.  **Sequence Separation:** Using `decoder_segment_ids`, attention is\n      confined within distinct sequences in a batch. This is crucial when\n      multiple unrelated sequences are packed together.\n    2.  **Causality:** Preventing attention to future positions. This is\n      standard for autoregressive decoding. For chunked prefill, as\n      described in the SARATHI paper [2], causality is adjusted based\n      on `previous_chunk` information.\n    3.  **Specialized Attention Patterns:** Depending on `self.attention_type`,\n      it can apply:\n      * Local Sliding Window Attention: Restricts attention to a\n          fixed-size window around each query position.\n      * Chunk Attention: Divides sequences into chunks and applies\n          masking at the chunk level.\n    4.  **Bidirectional Attention for Sub-sequences:** If `bidirectional_mask`\n      is provided (e.g., for image tokens in a multimodal model),\n      those parts of the sequence can attend bidirectionally, and this\n      mask is OR-ed with other generated masks.\n\n    The overall approach and specific masking techniques are influenced by\n    efficient attention mechanisms like those found in the Pallas MHA\n    Flash Attention reference [1].\n\n    Args:\n      query: The query tensor, typically of shape\n          `[batch_size, q_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      key: The key tensor, typically of shape\n          `[batch_size, kv_sequence_length, num_heads, head_dim]`.\n          Used primarily for deriving sequence length.\n      decoder_segment_ids: Optional `Array` of shape `[batch_size, q_sequence_length]`.\n          Identifies distinct sequences within the batch. Attention is\n          restricted to elements within the same segment ID. In autoregressive\n          mode, specific values (e.g., `common_types.DECODING_ACTIVE_SEQUENCE_INDICATOR`)\n          can mark the currently active sequence for decoding.\n      model_mode: A string (e.g., `common_types.MODEL_MODE_AUTOREGRESSIVE`,\n          `MODEL_MODE_PREFILL`) indicating the operational\n          mode. This significantly influences mask generation, particularly\n          how causality and segment separation are handled.\n      previous_chunk: Optional. Information about previously processed\n          key/value chunks, often a tensor representing the previous keys/values.\n          Used to correctly offset causal masks in chunked attention or\n          streaming scenarios. Its shape might be\n          `[batch_size, prev_kv_sequence_length, ...]`.\n      bidirectional_mask: Optional `Array` of shape `[batch_size, kv_sequence_length]`.\n          If provided, this boolean mask indicates tokens (e.g., image tokens)\n          that are allowed to attend bidirectionally. The resulting\n          block-wise bidirectional mask is combined with other masks using a\n          logical OR.\n\n    Returns:\n      An `Array` representing the attention mask, broadcastable to the shape\n      `[batch_size, num_heads, q_sequence_length, kv_sequence_length]`.\n      Positions with `0.0` allow attention, while positions with\n      `DEFAULT_MASK_VALUE` (a large negative number) prevent it.\n      Returns `None` if no masking is determined to be necessary based on\n      the inputs and configuration.\n\n    References:\n      [1] JAX Pallas MHA Flash Attention:\n          https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py\n      [2] SARATHI: Efficient LLM Inference by Piggybacking Decodes with\n          Chunked Prefills - ArXiv:2308.16369 (https://arxiv.org/abs/2308.16369)\n    \"\"\"\n    mask = None\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      mask = decoder_segment_ids[:, None, None, None, :] == DECODING_ACTIVE_SEQUENCE_INDICATOR\n    elif decoder_segment_ids is not None:\n      mask = decoder_segment_ids[:, :, None] == decoder_segment_ids[:, None, :]\n      mask = mask[:, None, None, :, :]\n\n    _, q_seq_len, _, _ = query.shape\n    _, kv_seq_len, _, _ = key.shape\n    next_pos = 0\n    if previous_chunk is not None:\n      next_pos = previous_chunk.shape[1]\n      if mask is not None:\n        mask = mask[:, :, :, next_pos : next_pos + q_seq_len, :]\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and q_seq_len == 1:\n      # In autoregression, the query position is the last position in the KV sequence.\n      next_pos = kv_seq_len - 1\n\n    causal_mask = None\n    # We enforce causality except for AUTOREGRESSION\n    if model_mode != MODEL_MODE_AUTOREGRESSIVE and self.attention_type != AttentionType.FULL:\n      mask_shape = (q_seq_len, kv_seq_len)\n      # row_ids indicates the position of query\n      # col_ids indicates the position of kv\n      row_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)\n      col_ids = jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)\n      # Attention mask for chunked prefill is generated in the same way\n      # as mentioned in SARATHI - https://arxiv.org/abs/2308.16369\n      causal_mask = (col_ids <= row_ids + next_pos)[None, None, None, :, :]\n\n    output_mask = None\n    if (mask is not None) and (causal_mask is not None):\n      output_mask = jnp.logical_and(mask, causal_mask)\n    elif mask is not None:\n      output_mask = mask\n    elif causal_mask is not None:\n      output_mask = causal_mask\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and output_mask is not None:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n\n      row_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (q_seq_len, 1), 0) + next_pos\n      col_ids_sliding = jax.lax.broadcasted_iota(jnp.int32, (1, kv_seq_len), 1)\n      sliding_mask = (col_ids_sliding > (row_ids_sliding - self.sliding_window_size)) & (\n          col_ids_sliding <= row_ids_sliding\n      )\n      output_mask = sliding_mask * output_mask\n    elif self.attention_type == AttentionType.CHUNK and output_mask is not None:\n      mask_shape = (q_seq_len, kv_seq_len)\n      chunk_mask = _generate_chunk_attention_mask(\n          mask_shape=(q_seq_len, kv_seq_len), chunk_size=self.chunk_attn_window_size, q_offset=next_pos\n      )\n      output_mask = chunk_mask * output_mask\n\n    if bidirectional_mask is not None:\n      image_mask = _make_bidirectional_block_mask(bidirectional_mask)\n      output_mask = output_mask | image_mask[:, None, None, ...]\n\n    return jnp.where(output_mask, 0.0, DEFAULT_MASK_VALUE) if output_mask is not None else None\n\n  def apply_attention(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      lengths: Array | None,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply attention\"\"\"\n    self.check_attention_inputs(query, key, value)\n    length = query.shape[-3]\n    target_hardware = self.mesh.devices[(0,) * self.mesh.devices.ndim].platform\n\n    if use_ragged_attention and model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      if lengths is None:\n        lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      if target_hardware == \"tpu\":\n        impl = self.tpu_ragged_attention\n      elif target_hardware == \"gpu\":\n        impl = self.gpu_ragged_attention\n      else:\n        raise NotImplementedError(target_hardware)\n      return impl(query, key, value, lengths, self.ragged_block_size)\n\n    elif (\n        self.attention_kernel == \"dot_product\"\n        or (self.attention_kernel == \"autoselected\" and model_mode == MODEL_MODE_AUTOREGRESSIVE)\n        or (self.attention_kernel == \"autoselected\" and length < 128)\n        or (self.attention_kernel == \"paged\")\n    ):\n      return self.apply_attention_dot(\n          query,\n          key,\n          value,\n          decoder_segment_ids,\n          model_mode,\n          previous_chunk,\n          bidirectional_mask=bidirectional_mask,\n          sinks=sinks,\n          qk_product_einsum=qk_product_einsum,\n          wv_product_einsum=wv_product_einsum,\n      )\n    elif self.attention_kernel in (\"flash\", \"autoselected\"):\n      if target_hardware == \"tpu\":\n        if isinstance(key, KVTensor):\n          key = key.dequant()\n        if isinstance(value, KVTensor):\n          value = value.dequant()\n\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          raise ValueError(\n              \"\"\"Decode not supported with flash attention.\n                              Use `dot_product` instead.\"\"\"\n          )\n        return self.tpu_flash_attention(query, key, value, decoder_segment_ids, self.attn_logits_soft_cap, sinks), None, None\n      else:\n        validate_flash_attention_with_sinks_on_gpu(sinks)\n        if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n          # fallback to dot_product as pallas gpu flash attention doesn't support decode stage\n          return self.apply_attention_dot(\n              query,\n              key,\n              value,\n              decoder_segment_ids,\n              model_mode,\n              bidirectional_mask=bidirectional_mask,\n              qk_product_einsum=qk_product_einsum,\n              wv_product_einsum=wv_product_einsum,\n          )\n        else:\n          head_axis = -2\n          num_query_heads = query.shape[head_axis]\n          num_kv_heads = key.shape[head_axis]\n          if num_query_heads != num_kv_heads:\n            # Handle cases where the number of query heads is different from the number of key/value heads.\n            if num_query_heads % num_kv_heads != 0:\n              raise ValueError(\n                  f\"Number of query heads ({num_query_heads}) must be divisible by number of key/value heads ({num_kv_heads}).\"\n              )\n            # TODO Investigate if the KV copy can be eliminated. It's likely redundant.\n            q_heads_per_kv_head = num_query_heads // num_kv_heads\n\n            key = jnp.repeat(\n                key, q_heads_per_kv_head, axis=head_axis\n            )  # key shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n            value = jnp.repeat(\n                value, q_heads_per_kv_head, axis=head_axis\n            )  # value shape [batch_size, kv_seq_len, num_kv_heads, head_dim]\n          out = gpu_pallas_attention.mha(query, key, value, decoder_segment_ids, sm_scale=1.0, causal=True)\n          return out, None, None\n    elif self.attention_kernel == \"cudnn_flash_te\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n        raise ValueError(\n            \"\"\"Decode not supported with flash attention.\n                           Use `dot_product` instead.\"\"\"\n        )\n      return self.cudnn_flash_attention(query, key, value, decoder_segment_ids, model_mode), None, None\n    elif self.attention_kernel == \"cudnn_flash_jax\":\n      validate_flash_attention_with_sinks_on_gpu(sinks)\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      if isinstance(value, KVTensor):\n        value = value.dequant()\n      return *self.cudnn_jax_flash_attention(query, key, value, decoder_segment_ids, model_mode), None\n    else:\n      raise ValueError(f\"Unexpected attention kernel {self.attention_kernel=}.\")\n\n  def gpu_ragged_attention(self, q: Array, k: Array | KVTensor, v: Array | KVTensor, lengths: Array, block_size: int):\n    \"\"\"gpu ragged attention\"\"\"\n    batch_size, q_length, q_heads, head_dim = q.shape\n\n    # Reshape q to match gqa's expected shape\n    q_for_gqa = q.squeeze(axis=1)\n\n    # Define logical axis names - clearer and avoids repeated calls.\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n    bnd = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS, CACHE_KV))\n    bn = nn.logical_to_mesh_axes((CACHE_BATCH, CACHE_HEADS))\n\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(bnd, bsnd, bsnd, b, None),\n        out_specs=(bnd, bn, bn),\n        check_rep=False,\n    )\n    def wrap_ragged_attention(\n        q: Array, k: Array, v: Array, lengths: Array, block_size: int\n    ) -> Tuple[Array, Array, Array]:\n      # Use the original gqa function to get the attention output\n      \"\"\"\n      Wraps the GQA function with appropriate sharding.\n\n      Args:\n          q: Query tensor.\n          k: Key tensor.\n          v: Value tensor.\n          lengths: Sequence lengths.\n          block_size: Block size for attention.\n\n      Returns:\n          A tuple containing the output, max, and sum tensors.\n      \"\"\"\n      # Use the original gqa function to get the attention output\n      local_out, (local_sum, local_max) = gpu_pallas_decode_attention.gqa(\n          q=q,\n          k=k,\n          v=v,\n          kv_seq_len=lengths,\n          block_k=block_size,\n          sm_scale=1.0,\n          return_residuals=True,\n          normalize_output=False,\n      )\n      return local_out, local_max, local_sum\n\n    local_out, local_max, local_sum = wrap_ragged_attention(q_for_gqa, k, v, lengths, block_size)\n\n    # Reshape local_out, local_max and local_sum to match Maxtext requirements\n    local_out = local_out.reshape(batch_size, q_length, q_heads, head_dim)\n    local_max = local_max.reshape(batch_size, q_length, q_heads, 1)\n    local_sum = local_sum.reshape(batch_size, q_length, q_heads, 1)\n    return local_out, local_max, local_sum\n\n  def tpu_ragged_attention(\n      self, query: Array, key: Array | KVTensor, value: Array | KVTensor, lengths: Array, block_size: int\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Ragged Attention.\"\"\"\n    if isinstance(query, KVTensor):\n      raise TypeError(\"Ragged attention does not currently support quantized tensors.\")\n    b = nn.logical_to_mesh_axes(self.ragged_lengths_names)\n    bsnd = nn.logical_to_mesh_axes(self.cache_logical_axis_names)\n\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            bsnd,\n            bsnd,\n            bsnd,\n            b,\n            None,\n        ),\n        out_specs=bsnd,\n        check_rep=False,\n    )\n    def wrap_ragged_attention(query, key, value, lengths, block_size):\n      if query.shape[-2] == key.shape[-2]:\n        return ragged_mha(query, key, value, lengths, block_size=block_size)\n      else:\n        return ragged_gqa(query, key, value, lengths, block_size=block_size)\n\n    return wrap_ragged_attention(query, key, value, lengths, block_size)\n\n  def tpu_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      attn_logits_soft_cap: float | None = None,\n      sinks: Array | None = None,\n  ) -> Array:\n    \"\"\"TPU Flash Attention.\"\"\"\n\n    cp_size = self.config.context_parallel_size\n    load_balanced_context_parallel = self.config.context_parallel_load_balance\n\n    # Transpose to ('batch', 'heads', 'length', 'kv')\n    query = jnp.transpose(query, axes=(0, 2, 1, 3))\n    key = jnp.transpose(key, axes=(0, 2, 1, 3))\n    value = jnp.transpose(value, axes=(0, 2, 1, 3))\n    segment_axis_names_q = None\n    segment_axis_names_kv = None\n    sink_axis_names = nn.logical_to_mesh_axes((HEAD,))\n    if decoder_segment_ids is not None:\n      if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH_NO_EXP, Q_LENGTH))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH_NO_EXP, KV_LENGTH))\n      else:\n        segment_axis_names_q = nn.logical_to_mesh_axes((BATCH, Q_LENGTH_NO_EXP))\n        segment_axis_names_kv = nn.logical_to_mesh_axes((BATCH, KV_LENGTH))\n\n    if self.config.expert_shard_attention_option == EP_AS_CONTEXT:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel_ep)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q_ep)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv_ep)\n    else:\n      axis_names_splash_kernel = nn.logical_to_mesh_axes(self.flash_axis_names_splash_kernel)\n      axis_names_q = nn.logical_to_mesh_axes(self.flash_axis_names_q)\n      axis_names_kv = nn.logical_to_mesh_axes(self.flash_axis_names_kv)\n\n    global global_block_q, global_block_kv, global_block_kv_compute, global_block_q_dkv, global_block_kv_dkv\n    global global_block_kv_dkv_compute, global_block_q_dq, global_block_kv_dq, global_use_fused_bwd_kernel\n    global global_q_layout, global_k_layout, global_v_layout\n    global_block_q = self.config.sa_block_q\n    global_block_kv = self.config.sa_block_kv\n    global_block_kv_compute = self.config.sa_block_kv_compute\n    global_block_q_dkv = self.config.sa_block_q_dkv\n    global_block_kv_dkv = self.config.sa_block_kv_dkv\n    global_block_kv_dkv_compute = self.config.sa_block_kv_dkv_compute\n    global_block_q_dq = self.config.sa_block_q_dq\n    global_block_kv_dq = self.config.sa_block_kv_dq\n    global_use_fused_bwd_kernel = self.config.sa_use_fused_bwd_kernel\n    global_q_layout = self.config.sa_q_layout\n    global_k_layout = self.config.sa_k_layout\n    global_v_layout = self.config.sa_v_layout\n\n    devices_in_data_fsdp = self.mesh.shape[\"data\"] * self.mesh.shape[\"fsdp\"]\n    assert (query.shape[0] / devices_in_data_fsdp).is_integer(), (\n        \"Batch dimension should be shardable among the devices in data and fsdp\"\n        \" axis\"\n        f\" got {query.shape[0]=}/{devices_in_data_fsdp=}\"\n    )\n\n    # create_splash_attention kernel\n    block_sizes = splash_attention_kernel.BlockSizes(\n        block_q=min(global_block_q, query.shape[2]),\n        block_kv=min(global_block_kv, key.shape[2]),\n        block_kv_compute=min(global_block_kv_compute, key.shape[2]),\n        block_q_dkv=min(global_block_q_dkv, query.shape[2]),\n        block_kv_dkv=min(global_block_kv_dkv, key.shape[2]),\n        block_kv_dkv_compute=min(global_block_kv_dkv_compute, query.shape[2]),\n        block_q_dq=None if global_use_fused_bwd_kernel else min(global_block_q_dq, query.shape[2]),\n        block_kv_dq=None if global_use_fused_bwd_kernel else min(global_block_kv_dq, query.shape[2]),\n        use_fused_bwd_kernel=global_use_fused_bwd_kernel,\n        q_layout=splash_attention_kernel.QKVLayout[global_q_layout],\n        k_layout=splash_attention_kernel.QKVLayout[global_k_layout],\n        v_layout=splash_attention_kernel.QKVLayout[global_v_layout],\n    )\n\n    mask_shape = (query.shape[2], key.shape[2])  # (q_seq_len, kv_seq_len)\n    if self.attention_type == AttentionType.FULL:\n      mask = splash_attention_mask.FullMask(mask_shape)\n    else:\n      mask = splash_attention_mask.CausalMask(shape=mask_shape)\n\n    # Create LoadBalancedCausalMask if cp and load_balancing\n    if cp_size > 1 and load_balanced_context_parallel:\n      mask = LoadBalancedCausalMask(shape=mask_shape, cp_size=cp_size)\n\n    # TODO: figure out local_sliding attention + load_balancing, default is global\n    # Apply local masking if local sliding attention is enabled.\n    if self.attention_type == AttentionType.LOCAL_SLIDING:\n      if self.sliding_window_size is None:\n        raise ValueError(\"Sliding_window_size must be set if Local Sliding attention type\")\n      mask &= splash_attention_mask.LocalMask(\n          shape=(query.shape[2], key.shape[2]),\n          window_size=(self.sliding_window_size, self.sliding_window_size),\n          offset=0,\n      )\n      # Apply local masking if local sliding attention is enabled.\n      if self.attention_type == AttentionType.LOCAL_SLIDING:\n        if self.sliding_window_size is None:\n          raise ValueError(\"Sliding_window_size must be set for Local Sliding attention type\")\n        mask &= splash_attention_mask.LocalMask(\n            shape=(query.shape[2], key.shape[2]),\n            window_size=(self.sliding_window_size, self.sliding_window_size),\n            offset=0,\n        )\n      elif self.attention_type == AttentionType.CHUNK:\n        if self.chunk_attn_window_size is None:\n          raise ValueError(\"chunk_attn_window_size must be set for chunk attention type\")\n\n        mask &= ChunkedCausalMask(shape=(query.shape[2], key.shape[2]), chunk_size=self.chunk_attn_window_size)\n\n    # Create multi-head mask\n    multi_head_mask = splash_attention_mask.MultiHeadMask(masks=(mask,) * query.shape[1])\n\n    # Create the splash attention kernel object separately, jit it for performance\n    @partial(\n        jax.jit,\n        static_argnames=[\n            \"multi_head_mask\",\n            \"shard_head_size\",\n        ],\n    )\n    def wrap_splash_kernel(multi_head_mask, shard_head_size=1):\n      splash_kernel = splash_attention_kernel.make_splash_mha(\n          mask=multi_head_mask,\n          head_shards=shard_head_size,  # the size of the axis if sharding over heads\n          q_seq_shards=cp_size,  # axis for sequence sharding\n          block_sizes=block_sizes,\n          attn_logits_soft_cap=attn_logits_soft_cap,\n          residual_checkpoint_name=\"context\",\n      )\n      return splash_kernel\n\n    logical_axis_rules_head = np.array(\n        [self.mesh.shape[physical_axes] for physical_axes in dict(self.config.logical_axis_rules)[HEAD]]\n    )\n    shard_head_size = np.prod(logical_axis_rules_head)\n    splash_kernel = wrap_splash_kernel(multi_head_mask, int(shard_head_size))\n    named_sharding = jax.sharding.NamedSharding(self.mesh, axis_names_splash_kernel)\n    segment_axis_names_splash_kernel = splash_kernel.manual_sharding_spec(named_sharding)\n\n    # Now call the function wrap_flash_attention which does the actual computation.\n    # The splash kernel is passed as a parameter to the function. Since we have the shard map\n    # decorating the wrap_flash_attention function, the data will be correctly sharded\n    # meaning q will be sharded over sequence aka context length but K and V will be duplicated\n    # The shardings are specified in the in_specs and out_specs of the shard_map decorator:\n    # 'segment_axis_names_q' maps to ['activation_q_length', ['context']] meaning that q is sharded over the context axis\n    #  'segment_axis_names_kv' maps to ['activation_kv_length', []] meaning that K and V are not sharded\n    # splash_kernel is sharded over (HEAD, LENGTH)\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            axis_names_q,\n            axis_names_kv,\n            axis_names_kv,\n            segment_axis_names_q,\n            segment_axis_names_kv,\n            segment_axis_names_splash_kernel,\n            None,  # no sharding for cp_size\n            None,  # no sharding for load_balanced_context_parallel\n            sink_axis_names,  # sharding align with query heads\n        ),\n        out_specs=axis_names_q,\n        check_rep=False,\n    )\n    def wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids_q,\n        decoder_segment_ids_kv,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    ):\n      # If load_balanced_context_parallel is enabled, reorder the key and value tensors\n      # to ensure that they are contiguous in memory.\n      # This is necessary for the splash attention kernel to work correctly because it expects\n      # the K and V to be contiguous. Note that K and V are not sharded over the sequence aka context axis\n      # This was we get the unsharded unpermuted key and value tensors\n      if cp_size > 1 and load_balanced_context_parallel:\n        key = max_utils.reorder_sequence(tensor=key, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        value = max_utils.reorder_sequence(tensor=value, cp_size=cp_size, seq_dim=2, to_contiguous=True)\n        decoder_segment_ids_unpermuted = max_utils.reorder_sequence(\n            tensor=decoder_segment_ids_kv, cp_size=cp_size, seq_dim=1, to_contiguous=True\n        )\n\n      if decoder_segment_ids_q is not None:\n        if cp_size > 1 and load_balanced_context_parallel:\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(\n              decoder_segment_ids_q, decoder_segment_ids_unpermuted\n          )\n        else:\n          # if cp=1, decoder_segment_ids_q is the same as decoder_segment_ids_kv\n          decoder_segment_ids_tuple = splash_attention_kernel.SegmentIds(decoder_segment_ids_q, decoder_segment_ids_kv)\n      else:\n        decoder_segment_ids_tuple = None\n      # TODO(ranran): remove if/else branch once b/441336842 is fixed\n      if version.parse(jax.__version__) < version.parse(\"0.7.2.dev20250824\"):\n        attention_output = jax.vmap(splash_kernel)(query, key, value, decoder_segment_ids_tuple)\n      else:\n        attention_output = jax.vmap(splash_kernel, in_axes=(0, 0, 0, 0, None))(\n            query, key, value, decoder_segment_ids_tuple, sinks\n        )\n      return attention_output\n\n    x = wrap_flash_attention(\n        query,\n        key,\n        value,\n        decoder_segment_ids,\n        decoder_segment_ids,\n        splash_kernel,\n        cp_size,\n        load_balanced_context_parallel,\n        sinks,\n    )\n\n    x = jnp.transpose(x, axes=(0, 2, 1, 3))\n\n    return x\n\n  def cudnn_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> Array:\n    \"\"\"CUDNN Flash Attention with Transformer Engine.\n    1. Stable API, supports GQA, SWA (only with causal masking)\n    2. Head_dim = 256 is also supported from TE-1.12 stable release with CUDNN 12.6\n    \"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from transformer_engine.jax.flax.transformer import DotProductAttention  # pytype: disable=import-error\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    using_context_parallelism = self.mesh.shape[\"context\"] > 1\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING and using_context_parallelism:\n      raise AssertionError(\"Sliding window attention is not supported when context parallelism is enabled\")\n\n    sliding_window_size = None\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or not self.config.enable_padding_causal_mask:\n      sliding_window_size = [self.sliding_window_size, 0]\n\n    if self.attention_type == AttentionType.LOCAL_SLIDING or using_context_parallelism:\n      mask_type = \"causal\"  # SWA and Context Parallelism only work with causal masking\n      attn_mask = None\n    else:\n      # generate attn_mask\n      mask_type = \"padding_causal\"  # only padding_causal mask type can take a created mask\n      attn_mask = self.generate_attention_mask(query, key, decoder_segment_ids, model_mode)\n\n    dpa_layer = DotProductAttention(\n        head_dim=head_dim,\n        num_attention_heads=self.num_query_heads,\n        num_gqa_groups=self.num_kv_heads,\n        attn_mask_type=mask_type,  # 'no_mask', 'padding', 'causal', or 'padding_causal'\n        attn_bias_type=\"no_bias\",  # 'no_bias', 'pre_scale_bias' or 'post_scale_bias'\n        attention_dropout=self.dropout_rate,\n        dropout_rng_name=\"aqt\",\n        dtype=self.dtype,\n        float32_logits=self.float32_logits,\n        qkv_layout=\"BSHD_BSHD_BSHD\",  # 'BS3HD', 'BSHD_BS2HD' or 'BSHD_BSHD_BSHD'\n        scale_factor=1.0,\n        transpose_batch_sequence=False,\n        window_size=sliding_window_size,\n        context_parallel_causal_load_balanced=self.config.context_parallel_load_balance,\n        context_parallel_axis=\"context\",\n    )\n    return dpa_layer(query, key, value, mask=attn_mask)\n\n  def cudnn_jax_flash_attention(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n  ) -> tuple[Array, Array]:\n    \"\"\"CUDNN Flash Attention with JAX SDPA API.\"\"\"\n    # These imports are only meant to work in a GPU build.\n    # pylint: disable=import-outside-toplevel\n    from jax._src.cudnn.fused_attention_stablehlo import (\n        dot_product_attention,\n        MaskType,\n    )\n\n    _, _, _, head_dim = query.shape  # pylint: disable=unused-variable\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      lengths = jnp.sum(decoder_segment_ids, axis=-1)\n\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          q_seqlen=lengths,\n          kv_seqlen=lengths,\n          mask_type=MaskType.PADDING,\n          scale=1.0,\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    else:\n      output, lse = dot_product_attention(\n          query,\n          key,\n          value,\n          mask_type=MaskType.CAUSAL,\n          scale=1.0 / math.sqrt(head_dim),\n          dropout_rate=self.dropout_rate,\n          qkv_layout=\"BTNH\",\n          return_residual=True,\n      )\n    output = checkpoint_name(output, \"context\")\n    lse = checkpoint_name(lse, \"context\")\n    return output, lse\n\n  def compute_local_attention(\n      self,\n      attn_weights: Array,\n      value: Array | KVTensor,\n      q_seq_len: int,\n      model_mode: str,\n      wv_product_einsum: Callable[..., Array],\n      sinks: Array | None = None,\n  ) -> tuple[Array, Array, Array]:\n    \"\"\"Computes the attention of a local subset of the kv cache.\n    Local attention results will need to be combined with any other local attentions and normalized\n    Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n\n    Args:\n        attn_weights (Array): Product of query and key\n        value (Array): Current value\n        aqt_rng (PRNGKey | None): Optional rng\n\n    Returns:\n        (local_out, local_max,): where\n          local_out is local unnormalized output\n          local_max is the local max of exponentials\n          local_sum is the sum of exponentials for this chunk, divided by exp(local_max).\n    \"\"\"\n    b, n_kv, g, t, s = attn_weights.shape\n    n_q = n_kv * g\n    logits = jnp.reshape(attn_weights, (b, n_q, t, s))\n    if sinks is not None:\n      # broadcast sinks to match the attn weights dimension and combine\n      sinks_param = sinks.astype(attn_weights.dtype)  # (n_q,)\n      sinks_logits = sinks_param[jnp.newaxis, :, jnp.newaxis, jnp.newaxis]  # (1, n_q, 1, 1)\n      sinks_logits = jnp.broadcast_to(sinks_logits, (b, n_q, t, 1))\n      logits = jnp.concatenate([logits, sinks_logits], axis=-1)\n\n    # softmax\n    local_max = jnp.max(logits, axis=-1, keepdims=True)\n    local_exps_combined = jnp.exp(logits - local_max)\n    local_sum = jnp.sum(local_exps_combined, axis=-1, keepdims=True)\n\n    # reshape and transpose\n    local_exps = local_exps_combined[..., :s]\n    local_exps = jnp.reshape(local_exps, (b, n_kv, g, t, s))\n    local_max = jnp.transpose(local_max, (0, 2, 1, 3))  # (b, t, n_q, 1)\n    local_sum = jnp.transpose(local_sum, (0, 2, 1, 3))  # (b, t, n_q, 1)\n\n    local_out = self.wv_product(local_exps, value, model_mode, wv_product_einsum)\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n    elif model_mode == MODEL_MODE_PREFILL:\n      local_out = partitioning.with_sharding_constraint(local_out, (BATCH, KV_LENGTH, HEAD, D_KV))\n\n    if self.reshape_q and q_seq_len == 1:\n      local_max = local_max[:, 0:1, :, :]\n      local_sum = local_sum[:, 0:1, :, :]\n      local_out = local_out[:, 0:1, :, :]\n\n    if model_mode == MODEL_MODE_AUTOREGRESSIVE and self.is_partition_in_decode(q_seq_len):\n      local_max = partitioning.with_sharding_constraint(local_max, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_sum = partitioning.with_sharding_constraint(local_sum, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n      local_out = partitioning.with_sharding_constraint(local_out, (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV))\n\n    return local_out, local_max, local_sum\n\n  def is_partition_in_decode(self, seq_len):\n    return self.config.ici_context_autoregressive_parallelism > 0 and seq_len == 1\n\n  def apply_attention_dot(\n      self,\n      query: Array,\n      key: Array | KVTensor,\n      value: Array | KVTensor,\n      decoder_segment_ids: Array | None,\n      model_mode: str = MODEL_MODE_TRAIN,\n      previous_chunk: Any = None,\n      bidirectional_mask: Any = None,\n      sinks: Array | None = None,\n      *,\n      qk_product_einsum: Callable[..., Array],\n      wv_product_einsum: Callable[..., Array],\n  ):\n    \"\"\"Apply Attention.\"\"\"\n    validate_compute_axis_order(self.compute_axis_order)\n    # Casting qk_product and softmaxt computation for float32 for model stability.\n    if self.float32_qk_product:\n      if isinstance(key, KVTensor):\n        key = key.dequant()\n      query = query.astype(jnp.float32)\n      key = key.astype(jnp.float32)\n\n    # special sharding for decode\n    q_seq_len = query.shape[1]\n    prefill_qkv_sharding = (BATCH, PREFILL_LENGTH, HEAD, D_KV)\n    decode_qkv_sharding = (DECODE_BATCH, DECODE_LENGTH, HEAD, D_KV)\n    if self.is_partition_in_decode(q_seq_len):\n      query = partitioning.with_sharding_constraint(query, decode_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, decode_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, decode_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, decode_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, decode_qkv_sharding)\n    elif model_mode == MODEL_MODE_PREFILL:\n      query = partitioning.with_sharding_constraint(query, prefill_qkv_sharding)\n      # avoid sharding scale tensor when using kv cache quantization\n      if self.kv_quant and isinstance(key, KVTensor) and isinstance(value, KVTensor):\n        key.qvalue = partitioning.with_sharding_constraint(key.qvalue, prefill_qkv_sharding)\n        value.qvalue = partitioning.with_sharding_constraint(value.qvalue, prefill_qkv_sharding)\n      else:\n        key = partitioning.with_sharding_constraint(key, prefill_qkv_sharding)\n        value = partitioning.with_sharding_constraint(value, prefill_qkv_sharding)\n\n    attn_weights = self.qk_product(query, key, q_seq_len, model_mode, qk_product_einsum)\n    if self.is_partition_in_decode(q_seq_len):\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_weights = partitioning.with_sharding_constraint(attn_weights, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n\n    if self.attn_logits_soft_cap:\n      attn_weights = jnp.tanh(attn_weights / self.attn_logits_soft_cap)\n      attn_weights = attn_weights * self.attn_logits_soft_cap\n\n    # Casting softmaxt computation for float32 for model stability.\n    if self.float32_logits:\n      attn_weights = attn_weights.astype(jnp.float32)\n    attn_mask = self.generate_attention_mask(\n        query, key, decoder_segment_ids, model_mode, previous_chunk, bidirectional_mask\n    )\n    if self.is_partition_in_decode(q_seq_len):\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (KV_LENGTH, HEAD, None, None, None))\n    elif model_mode == MODEL_MODE_PREFILL:\n      attn_mask = partitioning.with_sharding_constraint(attn_mask, (BATCH, HEAD, None, PREFILL_LENGTH, KV_LENGTH))\n    if attn_mask is not None:\n      attn_weights = apply_mask_to_logits(attn_weights, attn_mask)\n    return self.compute_local_attention(attn_weights, value, q_seq_len, model_mode, wv_product_einsum, sinks)\n\n  def qk_product(\n      self, query: Array, key: Array | KVTensor, q_seq_len: int, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"Query-Key product.\n\n    Args:\n      query: Query projection, in shape of [b, t, n, d]\n      key: Key projection in shape of [b, s, n_kv, d]\n\n    Returns:\n      results in shape [b, n_kv, n // n_kv, t, s].\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n    b, t, n, d = query.shape\n    n_kv = key.shape[-2]\n    assert n_kv == self.num_kv_heads\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, 2, n_kv, n // n_kv, d))\n      result = einsum(\"btkgd,bskd->bkgts\", query, key, **precision_kwargs)\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      query = jnp.transpose(query, axes=self.compute_axis_order)\n      key = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), key)\n      query = jnp.reshape(query, (b, n_kv, n // n_kv, t, d))\n      if self.reshape_q and q_seq_len == 1:\n        query = jnp.broadcast_to(query, (b, n_kv, n // n_kv, 2, d))\n      result = einsum(\"bkgtd,bksd->bkgts\", query, key, **precision_kwargs)\n    else:\n      raise NotImplementedError(self.compute_axis_order)\n    return result\n\n  def wv_product(\n      self, attn_weights: Array, value: Array | KVTensor, model_mode: str, einsum: Callable[..., Array]\n  ) -> Array:\n    \"\"\"weighted value product.\n\n    Args:\n      attn_weights: Computed results of qk_einsum, in shape [b, n_kv, n // n_kv, t, s]\n      value: Value projection, in shape of [b, s, n_kv, d]\n\n    Returns:\n      result in shape [b, t, n, d]\n\n    Annotations:\n      b: batch size\n      t: query length\n      s: key / value length\n      d: head / kv dimension\n      n: number of query heads\n      n_kv: number of kv heads, sometimes annotated as k\n      n // n_kv: number of group for query, sometimes annotated with g\n    \"\"\"\n\n    precision_kwargs = {\"precision\": self.config.matmul_precision} if einsum is jnp.einsum else {}\n    if self.kv_quant:\n      # manually cast to bf16 to avoid the fp32 XLA ops for speedup\n      if isinstance(value, KVTensor) and self.kv_quant.dtype == jnp.float8_e4m3fn:\n        value.qvalue = value.qvalue.astype(jnp.bfloat16)\n    if model_mode == MODEL_MODE_TRAIN or self.compute_axis_order == (0, 1, 2, 3):\n      out = einsum(\"bkgts,bskd->btkgd\", attn_weights, value, **precision_kwargs)\n      b, t, n_kv, g, d = out.shape\n      result = jnp.reshape(out, (b, t, n_kv * g, d))\n    elif self.compute_axis_order == (0, 2, 1, 3):\n      value = jax.tree.map(lambda x: jnp.transpose(x, axes=self.compute_axis_order), value)\n      out = einsum(\"bkgts,bksd->bkgtd\", attn_weights, value, **precision_kwargs)\n      b, n_kv, g, t, d = out.shape\n      result = jnp.reshape(out, (b, n_kv * g, t, d))\n      result = self.reverse_transepose(result, self.compute_axis_order)\n    return result\n\n  def reverse_transepose(self, transposed_array, transpose_axis_order):\n    return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)\n\n  def normalize_cudnn_attention(self, local_outs, local_stats):\n    \"\"\"Normalize across two cuDNN attentions\n\n    Args:\n        local_outs (list): List of outputs entries for each cudnn attention\n          in shape [b, t, n, d].\n        local_stats (list): List of logsumexp entries for each cudnn attention\n          in shape [b, n, t].\n\n    Returns:\n        Array: Combined attention that has been normalized in shape [b, t, n, d].\n    \"\"\"\n    # reshape stat to have shape [b, n, t, 1]\n    stat0 = local_stats[0].reshape((*local_stats[0].shape, 1))\n    stat1 = local_stats[1].reshape((*local_stats[1].shape, 1))\n    global_stat = jnp.log(jnp.exp(stat0) + jnp.exp(stat1))\n    # # transpose stat to have shape [b, t, n, 1] for elemenwise multiplication\n    attn_out = local_outs[0].astype(jnp.float32) * jnp.exp(stat0 - global_stat).transpose((0, 2, 1, 3)) + local_outs[\n        1\n    ].astype(jnp.float32) * jnp.exp(stat1 - global_stat).transpose((0, 2, 1, 3))\n    return attn_out.astype(local_stats[0].dtype)\n\n  def normalize_attention(self, local_outs, local_maxes, local_sums):\n    \"\"\"Normalize across multiple localized attentions\n\n    Args:\n        local_outs (list): List of unnormalized outputs entries for each local attention\n        local_maxes (list): List of max exponentials entries for each local attention\n        local_sums (list): List of exponential sum entries for each local attention\n\n    Returns:\n        Array: Combined attention that has been normalized\n    \"\"\"\n    # Based on https://github.com/google-research/google-research/blob/master/scaling_transformer_inference_efficiency/attention.py\n    global_max = functools.reduce(jnp.maximum, local_maxes)\n    global_sum = sum(\n        (jnp.exp(local_max - global_max) * local_sum for (local_sum, local_max) in zip(local_sums, local_maxes))\n    )\n\n    attn_out = 0\n    for local_max, local_out in zip(local_maxes, local_outs):\n      local_normalizer = jnp.exp(local_max - global_max) / global_sum\n      attn_out += local_normalizer * local_out\n    return attn_out\n\n  def __call__(\n      self,\n      query,\n      key,\n      value,\n      decoder_segment_ids,\n      model_mode,\n      cached_values=None,\n      previous_chunk=None,\n      bidirectional_mask=None,\n      sinks=None,\n      slot: Optional[int] = None,\n      page_state: Optional[page_manager.PageState] = None,\n  ):\n    if cached_values is None:\n      prefill_kv_cache, ar_kv_cache = None, None\n    else:\n      prefill_kv_cache, ar_kv_cache = cached_values[0], cached_values[1]\n    if model_mode != MODEL_MODE_TRAIN:\n      assert prefill_kv_cache\n      key, value, decoder_segment_ids = prefill_kv_cache\n\n    prefill_unnormalized_output, prefill_exponentials_max, prefill_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=None,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        previous_chunk=previous_chunk,\n        bidirectional_mask=bidirectional_mask,\n        sinks=sinks,\n        qk_product_einsum=self.AqtEinsum_0,\n        wv_product_einsum=self.AqtEinsum_1,\n    )\n\n    # Return the \"prefill\" cache if it actually the combined prefill+ar kv cache\n    if ar_kv_cache is None:\n      if prefill_exponentials_sum is not None:\n        return prefill_unnormalized_output / prefill_exponentials_sum\n      return prefill_unnormalized_output\n\n    key, value, decoder_segment_ids, lengths = ar_kv_cache\n    ar_unnormalized_output, ar_exponentials_max, ar_exponentials_sum = self.apply_attention(\n        query=query,\n        key=key,\n        value=value,\n        decoder_segment_ids=decoder_segment_ids,\n        lengths=lengths,\n        model_mode=model_mode,\n        use_ragged_attention=self.use_ragged_attention,\n        bidirectional_mask=bidirectional_mask,\n        qk_product_einsum=self.AqtEinsum_2,\n        wv_product_einsum=self.AqtEinsum_3,\n    )\n\n    if ar_unnormalized_output is not None:\n      unnormalized_outputs = [prefill_unnormalized_output, ar_unnormalized_output]\n      exponentials_maxes = [prefill_exponentials_max, ar_exponentials_max]\n      exponentials_sums = [prefill_exponentials_sum, ar_exponentials_sum]\n      if prefill_exponentials_max is not None and prefill_exponentials_sum is None:\n        prefill_stat = prefill_exponentials_max\n        ar_stat = ar_exponentials_max\n        stats = [prefill_stat, ar_stat]\n        return self.normalize_cudnn_attention(unnormalized_outputs, stats)\n      else:\n        return self.normalize_attention(unnormalized_outputs, exponentials_maxes, exponentials_sums)\n    else:\n      return prefill_unnormalized_output / prefill_exponentials_sum",
        "analysis": {
            "module_type": "attention_operation",
            "purpose": "Encapsulates various attention mechanisms (dot-product, flash, ragged) for different hardware (TPU, GPU) and operating modes (prefill, autoregressive), handling masking, quantization, and parallelism.",
            "input": {
                "shape": "query: [batch, q_len, num_q_heads, head_dim], key/value: [batch, kv_len, num_kv_heads, head_dim], decoder_segment_ids: [batch, q_len]",
                "dtype": "jnp.float32 or other configured dtype."
            },
            "processing_steps": [
                "The `__call__` method receives inputs and potentially splits cached values into prefill and autoregressive parts.",
                "It calls `apply_attention` for the prefill part of the cache.",
                "If an autoregressive cache exists, it calls `apply_attention` again for that part.",
                "It normalizes and combines the results if both prefill and autoregressive attention were computed."
            ],
            "output": {
                "shape": "[batch, q_len, num_q_heads, head_dim]"
            },
            "dependencies": [
                "flax.nnx.Module",
                "jax.experimental.shard_map.shard_map",
                "jax.experimental.pallas.ops.gpu.attention",
                "jax.experimental.pallas.ops.tpu.splash_attention",
                "transformer_engine.jax.flax.transformer.DotProductAttention",
                "MaxText.inference.kvcache.KVTensor",
                "MaxText.layers.quantizations.AqtQuantization",
                "MaxText.common_types.Config",
                "jax.sharding.Mesh"
            ],
            "parameters": {
                "attention_kernel": "The attention kernel to use (e.g., 'dot_product', 'flash', 'autoselected').",
                "num_query_heads": "The number of query heads.",
                "num_kv_heads": "The number of key/value heads.",
                "attention_type": "The type of attention pattern to apply (e.g., GLOBAL, LOCAL_SLIDING, CHUNK).",
                "use_ragged_attention": "A boolean indicating whether to use ragged attention for autoregressive decoding.",
                "kv_quant": "Configuration object for key/value cache quantization. If None, quantization is disabled."
            },
            "notes": [
                "This is a flexible `nnx.Module` that serves as a dispatcher for various attention implementations tailored for different hardware and use cases.",
                "It supports different operational modes like `prefill`, `autoregressive`, and `train`.",
                "If KV cache quantization is enabled (`kv_quant` is provided), it lazily initializes specialized `AqtEinsum` operations."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the attention operation module with configuration, and sets up einsum functions for potential KV quantization.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters as instance attributes.",
                        "Define a helper function `maybe_create_nnx` to wrap modules for lazy initialization.",
                        "If `kv_quant` is enabled, create dummy tensors to lazily initialize `AqtEinsum` instances for both prefill and autoregressive steps.",
                        "If `kv_quant` is disabled, set einsum functions to `jnp.einsum`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx_wrappers.ToNNX",
                        "jnp.zeros",
                        "jnp.einsum"
                    ],
                    "notes": [
                        "The initialization logic for `AqtEinsum` instances is conditional on the `kv_quant` parameter, using dummy tensors for shape inference."
                    ]
                },
                "check_attention_inputs": {
                    "purpose": "Validates the shapes and dimensions of query, key, and value tensors.",
                    "input": {
                        "shape": "query: [..., q_len, num_q_heads, head_dim], key/value: [..., kv_len, num_kv_heads, head_dim]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Assert that key and value have the same number of dimensions.",
                        "Assert that batch dimensions match across query, key, and value.",
                        "Assert that the number of KV heads and sequence lengths match for key and value.",
                        "Assert that the head dimension (depth) matches for query and key."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "KVTensor"
                    ],
                    "notes": [
                        "This is a validation helper method that raises an AssertionError on failure."
                    ]
                },
                "generate_attention_mask": {
                    "purpose": "Generates a combined attention mask based on model mode, segment IDs, causality, and specialized attention patterns like sliding window or chunked attention.",
                    "input": {
                        "shape": "query: [batch, q_len, ...], key: [batch, kv_len, ...], decoder_segment_ids: [batch, q_len]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Create a base mask from `decoder_segment_ids` for sequence separation.",
                        "Determine the query offset based on `previous_chunk` or autoregressive mode.",
                        "Create a causal mask if not in autoregressive mode and attention is not 'FULL'.",
                        "Combine the segment and causal masks.",
                        "Apply specialized masks (sliding window, chunked) if configured.",
                        "Combine with a bidirectional mask if provided.",
                        "Convert the final boolean mask to a float mask using `DEFAULT_MASK_VALUE`."
                    ],
                    "output": {
                        "shape": "Broadcastable to [batch, num_heads, q_len, kv_len] or None."
                    },
                    "dependencies": [
                        "jax.lax.broadcasted_iota",
                        "_generate_chunk_attention_mask",
                        "_make_bidirectional_block_mask"
                    ],
                    "notes": [
                        "The logic is complex, combining multiple masking strategies. Returns `None` if no mask is determined to be necessary."
                    ]
                },
                "apply_attention": {
                    "purpose": "A dispatch function that selects and applies the appropriate attention implementation based on configuration and hardware.",
                    "input": {
                        "shape": "query: [batch, q_len, num_q_heads, head_dim], key/value: [batch, kv_len, num_kv_heads, head_dim]",
                        "dtype": "self.dtype"
                    },
                    "processing_steps": [
                        "Check input shapes using `check_attention_inputs`.",
                        "Determine target hardware (TPU/GPU).",
                        "If `use_ragged_attention` in autoregressive mode, dispatch to `tpu_ragged_attention` or `gpu_ragged_attention`.",
                        "If `attention_kernel` is 'dot_product' (or autoselected for decode/short sequences), dispatch to `apply_attention_dot`.",
                        "If `attention_kernel` is 'flash' or 'autoselected', dispatch to a hardware-specific flash attention implementation.",
                        "If `attention_kernel` is 'cudnn_flash_te' or 'cudnn_flash_jax', dispatch to the corresponding cuDNN implementation.",
                        "Raise an error for unknown kernels."
                    ],
                    "output": {
                        "shape": "A tuple containing the attention output [batch, q_len, num_q_heads, head_dim] and potentially statistics (max, sum) for normalization."
                    },
                    "dependencies": [
                        "self.tpu_ragged_attention",
                        "self.gpu_ragged_attention",
                        "self.apply_attention_dot",
                        "self.tpu_flash_attention",
                        "gpu_pallas_attention.mha",
                        "self.cudnn_flash_attention",
                        "self.cudnn_jax_flash_attention"
                    ],
                    "notes": [
                        "This is the central control flow for choosing the attention backend. It handles dequantizing `KVTensor`s for flash attention implementations that don't support them directly."
                    ]
                },
                "__call__": {
                    "purpose": "The main entry point for the module, which handles attention computation for both prefill and autoregressive KV caches and combines the results.",
                    "input": {
                        "shape": "query: [batch, q_len, num_q_heads, head_dim], cached_values: A tuple of (prefill_cache, ar_cache) or None.",
                        "dtype": "self.dtype"
                    },
                    "processing_steps": [
                        "Unpack `cached_values` into `prefill_kv_cache` and `ar_kv_cache`.",
                        "If not in training mode, use the `prefill_kv_cache` for key, value, and segment IDs.",
                        "Call `apply_attention` for the prefill part.",
                        "If `ar_kv_cache` is `None`, normalize and return the prefill result.",
                        "If `ar_kv_cache` exists, call `apply_attention` for the autoregressive part.",
                        "Combine and normalize the results from both prefill and AR attention using `normalize_attention` or `normalize_cudnn_attention`."
                    ],
                    "output": {
                        "shape": "[batch, q_len, num_q_heads, head_dim]"
                    },
                    "dependencies": [
                        "self.apply_attention",
                        "self.normalize_attention",
                        "self.normalize_cudnn_attention"
                    ],
                    "notes": [
                        "This method orchestrates attention over potentially two separate KV caches (prefill and autoregressive), a common pattern in efficient inference."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/layers/attention_op.py#LoadBalancedCausalMask",
        "file_path": "src/MaxText/layers/attention_op.py",
        "code_block": "class LoadBalancedCausalMask(splash_attention_mask._ComputableMask):\n  \"\"\"Lazy causal mask, prevents the model from attending to future tokens.\n  Attributes:\n    offset: Offset of q start wrt kv. A positive offset shifts the bottom\n      triangle upward, a negative one shifts it downward. A negative offset\n      makes the first 'offset' rows of the attention matrix all 0s which leads\n      to undefined softmax.\n  \"\"\"\n\n  offset: int\n  shape: tuple[int, int]\n  cp_size: int\n\n  def __init__(self, shape: tuple[int, int], offset: int = 0, shard_count: int = 1, cp_size: int = 4):\n    self.offset = offset\n\n    def causal_mask_function(q_ids, kv_ids):\n      if self.offset == 0:\n        return q_ids >= kv_ids\n      else:\n        return q_ids + self.offset >= kv_ids\n\n    arr = np.arange(shape[0])\n    # we reorder the mask to be load balanced following the same approach as\n    # used to reorder the input tokens\n    out = max_utils.reorder_mask_load_balancing(arr[None, :, None, None], cp_size, 1)\n    q_sequence = out[0, :, 0, 0]\n\n    mask_function = causal_mask_function\n\n    super().__init__(\n        shape=shape,\n        mask_function=mask_function,\n        shard_count=shard_count,\n    )\n    self.q_sequence = q_sequence\n\n  def __eq__(self, other: object):\n    if not isinstance(other, type(self)):\n      return NotImplemented\n\n    return self.shape == other.shape and self.offset == other.offset and np.array_equal(self.q_sequence, other.q_sequence)\n\n  def __hash__(self):\n    return hash(\n        (\n            type(self),\n            self.shape,\n            self.offset,\n            self.q_sequence.tobytes() if self.q_sequence is not None else None,\n        )\n    )",
        "analysis": {
            "module_type": "load_balanced_causal_mask",
            "purpose": "Creates a lazy, computable causal attention mask that reorders query indices to improve load balancing in context parallel settings for splash attention.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines a `causal_mask_function` that returns true if a query index is greater than or equal to a key index, adjusted by an offset.",
                "Creates an array of sequence indices based on the input shape.",
                "Reorders the sequence indices using `max_utils.reorder_mask_load_balancing` to create a load-balanced query sequence.",
                "Initializes the parent `_ComputableMask` class with the shape and the defined mask function.",
                "Stores the reordered query sequence as `self.q_sequence`."
            ],
            "output": {
                "shape": "An object representing a boolean mask of the `shape` provided during initialization."
            },
            "dependencies": [
                "splash_attention_mask._ComputableMask",
                "max_utils.reorder_mask_load_balancing",
                "numpy"
            ],
            "parameters": {
                "shape": "A tuple (q_length, kv_length) defining the dimensions of the attention mask.",
                "offset": "An integer offset applied to the query indices in the causal comparison (q_ids + offset >= kv_ids).",
                "cp_size": "The context parallelism size, used to determine the reordering pattern for load balancing."
            },
            "notes": [
                "This is a 'lazy' mask, meaning the full boolean mask matrix is not materialized in memory.",
                "It is specifically designed for use with TPU splash attention when context parallelism and load balancing are enabled.",
                "The class inherits from a protected class `splash_attention_mask._ComputableMask`."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the mask by defining the causal logic and computing the load-balanced query sequence ordering.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Sets the `offset` attribute.",
                        "Defines a nested `causal_mask_function`.",
                        "Generates an array of indices from 0 to shape[0]-1.",
                        "Calls `max_utils.reorder_mask_load_balancing` to get the reordered query sequence.",
                        "Stores the reordered sequence in `self.q_sequence`.",
                        "Calls the `super().__init__` with the shape and mask function."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_utils.reorder_mask_load_balancing",
                        "numpy"
                    ],
                    "notes": [
                        "The key side effect is setting `self.offset` and `self.q_sequence` and calling the parent constructor."
                    ]
                },
                "__eq__": {
                    "purpose": "Defines equality comparison for two `LoadBalancedCausalMask` objects.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the other object is of the same type.",
                        "Compares `self.shape`, `self.offset`, and `self.q_sequence` for equality."
                    ],
                    "output": {
                        "shape": "A boolean value."
                    },
                    "dependencies": [
                        "numpy.array_equal"
                    ],
                    "notes": [
                        "Returns `True` if shape, offset, and the reordered query sequence are identical, `False` otherwise."
                    ]
                },
                "__hash__": {
                    "purpose": "Computes a hash value for the instance, making it hashable.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Creates a tuple of the object's type, shape, offset, and the bytes of the `q_sequence` array.",
                        "Computes the hash of the tuple."
                    ],
                    "output": {
                        "shape": "An integer hash value."
                    },
                    "dependencies": [],
                    "notes": [
                        "Ensures that equal objects have the same hash."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#paged_attention_op_as_linen",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "def paged_attention_op_as_linen(\n    *,\n    mesh: Mesh,\n    num_pages: int,\n    tokens_per_page: int,\n    max_pages_per_slot: int,\n    max_pages_per_prefill: int,\n    pages_per_compute_block: int,\n    num_kv_heads: int,\n    kv_head_dim_size: int,\n    dtype: DType = jnp.float32,\n    attn_logits_soft_cap: float | None = None,\n    query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n    kv_pages_axis_names: AxisNames = (\n        \"paged_kv_heads\",\n        \"num_pages\",\n        \"tokens_per_page\",\n        \"paged_kv_head_dim_size\",\n    ),\n):\n  \"\"\"A factory function to create a PagedAttentionOp as a Linen module.\n\n  This function serves as a bridge to use the NNX-based `PagedAttentionOp`\n  within a Linen model. It wraps the `PagedAttentionOp` module using\n  `nnx.bridge.to_linen`, making it compatible with the Linen API. This is\n  useful for gradual migration of a codebase from Linen to NNX.\n\n  Args:\n    mesh: The device mesh for sharding.\n    num_pages: The total number of pages in the KV cache.\n    tokens_per_page: The number of tokens each page can hold.\n    max_pages_per_slot: The maximum number of pages a single sequence can use.\n    max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n    pages_per_compute_block: The number of pages processed in one kernel block.\n    num_kv_heads: The number of key/value heads.\n    kv_head_dim_size: The dimension of each key/value head.\n    dtype: The data type for computations.\n    attn_logits_soft_cap: The soft cap for attention logits.\n    query_axis_names: The logical axis names for the query tensor.\n    kv_pages_axis_names: The logical axis names for the KV cache pages.\n\n  Returns:\n    A Linen module that wraps the NNX `PagedAttentionOp` module.\n  \"\"\"\n\n  return nnx.bridge.to_linen(\n      PagedAttentionOp,\n      mesh=mesh,\n      num_pages=num_pages,\n      tokens_per_page=tokens_per_page,\n      max_pages_per_slot=max_pages_per_slot,\n      max_pages_per_prefill=max_pages_per_prefill,\n      pages_per_compute_block=pages_per_compute_block,\n      num_kv_heads=num_kv_heads,\n      kv_head_dim_size=kv_head_dim_size,\n      dtype=dtype,\n      attn_logits_soft_cap=attn_logits_soft_cap,\n      query_axis_names=query_axis_names,\n      kv_pages_axis_names=kv_pages_axis_names,\n      metadata_fn=variable_to_logically_partitioned,\n  )",
        "analysis": {
            "module_type": "paged_attention_linen_factory",
            "purpose": "A factory function that creates a Flax Linen module by wrapping the NNX-based `PagedAttentionOp` class, serving as a compatibility bridge between the two frameworks.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx.bridge.to_linen` with the `PagedAttentionOp` class and its configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx.bridge.to_linen",
                "PagedAttentionOp",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "mesh": "The device mesh for sharding.",
                "num_pages": "The total number of pages in the KV cache.",
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_slot": "The maximum number of pages a single sequence can use.",
                "num_kv_heads": "The number of key/value heads.",
                "kv_head_dim_size": "The dimension of each key/value head.",
                "dtype": "The data type for computations."
            },
            "notes": [
                "This function is designed to facilitate the gradual migration of a codebase from Flax Linen to NNX by allowing NNX modules to be used within existing Linen models."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention.py#PagedAttentionOp",
        "file_path": "src/MaxText/inference/paged_attention.py",
        "code_block": "class PagedAttentionOp(nnx.Module):\n  \"\"\"An NNX module for paged attention.\n\n  This module implements the paged attention mechanism, which is an efficient\n  method for handling attention in autoregressive models with long sequences.\n  It divides the KV cache into fixed-size \"pages\" to manage memory dynamically.\n  \"\"\"\n\n  def __init__(\n      self,\n      mesh: Mesh,\n      num_pages: int,\n      tokens_per_page: int,\n      max_pages_per_slot: int,\n      max_pages_per_prefill: int,\n      pages_per_compute_block: int,\n      num_kv_heads: int,\n      kv_head_dim_size: int,\n      dtype: DType = jnp.float32,\n      attn_logits_soft_cap: float | None = None,\n      query_axis_names: AxisNames = (BATCH, LENGTH, HEAD, D_KV),\n      kv_pages_axis_names: AxisNames = (\n          \"paged_kv_heads\",\n          \"num_pages\",\n          \"tokens_per_page\",\n          \"paged_kv_head_dim_size\",\n      ),\n      *,\n      # Not used in Embed but passed in by nnx.bridge.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs,\n  ):\n    \"\"\"Initializes the PagedAttentionOp module.\n\n    Args:\n      mesh: The device mesh for sharding.\n      num_pages: The total number of pages in the KV cache.\n      tokens_per_page: The number of tokens each page can hold.\n      max_pages_per_slot: The maximum number of pages a single sequence can use.\n      max_pages_per_prefill: The maximum number of pages for a prefill sequence.\n      pages_per_compute_block: The number of pages processed in one kernel block.\n      num_kv_heads: The number of key/value heads.\n      kv_head_dim_size: The dimension of each key/value head.\n      dtype: The data type for computations.\n      attn_logits_soft_cap: The soft cap for attention logits.\n      query_axis_names: The logical axis names for the query tensor.\n      kv_pages_axis_names: The logical axis names for the KV cache pages.\n      rngs: The random number generators for initialization (required by NNX).\n    \"\"\"\n\n    self.mesh = mesh\n    self.num_pages = num_pages\n    self.tokens_per_page = tokens_per_page\n    self.max_pages_per_slot = max_pages_per_slot\n    self.max_pages_per_prefill = max_pages_per_prefill\n    self.pages_per_compute_block = pages_per_compute_block\n    self.num_kv_heads = num_kv_heads\n    self.kv_head_dim_size = kv_head_dim_size\n    self.dtype = dtype\n    self.attn_logits_soft_cap = attn_logits_soft_cap\n    self.query_axis_names = query_axis_names\n    self.kv_pages_axis_names = kv_pages_axis_names\n\n    self.kv_pages_shape = (\n        self.num_kv_heads,\n        self.num_pages,\n        self.tokens_per_page,\n        self.kv_head_dim_size,\n    )\n\n    self.key_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n    self.value_pages = nnx.Cache(\n        jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n        sharding=self.kv_pages_axis_names,\n    )\n\n  def _maybe_materialize_cache(self, cache: nnx.Cache) -> nnx.Cache:\n    \"\"\"Materializes the cache if it's currently a ShapeDtypeStruct.\"\"\"\n    if isinstance(cache.value, jax.ShapeDtypeStruct):\n      # This is needed because the Linen bridge lazily creates this state. We\n      # need to ensure the cache state is accessible at runtime.\n      # TODO: Delete this function when the to_linen bridge is no longer needed.\n      return nnx.Cache(\n          jnp.zeros(self.kv_pages_shape, dtype=self.dtype),\n          sharding=cache.sharding,\n      )\n    return cache\n\n  def get_kv_pages(self):\n    \"\"\"Retrieves the key and value page caches.\n\n    This method ensures the KV cache pages are materialized (if they are abstract\n    ShapeDtypeStructs, a temporary state during Linen bridge initialization) and\n    applies the necessary sharding constraints.\n\n    Returns:\n      A tuple containing the key pages and value pages caches (`nnx.Cache`).\n    \"\"\"\n\n    # TODO: Remove once to_linen bridge is no longer needed\n    self.key_pages = self._maybe_materialize_cache(self.key_pages)\n    self.value_pages = self._maybe_materialize_cache(self.value_pages)\n\n    self.key_pages.value = nn.with_logical_constraint(self.key_pages.value, self.kv_pages_axis_names)\n    self.value_pages.value = nn.with_logical_constraint(self.value_pages.value, self.kv_pages_axis_names)\n    return self.key_pages, self.value_pages\n\n  def pad_qkv(self, *qkv):\n    \"\"\"Pad input to kv_head_dim_size\"\"\"\n\n    def pad_to_kv_head_dim_size(x):\n      if x.shape[-1] != self.kv_head_dim_size:\n        return jnp.pad(\n            x,\n            ((0, 0), (0, 0), (0, 0), (0, self.kv_head_dim_size - x.shape[-1])),\n            mode=\"constant\",\n            constant_values=0.0,\n        )\n      else:\n        return x\n\n    # Align Q, K, V to the same head dim. This is required by the kernel.\n    return tuple(pad_to_kv_head_dim_size(x) for x in qkv)\n\n  def paged_dot_product_attention_with_max_and_sum(self, query, key, value):\n    \"\"\"paged dot product attention with max & sum\"\"\"\n    b, t, n, d = query.shape\n    _, s, n_kv, _ = key.shape\n    query = jnp.reshape(query, (b, t, n_kv, n // n_kv, d))\n\n    attn_weights = jnp.einsum(\"btkgd,bskd->bkgts\", query, key)\n\n    causal_mask = jnp.triu(jnp.ones((t, s)), k=1)\n    causal_mask = jnp.reshape(causal_mask, (1, 1, 1, t, s))\n    masked_weights = jnp.where(causal_mask, jnp.full_like(attn_weights, -1e10), attn_weights)\n\n    local_max = jnp.max(masked_weights, axis=-1, keepdims=True)\n    local_exps = jnp.exp(masked_weights - local_max)\n    local_sums = jnp.sum(local_exps, axis=-1, keepdims=True)\n\n    attn = jnp.einsum(\"bkgts,bskd->btkgd\", local_exps, value)\n    attn = jnp.reshape(attn, (b, t, n, d))\n\n    local_max = jnp.moveaxis(local_max, -2, 1)\n    local_max = jnp.reshape(local_max, (b, t, n, 1))\n\n    local_sums = jnp.moveaxis(local_sums, -2, 1)\n    local_sums = jnp.reshape(local_sums, (b, t, n, 1))\n\n    return attn, local_max, local_sums\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_prefill(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in prefill only. The assumption\n    is the batch_size is only 1\n    \"\"\"\n    assert query.shape[0] == 1  # ensure the batch size is 0\n    # shape of key_pages_cache.value is [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.array([0, page_state.sequence_lengths[0]])  # [0, prefill_true_length]\n    num_seqs = jnp.array([1])\n    query = query[0]  # [batch_size, max_num_tokens, num_kv_heads, head_dim] to [max_num_tokens, num_kv_heads, head_dim]\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,\n        kv_lens=jnp.array([query.shape[0]]),  # max_prefill_length\n        cu_q_lens=c_q_l,  # the accumulative real lengths of requests, starting from 0\n        page_indices=page_state.page_map,\n        num_seqs=num_seqs,\n        # TODO(rupliu) debug: repeated response when enabled below\n        # num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(result, axis=0)  # [batch_size, seq_len, n_kv_head, head_dim] and batch_size is 1 for now\n\n  # TODO(rupliu): add sharding when SPMD is fully supported\n  def paged_attention_v2_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply ragged input Paged Attention in decode only.\"\"\"\n    batch_size = query.shape[0]\n    query = jnp.squeeze(query, axis=1)  # [batch_size, seq_len, n_kv_head, head_dim] to [batch_size, n_kv_head, head_dim]\n    k_p = jnp.permute_dims(key_pages_cache.value, (1, 2, 0, 3))\n    v_p = jnp.permute_dims(value_pages_cache.value, (1, 2, 0, 3))\n    c_q_l = jnp.arange(batch_size + 1)  # one token per sequence\n    num_seqs = jnp.array([batch_size])  # real number of requests, set it to batch_size\n    result = paged_attention_kernel_v2.ragged_paged_attention(\n        q=query,  # [max_batched_num_tokens, num_kv_heads, head_dim]\n        k_pages=k_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        v_pages=v_p,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n        kv_lens=page_state.sequence_lengths,  # [max_num_seqs]\n        cu_q_lens=c_q_l,  # [max_num_seqs+1]\n        page_indices=page_state.page_map,  # [max_num_seqs, pages_per_seq]\n        num_seqs=num_seqs,\n        num_kv_pages_per_block=self.pages_per_compute_block,\n    )\n    return jnp.expand_dims(\n        result, axis=1\n    )  # [batch_size, n_kv_head, head_dim] to [batch_size, seq_len, n_kv_head, head_dim]\n\n  # v1 kernel has around 20% performance gain than v2 kernel in decode only task\n  def paged_attention_v1_decode(\n      self,\n      query: Array,\n      key_pages_cache: nnx.Cache,\n      value_pages_cache: nnx.Cache,\n      page_state: page_manager.PageState,\n  ) -> Array:\n    \"\"\"Apply Paged Attention v1 in decode only.\"\"\"\n    kv_pages_pspec = nn.logical_to_mesh_axes((\"paged_kv_heads\", None, None, None))\n    q_pspec = nn.logical_to_mesh_axes((None, None, \"paged_kv_heads\", None))\n\n    @functools.partial(\n        shard_map,\n        mesh=self.mesh,\n        in_specs=(\n            q_pspec,\n            kv_pages_pspec,\n            kv_pages_pspec,\n            P(None),\n            P(None, None),\n            None,\n        ),\n        out_specs=q_pspec,\n        check_rep=False,\n    )\n    def wrap_paged_attention(q, k_pages, v_pages, lengths, page_indices, pages_per_compute_block):\n      q = jnp.squeeze(q, axis=1)\n      result = paged_attention_kernel.paged_attention(\n          q=q,  # [batch_size, num_heads, head_dim]\n          k_pages=k_pages,\n          v_pages=v_pages,\n          lengths=lengths,\n          page_indices=page_indices,\n          pages_per_compute_block=pages_per_compute_block,\n      )\n      return jnp.expand_dims(result, axis=1)  # [batch_size, n_kv_head, head_dim] to [batch_size, 1, n_kv_head, head_dim]\n\n    return wrap_paged_attention(\n        query,\n        key_pages_cache.value,\n        value_pages_cache.value,\n        page_state.sequence_lengths,\n        page_state.page_map,\n        self.pages_per_compute_block,\n    )\n\n  def __call__(\n      self,\n      query: Array,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      previous_chunk=None,\n      slot: None | int = None,\n      page_state: None | page_manager.PageState = None,\n  ):\n    \"\"\"Applies the paged attention mechanism.\n\n    This is the main entry point for the module. It takes query, key, and value\n    tensors and performs paged attention based on the current model mode\n    (prefill or autoregressive).\n\n    Args:\n      query: The query tensor.\n      key: The key tensor for the current step.\n      value: The value tensor for the current step.\n      decoder_segment_ids: Segment IDs for the decoder, used for masking.\n      model_mode: The current operational mode, either 'prefill' or\n        'autoregressive'.\n      previous_chunk: Information about previously processed chunks, used for\n        chunked prefill.\n      slot: The batch slot index for the current request.\n      page_state: The current state of the page manager.\n\n    Returns:\n        A tuple (output, exponentials_max, exponentials_sum) containing:\n        - The attention output tensor.\n        - The max of the exponentials (for prefill mode with dot-product attention).\n        - The sum of the exponentials (for prefill mode with dot-product attention).\n        The latter two are None for autoregressive mode, as this is handled\n        internally by the paged attention kernel.\n    \"\"\"\n\n    key_pages_cache, value_pages_cache = self.get_kv_pages()\n    query, key, value = self.pad_qkv(query, key, value)\n\n    # update kv pages and call page attention kernel\n    if model_mode == MODEL_MODE_PREFILL:\n      self.update_prefill_step_pages(key_pages_cache, value_pages_cache, key, value, slot, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_prefill(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return self.paged_dot_product_attention_with_max_and_sum(query, key, value)\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE and page_state is not None:\n      self.update_decode_step_pages(key_pages_cache, value_pages_cache, key, value, page_state)\n      if _use_kernel_v2:\n        return (\n            self.paged_attention_v2_decode(query, key_pages_cache, value_pages_cache, page_state),\n            None,\n            None,\n        )\n      return (\n          self.paged_attention_v1_decode(query, key_pages_cache, value_pages_cache, page_state),\n          None,\n          None,\n      )\n    else:\n      raise NotImplementedError(model_mode)\n\n  def update_prefill_step_pages(\n      self,\n      key_pages_cache: nnx.Cache,  # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n      value_pages_cache: nnx.Cache,\n      key: Array,\n      value: Array,\n      slot: int,\n      page_state: page_manager.PageState,\n  ) -> None:\n    \"\"\"Update pages for prefill step.\"\"\"\n    assert (\n        key.shape == value.shape\n    ), f\"prefill_step key/value should have the same shape, but getting {key.shape=} and {value.shape=} instead\"\n    batch_size, seq_len, n_kv_head, head_dim = key.shape\n    assert seq_len % self.tokens_per_page == 0, f\"seq_length {seq_len} and  tokens_per_page {self.tokens_per_page}\"\n    assert key_pages_cache.value.shape == value_pages_cache.value.shape, (\n        f\"prefill_step key/value_pages_cache should have the same shape, but \"\n        f\"getting {key_pages_cache.shape=} and {value_pages_cache.shape=} instead\"\n    )\n\n    v_n_kv, _, v_p, v_d = key_pages_cache.value.shape\n    assert v_n_kv == n_kv_head, f\"{v_n_kv=} {n_kv_head=}\"\n    assert v_p == self.tokens_per_page, f\"{v_p=} {self.tokens_per_page=}\"\n    assert v_d == head_dim, f\"{v_d=} {head_dim=}\"\n    assert page_state.page_map.shape == (\n        page_state.num_pages_used.shape[0],\n        self.max_pages_per_slot,\n    )\n\n    # Handle both init (b>1) and runtime (b=1) cases\n    if batch_size == 1:\n      key = jnp.squeeze(key)  # [batch_size, seq_len, n_kv_head, head_dim] to [seq_len, n_kv_head, head_dim]\n      value = jnp.squeeze(value)\n    else:\n      key = key[0]\n      value = value[0]\n\n    key = jnp.transpose(key, axes=(1, 0, 2))\n    value = jnp.transpose(value, axes=(1, 0, 2))\n\n    key = jnp.reshape(\n        key,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n    value = jnp.reshape(\n        value,\n        shape=(\n            n_kv_head,\n            max(1, seq_len // self.tokens_per_page),\n            self.tokens_per_page,\n            head_dim,\n        ),\n    )\n\n    key_pages_cache.value = nn.with_logical_constraint(key, self.kv_pages_axis_names)\n    value_pages_cache.value = nn.with_logical_constraint(value, self.kv_pages_axis_names)\n\n  def update_decode_step_pages(self, key_pages_cache, value_pages_cache, key, value, page_state):\n    \"\"\"Update decode-step pages\"\"\"\n    key_pages = key_pages_cache.value\n    value_pages = value_pages_cache.value\n\n    batch_size, _, kv_heads, head_dim = key.shape\n    kv_heads, _, _, head_dim = key_pages.shape\n\n    new_key = key.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_key = jnp.transpose(new_key, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n    new_value = value.reshape(batch_size, kv_heads, head_dim)[:, :, :]\n    new_value = jnp.transpose(new_value, (1, 0, 2))  # [n_kv_heads, batch_size, head_dim]\n\n    broadcast_pages = jnp.tile(page_state.active_page, (kv_heads, 1))  # [n_kv_heads, batch_size]\n    broadcast_pos = jnp.tile(page_state.active_page_position, (kv_heads, 1))  # [n_kv_heads, batch_size]\n\n    kv_indices = jnp.arange(kv_heads)[:, None]  # [n_kv_heads, 1]\n    kv_indices = jnp.tile(kv_indices, (1, batch_size))  # [n_kv_heads, batch_size]\n\n    # [num_kv_heads, num_pages, tokens_per_page, head_dim]\n    key_pages_updated = key_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_key)\n    value_pages_updated = value_pages.at[kv_indices, broadcast_pages, broadcast_pos].set(new_value)\n\n    key_pages_cache.value = key_pages_updated\n    value_pages_cache.value = value_pages_updated\n    return key_pages_cache, value_pages_cache",
        "analysis": {
            "module_type": "paged_attention",
            "purpose": "Implements the paged attention mechanism, an efficient method for handling attention in autoregressive models with long sequences by dividing the KV cache into fixed-size 'pages' to manage memory dynamically.",
            "input": {
                "shape": "query: [batch_size, sequence_length, num_heads, head_dim], key/value: [batch_size, sequence_length, num_kv_heads, head_dim]",
                "dtype": "jnp.float32"
            },
            "processing_steps": [
                "Retrieves and materializes the key and value page caches via `get_kv_pages`.",
                "Pads the query, key, and value tensors to a uniform head dimension via `pad_qkv`.",
                "Checks the `model_mode` to determine the execution path ('prefill' or 'autoregressive').",
                "If 'prefill', updates the KV cache for the entire sequence using `update_prefill_step_pages` and computes attention using either a custom kernel (`paged_attention_v2_prefill`) or a standard dot-product attention implementation.",
                "If 'autoregressive', updates the KV cache for a single token using `update_decode_step_pages` and computes attention using a custom paged attention kernel (`paged_attention_v1_decode` or `paged_attention_v2_decode`).",
                "Returns the attention output and, for the prefill dot-product path, the max and sum of the exponentials."
            ],
            "output": {
                "shape": "A tuple (output, exponentials_max, exponentials_sum). The output tensor has shape [batch_size, sequence_length, num_heads, head_dim]. The other two are tensors or None."
            },
            "dependencies": [
                "nnx.Module",
                "nnx.Cache",
                "page_manager.PageState",
                "paged_attention_kernel",
                "paged_attention_kernel_v2"
            ],
            "parameters": {
                "mesh": "The device mesh for sharding.",
                "num_pages": "The total number of pages in the KV cache.",
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_slot": "The maximum number of pages a single sequence can use.",
                "num_kv_heads": "The number of key/value heads.",
                "kv_head_dim_size": "The dimension of each key/value head."
            },
            "notes": [
                "The class is implemented using Flax NNX.",
                "The behavior is controlled by the `model_mode` argument ('prefill' or 'autoregressive') and a global `_use_kernel_v2` flag.",
                "Includes temporary logic (`_maybe_materialize_cache`) to handle lazy initialization when used with the `nnx.bridge.to_linen` compatibility layer."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PagedAttentionOp module, setting up configuration parameters and creating the paged KV cache as `nnx.Cache` attributes.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Stores configuration parameters like mesh, page dimensions, and head counts.",
                        "Calculates the shape of the KV page tensors.",
                        "Initializes `self.key_pages` and `self.value_pages` as `nnx.Cache` objects containing zero-initialized tensors with specified sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "nnx.Cache",
                        "jnp.zeros"
                    ],
                    "notes": [
                        "The `rngs` parameter is required for compatibility with `nnx.bridge.to_linen` but is not used by the module itself."
                    ]
                },
                "_maybe_materialize_cache": {
                    "purpose": "Materializes an `nnx.Cache` value from a `jax.ShapeDtypeStruct` to a concrete zero-initialized tensor, a workaround for lazy initialization with the Linen bridge.",
                    "input": {
                        "shape": "Input is an `nnx.Cache` object.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Checks if the `cache.value` is an instance of `jax.ShapeDtypeStruct`.",
                        "If it is, creates a new `nnx.Cache` with a zero tensor of the correct shape and sharding.",
                        "Returns the original or the newly created cache."
                    ],
                    "output": {
                        "shape": "Returns an `nnx.Cache` object with a materialized tensor."
                    },
                    "dependencies": [
                        "jax.ShapeDtypeStruct",
                        "nnx.Cache",
                        "jnp.zeros"
                    ],
                    "notes": [
                        "This method is a temporary solution intended to be removed once the `to_linen` bridge is no longer needed."
                    ]
                },
                "get_kv_pages": {
                    "purpose": "Retrieves the key and value page caches, ensuring they are materialized and have the necessary sharding constraints applied.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `_maybe_materialize_cache` on `self.key_pages` and `self.value_pages`.",
                        "Applies logical sharding constraints to the cache values using `nn.with_logical_constraint`.",
                        "Returns a tuple of the key and value page caches."
                    ],
                    "output": {
                        "shape": "A tuple containing two `nnx.Cache` objects for keys and values."
                    },
                    "dependencies": [
                        "self._maybe_materialize_cache",
                        "nn.with_logical_constraint"
                    ],
                    "notes": []
                },
                "pad_qkv": {
                    "purpose": "Pads the input query, key, and value tensors along their last dimension to match `self.kv_head_dim_size`.",
                    "input": {
                        "shape": "[batch_size, sequence_length, num_heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Iterates through the input tensors (q, k, v).",
                        "If a tensor's last dimension is smaller than `self.kv_head_dim_size`, it pads it with zeros using `jnp.pad`.",
                        "Returns a tuple of the padded tensors."
                    ],
                    "output": {
                        "shape": "A tuple of tensors, each with shape [batch_size, sequence_length, num_heads, kv_head_dim_size]."
                    },
                    "dependencies": [
                        "jnp.pad"
                    ],
                    "notes": [
                        "This alignment is a requirement for the underlying paged attention kernels."
                    ]
                },
                "paged_dot_product_attention_with_max_and_sum": {
                    "purpose": "Performs a standard dot-product attention calculation with causal masking, returning the attention output along with the max and sum of exponentials for numerical stability.",
                    "input": {
                        "shape": "query: [b, t, n, d], key: [b, s, n_kv, d], value: [b, s, n_kv, d]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Reshapes the query for grouped query attention.",
                        "Computes attention weights with `jnp.einsum`.",
                        "Applies a causal mask.",
                        "Calculates the local max of weights for stable softmax.",
                        "Computes exponentiated weights and their sum.",
                        "Computes the final attention output by multiplying exponentiated weights with values.",
                        "Reshapes the output, max, and sum tensors to the final format."
                    ],
                    "output": {
                        "shape": "A tuple of (attn, local_max, local_sums) with shapes ([b, t, n, d], [b, t, n, 1], [b, t, n, 1])."
                    },
                    "dependencies": [
                        "jnp.einsum",
                        "jnp.triu",
                        "jnp.where",
                        "jnp.max",
                        "jnp.exp",
                        "jnp.sum"
                    ],
                    "notes": [
                        "This method is used as a fallback for prefill when `_use_kernel_v2` is False."
                    ]
                },
                "paged_attention_v2_prefill": {
                    "purpose": "Applies paged attention for a single prefill sequence using the v2 ragged paged attention kernel.",
                    "input": {
                        "shape": "query: [1, max_num_tokens, num_kv_heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Asserts that the batch size of the query is 1.",
                        "Permutes the dimensions of the key and value page caches to match the kernel's expected layout.",
                        "Prepares metadata for the kernel, such as cumulative query lengths and number of sequences.",
                        "Calls `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Adds a batch dimension back to the result."
                    ],
                    "output": {
                        "shape": "[1, seq_len, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "paged_attention_kernel_v2.ragged_paged_attention",
                        "jnp.permute_dims"
                    ],
                    "notes": [
                        "This method is specifically for prefill and assumes a batch size of 1."
                    ]
                },
                "paged_attention_v2_decode": {
                    "purpose": "Applies paged attention for a batch of decode steps using the v2 ragged paged attention kernel.",
                    "input": {
                        "shape": "query: [batch_size, 1, n_kv_head, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Squeezes the sequence length dimension from the query.",
                        "Permutes the dimensions of the key and value page caches.",
                        "Prepares metadata for the kernel, such as sequence lengths and cumulative query lengths.",
                        "Calls `paged_attention_kernel_v2.ragged_paged_attention`.",
                        "Adds the sequence length dimension back to the result."
                    ],
                    "output": {
                        "shape": "[batch_size, 1, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "paged_attention_kernel_v2.ragged_paged_attention",
                        "jnp.permute_dims"
                    ],
                    "notes": []
                },
                "paged_attention_v1_decode": {
                    "purpose": "Applies paged attention for a batch of decode steps using the v1 paged attention kernel, wrapped in a `shard_map` for SPMD.",
                    "input": {
                        "shape": "query: [batch_size, 1, n_kv_head, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Defines input and output sharding specifications (`in_specs`, `out_specs`).",
                        "Defines a wrapper function that squeezes the query's sequence dimension and calls `paged_attention_kernel.paged_attention`.",
                        "Applies `jax.experimental.shard_map.shard_map` to the wrapper function with the provided inputs."
                    ],
                    "output": {
                        "shape": "[batch_size, 1, n_kv_head, head_dim]"
                    },
                    "dependencies": [
                        "jax.experimental.shard_map.shard_map",
                        "paged_attention_kernel.paged_attention"
                    ],
                    "notes": [
                        "The docstring states this kernel offers better performance than v2 for decode-only tasks."
                    ]
                },
                "__call__": {
                    "purpose": "Main entry point that applies the paged attention mechanism by updating the KV cache and calling the appropriate attention function based on the model's operational mode.",
                    "input": {
                        "shape": "query: [batch, seq, heads, dim], key/value: [batch, seq, kv_heads, dim], model_mode: str, page_state: PageState",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Calls `get_kv_pages` to retrieve the KV caches.",
                        "Calls `pad_qkv` to pad inputs.",
                        "If `model_mode` is 'prefill', calls `update_prefill_step_pages` and then an appropriate prefill attention function.",
                        "If `model_mode` is 'autoregressive', calls `update_decode_step_pages` and then an appropriate decode attention function.",
                        "Returns the attention output."
                    ],
                    "output": {
                        "shape": "A tuple (output, exponentials_max, exponentials_sum)."
                    },
                    "dependencies": [
                        "self.get_kv_pages",
                        "self.pad_qkv",
                        "self.update_prefill_step_pages",
                        "self.update_decode_step_pages",
                        "self.paged_attention_v1_decode",
                        "self.paged_attention_v2_decode",
                        "self.paged_attention_v2_prefill",
                        "self.paged_dot_product_attention_with_max_and_sum"
                    ],
                    "notes": [
                        "The control flow is determined by the `model_mode` string and the global `_use_kernel_v2` boolean flag."
                    ]
                },
                "update_prefill_step_pages": {
                    "purpose": "Updates the KV cache pages with new key/value data from a full sequence during a prefill step.",
                    "input": {
                        "shape": "key/value: [batch_size, seq_len, n_kv_head, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Performs shape assertions on inputs.",
                        "Squeezes the batch dimension from the key and value tensors.",
                        "Transposes and reshapes the key and value tensors to match the paged cache layout: [n_kv_head, num_pages, tokens_per_page, head_dim].",
                        "Overwrites the `.value` of the key and value caches with the new, reshaped data."
                    ],
                    "output": {
                        "shape": "None"
                    },
                    "dependencies": [
                        "jnp.squeeze",
                        "jnp.transpose",
                        "jnp.reshape",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "This method performs a bulk update of the cache, assuming the input sequence length is a multiple of the page size."
                    ]
                },
                "update_decode_step_pages": {
                    "purpose": "Updates the KV cache with a single new key/value token for each sequence in the batch during a decode step.",
                    "input": {
                        "shape": "key/value: [batch_size, 1, kv_heads, head_dim]",
                        "dtype": "float32"
                    },
                    "processing_steps": [
                        "Reshapes and transposes the new key and value tensors to [n_kv_heads, batch_size, head_dim].",
                        "Retrieves the target page indices and positions within those pages from `page_state`.",
                        "Uses `tensor.at[...].set(...)` to perform a scattered update, inserting the new key/value data into the correct locations in the page caches.",
                        "Returns the updated caches."
                    ],
                    "output": {
                        "shape": "A tuple of the updated key and value `nnx.Cache` objects."
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "jnp.tile",
                        "jnp.arange"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#MultiPageAsyncCopyDescriptor",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "class MultiPageAsyncCopyDescriptor:\n  \"\"\"Descriptor for async copy of multiple K/V pages from HBM.\"\"\"\n\n  def __init__(\n      self,\n      pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads_per_blk, head_dim]\n      vmem_buf,  # [num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n      sem,\n      page_indices_ref,  # i32[max_num_seqs, pages_per_seq]\n      offset,  # [seq_idx, kv_pages_start]\n  ):\n    self._vmem_buf = vmem_buf\n    seq_id, kv_pages_start = offset\n    self._async_copies = [\n        pltpu.make_async_copy(\n            pages_hbm_ref.at[page_indices_ref[seq_id, kv_pages_start + i]],\n            vmem_buf.at[i],\n            sem,\n        )\n        for i in range(vmem_buf.shape[0])\n    ]\n\n  def start(self):\n    \"\"\"Starts the async copies.\"\"\"\n    for async_copy in self._async_copies:\n      async_copy.start()\n\n  def wait(self):\n    for async_copy in self._async_copies:\n      async_copy.wait()\n    return self._vmem_buf",
        "analysis": {
            "functionality": "The `MultiPageAsyncCopyDescriptor` class manages the asynchronous copying of multiple non-contiguous Key/Value (K/V) pages from High Bandwidth Memory (HBM) to a contiguous buffer in Vector Memory (VMEM) on a TPU. It encapsulates the setup, initiation, and synchronization of these copy operations.",
            "usage": "To use this class, instantiate it with references to the source HBM data (`pages_hbm_ref`), the destination VMEM buffer (`vmem_buf`), a semaphore (`sem`), page indices (`page_indices_ref`), and an offset (`offset`). Call the `start()` method to begin the non-blocking copy operations. Call the `wait()` method to block until all copies are complete and retrieve the populated VMEM buffer."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ref_ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ref_ragged_paged_attention(\n    queries: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1],\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n):\n  \"\"\"Ref ragged paged attention.\"\"\"\n  _, _, num_kv_heads, head_dim = k_pages.shape\n  num_q_heads = queries.shape[1]\n  assert num_q_heads % num_kv_heads == 0\n  num_query_per_kv = num_q_heads // num_kv_heads\n  outputs = []\n  for i in range(num_seqs[0]):\n    q_start = cu_q_lens[i]\n    q_end = cu_q_lens[i + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens[i]\n    indices = page_indices[i]\n    q = queries[q_start:q_end]\n    k = k_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    v = v_pages[indices, :, :, :].reshape(-1, num_kv_heads, head_dim)[:kv_len]\n    k = jnp.repeat(k, num_query_per_kv, axis=1)\n    v = jnp.repeat(v, num_query_per_kv, axis=1)\n    attn = jnp.einsum(\"qhd,khd->hqk\", q, k, preferred_element_type=jnp.float32)\n    attn *= sm_scale\n    q_span = (kv_len - q_len) + jax.lax.broadcasted_iota(jnp.int32, attn.shape, 1)\n    kv_span = jax.lax.broadcasted_iota(jnp.int32, attn.shape, 2)\n    attn += jnp.where(q_span < kv_span, mask_value, 0.0)\n    attn = jax.nn.softmax(attn, axis=-1).astype(v.dtype)\n    out = jnp.einsum(\"hqk,khd->qhd\", attn, v).astype(queries.dtype)\n    outputs.append(out)\n\n  return jnp.concatenate(outputs, axis=0)",
        "analysis": {
            "functionality": "Performs a reference implementation of ragged paged attention. It iterates through each sequence in a batch, gathers the corresponding key and value pages, computes causal self-attention, and concatenates the results.",
            "usage": "This function is used for calculating attention on batched sequences with varying lengths, where keys and values are stored in a paged memory format. It's primarily for correctness verification against more optimized kernels. Inputs include query tensors, paged key/value caches, and metadata arrays describing sequence lengths and page locations. It outputs a single tensor containing the concatenated attention results for all queries."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#validate_inputs_on_runtime",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def validate_inputs_on_runtime(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"validate inputs on runtime\"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  max_num_batched_tokens = q.shape[0]\n  page_size = k_pages.shape[1]\n  max_num_seqs, pages_per_seq = page_indices.shape\n  if num_seqs[0] > max_num_seqs:\n    raise ValueError(f\"{num_seqs[0]=} must be less or equal to {max_num_seqs=}\")\n  max_kv_len = jnp.max(kv_lens)\n  min_pages_per_seq = ceil_div(max_kv_len, page_size)\n  if pages_per_seq < min_pages_per_seq:\n    raise ValueError(\n        f\"{pages_per_seq=} must be greater or equal to\" f\" {min_pages_per_seq=} given {max_kv_len=} and {page_size=}.\"\n    )\n  if cu_q_lens[num_seqs[0]] > max_num_batched_tokens:\n    raise ValueError(f\"Total q tokens {cu_q_lens[num_seqs[0]]} must be less or equal to\" f\" {max_num_batched_tokens=}.\")\n  for i in range(num_seqs[0]):\n    q_len = cu_q_lens[i + 1] - cu_q_lens[i]\n    kv_len = kv_lens[i]\n    if q_len > kv_len:\n      raise ValueError(f\"{q_len=} must be less or equal to {kv_len=} at sequence {i}.\")",
        "analysis": {
            "module_type": "runtime_input_validator",
            "purpose": "Performs runtime validation of input tensors for a ragged paged attention operation, raising a ValueError if any inconsistency is found.",
            "input": {
                "shape": "q: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "q, k_pages, v_pages: float (e.g., bfloat16, float32); kv_lens, page_indices, cu_q_lens, num_seqs: int32"
            },
            "processing_steps": [
                "Calls `check_inputs_shapes` to validate static properties like tensor shapes and dtypes.",
                "Checks if the number of active sequences (`num_seqs`) exceeds the maximum capacity derived from `page_indices` shape.",
                "Calculates the minimum required pages per sequence based on the maximum `kv_len` and `page_size`, and verifies it against the provided `pages_per_seq`.",
                "Verifies that the total number of query tokens does not exceed the allocated buffer size (`max_num_batched_tokens`).",
                "Iterates through each active sequence to ensure its query length is not greater than its key/value length.",
                "Raises a `ValueError` if any of the checks fail."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "check_inputs_shapes",
                "jax.numpy.max",
                "ceil_div"
            ],
            "parameters": {},
            "notes": [
                "This function is intended to be called at runtime, as it checks the values within the tensors, not just their static shapes.",
                "The function has no return value; its purpose is to raise an exception on invalid input.",
                "It complements `check_inputs_shapes`, which is intended for compile-time checks."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#check_inputs_shapes",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def check_inputs_shapes(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs,  # i32[1]\n):\n  \"\"\"check shapes of inputs\"\"\"\n  _, num_q_heads, head_dim = q.shape\n  _, _, num_kv_heads, head_dim_k = k_pages.shape\n  max_num_seqs, _ = page_indices.shape\n  if num_seqs.shape != (1,):\n    raise ValueError(f\"{num_seqs.shape=} must be (1,)\")\n  if k_pages.shape != v_pages.shape:\n    raise ValueError(f\"{k_pages.shape=} and {v_pages.shape=} must have the same shape.\")\n  if head_dim_k != head_dim:\n    raise ValueError(f\"Q head_dim {head_dim} must be the same as that of K/V {head_dim_k}.\")\n  if kv_lens.shape != (max_num_seqs,):\n    raise ValueError(\n        f\"Expected {kv_lens.shape=} to be ({max_num_seqs},) where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if cu_q_lens.shape != (max_num_seqs + 1,):\n    raise ValueError(\n        f\"Expected {cu_q_lens.shape=} to be ({max_num_seqs + 1},)  where\" \" `max_num_seqs` is `page_indices.shape[0]`.\"\n    )\n  if kv_lens.dtype != jnp.int32 or page_indices.dtype != jnp.int32 or cu_q_lens.dtype != jnp.int32:\n    raise ValueError(\n        \"The dtype of `kv_lens`, `page_indices`, and `cu_q_lens` must be\"\n        f\" int32. Got {kv_lens.dtype=}, {page_indices.dtype=},\"\n        f\" {cu_q_lens.dtype=}.\"\n    )\n  if num_q_heads % num_kv_heads != 0:\n    raise ValueError(f\"{num_q_heads=} must be divisible by {num_kv_heads=}\")",
        "analysis": {
            "module_type": "attention_input_shape_validator",
            "purpose": "Validates the shapes, dtypes, and dimensional consistency of input tensors for a ragged paged attention operation, raising a ValueError upon failure.",
            "input": {
                "shape": "q: [max_num_batched_tokens, num_q_heads, head_dim], k_pages: [total_num_pages, page_size, num_kv_heads, head_dim], v_pages: [total_num_pages, page_size, num_kv_heads, head_dim], kv_lens: [max_num_seqs], page_indices: [max_num_seqs, pages_per_seq], cu_q_lens: [max_num_seqs + 1], num_seqs: [1]",
                "dtype": "q, k_pages, v_pages: float (e.g., bfloat16, float32); kv_lens, page_indices, cu_q_lens, num_seqs: int32"
            },
            "processing_steps": [
                "Verify that the shape of `num_seqs` is (1,).",
                "Ensure that `k_pages` and `v_pages` have identical shapes.",
                "Confirm that the head dimension (`head_dim`) of `q` is the same as that of `k_pages`.",
                "Check that the shape of `kv_lens` is `(max_num_seqs,)`.",
                "Check that the shape of `cu_q_lens` is `(max_num_seqs + 1,)`.",
                "Validate that the dtypes of `kv_lens`, `page_indices`, and `cu_q_lens` are all int32.",
                "Ensure that `num_q_heads` is evenly divisible by `num_kv_heads`."
            ],
            "output": {
                "shape": "N/A (This function does not return a value.)"
            },
            "dependencies": [
                "jax.numpy"
            ],
            "parameters": {},
            "notes": [
                "This function is intended for compile-time validation, as indicated by the comment in the source code.",
                "It raises a `ValueError` if any of the checks fail."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention_kernel",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention_kernel(\n    # Prefetch\n    kv_lens_ref,  # [max_num_seqs]\n    page_indices_ref,  # [max_num_seqs, pages_per_seq]\n    cu_q_lens_ref,  # [max_num_seqs + 1]\n    seq_buf_idx_ref,\n    # TODO(jevinjiang): if OOM in SMEM, consider pack to other scalar refs.\n    num_seqs_ref,\n    # Input\n    q_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    k_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages_hbm_ref,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    # Output\n    o_ref,  # [num_q_per_blk, num_q_heads_per_blk, head_dim]\n    # Scratch\n    k_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    v_bufs,  # [2, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, head_dim]\n    sems,  # [2, 2]\n    l_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    m_ref,  # [num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128]\n    *,\n    sm_scale: float,\n    mask_value: float,\n):\n  \"\"\"ragged paged-attention kernel\"\"\"\n  num_q_per_blk, num_q_heads_per_blk, head_dim = q_ref.shape\n  num_seqs = num_seqs_ref[0]\n  _, num_kv_pages_per_blk, page_size, num_kv_heads_per_blk, _ = k_bufs.shape\n  num_kv_per_blk = num_kv_pages_per_blk * page_size\n  num_q_heads_per_kv_head = num_q_heads_per_blk // num_kv_heads_per_blk\n  heads_blk_idx, q_blk_idx = (\n      pl.program_id(0),\n      pl.program_id(1),\n  )\n  num_heads_blks = pl.num_programs(0)\n  init_seq_idx = seq_buf_idx_ref[0]\n  init_buf_idx = seq_buf_idx_ref[1]\n  q_len_start = q_blk_idx * num_q_per_blk\n  q_len_end = q_len_start + num_q_per_blk\n\n  def create_kv_async_copy_descriptors(heads_blk_idx, seq_idx, kv_blk_idx, buf_idx):\n    offset = (seq_idx, kv_blk_idx * num_kv_pages_per_blk)\n    heads_start = heads_blk_idx * num_kv_heads_per_blk\n    async_copy_k = MultiPageAsyncCopyDescriptor(\n        k_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        k_bufs.at[buf_idx],\n        sems.at[buf_idx, 0],\n        page_indices_ref,\n        offset,\n    )\n    async_copy_v = MultiPageAsyncCopyDescriptor(\n        v_pages_hbm_ref.at[:, :, pl.ds(heads_start, num_kv_heads_per_blk), :],\n        v_bufs.at[buf_idx],\n        sems.at[buf_idx, 1],\n        page_indices_ref,\n        offset,\n    )\n    return async_copy_k, async_copy_v\n\n  # TODO(jevinjiang): Add these to Mosaic:\n  # 1. Support arbitrary strided load/store for any dtype.\n  # 2. Support arbitrary strided load/store for any last dimension.\n  def strided_load_kv(ref, start, step):\n    if ref.dtype == jnp.float32:\n      return ref[start::step, :]\n    packing = get_dtype_packing(ref.dtype)\n    assert ref.dtype == jnp.bfloat16\n    assert step % packing == 0\n    b_start = start // packing\n    b_offset = start % packing\n    b_step = step // packing\n    b_ref = ref.bitcast(jnp.int32)\n    b = b_ref[b_start::b_step, :]\n    bw = 32 // packing\n    b = jnp.right_shift(b, bw * b_offset)\n    b = jnp.left_shift(b, bw * (packing - 1))\n    return pltpu.bitcast(b, jnp.float32).astype(jnp.bfloat16)\n\n  def fold_on_2nd_minor(vec):\n    assert vec.dtype in (jnp.bfloat16, jnp.float32)\n    assert len(vec.shape) >= 2\n    last_dim = vec.shape[-1]\n    packing = get_dtype_packing(vec.dtype)\n    if vec.shape[-2] % packing != 0:\n      vec = vec.astype(jnp.float32)\n    return vec.reshape(-1, last_dim)\n\n  @pl.when(heads_blk_idx + q_blk_idx == 0)\n  def prefetch_first_kv_blk():\n    async_copy_k, async_copy_v = create_kv_async_copy_descriptors(heads_blk_idx, init_seq_idx, 0, init_buf_idx)\n    async_copy_k.start()\n    async_copy_v.start()\n\n  def is_cur_q_blk_needed(q_states):\n    done, cur_seq_idx, _ = q_states\n    return jnp.logical_and(done == 0, cur_seq_idx < num_seqs)\n\n  def compute_with_cur_q_blk(q_states):\n    done, cur_seq_idx, cur_buf_idx = q_states\n    q_start = cu_q_lens_ref[cur_seq_idx]\n    q_end = cu_q_lens_ref[cur_seq_idx + 1]\n    q_len = q_end - q_start\n    kv_len = kv_lens_ref[cur_seq_idx]\n\n    def get_next_prefetch_ids(heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx):\n      next_kv_blk_idx = kv_blk_idx + 1\n      is_last_kv_blk = next_kv_blk_idx * num_kv_per_blk >= kv_len\n      next_kv_blk_idx = lax.select(\n          is_last_kv_blk,\n          0,\n          next_kv_blk_idx,\n      )\n      is_cur_seq_end_in_cur_q_blk = q_end <= q_len_end\n      next_seq_idx = lax.select(\n          is_last_kv_blk,\n          lax.select(is_cur_seq_end_in_cur_q_blk, cur_seq_idx + 1, cur_seq_idx),\n          cur_seq_idx,\n      )\n      is_last_seq = next_seq_idx == num_seqs\n      next_seq_idx = lax.select(\n          is_last_seq,\n          0,\n          next_seq_idx,\n      )\n      next_heads_blk_idx = lax.select(\n          is_last_seq,\n          heads_blk_idx + 1,\n          heads_blk_idx,\n      )\n      next_buf_idx = lax.select(cur_buf_idx == 0, 1, 0)\n      return next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n\n    def flash_attention(\n        q,  # [num_q_per_blk * num_q_heads_per_kv_head, head_dim]\n        k,  # [num_kv_per_blk, head_dim]\n        v,  # [num_kv_per_blk, head_dim]\n        head_l_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_m_ref,  # [num_q_per_blk * num_q_heads_per_kv_head, 128]\n        head_o_ref,  # [num_q_per_blk, num_q_heads_per_kv_head, head_dim]\n        *,\n        kv_blk_idx,\n    ):\n      assert q.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          head_dim,\n      )\n      assert k.shape == (\n          num_kv_per_blk,\n          head_dim,\n      ), f\"{k.shape=}, {(num_kv_per_blk, head_dim)=} {k.dtype=}\"\n      assert v.shape == (num_kv_per_blk, head_dim)\n      assert head_m_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_l_ref.shape == (\n          num_q_per_blk * num_q_heads_per_kv_head,\n          128,\n      )\n      assert head_o_ref.shape == (\n          num_q_per_blk,\n          num_q_heads_per_kv_head,\n          head_dim,\n      )\n      kv_len_start = kv_blk_idx * num_kv_per_blk\n\n      def masked_store(ref, val, start, end, group=1):\n        iota = lax.broadcasted_iota(jnp.int32, ref.shape, 0) // group\n        mask = jnp.logical_and(iota >= start, iota < end)\n        pl.store(ref, tuple(slice(None) for _ in ref.shape), val, mask=mask)\n\n      qk = jnp.einsum(\"nd,md->nm\", q, k, preferred_element_type=jnp.float32) * sm_scale\n      store_start = jnp.maximum(q_start - q_len_start, 0)\n      store_end = jnp.minimum(q_end - q_len_start, num_q_per_blk)\n\n      @pl.when(kv_blk_idx == 0)\n      def init_scratch_ref():\n        masked_store(\n            head_m_ref,\n            jnp.full_like(head_m_ref, -jnp.inf),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_l_ref,\n            jnp.zeros_like(head_l_ref),\n            store_start,\n            store_end,\n            num_q_heads_per_kv_head,\n        )\n        masked_store(\n            head_o_ref,\n            jnp.zeros_like(head_o_ref),\n            store_start,\n            store_end,\n        )\n\n      row_ids = (\n          (kv_len - q_len)\n          + q_len_start\n          - q_start\n          + jax.lax.broadcasted_iota(\n              jnp.int32,\n              (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n              0,\n          )\n          // num_q_heads_per_kv_head\n      )\n      col_ids = kv_len_start + jax.lax.broadcasted_iota(\n          jnp.int32,\n          (num_q_per_blk * num_q_heads_per_kv_head, num_kv_per_blk),\n          1,\n      )\n      causal_mask = row_ids < col_ids\n      qk += jnp.where(causal_mask, mask_value, 0.0)\n      m_curr = jnp.max(qk, axis=1, keepdims=True)\n      s_curr = jnp.exp(qk - m_curr)\n      qkv = jnp.dot(s_curr, v, preferred_element_type=jnp.float32)\n      lm_store_shape = head_m_ref.shape\n      m_curr = jnp.broadcast_to(m_curr, lm_store_shape)\n      l_curr = jnp.broadcast_to(s_curr.sum(axis=1, keepdims=True), lm_store_shape)\n      m_prev = head_m_ref[...]\n      l_prev = head_l_ref[...]\n      m_next = jnp.maximum(m_prev, m_curr)\n      masked_store(head_m_ref, m_next, store_start, store_end, num_q_heads_per_kv_head)\n      alpha = jnp.exp(m_prev - m_next)\n      beta = jnp.exp(m_curr - m_next)\n      l_alpha = alpha * l_prev\n      l_next = l_alpha + beta * l_curr\n      l_next_safe = jnp.where(l_next == 0.0, 1.0, l_next)\n      masked_store(\n          head_l_ref,\n          l_next_safe,\n          store_start,\n          store_end,\n          num_q_heads_per_kv_head,\n      )\n\n      def broadcast_to_shape(arr, shape):\n        if arr.shape == shape:\n          return arr\n        assert len(arr.shape) == len(shape)\n        assert arr.shape[0] == shape[0]\n        assert shape[1] % arr.shape[1] == 0\n        # no-op concatenation.\n        return jnp.concatenate([arr for _ in range(shape[1] // arr.shape[1])], axis=1)\n\n      o_curr = head_o_ref[...].reshape(-1, head_dim)\n      l_alpha = broadcast_to_shape(l_alpha, qkv.shape)\n      beta = broadcast_to_shape(beta, qkv.shape)\n      l_next_safe = broadcast_to_shape(l_next_safe, qkv.shape)\n      out = lax.div(\n          l_alpha * o_curr + beta * qkv,\n          l_next_safe,\n      ).astype(head_o_ref.dtype)\n      masked_store(\n          head_o_ref,\n          out.reshape(head_o_ref.shape),\n          store_start,\n          store_end,\n      )\n\n    def is_valid_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, _ = kv_states\n      return kv_blk_idx * num_kv_per_blk < kv_len\n\n    def compute_with_kv_blk_in_cur_seq(kv_states):\n      kv_blk_idx, cur_buf_idx = kv_states\n      next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx = get_next_prefetch_ids(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n\n      @pl.when(next_heads_blk_idx < num_heads_blks)\n      def prefetch_next_kv_blk():\n        # TODO(jevinjiang): reuse the same buffer if it is already prefetched!\n        # TODO(jevinjiang): only fetch effective dynamic size to hold kv_len and\n        # DMA to fixed size buffer!\n        next_async_copy_k, next_async_copy_v = create_kv_async_copy_descriptors(\n            next_heads_blk_idx, next_seq_idx, next_kv_blk_idx, next_buf_idx\n        )\n        next_async_copy_k.start()\n        next_async_copy_v.start()\n\n      cur_async_copy_k, cur_async_copy_v = create_kv_async_copy_descriptors(\n          heads_blk_idx, cur_seq_idx, kv_blk_idx, cur_buf_idx\n      )\n      kv_to_load_shape = (\n          num_kv_pages_per_blk * page_size * num_kv_heads_per_blk,\n          head_dim,\n      )\n      k_ref = cur_async_copy_k.wait().reshape(kv_to_load_shape)\n      v_ref = cur_async_copy_v.wait().reshape(kv_to_load_shape)\n      for kv_head_idx in range(num_kv_heads_per_blk):\n        q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n        # TODO(jevinjiang): extra handling for packed type that can start at\n        # unaligned position!\n        q = fold_on_2nd_minor(q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :])\n        k = strided_load_kv(k_ref, kv_head_idx, num_kv_heads_per_blk)\n        v = strided_load_kv(v_ref, kv_head_idx, num_kv_heads_per_blk)\n        flash_attention(\n            q,\n            k,\n            v,\n            l_ref.at[kv_head_idx],\n            m_ref.at[kv_head_idx],\n            o_ref.at[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :],\n            kv_blk_idx=kv_blk_idx,\n        )\n      return kv_blk_idx + 1, next_buf_idx\n\n    _, next_buf_idx = lax.while_loop(\n        is_valid_kv_blk_in_cur_seq,\n        compute_with_kv_blk_in_cur_seq,\n        (0, cur_buf_idx),  # (kv_blk_idx, buf_idx)\n    )\n    next_seq_idx = lax.select(q_end <= q_len_end, cur_seq_idx + 1, cur_seq_idx)\n    done = lax.select(q_end < q_len_end, done, 1)\n    return done, next_seq_idx, next_buf_idx\n\n  _, seq_idx, buf_idx = lax.while_loop(\n      is_cur_q_blk_needed,\n      compute_with_cur_q_blk,\n      (0, init_seq_idx, init_buf_idx),  # (done, seq_idx, buf_idx)\n  )\n  # Reset seq_idx for next kv_heads_blk if run out of seqs!\n  seq_buf_idx_ref[0] = lax.select(seq_idx < num_seqs, seq_idx, 0)\n  seq_buf_idx_ref[1] = buf_idx",
        "analysis": {
            "functionality": "Performs a block-wise, memory-efficient paged attention calculation for ragged sequences on a TPU. It uses an online softmax (FlashAttention) algorithm with double-buffered prefetching of key/value pages from HBM to on-chip VMEM.",
            "usage": "This is a Pallas kernel, not meant to be called directly but via `pl.pallas_call`. It takes references to query blocks, full K/V pages in HBM, and metadata about sequence lengths and page mappings. It computes a block of the attention output and writes it to the output reference. The kernel is parallelized across query blocks and head blocks."
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ceil_div",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ceil_div(a, b):\n  assert b != 0\n  return (a + b - 1) // b",
        "analysis": {
            "module_type": "integer_utility_function",
            "purpose": "Calculates the ceiling of the integer division of two numbers (a / b).",
            "input": {
                "shape": "N/A",
                "dtype": "integer"
            },
            "processing_steps": [
                "Assert that the divisor `b` is not zero.",
                "Calculate `(a + b - 1) // b` using integer arithmetic and return the result."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "a": "The dividend.",
                "b": "The divisor."
            },
            "notes": [
                "This function implements ceiling division using only integer arithmetic, which is an efficient method for positive integers that avoids floating-point operations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_dtype_packing",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_dtype_packing(dtype):\n  if dtype == jnp.float32:\n    return 1\n  if dtype == jnp.bfloat16:\n    return 2\n  if dtype == jnp.int8:\n    return 4\n  if dtype == jnp.int4:\n    return 8\n  raise ValueError(f\"Not implemented: unsupported {dtype=}\")",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Returns an integer packing factor for a given JAX numpy data type, raising an error for unsupported types.",
            "input": {
                "shape": "N/A",
                "dtype": "A jax.numpy dtype object (e.g., jnp.float32, jnp.bfloat16)."
            },
            "processing_steps": [
                "Check if the input dtype is jnp.float32 and return 1.",
                "Check if the input dtype is jnp.bfloat16 and return 2.",
                "Check if the input dtype is jnp.int8 and return 4.",
                "Check if the input dtype is jnp.int4 and return 8.",
                "If the dtype is not supported, raise a ValueError."
            ],
            "output": {
                "shape": "A scalar integer (1, 2, 4, or 8)."
            },
            "dependencies": [
                "jax.numpy as jnp"
            ],
            "parameters": {},
            "notes": [
                "The packing factor appears to correspond to how many elements of the given dtype can fit into a 32-bit word (32 / bits_in_dtype).",
                "This function is used to handle data types of different bit widths within the TPU kernel."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#get_min_heads_per_blk",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def get_min_heads_per_blk(num_q_heads, num_kv_heads, q_dtype, kv_dtype):\n  \"\"\"get min heads per block\"\"\"\n  q_packing = get_dtype_packing(q_dtype)\n  kv_packing = get_dtype_packing(kv_dtype)\n\n  def can_be_xla_fully_tiled(x, packing):\n    if x % packing != 0:\n      return False\n    x //= packing\n    return x in (1, 2, 4, 8) or x % 8 == 0\n\n  # TODO(jevinjiang): support unaligned number of heads!\n  if not can_be_xla_fully_tiled(num_kv_heads, kv_packing):\n    raise ValueError(f\"Not implemented: {num_kv_heads=} can not be XLA fully tiled.\")\n  assert num_q_heads % num_kv_heads == 0\n  ratio = num_q_heads // num_kv_heads\n  # TODO(jevinjiang): we can choose smaller tiling for packed type if large\n  # second minor tiling is not on.\n  max_kv_tiling = 8 * kv_packing\n  min_kv_heads = max_kv_tiling if num_kv_heads % max_kv_tiling == 0 else num_kv_heads\n  min_q_heads = min_kv_heads * ratio\n  if can_be_xla_fully_tiled(min_q_heads, q_packing):\n    return min_q_heads, min_kv_heads\n  return num_q_heads, num_kv_heads",
        "analysis": {
            "module_type": "tpu_tiling_head_calculator",
            "purpose": "Calculates the minimum number of query and key/value heads per processing block that satisfies XLA tiling constraints for TPU execution.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Get data type packing factors for query and key/value dtypes using `get_dtype_packing`.",
                "Define and use an inner function `can_be_xla_fully_tiled` to check if a head count is compatible with XLA tiling rules based on its packing factor.",
                "Validate that the total number of KV heads is XLA-tileable; raise a ValueError if not.",
                "Assert that the number of query heads is a multiple of the number of KV heads.",
                "Calculate the ratio of query heads to key/value heads.",
                "Determine a potential minimum number of KV heads per block based on a maximum tiling size (8 * kv_packing), falling back to the total number of KV heads if not divisible.",
                "Calculate the corresponding minimum number of Q heads per block using the ratio.",
                "If the calculated minimum Q heads is also XLA-tileable, return the calculated minimums for Q and KV heads.",
                "Otherwise, fall back and return the total number of Q and KV heads."
            ],
            "output": {
                "shape": "A tuple of two integers: (min_q_heads_per_block, min_kv_heads_per_block)."
            },
            "dependencies": [
                "get_dtype_packing"
            ],
            "parameters": {
                "num_q_heads": "Total number of query heads.",
                "num_kv_heads": "Total number of key/value heads.",
                "q_dtype": "The JAX numpy data type for query tensors.",
                "kv_dtype": "The JAX numpy data type for key/value tensors."
            },
            "notes": [
                "This function is specifically designed to determine optimal block sizes for attention kernels running on TPUs.",
                "The tiling compatibility check is based on whether the number of heads, after accounting for data type packing, is 1, 2, 4, 8, or a multiple of 8.",
                "The code includes a TODO indicating that support for unaligned numbers of heads is not yet implemented."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/paged_attention_kernel_v2.py#ragged_paged_attention",
        "file_path": "src/MaxText/inference/paged_attention_kernel_v2.py",
        "code_block": "def ragged_paged_attention(\n    q: jax.Array,  # [max_num_batched_tokens, num_q_heads, head_dim]\n    # TODO(jevinjiang): create a write_to_kv_cache kernel!\n    k_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    v_pages: jax.Array,  # [total_num_pages, page_size, num_kv_heads, head_dim]\n    kv_lens: jax.Array,  # i32[max_num_seqs]\n    page_indices: jax.Array,  # i32[max_num_seqs, pages_per_seq]\n    cu_q_lens: jax.Array,  # i32[max_num_seqs + 1]\n    num_seqs: jax.Array,  # i32[1]\n    *,\n    sm_scale: float = 1.0,\n    mask_value: float = DEFAULT_MASK_VALUE,\n    num_kv_pages_per_block: int = 16,\n    num_queries_per_block: int = 128,\n    vmem_limit_bytes: int | None = None,\n):\n  \"\"\"Ragged paged attention that supports mixed prefill and decode.\n\n  Args:\n    q: concatenated all sequences' queries.\n    k_pages: paged K cache. Normally in HBM.\n    v_pages: paged V cache. Normally in HBM.\n    kv_lens: padded kv lengths. Only the first num_seqs values are valid.\n    page_indices: the first index indicates which page to use in the kv cache\n      for each sequence. Only the first num_seqs values are valid.\n    cu_q_lens: the cumulative sum of the effective query lengths. Similar to\n      kv_lens, only the first num_seqs+1 values are valid.\n    num_seqs: the dynamic number of sequences.\n    sm_scale: the softmax scale which will be applied to the Q@K^T.\n    mask_value: mask value for causal mask.\n    num_kv_pages_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    num_queries_per_block: number of kv pages to be processed in one flash\n      attention block in the pallas kernel.\n    vmem_limit_bytes: the vmem limit for the pallas kernel.\n\n  Returns:\n    The output of the attention.\n  \"\"\"\n  check_inputs_shapes(q, k_pages, v_pages, kv_lens, page_indices, cu_q_lens, num_seqs)\n  _, num_q_heads, head_dim = q.shape\n  _, page_size, num_kv_heads, _ = k_pages.shape\n  num_q_per_blk = num_queries_per_block\n  num_kv_pages_per_blk = num_kv_pages_per_block\n  num_q_heads_per_kv_head = num_q_heads // num_kv_heads\n  num_q_blks = ceil_div(cu_q_lens[num_seqs[0]], num_q_per_blk)\n  num_q_heads_per_blk, num_kv_heads_per_blk = get_min_heads_per_blk(num_q_heads, num_kv_heads, q.dtype, k_pages.dtype)\n  assert num_q_heads_per_blk % num_q_heads_per_kv_head == 0\n  num_heads_blks = num_q_heads // num_q_heads_per_blk\n  grid = (num_heads_blks, num_q_blks)\n\n  def q_index_map(heads_blk_idx, q_blk_idx, *_):\n    return (q_blk_idx, heads_blk_idx, 0)\n\n  q_block_spec = pl.BlockSpec(\n      (num_q_per_blk, num_q_heads_per_blk, head_dim),\n      q_index_map,\n  )\n  in_specs = [\n      q_block_spec,\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n      pl.BlockSpec(memory_space=pl.MemorySpace.ANY),\n  ]\n  out_specs = q_block_spec\n  lm_scratch = pltpu.VMEM(\n      # TODO(jevinjiang): use 128 instead of 1 is due to Mosaic does not support\n      # unaligned slicing!\n      (num_kv_heads_per_blk, num_q_per_blk * num_q_heads_per_kv_head, 128),\n      jnp.float32,\n  )\n  double_buf_scratch = pltpu.VMEM(\n      (\n          2,  # For double buffering during DMA copies.\n          num_kv_pages_per_blk,\n          page_size,\n          num_kv_heads_per_blk,\n          head_dim,\n      ),\n      k_pages.dtype,\n  )\n  scratch_shapes = [\n      double_buf_scratch,  # k_bufs\n      double_buf_scratch,  # v_bufs\n      pltpu.SemaphoreType.DMA((2, 2)),  # [double_buffers, k_sem/v_sem]\n      lm_scratch,  # l_ref\n      lm_scratch,  # m_ref\n  ]\n  scalar_prefetches = (\n      kv_lens,\n      page_indices,\n      cu_q_lens,\n      jnp.array((0, 0), jnp.int32),  # seq_idx, buf_idx\n      num_seqs,\n  )\n  kernel = pl.pallas_call(\n      functools.partial(\n          ragged_paged_attention_kernel,\n          sm_scale=sm_scale,\n          mask_value=mask_value,\n      ),\n      grid_spec=pltpu.PrefetchScalarGridSpec(\n          num_scalar_prefetch=len(scalar_prefetches),\n          in_specs=in_specs,\n          out_specs=out_specs,\n          grid=grid,\n          scratch_shapes=scratch_shapes,\n      ),\n      compiler_params=pltpu.CompilerParams(\n          dimension_semantics=(\n              \"arbitrary\",\n              \"arbitrary\",\n          ),\n          vmem_limit_bytes=vmem_limit_bytes,\n      ),\n      out_shape=jax.ShapeDtypeStruct(shape=q.shape, dtype=jnp.float32),\n      name=\"ragged_paged_attention_kernel\",\n  )\n  # TODO(jevinjiang): Use f32 acc scratch for output! So we only need\n  # to transfer output with desired dtype back to HBM.\n  return kernel(*scalar_prefetches, q, k_pages, v_pages).astype(q.dtype)",
        "analysis": {
            "functionality": "Performs ragged paged attention for mixed prefill and decode workloads by wrapping a JAX Pallas kernel. It handles the setup, configuration, and invocation of the underlying TPU-optimized kernel.",
            "usage": "Call this function with query tensors (`q`), paged key-value caches (`k_pages`, `v_pages`), and metadata tensors (`kv_lens`, `page_indices`, `cu_q_lens`, `num_seqs`) that describe the ragged batch. It returns the attention output tensor with the same shape and dtype as the input query tensor."
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#reverse_transpose",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def reverse_transpose(transposed_array, transpose_axis_order):\n  return jax.numpy.moveaxis(transposed_array, (0, 1, 2, 3), transpose_axis_order)",
        "analysis": {
            "module_type": "array_utility_function",
            "purpose": "Reverses a transpose operation on a 4D JAX array by moving its axes back to their original order.",
            "input": {
                "shape": "transposed_array: [dim0, dim1, dim2, dim3], transpose_axis_order: tuple of 4 integers.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `jax.numpy.moveaxis` to move the source axes (0, 1, 2, 3) to the destination positions specified by `transpose_axis_order`."
            ],
            "output": {
                "shape": "The input shape with dimensions reordered to reverse the original transposition."
            },
            "dependencies": [
                "jax.numpy.moveaxis"
            ],
            "parameters": {
                "transposed_array": "The input 4D JAX array that has been transposed.",
                "transpose_axis_order": "A tuple of integers representing the original axis permutation, which now serves as the destination for the current axes."
            },
            "notes": [
                "The function is hardcoded for 4-dimensional arrays as it specifies source axes as (0, 1, 2, 3).",
                "This function effectively inverts an operation like `jnp.transpose(original_array, transpose_axis_order)`."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#transpose_tuple",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def transpose_tuple(items: tuple[Any, ...], axis_order: AxisIdxes) -> tuple[Any, ...]:\n  return tuple((items[i] for i in axis_order))",
        "analysis": {
            "module_type": "utility_function",
            "purpose": "Reorders the elements of a tuple according to a specified sequence of indices.",
            "input": {
                "shape": "items: tuple[Any, ...], axis_order: tuple[int, ...]",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a generator that yields elements from the input 'items' tuple at the indices specified by 'axis_order'.",
                "Converts the generator into a new tuple."
            ],
            "output": {
                "shape": "A new tuple with elements reordered, with length equal to len(axis_order)."
            },
            "dependencies": [],
            "parameters": {
                "items": "The input tuple whose elements are to be reordered.",
                "axis_order": "A tuple of integers representing the desired order of indices from the 'items' tuple."
            },
            "notes": [
                "The type hint `AxisIdxes` is used for the `axis_order` parameter.",
                "This function is conceptually similar to transposing axes of a multi-dimensional array but for a flat tuple."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVQuant",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVQuant:\n  \"\"\"Class to configure quantization for KV cache.\"\"\"\n\n  axis_cfg = \"\"\n  dtype = None\n\n  def __init__(self, config: Config):\n    assert config.quantize_kvcache\n    self.axis_cfg = config.kv_quant_axis\n    self.dtype = self._get_dtype(config.kv_quant_dtype)\n\n  def _get_dtype(self, dtype_cfg: str):\n    if dtype_cfg == \"int4\":\n      return jnp.int4\n    if dtype_cfg == \"int8\":\n      return jnp.int8\n    if dtype_cfg == \"fp8\":\n      return jnp.float8_e4m3fn\n    raise ValueError(f\"Invalid kv_quant_dtype: {dtype_cfg}\")\n\n  def _get_max_axis(self, axis_names: AxisNames):\n    if self.axis_cfg == \"dkv\":\n      return axis_names.index(CACHE_KV)\n    if self.axis_cfg == \"heads_and_dkv\":\n      return (axis_names.index(CACHE_HEADS), axis_names.index(CACHE_KV))\n    raise ValueError(f\"Invalid KV quant axis cfg: {self.axis_cfg}\")\n\n  def quantize(self, kv: Array, axis_names: AxisNames):\n    \"\"\"Quantize key/values stored in kvcache.\"\"\"\n    assert self.axis_cfg, \"KV quant axis cannot be None\"\n    max_axis = self._get_max_axis(axis_names)\n    scale = jnp.max(jnp.abs(kv), axis=max_axis, keepdims=True)\n    if self.dtype == jnp.int8:\n      value = jnp.int8(jnp.rint(kv * (MAX_INT8 / scale)))\n      return value, scale\n    if self.dtype == jnp.int4:\n      value = jnp.int4(jnp.rint(kv * (MAX_INT4 / scale)))\n      return value, scale\n    if self.dtype == jnp.float8_e4m3fn:\n      value = jnp.float8_e4m3fn(kv * (E4M3_MAX / scale))\n      return value, scale\n    raise ValueError(f\"Invalid KV quant dtype:{self.dtype}.\")\n\n  def einsum_fn_with_rhs_qtensor(\n      self,\n      rhs_dequant_mode=None,\n      rhs_calibration_mode=None,\n      lhs_dequant_mode=None,\n      lhs_calibration_mode=None,\n  ):\n    \"\"\"einsum function where QTensor is the right-hand-side\"\"\"\n    # Assumes kv is already quantized.\n    einsum = jnp.einsum\n    if self.dtype != jnp.float8_e4m3fn:\n      num_bits = 4 if self.dtype == jnp.int4 else 8\n      kv_cfg = aqt_config.dot_general_make(\n          lhs_bits=None,\n          rhs_bits=num_bits,\n          bwd_bits=None,\n          use_fwd_quant=False,\n      )\n    else:\n      kv_cfg = aqt_config.config_fwd_fp8()\n\n    if rhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, rhs_dequant_mode=rhs_dequant_mode)\n    if rhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          rhs_calibration_mode=rhs_calibration_mode,\n      )\n    if lhs_dequant_mode:\n      aqt_config.set_fwd_dequant_mode(kv_cfg, lhs_dequant_mode=lhs_dequant_mode)\n    if lhs_calibration_mode:\n      aqt_config.set_fwd_calibration_mode(\n          kv_cfg,\n          lhs_calibration_mode=lhs_calibration_mode,\n      )\n    einsum = aqt_flax.AqtEinsum(\n        rhs_quant_mode=aqt_flax.QuantMode.TRAIN,\n        lhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        rhs_freeze_mode=aqt_flax.FreezerMode.NONE,\n        cfg=kv_cfg,\n    )\n    return einsum\n\n  def einsum_fn_with_rhs_qtensor_and_dequant(self):\n    \"\"\"Get einstein summation for different dequant modes.\"\"\"\n    if self.dtype == jnp.float8_e4m3fn:\n      return self.einsum_fn_with_rhs_qtensor(\n          lhs_dequant_mode=aqt_config.DequantMode.THIS_INPUT,\n          lhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )\n    else:\n      return self.einsum_fn_with_rhs_qtensor(\n          rhs_dequant_mode=aqt_config.DequantMode.OTHER_INPUT,\n          rhs_calibration_mode=aqt_config.CalibrationMode.REMAINING_AXIS,\n      )",
        "analysis": {
            "module_type": "kv_cache_quantization",
            "purpose": "A class to configure and apply quantization (int4, int8, or fp8) to the Key-Value cache in a transformer model.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes quantization settings (dtype, axis) from a configuration object.",
                "Provides a `quantize` method to convert a float tensor into a quantized tensor and a scale factor.",
                "Provides methods to generate a configured `AqtEinsum` function for performing attention with the quantized KV cache."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.numpy",
                "aqt.jax.v2.config",
                "aqt.jax.v2.flax",
                "MaxText.common_types.Config"
            ],
            "parameters": {
                "config.quantize_kvcache": "A boolean flag that must be true to instantiate this class.",
                "config.kv_quant_axis": "A string ('dkv' or 'heads_and_dkv') specifying the tensor axes to quantize over.",
                "config.kv_quant_dtype": "A string ('int4', 'int8', or 'fp8') specifying the target data type for quantization."
            },
            "notes": [
                "This is a helper class and does not inherit from a standard neural network module like `nn.Module`.",
                "It encapsulates the logic for symmetric quantization and the setup for AQT (Accurate Quantized Training) einsum operations."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the quantization configuration based on a `Config` object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "Config object"
                    },
                    "processing_steps": [
                        "Assert that `config.quantize_kvcache` is true.",
                        "Set `self.axis_cfg` from `config.kv_quant_axis`.",
                        "Call `self._get_dtype` with `config.kv_quant_dtype` to set `self.dtype`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "self._get_dtype"
                    ],
                    "notes": [
                        "This method ensures the class is only instantiated when KV cache quantization is enabled."
                    ]
                },
                "_get_dtype": {
                    "purpose": "Converts a string configuration for dtype into a `jnp` dtype object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "string"
                    },
                    "processing_steps": [
                        "Check if the input string is 'int4', 'int8', or 'fp8'.",
                        "Return the corresponding `jnp` dtype.",
                        "Raise a ValueError for invalid input."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jax.numpy"
                    ],
                    "notes": [
                        "Internal helper method."
                    ]
                },
                "_get_max_axis": {
                    "purpose": "Determines the axis or axes over which to compute the maximum absolute value for quantization scaling.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "AxisNames (tuple of strings)"
                    },
                    "processing_steps": [
                        "Check `self.axis_cfg`.",
                        "If 'dkv', return the index of the `CACHE_KV` axis.",
                        "If 'heads_and_dkv', return the indices of `CACHE_HEADS` and `CACHE_KV` axes.",
                        "Raise a ValueError for invalid configuration."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxText.common_types.CACHE_KV",
                        "MaxText.common_types.CACHE_HEADS"
                    ],
                    "notes": [
                        "Internal helper method."
                    ]
                },
                "quantize": {
                    "purpose": "Quantizes an input key/value tensor to the configured data type.",
                    "input": {
                        "shape": "[batch, sequence, heads, d_kv]",
                        "dtype": "float"
                    },
                    "processing_steps": [
                        "Determine the reduction axis/axes for scaling using `_get_max_axis`.",
                        "Calculate the scale by finding the maximum absolute value of the input tensor along the determined axis.",
                        "Scale the input tensor, round the values, and cast to the target dtype (int8, int4, or fp8).",
                        "Return the quantized tensor and the scale."
                    ],
                    "output": {
                        "shape": "A tuple containing the quantized tensor (same shape as input) and the scale tensor (shape depends on reduction axes)."
                    },
                    "dependencies": [
                        "jax.numpy",
                        "self._get_max_axis"
                    ],
                    "notes": [
                        "Implements symmetric quantization."
                    ]
                },
                "einsum_fn_with_rhs_qtensor": {
                    "purpose": "Creates and configures an `AqtEinsum` layer for dot-product operations where the right-hand side is a quantized tensor.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "Optional configuration arguments for dequantization and calibration modes."
                    },
                    "processing_steps": [
                        "Create an AQT configuration (`kv_cfg`) based on the quantization dtype.",
                        "Optionally, set dequantization and calibration modes on the configuration.",
                        "Instantiate and return `aqt_flax.AqtEinsum` with the configuration."
                    ],
                    "output": {
                        "shape": "An `aqt_flax.AqtEinsum` instance."
                    },
                    "dependencies": [
                        "aqt.jax.v2.config",
                        "aqt.jax.v2.flax.AqtEinsum"
                    ],
                    "notes": [
                        "This prepares the einsum operation that will handle on-the-fly dequantization of the KV cache during attention computation."
                    ]
                },
                "einsum_fn_with_rhs_qtensor_and_dequant": {
                    "purpose": "A convenience wrapper that provides default dequantization configurations for the `AqtEinsum` layer.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check the quantization dtype.",
                        "Call `self.einsum_fn_with_rhs_qtensor` with pre-defined dequantization and calibration modes suitable for the dtype."
                    ],
                    "output": {
                        "shape": "An `aqt_flax.AqtEinsum` instance."
                    },
                    "dependencies": [
                        "self.einsum_fn_with_rhs_qtensor",
                        "aqt.jax.v2.config"
                    ],
                    "notes": [
                        "Simplifies getting a correctly configured einsum function for attention with a quantized KV cache."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_heads: int,\n    value_heads: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n    cache_scale_logical_axis_names: AxisNames = (\n        CACHE_SCALE_BATCH,\n        CACHE_SCALE_SEQUENCE,\n        CACHE_SCALE_HEADS,\n        CACHE_SCALE_KV,\n    ),\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    key_axis_order: AxisIdxes = (2, 0, 1, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the KVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n    cache_logical_axis_names: The logical axis names for the cache.\n    cache_scale_logical_axis_names: The logical axis names for the cache scale.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    key_axis_order: The axis order for the key.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `KVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      KVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      kv_quant=kv_quant,\n      prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n      cache_logical_axis_names=cache_logical_axis_names,\n      cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      key_axis_order=key_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "kv_cache_factory",
            "purpose": "A factory function that initializes the KVCache NNX module with specified parameters and wraps it as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `nnx_wrappers.to_linen` to convert the `KVCache` NNX module into a Linen module, passing along all the configuration parameters."
            ],
            "output": {
                "shape": "Returns a Flax Linen module instance, not a tensor."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "KVCache",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "max_prefill_length": "The maximum number of tokens in the prefill phase.",
                "max_target_length": "The maximum total sequence length (prefill + generation).",
                "batch": "The batch size for the cache.",
                "key_heads": "The number of key attention heads.",
                "value_heads": "The number of value attention heads.",
                "key_head_size": "The dimension of each key head.",
                "value_head_size": "The dimension of each value head.",
                "dtype": "The data type for the cache tensors.",
                "kv_quant": "Optional configuration object for quantizing the key-value cache.",
                "model_mode": "The operational mode, typically 'prefill' or 'autoregressive'."
            },
            "notes": [
                "This function serves as a bridge to use the NNX-defined `KVCache` within a Linen-based model architecture.",
                "It passes `variable_to_logically_partitioned` as the `metadata_fn` to handle tensor sharding annotations during the conversion."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#KVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class KVCache(nnx.Module):\n  \"\"\"Implementation of the KVCache.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len, key_heads,\n      # and value_heads from key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_heads: int,\n      value_heads: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in KVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the KVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      model_mode: The model mode.\n      use_chunked_prefill: Whether to use chunked prefill.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    self.max_prefill_length = max_prefill_length\n    self.max_target_length = max_target_length\n    self.batch = batch\n    self.key_seq_len = key_seq_len\n    self.value_seq_len = value_seq_len\n    self.key_heads = key_heads\n    self.value_heads = value_heads\n    self.key_head_size = key_head_size\n    self.value_head_size = value_head_size\n    self.dtype = dtype\n    self.kv_quant = kv_quant\n    self.prefill_cache_logical_axis_names = prefill_cache_logical_axis_names\n    self.cache_logical_axis_names = cache_logical_axis_names\n    self.cache_scale_logical_axis_names = cache_scale_logical_axis_names\n    self.prefill_cache_axis_order = prefill_cache_axis_order\n    self.ar_cache_axis_order = ar_cache_axis_order\n    self.key_axis_order = key_axis_order\n    self.model_mode = model_mode\n    self.use_chunked_prefill = use_chunked_prefill\n\n    self._initialize_prefill_caches(model_mode)\n    self._initialize_ar_cache_vars(model_mode)\n\n  @property\n  def prefill_key_vars(self):\n    return (self.cached_prefill_key, self.cached_prefill_key_scale)\n\n  @property\n  def prefill_value_vars(self):\n    return (self.cached_prefill_value, self.cached_prefill_value_scale)\n\n  @property\n  def ar_key_vars(self):\n    return (self.cached_ar_key, self.cached_ar_key_scale)\n\n  @property\n  def ar_value_vars(self):\n    return (self.cached_ar_value, self.cached_ar_value_scale)\n\n  def _get_cached_kv_dtype(self):\n    return self.kv_quant.dtype if self.kv_quant else self.dtype\n\n  def _get_cache_scale_logical_shape(self, heads, cache_length):\n    assert self.kv_quant\n    if self.kv_quant.axis_cfg == \"dkv\":\n      return (self.batch, cache_length, heads, 1)\n    if self.kv_quant.axis_cfg == \"heads_and_dkv\":\n      return (self.batch, cache_length, 1, 1)\n    raise ValueError(f\"Invalid config for kv_quant_axis:{self.kv_quant.axis_cfg}\")\n\n  def _initialize_prefill_caches(self, model_mode):\n    \"\"\"Get a shaped abstraction of the state\"\"\"\n\n    cache_length = self.max_prefill_length\n    dtype = self._get_cached_kv_dtype()\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.prefill_cache_axis_order)\n\n    self.cached_prefill_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_prefill_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n\n    self.cache_prefill_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.prefill_cache_axis_order)\n\n      self.cached_prefill_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_prefill_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_prefill_key_scale = None\n      self.cached_prefill_value_scale = None\n\n  def _get_prefill_cache_vars(self):\n    return self.prefill_key_vars, self.prefill_value_vars, self.cache_prefill_segment_id\n\n  def _initialize_ar_cache_vars(self, model_mode):\n    \"\"\"get ar cache vars\"\"\"\n\n    dtype = self._get_cached_kv_dtype()\n    if self.max_target_length <= self.max_prefill_length:\n      raise ValueError(\n          f\"max_target_length: {self.max_target_length} should be greater than max_prefill_length:\"\n          f\" {self.max_prefill_length}!\"\n      )\n    cache_length = self.max_target_length - self.max_prefill_length\n\n    if model_mode == MODEL_MODE_PREFILL:\n      cache_logical_axis_names = self.prefill_cache_logical_axis_names\n    else:\n      cache_logical_axis_names = self.cache_logical_axis_names\n    cache_axis_names = transpose_tuple(cache_logical_axis_names, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.key_heads, self.key_head_size)\n    cache_shape_key = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    cache_logical_shape = (self.batch, cache_length, self.value_heads, self.value_head_size)\n    cache_shape_value = transpose_tuple(cache_logical_shape, self.ar_cache_axis_order)\n\n    # TODO(b/339703100): investigate the issue why with_logical_partitioning doesn't enforce sharding\n    self.cached_ar_key = nnx.Cache(\n        jnp.zeros(cache_shape_key, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_key.value = nn.with_logical_constraint(\n        self.cached_ar_key.value,\n        cache_axis_names,\n    )\n\n    self.cached_ar_value = nnx.Cache(\n        jnp.zeros(cache_shape_value, dtype=dtype),\n        sharding=cache_axis_names,\n    )\n    self.cached_ar_value.value = nn.with_logical_constraint(\n        self.cached_ar_value.value,\n        cache_axis_names,\n    )\n\n    if model_mode == MODEL_MODE_PREFILL:\n      segment_id_axis_names = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE)\n    else:\n      segment_id_axis_names = (CACHE_BATCH, CACHE_SEQUENCE)\n    self.cache_ar_segment_id = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0], cache_length), dtype=jnp.int32),\n        sharding=segment_id_axis_names,\n    )\n\n    self.cached_ar_lengths = nnx.Cache(\n        jnp.zeros((cache_logical_shape[0],), dtype=jnp.int32),\n        sharding=(CACHE_BATCH,),\n    )\n\n    if self.kv_quant:\n      cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.key_heads, cache_length)\n      cache_key_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      cache_scale_logical_shape = self._get_cache_scale_logical_shape(self.value_heads, cache_length)\n      cache_value_scale_shape = transpose_tuple(cache_scale_logical_shape, self.ar_cache_axis_order)\n\n      self.cached_ar_key_scale = nnx.Cache(\n          jnp.zeros(cache_key_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n      self.cached_ar_value_scale = nnx.Cache(\n          jnp.zeros(cache_value_scale_shape, dtype=jnp.bfloat16),\n          sharding=cache_scale_axis_names,\n      )\n    else:\n      self.cached_ar_key_scale = None\n      self.cached_ar_value_scale = None\n\n    self.cache_ar_index = nnx.Cache(\n        jnp.zeros((1,), dtype=jnp.int32),\n        sharding=(),\n    )\n\n  def _get_ar_cache_vars(self):\n    return self.ar_key_vars, self.ar_value_vars, self.cache_ar_segment_id, self.cache_ar_index, self.cached_ar_lengths\n\n  def kv_cache_chunked_prefill(\n      self, key: Array, value: Array, decoder_segment_ids: Array, previous_chunk: None | Array = None\n  ):\n    \"\"\"Update the current kv cache into previous chunk and return needed length.\n\n    The previous chunk kv cache should be in the model's param.\n\n    Prefill cache need to be max prefill length to prevent different shape of kv cache.\n    Different shape of kv cache in previous chunk could produce different compiled graph.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n      previous_chunk:\n        In shape [b, s]. The tokens without padding in previous chunk.\n        Use to preserve the previous kv cache.\n\n    Returns:\n      key, value, decoder_segment_id.\n    \"\"\"\n\n    assert not self.kv_quant, \"Not support kv_quant now.\"\n    if decoder_segment_ids is not None:\n      self.batch, segment_id_seq_len = decoder_segment_ids.shape\n      assert self.key_seq_len == segment_id_seq_len, f\"{self.key_seq_len=}, {segment_id_seq_len=} should match.\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n    assert self.key_seq_len == self.value_seq_len, f\"{self.key_seq_len=}, {self.value_seq_len=} should match.\"\n\n    next_pos = 0\n    if previous_chunk is not None:\n      # We only have 1 prompt in prefill mode.\n      next_pos = previous_chunk.shape[1]\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n    # TODO: Find a way to not enable the ar cache for prefill mode.\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    # For quantized kv cached. Could be get without transpose twice.\n    cached_key = self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order)\n    cached_value = self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order)\n    cached_key_value = jnp.transpose(cached_key, self.prefill_cache_axis_order)\n    cached_value_value = jnp.transpose(cached_value, self.prefill_cache_axis_order)\n\n    seq_axis = self.prefill_cache_logical_axis_names.index(CACHE_SEQUENCE)\n    cache_seq_axis = self.prefill_cache_axis_order.index(seq_axis)\n\n    assert next_pos + key_shaped_for_cache.shape[cache_seq_axis] <= self.max_prefill_length, (\n        f\"Previous kv cache[{next_pos}] + \"\n        f\"current kv cache[{key_shaped_for_cache.shape[cache_seq_axis]}] \"\n        f\"> max length[{self.max_prefill_length}]\"\n    )\n\n    # We don't zero out remain values. Use segment id to mask out.\n    cached_prefill_key_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_key_value, key_shaped_for_cache, next_pos, cache_seq_axis\n    )\n    cached_prefill_value_vars[0].value = jax.lax.dynamic_update_slice_in_dim(\n        cached_value_value, value_shaped_for_cache, next_pos, cache_seq_axis\n    )\n\n    if decoder_segment_ids is not None:\n      # Need zero out the remain values to prevent wrong mask in autoregressive.\n      previous_segment_id = cached_prefill_segment_id_var.value[:, :next_pos]\n      cached_prefill_segment_id_var.value = jnp.zeros_like(cached_prefill_segment_id_var.value, dtype=jnp.int32)\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, previous_segment_id, start_index=0, axis=1\n      )\n      cached_prefill_segment_id_var.value = jax.lax.dynamic_update_slice_in_dim(\n          cached_prefill_segment_id_var.value, decoder_segment_ids, next_pos, axis=1\n      )\n\n    # Return needed kv cache to reduce computation of attention.\n    needed_prefill_key_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_key_vars[0].value, start_index=0, slice_size=(next_pos + self.key_seq_len), axis=cache_seq_axis\n    )\n    needed_prefill_value_value = jax.lax.dynamic_slice_in_dim(\n        cached_prefill_value_vars[0].value, start_index=0, slice_size=(next_pos + self.value_seq_len), axis=cache_seq_axis\n    )\n    needed_segment_id = None\n    if decoder_segment_ids is not None:\n      needed_segment_id = jax.lax.dynamic_slice_in_dim(\n          cached_prefill_segment_id_var.value, start_index=0, slice_size=(next_pos + segment_id_seq_len), axis=1\n      )\n\n    return (\n        jnp.transpose(needed_prefill_key_value, self.key_axis_order),\n        jnp.transpose(needed_prefill_value_value, self.key_axis_order),\n        needed_segment_id,\n    )\n\n  def kv_cache_prefill(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n  ):\n    \"\"\"In prefill mode, we zero out the existing cache, run the computation and\n    prepare the cache as necessary.\n\n    Args:\n      key: in shape [b, s, n, d].\n      value: in shape [b, s, n, d].\n      decoder_segment_ids: [b, s] -- marking segment ids for tokens\n\n    Returns:\n      key, value, decoder_segment_id.\n\n    \"\"\"\n\n    assert key.dtype == value.dtype, \"Key and Value Dtypes should match.\"\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    key_shaped_for_cache = jnp.transpose(key, self.prefill_cache_axis_order)\n    value_shaped_for_cache = jnp.transpose(value, self.prefill_cache_axis_order)\n\n    if self.kv_quant:\n      prefill_key_axis_names = transpose_tuple(self.cache_logical_axis_names, self.prefill_cache_axis_order)\n      key_shaped_for_cache, key_scale_shaped_for_cache = self.kv_quant.quantize(\n          key_shaped_for_cache, prefill_key_axis_names\n      )\n      value_shaped_for_cache, value_scale_shaped_for_cache = self.kv_quant.quantize(\n          value_shaped_for_cache, prefill_key_axis_names\n      )\n      cached_prefill_key_vars[1].value = key_scale_shaped_for_cache\n      cached_prefill_value_vars[1].value = value_scale_shaped_for_cache\n\n    cached_prefill_key_vars[0].value = key_shaped_for_cache\n    cached_prefill_value_vars[0].value = value_shaped_for_cache\n\n    if decoder_segment_ids is not None:\n      cached_prefill_segment_id_var.value = decoder_segment_ids\n    return key, value, decoder_segment_ids\n\n  def update_ar_key_value(\n      self,\n      one_token_key: Array,\n      one_token_value: Array,\n      key_caches: tuple[nnx.Cache, nnx.Cache | None],\n      value_caches: tuple[nnx.Cache, nnx.Cache | None],\n      one_hot_indices: Array,\n      lengths: Array,\n      use_ragged_attention: bool,\n  ) -> None:\n    \"\"\"Adds a single token's results to the ar kv cache\n\n    Args:\n        one_token_key (Array): Key of one token to add to the cache\n        one_token_value (Array): Value of one token to add to the cache\n        cached_ar_key (tuple[nnx.Cache, nnx.Cache|None],): Cached keys to add new token key to, possibly with scale\n        cached_ar_value (tuple[nnx.Cache, nnx.Cache|None],: Cached values to add new token value to, possible with scale\n        one_hot_indices (Array): Location of the new token within the cache\n\n    Returns:\n        tuple[Array, Array]: Updated caches for key and value with new token info added\n    \"\"\"\n\n    cached_key, cached_key_scale = key_caches\n    cached_value, cached_value_scale = value_caches\n\n    # In order to update the key, value caches with the current key and\n    # value, we reshape the one_token_key and one_token_value\n    one_token_key_shaped_for_cache = jnp.transpose(one_token_key, self.ar_cache_axis_order)\n    one_token_value_shaped_for_cache = jnp.transpose(one_token_value, self.ar_cache_axis_order)\n\n    ar_cache_axis_names = transpose_tuple(self.cache_logical_axis_names, self.ar_cache_axis_order)\n    if self.kv_quant:\n      one_token_key_shaped_for_cache, one_token_key_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_key_shaped_for_cache, ar_cache_axis_names\n      )\n      one_token_value_shaped_for_cache, one_token_value_scale_shaped_for_cache = self.kv_quant.quantize(\n          one_token_value_shaped_for_cache, ar_cache_axis_names\n      )\n\n    ar_cache_update_idx = jnp.squeeze(one_hot_indices)\n    ar_cache_sequence_axis = ar_cache_update_axis = ar_cache_axis_names.index(CACHE_SEQUENCE)\n    ar_cache_batch_axis = ar_cache_axis_names.index(CACHE_BATCH)\n\n    if use_ragged_attention:\n      cache_locations = [slice(None)] * 4\n      new_token_locations = [slice(None)] * 4\n      new_token_locations[ar_cache_sequence_axis] = 0\n\n      def key_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_key_shaped_for_cache[tuple(new_token_locations)])\n\n      def value_body(i, val):\n        cache_locations[ar_cache_batch_axis] = i\n        cache_locations[ar_cache_sequence_axis] = lengths[i]\n        new_token_locations[ar_cache_batch_axis] = i\n        return val.at[tuple(cache_locations)].set(one_token_value_shaped_for_cache[tuple(new_token_locations)])\n\n      cached_key.value = jax.lax.fori_loop(\n          0, one_token_key_shaped_for_cache.shape[0], key_body, cached_key.value, unroll=8\n      )\n      cached_value.value = jax.lax.fori_loop(\n          0, one_token_value_shaped_for_cache.shape[0], value_body, cached_value.value, unroll=8\n      )\n\n    else:\n      one_hot_indices = one_hot_indices.astype(int)\n      cached_key.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key.value, one_token_key_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n      cached_value.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value.value, one_token_value_shaped_for_cache, ar_cache_update_idx, ar_cache_update_axis\n      )\n    cached_key.value = nn.with_logical_constraint(cached_key.value, ar_cache_axis_names)\n    cached_value.value = nn.with_logical_constraint(cached_value.value, ar_cache_axis_names)\n\n    if self.kv_quant:\n      ar_cache_scale_axis_names = transpose_tuple(self.cache_scale_logical_axis_names, self.ar_cache_axis_order)\n      ar_cache_scale_update_axis = ar_cache_scale_axis_names.index(CACHE_SCALE_SEQUENCE)\n      assert cached_key_scale is not None, \"cached_key_scale_var cannot be None\"\n      assert cached_value_scale is not None, \"cached_value_scale_var cannot be None\"\n      cached_key_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_key_scale.value, one_token_key_scale_shaped_for_cache, ar_cache_update_idx, ar_cache_scale_update_axis\n      )\n      cached_value_scale.value = jax.lax.dynamic_update_index_in_dim(\n          cached_value_scale.value,\n          one_token_value_scale_shaped_for_cache,\n          ar_cache_update_idx,\n          ar_cache_scale_update_axis,\n      )\n\n  def get_cached_values(self, cache_vars, target_dtype, cache_axis_order) -> jax.Array | KVTensor:\n    \"\"\"get cached values\"\"\"\n    cache_var, cache_scale_var = cache_vars\n    cache_value = cache_var.value\n    if cache_scale_var is not None:\n      scale_value = cache_scale_var.value\n      dtype = cache_value.dtype\n      if dtype == jnp.int8:\n        scale_value /= MAX_INT8\n      elif dtype == jnp.int4:\n        scale_value /= MAX_INT4\n      elif dtype == jnp.float8_e4m3fn:\n        scale_value /= E4M3_MAX\n\n      cache_value = KVTensor(qvalue=cache_value, scale=[scale_value], scale_t=None, dequant_dtype=target_dtype, bias=[])\n    cache_value_in_logical_shape = jax.tree.map(lambda x: reverse_transpose(x, cache_axis_order), cache_value)\n    return cache_value_in_logical_shape\n\n  def kv_cache_autoregressive(\n      self,\n      key: Array,\n      value: Array,\n      use_ragged_attention: bool = False,\n  ):\n    \"\"\"In autoregressive mode, we update the cache for this entry and\n       then return the full cache.\n\n    Args:\n      key: in shape [b, 1, n, d].\n      value: in shape [b, 1, n, d].\n      decoder_segment_ids: [b, 1] -- marking segment ids for tokens\n\n    Returns:\n      tuple of (key, value, segment_id) for both prefill and ar cache,\n    Raises:\n      ValueError: when key/value shape is not [batch, 1, num_heads, heads_dim].\n    \"\"\"\n    _, sequence, _, _ = value.shape\n    if sequence != 1:\n      raise ValueError(f\"Sequence length should be 1 during autoregression, got {sequence=}\")\n\n    cached_ar_key_vars, cached_ar_value_vars, cached_ar_segment_id_var, cache_ar_index_var, cache_ar_lengths_var = (\n        self._get_ar_cache_vars()\n    )\n\n    self.update_ar_key_value(\n        key,\n        value,\n        cached_ar_key_vars,\n        cached_ar_value_vars,\n        cache_ar_index_var.value,\n        cache_ar_lengths_var.value,\n        use_ragged_attention,\n    )\n    active_indicator = jnp.zeros((self.batch, 1), dtype=jnp.int32) + DECODING_ACTIVE_SEQUENCE_INDICATOR\n    cached_ar_segment_id_var.value = jax.lax.dynamic_update_index_in_dim(\n        cached_ar_segment_id_var.value, active_indicator, jnp.squeeze(cache_ar_index_var.value), 1\n    )\n    cache_ar_index_var.value = jnp.mod(cache_ar_index_var.value + 1, self.max_target_length - self.max_prefill_length)\n    cache_ar_lengths_var.value = cache_ar_lengths_var.value.at[:].add(1)\n\n    cached_prefill_key_vars, cached_prefill_value_vars, cached_prefill_segment_id_var = self._get_prefill_cache_vars()\n\n    cached_prefill = (\n        self.get_cached_values(cached_prefill_key_vars, key.dtype, self.prefill_cache_axis_order),\n        self.get_cached_values(cached_prefill_value_vars, value.dtype, self.prefill_cache_axis_order),\n        cached_prefill_segment_id_var.value,\n    )\n\n    cached_ar = (\n        self.get_cached_values(cached_ar_key_vars, key.dtype, self.ar_cache_axis_order),\n        self.get_cached_values(cached_ar_value_vars, value.dtype, self.ar_cache_axis_order),\n        cached_ar_segment_id_var.value,\n        cache_ar_lengths_var.value,\n    )\n    return cached_prefill, cached_ar\n\n  def __call__(\n      self,\n      key: Array,\n      value: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple:\n    \"\"\"KV cache takes the current state and updates the state accordingly.\n\n    The key and value have dimension [b, s, n_kv, d],\n    but we cache them with a reshape as defined in *_axis_order config as a TPU\n    fusion optimization. This also enables the \"scatter via one-hot\n    broadcast\" trick, which means we do a one-hot broadcast instead of a\n    scatter/gather operations, resulting in a 3-4x speedup in practice.\n\n    Args:\n      key: in shape [b, s, n_kv, d].\n      value: in shape [b, s, n_kv, d].\n      model_mode: model mode controlling model.\n\n    Returns:\n      two tuples of (k, v, decoder_segments) -- either can be Nones\n\n    \"\"\"\n    if model_mode == MODEL_MODE_PREFILL:\n      if self.use_chunked_prefill:\n        return self.kv_cache_chunked_prefill(key, value, decoder_segment_ids, previous_chunk), None\n      else:\n        return self.kv_cache_prefill(key, value, decoder_segment_ids), None\n    elif model_mode == MODEL_MODE_AUTOREGRESSIVE:\n      return self.kv_cache_autoregressive(key, value, use_ragged_attention)\n    else:\n      raise ValueError(f\"Model Mode isn't supported! {model_mode=}\")",
        "analysis": {
            "module_type": "key_value_cache",
            "purpose": "Manages the key-value cache for attention mechanisms, supporting both prefill and autoregressive decoding phases, with optional quantization.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes separate caches for prefill and autoregressive (AR) decoding during `__init__`.",
                "The `__call__` method routes to the appropriate caching logic based on the `model_mode`.",
                "For `MODEL_MODE_PREFILL`, it populates the prefill cache using `kv_cache_prefill` or `kv_cache_chunked_prefill`.",
                "For `MODEL_MODE_AUTOREGRESSIVE`, it updates the AR cache with the new token's key/value and returns the combined prefill and AR caches using `kv_cache_autoregressive`."
            ],
            "output": {
                "shape": "A tuple of two items: `(prefill_cache, ar_cache)`. One of these will be `None` depending on the `model_mode`. The other will be a tuple containing the cached key, value, and segment IDs."
            },
            "dependencies": [
                "flax.nnx",
                "jax",
                "MaxText.layers.kv_cache.KVQuant"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum total sequence length for autoregressive decoding.",
                "batch": "The batch size.",
                "key_heads": "The number of key heads.",
                "value_heads": "The number of value heads.",
                "key_head_size": "The dimension of each key head.",
                "value_head_size": "The dimension of each value head.",
                "dtype": "The data type for the cache tensors.",
                "kv_quant": "An optional configuration object for quantizing the key-value cache.",
                "model_mode": "Specifies the operating mode, typically 'prefill' or 'autoregressive'.",
                "use_chunked_prefill": "A boolean indicating whether to use chunked prefilling for long sequences."
            },
            "notes": [
                "The class maintains separate cache tensors for the prefill and autoregressive phases to handle their different characteristics.",
                "It uses `nnx.Cache` to manage state, which is suitable for Flax's functional programming model.",
                "The cache tensors are often transposed based on `*_axis_order` configurations for performance optimization on TPUs."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the KVCache module, setting up dimensions and initializing prefill and autoregressive caches.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like max lengths, batch size, and head dimensions.",
                        "Call `_initialize_prefill_caches` to set up the cache for the prefill phase.",
                        "Call `_initialize_ar_cache_vars` to set up the cache for the autoregressive phase."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "self._initialize_prefill_caches",
                        "self._initialize_ar_cache_vars"
                    ],
                    "notes": [
                        "The `rngs` argument is a placeholder for compatibility with `nnx_wrappers.to_linen` and is not used by the KVCache itself."
                    ]
                },
                "kv_cache_chunked_prefill": {
                    "purpose": "Updates the prefill cache for a chunk of a long sequence, appending it to any previous chunks.",
                    "input": {
                        "shape": "key/value: [batch, seq_len, num_heads, head_dim], decoder_segment_ids: [batch, seq_len], previous_chunk: [batch, prev_seq_len]",
                        "dtype": "key/value: DType, decoder_segment_ids: int32, previous_chunk: Array"
                    },
                    "processing_steps": [
                        "Determine the starting position based on the length of `previous_chunk`.",
                        "Transpose input key and value to match the cache's physical layout.",
                        "Update the prefill cache tensors by slicing in the new chunk's data.",
                        "Update the segment ID cache.",
                        "Slice the updated cache to return only the portion containing valid data.",
                        "Transpose the sliced key and value back to the logical layout."
                    ],
                    "output": {
                        "shape": "A tuple `(key, value, segment_id)` with a sequence length equal to the combined length of previous and current chunks."
                    },
                    "dependencies": [
                        "jax.lax.dynamic_update_slice_in_dim",
                        "jax.lax.dynamic_slice_in_dim",
                        "jnp.transpose"
                    ],
                    "notes": [
                        "This method is used when `use_chunked_prefill` is True.",
                        "It currently asserts that KV quantization is not supported in this mode."
                    ]
                },
                "kv_cache_prefill": {
                    "purpose": "Populates the prefill cache with the initial key-value states, overwriting any existing data.",
                    "input": {
                        "shape": "key/value: [batch, seq_len, num_heads, head_dim], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "key/value: DType, decoder_segment_ids: int32"
                    },
                    "processing_steps": [
                        "Transpose the input key and value to match the cache's physical layout.",
                        "If quantization is enabled, quantize the tensors and store both the quantized values and scales.",
                        "Store the transposed (and possibly quantized) key and value in the prefill cache variables.",
                        "Store the `decoder_segment_ids`."
                    ],
                    "output": {
                        "shape": "Returns the original `(key, value, decoder_segment_ids)` tuple."
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "self.kv_quant.quantize"
                    ],
                    "notes": []
                },
                "update_ar_key_value": {
                    "purpose": "Adds a single token's key and value to the autoregressive (AR) cache.",
                    "input": {
                        "shape": "one_token_key/value: [batch, 1, num_heads, head_dim], one_hot_indices: [1,], lengths: [batch,]",
                        "dtype": "key/value: DType, indices/lengths: int32"
                    },
                    "processing_steps": [
                        "Transpose the single-token key and value to match the cache layout.",
                        "If quantization is enabled, quantize the key and value.",
                        "Update the cache at the specified index using either `jax.lax.fori_loop` (for ragged attention) or `jax.lax.dynamic_update_index_in_dim`.",
                        "Apply logical constraints to the updated cache tensors for sharding.",
                        "If quantization is enabled, update the scale caches as well."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "jnp.transpose",
                        "jax.lax.dynamic_update_index_in_dim",
                        "jax.lax.fori_loop",
                        "nn.with_logical_constraint"
                    ],
                    "notes": [
                        "This method modifies the cache variables passed into it directly."
                    ]
                },
                "get_cached_values": {
                    "purpose": "Retrieves values from a cache, dequantizing them if necessary, and transposing them to the logical shape.",
                    "input": {
                        "shape": "cache_vars: a tuple of (nnx.Cache, nnx.Cache | None)",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Unpack the cache and optional scale variables.",
                        "If a scale variable exists, wrap the data in a `KVTensor` to represent the quantized state.",
                        "Apply a reverse transpose to the cached value to restore its logical shape."
                    ],
                    "output": {
                        "shape": "A `jax.Array` or `KVTensor` with the logical shape, e.g., `[batch, sequence_length, num_heads, head_dim]`."
                    },
                    "dependencies": [
                        "MaxText.layers.kv_cache.KVTensor",
                        "MaxText.layers.kv_cache.reverse_transpose"
                    ],
                    "notes": []
                },
                "kv_cache_autoregressive": {
                    "purpose": "Updates the autoregressive cache with a new token and returns the full combined (prefill + AR) cache.",
                    "input": {
                        "shape": "key/value: [batch, 1, num_heads, head_dim]",
                        "dtype": "DType"
                    },
                    "processing_steps": [
                        "Validate that the input sequence length is 1.",
                        "Call `update_ar_key_value` to insert the new key/value into the AR cache.",
                        "Update the AR segment ID cache to mark the new token as active.",
                        "Increment the AR cache index and sequence lengths.",
                        "Call `get_cached_values` for both the prefill and AR caches to retrieve and dequantize them.",
                        "Return the retrieved prefill and AR cache contents as separate tuples."
                    ],
                    "output": {
                        "shape": "A tuple of two tuples: `(cached_prefill, cached_ar)`, where each contains `(key, value, segment_id, ...)`."
                    },
                    "dependencies": [
                        "self.update_ar_key_value",
                        "self.get_cached_values"
                    ],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Main entry point that routes to the correct caching logic based on `model_mode`.",
                    "input": {
                        "shape": "key/value: [batch, seq_len, num_kv_heads, head_dim], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "key/value: DType, decoder_segment_ids: int32"
                    },
                    "processing_steps": [
                        "Check `model_mode`.",
                        "If 'prefill', call `kv_cache_prefill` or `kv_cache_chunked_prefill`.",
                        "If 'autoregressive', call `kv_cache_autoregressive`.",
                        "Raise an error for unsupported modes."
                    ],
                    "output": {
                        "shape": "A tuple `(prefill_cache, ar_cache)`, where one element is a tuple of tensors and the other is `None`."
                    },
                    "dependencies": [
                        "self.kv_cache_chunked_prefill",
                        "self.kv_cache_prefill",
                        "self.kv_cache_autoregressive"
                    ],
                    "notes": [
                        "This method orchestrates the different caching strategies."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#mla_kv_cache_as_linen",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "def mla_kv_cache_as_linen(\n    *,\n    max_prefill_length: int,\n    max_target_length: int,\n    batch: int,\n    key_seq_len: int,\n    value_seq_len: int,\n    key_head_size: int,\n    value_head_size: int,\n    dtype: DType,\n    key_heads: int = 1,\n    value_heads: int = 1,\n    kv_quant: None | KVQuant = None,\n    prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n    use_chunked_prefill: bool = False,\n    model_mode: str = MODEL_MODE_PREFILL,\n    name: str | None = None,\n):\n  \"\"\"Initializes the MlaKVCache module and returns it as a Linen module.\n\n  Args:\n    max_prefill_length: The maximum prefill length.\n    max_target_length: The maximum target length.\n    batch: The batch size.\n    key_seq_len: The key sequence length.\n    value_seq_len: The value sequence length.\n    key_head_size: The key head size.\n    value_head_size: The value head size.\n    dtype: The data type.\n    key_heads: The number of key heads.\n    value_heads: The number of value heads.\n    kv_quant: The KVQuant configuration.\n    prefill_cache_axis_order: The axis order for the prefill cache.\n    ar_cache_axis_order: The axis order for the autoregressive cache.\n    use_chunked_prefill: Whether to use chunked prefill.\n    model_mode: The model mode.\n    name: The name of the Linen module.\n\n  Returns:\n    A Linen module that wraps the NNX `MlaKVCache` module.\n  \"\"\"\n  return nnx_wrappers.to_linen(\n      MlaKVCache,\n      max_prefill_length=max_prefill_length,\n      max_target_length=max_target_length,\n      batch=batch,\n      key_seq_len=key_seq_len,\n      value_seq_len=value_seq_len,\n      key_head_size=key_head_size,\n      value_head_size=value_head_size,\n      dtype=dtype,\n      key_heads=key_heads,\n      value_heads=value_heads,\n      kv_quant=kv_quant,\n      prefill_cache_axis_order=prefill_cache_axis_order,\n      ar_cache_axis_order=ar_cache_axis_order,\n      use_chunked_prefill=use_chunked_prefill,\n      model_mode=model_mode,\n      metadata_fn=variable_to_logically_partitioned,\n      name=name,\n      abstract_init=False,\n  )",
        "analysis": {
            "module_type": "mla_kv_cache_factory",
            "purpose": "A factory function that initializes an NNX MlaKVCache module and wraps it as a Flax Linen module.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Accepts configuration parameters for the MlaKVCache such as max lengths, batch size, head dimensions, and data type.",
                "Calls `nnx_wrappers.to_linen` to convert the `MlaKVCache` NNX module into a Flax Linen module.",
                "Passes all configuration parameters, along with a `metadata_fn` for logical partitioning, to the wrapper function."
            ],
            "output": {
                "shape": "Returns a Flax Linen Module instance, not a tensor. Shape is N/A."
            },
            "dependencies": [
                "nnx_wrappers.to_linen",
                "MlaKVCache",
                "variable_to_logically_partitioned"
            ],
            "parameters": {
                "max_prefill_length": "The maximum sequence length for the prefill phase.",
                "max_target_length": "The maximum total sequence length (prefill + generation) for the cache.",
                "batch": "The batch size for the cache.",
                "model_mode": "Specifies the operational mode, typically 'prefill' or 'autoregressive'.",
                "kv_quant": "Optional configuration object for quantizing the key-value cache."
            },
            "notes": [
                "This function serves as a bridge to use an NNX-defined module (`MlaKVCache`) within a Flax Linen-based model architecture.",
                "The `metadata_fn` argument is used to apply logical partitioning rules to the cache variables for distributed computation.",
                "The `abstract_init=False` argument ensures that the module's variables are concretely initialized."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/kvcache.py#MlaKVCache",
        "file_path": "src/MaxText/inference/kvcache.py",
        "code_block": "class MlaKVCache(KVCache):\n  \"\"\"Implementation of the KVCache for MLA.\"\"\"\n\n  def __init__(\n      self,\n      max_prefill_length: int,\n      max_target_length: int,\n      # TODO(bvandermoon): Can we get batch, key_seq_len, value_seq_len,\n      # key_head_size, value_head_size, key_heads, and value_heads from\n      # key/value after migrating Attention to NNX?\n      batch: int,\n      key_seq_len: int,\n      value_seq_len: int,\n      key_head_size: int,\n      value_head_size: int,\n      dtype: DType,\n      key_heads: int = 1,\n      value_heads: int = 1,\n      kv_quant: None | KVQuant = None,\n      prefill_cache_logical_axis_names: AxisNames = (CACHE_BATCH_PREFILL, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_logical_axis_names: AxisNames = (CACHE_BATCH, CACHE_SEQUENCE, CACHE_HEADS_NONE, CACHE_KV),\n      cache_scale_logical_axis_names: AxisNames = (\n          CACHE_SCALE_BATCH,\n          CACHE_SCALE_SEQUENCE,\n          CACHE_SCALE_HEADS,\n          CACHE_SCALE_KV,\n      ),\n      prefill_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      ar_cache_axis_order: AxisIdxes = (1, 2, 0, 3),\n      key_axis_order: AxisIdxes = (2, 0, 1, 3),\n      use_chunked_prefill: bool = False,\n      model_mode: str = MODEL_MODE_PREFILL,\n      *,\n      # Not used in MlaKVCache but passed in by nnx_wrappers.to_linen.\n      # TODO: Remove when bridge no longer needed\n      rngs: nnx.Rngs = None,\n  ):\n    \"\"\"Initializes the MlaKVCache module.\n\n    Args:\n      max_prefill_length: The maximum prefill length.\n      max_target_length: The maximum target length.\n      batch: The batch size.\n      key_seq_len: The key sequence length.\n      value_seq_len: The value sequence length.\n      key_head_size: The key head size.\n      value_head_size: The value head size.\n      dtype: The data type.\n      key_heads: The number of key heads.\n      value_heads: The number of value heads.\n      kv_quant: The KVQuant configuration.\n      prefill_cache_logical_axis_names: The logical axis names for the prefill\n        cache.\n      cache_logical_axis_names: The logical axis names for the cache.\n      cache_scale_logical_axis_names: The logical axis names for the cache\n        scale.\n      prefill_cache_axis_order: The axis order for the prefill cache.\n      ar_cache_axis_order: The axis order for the autoregressive cache.\n      key_axis_order: The axis order for the key.\n      use_chunked_prefill: Whether to use chunked prefill.\n      model_mode: The model mode.\n      rngs: The random number generators for initialization.\n    \"\"\"\n    super().__init__(\n        max_prefill_length=max_prefill_length,\n        max_target_length=max_target_length,\n        batch=batch,\n        key_seq_len=key_seq_len,\n        value_seq_len=value_seq_len,\n        key_heads=key_heads,\n        value_heads=value_heads,\n        key_head_size=key_head_size,\n        value_head_size=value_head_size,\n        dtype=dtype,\n        kv_quant=kv_quant,\n        prefill_cache_logical_axis_names=prefill_cache_logical_axis_names,\n        cache_logical_axis_names=cache_logical_axis_names,\n        cache_scale_logical_axis_names=cache_scale_logical_axis_names,\n        prefill_cache_axis_order=prefill_cache_axis_order,\n        ar_cache_axis_order=ar_cache_axis_order,\n        key_axis_order=key_axis_order,\n        use_chunked_prefill=use_chunked_prefill,\n        model_mode=model_mode,\n        rngs=rngs,\n    )\n\n  def key_latent_add_head_dim(self, key_latent: Array):\n    b, l, hz = key_latent.shape\n    return key_latent.reshape(b, l, 1, hz)\n\n  def key_latent_remove_head_dim(self, key_latent: Array):\n    b, l, _, hz = key_latent.shape\n    return key_latent.reshape(b, l, hz)\n\n  def __call__(\n      self,\n      key_latent: Array,\n      key_rope: Array,\n      decoder_segment_ids: Array,\n      model_mode: str,\n      use_ragged_attention: bool = False,\n      previous_chunk: Any = None,\n  ) -> tuple[\n      None | tuple[Array, Array, Array],\n      None | tuple[Array, Array, Array, Array],\n  ]:\n    assert model_mode != MODEL_MODE_TRAIN, \"incorrectly updating kvcache in train mode.\"\n    assert self.kv_quant is None, \"kvcache quantization not supported with mla.\"\n    key_latent = self.key_latent_add_head_dim(key_latent)\n    prefill_cache, ar_cache = super().__call__(key_latent, key_rope, decoder_segment_ids, model_mode)\n    if prefill_cache:\n      key_latent, key_rope, decoder_segments_ids = prefill_cache\n      prefill_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n      )\n    if ar_cache:\n      key_latent, key_rope, decoder_segments_ids, lengths = ar_cache\n      ar_cache = (\n          self.key_latent_remove_head_dim(key_latent),\n          key_rope,\n          decoder_segments_ids,\n          lengths,\n      )\n    return prefill_cache, ar_cache",
        "analysis": {
            "module_type": "mla_kv_cache",
            "purpose": "A specialized Key-Value cache implementation for Multi-Layer Attention (MLA) models, which adapts tensor shapes to be compatible with the base KVCache logic.",
            "input": {
                "shape": "See the `__call__` method.",
                "dtype": "See the `__call__` method."
            },
            "processing_steps": [
                "Initializes the base KVCache with parameters suitable for MLA, particularly logical axis names that treat the head dimension as non-existent.",
                "In the `__call__` method, it adds a singleton head dimension to the input `key_latent`.",
                "Calls the parent `KVCache`'s `__call__` method to perform the actual caching.",
                "Removes the singleton head dimension from the key latent tensor in the returned caches."
            ],
            "output": {
                "shape": "See the `__call__` method."
            },
            "dependencies": [
                "KVCache"
            ],
            "parameters": {
                "max_prefill_length": "The maximum prefill length.",
                "max_target_length": "The maximum target length for decoding.",
                "batch": "The batch size.",
                "key_head_size": "The size of the key head dimension.",
                "dtype": "The data type for the cache tensors.",
                "prefill_cache_logical_axis_names": "Logical axis names for the prefill cache, defaulting to a layout without a distinct head dimension.",
                "cache_logical_axis_names": "Logical axis names for the autoregressive cache, defaulting to a layout without a distinct head dimension."
            },
            "notes": [
                "This class inherits from `KVCache` and overrides its behavior to handle the specific tensor shapes of MLA.",
                "It explicitly asserts that KV cache quantization is not supported (`kv_quant` must be None).",
                "The `key_rope` input to `__call__` is passed as the `value` argument to the parent `KVCache`'s `__call__` method."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the MlaKVCache module by calling the parent `KVCache` constructor with all provided arguments.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Calls `super().__init__` to initialize the parent `KVCache` class."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "KVCache.__init__"
                    ],
                    "notes": [
                        "It passes specific default values for `prefill_cache_logical_axis_names` and `cache_logical_axis_names` that are suitable for MLA, which lacks a separate head dimension."
                    ]
                },
                "key_latent_add_head_dim": {
                    "purpose": "Adds a singleton head dimension to the key latent tensor to make it compatible with the 4D shape expected by the parent `KVCache`.",
                    "input": {
                        "shape": "[batch, sequence_length, hidden_size]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshapes the 3D input tensor into a 4D tensor by inserting a dimension of size 1."
                    ],
                    "output": {
                        "shape": "[batch, sequence_length, 1, hidden_size]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "key_latent_remove_head_dim": {
                    "purpose": "Removes the singleton head dimension from a key latent tensor, restoring its original 3D shape.",
                    "input": {
                        "shape": "[batch, sequence_length, 1, hidden_size]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reshapes the 4D input tensor into a 3D tensor by removing the singleton dimension."
                    ],
                    "output": {
                        "shape": "[batch, sequence_length, hidden_size]"
                    },
                    "dependencies": [],
                    "notes": []
                },
                "__call__": {
                    "purpose": "Updates the KV cache with new key and value latents for the current decoding step and returns the updated cache contents.",
                    "input": {
                        "shape": "key_latent: [batch, seq_len, hidden_dim], key_rope: [batch, seq_len, hidden_dim], decoder_segment_ids: [batch, seq_len]",
                        "dtype": "Array"
                    },
                    "processing_steps": [
                        "Assert that the model is not in training mode.",
                        "Assert that KV cache quantization is disabled.",
                        "Call `key_latent_add_head_dim` to add a singleton dimension to `key_latent`.",
                        "Call the parent `KVCache.__call__` method with the modified `key_latent`, `key_rope` (as value), and other arguments.",
                        "If `prefill_cache` is returned, call `key_latent_remove_head_dim` on its key latent tensor.",
                        "If `ar_cache` is returned, call `key_latent_remove_head_dim` on its key latent tensor.",
                        "Return the processed caches."
                    ],
                    "output": {
                        "shape": "A tuple of (prefill_cache, ar_cache). Each element is either None or a tuple of tensors. The key latent tensor within these tuples has its singleton head dimension removed."
                    },
                    "dependencies": [
                        "KVCache.__call__",
                        "self.key_latent_add_head_dim",
                        "self.key_latent_remove_head_dim"
                    ],
                    "notes": [
                        "This method acts as a wrapper around the parent class's `__call__` method, adapting the tensor shapes for MLA."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageState",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageState:\n  \"\"\"Represents the global state of memory pages managed by the `PageManager`.\n\n  This dataclass tracks the allocation status of each page across the entire system,\n  the mapping of pages to page groups (requests), and the current position within\n  each sequence's pages. State is managed globally, providing a single view\n  across all potential layers using this manager.\n\n  Attributes:\n    page_status: A `jnp.ndarray` of shape `[num_pages]`. Each element\n      indicates whether the corresponding page in the global pool is free (0)\n      or allocated (1).\n    page_map: A `jnp.ndarray` of shape `[max_page_groups, max_pages_per_group]`.\n      This array maps each page group to the indices (within the global pool)\n      of its allocated pages. Entries beyond `num_pages_used` for a group are invalid.\n    num_pages_used: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      tracks the number of pages currently allocated to each page group. This\n      determines the valid entries in `page_map` for each group.\n    sequence_lengths: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the current true length of each sequence (in tokens) associated\n      with a page group.\n    active_page: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the global index of the *currently active* page (the page where the\n      next token will be written) for each page group. Only valid if the\n      corresponding `has_active_page` is True.\n    has_active_page: A `jnp.ndarray` of shape `[max_page_groups]`. Boolean mask\n      indicating whether a page group currently represents an active sequence\n      and thus whether its `active_page` and `active_page_position` entries\n      are meaningful.\n    active_page_position: A `jnp.ndarray` of shape `[max_page_groups]`. This array\n      stores the index (offset, 0 to tokens_per_page-1) of the next available\n      token *within the `active_page`* for each page group. Only valid if\n      `has_active_page` is True.\n  \"\"\"\n\n  page_status: PagesInt1d\n  page_map: GroupsPagesInt2d\n  num_pages_used: GroupsInt1d\n  sequence_lengths: GroupsInt1d\n  active_page: GroupsInt1d\n  has_active_page: GroupsBool1d\n  active_page_position: GroupsInt1d",
        "analysis": {
            "module_type": "paged_attention_state",
            "purpose": "A Flax dataclass that encapsulates the entire state of the paged memory system, tracking page allocation, group mappings, and sequence positions for a PageManager.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "flax.struct",
                "jaxtyping.Array",
                "jaxtyping.Integer",
                "jaxtyping.Bool"
            ],
            "parameters": {},
            "notes": [
                "This is a Flax `struct.dataclass`, making it a JAX-compatible and immutable data structure.",
                "It serves as a container for all state tensors managed by the `PageManager`.",
                "The state is managed globally, providing a single view of memory across all potential layers.",
                "The attributes of this class are JAX arrays with specific shapes determined by the paged attention configuration: `page_status` [num_pages], `page_map` [max_page_groups, max_pages_per_group], and others with shape [max_page_groups]."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#initialize_page_state",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def initialize_page_state(\n    num_pages: int,\n    max_page_groups: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Creates and initializes a global `PageState` object.\n\n  All pages in the global pool are initially marked as free (status 0), except\n  for page 0 which is marked used as a workaround. No pages are assigned to any\n  page group. Sequence lengths and page usage counts are initialized to zero.\n  Active page tracking is also reset.\n\n  Args:\n    num_pages: The total number of available pages in the global pool.\n    max_page_groups: The maximum number of page groups (concurrent sequences/requests)\n      the system can track.\n    max_pages_per_group: The maximum number of pages that can be allocated to\n      a single page group (determines the size of the second dimension of `page_map`).\n\n  Returns:\n    An initialized `PageState` object with all values set to their defaults (zeros/False).\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  initial_page_status = jnp.zeros((num_pages,), dtype=jnp.int32)\n  initial_page_status = initial_page_status.at[0].set(1)  # Workaround page 0\n  return PageState(\n      page_status=initial_page_status,\n      page_map=jnp.zeros((max_page_groups, max_pages_per_group), dtype=jnp.int32),\n      num_pages_used=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      sequence_lengths=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      active_page=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n      has_active_page=jnp.zeros((max_page_groups,), dtype=jnp.bool_),\n      active_page_position=jnp.zeros((max_page_groups,), dtype=jnp.int32),\n  )",
        "analysis": {
            "module_type": "page_state_initializer",
            "purpose": "Creates and initializes a global `PageState` object with default values for managing paged attention memory.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a zero-initialized integer array `initial_page_status` of shape [num_pages].",
                "Set the status of the first page (index 0) to 1 (used) as a workaround.",
                "Instantiate a `PageState` object, initializing its tensor attributes with zeros or False values based on the provided dimensions."
            ],
            "output": {
                "shape": "A `PageState` object containing multiple tensors with shapes derived from input parameters."
            },
            "dependencies": [
                "jax.numpy",
                "PageState"
            ],
            "parameters": {
                "num_pages": "The total number of available pages in the global pool.",
                "max_page_groups": "The maximum number of concurrent sequences/requests the system can track.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "All pages are marked as free (status 0) except for page 0, which is explicitly marked as used (status 1) as a workaround.",
                "All other state arrays like `page_map`, `num_pages_used`, `sequence_lengths`, etc., are initialized to zeros or False."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_find_next_free_page_index",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _find_next_free_page_index(page_status: PagesInt1d) -> ScalarInt:\n  \"\"\"Finds the index of the next available free page in the global pool.\n\n  Searches the `page_status` array for the first occurrence of 0 (indicating\n  a free page), skipping index 0 due to potential issues.\n\n  Args:\n    page_status: A 1D `jnp.ndarray` representing the global status of pages\n      (0 for free, 1 for allocated). Should have shape [num_pages].\n\n  Returns:\n    A scalar `jnp.int32` array containing the index of the next free page\n    (the lowest index >= 1 where `page_status` is 0).\n    Returns -1 if no free pages (at index >= 1) are found.\n  \"\"\"\n  # TODO(patemotter): Produces garbage output for any request that uses page 0\n  search_status = page_status[1:]\n  overall_free_mask = search_status == 0\n\n  # argmax returns the index of the *first* True. If none are True, it returns 0.\n  next_free_relative = jnp.argmax(overall_free_mask)\n  # Add 1 to compensate for the slice [1:]\n  next_free_overall = next_free_relative + 1\n  # Check if a free page exists\n  has_free_overall = jnp.any(overall_free_mask)\n  # If a free page exists, return its index, otherwise return -1\n  return jnp.where(has_free_overall, next_free_overall, -1)",
        "analysis": {
            "module_type": "free_page_finder",
            "purpose": "Searches a page status array to find the index of the first available free page, skipping index 0.",
            "input": {
                "shape": "[num_pages]",
                "dtype": "int32"
            },
            "processing_steps": [
                "Slice the input `page_status` array to exclude the element at index 0.",
                "Create a boolean mask to identify free pages (where status is 0) in the sliced array.",
                "Use `jnp.argmax` on the mask to find the relative index of the first free page.",
                "Add 1 to the relative index to get the absolute index in the original array.",
                "Check if any free pages exist using `jnp.any`.",
                "Use `jnp.where` to return the absolute index if a free page was found, otherwise return -1."
            ],
            "output": {
                "shape": "[] (Scalar)"
            },
            "dependencies": [
                "jax.numpy.argmax",
                "jax.numpy.any",
                "jax.numpy.where"
            ],
            "parameters": {
                "N/A": "This function does not take parameters from a configuration object."
            },
            "notes": [
                "The function is decorated with `@jax.jit` for JIT compilation.",
                "It intentionally ignores the page at index 0 during the search, as indicated by the `page_status[1:]` slice and comments.",
                "It relies on the behavior of `jnp.argmax` which returns 0 if no True elements are found, and uses a separate `jnp.any` check to handle this case correctly.",
                "Returns -1 as a scalar integer if no free pages (at index >= 1) are available."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_pages_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases all pages associated with a given page group.\n\n  This function iterates through the potential pages allocated to the specified\n  `page_group_id` (up to `max_pages_per_group`). For each page index actually\n  used by the group (determined by `num_pages_used`), it retrieves the global\n  page index from `page_map` and resets its status to 0 (free) in the global\n  `page_status` array. It also resets all state fields related to the\n  `page_group_id` (length, count, active status, etc.) to their initial values.\n\n  Args:\n    page_state: The current global `PageState`.\n    page_group_id: The index of the page group whose pages are to be released.\n    max_pages_per_group: The maximum number of pages a group can hold (used as\n      the loop bound).\n\n  Returns:\n    A new `PageState` object where the specified group's pages are marked as free\n    in `page_status`, and the group's specific state entries are reset.\n  \"\"\"\n  current_page_status = page_state.page_status\n  current_page_map = page_state.page_map\n  num_valid_pages = page_state.num_pages_used[page_group_id]\n\n  def release_page(i: int, status: PagesInt1d) -> PagesInt1d:\n    is_valid = i < num_valid_pages\n    page_idx = current_page_map[page_group_id, i]\n    # Only release if index 'i' points to a valid allocated page\n    should_release = jnp.logical_and(is_valid, page_idx > 0)\n\n    return jax.lax.cond(should_release, lambda s: s.at[page_idx].set(0), lambda s: s, status)\n\n  new_page_status = jax.lax.fori_loop(0, max_pages_per_group, release_page, current_page_status)\n\n  return page_state.replace(\n      page_status=new_page_status,\n      num_pages_used=page_state.num_pages_used.at[page_group_id].set(0),\n      sequence_lengths=page_state.sequence_lengths.at[page_group_id].set(0),\n      active_page=page_state.active_page.at[page_group_id].set(0),\n      has_active_page=page_state.has_active_page.at[page_group_id].set(False),\n      active_page_position=page_state.active_page_position.at[page_group_id].set(0),\n  )",
        "analysis": {
            "module_type": "paged_attention_page_release",
            "purpose": "Releases all memory pages associated with a specific page group and resets that group's state to its initial values.",
            "input": {
                "shape": "page_state: PageState dataclass, page_group_id: [], max_pages_per_group: Python int",
                "dtype": "int32"
            },
            "processing_steps": [
                "Extract `page_status`, `page_map`, and `num_pages_used` from the input `page_state`.",
                "Define a nested function `release_page` for use within a loop.",
                "Use `jax.lax.fori_loop` to iterate from 0 to `max_pages_per_group`.",
                "Inside the loop, for each potential page of the group, check if it's a validly allocated page (`i < num_pages_used` and `page_idx > 0`).",
                "If the page should be released, use `jax.lax.cond` to update the `page_status` array by setting the corresponding global page index to 0 (free).",
                "After the loop, create a new `PageState` object using `page_state.replace`.",
                "Update the `page_status` with the result from the loop.",
                "Reset all state fields for the given `page_group_id` to their initial values (0 or False), including `num_pages_used`, `sequence_lengths`, `active_page`, `has_active_page`, and `active_page_position`."
            ],
            "output": {
                "shape": "A new PageState object with the same tensor shapes as the input."
            },
            "dependencies": [
                "PageState",
                "jax.lax.fori_loop",
                "jax.lax.cond",
                "jnp.logical_and"
            ],
            "parameters": {
                "max_pages_per_group": "The maximum number of pages a single group can hold, which is used as the loop boundary and is a static argument for JIT compilation."
            },
            "notes": [
                "The function is JIT-compiled with `max_pages_per_group` as a static argument, meaning a different version of the compiled function is created for each unique value of this parameter.",
                "The condition `page_idx > 0` suggests that page index 0 is treated as a special case and is not released.",
                "This function not only frees pages in the global pool but also completely resets the metadata for the specified page group."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_reserve_pages_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _reserve_pages_for_group(\n    released_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Reserves pages for a specific group, assuming true_length > 0.\n\n  PRECONDITION: `true_length` must be > 0. This function assumes the caller\n  (e.g., `PageManager.update_prefill_pages`) has validated this.\n\n  Calculates the number of pages required for `true_length`. Checks if enough\n  free pages exist globally and if the group has capacity based on the state\n  provided in `released_state`. If resources are sufficient, it iteratively\n  finds free pages, marks them allocated, records them in the map, and updates\n  the group's state fields. If resources are insufficient, it returns the\n  `released_state` unchanged (effectively leaving the group empty).\n\n  Args:\n      released_state: The global `PageState` after pages for `page_group_id`\n          have already been released.\n      page_group_id: The index of the page group to allocate pages for.\n      true_length: The target sequence length for the prefill. MUST BE > 0.\n      tokens_per_page: The capacity of each page.\n      max_pages_per_group: The maximum number of pages the group can hold.\n\n  Returns:\n      A new `PageState` with pages allocated for the group and its state updated,\n      or the input `released_state` if allocation failed due to resource limits.\n  \"\"\"\n  num_pages_needed = (true_length + tokens_per_page - 1) // tokens_per_page\n  last_token_abs_idx = true_length - 1\n  last_page_position_idx = last_token_abs_idx % tokens_per_page\n  next_write_position = (last_page_position_idx + 1) % tokens_per_page\n\n  current_page_status = released_state.page_status\n  current_page_map = released_state.page_map\n  current_num_pages_used = released_state.num_pages_used\n\n  num_free_pages = jnp.sum(current_page_status == 0)\n  group_has_capacity = jax.lax.le(num_pages_needed, max_pages_per_group)\n  sufficient_free_pages = jax.lax.ge(num_free_pages, num_pages_needed)\n  has_enough_resources = jnp.logical_and(sufficient_free_pages, group_has_capacity)\n\n  def allocate_and_update_state(initial_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]) -> PageState:\n    \"\"\"Allocates pages iteratively if resources are sufficient.\"\"\"\n    initial_status, initial_map, initial_num_used = initial_state_tuple\n\n    def allocate_one_page(\n        page_idx_in_group: ScalarInt, loop_state_tuple: tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]\n    ) -> tuple[PagesInt1d, GroupsPagesInt2d, GroupsInt1d]:\n      \"\"\"Allocates a single page within the fori_loop.\"\"\"\n      current_loop_status, current_loop_map, current_loop_num_used = loop_state_tuple\n      next_free_page_global = _find_next_free_page_index(current_loop_status)\n      page_allocated = jax.lax.ge(next_free_page_global, 0)\n\n      new_loop_status = jax.lax.cond(\n          page_allocated,\n          lambda s: s.at[next_free_page_global].set(1),\n          lambda s: s,\n          current_loop_status,\n      )\n      new_loop_map = jax.lax.cond(\n          page_allocated,\n          lambda m: m.at[page_group_id, page_idx_in_group].set(next_free_page_global),\n          lambda m: m,\n          current_loop_map,\n      )\n      new_loop_num_used = jax.lax.cond(\n          page_allocated,\n          lambda n: n.at[page_group_id].add(1),\n          lambda n: n,\n          current_loop_num_used,\n      )\n      return new_loop_status, new_loop_map, new_loop_num_used\n\n    final_page_status, final_page_map, final_num_pages_used = jax.lax.fori_loop(\n        0,\n        num_pages_needed,\n        allocate_one_page,\n        (initial_status, initial_map, initial_num_used),\n    )\n    active_page_global_index = final_page_map[page_group_id, num_pages_needed - 1]\n\n    return released_state.replace(\n        page_status=final_page_status,\n        page_map=final_page_map,\n        num_pages_used=final_num_pages_used,\n        sequence_lengths=released_state.sequence_lengths.at[page_group_id].set(true_length),\n        active_page=released_state.active_page.at[page_group_id].set(active_page_global_index),\n        has_active_page=released_state.has_active_page.at[page_group_id].set(True),\n        active_page_position=released_state.active_page_position.at[page_group_id].set(next_write_position),\n    )\n\n  # Conditionally perform allocation or return the released state\n  final_state = jax.lax.cond(\n      has_enough_resources,\n      allocate_and_update_state,\n      lambda _: released_state,\n      operand=(current_page_status, current_page_map, current_num_pages_used),\n  )\n  return final_state",
        "analysis": {
            "module_type": "paged_attention_page_reservation",
            "purpose": "Calculates the required pages for a sequence, and if resources are sufficient, allocates them and updates the global page state; otherwise, it returns the state unchanged.",
            "input": {
                "shape": "released_state: PageState object, page_group_id: [], true_length: [], tokens_per_page: scalar, max_pages_per_group: scalar",
                "dtype": "released_state: mixed (int32, bool), page_group_id: int32, true_length: int32, tokens_per_page: int, max_pages_per_group: int"
            },
            "processing_steps": [
                "Calculate the number of pages needed for the given `true_length`.",
                "Determine the next write position within the last page.",
                "Check for resource availability by counting global free pages and comparing needed pages against the group's capacity.",
                "Use `jax.lax.cond` to conditionally execute the allocation logic based on resource availability.",
                "If resources are sufficient, use `jax.lax.fori_loop` to iteratively find and allocate free pages by calling `_find_next_free_page_index`.",
                "Within the loop, update `page_status` (mark as used), `page_map` (record the mapping), and `num_pages_used` for the group.",
                "Create and return a new `PageState` with updated fields for the group (sequence length, active page, etc.).",
                "If resources are insufficient, return the input `released_state` without modification."
            ],
            "output": {
                "shape": "A PageState object with the same structure as the input."
            },
            "dependencies": [
                "PageState",
                "_find_next_free_page_index",
                "jax.lax.cond",
                "jax.lax.fori_loop",
                "jax.numpy"
            ],
            "parameters": {
                "tokens_per_page": "The capacity of each page in tokens.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single group."
            },
            "notes": [
                "This function is JIT-compiled via `partial(jax.jit, ...)` with static arguments `tokens_per_page` and `max_pages_per_group`.",
                "It has a precondition that the input `true_length` must be greater than 0.",
                "The core logic is conditional: allocation only occurs if both global page availability and group capacity are sufficient.",
                "The function is pure and returns a new `PageState` object rather than modifying the input in place."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_release_and_reserve_for_group",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _release_and_reserve_for_group(\n    page_state: PageState,\n    page_group_id: ScalarInt,\n    true_length: ScalarInt,\n    tokens_per_page: int,\n    max_pages_per_group: int,\n) -> PageState:\n  \"\"\"Releases existing pages and reserves new pages for a group during prefill.\n\n  Assumes true_length > 0. Caller MUST validate inputs.\n  \"\"\"\n  released_state = _release_pages_for_group(page_state, page_group_id, max_pages_per_group)\n  final_state = _reserve_pages_for_group(released_state, page_group_id, true_length, tokens_per_page, max_pages_per_group)\n  return final_state",
        "analysis": {
            "module_type": "page_management_helper",
            "purpose": "Releases all existing pages for a specific group and then reserves a new set of pages to accommodate a given sequence length, typically used during prefill.",
            "input": {
                "shape": "Inputs are a `PageState` object, a scalar `page_group_id`, a scalar `true_length`, and two Python integers `tokens_per_page` and `max_pages_per_group`.",
                "dtype": "The `PageState` contains jax arrays of int32 and bool types. Other inputs are integers."
            },
            "processing_steps": [
                "Call `_release_pages_for_group` to free all pages currently allocated to the specified `page_group_id`, resetting its state.",
                "Call `_reserve_pages_for_group` on the resulting state to allocate a new set of pages for the same group based on the `true_length`.",
                "Return the final `PageState`."
            ],
            "output": {
                "shape": "Returns a `PageState` object with the same shape as the input `page_state`."
            },
            "dependencies": [
                "PageState",
                "_release_pages_for_group",
                "_reserve_pages_for_group"
            ],
            "parameters": {
                "tokens_per_page": "The number of tokens each page can hold.",
                "max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "This function is decorated with `partial(jax.jit, ...)` making `tokens_per_page` and `max_pages_per_group` static arguments for JIT compilation.",
                "It assumes `true_length > 0` and that the caller is responsible for validating inputs."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#_update_decode_pages_global",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "def _update_decode_pages_global(\n    page_state: PageState,\n    tokens_per_page: ScalarInt,\n    max_pages_per_group: ScalarInt,\n) -> PageState:\n  \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n  This function performs the following steps for all page groups simultaneously:\n  1. Increments `sequence_lengths` for groups marked as `has_active_page`.\n  2. Calculates the new `active_page_position` based on the incremented length.\n  3. Determines which active groups now require a new page because their sequence\n     length has crossed a page boundary (`required_pages > num_pages_used`) and\n     they still have capacity (`required_pages <= max_pages_per_group`).\n  4. For each group identified in step 3, it attempts to find a free page globally and\n     allocate it, updating `page_status`, `page_map`, `num_pages_used`, and\n     `active_page` for that group.\n\n  Args:\n    page_state: The current global `PageState`.\n    tokens_per_page: The capacity of each page.\n    max_pages_per_group: The maximum number of pages allowed per group.\n\n  Returns:\n    A new `PageState` object reflecting the state after the decode step, potentially\n    with new pages allocated to groups that crossed page boundaries.\n  \"\"\"\n  max_page_groups = page_state.sequence_lengths.shape[0]\n\n  seq_len_increment = jnp.where(page_state.has_active_page, 1, 0)\n  new_sequence_lengths = page_state.sequence_lengths + seq_len_increment\n\n  new_active_page_position = jnp.where(\n      page_state.has_active_page,\n      (new_sequence_lengths - 1) % tokens_per_page,\n      page_state.active_page_position,\n  )\n\n  required_pages_per_group = (new_sequence_lengths + tokens_per_page - 1) // tokens_per_page\n  needs_new_page_mask = jnp.logical_and(page_state.has_active_page, required_pages_per_group > page_state.num_pages_used)\n  has_capacity_mask = required_pages_per_group <= max_pages_per_group\n  needs_allocation_mask = jnp.logical_and(needs_new_page_mask, has_capacity_mask)\n\n  def allocate_for_group_if_needed(group_idx: ScalarInt, current_state: PageState) -> PageState:\n    \"\"\"Inner function for fori_loop to conditionally allocate a page.\"\"\"\n    current_status = current_state.page_status\n    current_map = current_state.page_map\n    current_num_used = current_state.num_pages_used\n    current_active_page = current_state.active_page\n\n    needs_alloc = needs_allocation_mask[group_idx]\n    next_free_page_global = _find_next_free_page_index(current_status)\n    can_allocate = jnp.logical_and(needs_alloc, next_free_page_global >= 0)\n\n    new_status = jax.lax.cond(can_allocate, lambda s: s.at[next_free_page_global].set(1), lambda s: s, current_status)\n\n    page_map_index = current_num_used[group_idx]\n    new_map = jax.lax.cond(\n        can_allocate, lambda m: m.at[group_idx, page_map_index].set(next_free_page_global), lambda m: m, current_map\n    )\n    new_num_used = jax.lax.cond(can_allocate, lambda n: n.at[group_idx].add(1), lambda n: n, current_num_used)\n    new_active_page = jax.lax.cond(\n        can_allocate, lambda a: a.at[group_idx].set(next_free_page_global), lambda a: a, current_active_page\n    )\n\n    # Reconstruct state for loop carry/return\n    return current_state.replace(\n        page_status=new_status,\n        page_map=new_map,\n        num_pages_used=new_num_used,\n        active_page=new_active_page,\n    )\n\n  # Initialize loop state with pre-calculated lengths and positions\n  initial_loop_state = page_state.replace(\n      sequence_lengths=new_sequence_lengths,\n      active_page_position=new_active_page_position,\n  )\n\n  # Apply conditional allocation across all groups\n  final_state = jax.lax.fori_loop(0, max_page_groups, allocate_for_group_if_needed, initial_loop_state)\n  return final_state",
        "analysis": {
            "module_type": "page_state_decode_update",
            "purpose": "Updates the global page state for a single step of autoregressive decoding, allocating new pages for sequences that cross a page boundary.",
            "input": {
                "shape": "Input is a `PageState` object containing multiple tensors with shapes like `[num_pages]`, `[max_page_groups]`, and `[max_page_groups, max_pages_per_group]`.",
                "dtype": "int32, bool"
            },
            "processing_steps": [
                "Increment sequence lengths for all active page groups.",
                "Calculate the new active page position for each active group based on the new length.",
                "Determine which groups require a new page by comparing the number of pages needed for the new length to the number of pages currently used.",
                "Create a mask for groups that both need a new page and have not exceeded their maximum page capacity.",
                "Initialize a loop state with the updated sequence lengths and active page positions.",
                "Iterate through each page group using `jax.lax.fori_loop`.",
                "Within the loop, for each group marked for allocation, find the next free global page using `_find_next_free_page_index`.",
                "Conditionally update `page_status`, `page_map`, `num_pages_used`, and `active_page` if a new page can be allocated to the group.",
                "Return the final updated `PageState` after the loop completes."
            ],
            "output": {
                "shape": "Output is a `PageState` object with the same tensor shapes as the input."
            },
            "dependencies": [
                "PageState",
                "_find_next_free_page_index",
                "jax.lax.fori_loop",
                "jax.lax.cond",
                "jax.numpy"
            ],
            "parameters": {
                "tokens_per_page": "The capacity of each page in tokens.",
                "max_pages_per_group": "The maximum number of pages allowed per group."
            },
            "notes": [
                "This function is JIT-compiled using `jax.jit` for performance.",
                "`tokens_per_page` and `max_pages_per_group` are static arguments to the JIT-compiled function.",
                "The core logic uses a `jax.lax.fori_loop` to iterate over all page groups, which is necessary for JIT compilation.",
                "Page allocation is conditional; if a group needs a page but none are free globally, it will not be allocated a new page."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/page_manager.py#PageManager",
        "file_path": "src/MaxText/inference/page_manager.py",
        "code_block": "class PageManager:\n  \"\"\"Manages the global allocation and release of pages for paged attention.\n\n  This class provides an interface for reserving pages during prefill and\n  decoding, and for releasing pages when a sequence (page group) is complete.\n  It encapsulates the logic for tracking page allocation globally and managing\n  the `PageState`. It uses the concept of page groups, where each group typically\n  corresponds to a single request or sequence being processed.\n\n  Example:\n    ```python\n    # Initialize a PageManager from configuration\n    config = YourConfig(...) # Set pagedattn_num_pages, etc.\n    page_manager = PageManager(config)\n\n    # Get initial page state (all pages free, except potentially page 0)\n    state = page_manager.get_initial_page_state()\n\n    # Update pages for prefill of a sequence in group 0 with length 16\n    state = page_manager.update_prefill_pages(\n        page_state=state,\n        page_group_id=0,\n        true_length=16\n    )\n\n    # Update pages for a single decode step (increments lengths, allocates if needed)\n    state = page_manager.update_decode_pages(state)\n\n    # Release pages associated with group 0 when the sequence is finished\n    state = page_manager.release_pages(\n        page_state=state,\n        page_group_id=0\n    )\n    ```\n  \"\"\"\n\n  def __init__(self, config: Config):\n    \"\"\"Initializes the `PageManager` from a configuration object.\n\n    Args:\n      config: A `Config` object containing the necessary parameters:\n        * `max_target_length`: The maximum sequence length supported.\n        * `pagedattn_num_pages`: The total number of pages available globally.\n        * `pagedattn_tokens_per_page`: The number of tokens each page can hold.\n        * `global_batch_size_to_load`: Used to determine the maximum number of concurrent\n          page groups (`max_page_groups`) the system can manage.\n        * `pagedattn_max_pages_per_group`: The maximum number of pages that can be\n          allocated to a single page group.\n\n    Raises:\n      ValueError: If the configuration parameters are invalid (e.g., non-positive\n        values, insufficient pages per group for max length).\n    \"\"\"\n    self.num_pages: int = config.pagedattn_num_pages\n    self.tokens_per_page: int = config.pagedattn_tokens_per_page\n    self.max_target_length: int = config.max_target_length\n    self.max_page_groups: int = config.global_batch_size_to_load\n    self.max_pages_per_group: int = config.pagedattn_max_pages_per_group\n    self._validate_init_params()\n\n  def _validate_init_params(self) -> None:\n    \"\"\"Validates initialization parameters for logical consistency.\"\"\"\n    if self.max_pages_per_group <= 0:\n      raise ValueError(\"`pagedattn_max_pages_per_group` must be positive.\")\n    min_required = (self.max_target_length + self.tokens_per_page - 1) // self.tokens_per_page\n    if self.max_pages_per_group < min_required:\n      raise ValueError(\n          f\"`pagedattn_max_pages_per_group` ({self.max_pages_per_group}) is insufficient for `max_target_length` \"\n          f\"({self.max_target_length}). Needs {min_required}.\"\n      )\n    # Check > 1 due to potential page 0 workaround\n    if self.num_pages <= 1:\n      raise ValueError(\"`pagedattn_num_pages` must be greater than 1.\")\n    if self.tokens_per_page <= 0:\n      raise ValueError(\"`pagedattn_tokens_per_page` must be positive.\")\n    if self.max_page_groups <= 0:\n      raise ValueError(\"`pagedattn_max_page_groups` must be positive.\")\n\n  def update_prefill_pages(self, page_state: PageState, page_group_id: int, true_length: int) -> PageState:\n    \"\"\"Reserves pages for a specific page group during prefill (global state).\n\n    This method first releases any pages currently allocated to the given\n    `page_group_id`. It then attempts to allocate the necessary number of pages\n    from the global pool to accommodate a sequence of `true_length`. If successful,\n    it updates the `PageState` to reflect the new allocation and marks the group\n    as active. If there are not enough free pages globally or the group exceeds\n    its `max_pages_per_group` limit, the group's state remains cleared (as after\n    the initial release). Input validation ensures `page_group_id` and `true_length`\n    are within valid ranges.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to allocate pages for. Must\n        be between 0 and `max_page_groups - 1`.\n      true_length: The sequence length to allocate pages for. Must be between 0\n        and `max_target_length`.\n\n    Returns:\n      The updated `PageState`. If allocation fails due to resource limits, the\n      returned state will have the specified `page_group_id` cleared.\n\n    Raises:\n      ValueError: If `page_group_id` or `true_length` are outside their valid ranges.\n\n    Example:\n      ```python\n      # Reserve pages for a 16-token sequence in group 0\n      state = page_manager.update_prefill_pages(\n          page_state=state,\n          page_group_id=0,\n          true_length=16\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    if true_length <= 0 or true_length > self.max_target_length:\n      raise ValueError(f\"PageManager: true_length ({true_length}) out of range (0, {self.max_target_length}]\")\n\n    return _release_and_reserve_for_group(\n        page_state, page_group_id, true_length, self.tokens_per_page, self.max_pages_per_group\n    )\n\n  def update_decode_pages(self, page_state: PageState) -> PageState:\n    \"\"\"Updates pages globally for one step of autoregressive decoding.\n\n    This method advances the state for all active page groups. It increments\n    their sequence lengths by one and updates their position within the current\n    active page. If this increment causes a sequence to cross a page boundary\n    (i.e., it needs more pages than currently allocated), this method attempts\n    to allocate a new page from the global pool, provided the group has not\n    reached its `max_pages_per_group` limit and free pages are available.\n\n    Args:\n      page_state: The current global `PageState`.\n\n    Returns:\n      The updated `PageState` reflecting the state after the decode step.\n      Sequence lengths and active positions are updated for all active groups.\n      Groups that required and successfully obtained a new page will have their\n      `num_pages_used`, `page_map`, and `active_page` updated.\n\n    Example:\n      ```python\n      # Advance state for all active sequences by one decode step\n      state = page_manager.update_decode_pages(state)\n      ```\n    \"\"\"\n    return _update_decode_pages_global(page_state, self.tokens_per_page, self.max_pages_per_group)\n\n  def release_pages(self, page_state: PageState, page_group_id: int) -> PageState:\n    \"\"\"Releases all pages associated with a given page group (global state).\n\n    This method identifies all pages currently allocated to the specified\n    `page_group_id` using the `page_map` and `num_pages_used`. It marks these\n    pages as free (status 0) in the global `page_status` array. It also resets\n    all state information specific to the `page_group_id` (sequence length,\n    page count, active status, etc.) to their initial zero/False values.\n    Input validation ensures the `page_group_id` is within the valid range.\n\n    Args:\n      page_state: The current global `PageState`.\n      page_group_id: The ID of the page group (request) to release. Must be\n        between 0 and `max_page_groups - 1`.\n\n    Returns:\n      The updated `PageState` after releasing the pages and resetting the group's\n      state.\n\n    Raises:\n      ValueError: If `page_group_id` is outside its valid range.\n\n    Example:\n      ```python\n      # Release all pages currently held by group 0\n      state = page_manager.release_pages(\n          page_state=state,\n          page_group_id=0\n      )\n      ```\n    \"\"\"\n    if page_group_id < 0 or page_group_id >= self.max_page_groups:\n      raise ValueError(f\"PageManager: page_group_id ({page_group_id}) out of range [0, {self.max_page_groups})\")\n    return _release_pages_for_group(page_state, page_group_id, self.max_pages_per_group)\n\n  def get_initial_page_state(self) -> PageState:\n    \"\"\"Creates and returns an initial global `PageState`.\n\n    This is a convenience method that calls `initialize_page_state` with\n    the parameters (`num_pages`, `max_page_groups`, `max_pages_per_group`)\n    stored during the `PageManager` initialization.\n\n    Returns:\n      An initialized `PageState` object where all pages are free (except possibly 0)\n      and no groups are active.\n\n    Example:\n      ```python\n      # Get a fresh, empty page state\n      initial_state = page_manager.get_initial_page_state()\n      ```\n    \"\"\"\n    return initialize_page_state(\n        num_pages=self.num_pages,\n        max_page_groups=self.max_page_groups,\n        max_pages_per_group=self.max_pages_per_group,\n    )",
        "analysis": {
            "module_type": "paged_attention_manager",
            "purpose": "Manages the global allocation and release of memory pages for paged attention, encapsulating the logic for tracking page allocation and managing the PageState.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "Config",
                "PageState",
                "initialize_page_state",
                "_release_and_reserve_for_group",
                "_update_decode_pages_global",
                "_release_pages_for_group"
            ],
            "parameters": {
                "pagedattn_num_pages": "The total number of pages available globally.",
                "pagedattn_tokens_per_page": "The number of tokens each page can hold.",
                "max_target_length": "The maximum sequence length supported.",
                "global_batch_size_to_load": "Used to determine the maximum number of concurrent page groups.",
                "pagedattn_max_pages_per_group": "The maximum number of pages that can be allocated to a single page group."
            },
            "notes": [
                "This class provides an interface for reserving pages during prefill and decoding, and for releasing pages when a sequence is complete.",
                "It uses the concept of page groups, where each group typically corresponds to a single request or sequence being processed."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PageManager from a configuration object.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Extracts paging-related parameters from the config object.",
                        "Stores parameters as instance attributes.",
                        "Calls `_validate_init_params` to ensure logical consistency."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "Config",
                        "_validate_init_params"
                    ],
                    "notes": [
                        "Raises ValueError if the configuration parameters are invalid (e.g., non-positive values, insufficient pages per group for max length)."
                    ]
                },
                "_validate_init_params": {
                    "purpose": "Validates initialization parameters for logical consistency.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `max_pages_per_group` is positive.",
                        "Check if `max_pages_per_group` is sufficient for `max_target_length`.",
                        "Check if `num_pages` is greater than 1.",
                        "Check if `tokens_per_page` is positive.",
                        "Check if `max_page_groups` is positive."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [],
                    "notes": [
                        "Raises ValueError if any of the validation checks fail."
                    ]
                },
                "update_prefill_pages": {
                    "purpose": "Reserves pages for a specific page group during prefill.",
                    "input": {
                        "shape": "Inputs are `page_state` (a PageState object), `page_group_id` (scalar int), and `true_length` (scalar int).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate `page_group_id` and `true_length` are within valid ranges.",
                        "Call `_release_and_reserve_for_group` to perform the core logic."
                    ],
                    "output": {
                        "shape": "Returns an updated `PageState` object."
                    },
                    "dependencies": [
                        "PageState",
                        "_release_and_reserve_for_group"
                    ],
                    "notes": [
                        "This method first releases any pages currently allocated to the group before attempting to reserve new ones.",
                        "If allocation fails due to resource limits, the returned state will have the specified page group cleared."
                    ]
                },
                "update_decode_pages": {
                    "purpose": "Updates pages globally for one step of autoregressive decoding.",
                    "input": {
                        "shape": "Input is `page_state` (a PageState object).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `_update_decode_pages_global` to perform the core logic."
                    ],
                    "output": {
                        "shape": "Returns an updated `PageState` object."
                    },
                    "dependencies": [
                        "PageState",
                        "_update_decode_pages_global"
                    ],
                    "notes": [
                        "This method advances the state for all active page groups, incrementing their sequence lengths and allocating a new page if a boundary is crossed."
                    ]
                },
                "release_pages": {
                    "purpose": "Releases all pages associated with a given page group.",
                    "input": {
                        "shape": "Inputs are `page_state` (a PageState object) and `page_group_id` (scalar int).",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Validate `page_group_id` is within the valid range.",
                        "Call `_release_pages_for_group` to perform the core logic."
                    ],
                    "output": {
                        "shape": "Returns an updated `PageState` object."
                    },
                    "dependencies": [
                        "PageState",
                        "_release_pages_for_group"
                    ],
                    "notes": [
                        "This method marks the group's pages as free and resets all state information for that group."
                    ]
                },
                "get_initial_page_state": {
                    "purpose": "Creates and returns an initial global PageState.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `initialize_page_state` with the manager's stored configuration parameters."
                    ],
                    "output": {
                        "shape": "Returns an initialized `PageState` object."
                    },
                    "dependencies": [
                        "PageState",
                        "initialize_page_state"
                    ],
                    "notes": [
                        "This is a convenience method to get a fresh state where all pages are free and no groups are active."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InputData",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InputData:\n  \"\"\"Container for input data and metadata.\n\n  Attributes:\n      id: Unique identifier for this input\n      tokens: JAX array containing the tokenized input\n      true_length: Actual length of the input before padding\n  \"\"\"\n\n  id: str\n  tokens: jax.Array | np.ndarray\n  true_length: int",
        "analysis": {
            "module_type": "input_data_container",
            "purpose": "A dataclass to hold input data and its associated metadata for inference.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes an object with 'id', 'tokens', and 'true_length' attributes upon instantiation."
            ],
            "output": {
                "shape": "An instance of the InputData class."
            },
            "dependencies": [
                "dataclasses",
                "jax.Array",
                "numpy.ndarray"
            ],
            "parameters": {
                "id": "A string representing the unique identifier for this input.",
                "tokens": "A JAX or NumPy array containing the tokenized input sequence, which may be padded.",
                "true_length": "An integer representing the actual length of the input sequence before any padding was applied."
            },
            "notes": [
                "This is a dataclass, which automatically generates methods like __init__ and __repr__.",
                "It serves as a standardized structure for passing individual inference requests within the offline engine."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#CompletionOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class CompletionOutput:\n  \"\"\"Container for model generation output.\n\n  Attributes:\n      index: The index of the output in the request.\n      token_ids: The token IDs of the prompt and generated output text.\n      logprobs: The log probabilities of the prompt and generated output tokens.\n      prompt_length: The number of prompt tokens.\n  \"\"\"\n\n  index: int\n  token_ids: np.ndarray\n  logprobs: np.ndarray\n  prompt_length: int",
        "analysis": {
            "module_type": "data_container",
            "purpose": "A data class that serves as a container for the output of a model generation process.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "This class is a data container and does not perform any processing."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy"
            ],
            "parameters": {
                "index": "The index of the output corresponding to the input request.",
                "token_ids": "A numpy array containing the token IDs of the prompt and the generated output text.",
                "logprobs": "A numpy array containing the log probabilities of the prompt and generated output tokens.",
                "prompt_length": "An integer representing the number of tokens in the original prompt."
            },
            "notes": [
                "This class is used to structure the final output of an inference request, combining the generated tokens, their log probabilities, and relevant metadata."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#TokenOutput",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class TokenOutput:\n  \"\"\"Container for individual token generation result.\"\"\"\n\n  token: np.ndarray\n  log_prob: np.ndarray",
        "analysis": {
            "module_type": "data_container",
            "purpose": "A data container for the result of an individual token generation step, holding the token ID and its log probability.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "dataclasses"
            ],
            "parameters": {
                "token": "An np.ndarray containing the generated token ID.",
                "log_prob": "An np.ndarray containing the log probability of the generated token."
            },
            "notes": [
                "This is a dataclasses.dataclass, which automatically generates methods like __init__ and __repr__.",
                "It is used within the `InferenceWorker` to store the output of each decoding step for a given sequence."
            ],
            "methods": {}
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#SafeThread",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class SafeThread(threading.Thread):\n  \"\"\"Thread class with exception handling to prevent silent failures.\"\"\"\n\n  def run(self):\n    try:\n      super().run()\n    except Exception as _:  # pylint: disable=broad-exception-caught\n      traceback.print_exc()\n      # Kill the process if a thread encounters an error\n      os.kill(os.getpid(), signal.SIGKILL)",
        "analysis": {
            "module_type": "exception_handling_thread",
            "purpose": "A subclass of `threading.Thread` that catches any unhandled exceptions within the thread, prints a traceback, and then terminates the entire process to prevent silent failures.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "threading.Thread",
                "traceback",
                "os",
                "signal"
            ],
            "parameters": {},
            "notes": [
                "This class is designed to make thread failures catastrophic and visible, rather than silent.",
                "It is used in the `InferenceWorker` to run the token emission loop in the background, ensuring that if that loop fails, the whole inference process stops."
            ],
            "methods": {
                "run": {
                    "purpose": "Executes the thread's target function and handles any exceptions by printing a traceback and killing the process.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Execute the parent `threading.Thread.run()` method within a try-except block.",
                        "If any `Exception` is caught, print the exception's traceback to standard error.",
                        "Kill the current process using `os.kill` with the `SIGKILL` signal."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "super().run",
                        "traceback.print_exc",
                        "os.kill",
                        "os.getpid",
                        "signal.SIGKILL"
                    ],
                    "notes": [
                        "The method catches a broad `Exception` to ensure any failure is handled.",
                        "Process termination is immediate and forceful via `SIGKILL`."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillType",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillType(Enum):\n  \"\"\"Enumeration of supported prefill processing methods.\"\"\"\n\n  DEFAULT = \"default\"\n  BATCH = \"batch\"",
        "analysis": {
            "module_type": "enumeration",
            "purpose": "Defines an enumeration for the supported prefill processing methods.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This class defines two members: 'DEFAULT' with the value 'default' and 'BATCH' with the value 'batch'.",
                "These values are used to select a prefill processing strategy within the `PrefillHelper` class."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillResult",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillResult:\n  \"\"\"Result from prefill processing operation.\"\"\"\n\n  result_tokens: engine_api.ResultTokens\n  slot: int\n  prompt_logp: None | jax.Array",
        "analysis": {
            "module_type": "data_container",
            "purpose": "A dataclass to store the results of a prefill processing operation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes instance attributes `result_tokens`, `slot`, and `prompt_logp` via the dataclass-generated `__init__` method."
            ],
            "output": {
                "shape": "An instance of the PrefillResult class."
            },
            "dependencies": [
                "dataclasses",
                "engine_api.ResultTokens",
                "jax.Array"
            ],
            "parameters": {
                "result_tokens": "An `engine_api.ResultTokens` object containing the generated tokens from the prefill step.",
                "slot": "An integer representing the specific decode slot used for this prefill operation.",
                "prompt_logp": "An optional JAX array containing the log probabilities of the input prompt tokens."
            },
            "notes": [
                "This is a dataclass, meaning methods like `__init__`, `__repr__`, etc., are automatically generated.",
                "It serves as a structured container to pass data from a prefill operation (e.g., within `PrefillHelper`) to a callback function (`prefill_done`)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#PrefillHelper",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class PrefillHelper:\n  \"\"\"Abstraction layer for different prefill processing strategies.\n\n  Provides a unified interface for both default (single-sequencse) and batch\n  (packed multi-sequence) prefill processing methods.\n  \"\"\"\n\n  def __init__(\n      self,\n      prefill_type: PrefillType,\n      engine: MaxEngine,\n      prefill_lengths: list[int],\n      batch_prefill_max_batch_size: int = 16,\n      rng=None,\n  ):\n    \"\"\"Initialize the PrefillHelper.\n\n    Args:\n        type: The type of prefill processor to use (\"default\" or \"batch\")\n        engine: The MaxEngine instance to use for prefill operations\n        prefill_lengths: list of prompt lengths to support\n        batch_prefill_max_batch_size: Maximum number of prompts in one packed\n            sequence for batch prefill\n    \"\"\"\n    self._type = prefill_type\n    self.engine = engine\n    self.prefill_lengths = sorted(prefill_lengths)\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    if prefill_type == PrefillType.DEFAULT:\n      self._processor = PrefillProcessor(engine)\n    elif prefill_type == PrefillType.BATCH:\n      self._batch_processor = BatchedPrefillProcessor(\n          engine=engine,\n          max_batch_size=batch_prefill_max_batch_size,\n          auto_layout_supported=False,\n      )\n      # Keep fallback processor for edge cases\n      self._processor = PrefillProcessor(engine)\n    else:\n      raise ValueError(f\"Invalid prefill type: {prefill_type}\")\n\n  @functools.partial(jax.jit, static_argnums=(0), donate_argnames=(\"decode_state\",))\n  def _jitted_single_prefill(\n      self, params, tokens, slot, true_length, decode_state, rng\n  ) -> tuple[jax.Array, jax.Array, DecodeState, jax.Array] | tuple[jax.Array, jax.Array, DecodeState]:\n    \"\"\"Prefill a single input.\"\"\"\n    # pylint: disable=protected-access\n    first_token, decode_state = self._processor._process(\n        params,\n        tokens,\n        slot,\n        true_length,\n        decode_state,\n        rng,\n        return_prompt_logp=True,\n    )\n    # ResultTokens, decode_state, prompt_logp\n    return (\n        first_token,\n        decode_state,\n        decode_state[\"prompt_logp\"],\n    )\n\n  def process(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      decode_slot: int,\n      input_id: int,\n      input_tokens_padded: jax.Array,\n      input_true_length: int,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Process an input through the appropriate prefill processor.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decode state\n        decode_slot: The decode slot index to use for this input\n        input_id: Unique identifier for this input\n        input_tokens_padded: Padded token array for the input\n        input_true_length: Actual length of the input before padding\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    padded_length = len(input_tokens_padded)\n    # Use default processor if configured or if input is already at max length\n    if self._type == PrefillType.DEFAULT or padded_length == self.max_prefill_length:\n      first_token, decode_state, prompt_logp = self._jitted_single_prefill(\n          model_params,\n          input_tokens_padded,\n          decode_slot,\n          input_true_length,\n          decode_state,\n          self.rng,\n      )\n      prefill_done(\n          [PrefillResult(first_token, decode_slot, prompt_logp)],\n          [input_id],\n          decode_state,\n      )\n    # Use batch processor for inputs that can benefit from prefill packing\n    elif self._type == PrefillType.BATCH:\n      self._batch_processor.process(\n          model_params,\n          decode_state,\n          decode_slot,\n          input_id,\n          input_tokens_padded[:input_true_length],\n          padded_length,\n          self.max_prefill_length,\n          prefill_done,\n          return_prompt_logp=True,\n      )\n\n  def finalize(\n      self,\n      model_params: Params,\n      decode_state: DecodeState,\n      prefill_done: Callable,\n  ) -> None:\n    \"\"\"Finalize prefill operations, flushing any pending inputs.\n\n    Args:\n        model_params: Model parameters for inference\n        decode_state: Current decoder state\n        prefill_done: Callback function called when prefill completes\n    \"\"\"\n    if self._type == PrefillType.DEFAULT:\n      # No finalization needed for default processor\n      pass\n    elif self._type == PrefillType.BATCH:\n      # Flush any remaining inputs in the batch processor\n      self._batch_processor.flush(model_params, decode_state, prefill_done, return_prompt_logp=True)",
        "analysis": {
            "module_type": "prefill_helper",
            "purpose": "Provides a unified interface to manage different prefill strategies, either processing sequences individually ('DEFAULT') or packing them into batches ('BATCH').",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes either a `PrefillProcessor` for single-sequence mode or a `BatchedPrefillProcessor` for packed multi-sequence mode based on the `prefill_type`.",
                "Provides a `process` method to route incoming sequences to the appropriate processor.",
                "Provides a `finalize` method to flush any pending sequences in batch mode."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "PrefillType",
                "MaxEngine",
                "PrefillProcessor",
                "BatchedPrefillProcessor",
                "jax"
            ],
            "parameters": {
                "prefill_type": "The prefill strategy to use, either 'DEFAULT' for single-sequence processing or 'BATCH' for packed multi-sequence processing.",
                "prefill_lengths": "A list of supported prompt lengths used for padding and bucketing.",
                "batch_prefill_max_batch_size": "The maximum number of prompts to pack into a single sequence when using the 'BATCH' prefill type."
            },
            "notes": [
                "This class acts as an abstraction layer, hiding the complexity of different prefill methods from the caller.",
                "When `prefill_type` is 'BATCH', it also instantiates a default `PrefillProcessor` as a fallback for certain edge cases (e.g., inputs that are already at the maximum prefill length)."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the PrefillHelper, selecting and creating the appropriate prefill processor(s) based on the specified prefill type.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters like `prefill_type`, `engine`, and `prefill_lengths`.",
                        "Set up a JAX random number generator key.",
                        "If `prefill_type` is 'DEFAULT', instantiate `PrefillProcessor`.",
                        "If `prefill_type` is 'BATCH', instantiate `BatchedPrefillProcessor` and a fallback `PrefillProcessor`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillType",
                        "MaxEngine",
                        "PrefillProcessor",
                        "BatchedPrefillProcessor",
                        "jax.random.PRNGKey"
                    ],
                    "notes": [
                        "Raises a ValueError for an invalid `prefill_type`."
                    ]
                },
                "_jitted_single_prefill": {
                    "purpose": "A JIT-compiled internal method to process a single input sequence for prefill.",
                    "input": {
                        "shape": "{'tokens': '[padded_length]', 'decode_state': 'PyTree'}",
                        "dtype": "{'tokens': 'integer'}"
                    },
                    "processing_steps": [
                        "Calls the internal `PrefillProcessor`'s `_process` method to get the first token and updated state, requesting prompt log probabilities."
                    ],
                    "output": {
                        "shape": "{'first_token': '[1]', 'decode_state': 'PyTree', 'prompt_logp': '[1, true_length]'}"
                    },
                    "dependencies": [
                        "PrefillProcessor._process"
                    ],
                    "notes": [
                        "Decorated with `jax.jit` for performance.",
                        "The 'decode_state' argument is donated to allow for potential in-place modification by JAX's compiler."
                    ]
                },
                "process": {
                    "purpose": "Routes an input sequence to the appropriate prefill processor (single or batched) based on configuration and input length.",
                    "input": {
                        "shape": "{'input_tokens_padded': '[padded_length]', 'decode_state': 'PyTree'}",
                        "dtype": "{'input_tokens_padded': 'integer'}"
                    },
                    "processing_steps": [
                        "Check if the prefill type is 'DEFAULT' or if the input length matches the maximum prefill length.",
                        "If true, call `_jitted_single_prefill` and then invoke the `prefill_done` callback with the results.",
                        "If false and prefill type is 'BATCH', call the `BatchedPrefillProcessor`'s `process` method, which will handle packing and eventual processing."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_jitted_single_prefill",
                        "BatchedPrefillProcessor.process",
                        "PrefillResult"
                    ],
                    "notes": [
                        "This method does not return a value directly; it triggers a callback function `prefill_done` upon completion of the prefill operation."
                    ]
                },
                "finalize": {
                    "purpose": "Flushes any pending inputs from the batched prefill processor to ensure all sequences are processed.",
                    "input": {
                        "shape": "{'decode_state': 'PyTree'}",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If the prefill type is 'BATCH', call the `BatchedPrefillProcessor`'s `flush` method."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "BatchedPrefillProcessor.flush"
                    ],
                    "notes": [
                        "This method has no effect if the prefill type is 'DEFAULT'."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#InferenceWorker",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class InferenceWorker:\n  \"\"\"\n  InferenceWorker runs continuous batching over\n       a queue of inputs.\n\n      Continuous batching workflow:\n    1. Process inputs one at a time from queue\n    2. Prefill input and insert into KV cache\n    3. Continue prefilling until enough samples for batch decode\n    4. Decode until at least one sequence completes\n    5. Refill newly available decode slots with prefill\n    6. Repeat until all sequences complete\n\n    Prefill Packing:\n        When enable_batch_prefill is True, the prefill processor\n        will pack multiple inputs into a single sequence before\n        doing the prefill.\n\n        There are multiple buckets for packed sequences, where each bucket\n        contains inputs with the same padded length. Only inputs with the same\n        padded length can be packed together.\n\n        It is important to sort the inputs by padded length so that the\n        buckets fill up quickly.\n\n        When a decode slot frees up, the prefill processor will add the\n        sequence to a bucket. If the bucket becomes full, the packed sequence\n        will be prefilled.\n\n        E.g.\n        Bucket for length 64: [...seq1, ...seq2, ...seq3, ...seq4]\n        Bucket for length 128: [...seq1, ...seq2]\n        Bucket for length 256: [...seq1]\n\n  \"\"\"\n\n  def __init__(\n      self,\n      config: MaxTextConfig,\n      params: Params | None,\n      min_decode_steps: int,\n      enable_batch_prefill: bool,\n      devices: list[Any],\n      tokenizer: Any,\n      eos_ids: list[int],\n      prefill_lengths: list[int],\n      max_decode_length: int,\n      batch_prefill_max_batch_size: int,\n      is_pw_reshard: bool = True,\n      rng: jax.random.PRNGKey = None,\n      mesh: Mesh = None,\n      debug: bool = False,\n  ):\n    \"\"\"\n    Args:\n        config: MaxText configuration\n        params: Model parameters, if None, the params will be loaded from the config\n        min_decode_steps: Minimum number of decode steps to run at once\n        enable_batch_prefill: Whether to enable batch prefill\n        devices: JAX devices to use for this worker\n        tokenizer: Tokenizer to use\n        eos_ids: End-of-sequence token IDs\n        prefill_lengths: list of supported prefill lengths\n        max_decode_length: Maximum tokens to generate per sequence\n        batch_prefill_max_batch_size: Maximum batch size for batch prefill\n        run_as_a_thread: Whether to run in a separate thread\n        rng: Random number generator key\n        mesh: JAX mesh for distributed computation\n        is_pw_reshard: Whether to use Pathways for resharding\n    \"\"\"\n    # Configurations\n    self.config = config\n    self.params = params\n    self.devices = devices\n    self.is_pw_reshard = is_pw_reshard\n    self.enable_batch_prefill = enable_batch_prefill\n    self.prefill_type = PrefillType.BATCH if enable_batch_prefill else PrefillType.DEFAULT\n    self.prefill_lengths = prefill_lengths\n    self.max_prefill_length = self.prefill_lengths[-1]\n    self.max_decode_length = max_decode_length\n    self.eos_ids = eos_ids\n    self.tokenizer = tokenizer\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.min_decode_steps = min_decode_steps\n    self.mesh = mesh\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n\n    # Inference state (initialized later)\n    self.running = False\n    self.generated_token_backlog = queue.Queue()\n    self.empty_decode_slots: list[int] = []\n    self.slot_to_id: dict[int, int] = {}\n    self.decode_state: DecodeState = None\n    self.completion_tokens_by_id: dict[Hashable, list[TokenOutput]] = {}\n    self.prompt_logprobs_by_id: dict[Hashable, list[np.ndarray]] = {}\n    self.true_lengths: dict[Hashable, int] = {}\n    # Model components (initialized later)\n    self.engine = None\n    self.decode_batch_size = None\n    self.prefill_helper = None\n    self.generate_fn = None\n\n    start_time = time.time()\n    # Initialize MaxEngine(s)\n    self.params, self.engine = self._init_engine(self.params)\n    self.tokenizer = self._init_tokenizer()\n    self.decode_batch_size = self.engine.max_concurrent_decodes\n\n    # Initialize prefill helper\n    self.prefill_helper = PrefillHelper(\n        self.prefill_type,\n        self.engine,\n        self.prefill_lengths,\n        self.batch_prefill_max_batch_size,\n        rng=self.rng,\n    )\n\n    # Initialize decode state\n    start_time_decode_state = time.time()\n    self.generate_fn = self.engine.generate\n    self.decode_state = self.engine.init_decode_state(self.rng)\n\n    if self.debug:\n      max_logging.log(f\"time taken to initialize decode_state: {time.time() - start_time_decode_state} seconds\")\n    max_logging.log(f\"Initialized Inference worker in {time.time() - start_time} seconds\")\n\n  def _init_engine(self, params):\n    \"\"\"Initialize the MaxEngine.\n\n    Args:\n        params: Model parameters\n\n    Returns:\n        tuple of (params, engine)\n    \"\"\"\n    start_time = time.time()\n    engine = MaxEngine(self.config, self.devices)\n    params = engine.load_params(params=params, rng=self.rng)\n    max_logging.log(f\"Time taken to initialize engine: {time.time() - start_time} seconds\")\n    return params, engine\n\n  def _init_tokenizer(self):\n    \"\"\"Initialize the tokenizer.\n\n    Returns:\n        Initialized tokenizer\n    \"\"\"\n    if self.eos_ids is None and self.tokenizer is None:\n      tokenizer_params = self.engine.get_tokenizer()\n      self.tokenizer = self.engine.build_tokenizer(tokenizer_params)\n    if self.eos_ids is None:\n      self.eos_ids = [self.tokenizer.eos_id]\n    return self.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n      destination_sharding: jax.sharding.NamedSharding,\n      is_pw_reshard: bool,\n  ):\n    \"\"\"Update the model parameters\"\"\"\n    if is_pw_reshard:\n      with (\n          jax.transfer_guard_device_to_host(\"disallow_explicit\"),\n          jax.transfer_guard_host_to_device(\"disallow_explicit\"),\n      ):\n        self.params = pathways_reshard.reshard(params, destination_sharding, cache_resharding_plans=True)\n    else:\n      self.params = jax.device_put(params, destination_sharding)\n\n  def run_inference(self, data: list[InputData], rng=None):\n    \"\"\"Start the inference process.\n\n    Args:\n        data: list of InputData objects containing input sequences\n        rng: Random number generator key. If None, the previous key will be used.\n    \"\"\"\n\n    # Reset rng\n    if rng is not None:\n      self.rng = rng\n\n    # Reset state for new inference run\n    self.completion_tokens_by_id = defaultdict(list)\n    self.prompt_logprobs_by_id = defaultdict(list)\n    self.empty_decode_slots = list(range(self.decode_batch_size))\n    self.slot_to_id = {}\n    self.running = True\n    self.true_lengths = {input.id: input.true_length for input in data}\n\n    max_logging.log(\"Continuous batching started\")\n\n    self._run_continous_batching(data)\n\n    return self._build_final_outputs(data)\n\n  def _run_continous_batching(\n      self,\n      data: list[InputData],\n  ):\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects containing input sequences\n    \"\"\"\n\n    # Start token emission thread\n    token_emission_thread = SafeThread(\n        target=functools.partial(\n            self.background_token_emission,\n        ),\n        name=\"token_emission\",\n    )\n    token_emission_thread.start()\n\n    # Process each input\n    for row in data:\n      # 1. Wait for an empty slot\n      while not self.empty_decode_slots:\n        self.decode()\n\n      # 2. Get an available slot\n      slot = self.empty_decode_slots.pop()\n      # 3. Prefill and insert kv cache\n      self.prefill_helper.process(\n          model_params=self.params,\n          decode_state=self.decode_state,\n          decode_slot=slot,\n          input_id=int(row.id),\n          input_tokens_padded=row.tokens,\n          input_true_length=row.true_length,\n          prefill_done=self.prefill_done,\n      )\n\n    # 4. Flush any pending inputs in batch prefill mode\n    self.prefill_helper.finalize(self.params, self.decode_state, self.prefill_done)\n\n    # 5. Continue decoding until all sequences are complete\n    while not all(value is None for value in self.slot_to_id.values()):\n      self.decode()\n\n    # Wait for detokenization to complete\n    self.running = False\n    max_logging.log(\n        f\"Inference worker: joining token emission thread. \"\n        f\"There are {self.generated_token_backlog.qsize()} elements in the backlog\"\n    )\n    start_time = time.time()\n    with jax.profiler.TraceAnnotation(\"Flushing token emission thread\"):\n      token_emission_thread.join()\n    max_logging.log(f\"Inference worker: token emission thread joined in {time.time() - start_time} seconds\")\n\n  def _build_final_outputs(self, input_data: list[InputData]) -> list[CompletionOutput]:\n    \"\"\"Build the final list of CompletionOutput.\"\"\"\n\n    with jax.profiler.TraceAnnotation(\"offline_engine.batch_inference.return_final_output\"):\n      completion_outputs = []\n      for row in input_data:\n        input_id = row.id\n        prompt_length = row.true_length\n        prompt_tokens = row.tokens[: row.true_length].squeeze()\n        completion_tokens = np.array(\n            [token_output.token for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        logprobs = np.array(\n            [token_output.log_prob.flatten() for token_output in self.completion_tokens_by_id[input_id]]\n        ).flatten()\n        prompt_logprobs = self.prompt_logprobs_by_id[input_id].flatten()\n        completion_outputs.append(\n            CompletionOutput(\n                index=input_id,\n                prompt_length=prompt_length,\n                token_ids=np.concatenate(\n                    (\n                        prompt_tokens,\n                        completion_tokens,\n                    )\n                ),\n                logprobs=np.concatenate(\n                    (\n                        prompt_logprobs,\n                        logprobs,\n                    )\n                ),\n            )\n        )\n    return completion_outputs\n\n  def prefill_done(self, prefill_result: list[PrefillResult], prompt_ids: list[any], decode_state: DecodeState):\n    \"\"\"Callback function called when prefill completes.\n    This function adds the prefill tokens to the detokenization queue,\n    which manages the token emission and decode slot evictions.\n\n    Args:\n        prefill_result: list of (token, slot) tuples\n        prompt_ids: list of prompt IDs\n        decode_state: Updated decode state\n    \"\"\"\n    # Update decode state\n    self.decode_state = decode_state\n    # Process each prefill result\n    for i, result in enumerate(prefill_result):\n      input_id = prompt_ids[i]\n      result_tokens = result.result_tokens\n      slot = result.slot\n      prompt_logp = result.prompt_logp\n      true_length = self.true_lengths[input_id]\n\n      self.slot_to_id[slot] = input_id\n\n      # Add token to detokenization queue\n      start_time = time.time()\n\n      with jax.profiler.TraceAnnotation(\"convert_to_numpy\"):\n        first_token = np.array(result_tokens.data[:, 0])\n        log_prob = np.array(result_tokens.log_prob)\n        prompt_logp = np.array(prompt_logp)[:, :true_length]\n\n      if self.debug:\n        max_logging.log(f\"Inference worker: convert to numpy in Prefill in {time.time() - start_time} seconds\")\n      self.generated_token_backlog.put_nowait((first_token, log_prob, True, prompt_ids[i], slot, prompt_logp))\n\n  def decode(self):\n    \"\"\"Run decode steps on current decoder state.\n\n    Performs `self.min_decode_steps` decode operations\n    and puts results in the detokenization queue.\n    \"\"\"\n\n    buffer = []\n    for _ in range(self.min_decode_steps):\n      # Generate next tokens\n      self.decode_state, result_tokens, log_prob = self._jitted_generate_fn(self.params, self.decode_state, self.rng)\n      # Add token to detokenization queue\n      start_time = time.time()\n      with jax.profiler.TraceAnnotation(\"convert_to_numpy\"):\n        result_tokens = np.array(result_tokens)\n        log_prob = np.array(log_prob)\n\n      if self.debug:\n        max_logging.log(f\"Inference worker: convert to numpy \" f\"in Decode in {time.time() - start_time} seconds\")\n\n      buffer.append((result_tokens, log_prob))\n\n    # Add results to detokenization queue\n    self.generated_token_backlog.put_nowait(\n        (\n            [result_token for result_token, _ in buffer],\n            [log_prob for _, log_prob in buffer],\n            False,\n            0,\n            0,\n            None,\n        )\n    )\n\n  @functools.partial(jax.jit, static_argnums=(0,), donate_argnums=(2,))\n  def _jitted_generate_fn(self, params, decode_state, rng):\n    decode_state, result_tokens = self.engine.generate(params, decode_state, rng=rng)\n    return decode_state, result_tokens.data[:, 0], result_tokens.log_prob\n\n  def background_token_emission(self):\n    \"\"\"Emit tokens and manage decode slots.\n\n    Runs in a background thread to process tokens from\n    the backlog, emit tokens, and free up\n    decode slots when sequences complete.\n    \"\"\"\n    max_logging.log(\"Inference worker: starting background token emission thread\")\n    while self.running or not self.generated_token_backlog.empty():\n      newly_empty = []\n\n      # Get next item from queue with timeout\n      try:\n        result_tokens, log_prob, is_first_token, row_id, slot, prompt_logp = self.generated_token_backlog.get(\n            timeout=0.01\n        )\n      except queue.Empty:\n        if not self.running:\n          break\n        continue\n\n      # Process generated tokens\n      start_time = time.time()\n      if is_first_token:\n        should_terminate = self.emit_token(row_id, int(result_tokens), log_prob, prompt_logp=prompt_logp)\n        if should_terminate:\n          newly_empty.append(slot)\n      else:\n        for decode_step in range(self.min_decode_steps):\n          for slot, id_ in self.slot_to_id.items():\n            if id_ is None:\n              continue\n            log_prob_at_slot = log_prob[decode_step][slot]\n            result_tokens_at_slot = result_tokens[decode_step][slot]\n            should_terminate = self.emit_token(id_, int(result_tokens_at_slot), log_prob_at_slot)\n            if should_terminate:\n              newly_empty.append(slot)\n\n      # Update decode slots\n      for slot in newly_empty:\n        self.slot_to_id[slot] = None\n        if slot not in self.empty_decode_slots:\n          self.empty_decode_slots.append(slot)\n      end_time = time.time()\n      if self.debug:\n        max_logging.log(f\"Inference worker: token emission in {end_time - start_time} seconds\")\n\n  def emit_token(\n      self,\n      prompt_id,\n      result_token: int,\n      log_prob: float,\n      prompt_logp: np.ndarray = None,\n  ):\n    \"\"\"Adds the token to the results for the specified prompt ID and\n    determines if generation should terminate.\n\n    Args:\n        prompt_id: ID of the prompt\n        token: Token to emit\n\n    Returns:\n        True if this token signals the end of generation, False otherwise\n    \"\"\"\n    # Return if already reached max decode length\n    if len(self.completion_tokens_by_id[prompt_id]) == self.max_decode_length:\n      return True\n\n    # Return if already reached eos\n    if (\n        len(self.completion_tokens_by_id[prompt_id]) > 0\n        and self.completion_tokens_by_id[prompt_id][-1].token in self.eos_ids\n    ):\n      return True\n\n    index = len(self.completion_tokens_by_id[prompt_id])\n    if prompt_logp is not None:\n      self.prompt_logprobs_by_id[prompt_id] = [prompt_logp]\n    self.completion_tokens_by_id[prompt_id].append(TokenOutput(np.array(result_token), np.array(log_prob)))\n    return (result_token in self.eos_ids) or (index + 1 == self.max_decode_length)",
        "analysis": {
            "module_type": "inference_worker",
            "purpose": "Manages the continuous batching inference process for a set of input sequences, handling prefill, decoding, and token emission.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes a MaxEngine, tokenizer, and prefill helper upon creation.",
                "Receives a list of `InputData` objects via the `run_inference` method.",
                "Starts a background thread for token emission.",
                "Iteratively processes each input by waiting for a free decode slot, then performing a prefill operation.",
                "Continuously calls a decode function to generate new tokens for all active sequences in the batch.",
                "The background thread processes generated tokens, checks for completion (EOS or max length), and frees up decode slots.",
                "Once all sequences are complete, it joins the background thread.",
                "Builds and returns a list of `CompletionOutput` objects."
            ],
            "output": {
                "shape": "A list of `CompletionOutput` objects."
            },
            "dependencies": [
                "MaxEngine",
                "PrefillHelper",
                "SafeThread",
                "InputData",
                "CompletionOutput",
                "PrefillResult",
                "jax",
                "numpy",
                "queue"
            ],
            "parameters": {
                "config": "MaxText configuration object.",
                "params": "Model parameters, which can be loaded from the config if None.",
                "min_decode_steps": "Minimum number of decode steps to run at once before checking for completion.",
                "enable_batch_prefill": "A boolean flag to enable or disable prefill packing.",
                "max_decode_length": "Maximum number of tokens to generate per sequence.",
                "prefill_lengths": "A list of supported padded lengths for prefill.",
                "batch_prefill_max_batch_size": "Maximum batch size for batch prefill when enabled."
            },
            "notes": [
                "Implements a continuous batching workflow to maximize hardware utilization.",
                "Uses a background thread (`background_token_emission`) to decouple token generation (on device) from completion checking and state management (on CPU).",
                "Supports prefill packing (batch prefill) to combine multiple short inputs into a single prefill operation for efficiency."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the InferenceWorker, setting up the configuration, model engine, prefill helper, and initial decode state.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters from arguments.",
                        "Call `_init_engine` to initialize `MaxEngine` and load model parameters.",
                        "Call `_init_tokenizer` to set up the tokenizer and EOS IDs.",
                        "Initialize `PrefillHelper` for either default or batch prefill.",
                        "Initialize the `decode_state` using `engine.init_decode_state`."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "MaxEngine",
                        "PrefillHelper",
                        "PrefillType",
                        "jax.random.PRNGKey",
                        "_init_engine",
                        "_init_tokenizer"
                    ],
                    "notes": [
                        "Sets up all necessary components for inference but does not start the process."
                    ]
                },
                "_init_engine": {
                    "purpose": "Initializes the MaxEngine and loads the model parameters.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Instantiate `MaxEngine` with the config and devices.",
                        "Call `engine.load_params` to load or initialize model parameters."
                    ],
                    "output": {
                        "shape": "A tuple of (params, engine)."
                    },
                    "dependencies": [
                        "MaxEngine"
                    ],
                    "notes": []
                },
                "_init_tokenizer": {
                    "purpose": "Initializes the tokenizer if not provided, and sets the end-of-sequence token IDs.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If tokenizer is not provided, get tokenizer parameters from the engine.",
                        "Build the tokenizer using `engine.build_tokenizer`.",
                        "If `eos_ids` are not provided, get them from the tokenizer."
                    ],
                    "output": {
                        "shape": "The initialized tokenizer instance."
                    },
                    "dependencies": [
                        "MaxEngine"
                    ],
                    "notes": [
                        "Ensures that both `self.tokenizer` and `self.eos_ids` are available for the worker."
                    ]
                },
                "update_params": {
                    "purpose": "Updates the model parameters, potentially resharding them across devices.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "If `is_pw_reshard` is True, use `pathways_reshard.reshard` to update and reshard parameters.",
                        "Otherwise, use `jax.device_put` to place parameters on the correct devices with the specified sharding."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "pathways_reshard",
                        "jax.device_put",
                        "jax.sharding.NamedSharding"
                    ],
                    "notes": [
                        "Updates `self.params` in place."
                    ]
                },
                "run_inference": {
                    "purpose": "Starts and manages the entire inference process for a batch of input data.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Reset the random number generator if a new one is provided.",
                        "Reset the internal state (completion tokens, slots, etc.).",
                        "Call `_run_continous_batching` to perform the main inference loop.",
                        "Call `_build_final_outputs` to format and return the results."
                    ],
                    "output": {
                        "shape": "A list of `CompletionOutput` objects."
                    },
                    "dependencies": [
                        "_run_continous_batching",
                        "_build_final_outputs",
                        "InputData",
                        "CompletionOutput"
                    ],
                    "notes": [
                        "This is the main public entry point for running inference with the worker."
                    ]
                },
                "_run_continous_batching": {
                    "purpose": "Executes the core continuous batching loop, managing prefill and decode steps.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Start the `background_token_emission` thread.",
                        "Iterate through each input in `data`.",
                        "For each input, wait for an empty decode slot (calling `decode()` if necessary).",
                        "Assign the input to a slot and process it using `prefill_helper.process`.",
                        "After all inputs are submitted, call `prefill_helper.finalize` to flush any pending prefills.",
                        "Continuously call `decode()` until all sequences are complete.",
                        "Stop the `running` flag and wait for the `background_token_emission` thread to join."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "SafeThread",
                        "PrefillHelper",
                        "decode",
                        "prefill_done"
                    ],
                    "notes": [
                        "Implements the continuous batching logic described in the class docstring."
                    ]
                },
                "_build_final_outputs": {
                    "purpose": "Constructs the final list of CompletionOutput objects from the generated tokens and log probabilities.",
                    "input": {
                        "shape": "A list of `InputData` objects.",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through the original `input_data`.",
                        "For each input, retrieve the prompt tokens, generated completion tokens, and their corresponding log probabilities from internal state.",
                        "Concatenate prompt and completion data into token_ids and logprobs arrays.",
                        "Create a `CompletionOutput` object and append it to a list."
                    ],
                    "output": {
                        "shape": "A list of `CompletionOutput` objects."
                    },
                    "dependencies": [
                        "CompletionOutput",
                        "InputData",
                        "numpy"
                    ],
                    "notes": []
                },
                "prefill_done": {
                    "purpose": "A callback function that processes the results of a prefill operation.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Update the worker's `decode_state`.",
                        "For each result in `prefill_result`, map the slot to the prompt ID.",
                        "Convert JAX arrays for the first token and log probabilities to NumPy arrays.",
                        "Put the processed prefill result onto the `generated_token_backlog` queue for the emission thread."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "PrefillResult",
                        "queue.Queue",
                        "numpy"
                    ],
                    "notes": [
                        "Acts as a bridge between the asynchronous prefill operation and the token emission/management logic."
                    ]
                },
                "decode": {
                    "purpose": "Performs a batch of decoding steps and places the results in a queue for processing.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Loop `self.min_decode_steps` times.",
                        "In each loop, call the JIT-compiled `_jitted_generate_fn` to generate the next token for all active sequences.",
                        "Convert the resulting tokens and log probabilities to NumPy arrays and buffer them.",
                        "After the loop, put the entire buffer of results onto the `generated_token_backlog` queue."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "_jitted_generate_fn",
                        "queue.Queue",
                        "numpy"
                    ],
                    "notes": [
                        "This method performs the autoregressive generation step for all sequences currently in the KV cache."
                    ]
                },
                "_jitted_generate_fn": {
                    "purpose": "A JIT-compiled function to perform a single token generation step using the MaxEngine.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Call `self.engine.generate` with the current parameters, state, and RNG.",
                        "Extract and return the updated state, the generated token data, and the log probabilities."
                    ],
                    "output": {
                        "shape": "A tuple of (`decode_state`, `result_tokens`, `log_prob`)."
                    },
                    "dependencies": [
                        "MaxEngine.generate",
                        "jax.jit"
                    ],
                    "notes": [
                        "Decorated with `jax.jit` for performance, with `decode_state` marked for donation to save memory."
                    ]
                },
                "background_token_emission": {
                    "purpose": "Runs in a background thread to process generated tokens, manage sequence completion, and free up decode slots.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Loop while the worker is running or the backlog queue is not empty.",
                        "Get a batch of generated tokens from `generated_token_backlog`.",
                        "If it's a first token (from prefill), call `emit_token` once.",
                        "If it's from a decode step, iterate through all active slots and call `emit_token` for each.",
                        "If `emit_token` indicates a sequence has terminated, record its slot as newly empty.",
                        "After processing the batch, update `slot_to_id` and `empty_decode_slots` with the newly freed slots."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "queue.Queue",
                        "emit_token"
                    ],
                    "notes": [
                        "This decouples the GPU-intensive generation from the CPU-bound logic of checking for EOS tokens and managing state."
                    ]
                },
                "emit_token": {
                    "purpose": "Appends a generated token to a sequence's output and checks for termination conditions.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if the maximum decode length has been reached.",
                        "Check if the previous token was an EOS token.",
                        "If not terminated, append the new `TokenOutput` to `completion_tokens_by_id`.",
                        "If `prompt_logp` is provided (for the first token), store it in `prompt_logprobs_by_id`.",
                        "Return True if the new token is an EOS token or if the max decode length is now met, otherwise False."
                    ],
                    "output": {
                        "shape": "A boolean indicating if the sequence should terminate."
                    },
                    "dependencies": [
                        "TokenOutput",
                        "numpy"
                    ],
                    "notes": [
                        "This is the final step where individual token results are stored and completion is checked."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/offline_engine.py#OfflineEngine",
        "file_path": "src/MaxText/inference/offline_engine.py",
        "code_block": "class OfflineEngine:\n  \"\"\"Class for handling offline inference on batches of inputs.\"\"\"\n\n  def __init__(\n      self,\n      config: Any,\n      params: None | Params = None,\n      enable_batch_prefill: bool = False,\n      min_decode_steps: int = 10,\n      tokenizer: Any = None,\n      eos_ids: list[int] | None = None,\n      prefill_lengths: list[int] | str = \"auto\",\n      batch_prefill_max_batch_size: int = 16,\n      mesh: Mesh = None,\n      rng: jax.random.PRNGKey = None,\n      debug: bool = False,\n  ):\n    \"\"\"Initialize the OfflineEngine.\n\n    Args:\n        config: The MaxText config object which will be used to\n          create MaxEngine instance(s).\n        params: Model parameters (loaded from engine if None)\n        enable_batch_prefill: Whether to use prefill packing.\n            config.scan_layers must be False if this is True\n        min_decode_steps: Number of decode steps to perform at a time,\n            before checking for completion.\n        eos_ids: list of EOS token IDs for checking sequence completion.\n          If None, the tokenizer's EOS token will be used.\n        tokenizer: Tokenizer instance for encoding/decoding text. If None,\n          will be created using the config if eos_ids is not provided.\n        prefill_lengths: list of expected prefill lengths, or \"auto\" to\n            automatically determine appropriate lengths from the engine\n            config. Input sequences will be padded to the nearest length\n            in this list.\n        batch_prefill_max_batch_size: Maximum number of inputs to pack\n          into a single prefill. This is only used when enable_batch_prefill\n          is True.\n        mesh: JAX Mesh object. Use this\n          argument if you want to use only some of the devices for OfflineEngine and\n          reserve the rest for other tasks. If None, OfflineEngine will create the mesh\n          automatically.\n        rng: Random number generator key. If None, a new key will be created.\n    \"\"\"\n    max_logging.log(\"Initializing OfflineEngine\")\n    # Configurations\n    self.config = config\n    self.params = params\n    self.min_decode_steps = min_decode_steps\n    self.enable_batch_prefill = enable_batch_prefill\n    self.mesh = mesh\n    self.tokenizer = tokenizer\n    self.eos_ids = eos_ids\n    self.prefill_lengths = prefill_lengths\n    self.batch_prefill_max_batch_size = batch_prefill_max_batch_size\n    self.max_prefill_length = self.config.max_prefill_predict_length\n    self.max_decode_length = self.config.max_target_length - self.max_prefill_length\n    self.rng = jax.random.PRNGKey(0) if rng is None else rng\n    self.debug = debug\n    self._validate_config()\n\n    # Create prefill buckets: [0, 64], (64, 128], (128, 256], ..., [max_length//2, max_length]\n    if prefill_lengths == \"auto\":\n      self.prefill_lengths = [2**i for i in range(6, max(6, (self.max_prefill_length - 1).bit_length()) + 1)]\n    else:\n      self.prefill_lengths = sorted(prefill_lengths)\n\n    # Create meshes\n    if not self.mesh:\n      self.mesh = OfflineEngine.create_mesh(jax.devices(), self.config)\n\n    self.worker = InferenceWorker(\n        config=self.config,\n        params=self.params,\n        min_decode_steps=self.min_decode_steps,\n        enable_batch_prefill=self.enable_batch_prefill,\n        mesh=self.mesh,\n        devices=self.mesh.devices.flatten(),\n        tokenizer=self.tokenizer,\n        eos_ids=self.eos_ids,\n        prefill_lengths=self.prefill_lengths,\n        max_decode_length=self.max_decode_length,\n        batch_prefill_max_batch_size=self.batch_prefill_max_batch_size,\n        rng=self.rng,\n        debug=self.debug,\n    )\n\n    self.tokenizer = self.worker.tokenizer\n\n  def update_params(\n      self,\n      params: Params,\n      parition_spec: PartitionSpec,\n      is_pw_reshard: bool,\n  ):\n    \"\"\"Update model weights.\"\"\"\n    self.worker.update_params(\n        params,\n        jax.tree_util.tree_map(\n            lambda ps: jax.sharding.NamedSharding(self.mesh, ps),\n            parition_spec,\n        ),\n        is_pw_reshard,\n    )\n\n  def batch_inference(\n      self,\n      data: list[InputData] | list[jax.Array] | list[np.ndarray],\n      desc: str = \"\",\n      rng=None,\n  ) -> list[CompletionOutput]:\n    \"\"\"Run inference on a batch of inputs.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays.\n            If input is JAX or numpy array, it must not contain padding tokens.\n        desc: Description string for logging\n        rng: Random number generator key. If None, the previous key will be used.\n\n    Returns:\n        list of CompletionOutput objects, one for each input in data\n    \"\"\"\n    data = self.prepare_data(data)\n\n    return self.worker.run_inference(data, rng)\n\n  def prepare_data(self, data: list[InputData | jax.Array | np.ndarray]) -> list[InputData]:\n    \"\"\"Pad and if batch prefill is enabled, sort data by length.\n\n    Args:\n        data: list of InputData objects, or JAX or numpy arrays\n\n    Returns:\n        list of prepared InputData objects\n    \"\"\"\n    # Convert JAX arrays to numpy arrays\n    if isinstance(data[0], jax.Array):\n      data = [np.array(array) for array in data]\n\n    # Convert numpy arrays to InputData objects\n    if isinstance(data[0], np.ndarray):\n      max_logging.log(\n          \"When you provide JAX/numpy arrays to Offline Engine, \"\n          \"make sure that the arrays are not padded with padding tokens.\"\n      )\n      data = [InputData(id=str(i), tokens=array, true_length=len(array)) for i, array in enumerate(data)]\n\n    # Make sure all data id is unique\n    if len(data) != len({item.id for item in data}):\n      raise ValueError(\"All data ids must be unique\")\n\n    data = self.pad_data(data)\n\n    if self.enable_batch_prefill:\n      return sorted(data, key=lambda x: x.tokens.shape[0])\n\n    return data\n\n  def pad_data(self, data: list[InputData]) -> list[InputData]:\n    \"\"\"For each input, pad it to the next length in self.prefill_lengths\n    that is greater than or equal to its true length.\n\n    Args:\n        data: list of InputData objects\n\n    Returns:\n        list of padded InputData objects\n    \"\"\"\n    padded_data = []\n\n    for item in data:\n      # Find the smallest prefill length that can accommodate this input\n      target_length = None\n      for length in self.prefill_lengths:\n        if length >= item.true_length:\n          target_length = length\n          break\n\n      # If no suitable length found, use the maximum prefill length\n      if target_length is None:\n        target_length = self.max_prefill_length\n\n      # Pad or truncate as needed\n      if len(item.tokens) < target_length:\n        # Pad with zeros\n        padded_tokens = np.zeros(target_length, dtype=item.tokens.dtype)\n        padded_tokens[: item.true_length] = item.tokens[: item.true_length]\n      else:\n        # Input is too long, truncate to max_prefill_length\n        padded_tokens = item.tokens[:target_length]\n\n      # Create new InputData with padded tokens\n      padded_data.append(InputData(id=item.id, tokens=padded_tokens, true_length=item.true_length))\n\n    return padded_data\n\n  @staticmethod\n  def create_mesh(devices, config):\n    \"\"\"Create data parallelism meshes for each Inference worker.\"\"\"\n    ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), len(devices), \"ICI\")\n    devices_array = mesh_utils.create_device_mesh(\n        ici_parallelism,\n        devices,\n        contiguous_submeshes=False,\n        allow_split_physical_axes=config.allow_split_physical_axes or False,\n    )\n    mesh = Mesh(devices_array.reshape(ici_parallelism), config.mesh_axes)\n    return mesh\n\n  def _validate_config(self):\n    \"\"\"Validate configuration parameters and check for incompatible settings.\"\"\"\n    if not self.config.return_log_prob:\n      raise ValueError(\"return_log_prob must be True when using OfflineEngine\")\n    if self.enable_batch_prefill and self.config.scan_layers:\n      raise ValueError(\"scan_layers must be False if enable_batch_prefill is True\")\n\n    if self.max_decode_length <= 0:\n      raise ValueError(\"Make sure max_target_length - max_prefill_predict_length is greater than 0\")\n    if self.config.scan_layers:\n      max_logging.log(\n          \"WARNING: scan_layers=True will result in slow step time. \" \"It is recommended for debugging purposes only.\"\n      )",
        "analysis": {
            "module_type": "offline_inference_engine",
            "purpose": "A high-level class for handling offline inference on batches of inputs, managing data preparation, prefilling, and decoding.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initializes configuration, JAX mesh, and an `InferenceWorker`.",
                "Receives a batch of inputs via the `batch_inference` method.",
                "Prepares the data by converting, padding, and optionally sorting it using `prepare_data`.",
                "Delegates the core continuous batching and inference logic to the `InferenceWorker`.",
                "Returns the final generated sequences and log probabilities."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "InferenceWorker",
                "jax",
                "numpy",
                "InputData",
                "CompletionOutput",
                "max_logging",
                "max_utils"
            ],
            "parameters": {
                "config": "The MaxText config object which will be used to create MaxEngine instance(s).",
                "params": "Model parameters (loaded from engine if None).",
                "enable_batch_prefill": "Whether to use prefill packing. config.scan_layers must be False if this is True.",
                "min_decode_steps": "Number of decode steps to perform at a time, before checking for completion.",
                "prefill_lengths": "List of expected prefill lengths for padding/bucketing, or 'auto' to automatically determine them.",
                "batch_prefill_max_batch_size": "Maximum number of inputs to pack into a single prefill when enable_batch_prefill is True.",
                "mesh": "Optional JAX Mesh object. If None, one is created automatically."
            },
            "notes": [
                "This class acts as a user-friendly facade over the more complex `InferenceWorker`.",
                "It automatically handles the creation of a JAX mesh if one is not provided.",
                "Configuration is validated upon initialization to prevent incompatible settings."
            ],
            "methods": {
                "__init__": {
                    "purpose": "Initializes the OfflineEngine, sets up configuration, creates a JAX mesh, and instantiates an InferenceWorker.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Store configuration parameters from arguments.",
                        "Call `_validate_config` to check for incompatible settings.",
                        "Determine prefill length buckets, either from user input or automatically if set to 'auto'.",
                        "Create a JAX `Mesh` using `create_mesh` if one is not provided.",
                        "Instantiate an `InferenceWorker` with the prepared configuration to handle the core logic.",
                        "Set the engine's tokenizer to the one initialized by the worker."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker",
                        "jax",
                        "max_logging",
                        "OfflineEngine.create_mesh",
                        "OfflineEngine._validate_config"
                    ],
                    "notes": [
                        "The `prefill_lengths` can be set to 'auto' to automatically create power-of-two buckets up to the `max_prefill_predict_length`."
                    ]
                },
                "update_params": {
                    "purpose": "Updates the model weights in the underlying InferenceWorker.",
                    "input": {
                        "shape": "params: Pytree, parition_spec: Pytree of PartitionSpec",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Map the `parition_spec` to a `jax.sharding.NamedSharding` object using the engine's mesh.",
                        "Call `self.worker.update_params` with the new parameters and sharding information."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "InferenceWorker.update_params",
                        "jax.sharding.NamedSharding",
                        "jax.tree_util.tree_map"
                    ],
                    "notes": [
                        "This method allows for hot-swapping model parameters without re-initializing the entire engine."
                    ]
                },
                "batch_inference": {
                    "purpose": "Runs inference on a batch of inputs by preparing the data and delegating to the InferenceWorker.",
                    "input": {
                        "shape": "list[InputData | jax.Array | np.ndarray]",
                        "dtype": "int (for token IDs)"
                    },
                    "processing_steps": [
                        "Call `self.prepare_data` to process the input list.",
                        "Call `self.worker.run_inference` with the prepared data and an optional RNG key.",
                        "Return the results from the worker."
                    ],
                    "output": {
                        "shape": "list[CompletionOutput]"
                    },
                    "dependencies": [
                        "OfflineEngine.prepare_data",
                        "InferenceWorker.run_inference"
                    ],
                    "notes": [
                        "The input can be raw JAX/NumPy arrays of token IDs, which will be converted to `InputData` objects internally."
                    ]
                },
                "prepare_data": {
                    "purpose": "Converts various input formats to a list of padded `InputData` objects, and sorts them if batch prefilling is enabled.",
                    "input": {
                        "shape": "list[InputData | jax.Array | np.ndarray]",
                        "dtype": "int"
                    },
                    "processing_steps": [
                        "If input is `jax.Array`, convert to `np.ndarray`.",
                        "If input is `np.ndarray`, convert each array to an `InputData` object.",
                        "Validate that all `InputData` IDs are unique.",
                        "Call `self.pad_data` to pad all inputs to the appropriate bucket size.",
                        "If `enable_batch_prefill` is True, sort the padded data by token length.",
                        "Return the prepared list of `InputData` objects."
                    ],
                    "output": {
                        "shape": "list[InputData]"
                    },
                    "dependencies": [
                        "InputData",
                        "OfflineEngine.pad_data",
                        "numpy"
                    ],
                    "notes": [
                        "Sorting by length when batch prefilling is enabled helps to efficiently pack inputs into batches."
                    ]
                },
                "pad_data": {
                    "purpose": "Pads or truncates each input sequence to the nearest pre-defined bucket length.",
                    "input": {
                        "shape": "list[InputData]",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Iterate through each `InputData` item.",
                        "Find the smallest prefill length from `self.prefill_lengths` that is >= the item's true length.",
                        "If no suitable length is found, use the maximum prefill length.",
                        "Create a new zero-padded `np.ndarray` of the target length.",
                        "Copy the original tokens into the new array, truncating if necessary.",
                        "Create a new `InputData` object with the padded tokens.",
                        "Return the list of new, padded `InputData` objects."
                    ],
                    "output": {
                        "shape": "list[InputData]"
                    },
                    "dependencies": [
                        "InputData",
                        "numpy"
                    ],
                    "notes": [
                        "This bucketing strategy is crucial for efficient prefilling, especially when packing is enabled."
                    ]
                },
                "create_mesh": {
                    "purpose": "A static method to create a JAX device mesh based on the configuration.",
                    "input": {
                        "shape": "devices: list[jax.Device], config: MaxTextConfig",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Determine ICI parallelism using `max_utils.fill_unspecified_mesh_axes`.",
                        "Create a device array using `mesh_utils.create_device_mesh`.",
                        "Instantiate and return a `jax.sharding.Mesh` object."
                    ],
                    "output": {
                        "shape": "jax.sharding.Mesh"
                    },
                    "dependencies": [
                        "jax.sharding.Mesh",
                        "jax.experimental.mesh_utils",
                        "max_utils"
                    ],
                    "notes": [
                        "This is a helper utility for setting up the distributed computation environment."
                    ]
                },
                "_validate_config": {
                    "purpose": "Validates configuration parameters and raises errors for incompatible settings.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Check if `config.return_log_prob` is True.",
                        "Check for the incompatible combination of `enable_batch_prefill=True` and `config.scan_layers=True`.",
                        "Check if the calculated `max_decode_length` is positive.",
                        "Log a warning if `config.scan_layers` is True."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "max_logging"
                    ],
                    "notes": [
                        "This is an internal method called during initialization to ensure the engine is configured correctly."
                    ]
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "module_type": "validation_function",
            "purpose": "Validates specific configuration settings to ensure they are compatible with the interleaved inference script.",
            "input": {
                "shape": "N/A",
                "dtype": "pyconfig.Config object"
            },
            "processing_steps": [
                "Assert that `config.load_full_state_path` is an empty string, as decoding does not operate on full states.",
                "Assert that the number of initial prefill streams is greater than 0 and less than or equal to the total number of streams."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "_INITIAL_PREFILL_STREAMS",
                "_NUM_STREAMS"
            ],
            "parameters": {
                "config.load_full_state_path": "The path to a full training state checkpoint. This must be empty for the script to run.",
                "_INITIAL_PREFILL_STREAMS": "A global constant defining how many streams to prefill before starting generation.",
                "_NUM_STREAMS": "A global constant defining the total number of concurrent streams."
            },
            "notes": [
                "This function does not return any value. It raises an AssertionError if any of the validation checks fail.",
                "It is a prerequisite check to prevent runtime errors or incorrect behavior during the inference process."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/decode_multi.py#main",
        "file_path": "src/MaxText/inference/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "module_type": "interleaved_inference_runner",
            "purpose": "Executes a multi-stream, interleaved inference process, demonstrating continuous batching by prefilling new requests while generating tokens for active ones.",
            "input": {
                "shape": "N/A",
                "dtype": "Sequence[str]"
            },
            "processing_steps": [
                "Initialize JAX, environment, and configuration from command-line arguments (`argv`).",
                "Instantiate the `maxengine.MaxEngine` with the configuration.",
                "Load model parameters using `engine.load_params`.",
                "Tokenize the input prompt from `config.prompt`.",
                "Initialize the decode state using `engine.init_decode_state`.",
                "Execute an initial prefill phase for `_INITIAL_PREFILL_STREAMS` streams using `engine.prefill`.",
                "Insert the initial prefill results into the decode state using `engine.insert`, activating those streams.",
                "Enter a generation loop that runs until `max_target_length` is reached.",
                "Inside the loop, generate one token for all active streams simultaneously using `engine.generate`.",
                "Store the generated tokens and check if any streams have completed.",
                "For completed streams, release their resources using `engine.release_pages`.",
                "If there is capacity and more streams are pending, prefill and insert a new stream into an available slot.",
                "After the loop, decode the generated token sequences for each stream.",
                "Print the final results and perform an assertion check on the output of the first stream."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "max_utils",
                "maxengine.MaxEngine",
                "pyconfig",
                "uuid"
            ],
            "parameters": {
                "config.prompt": "The initial text prompt for generation.",
                "config.max_prefill_predict_length": "The maximum length of the input prompt.",
                "config.max_target_length": "The total maximum length of the generated sequence (prompt + new tokens).",
                "config.per_device_batch_size": "The batch size per device, used to calculate the total batch size and available slots.",
                "config.autoregressive_decode_assert": "A string used to validate the beginning of the generated output for the first stream.",
                "_NUM_STREAMS": "A global constant defining the total number of generation streams to process.",
                "_INITIAL_PREFILL_STREAMS": "A global constant defining how many streams to prefill before starting the generation loop."
            },
            "notes": [
                "This function simulates a continuous batching server by managing a fixed number of streams (`_NUM_STREAMS`).",
                "It interleaves `generate` calls for active streams with `prefill` and `insert` calls for new streams as old ones finish, aiming to maximize hardware utilization.",
                "The function returns `None` and its primary output is printed to the console."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#latency_bound_comms",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def latency_bound_comms(comm: float, latency=1e-6):\n  return max(comm, latency)",
        "analysis": {
            "module_type": "communication_latency_model",
            "purpose": "Ensures that a calculated communication time does not fall below a specified minimum latency value by returning the maximum of the two.",
            "input": {
                "shape": "N/A",
                "dtype": "float"
            },
            "processing_steps": [
                "Calculates the maximum value between the input communication time (`comm`) and the minimum latency (`latency`)."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "comm": "The calculated communication time, typically in seconds.",
                "latency": "The minimum latency for a communication operation, in seconds. Defaults to 1e-6."
            },
            "notes": [
                "This function models the physical constraint that any communication operation has a minimum time cost (latency), regardless of the data volume."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#calculate_matmul_resources",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def calculate_matmul_resources(\n    activations_shape: tuple[int, ...],\n    weights_shape: tuple[int, ...],\n    ici_bandwidth: float,\n    peak_flops: float,\n    sD: int = 1,\n    sK: int = 1,\n    sW: int = 1,\n    sF: int = 1,\n    sE: int = 1,\n    activation_size_bytes: int = 2,\n    weight_size_bytes: int = 2,\n    ici_latency: float = 1e-6,\n    all_gather_axes: Sequence[str] = tuple(),\n    debug=True,\n) -> dict[str, float]:\n  \"\"\"\n  Calculates estimated FLOPs, communication volume, and memory for a distributed matrix multiplication.\n\n  The multiplication is A @ W.\n  A (activations) has shape (M, K).\n  W (weights) has shape (G, K, F).\n\n  Sharding strategy assumed:\n  - Data Parallelism: `sD` shards the M dim of A.\n  - Embedding Parallelism: `sK` shards on the embedding dim of A.\n  - Tensor Parallelism for W dim: `sK` shards the W dimension of W.\n  - Tensor Parallelism for F dim: `sF` shards the second weight dim of W.\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n                     G is the number of groups if this is a GMM (e.g in MoE layer).\n      sD: Number of data parallel shards (sD). Must be >= 1.\n      sK: Sharding factor for the activation embedding dimension.\n      sW: Sharding factor for the first weight dimension.\n      sF: Sharding factor for the second weight dimension.\n      sE: Sharding factor to split up expert weights.\n      activation_size_bytes: Size of a single element in bytes for the activations.\n      weight_size_bytes: Size of a single element in bytes for the weights.\n      ici_latency: The latency overhead of communicating between TPUs.\n      all_gather_axes: Optional additional output axes that need to be all-gathered (e.g. \"M\", \"F\").\n      debug: Whether to print intermediate resource calculations.\n\n  Returns:\n      A dictionary with keys:\n          \"t_flops\": Estimated FLOPs latency.\n          \"t_comms\": Estimated communication latency.\n          \"memory\": Estimated memory footprint per device for storing\n                                   local shards of activations, weights, and output (bytes).\n  \"\"\"\n\n  M, K_act = activations_shape[0], activations_shape[-1]\n  # Intermediate activation shape\n  I = np.prod(np.array(activations_shape[1:-1]))\n  if len(weights_shape) == 3:\n    G, K_w, F = weights_shape\n  elif len(weights_shape) == 2:\n    K_w, F = weights_shape\n    G = 1\n  else:\n    raise ValueError(f\"weights_shape={weights_shape} is not supported!.\")\n\n  def _gather_dim_to_shard():\n    # # Used to map all-gather arguments to the respective shardings.\n    return {\"D\": sD, \"K\": sK, \"W\": sW, \"F\": sF, \"E\": sE}\n\n  gather_dim_to_shard = _gather_dim_to_shard()\n\n  def _validate_shardings_and_shapes():\n    if not (sD >= 1 and sK >= 1 and sW >= 1 and sF >= 1 and sE >= 1):\n      raise ValueError(\"All sharding amounts must be >= 1.\")\n    if sK > 1 and sF > 1:\n      raise ValueError(\"Cannot have both sK & sF > 1!\")\n    if K_act != K_w:\n      raise ValueError(f\"K dimension of activations ({K_act}) must match K dimension of weights ({K_w})\")\n    if sK > 1 and sW > 1 and sK != sW:\n      raise ValueError(\"Sharding amounts between embedding dim and first weight matricx dim are different!.\")\n    # Warnings for non-divisibility. Calculations proceed with float division,\n    # implying an average or approximation if not perfectly divisible.\n    if M % sD != 0:\n      print(\n          f\"Warning: Activations M dimension ({M}) is not perfectly divisible by sharding amount {sD}.\",\n          \"Results are approximate.\",\n      )\n    if K_act % sK != 0:\n      print(\n          f\"Warning: Common K dimension ({K_act}) is not perfectly divisible by sharding amount {sK}.\",\n          \"Results are approximate.\",\n      )\n    if K_w % sW != 0:\n      print(\n          f\"Warning: Common W dimension ({K_w}) is not perfectly divisible by sharding amount {sW}. Results are approximate.\"\n      )\n    if F % sF != 0:\n      print(\n          f\"Warning: Weights F dimension ({F}) is not perfectly divisible by sharding amount {sF}. Results are approximate.\"\n      )\n    if G % sE != 0:\n      print(\n          f\"Warning: Experts G dimension ({G}) is not perfectly divisible by sharding amount {sE}. Results are approximate.\"\n      )\n\n  _validate_shardings_and_shapes()\n  K = K_act\n\n  # Implied all-gather flags\n  is_fsdp_act = sK > 1 and sW == 1\n  is_fsdp_weight = sK == 1 and sW > 1\n\n  # Local device dimensions\n  local_M_dim = M // sD\n  local_K_dim = K // sK\n  local_W_dim = K // sW\n  local_G_dim = G // sE\n  local_F_dim = F // sF\n\n  # 1. Total FLOPs\n  # For A(M,K) @ W(K,F), FLOPs = 2 * M * K * F\n  total_flops = 2.0 * np.prod(activations_shape) * G * F / (sF * sE * sD * sK * sW)\n  if debug:\n    print(f\"Total GFlops = {total_flops/1e9}\")\n  if is_fsdp_act:\n    total_flops *= sK\n    if debug:\n      print(f\"Total GFlops after activation all-gather = {total_flops/1e9}\")\n  elif is_fsdp_weight:\n    total_flops *= sW\n    if debug:\n      print(f\"Total GFlops after weights all-gather = {total_flops/1e9}\")\n  t_flops = total_flops / peak_flops\n\n  # 2. Memory per device\n  # A_local: (M/sD, K/sK)\n  # W_local: (G/gE, K/sK, N/sF)\n  # Out_local: (M/sD, N/sF) (buffer for local output)\n  mem_activations_bytes = local_M_dim * I * local_K_dim * activation_size_bytes\n  mem_weights_bytes = local_G_dim * local_W_dim * local_F_dim * weight_size_bytes\n  if debug:\n    print(f\"Activation memory (GB): {mem_activations_bytes/1e9}\")\n    print(f\"Weights memory (GB): {mem_weights_bytes/1e9}\")\n  # All-gather\n  if is_fsdp_act:\n    mem_activations_bytes *= sK\n    if debug:\n      print(f\"Activation memory (GB) after all-gather: {mem_activations_bytes/1e9}\")\n  elif is_fsdp_weight:\n    mem_weights_bytes *= sW\n    if debug:\n      print(f\"Weight memory (GB) after all-gather: {mem_weights_bytes/1e9}\")\n\n  local_output_bytes = local_M_dim * I * local_G_dim * local_F_dim * max(activation_size_bytes, weight_size_bytes)\n  if debug:\n    print(f\"Output memory (GB): {local_output_bytes/1e9}\")\n\n  gathered_output_bytes = local_output_bytes * np.prod([gather_dim_to_shard[axis] for axis in all_gather_axes])\n  if debug:\n    print(f\"Output memory (GB) after additional axes gathers: {gathered_output_bytes/1e9}\")\n  memory_per_TPU_bytes = mem_activations_bytes + mem_weights_bytes + gathered_output_bytes\n  if debug:\n    print(f\"Total memory (GB): {memory_per_TPU_bytes/1e9}\")\n\n  # 3. Communication Volume per TPU\n  t_comms = 0.0\n\n  # For FSDP-style comms, all-gather the tensor.\n  if is_fsdp_act:\n    communication_volume_per_TPU_bytes = np.prod(np.array(activations_shape)) / sK * activation_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for activation all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif is_fsdp_weight:\n    communication_volume_per_TPU_bytes = np.prod(np.array(weights_shape)) / sW * weight_size_bytes\n    t_comms += latency_bound_comms(communication_volume_per_TPU_bytes / ici_bandwidth, ici_latency) * (sW - 1)\n    if debug:\n      print(f\"Per-TPU comms for weights all-gather (GB): {communication_volume_per_TPU_bytes/1e9}\")\n\n  elif sK > 1 and sW > 1:\n    # Perform reduce-scatter on the output.\n    t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sK - 1)\n    if debug:\n      print(f\"Per-TPU comms for all-reduce (GB): {local_output_bytes/1e9}\")\n\n  # All-to-all on the output during expert parallelism (assuming equal loads. i.e. 1/4 * comms(all-gather))\n  if sE > 1:\n    t_comms += latency_bound_comms(local_output_bytes / ici_bandwidth, ici_latency) * (sE - 1) / 4\n    if debug:\n      print(f\"Per-TPU comms for all-to-all (GB): {local_output_bytes/1e9}\")\n\n  for axis in all_gather_axes:\n    current_output_bytes = local_output_bytes\n    current_sharding = gather_dim_to_shard[axis]\n    t_comms += latency_bound_comms(current_output_bytes / ici_bandwidth, ici_latency) * (current_sharding - 1)\n    if debug:\n      print(f\"Per-TPU comms for axis {axis} all-gather (GB): {current_output_bytes/1e9}\")\n    current_output_bytes *= current_sharding\n\n  return {\n      \"t_flops\": t_flops,\n      \"t_comms\": t_comms,\n      \"memory_per_TPU_bytes\": memory_per_TPU_bytes,\n  }",
        "analysis": {
            "module_type": "distributed_matmul_resource_estimator",
            "purpose": "Calculates the estimated latency from computation (FLOPs), communication, and the required memory per device for a distributed matrix multiplication based on various sharding strategies.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Parse and validate input tensor shapes and sharding parameters.",
                "Determine local device tensor dimensions based on the provided sharding factors (sD, sK, sW, sF, sE).",
                "Calculate the total FLOPs for the local matrix multiplication, adjusting for data replication from all-gather operations in FSDP-style parallelism.",
                "Convert total FLOPs to computation time ('t_flops') by dividing by the device's peak FLOPs.",
                "Calculate the memory required for local shards of activations, weights, and the output buffer, including memory expansion from all-gather operations.",
                "Calculate the total communication time ('t_comms') by summing the latencies of various collective operations like all-gather, reduce-scatter, and all-to-all, based on the sharding strategy.",
                "Return a dictionary containing the calculated 't_flops', 't_comms', and 'memory_per_TPU_bytes'."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "latency_bound_comms"
            ],
            "parameters": {
                "activations_shape": "Shape of the input activation tensor, e.g., (M, K).",
                "weights_shape": "Shape of the weight tensor, e.g., (G, K, F) for MoE or (K, F).",
                "ici_bandwidth": "The inter-chip interconnect bandwidth of the hardware in bytes/sec.",
                "peak_flops": "The peak floating-point operations per second of a single device.",
                "sD": "The number of data parallel shards.",
                "sK": "The sharding factor for the activation's embedding dimension (K).",
                "sW": "The sharding factor for the weight's first dimension (K).",
                "sF": "The sharding factor for the weight's second dimension (F).",
                "sE": "The sharding factor for the number of experts (G).",
                "all_gather_axes": "A sequence of dimension names ('D', 'K', 'W', 'F', 'E') indicating which dimensions of the output tensor should be all-gathered."
            },
            "notes": [
                "The function models the resource usage for a matrix multiplication A @ W.",
                "It supports various parallelism strategies including Data Parallelism (DP), Tensor Parallelism (TP), Fully Sharded Data Parallelism (FSDP), and Expert Parallelism.",
                "The output is a dictionary with keys 't_flops' (computation latency in seconds), 't_comms' (communication latency in seconds), and 'memory_per_TPU_bytes' (memory footprint per device in bytes).",
                "The function raises a ValueError for unsupported or conflicting sharding configurations, such as sharding both K and F dimensions simultaneously (sK > 1 and sF > 1)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/sharding_utils.py#plot_sharding_scheme_comparison",
        "file_path": "src/MaxText/inference/scripts/sharding_utils.py",
        "code_block": "def plot_sharding_scheme_comparison(\n    calc_resource_func,\n    activations_shape,\n    weights_shape,\n    sharding_schemes: list[dict],\n):\n  \"\"\"\n  Generates plots comparing different sharding schemes:\n  1. Communication latency vs. FLOPs latency\n  2. Communication latency / memory per device\n  3. Memory & Communication Latency\n\n  Args:\n      activations_shape: Shape of the activations tensor (M, K).\n      weights_shape: Shape of the weights tensor (G, K, F).\n      sharding_schemes: A list of dictionaries. Each dictionary must contain:\n          - \"label\": A string label for the scheme (e.g., \"DP=8\").\n          - \"shard_settings\": A dictionary with sharding parameters used for calc_resource_func().\n          E.g:\n          [\n              {\n                  \"label\": \"DP=8\", # Pure Data Parallelism\n                  \"shard_settings\": {\n                      \"sD\": 8,\n                      \"all_gather_axes\": (\"D\",)\n                  }\n              },\n          ]\n      element_size_bytes: Size of a single element in bytes.\n  \"\"\"\n  results = []\n  valid_schemes_labels = []\n\n  print(\"Calculating resources for sharding schemes...\")\n  for scheme in sharding_schemes:\n    label = scheme.get(\"label\", \"Unknown Scheme\")\n    shard_settings = scheme.get(\"shard_settings\")\n\n    print(f\"\\n--- Scheme: {label} ---\")\n    try:\n      # Clear previous warnings for divisibility for cleaner output per iteration\n      with warnings.catch_warnings(record=True) as caught_warnings:\n        del caught_warnings\n        warnings.simplefilter(\"always\")  # Catch all warnings\n\n        # Call the resource calculation function\n        res = calc_resource_func(activations_shape, weights_shape, **shard_settings)\n        print(\"Workload stats:\\n\")\n        pprint.PrettyPrinter(indent=4).pprint(res)\n\n      results.append(res)\n      valid_schemes_labels.append(label)\n    except ValueError as e:\n      print(f\"Error calculating resources for scheme '{label}': {e}. Skipping.\")\n    except (TypeError, KeyError, ZeroDivisionError, AttributeError) as e:\n      print(f\"An unexpected error occurred for scheme '{label}': {e}. Skipping.\")\n\n  if not results:\n    print(\"No valid data points generated. Cannot create plots.\")\n    return\n\n  # Extract data for plotting\n  t_flops_list = np.array([r[\"t_flops\"] for r in results])\n  t_comms_list = np.array([r[\"t_comms\"] for r in results])\n  mem_list = np.array([r[\"memory_per_TPU_bytes\"] for r in results]) / (1024**3)  # GB\n  title_suffix_context = f\": A{activations_shape} @ W{weights_shape}\"\n  num_schemes = len(valid_schemes_labels)  # Number of successfully processed schemes\n  colors = plt.cm.viridis(np.linspace(0, 1, num_schemes)) if num_schemes > 0 else []\n\n  # Calculate FLOPs/Communication ratio\n  flops_per_comm_ratio = np.zeros(num_schemes)\n  has_infinite_ratio = [False] * num_schemes\n  for i in range(num_schemes):\n    if t_comms_list[i] > 1e-9:  # Threshold to avoid near-zero division issues\n      flops_per_comm_ratio[i] = t_flops_list[i] / t_comms_list[i]\n    elif t_flops_list[i] > 1e-9:  # Positive FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = np.inf\n      has_infinite_ratio[i] = True\n    else:  # Zero FLOPs and zero/tiny communication\n      flops_per_comm_ratio[i] = 0\n\n  finite_ratios = flops_per_comm_ratio[np.isfinite(flops_per_comm_ratio)]\n  placeholder_for_inf = 0\n  if finite_ratios.size > 0:\n    placeholder_for_inf = np.max(finite_ratios) * 1.5 if np.max(finite_ratios) > 0 else 1000\n  elif np.any(has_infinite_ratio):\n    placeholder_for_inf = 1000\n\n  plot_ratios = np.array(\n      [placeholder_for_inf if r_inf else r_val for r_val, r_inf in zip(flops_per_comm_ratio, has_infinite_ratio)]\n  )\n  plot_ratios = np.nan_to_num(plot_ratios, nan=0.0, posinf=placeholder_for_inf, neginf=-placeholder_for_inf)\n\n  # --- Create Plots ---\n  categorical_x = np.arange(num_schemes)  # For categorical x-axis\n\n  # Plot 1: FLOPs & Communication (Grouped Bar Plot)\n  grouped_bar_width_fc = 0.35\n  fig_flops_comm_grouped, ax_flops_comm_grouped = plt.subplots(figsize=(max(10, num_schemes * 1.7), 7))\n\n  rects_flops = ax_flops_comm_grouped.bar(\n      categorical_x - grouped_bar_width_fc / 2,\n      t_flops_list,\n      grouped_bar_width_fc,\n      label=\"T_flops\",\n      color=\"mediumseagreen\",\n  )\n  rects_comms_grouped = ax_flops_comm_grouped.bar(\n      categorical_x + grouped_bar_width_fc / 2, t_comms_list, grouped_bar_width_fc, label=\"T_comms\", color=\"deepskyblue\"\n  )\n\n  ax_flops_comm_grouped.set_xlabel(\"Sharding Scheme\")\n  ax_flops_comm_grouped.set_ylabel(\"Seconds\")\n  ax_flops_comm_grouped.set_title(f\"T_flops & T_comms by Sharding Scheme{title_suffix_context}\", fontsize=14)\n  ax_flops_comm_grouped.set_xticks(categorical_x)\n  ax_flops_comm_grouped.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  if num_schemes > 0:\n    ax_flops_comm_grouped.legend(fontsize=10)\n\n  ax_flops_comm_grouped.bar_label(rects_flops, padding=3, fmt=\"%.2e\", fontsize=9)\n  ax_flops_comm_grouped.bar_label(rects_comms_grouped, padding=3, fmt=\"%.2e\", fontsize=9)\n\n  ax_flops_comm_grouped.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  max_y_val_fc = 0\n  if t_flops_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_flops_list))\n  if t_comms_list.size > 0:\n    max_y_val_fc = max(max_y_val_fc, np.max(t_comms_list))\n  print(f\"max_y_val_fc = {max_y_val_fc}\")\n  ax_flops_comm_grouped.set_ylim(0, max_y_val_fc * 1.15)\n\n  fig_flops_comm_grouped.tight_layout()\n  plt.show()\n\n  # Plot 2: FLOPs/Communication Ratio\n  fig_ratio, ax_ratio = plt.subplots(figsize=(max(10, num_schemes * 1.1), 7))\n\n  bars_ratio = ax_ratio.bar(categorical_x, plot_ratios, width=0.6, color=colors, alpha=0.9)\n\n  ax_ratio.set_xlabel(\"Sharding Scheme\")\n  ax_ratio.set_ylabel(\"T_flops / T_comms\")\n  ax_ratio.set_title(f\"Roofline (T_flops vs. T_comms) for {title_suffix_context}\", fontsize=14)\n  ax_ratio.set_xticks(categorical_x)\n  ax_ratio.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_ratio.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  for i, bar in enumerate(bars_ratio):\n    yval = bar.get_height()\n    label_text = f\"{yval:.2f}\"\n    if has_infinite_ratio[i] and yval == placeholder_for_inf:\n      label_text = f\"> {np.max(finite_ratios):.2f}\\n(Effectively Inf)\" if finite_ratios.size > 0 else \"Very High\"\n    ax_ratio.text(bar.get_x() + bar.get_width() / 2.0, yval, label_text, va=\"bottom\", ha=\"center\", fontsize=9)\n\n  if plot_ratios.size > 0:\n    max_ratio_plot_val = np.max(plot_ratios)\n    ax_ratio.set_ylim(0, max_ratio_plot_val * 1.15)\n\n  fig_ratio.tight_layout()\n  plt.show()\n\n  # Plot 3: Memory vs. Communication (Bars positioned by Communication Volume)\n  fig_mem, ax_mem = plt.subplots(figsize=(max(10, num_schemes * 1.3), 7))  # Slightly wider for labels\n  bar_width_mem = 0.6\n\n  ax_mem.bar(\n      categorical_x,\n      mem_list,\n      width=bar_width_mem,\n      color=colors,\n      alpha=0.85,\n      edgecolor=[np.array(c[:3]) * 0.6 for c in colors],\n  )\n\n  ax_mem.set_xlabel(\"Sharding Scheme\")\n  ax_mem.set_ylabel(\"Memory per TPU (GB)\")\n  ax_mem.set_title(f\"Memory & Comm. by Sharding Scheme{title_suffix_context}\", fontsize=14)  # Updated title\n  ax_mem.set_xticks(categorical_x)\n  ax_mem.set_xticklabels(valid_schemes_labels, rotation=45, ha=\"right\", fontsize=10)\n  ax_mem.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n\n  # Add custom labels in scientific notation\n  for i in range(num_schemes):\n    mem_val = mem_list[i]\n    comm_val = t_comms_list[i]  # This is assumed to be in MB\n\n    # Format the label string as requested\n    # Using \\n for a new line to make it more readable on the plot\n    label_text = f\"mem: {mem_val:.2e} GB\\nt_comms: {comm_val:.2e} sec\"\n\n    ax_mem.text(\n        categorical_x[i],  # x-position: center of the bar\n        mem_val,  # y-position: top of the bar\n        label_text,\n        ha=\"center\",  # Horizontal alignment\n        va=\"bottom\",  # Vertical alignment (anchor at bottom of text, so text is above y)\n        fontsize=8,\n        rotation=0,\n        bbox={\"facecolor\": \"white\", \"alpha\": 0.6, \"pad\": 2, \"boxstyle\": \"round,pad=0.3\"},  # Added bbox\n    )\n\n  if mem_list.size > 0:\n    max_mem_val = np.max(mem_list)\n    # Adjust y-limit to accommodate multi-line labels; factor might need tuning\n    ax_mem.set_ylim(0, max_mem_val * 1.35)  # Increased padding for multi-line labels\n  else:\n    ax_mem.set_ylim(0, 1)\n\n  fig_mem.tight_layout()\n  plt.show()",
        "analysis": {
            "module_type": "sharding_scheme_plotter",
            "purpose": "Calculates performance metrics for various tensor sharding schemes and generates three plots to visually compare them based on FLOPs latency, communication latency, and memory usage.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Iterate through each provided sharding scheme.",
                "Call the `calc_resource_func` with tensor shapes and sharding settings for the current scheme.",
                "Catch and report any exceptions during resource calculation, skipping invalid schemes.",
                "Collect the results (t_flops, t_comms, memory_per_TPU_bytes) for all valid schemes.",
                "Return early if no valid results were generated.",
                "Extract and process the collected data into numpy arrays for plotting.",
                "Calculate the FLOPs-to-communication latency ratio, handling potential division-by-zero cases.",
                "Generate and display Plot 1: A grouped bar chart comparing FLOPs latency (T_flops) and communication latency (T_comms).",
                "Generate and display Plot 2: A bar chart showing the T_flops / T_comms ratio (roofline).",
                "Generate and display Plot 3: A bar chart showing memory per TPU, with data labels indicating both memory and communication latency.",
                "Display each plot using `plt.show()`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "numpy",
                "matplotlib.pyplot",
                "pprint",
                "warnings"
            ],
            "parameters": {
                "calc_resource_func": "A callable function that accepts tensor shapes and sharding parameters, and returns a dictionary with 't_flops', 't_comms', and 'memory_per_TPU_bytes'.",
                "activations_shape": "A tuple representing the shape of the activations tensor.",
                "weights_shape": "A tuple representing the shape of the weights tensor.",
                "sharding_schemes": "A list of dictionaries, where each dictionary defines a sharding scheme with a 'label' and 'shard_settings'."
            },
            "notes": [
                "This function does not return any value; its primary side effect is displaying plots.",
                "It includes error handling to gracefully skip any sharding schemes that are invalid or cause calculation errors.",
                "Special logic is implemented to handle and visualize infinite FLOPs/communication ratios by substituting a large placeholder value for plotting.",
                "The function prints the detailed resource calculations for each valid scheme to the console."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#_validate_config",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def _validate_config(config):\n  \"\"\"Validate configuration settings.\"\"\"\n  assert config.load_full_state_path == \"\", (\n      \"Decode doesn't operate on full states! Convert to parameter checkpoint first.\"\n      \"Using generate_param_only_checkpoint.\"\n  )\n  assert (\n      0 < _INITIAL_PREFILL_STREAMS <= _NUM_STREAMS\n  ), f\"_INITIAL_PREFILL_STREAMS ({_INITIAL_PREFILL_STREAMS}) must be > 0 and <= _NUM_STREAMS ({_NUM_STREAMS})\"",
        "analysis": {
            "module_type": "configuration_validator",
            "purpose": "Validates specific configuration settings for the interleaved inference script, raising an error if conditions are not met.",
            "input": {
                "shape": "N/A",
                "dtype": "pyconfig.Config object"
            },
            "processing_steps": [
                "Assert that `config.load_full_state_path` is an empty string, ensuring that decoding does not operate on full training states.",
                "Assert that the global `_INITIAL_PREFILL_STREAMS` is greater than 0 and less than or equal to the global `_NUM_STREAMS`."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "config.load_full_state_path": "The path to a full training state checkpoint. This must be empty for this script.",
                "_INITIAL_PREFILL_STREAMS": "A global constant defining how many streams to prefill before starting generation.",
                "_NUM_STREAMS": "A global constant defining the total number of streams to process."
            },
            "notes": [
                "This function does not return a value. It either completes successfully or halts execution by raising an AssertionError if a validation check fails."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/decode_multi.py#main",
        "file_path": "src/MaxText/inference/scripts/decode_multi.py",
        "code_block": "def main(argv: Sequence[str]) -> None:\n  \"\"\"Main function to run interleaved inference.\"\"\"\n  jax.config.update(\"jax_default_prng_impl\", \"unsafe_rbg\")\n  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n\n  config = pyconfig.initialize(argv)\n  _validate_config(config)\n  max_utils.print_system_information()\n\n  engine = maxengine.MaxEngine(config)\n  rng = jax.random.PRNGKey(1234)\n  rng, rng_load_params = jax.random.split(rng)\n  params = engine.load_params(rng=rng_load_params)\n\n  text = config.prompt\n  metadata = engine.get_tokenizer()\n  tokenizer_model = engine.build_tokenizer(metadata)\n  tokens, true_length = tokenizer_model.encode(text, is_bos=True, prefill_lengths=[config.max_prefill_predict_length])\n  assert true_length <= config.max_prefill_predict_length, \"Prompt too long for prefill length\"\n\n  batch_size = int(config.per_device_batch_size * jax.device_count())\n  assert (\n      0 < _NUM_STREAMS <= batch_size\n  ), f\"The number of streams {_NUM_STREAMS} must be > 0 and <= batch size {batch_size}\"\n\n  # Initialize decode state\n  rng, rng_init_decode = jax.random.split(rng)\n  decode_state = engine.init_decode_state(rng=rng_init_decode)\n  print(\"Initial decode state initialized.\")\n\n  # Keep track of results per stream (slot)\n  streams_results: dict[int, list[int]] = {i: [] for i in range(_NUM_STREAMS)}\n  streams_active: list[bool] = [False] * _NUM_STREAMS  # Track which slots are active\n  streams_finished: list[bool] = [False] * _NUM_STREAMS  # Track finished streams\n  streams_prefilled_count = 0\n  streams_inserted_count = 0\n\n  # --- Initial Prefill Phase ---\n  print(f\"Starting initial prefill for {_INITIAL_PREFILL_STREAMS} streams...\")\n  prefill_results_to_insert = {}  # Store prefill results before inserting\n  for i in range(_INITIAL_PREFILL_STREAMS):\n    slot_idx = i\n    print(f\"  Prefilling stream for slot {slot_idx}...\")\n    rng, rng_prefill = jax.random.split(rng)\n    request_id = uuid.uuid4()\n    prefill_result, first_token = engine.prefill(\n        params=params,\n        padded_tokens=tokens,\n        true_length=true_length,\n        rng=rng_prefill,\n        slot=slot_idx,\n        request_id=request_id,\n    )\n    prefill_results_to_insert[slot_idx] = prefill_result\n    streams_results[slot_idx].append(first_token.get_result_at_slot(0).tokens.item())\n    streams_prefilled_count += 1\n    print(f\"After prefill stream {slot_idx}\")\n\n  # --- Insert Initial Prefills ---\n  print(\"Inserting initial prefill results...\")\n  for slot_idx, prefill_result in prefill_results_to_insert.items():\n    request_id = uuid.uuid4()\n    decode_state = engine.insert(\n        prefix=prefill_result,\n        decode_state=decode_state,\n        slot=slot_idx,\n        request_id=request_id,  # Pass request_id\n    )\n    streams_active[slot_idx] = True  # Mark stream as active\n    streams_inserted_count += 1\n    print(f\"  Inserted prefill for slot {slot_idx}\")\n\n  print(\"Starting interleaved generation loop...\")\n  total_steps = config.max_target_length - config.max_prefill_predict_length\n  for step in range(total_steps):\n    print(f\"\\n--- Step {step + 1} / {total_steps} ---\")\n\n    # Generate step for all active streams\n    active_stream_indices = [i for i, active in enumerate(streams_active) if active and not streams_finished[i]]\n    if active_stream_indices:\n      print(f\"  Generating for active slots: {active_stream_indices}\")\n      rng, rng_generate = jax.random.split(rng)\n      decode_state, sampled_tokens = engine.generate(params, decode_state, rng=rng_generate)\n\n      # Store the generated token and check for finished streams\n      for slot_idx in active_stream_indices:\n        # Check if the stream finished this step\n        current_len = config.max_prefill_predict_length + step + 1  # Includes prefill + current step\n        finished_this_step = False\n        if current_len >= config.max_target_length:\n          print(f\"    Stream in slot {slot_idx} reached max target length.\")\n          streams_finished[slot_idx] = True\n          streams_active[slot_idx] = False\n          finished_this_step = True\n\n        # Store token if it wasn't already finished before this step or if it finished on this step\n        if not streams_finished[slot_idx] or finished_this_step:\n          # Ensure we don't try to access results for a slot that might not exist\n          if slot_idx < sampled_tokens.data.shape[0]:\n            token_for_slot = sampled_tokens.get_result_at_slot(slot_idx).tokens.item()\n            streams_results[slot_idx].append(token_for_slot)\n          else:\n            print(f\"Warning: Tried to get token for slot {slot_idx}, but batch size seems smaller.\")\n\n        # Call release_pages if finished this step\n        if finished_this_step:\n          print(f\"    Calling engine to release pages for finished slot {slot_idx}...\")\n          engine.release_pages(slot=slot_idx)\n\n    else:\n      print(\"  No active streams to generate for.\")\n\n    # 2. Check if all streams are finished (can exit loop early)\n    if all(streams_finished):\n      print(\"\\nAll streams finished generation.\")\n      break\n\n    # 3. Prefill and Insert new streams if capacity allows\n    num_active_not_finished = sum(1 for i in range(_NUM_STREAMS) if streams_active[i] and not streams_finished[i])\n    available_slots = batch_size - num_active_not_finished\n    can_prefill_more = streams_prefilled_count < _NUM_STREAMS\n\n    if can_prefill_more and available_slots > 0:\n      try:\n        next_available_slot = streams_active.index(False)\n        print(f\"  Prefilling new stream for slot {next_available_slot}...\")\n        rng, rng_prefill = jax.random.split(rng)\n        request_id = uuid.uuid4()\n        prefill_result, first_token = engine.prefill(\n            params=params,\n            padded_tokens=tokens,\n            true_length=true_length,\n            rng=rng_prefill,\n            slot=next_available_slot,\n            request_id=request_id,\n        )\n        streams_prefilled_count += 1\n\n        # Insert the new prefill\n        print(f\"  Inserting new stream into slot {next_available_slot}...\")\n        request_id_insert = uuid.uuid4()\n        decode_state = engine.insert(\n            prefix=prefill_result,\n            decode_state=decode_state,\n            slot=next_available_slot,\n            request_id=request_id_insert,\n        )\n        streams_active[next_available_slot] = True\n        streams_inserted_count += 1\n        streams_results[next_available_slot].append(first_token.get_result_at_slot(0).tokens.item())\n\n      except ValueError:\n        print(\"  Warning: Available slots detected but couldn't find an inactive one.\")\n    elif can_prefill_more:\n      print(\"  Generate step finished, but no available slots to prefill new stream.\")\n    else:\n      print(\"  Generate step finished, all streams already prefilled.\")\n\n  print(\"\\n--- Final Results ---\")\n  for i in range(_NUM_STREAMS):\n    if streams_results[i]:\n      output = tokenizer_model.decode(streams_results[i])\n      print(f\"Stream {i}: Input=`{text}` -> Output=`{output}`\")\n\n      if i == 0:  # Check first stream as an example\n        assert output.startswith(\n            config.autoregressive_decode_assert\n        ), f\"Stream {i} generated text mismatch: `{output}` vs expected start `{config.autoregressive_decode_assert}`\"\n    else:\n      print(f\"Stream {i}: Was not activated.\")",
        "analysis": {
            "module_type": "interleaved_inference_runner",
            "purpose": "Runs a multi-stream inference loop that interleaves prefilling new prompts with generating tokens for active prompts, simulating a continuous batching scenario.",
            "input": {
                "shape": "N/A",
                "dtype": "Sequence[str]"
            },
            "processing_steps": [
                "Initialize configuration, JAX, and the MaxEngine from command-line arguments.",
                "Load model parameters and build the tokenizer.",
                "Tokenize the input prompt specified in the configuration.",
                "Initialize an empty decode state for the batch using `engine.init_decode_state`.",
                "Prefill an initial set of streams using `engine.prefill`.",
                "Insert the prefilled KV caches into the main decode state using `engine.insert`.",
                "Enter a generation loop that runs for `max_target_length - max_prefill_predict_length` steps.",
                "In each step, call `engine.generate` to produce one token for all active streams.",
                "Check for finished streams (reaching `max_target_length`) and release their resources using `engine.release_pages`.",
                "If capacity is available and not all streams have been started, prefill and insert a new stream into an empty slot.",
                "After the loop, decode the complete token sequences for each stream and print the results to the console.",
                "Assert that the output of the first stream matches an expected string from the config."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax",
                "maxengine.MaxEngine",
                "pyconfig",
                "max_utils",
                "_validate_config",
                "uuid"
            ],
            "parameters": {
                "config.prompt": "The input text prompt used for all generation streams.",
                "config.max_prefill_predict_length": "The maximum length of the input prompt for the prefill operation.",
                "config.max_target_length": "The total desired output length for each stream, including the prompt.",
                "config.per_device_batch_size": "Batch size per device, used to calculate the total available slots for streams.",
                "config.autoregressive_decode_assert": "A string used to verify the beginning of the generated output for correctness."
            },
            "notes": [
                "This function simulates a continuous batching server by managing multiple independent generation streams within a fixed-size batch.",
                "It starts by prefilling a subset of streams (`_INITIAL_PREFILL_STREAMS`), then enters a loop where it generates tokens for active streams and adds new streams as others finish.",
                "The total number of streams to process is controlled by the global constant `_NUM_STREAMS`.",
                "The function's primary output is printed to standard output; it does not return a value."
            ]
        }
    },
    {
        "block_name": "src/MaxText/inference/scripts/test_sharding_utils.py#ShardingTests",
        "file_path": "src/MaxText/inference/scripts/test_sharding_utils.py",
        "code_block": "class ShardingTests(unittest.TestCase):\n  \"\"\"Test suite for sharding resource calculation utilities.\"\"\"\n\n  def test_no_sharding(self):\n    \"\"\"Tests the basic case with no sharding.\"\"\"\n    sD, sK, sW, sF, sE = 1, 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    # Total FLOPs = 2 * M * K * F\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF(self):\n    \"\"\"Tests sharding on the F dimension of weights (sF > 1).\"\"\"\n    sF = 4\n    sD, sK, sW, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_data_parallelism_sD(self):\n    \"\"\"Tests sharding on the M dimension of activations (sD).\"\"\"\n    sD = 4\n    sK, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs:\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    expected_t_comms = 0.0\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_activation_sharding_sK(self):\n    \"\"\"Tests FSDP-style sharding on the K dimension of activations (sK).\n\n    In this scenario, the weights are not sharded (sW=1).\n    \"\"\"\n    sK = 4\n    sD, sW, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (M * K / sK) * activation_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_fsdp_weight_sharding_sW(self):\n    \"\"\"Tests FSDP-style sharding on the W dimension of weights (sW).\n\n    In this scenario, the activations are not sharded (sK=1).\n    \"\"\"\n    sW = 4\n    sD, sK, sF, sE = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    comm_data_size = (K * F / sW) * weight_size_bytes_val\n    # t_comms\n    expected_t_comms = latency_bound_comms(comm_data_size / ici_bandwidth_val, ici_latency_val) * (sW - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_tensor_parallel_sK_sW(self):\n    \"\"\"Tests tensor parallelism where both sK and sW are used.\n\n    This test assumes sK == sW and a reduce-scatter operation for partial\n    results.\n    \"\"\"\n    sK = 2\n    sW = 2  # Must be equal to sK for this path\n    sD, sF, sE = 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_output_feature_parallelism_sF_with_all_gather_F(self):\n    \"\"\"Tests sF sharding with a subsequent all-gather on the F dimension.\"\"\"\n    sF = 4  # Shard the output feature dimension\n    sD, sK, sW, sE = 1, 1, 1, 1  # Isolate sF effect\n    all_gather_axes = [\"F\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,  # (K, F)\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * (F / sF)) / peak_flops_val\n    assert (\n        abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n    ), f\"FLOPs mismatch: got {result['t_flops']}, expected {expected_t_flops}\"\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_for_gather = M * (F / sF) * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_for_gather / ici_bandwidth_val, ici_latency_val) * (sF - 1)\n    assert (\n        abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n    ), f\"Comms mismatch: got {result['t_comms']}, expected {expected_t_comms}\"\n\n    # Expected Memory per TPU:\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * (F / sF) * weight_size_bytes_val\n    # Outputs\n    expected_mem_output_gathered = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output_gathered\n    assert (\n        abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n    ), f\"Memory mismatch: got {result['memory_per_TPU_bytes']}, expected {expected_memory_per_TPU}\"\n\n  def test_expert_parallelism_sE(self):\n    \"\"\"Tests expert parallelism sharding on the G dimension (sE).\"\"\"\n    G_val = 8\n    weights_shape_3d = (G_val, K, F)\n    sE = 4\n    sD, sK, sW, sF = 1, 1, 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_3d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F * G_val) / (peak_flops_val * sE)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sE - 1) / 4\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = M * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (G_val / sE) * K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * (G_val / sE) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_mixed_sharding_sD_sK_sW(self):\n    \"\"\"Tests a mix of data and tensor parallelism (reduce-scatter).\"\"\"\n    sD = 2\n    sK = 2\n    sW = 2  # sK == sW\n    sF, sE = 1, 1\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * (M / sD) * (K / (sK * sW)) * F) / peak_flops_val\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes / ici_bandwidth_val, ici_latency_val) * (sK - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * (K / sK) * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = (K / sW) * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE\n\n  def test_additional_all_gather_axes_D(self):\n    \"\"\"Tests an additional all-gather on the 'D' dimension of the output.\"\"\"\n    sD = 2\n    sK, sW, sF, sE = 1, 1, 1, 1\n    all_gather_axes = [\"D\"]\n\n    result = calculate_matmul_resources(\n        activations_shape=activations_shape_2d,\n        weights_shape=weights_shape_2d,\n        ici_bandwidth=ici_bandwidth_val,\n        peak_flops=peak_flops_val,\n        sD=sD,\n        sK=sK,\n        sW=sW,\n        sF=sF,\n        sE=sE,\n        activation_size_bytes=activation_size_bytes_val,\n        weight_size_bytes=weight_size_bytes_val,\n        ici_latency=ici_latency_val,\n        all_gather_axes=all_gather_axes,\n        debug=False,\n    )\n\n    # Expected FLOPs\n    expected_t_flops = (2.0 * M * K * F) / (peak_flops_val * sD)\n    assert abs(result[\"t_flops\"] - expected_t_flops) < TOLERANCE\n\n    # Expected comms\n    # per TPU\n    local_output_bytes_base = (M / sD) * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # t_comms\n    expected_t_comms = latency_bound_comms(local_output_bytes_base / ici_bandwidth_val, ici_latency_val) * (sD - 1)\n    assert abs(result[\"t_comms\"] - expected_t_comms) < TOLERANCE\n\n    # Expected Memory per TPU\n    # Activations\n    expected_mem_activations = (M / sD) * K * activation_size_bytes_val\n    # Weights\n    expected_mem_weights = K * F * weight_size_bytes_val\n    # Output\n    expected_mem_output = M * F * max(activation_size_bytes_val, weight_size_bytes_val)\n    # Total\n    expected_memory_per_TPU = expected_mem_activations + expected_mem_weights + expected_mem_output\n    assert abs(result[\"memory_per_TPU_bytes\"] - expected_memory_per_TPU) < TOLERANCE",
        "analysis": {
            "module_type": "unit_test_suite",
            "purpose": "A test suite to verify the correctness of sharding resource calculation utilities for matrix multiplication.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Defines multiple test methods, each corresponding to a specific sharding strategy.",
                "Each test method calls `calculate_matmul_resources` with a different set of sharding parameters.",
                "It then calculates the expected FLOPs, communication time, and memory usage for that strategy.",
                "Finally, it asserts that the results from the function match the expected values within a defined tolerance."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "unittest.TestCase",
                "MaxText.inference.scripts.sharding_utils.calculate_matmul_resources",
                "MaxText.inference.scripts.sharding_utils.latency_bound_comms"
            ],
            "parameters": {},
            "notes": [
                "This class covers a wide range of sharding scenarios, including no sharding, data parallelism (sD), output feature parallelism (sF), FSDP-style sharding (sK, sW), tensor parallelism (sK & sW), expert parallelism (sE), and mixed strategies.",
                "It also tests the effect of subsequent all-gather operations on communication and memory calculations."
            ],
            "methods": {
                "test_no_sharding": {
                    "purpose": "Tests the baseline case where no sharding is applied to the matrix multiplication.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set all sharding factors (sD, sK, sW, sF, sE) to 1.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (expected to be 0), and memory per TPU match the manually calculated expected values for an unsharded operation."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_output_feature_parallelism_sF": {
                    "purpose": "Tests sharding on the output feature dimension (F) of the weights, a form of tensor parallelism.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the feature sharding factor `sF` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (expected to be 0), and memory per TPU match the expected values for this sharding strategy."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_data_parallelism_sD": {
                    "purpose": "Tests data parallelism by sharding on the batch dimension (M) of the activations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the data parallelism factor `sD` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (expected to be 0), and memory per TPU match the expected values for data parallelism."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources"
                    ],
                    "notes": []
                },
                "test_fsdp_activation_sharding_sK": {
                    "purpose": "Tests FSDP-style sharding where the contracting dimension (K) of the activations is sharded, requiring an all-gather of activations.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the activation sharding factor `sK` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (for the all-gather), and memory per TPU match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "In this scenario, weights are not sharded (sW=1)."
                    ]
                },
                "test_fsdp_weight_sharding_sW": {
                    "purpose": "Tests FSDP-style sharding where the contracting dimension (K) of the weights is sharded, requiring an all-gather of weights.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set the weight sharding factor `sW` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (for the all-gather), and memory per TPU match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "In this scenario, activations are not sharded (sK=1)."
                    ]
                },
                "test_tensor_parallel_sK_sW": {
                    "purpose": "Tests tensor parallelism where both activations and weights are sharded along the contracting dimension (K), requiring a reduce-scatter on the output.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set both `sK` and `sW` to 2.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (for the reduce-scatter), and memory per TPU match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": [
                        "This test assumes `sK` is equal to `sW`."
                    ]
                },
                "test_output_feature_parallelism_sF_with_all_gather_F": {
                    "purpose": "Tests output feature parallelism (sF) followed by an explicit all-gather on the sharded feature dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `sF` to 4 and specify `all_gather_axes=['F']`.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (for the all-gather), and memory per TPU (including the gathered output) match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_expert_parallelism_sE": {
                    "purpose": "Tests expert parallelism by sharding on the expert dimension (G) of 3D weights.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Define a 3D weight shape `(G, K, F)`.",
                        "Set the expert parallelism factor `sE` to 4.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (for the all-to-all), and memory per TPU match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_mixed_sharding_sD_sK_sW": {
                    "purpose": "Tests a hybrid strategy combining data parallelism (sD) and tensor parallelism (sK, sW with reduce-scatter).",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `sD`, `sK`, and `sW` to 2.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time, and memory per TPU match the expected values for the combined strategy."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                },
                "test_additional_all_gather_axes_D": {
                    "purpose": "Tests data parallelism (sD) followed by an explicit all-gather on the sharded data dimension.",
                    "input": {
                        "shape": "N/A",
                        "dtype": "N/A"
                    },
                    "processing_steps": [
                        "Set `sD` to 2 and specify `all_gather_axes=['D']`.",
                        "Call `calculate_matmul_resources`.",
                        "Assert that the calculated FLOPs, communication time (for the all-gather), and memory per TPU (including the gathered output) match the expected values."
                    ],
                    "output": {
                        "shape": "N/A"
                    },
                    "dependencies": [
                        "calculate_matmul_resources",
                        "latency_bound_comms"
                    ],
                    "notes": []
                }
            }
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#DecoderBlockType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class DecoderBlockType(enum.Enum):\n  \"\"\"Decoder block types.\"\"\"\n\n  DEFAULT = \"default\"\n  LLAMA2 = \"llama2\"\n  MISTRAL = \"mistral\"\n  MIXTRAL = \"mixtral\"\n  DEEPSEEK = \"deepseek\"\n  GEMMA = \"gemma\"\n  GEMMA2 = \"gemma2\"\n  GEMMA3 = \"gemma3\"\n  QWEN3 = \"qwen3\"\n  QWEN3_MOE = \"qwen3_moe\"\n  GPT3 = \"gpt3\"\n  GPT_OSS = \"gpt_oss\"\n  SIMPLE = \"simple\"\n  SIMPLE_MLP = \"simple_mlp\"\n  LLAMA4 = \"llama4\"",
        "analysis": {
            "module_type": "decoder_block_type_enum",
            "purpose": "Defines an enumeration of supported decoder block architectures for transformer models.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This class provides a set of named constants (e.g., LLAMA2, MISTRAL, GEMMA) that correspond to specific decoder block implementations.",
                "It is used for configuring which type of decoder block to use within a larger model architecture."
            ]
        }
    },
    {
        "block_name": "src/MaxText/common_types.py#AttentionType",
        "file_path": "src/MaxText/common_types.py",
        "code_block": "class AttentionType(enum.Enum):\n  GLOBAL = \"global\"  # default, with causality\n  LOCAL_SLIDING = \"local_sliding\"\n  CHUNK = \"chunk\"\n  MLA = \"mla\"\n  FULL = \"full\"",
        "analysis": {
            "module_type": "attention_type_enum",
            "purpose": "Defines a set of named constants representing different types of attention mechanisms.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "enum.Enum"
            ],
            "parameters": {},
            "notes": [
                "This class provides a standardized way to refer to different attention configurations within the model.",
                "The members are GLOBAL, LOCAL_SLIDING, CHUNK, MLA, and FULL, each mapping to a specific string identifier.",
                "The 'GLOBAL' type is commented as the default, which includes causality."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_input_data_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_input_data_sharding(config, mesh):\n  \"\"\"Get the input data sharding for the model\"\"\"\n  return nn.logical_to_mesh_sharding(P(*config.input_data_sharding_logical_axes), mesh, config.logical_axis_rules)",
        "analysis": {
            "module_type": "sharding_utility",
            "purpose": "Converts logical axis names for input data sharding from a configuration into a concrete mesh-specific sharding object.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a jax.sharding.PartitionSpec from the logical axes specified in `config.input_data_sharding_logical_axes`.",
                "Call `nn.logical_to_mesh_sharding` to map the logical PartitionSpec to the physical device mesh using the provided logical axis rules.",
                "Return the resulting mesh sharding object."
            ],
            "output": {
                "shape": "Returns a jax.sharding.NamedSharding object, which is not a tensor, so shape is 'N/A'."
            },
            "dependencies": [
                "flax.linen.partitioning.logical_to_mesh_sharding",
                "jax.sharding.PartitionSpec"
            ],
            "parameters": {
                "config.input_data_sharding_logical_axes": "A sequence of strings representing the logical axes for sharding the input data (e.g., ('data', 'mdl')).",
                "config.logical_axis_rules": "A set of rules that map logical axis names to physical mesh axis names."
            },
            "notes": [
                "This function encapsulates the standard process of defining data layout in a distributed JAX environment, separating logical intent from the physical device topology."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_train_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_train_with_signature(train_step, data_sharding, state_mesh_shardings, model, config):\n  \"\"\"Get the shardings (both state and data) for `train_step`.\"\"\"\n  functional_train = functools.partial(train_step, model, config, state_mesh_shardings)\n  functional_train.__name__ = \"train_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = (state_mesh_shardings, None)  # State, metrics\n  static_argnums = ()  # We partial out the static argnums of model and config\n  donate_argnums = 0  # This is the index of the state - we allow the compiler to make use of this memory.\n  return functional_train, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "train_step_wrapper",
            "purpose": "Wraps a training step function with its JAX sharding annotations and compilation hints (e.g., for `pjit`).",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Creates a partial function `functional_train` by binding `model`, `config`, and `state_mesh_shardings` to the input `train_step` function using `functools.partial`.",
                "Sets the `__name__` of the new partial function to 'train_step'.",
                "Defines a tuple `in_shardings` specifying the sharding for the state, batch, and RNG inputs.",
                "Defines a tuple `out_shardings` specifying the sharding for the output state and metrics.",
                "Defines `static_argnums` as an empty tuple, as static arguments are handled by the partial function.",
                "Sets `donate_argnums` to 0 to allow the compiler to reuse the memory of the input state for the output state."
            ],
            "output": {
                "shape": "Returns a tuple: (callable, in_shardings, out_shardings, static_argnums, donate_argnums)."
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "train_step": "A function that executes a single training step.",
                "data_sharding": "The sharding configuration for the input data batch.",
                "state_mesh_shardings": "The sharding configuration for the model's training state.",
                "model": "The model object.",
                "config": "The configuration object."
            },
            "notes": [
                "This function is a utility to package a function with its distributed execution metadata, which is a common pattern when using `jax.pjit`.",
                "The `donate_argnums=0` is a memory optimization hint for the JAX compiler."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_functional_eval_with_signature",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_functional_eval_with_signature(eval_step, data_sharding, state_mesh_shardings, model, config):\n  \"\"\"Get the shardings (both state and data) for `eval_step`.\"\"\"\n  functional_eval = functools.partial(eval_step, model, config)\n  functional_eval.__name__ = \"eval_step\"\n  in_shardings = (state_mesh_shardings, data_sharding, None)  # State, batch, rng\n  out_shardings = None  # metrics\n  static_argnums = ()  # We partial out the static argnums of model, config\n  donate_argnums = ()  # state will be kept instead of being donated in eval_step\n  return functional_eval, in_shardings, out_shardings, static_argnums, donate_argnums",
        "analysis": {
            "module_type": "pjit_signature_wrapper",
            "purpose": "Prepares an evaluation step function for JAX compilation (e.g., with pjit) by creating a partial function and defining its input/output sharding specifications and other compilation arguments.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a partial function `functional_eval` from `eval_step` with `model` and `config` arguments pre-filled using `functools.partial`.",
                "Set the `__name__` of the new partial function to 'eval_step'.",
                "Define the input sharding (`in_shardings`) for the model state, data batch, and RNG key.",
                "Define the output sharding (`out_shardings`) for the metrics as fully replicated (`None`).",
                "Define empty tuples for `static_argnums` and `donate_argnums`.",
                "Return the partial function and the sharding/compilation specifications."
            ],
            "output": {
                "shape": "A tuple containing: (a partial function, input shardings tuple, output sharding, static argnums tuple, donate argnums tuple)."
            },
            "dependencies": [
                "functools.partial"
            ],
            "parameters": {
                "eval_step": "The evaluation function to be wrapped.",
                "data_sharding": "The sharding specification for the input data batch.",
                "state_mesh_shardings": "The sharding specification for the model's state.",
                "model": "The model object, which is partialed into the `eval_step` function.",
                "config": "The configuration object, which is partialed into the `eval_step` function."
            },
            "notes": [
                "This function is a utility for setting up the necessary components to compile `eval_step` with `jax.pjit`.",
                "The `donate_argnums` is explicitly empty, indicating that the model state is preserved during evaluation and its memory is not reused for outputs.",
                "The output `out_shardings` is `None`, implying the returned metrics are expected to be fully replicated across all devices."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_shaped_batch",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_shaped_batch(config):\n  \"\"\"Return the shape of the batch - this is what eval_shape would return for the\n  output of create_data_iterator, but eval_shape doesn't work, see b/306901078.\"\"\"\n  batch_shape = (config.global_batch_size_to_load, config.max_target_length)\n  shaped_batch = {}\n  shaped_batch[\"inputs\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"inputs_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_position\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  shaped_batch[\"targets_segmentation\"] = jax.ShapeDtypeStruct(batch_shape, jnp.int32)\n  if config.use_multimodal:\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(config.model_name, batch_size=config.micro_batch_size_to_train_on)\n    shaped_batch[\"images\"] = jax.ShapeDtypeStruct(image_shape, jnp.int32)\n  return shaped_batch",
        "analysis": {
            "functionality": "Constructs a dictionary of JAX ShapeDtypeStruct objects that represent the shape and data type of a typical input batch. This is used to provide abstract shape information for JAX transformations like `jit` compilation without needing actual data.",
            "usage": "Call this function with a configuration object to get a dictionary representing the abstract shape of a data batch. The function uses parameters from the config like `global_batch_size_to_load`, `max_target_length`, and `use_multimodal` to build the shape structures. The output is a dictionary where keys are tensor names (e.g., 'inputs', 'targets', 'images') and values are `jax.ShapeDtypeStruct` instances."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#load_compiled",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def load_compiled(config, partial_train, state):\n  \"\"\"# Loading a serialized compiled train step function.\"\"\"\n\n  # Currently partial_train and state  are needed to reconstruct\n  # input/output shapes to construct the in_trees and out_trees for load API\n  # Parker is working on a serializing these\n  def load_serialized_compiled(save_name):\n    with open(save_name, \"rb\") as f:\n      serialized_compiled = pickle.load(f)\n    return serialized_compiled\n\n  def get_train_input_output_trees(func, input_args, input_kwargs):\n    _, in_tree_recreated = jax.tree_util.tree_flatten((input_args, input_kwargs))\n    out_shaped = jax.eval_shape(func, *input_args, **input_kwargs)\n    _, out_tree_recreated = jax.tree_util.tree_flatten(out_shaped)\n    return in_tree_recreated, out_tree_recreated\n\n  serialized_compiled = load_serialized_compiled(config.compiled_trainstep_file)\n  shaped_batch = get_shaped_batch(config)\n  example_rng = jax.random.PRNGKey(0)\n  shaped_input_args = (state, shaped_batch, example_rng)\n  shaped_input_kwargs = {}\n  in_tree, out_tree = get_train_input_output_trees(partial_train, shaped_input_args, shaped_input_kwargs)\n  p_train_step = deserialize_and_load(serialized_compiled, in_tree, out_tree)\n  return p_train_step",
        "analysis": {
            "module_type": "compiled_function_loader",
            "purpose": "Loads a serialized, pre-compiled JAX training step function from a file to avoid recompilation on startup.",
            "input": {
                "shape": "config: Configuration object, partial_train: Callable (partially applied train function), state: PyTree (e.g., flax.training.train_state.TrainState)",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Loads a serialized object from the path specified in `config.compiled_trainstep_file` using `pickle`.",
                "Creates a dummy batch with the expected shapes using `get_shaped_batch`.",
                "Determines the input and output PyTree structures of the `partial_train` function by calling `jax.eval_shape` with the dummy batch and initial state.",
                "Reconstructs the compiled training step function by calling `deserialize_and_load` with the serialized object and the determined PyTree structures.",
                "Returns the reconstructed, compiled training step function."
            ],
            "output": {
                "shape": "A callable JAX pjit-compiled training step function."
            },
            "dependencies": [
                "pickle.load",
                "jax.tree_util.tree_flatten",
                "jax.eval_shape",
                "jax.random.PRNGKey",
                "jax.experimental.serialize_executable.deserialize_and_load",
                "get_shaped_batch"
            ],
            "parameters": {
                "compiled_trainstep_file": "The file path to the serialized compiled training step function."
            },
            "notes": [
                "This function is an optimization to reduce startup time by reusing a previously compiled JAX function.",
                "It requires the original `partial_train` function and a `state` object to reconstruct the input/output shapes and PyTree definitions needed by the `deserialize_and_load` API, as noted in the code comments."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tokens_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tokens_training_per_device(config):\n  \"\"\"Calculate training Tokens per device\"\"\"\n  return config.max_target_length * config.per_device_batch_size * config.gradient_accumulation_steps",
        "analysis": {
            "module_type": "training_token_calculator",
            "purpose": "Calculates the total number of tokens processed per device in a single effective training step, considering gradient accumulation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Multiplies `config.max_target_length`, `config.per_device_batch_size`, and `config.gradient_accumulation_steps`."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing `max_target_length`, `per_device_batch_size`, and `gradient_accumulation_steps`."
            },
            "notes": [
                "The result represents the effective number of tokens processed per device for a single optimizer update, not just a single forward pass."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma2_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma2_tflops_training_per_device(config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops):\n  \"\"\"\n  Calculate training TFLOP for Gemma2 as in Gemma2 we combine [local_attention, global_attention] into one decoder\n  layer and we use sliding window attention in local_attention\n  \"\"\"\n  noncausal_attention_flops = (\n      # global attention\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n      +\n      # local attention\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  # multiply num_decoder_layers by 2 because we combine [local_attention, global_attention] into one decoder layer\n  learnable_weight_tflops = (\n      ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers * 2 + embedding_flops) * 3 / 10**12\n  )\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "module_type": "tflops_calculator",
            "purpose": "Calculates the training TeraFLOPs (TFLOPs) per device for a Gemma2 model, accounting for its specific architecture which combines local and global attention within a single decoder layer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate non-causal attention FLOPs by summing the FLOPs for global attention and local sliding window attention.",
                "Convert non-causal attention FLOPs to causal attention FLOPs by dividing by 2.",
                "Calculate total attention TFLOPs by scaling for the number of decoder layers, forward/backward passes (factor of 3), and converting to TeraFLOPs (divide by 10^12).",
                "Calculate learnable weight TFLOPs by summing FFN, QKV, projection, and embedding FLOPs.",
                "Scale the learnable weight FLOPs by the number of decoder layers (multiplied by 2 for Gemma2's combined architecture) and a factor of 3 for forward/backward/optimizer passes, then convert to TeraFLOPs.",
                "Return the calculated attention TFLOPs and learnable weight TFLOPs."
            ],
            "output": {
                "shape": "A tuple containing two scalar values: (attention_tflops, learnable_weight_tflops)."
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model parameters like `per_device_batch_size`, `max_target_length`, `num_query_heads`, `head_dim`, `sliding_window_size`, and `num_decoder_layers`.",
                "total_ffn_flops": "The total pre-calculated FLOPs for the Feed-Forward Network layers.",
                "qkv_flops": "The pre-calculated FLOPs for the Query, Key, and Value projection layers.",
                "projection_flops": "The pre-calculated FLOPs for the attention output projection layer.",
                "embedding_flops": "The pre-calculated FLOPs for the embedding layer."
            },
            "notes": [
                "This calculation is specific to the Gemma2 architecture, where local (sliding window) and global attention mechanisms are combined into a single decoder layer.",
                "The number of decoder layers is multiplied by 2 when calculating learnable weight FLOPs to reflect this combined structure.",
                "The final TFLOPs values are scaled by a factor of 3 to account for the forward pass, backward pass, and optimizer updates."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mixed_attention_model_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mixed_attention_model_tflops_training_per_device(\n    config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length\n):\n  \"\"\"\n  Calculate training TFLOPs for models with a mixed attention pattern of local\n  and global attention layers, like Gemma3 and GPT-OSS.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n\n  num_global_layers = num_layers // attention_pattern_length\n  num_local_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention)\n  # Formula: 4 * batch_size * seq_len^2 * num_heads * head_dim\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single local attention layer (sliding window)\n  # Formula: 4 * batch_size * seq_len * window_size * num_heads * head_dim\n  local_attention_flops_per_layer = (\n      4\n      * config.per_device_batch_size\n      * config.max_target_length\n      * min(config.sliding_window_size, config.max_target_length)\n      * config.num_query_heads\n      * config.head_dim\n  )\n\n  # Total attention FLOPs = (num_global_layers * FLOPs_per_global) + (num_local_layers * FLOPs_per_local)\n  noncausal_attention_flops = (\n      num_global_layers * global_attention_flops_per_layer + num_local_layers * local_attention_flops_per_layer\n  )\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Convert to TFLOPs and multiply by 3 for fwd/bwd pass\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  # Learnable weights (FFN, QKV, Projections) are present in every layer.\n  learnable_weight_tflops = ((total_ffn_flops + qkv_flops + projection_flops) * num_layers + embedding_flops) * 3 / 10**12\n\n  return attention_tflops, learnable_weight_tflops",
        "analysis": {
            "module_type": "mixed_attention_tflops_calculator",
            "purpose": "Calculates the training TeraFLOPs (TFLOPs) for transformer models that use a mix of global and local (sliding window) attention layers, such as Gemma3 and GPT-OSS.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine the number of global and local attention layers based on `num_decoder_layers` and `attention_pattern_length`.",
                "Calculate the FLOPs for a single global (full) attention layer.",
                "Calculate the FLOPs for a single local (sliding window) attention layer.",
                "Compute the total non-causal attention FLOPs by summing the FLOPs from all global and local layers.",
                "Calculate causal attention FLOPs by dividing the non-causal FLOPs by 2.",
                "Convert causal attention FLOPs to TFLOPs and multiply by 3 to account for the forward pass, backward pass, and optimizer update, yielding `attention_tflops`.",
                "Calculate the total TFLOPs for learnable weights (FFN, QKV, projections, embeddings) across all layers, also multiplying by 3, yielding `learnable_weight_tflops`.",
                "Return the calculated `attention_tflops` and `learnable_weight_tflops`."
            ],
            "output": {
                "shape": "A tuple of two scalar floats: (attention_tflops, learnable_weight_tflops)."
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model parameters like `num_decoder_layers`, `per_device_batch_size`, `max_target_length`, `num_query_heads`, `head_dim`, and `sliding_window_size`.",
                "total_ffn_flops": "The total FLOPs for the Feed-Forward Network layers.",
                "qkv_flops": "The FLOPs for the Query, Key, and Value projection layers.",
                "projection_flops": "The FLOPs for the output projection layer in the attention mechanism.",
                "embedding_flops": "The FLOPs for the embedding layer.",
                "attention_pattern_length": "The length of the repeating pattern of attention types. For example, for a pattern of 1 global then 5 local layers, this would be 6."
            },
            "notes": [
                "The calculation assumes a causal attention mechanism, hence dividing the non-causal attention FLOPs by 2.",
                "The factor of 3 in the TFLOPs calculation is a common approximation for the cost of a training step (forward pass, backward pass, and optimizer update)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_calculate_chunked_attention_flops_per_layer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size):\n  \"\"\"Calculates the non-causal FLOPs for a single layer of chunked attention.\"\"\"\n  num_chunks = seq_len // chunk_size\n  rem_chunk_size = seq_len % chunk_size\n  # The complexity of chunked attention is the sum of squares of chunk lengths.\n  chunked_complexity = (num_chunks * chunk_size**2) + (rem_chunk_size**2)\n  # The formula for non-causal attention FLOPs is 4 * B * complexity * H * D,\n  # where B=batch_size, H=num_heads, D=head_dim.\n  return 4 * config.per_device_batch_size * chunked_complexity * config.num_query_heads * config.head_dim",
        "analysis": {
            "functionality": "Calculates the non-causal Floating Point Operations (FLOPs) for a single layer of chunked attention.",
            "usage": "To use this function, provide a configuration object `config` (containing `per_device_batch_size`, `num_query_heads`, `head_dim`), the sequence length `seq_len`, and the `chunk_size`. It returns a single numerical value representing the total FLOPs."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_attention_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_attention_tflops(config):\n  \"\"\"\n  Calculates attention-only training TFLOPs for Llama4's specific architecture,\n  which has an alternating pattern of global and chunked attention layers.\n  \"\"\"\n  num_layers = config.num_decoder_layers\n  seq_len = config.max_target_length\n  chunk_size = config.chunk_attn_window_size\n\n  # Determine number of global vs. chunked layers based on the NoPE interval.\n  # A \"NoPE\" layer uses global attention.\n  num_global_layers = num_layers // config.nope_layer_interval\n  num_chunked_layers = num_layers - num_global_layers\n\n  # FLOPs for a single global attention layer (full attention, non-causal)\n  global_attention_flops_per_layer = (\n      4 * config.per_device_batch_size * seq_len**2 * config.num_query_heads * config.head_dim\n  )\n\n  # FLOPs for a single chunked attention layer (non-causal)\n  chunked_attention_flops_per_layer = _calculate_chunked_attention_flops_per_layer(config, seq_len, chunk_size)\n\n  # Total non-causal attention FLOPs is the sum of all global and all chunked layers\n  noncausal_attention_flops = (num_global_layers * global_attention_flops_per_layer) + (\n      num_chunked_layers * chunked_attention_flops_per_layer\n  )\n\n  # Apply causal mask and convert to TFLOPs (multiply by 3 for fwd/bwd pass)\n  causal_attention_flops = noncausal_attention_flops / 2\n  attention_tflops = causal_attention_flops * 3 / 10**12\n\n  return attention_tflops",
        "analysis": {
            "functionality": "Calculates the attention-only training TeraFLOPs (TFLOPs) for a Llama4-style model. This model architecture features an alternating pattern of global attention layers and chunked attention layers.",
            "usage": "To use this function, pass a configuration object containing model parameters such as `num_decoder_layers`, `max_target_length`, `chunk_attn_window_size`, `nope_layer_interval`, `per_device_batch_size`, `num_query_heads`, and `head_dim`. The function returns a single float value representing the estimated attention TFLOPs for a full training step (forward, backward, and optimizer)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_mla_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_mla_tflops_per_device(config):\n  \"\"\"Calculate Multi-Head Latent Attention TFLOP\"\"\"\n  batch_len = config.per_device_batch_size * config.max_target_length\n  qk_head_dim_sum = config.qk_nope_head_dim + config.qk_rope_head_dim\n  # calculate mla query projection\n  if config.q_lora_rank == 0:\n    q_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * qk_head_dim_sum\n  else:\n    # calculate query down and up flops\n    q_flops = (\n        2\n        * batch_len\n        * (config.emb_dim * config.q_lora_rank + config.q_lora_rank * config.num_query_heads * qk_head_dim_sum)\n    )\n  # calculate mla kv projection with down and up flops\n  kv_flops = (\n      2\n      * batch_len\n      * (\n          config.emb_dim * (config.kv_lora_rank + config.qk_rope_head_dim)\n          + config.kv_lora_rank * config.num_query_heads * (config.qk_nope_head_dim + config.v_head_dim)\n      )\n  )\n  qkv_flops = q_flops + kv_flops\n\n  attention_flops = (\n      2 * batch_len * config.max_target_length * config.num_query_heads * (qk_head_dim_sum + config.v_head_dim)\n  )\n  projection_flops = 2 * batch_len * config.emb_dim * config.num_query_heads * config.v_head_dim\n  return qkv_flops, attention_flops, projection_flops",
        "analysis": {
            "module_type": "attention_flop_calculator",
            "purpose": "Calculates the theoretical FLOPs for the QKV projection, attention mechanism, and output projection of a Multi-Head Latent Attention (MLA) layer.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the total batch length by multiplying per_device_batch_size and max_target_length.",
                "Calculate the sum of query/key head dimensions.",
                "Calculate query projection FLOPs, with a conditional path for LoRA based on q_lora_rank.",
                "Calculate key/value projection FLOPs, assuming a LoRA-like down and up projection structure.",
                "Sum the query and key/value projection FLOPs.",
                "Calculate the attention mechanism FLOPs.",
                "Calculate the output projection FLOPs.",
                "Return the calculated qkv_flops, attention_flops, and projection_flops."
            ],
            "output": {
                "shape": "A tuple of three scalar values: (qkv_flops, attention_flops, projection_flops)."
            },
            "dependencies": [],
            "parameters": {
                "config": "A configuration object containing model architecture and batch size parameters such as per_device_batch_size, max_target_length, emb_dim, num_query_heads, head dimensions, and LoRA ranks."
            },
            "notes": [
                "The function calculates raw FLOP counts, not TFLOPs (TeraFLOPs), despite the function name. The caller is responsible for scaling the result if TFLOPs are needed.",
                "The calculation for query projection FLOPs differs depending on whether LoRA is used (i.e., if config.q_lora_rank > 0).",
                "The calculation for key/value projection FLOPs assumes a LoRA-like structure with down and up projections."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_ffn_mamtul_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_ffn_mamtul_tflops_per_device(config, mlp_dim):\n  \"\"\"Helper function to calculate matmul TFLOP in ffn based on MLP dimension.\n\n  Applies to:\n    - Dense FFN layers (mlp_dim = config.mlp_dim).\n    - MoE FFN layers (mlp_dim = config.moe_mlp_dim),\n      need to scale by shared_experts or num_experts_per_tok.\n  \"\"\"\n  ffn1_flops = (\n      2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim * len(config.mlp_activations)\n  )\n  ffn2_flops = 2 * config.per_device_batch_size * config.max_target_length * mlp_dim * config.emb_dim\n  return ffn1_flops + ffn2_flops",
        "analysis": {
            "functionality": "Calculates the total floating-point operations (FLOPs) for the matrix multiplications within a Feed-Forward Network (FFN) layer.",
            "usage": "Call this function with a configuration object and the MLP dimension to get the total FLOPs for an FFN layer. The configuration object must provide `per_device_batch_size`, `max_target_length`, `emb_dim`, and `mlp_activations`. The `mlp_dim` is an integer for the hidden layer size. The function returns a single numerical value representing the total FLOPs."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_routed_and_shared_ffn_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_routed_and_shared_ffn_tflops_per_device(config):\n  \"\"\"Helper function to calculate DeepSeek-style ffn TFLOP\"\"\"\n  gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n  # Due to the mixed decoder layers, the flops is multiplied by num of layers for both dense and moe\n  num_dense_layers, num_moe_layers = get_dense_moe_layers(config)\n  dense_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * num_dense_layers\n  shared_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.shared_experts\n  routed_experts_flops = calculate_ffn_mamtul_tflops_per_device(config, config.moe_mlp_dim) * config.num_experts_per_tok\n  moe_ffn_flops = (gate_flops + shared_experts_flops + routed_experts_flops) * num_moe_layers\n  total_ffn_flops = dense_ffn_flops + moe_ffn_flops\n  return total_ffn_flops",
        "analysis": {
            "module_type": "moe_ffn_tflops_calculator",
            "purpose": "Calculates the total TeraFLOPs (TFLOPs) for the Feed-Forward Network (FFN) layers in a model with a mix of dense and DeepSeek-style Mixture-of-Experts (MoE) layers.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculate the TFLOPs for the MoE gating mechanism.",
                "Determine the number of dense and MoE layers by calling `get_dense_moe_layers`.",
                "Calculate the TFLOPs for all dense FFN layers using `calculate_ffn_mamtul_tflops_per_device`.",
                "Calculate the TFLOPs for the shared experts in the MoE layers using `calculate_ffn_mamtul_tflops_per_device`.",
                "Calculate the TFLOPs for the routed (per-token) experts in the MoE layers using `calculate_ffn_mamtul_tflops_per_device`.",
                "Sum the gate, shared, and routed expert TFLOPs and multiply by the number of MoE layers to get total MoE FFN TFLOPs.",
                "Sum the TFLOPs from the dense and MoE layers to get the final total FFN TFLOPs.",
                "Return the total FFN TFLOPs."
            ],
            "output": {
                "shape": "scalar"
            },
            "dependencies": [
                "get_dense_moe_layers",
                "calculate_ffn_mamtul_tflops_per_device"
            ],
            "parameters": {
                "config": "A configuration object containing model hyperparameters such as per_device_batch_size, max_target_length, emb_dim, num_experts, mlp_dim, moe_mlp_dim, shared_experts, and num_experts_per_tok."
            },
            "notes": [
                "This calculation is specific to models that mix standard dense FFN layers with DeepSeek-style MoE layers, which include both shared and routed experts.",
                "The function assumes a mixed decoder layer architecture where the total number of layers is split between dense and MoE types."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_dense_moe_layers",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_dense_moe_layers(config):\n  \"\"\"Helper function to calculate number of dense and moe layers\"\"\"\n  if config.decoder_block == DecoderBlockType.DEEPSEEK:\n    num_dense_layers = config.first_num_dense_layers\n    num_moe_layers = config.num_decoder_layers - config.first_num_dense_layers\n    return num_dense_layers, num_moe_layers\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    num_moe_layers = config.num_decoder_layers // config.interleave_moe_layer_step\n    num_dense_layers = config.num_decoder_layers - num_moe_layers\n  else:\n    raise ValueError(\"Currently we only support DeepSeek and Llama4 calculation.\")\n\n  return num_dense_layers, num_moe_layers",
        "analysis": {
            "functionality": "Calculates the number of dense and Mixture-of-Experts (MoE) layers based on the model's configuration and decoder block type.",
            "usage": "Call this function with a configuration object to get a tuple containing the number of dense layers and MoE layers. The calculation logic depends on the `config.decoder_block` attribute, supporting 'DEEPSEEK' and 'LLAMA4' types. For example: `num_dense, num_moe = get_dense_moe_layers(config)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_gemma3_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_gemma3_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Gemma3 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.image_size_for_vit  # Gemma3 default 896\n  embed_dim = config.emb_dim  # text embedding dim after projection\n  # Values below are hardcoded in Gemma3VisionEncoderLayer\n  patch_size = 14\n  hidden_dim = 1152\n  intermediate_dim = 4304\n  num_layers = 27\n  vision_exit_pooling_window = 4\n\n  # 1. Patch embedding (Conv2D)\n  num_patches_h = H // patch_size\n  num_patches_w = W // patch_size\n  seq_len = num_patches_h * num_patches_w  # 64*64=4096\n  patch_embed_flops = 2 * B * seq_len * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. gemma3.Encoder: num_layers * gemma3.Encoder1DBlock\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 4. VisionEmbedder\n  seq_len_after_pooling = (num_patches_h // vision_exit_pooling_window) * (num_patches_w // vision_exit_pooling_window)\n  vision_embedder_flops = 2 * B * seq_len_after_pooling * hidden_dim * embed_dim  # One linear projection\n\n  # Learnable weights summation\n  learnable_weight_flops = patch_embed_flops + encoder_flops + vision_embedder_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * vision_embedder_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "module_type": "vision_transformer_tflops_calculator",
            "purpose": "Estimates the total, learnable weight, and attention TFLOPs per device for a Gemma3-style vision encoder based on a given configuration.",
            "input": {
                "shape": "N/A",
                "dtype": "Config object"
            },
            "processing_steps": [
                "Extract model dimensions from the input config object and use hardcoded architectural values for Gemma3.",
                "Calculate the FLOPs for the patch embedding (Conv2D) operation.",
                "Calculate the FLOPs for the transformer encoder layers, separating them into learnable components (QKV, projection, MLP) and non-learnable attention matrix multiplications.",
                "Calculate the FLOPs for the final vision embedder projection layer.",
                "Sum the FLOPs from all learnable weight components (patch embedding, encoder, and vision embedder).",
                "Scale the learnable weight FLOPs to account for forward, backward, and optimizer steps, adjusting the calculation if the vision encoder is frozen.",
                "Convert the raw FLOP counts for learnable weights and attention to TFLOPs by dividing by 1e12.",
                "Sum the learnable weight TFLOPs and attention TFLOPs to get the total TFLOPs."
            ],
            "output": {
                "shape": "A tuple of three scalar float values: (total_tflops, learnable_weight_tflops, attention_tflops)."
            },
            "dependencies": [],
            "parameters": {
                "per_device_batch_size": "The batch size processed by a single device.",
                "num_channels_for_vit": "The number of input channels for the vision transformer (e.g., 3 for RGB).",
                "image_size_for_vit": "The height and width of the input image.",
                "emb_dim": "The dimension of the text embedding space, which the vision embeddings are projected into.",
                "freeze_vision_encoder_params": "A boolean flag that determines how to scale the learnable weight FLOPs for training."
            },
            "notes": [
                "The function uses several hardcoded architectural parameters specific to the Gemma3 vision encoder, such as patch size (14), hidden dimension (1152), number of layers (27), etc.",
                "The TFLOPs calculation for learnable weights accounts for the forward pass, backward pass, and optimizer updates by typically multiplying the forward FLOPs by 3.",
                "The function separates the TFLOPs into contributions from learnable weights (convolutions, linear layers) and non-learnable attention matrix multiplications."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_llama4_vision_layers_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_llama4_vision_layers_tflops_per_device(config):\n  \"\"\"\n  Estimate TFLOPs for Llama4 vision encoder (ViT-style).\n  Returns:\n      total_tflops: Total TFLOPs (counts for fwd + bwd + optimizer)\n      learnable_weight_tflops: TFLOPs from learnable weights (patch embedding, qkv, MLP, projections)\n      attention_tflops: TFLOPs from attention multiplications\n  \"\"\"\n  # Config values\n  B = config.per_device_batch_size\n  C = config.num_channels_for_vit\n  H = W = config.tile_size_for_vit\n  patch_size = config.patch_size_for_vit\n  hidden_dim = config.hidden_size_for_vit\n  intermediate_dim = config.intermediate_size_for_vit\n  num_layers = config.num_hidden_layers_for_vit\n  pixel_shuffle_fc1_out_dim = config.projector_input_dim_for_vit  # 4096\n  pixel_shuffle_fc2_out_dim = config.projector_output_dim_for_vit  # 4096\n  base_emb_dim = config.base_emb_dim\n  pixel_shuffle_ratio = config.pixel_shuffle_ratio_for_vit  # 0.5\n  num_patches = (H // patch_size) * (W // patch_size)  # 24*24 = 576\n  pixel_shuffle_tokens = num_patches * pixel_shuffle_ratio**2  # 144\n\n  # 1. Llama4UnfoldConvolution (flops by linear projection)\n  # lax.conv_general_dilated_patches extracts patches through reshaping/indexing without flops\n  # Each patch: C * patch_size * patch_size -> hidden_dim\n  patch_embed_flops = 2 * B * num_patches * (C * patch_size * patch_size) * hidden_dim\n\n  # 2. Llama4VisionEncoder: num_layers * (qkv + att_projection + mlp)\n  seq_len = num_patches + 1  # +1 for class token, so 577\n  qkv_flops_per_layer = 3 * (2 * B * seq_len * hidden_dim * hidden_dim)  # Q, K, V projections\n  attn_flops_per_layer = 4 * B * seq_len * seq_len * hidden_dim  # Attention scores and weighted sum\n  projection_flops_per_layer = 2 * B * seq_len * hidden_dim * hidden_dim  # projection after attention multiplication\n  mlp_flops_per_layer = 2 * (2 * B * seq_len * hidden_dim * intermediate_dim)  # two fc layers\n  total_attn_flops = attn_flops_per_layer * num_layers\n  vision_encoder_flops = (+qkv_flops_per_layer + projection_flops_per_layer + mlp_flops_per_layer) * num_layers\n\n  # 3. Llama4VisionPixelShuffleMLP\n  # (B, 144, 5632) -> (B, 144, 4096) -> (B, 144, 4096)\n  pixel_shuffle_fc1_flops = 2 * B * pixel_shuffle_tokens * intermediate_dim * pixel_shuffle_fc1_out_dim\n  pixel_shuffle_fc2_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * pixel_shuffle_fc2_out_dim\n  pixel_shuffle_total_flops = pixel_shuffle_fc1_flops + pixel_shuffle_fc2_flops\n\n  # 4. Llama4MultiModalProjector: (B, 144, 5120) x (5120, base_emb_dim)\n  projector_flops = 2 * B * pixel_shuffle_tokens * pixel_shuffle_fc1_out_dim * base_emb_dim\n\n  # Learnable weights: all matmuls above\n  learnable_weight_flops = patch_embed_flops + vision_encoder_flops + pixel_shuffle_total_flops + projector_flops\n\n  if config.freeze_vision_encoder_params:\n    learnable_weight_flops += 2 * projector_flops  # only projector is learnable, add fwd+optimizer\n  else:\n    learnable_weight_flops *= 3  # multiply by 3 for fwd + bwd + optimizer\n\n  # Convert to TFLOPs\n  learnable_weight_tflops = learnable_weight_flops / 1e12\n  total_attn_tflops = total_attn_flops / 1e12\n  total_tflops = learnable_weight_tflops + total_attn_tflops\n\n  return total_tflops, learnable_weight_tflops, total_attn_tflops",
        "analysis": {
            "module_type": "vision_encoder_tflops_calculator",
            "purpose": "Estimates the training TFLOPs per device for the Llama4 vision encoder, breaking it down into learnable weights and attention computations.",
            "input": {
                "shape": "N/A",
                "dtype": "A configuration object (e.g., a class instance with attributes)."
            },
            "processing_steps": [
                "Extract model dimensions and training parameters from the input config object.",
                "Calculate FLOPs for the patch embedding linear projection.",
                "Calculate FLOPs for the vision encoder layers, including QKV projections, attention multiplication, output projection, and MLP layers.",
                "Calculate FLOPs for the pixel shuffle MLP.",
                "Calculate FLOPs for the final multi-modal projector.",
                "Sum the FLOPs from all learnable weight components (projections and MLPs).",
                "Adjust the learnable weight FLOPs for training (forward, backward, optimizer) based on whether the vision encoder is frozen.",
                "Convert raw FLOP counts for learnable weights and attention to TFLOPs by dividing by 1e12.",
                "Sum the learnable weight TFLOPs and attention TFLOPs to get the total TFLOPs."
            ],
            "output": {
                "shape": "A tuple of three scalar float values: (total_tflops, learnable_weight_tflops, attention_tflops)."
            },
            "dependencies": [],
            "parameters": {
                "per_device_batch_size": "The batch size processed by a single device.",
                "tile_size_for_vit": "The height and width of the input image tile.",
                "patch_size_for_vit": "The size of each square patch the image is divided into.",
                "hidden_size_for_vit": "The dimensionality of the transformer's hidden states.",
                "intermediate_size_for_vit": "The dimensionality of the MLP's intermediate layer.",
                "num_hidden_layers_for_vit": "The number of transformer layers in the vision encoder.",
                "projector_input_dim_for_vit": "The input dimension for the pixel shuffle MLP.",
                "projector_output_dim_for_vit": "The output dimension for the pixel shuffle MLP.",
                "base_emb_dim": "The final embedding dimension after the multi-modal projector.",
                "freeze_vision_encoder_params": "A boolean indicating whether to freeze the vision encoder weights during training."
            },
            "notes": [
                "The calculation assumes a standard Vision Transformer (ViT) architecture with a class token.",
                "FLOPs for learnable weights are multiplied by 3 to account for the forward pass, backward pass, and optimizer updates, unless the encoder is frozen.",
                "The function separates FLOPs from matrix multiplications involving learnable weights from FLOPs from the attention score calculations.",
                "The calculation includes FLOPs from the patch embedding, all transformer layers, a pixel shuffle MLP, and a final multi-modal projector."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_vision_encoder_tflops",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_vision_encoder_tflops(config):\n  \"\"\"Calculate vision encoder TFLOPs per prefill step per device.\"\"\"\n  if config.model_name.startswith(\"gemma3\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_gemma3_vision_layers_tflops_per_device(\n        config\n    )\n  elif config.model_name.startswith(\"llama4\"):\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_llama4_vision_layers_tflops_per_device(\n        config\n    )\n  else:\n    max_logging.log(\n        f\"Vision encoder TFLOPs calculation not implemented for model {config.model_name}, counting as 0 for now.\"\n    )\n    mm_total_tflops = mm_learnable_weight_tflops = mm_attention_tflops = 0\n\n  return mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops",
        "analysis": {
            "module_type": "vision_encoder_tflops_calculator",
            "purpose": "Calculates the total, learnable weight, and attention TFLOPs for a vision encoder per prefill step per device, dispatching to the appropriate model-specific calculation function.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.model_name` starts with 'gemma3' and call `calculate_gemma3_vision_layers_tflops_per_device` if it does.",
                "Check if `config.model_name` starts with 'llama4' and call `calculate_llama4_vision_layers_tflops_per_device` if it does.",
                "If the model name is not recognized, log a warning and set all TFLOPs values to 0.",
                "Return the calculated total, learnable weight, and attention TFLOPs."
            ],
            "output": {
                "shape": "A tuple of three scalar values: (mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops)."
            },
            "dependencies": [
                "calculate_gemma3_vision_layers_tflops_per_device",
                "calculate_llama4_vision_layers_tflops_per_device",
                "max_logging.log"
            ],
            "parameters": {
                "config.model_name": "A string representing the model name, used to select the correct TFLOPs calculation logic (e.g., 'gemma3', 'llama4')."
            },
            "notes": [
                "This function acts as a dispatcher based on the model name provided in the configuration.",
                "For unsupported model names, it defaults to returning zero for all TFLOPs values."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_tflops_training_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_tflops_training_per_device(config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  # MLP flops\n  if config.num_experts > 1:\n    # calculation based on dropless implementation\n    if config.decoder_block in (DecoderBlockType.DEEPSEEK, DecoderBlockType.LLAMA4):\n      total_ffn_flops = calculate_routed_and_shared_ffn_tflops_per_device(config)\n    else:\n      gate_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.num_experts\n      total_ffn_flops = (\n          gate_flops + calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim) * config.num_experts_per_tok\n      )\n  else:\n    total_ffn_flops = calculate_ffn_mamtul_tflops_per_device(config, config.mlp_dim)\n\n  # Attention flops\n  if config.attention_type == \"mla\":\n    qkv_flops, noncausal_attention_flops, projection_flops = calculate_mla_tflops_per_device(config)\n  else:\n    qkv_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * (config.num_query_heads + 2 * config.num_kv_heads)\n        * config.head_dim\n    )\n    noncausal_attention_flops = (\n        4 * config.per_device_batch_size * config.max_target_length**2 * config.num_query_heads * config.head_dim\n    )\n    projection_flops = (\n        2\n        * config.per_device_batch_size\n        * config.max_target_length\n        * config.emb_dim\n        * config.num_query_heads\n        * config.head_dim\n    )\n\n  # Divide attention flops by 2 due to causal mask\n  # References:\n  # NVIDIA/Megatron-LM (2025 March): https://github.com/NVIDIA/Megatron-LM/blob/250b79415dcc4b660521273c87f15334c804eeae/megatron/training/training.py#L361-L362\n  # NVIDIA/NeMo (2025 April): https://github.com/NVIDIA/NeMo/blob/ba4d6d116463de512ff0cfc14641aa6cf4577a42/nemo/utils/flops_formulas.py#L259-L272\n  causal_attention_flops = noncausal_attention_flops / 2\n\n  # Embedding flops\n  embedding_flops = 2 * config.per_device_batch_size * config.max_target_length * config.emb_dim * config.vocab_size\n\n  # Combine flops with number of decoder layers\n  if config.decoder_block == DecoderBlockType.GEMMA2:\n    attention_tflops, learnable_weight_tflops = calculate_gemma2_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops\n    )\n  elif config.decoder_block == DecoderBlockType.GEMMA3:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device (\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=6\n    )\n  elif config.decoder_block == DecoderBlockType.GPT_OSS:\n    attention_tflops, learnable_weight_tflops = calculate_mixed_attention_model_tflops_training_per_device(\n        config, total_ffn_flops, qkv_flops, projection_flops, embedding_flops, attention_pattern_length=2\n    )\n  elif config.decoder_block == DecoderBlockType.LLAMA4:\n    # Use the new helper to calculate attention TFLOPs correctly.\n    attention_tflops = calculate_llama4_attention_tflops(config)\n    # The learnable weight calculation remains the same as it correctly handles Llama4's MoE structure.\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n  elif config.decoder_block == DecoderBlockType.DEEPSEEK:\n    learnable_weight_tflops = (\n        (total_ffn_flops + (qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n  else:\n    # multiply by 3 for both feed forward and back propagation flops\n    learnable_weight_tflops = (\n        ((total_ffn_flops + qkv_flops + projection_flops) * config.num_decoder_layers + embedding_flops) * 3 / 10**12\n    )\n    attention_tflops = causal_attention_flops * config.num_decoder_layers * 3 / 10**12\n\n  learnable_weight_tflops = learnable_weight_tflops * config.gradient_accumulation_steps\n  attention_tflops = attention_tflops * config.gradient_accumulation_steps\n\n  # DPO includes one additional forward pass per gradient accumulation step\n  if config.use_dpo:\n    reference_model_tflops = learnable_weight_tflops / 3  # additional forward pass\n    reference_model_attention_tflops = attention_tflops / 3\n    attention_tflops = attention_tflops + reference_model_attention_tflops\n  else:\n    reference_model_tflops = 0\n\n  total_tflops = learnable_weight_tflops + attention_tflops + reference_model_tflops\n\n  if config.use_multimodal:\n    # Add vision layers TFLOPs for multimodal models\n    mm_total_tflops, mm_learnable_weight_tflops, mm_attention_tflops = calculate_vision_encoder_tflops(config)\n    if log:\n      print(\n          f\"{config.model_name} vision layers per train step:\\n\",\n          f\"Total TFLOPs: {mm_total_tflops:.2f} \\n\",\n          f\"split as {100 * mm_learnable_weight_tflops/mm_total_tflops:.2f}% learnable weight flops\",\n          f\"and {100 * mm_attention_tflops/mm_total_tflops:.2f}% attention flops;\\n\",\n          f\"learnable weight {mm_learnable_weight_tflops:.2f} TFLOPs, attention {mm_attention_tflops:.2f} TFLOPs\",\n      )\n    total_tflops += mm_total_tflops\n    learnable_weight_tflops += mm_learnable_weight_tflops\n    attention_tflops += mm_attention_tflops\n\n  if log:\n    print(\n        \"Per train step:\\n\",\n        f\"Total TFLOPs: {total_tflops:.2f} \\n\",\n        f\"split as {100 * learnable_weight_tflops/total_tflops:.2f}% learnable weight flops\",\n        f\"and {100 * attention_tflops/total_tflops:.2f}% attention flops\",\n    )\n  return total_tflops, learnable_weight_tflops, attention_tflops",
        "analysis": {
            "functionality": "Calculates the total TeraFLOPs (TFLOPs) per device for a single training step, breaking it down by learnable weights and attention operations.",
            "usage": "Call this function with a configuration object to estimate the computational cost of a training step. It returns a tuple of three scalar floats: (total_tflops, learnable_weight_tflops, attention_tflops)."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#calculate_prefill_tflops_per_device",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def calculate_prefill_tflops_per_device(num_model_parameters, prefill_length, config, log=True):\n  \"\"\"Calculate training TFLOP\"\"\"\n  learnable_weight_tflops = 2 * num_model_parameters * prefill_length / jax.device_count() / 1e12\n  noncausal_attention_flops = (\n      4\n      * config.num_query_heads\n      * config.num_decoder_layers\n      * config.head_dim\n      * prefill_length**2\n      / jax.device_count()\n      / 1e12\n  )\n  causal_attention_tflops = noncausal_attention_flops / 2  # due to causality in attention\n  total_tflops = learnable_weight_tflops + causal_attention_tflops\n\n  if log:\n    print(\n        \"Per prefill step per device: \\n\",\n        f\"\\tTotal TFLOPs: {total_tflops:.2f} \\n\",\n        f\"\\t\\tLearnable weight TFLOPs: {learnable_weight_tflops:.2f} \",\n        f\"({100 * learnable_weight_tflops/total_tflops:.2f})% of Total\\n\",\n        f\"\\t\\tCausal attention TFLOPs: {causal_attention_tflops:.2f} \",\n        f\"({100 * causal_attention_tflops/total_tflops:.2f})% of Total\",\n    )\n  return total_tflops, learnable_weight_tflops, causal_attention_tflops",
        "analysis": {
            "functionality": "Calculates the estimated TeraFLOPs (TFLOPs) per device for a single prefill (forward pass) step of a transformer model.",
            "usage": "To use this function, provide the total number of model parameters, the length of the prefill sequence, and a configuration object containing model architecture details. It returns a tuple of floats: (total_tflops, learnable_weight_tflops, causal_attention_tflops). Example: `total, weights, attention = calculate_prefill_tflops_per_device(1.5e9, 1024, model_config)`."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_mesh_axes_used_by_tensor_spec",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_mesh_axes_used_by_tensor_spec(tensor_sharding_spec):\n  \"\"\"\n  Extracts the set of mesh axis names that a tensor's PartitionSpec uses.\n\n  This function inspects a tensor's sharding specification (PartitionSpec) and\n  identifies which mesh axes are actively used for sharding. If a tensor is not\n  sharded (i.e., fully replicated), the resulting set will be empty.\n\n  Args:\n    tensor_sharding_spec: The PartitionSpec of a tensor, which defines how it's partitioned across the mesh.\n    It can be None or contain strings and iterables representing the mesh axes.\n    all_mesh_axis_names: A collection of all available mesh axis names in the current device mesh.\n\n  Returns:\n    A set of strings, where each string is a mesh axis name used by the\n    tensor's sharding spec. Returns an empty set for unsharded tensors.\n  \"\"\"\n  # Flatten the sharding spec, as it can contain nested iterables (e.g., ('data', 'mdl')).\n  tensor_sharding_spec = sum(\n      [\n          [axis] if isinstance(axis, str) else list(axis) if isinstance(axis, Iterable) else []\n          for axis in tensor_sharding_spec\n      ],\n      [],\n  )\n  return tensor_sharding_spec",
        "analysis": {
            "functionality": "Flattens a tensor's sharding specification (PartitionSpec) to extract a simple list of all mesh axis names used for sharding.",
            "usage": "Input a tensor's PartitionSpec, which is an iterable that may contain nested iterables of strings (mesh axis names). The function returns a flattened list of all axis names found in the spec."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_get_nontrival_mesh_axes",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _get_nontrival_mesh_axes(mesh):\n  \"\"\"\n  Returns mesh axes from config that are valid and have more than one shard.\n\n  This function identifies which of the predefined potential sharding axes are\n  actually present in the current device mesh and are configured with a size\n  greater than one (i.e., are actually sharded).\n\n  Args:\n    mesh: The device mesh object, which contains information about the mesh topology, including axis names and their sizes.\n\n  Returns:\n    A set of strings, where each string is a mesh axis name that is both\n    pre-configured as a target for sharding and has more than one shard in the mesh.\n  \"\"\"\n\n  target_sharding_axes_config = [\n      \"fsdp\",\n      \"fsdp_transpose\",\n      \"sequence\",\n      \"context\",\n      \"context_autoregressive\",\n      \"tensor\",\n      \"tensor_transpose\",\n      \"tensor_sequence\",\n      \"stage\",\n      \"expert\",\n  ]\n\n  # Filter the target axes to find those that exist in the current mesh\n  # and have a size greater than 1, meaning they are actually used for sharding.\n  return {axis for axis in target_sharding_axes_config if axis in mesh.axis_names and mesh.shape[axis] > 1}",
        "analysis": {
            "module_type": "mesh_axis_filter",
            "purpose": "Filters a predefined list of potential sharding axes to find those that are actively used (size > 1) in the provided device mesh.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a static list of target sharding axis names.",
                "Iterate through the target axis names using a set comprehension.",
                "Filter for axes that are present in the input `mesh.axis_names`.",
                "Further filter for axes where the corresponding size in `mesh.shape` is greater than 1.",
                "Return the resulting set of axis names that meet both conditions."
            ],
            "output": {
                "shape": "A Python set of strings (e.g., {'fsdp', 'tensor'})."
            },
            "dependencies": [],
            "parameters": {},
            "notes": [
                "The function identifies axes that are 'nontrivial', meaning they are actually used for sharding because their dimension size is greater than one.",
                "The list of potential sharding axes (`target_sharding_axes_config`) is hardcoded within the function.",
                "The input `mesh` is expected to be a JAX Mesh object with `axis_names` and `shape` attributes."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_analyze_sharding",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _analyze_sharding(params, mesh, valid_target_mesh_axes):\n  \"\"\"\n  Analyzes parameters to find which are unsharded on any valid mesh axis.\n\n  This function iterates through all parameters in a model, checking their\n  sharding specifications. It identifies parameters that are not sharded along any\n  of the provided valid target axes (i.e., they are fully replicated across these axes).\n\n  Args:\n    params: A PyTree of model parameters.\n    mesh: The device mesh object.\n    valid_target_mesh_axes: A set of mesh axis names that are considered valid targets for sharding.\n\n  Returns:\n    A tuple containing:\n      - unsharded_params_total_size (int): The total size (number of elements) of all parameters found to be\n        unsharded on the target axes.\n      - problematic_tensors_details (list): A list of dictionaries, where each\n        dictionary contains details about a tensor that is not sharded on any of the target axes.\n  \"\"\"\n  unsharded_params_total_size = 0  # Initialize a counter for the size of unsharded parameters.\n  problematic_tensors_details = []  # Initialize a list to store details of problematic tensors.\n\n  # Get a flattened list of all parameters (leaves) in the PyTree, along with their paths.\n  all_params_leaves = jtu.tree_leaves_with_path(params)\n\n  for path, p_leaf in all_params_leaves:  # Iterate over each parameter leaf\n    param_name_str = jtu.keystr(path)  # Convert the tree path to a readable string\n\n    # Check that sharding and spec exist and are valid\n    sharding = getattr(p_leaf, \"sharding\", None)\n    spec = getattr(sharding, \"spec\", None)\n    assert sharding is not None and spec is not None and isinstance(spec, P), (\n        f\"Parameter '{param_name_str}' is missing a valid '.sharding.spec'.\"\n        \"Expected 'p_leaf.sharding.spec' to be a non-null 'partitionspec'.\"\n    )\n\n    current_sharding_spec = p_leaf.sharding.spec  # Extract the current tensor's sharding spec\n    # Identify axes used for sharding\n    mesh_axes_used = get_mesh_axes_used_by_tensor_spec(current_sharding_spec)\n    # Check if the parameter is sharded on all the valid target axes.\n    is_sharded_on_all_target_axis = all(axis in mesh_axes_used for axis in valid_target_mesh_axes)\n\n    # If the parameter is not sharded on all of the target axes, it's considered \"problematic.\"\n    if not is_sharded_on_all_target_axis:\n      unsharded_params_total_size += p_leaf.size  # Add to total unsharded parameter size\n      unsharded_axes = set(valid_target_mesh_axes) - set(mesh_axes_used)\n      # Add detailed info to list of problematic tensors\n      problematic_tensors_details.append(\n          {\n              \"name\": param_name_str,  # Tensor name\n              \"size\": p_leaf.size,  # tensor size\n              \"shape\": p_leaf.shape,  # tensor shape\n              \"spec\": str(current_sharding_spec),  # Tensor sharding spec as string\n              \"available_axes\": sorted(list(valid_target_mesh_axes)),  # Axes that could be used for sharding\n              \"unsharded_axes\": sorted(list(unsharded_axes)),  # Unsharded axes\n          }\n      )\n  # Return the total size of unsharded parameters and the list of problematic tensors.\n  return unsharded_params_total_size, problematic_tensors_details",
        "analysis": {
            "functionality": "Analyzes a PyTree of model parameters to identify tensors that are not sharded along a specified set of valid mesh axes. It calculates the total size of these unsharded parameters and returns detailed information about each one.",
            "usage": "Call this function with a PyTree of parameters, a JAX mesh object, and a set of target mesh axis names. It returns a tuple containing the total size of unsharded parameters and a list of dictionaries with details about each problematic tensor."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#_raise_if_unsharded_exceeds_tolerance",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def _raise_if_unsharded_exceeds_tolerance(unsharded_size, total_size, tolerance, problematic_tensors_details):\n  \"\"\"\n  Raises an AssertionError if the percentage of unsharded parameters exceeds the given tolerance.\n\n  This function calculates the proportion of model parameters that are unsharded\n  and compares it against a specified tolerance. If the tolerance is exceeded,\n  it constructs and raises a detailed error message.\n\n  Args:\n    unsharded_size: The total size of parameters not sharded on target axes.\n    total_size: The total size of all parameters in the model.\n    tolerance: A float (e.g., 0.05 for 5%) representing the maximum allowed percentage of unsharded parameters.\n    problematic_tensors_details: A list of details about the unsharded tensors,\n    used to generate an informative error message.\n\n  Raises:\n    AssertionError: If the percentage of unsharded parameters is greater than the tolerance.\n  \"\"\"\n  if total_size <= 0:\n    raise ValueError(\"Total size must be greater than zero.\")\n\n  # Calculate the percentage of unsharded parameters.\n  unsharded_param_perc = unsharded_size / total_size\n\n  # If the percentage is over the tolerance, prepare and raise an error.\n  if unsharded_param_perc > tolerance:\n    # Sort the problematic tensors by size to show the largest ones first.\n    problematic_tensors_details.sort(key=lambda x: x[\"size\"], reverse=True)\n\n    # Begin constructing the error message.\n    error_msg_lines = [\n        f\"Unsharded parameter percentage ({unsharded_param_perc:.2%})\" f\"exceeds tolerance ({tolerance:.2%}).\"\n    ]\n    # Add a header explaining the issue.\n    error_msg_lines.append(\n        \"The following large tensors are replicated (unsharded) but could be sharded on at \"\n        \"least one of the available axes:\"\n    )\n    # Add details for the top 5 largest problematic tensors.\n    for detail in problematic_tensors_details[:5]:  # Show top 5 largest problematic tensors\n      error_msg_lines.append(\n          f\" - Name: {detail['name']}(Size: {detail['size']}, Shape: {detail['spec']}, Spec: {detail['spec']}) \"\n          f\" is unsharded on axis: {detail['unsharded_axes']}\"\n          f\" could be sharded on: {detail['available_axes']}\"\n      )\n\n    # Raise the assertion error with the combined, formatted message.\n    raise AssertionError(\"\\n\".join(error_msg_lines))",
        "analysis": {
            "module_type": "sharding_validation_utility",
            "purpose": "Calculates the percentage of unsharded model parameters and raises a detailed AssertionError if this percentage exceeds a specified tolerance.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Validate that `total_size` is greater than zero, raising a ValueError otherwise.",
                "Calculate the percentage of unsharded parameters by dividing `unsharded_size` by `total_size`.",
                "Compare the calculated percentage against the `tolerance`.",
                "If the percentage exceeds the tolerance, sort the `problematic_tensors_details` list by tensor size in descending order.",
                "Construct a detailed error message including the percentage, tolerance, and details of the top 5 largest problematic tensors.",
                "Raise an AssertionError with the constructed error message."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "unsharded_size": "The total size of parameters not sharded on target axes.",
                "total_size": "The total size of all parameters in the model.",
                "tolerance": "A float (e.g., 0.05 for 5%) representing the maximum allowed percentage of unsharded parameters.",
                "problematic_tensors_details": "A list of dictionaries containing details about the unsharded tensors, used to generate an informative error message."
            },
            "notes": [
                "This function does not return a value; its purpose is to raise an exception if a condition is met.",
                "The error message is formatted to be highly informative, showing the top 5 largest unsharded tensors to aid in debugging sharding configurations."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#assert_params_sufficiently_sharded",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def assert_params_sufficiently_sharded(params, mesh, tolerance):\n  \"\"\"\n  Asserts that the total size of replicated parameters is within a given tolerance.\n\n  This is the main function that orchestrates the sharding analysis. It determines\n  the total number of parameters, identifies valid sharding axes, analyzes the\n  sharding of all parameters, and then raises an error if the amount of\n  unsharded parameters exceeds the specified tolerance.\n\n  Args:\n    params: A PyTree of model parameters.\n    mesh: The device mesh object.\n    tolerance: A float representing the maximum allowed percentage of unsharded parameters.\n  \"\"\"\n  # Calculate the total size of all parameters in the model.\n  total_num_params = max_utils.calculate_bytes_from_pytree(params)\n\n  # Get the set of nontrival mesh axes that can be used for sharding.\n  valid_target_mesh_axes = _get_nontrival_mesh_axes(mesh)\n  # If there are no valid axes to shard along, there's nothing to check, so we can exit.\n  if not valid_target_mesh_axes:\n    return  # Exit early\n\n  # Analyze the parameters to find the total size of unsharded parameters\n  # and get details on which tensors are problematic.\n  unsharded_params_total_size, problematic_tensors_details = _analyze_sharding(params, mesh, valid_target_mesh_axes)\n\n  # Check if the amount of unsharded parameters is within the tolerance and\n  # raise an exception if it is not.\n  _raise_if_unsharded_exceeds_tolerance(\n      unsharded_params_total_size, total_num_params, tolerance, problematic_tensors_details\n  )",
        "analysis": {
            "functionality": "Asserts that the total size of replicated (unsharded) model parameters is within a specified tolerance, raising an error if the threshold is exceeded.",
            "usage": "Call this function with a PyTree of model parameters, a JAX device mesh, and a float tolerance (e.g., 0.05 for 5%). The function will either complete silently if the sharding is sufficient or raise an `AssertionError` with details about unsharded tensors if the tolerance is exceeded. It is used to validate that model parameters are being sharded effectively to save memory."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#apply_gradient_clipping",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def apply_gradient_clipping(raw_grads, state, clipping_threshold):\n  \"\"\"Applies gradient clipping to raw gradients, with special handing for FLAX fp8 stats.\n\n  Args:\n    raw_grads: A pytree of raw gradients.\n    state: The current optimizer state.\n    clipping_threshold: The gradient clipping threshold.\n\n  Returns:\n    A pytree of clipped gradients.\n  \"\"\"\n  gradient_clip_transformation = optax.clip_by_global_norm(clipping_threshold)\n  if OVERWRITE_WITH_GRADIENT in raw_grads:\n    # Scales + Amax History for Delayed Tensor Scaling SHOULD NOT be clipped or affect clipping\n    fp8_stats = raw_grads.pop(OVERWRITE_WITH_GRADIENT)\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n    grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n    raw_grads[OVERWRITE_WITH_GRADIENT] = fp8_stats  # pytype: disable=unsupported-operands\n  else:\n    grads, _ = gradient_clip_transformation.update(raw_grads, state, None)\n\n  return grads",
        "analysis": {
            "functionality": "This function applies global norm gradient clipping to a pytree of gradients. It includes special handling to exclude FLAX fp8 statistics, identified by the `OVERWRITE_WITH_GRADIENT` key, from the clipping process.",
            "usage": "To use this function, provide a pytree of raw gradients (`raw_grads`), the current optimizer state (`state`), and a `clipping_threshold` value. The function returns a new pytree of clipped gradients. If `raw_grads` contains a key `_overwrite_with_gradient`, the corresponding value is preserved and not clipped."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_nested_value",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_nested_value(dictionary, nested_key, default=None):\n  \"\"\"\n  Retrieves a value from a nested key in a dictionary.\n\n  Args:\n      dictionary: The dictionary to search in.\n      nested_key: A tuple representing the nested key, e.g., ('level1', 'level2', 'key').\n      default: The value to return if the nested key is not found.\n\n  Returns:\n      The value associated with the nested key, or the default value if not found.\n  \"\"\"\n  current_level = dictionary\n\n  for key in nested_key:\n    if not isinstance(current_level, dict) or key not in current_level:\n      return default\n    current_level = current_level[key]\n  return current_level",
        "analysis": {
            "module_type": "nested_dictionary_accessor",
            "purpose": "Safely retrieves a value from a dictionary using a sequence of nested keys, returning a default value if any key in the path is not found.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize a 'current_level' variable with the input dictionary.",
                "Iterate through each key in the 'nested_key' tuple.",
                "For each key, check if the 'current_level' is a dictionary and if the key exists within it.",
                "If the check fails, return the default value immediately.",
                "If the check passes, update 'current_level' to the value associated with the key.",
                "After the loop completes successfully, return the final 'current_level' value."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [],
            "parameters": {
                "dictionary": "The dictionary to search in.",
                "nested_key": "A tuple or other iterable representing the path of keys to traverse, e.g., ('level1', 'level2', 'key').",
                "default": "The value to return if the nested key path is not found or invalid."
            },
            "notes": [
                "The function handles cases where an intermediate value in the path is not a dictionary, preventing potential TypeErrors.",
                "It short-circuits and returns the default value as soon as an invalid key or a non-dictionary level is encountered."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_decode_state(apply_fn, params) -> train_state.TrainState:\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState(step=0, apply_fn=apply_fn, params=params, tx=None, opt_state={})  # type: ignore\n  return state",
        "analysis": {
            "module_type": "decode_state_initializer",
            "purpose": "Initializes a Flax TrainState object specifically for decoding or inference, with a null optimizer state.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Instantiates a `flax.training.train_state.TrainState` object.",
                "Initializes the state with `step=0`, the provided `apply_fn` and `params`.",
                "Sets the optimizer transaction (`tx`) to `None` and the optimizer state (`opt_state`) to an empty dictionary."
            ],
            "output": {
                "shape": "A `flax.training.train_state.TrainState` object containing the provided `params`."
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {},
            "notes": [
                "This function is specifically designed for creating a state for inference, where optimizer-related fields are not necessary.",
                "The `type: ignore` comment suggests that the type checker might complain about `tx` being `None` or `opt_state` being `{}`, but this is the intended behavior for a decode state."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_training_state(apply_fn, params, tx):\n  \"\"\"Init train state with null opt state for decode.\"\"\"\n  state = train_state.TrainState.create(apply_fn=apply_fn, params=params, tx=tx)\n  return state",
        "analysis": {
            "module_type": "training_state_initializer",
            "purpose": "Creates and initializes a Flax `TrainState` object, which bundles the model's apply function, parameters, and an optimizer transformation.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calls `train_state.TrainState.create` using the provided `apply_fn`, `params`, and `tx` to instantiate a new training state."
            ],
            "output": {
                "shape": "A Flax `train_state.TrainState` object containing the model's apply function, parameters, and the initialized optimizer state."
            },
            "dependencies": [
                "flax.training.train_state.TrainState"
            ],
            "parameters": {
                "apply_fn": "The model's forward pass function.",
                "params": "A PyTree of the model's trainable parameters.",
                "tx": "An Optax gradient transformation (optimizer)."
            },
            "notes": [
                "The function's docstring, 'Init train state with null opt state for decode.', appears to be a copy-paste error from the `init_decode_state` function, as this function initializes a state for training with an active optimizer (`tx`)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#init_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def init_initial_state(model, tx, config, is_training, key):\n  \"\"\"\n  We pass in \"static\" objects like model, tx, config as JAX compares them by\n  object hash, and instantiating them inside causes pjit top-level annotations\n  to fail to match as pytree prefixes if we re-instantiate.\n\n  Args: model, tx, config, is_training, key\n  \"\"\"\n  input_shape = (config.micro_batch_size_to_train_on, config.max_target_length)\n  image_shape = multimodal_utils.get_dummy_image_shape_for_init(config.model_name, batch_size=config.micro_batch_size_to_train_on)\n  model_vars = model.init(\n      {\"params\": key, \"dropout\": key, \"aqt\": key},\n      np.ones(input_shape, dtype=jnp.int32),\n      np.ones(input_shape, dtype=jnp.int32),\n      encoder_images=np.ones(image_shape, dtype=jnp.int32) if config.use_multimodal else None,\n      # nnx_method=\"no_op\",\n  )\n  if is_training:\n    return init_training_state(model.apply, model_vars, tx)\n  return init_decode_state(model.apply, model_vars)",
        "analysis": {
            "module_type": "state_initializer",
            "purpose": "Initializes the model's parameters and creates a Flax TrainState for either training or decoding based on a boolean flag.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Determine the shape for dummy text inputs using `config.micro_batch_size_to_train_on` and `config.max_target_length`.",
                "If `config.use_multimodal` is true, determine the shape for dummy image inputs using `multimodal_utils.get_dummy_image_shape_for_init`.",
                "Initialize model variables by calling `model.init` with PRNG keys and dummy tensors created with `np.ones`.",
                "Check the boolean `is_training` flag.",
                "If `is_training` is true, call `init_training_state` to create a `TrainState` object that includes the optimizer state.",
                "If `is_training` is false, call `init_decode_state` to create a `TrainState` object without an optimizer state.",
                "Return the created `TrainState` object."
            ],
            "output": {
                "shape": "Returns a `flax.training.train_state.TrainState` object, which is a PyTree containing model parameters, an apply function, and optionally an optimizer state."
            },
            "dependencies": [
                "numpy",
                "jax.numpy",
                "multimodal_utils.get_dummy_image_shape_for_init",
                "init_training_state",
                "init_decode_state"
            ],
            "parameters": {
                "is_training": "A boolean flag that determines whether to initialize a state for training (with optimizer) or for decoding (without optimizer).",
                "config.micro_batch_size_to_train_on": "The batch size used for creating dummy input tensors for initialization.",
                "config.max_target_length": "The sequence length used for creating dummy text input tensors for initialization.",
                "config.use_multimodal": "A boolean flag to control the creation of dummy image inputs for multimodal model initialization.",
                "config.model_name": "The name of the model, passed to `get_dummy_image_shape_for_init`."
            },
            "notes": [
                "The function is designed to be compatible with JAX's JIT compilation. Static objects like `model`, `tx`, and `config` are passed as arguments to prevent re-instantiation, which would cause issues with JAX's object hashing and pjit annotations.",
                "It uses dummy tensors filled with ones to trigger the model's `init` method, which builds and returns the initial model parameters."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_decode_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_decode_state(model, config, rng, mesh, checkpoint_manager):\n  \"\"\"Setup decode state by loading params from a checkpoint.\n  Args:\n    model: the flax model to initialize\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: Checkpoint manager\n\n  Returns:\n    state: state with decode params loaded from the checkpoint\n    state_mesh_annotations: the mesh annotations for the state\n  \"\"\"\n  if not config.load_parameters_path:\n    # generate random params\n    max_logging.log(\"No decode checkpoint specified - generating random weights.\")\n    state, state_mesh_annotations, _, _ = setup_initial_state(\n        model, None, None, config, rng, mesh, checkpoint_manager, False\n    )\n  else:\n    # Load params from checkpoint\n    max_logging.log(f\"Loading decode params from {config.load_parameters_path}\")\n    unboxed_abstract_state, state_mesh_annotations, _ = get_abstract_state(model, None, config, rng, mesh, False)\n    with nn_partitioning.axis_rules(config.logical_axis_rules):\n      params = checkpointing.load_params_from_path(\n          config.load_parameters_path,\n          unboxed_abstract_state.params,\n          config.checkpoint_storage_concurrent_gb,\n          config.checkpoint_storage_use_ocdbt,\n          config.checkpoint_storage_use_zarr3,\n      )\n    state = init_decode_state(None, params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n  return state, state_mesh_annotations",
        "analysis": {
            "module_type": "decode_state_initializer",
            "purpose": "Initializes a model's state for decoding, either by loading parameters from a checkpoint or by creating new random parameters.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if `config.load_parameters_path` is provided.",
                "If a path is provided, get the abstract state shape and sharding, then load parameters from the checkpoint using `checkpointing.load_params_from_path`.",
                "If no path is provided, call `setup_initial_state` to generate a state with random weights.",
                "Initialize a `flax.training.train_state.TrainState` for decoding using `init_decode_state` (if loading from checkpoint).",
                "Unbox the logically partitioned state using `max_utils.unbox_logicallypartioned`.",
                "Return the final state and its corresponding mesh annotations."
            ],
            "output": {
                "shape": "Returns a tuple: (state, state_mesh_annotations). `state` is a `flax.training.train_state.TrainState` object. `state_mesh_annotations` is a PyTree of sharding specifications."
            },
            "dependencies": [
                "max_logging",
                "setup_initial_state",
                "get_abstract_state",
                "flax.linen.partitioning.axis_rules",
                "checkpointing.load_params_from_path",
                "init_decode_state",
                "max_utils.unbox_logicallypartioned"
            ],
            "parameters": {
                "config.load_parameters_path": "Path to the checkpoint file to load model parameters from. If not provided, random weights are generated."
            },
            "notes": [
                "The function's behavior is conditional on the presence of `config.load_parameters_path`.",
                "The returned state is specifically for decoding and does not contain an optimizer state.",
                "It uses `get_abstract_state` to determine the correct shapes and sharding for the parameters before loading them from a checkpoint."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_training_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_training_state(model, data_iterator, tx, config, rng, mesh, checkpoint_manager):\n  is_training = True\n  return setup_initial_state(\n      model,\n      data_iterator,\n      tx,\n      config,\n      rng,\n      mesh,\n      checkpoint_manager,\n      is_training,\n  )",
        "analysis": {
            "module_type": "training_state_setup_wrapper",
            "purpose": "A wrapper function that initializes the model's state for training by calling `setup_initial_state` with `is_training` set to True.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Set the `is_training` flag to True.",
                "Call the `setup_initial_state` function, passing all arguments along with the `is_training` flag.",
                "Return the result from the `setup_initial_state` call."
            ],
            "output": {
                "shape": "Returns a tuple: (state, state_mesh_annotations, state_mesh_shardings, data_iterator)."
            },
            "dependencies": [
                "setup_initial_state"
            ],
            "parameters": {
                "model": "The Flax model to initialize.",
                "data_iterator": "The iterator for the training dataset.",
                "tx": "The Optax optimizer (GradientTransformation).",
                "config": "The configuration object for the training run.",
                "rng": "The JAX PRNG key for initialization.",
                "mesh": "The JAX device mesh.",
                "checkpoint_manager": "The Orbax checkpoint manager."
            },
            "notes": [
                "This function acts as a convenience wrapper, simplifying the call to `setup_initial_state` for the specific case of setting up a training run.",
                "The core logic for state initialization, including potential checkpoint loading, is handled within the `setup_initial_state` function."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#setup_initial_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def setup_initial_state(\n    model,\n    data_iterator,\n    tx,\n    config,\n    rng,\n    mesh,\n    checkpoint_manager,\n    is_training=True,\n):\n  \"\"\"We initialize the model and optimizer state, and optionally load from a\n  checkpoint as necessary.\n\n  Args:\n    model: the flax model to initialize\n    tx: the optax.GradientTransformation\n    config: config object\n    rng: jax.prng key\n    mesh: jax.devices() mesh\n    checkpoint_manager: an Orbax checkpointing.CheckpointManager object\n    is_training: True to initialize training state, False for decode state\n\n  Returns:\n    state: the initialized train state\n    state_mesh_annotations: the mesh annotations for the train state\n  \"\"\"\n\n  unboxed_abstract_state, state_mesh_annotations, state_mesh_shardings = get_abstract_state(\n      model, tx, config, rng, mesh, is_training\n  )\n\n  # Initialization\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    restored, raw_params = checkpointing.load_state_if_possible(\n        checkpoint_manager,\n        data_iterator,\n        config.load_parameters_path,\n        config.load_full_state_path,\n        config.checkpoint_storage_concurrent_gb,\n        unboxed_abstract_state,\n        config.enable_single_replica_ckpt_restoring,\n        config.dataset_type,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n        enable_orbax_v1=config.enable_orbax_v1,\n        checkpoint_conversion_fn=config.checkpoint_conversion_fn,\n        source_checkpoint_layout=config.source_checkpoint_layout,\n    )\n\n    if restored:\n      if isinstance(\n          checkpoint_manager,\n          (\n              emergency_checkpoint_manager.CheckpointManager,\n              emergency_replicator_checkpoint_manager.ReplicatorCheckpointManager,\n          ),\n      ):\n        state = restored\n      else:\n        if \"iter\" in restored and restored[\"iter\"] is not None:\n          data_iterator.local_iterator = restored[\"iter\"]\n        state = restored[\"items\"]\n    else:\n      init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training)\n      init_state_partial.__name__ = \"initialize_state\"\n      # pylint: disable=not-callable\n      state = jax.jit(\n          init_state_partial,\n          in_shardings=None,\n          out_shardings=state_mesh_shardings,\n      )(rng)\n      if raw_params:  # If we loaded a partial state, we need to merge it.\n        state = state.replace(params=raw_params)\n\n  state = max_utils.unbox_logicallypartioned(state)\n\n  return state, state_mesh_annotations, state_mesh_shardings, data_iterator",
        "analysis": {
            "functionality": "Initializes the model and optimizer state, with an option to restore from a checkpoint. It determines the correct sharding for the state, attempts to load a saved state, and if unsuccessful, initializes a new state from scratch.",
            "usage": "This function is called at the beginning of a training or decoding process. It takes a Flax model, a data iterator, an optimizer transformation (tx), a configuration object, a JAX random key, a device mesh, and a checkpoint manager. It returns the initialized or restored state, its mesh annotations and shardings, and the potentially updated data iterator."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_abstract_state",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_abstract_state(model, tx, config, rng, mesh, is_training=True):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n  init_state_partial = functools.partial(init_initial_state, model, tx, config, is_training, rng)\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    abstract_state = jax.eval_shape(init_state_partial)\n\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n\n  state_mesh_shardings = nn.logical_to_mesh_sharding(state_logical_annotations, mesh, config.logical_axis_rules)\n  if is_training and config.optimizer_memory_host_offload:\n    opt_state = jax.tree_util.tree_map(lambda x: x.with_memory_kind(kind=\"pinned_host\"), state_mesh_shardings.opt_state)\n    state_mesh_shardings = state_mesh_shardings.replace(opt_state=opt_state)\n  if is_training and config.parameter_memory_host_offload:\n    assert config.param_scan_axis == 0, \"You must set the scan axis 0 to enable parameter offloading.\"\n\n    def move(path, x):\n      max_logging.log(f\"max_utils.py: Moving {path} to host\")\n      return x.with_memory_kind(kind=\"pinned_host\")\n\n    params = jax.tree_util.tree_map_with_path(move, state_mesh_shardings.params)\n    state_mesh_shardings = state_mesh_shardings.replace(params=params)\n\n  abstract_sharded_state = jax.jit(init_state_partial, in_shardings=None, out_shardings=state_mesh_shardings).eval_shape()\n\n  unboxed_abstract_sharded_state = max_utils.unbox_logicallypartioned(abstract_sharded_state)\n  # Initialization\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return (\n      unboxed_abstract_sharded_state,\n      state_mesh_annotations,\n      state_mesh_shardings,\n  )",
        "analysis": {
            "module_type": "state_abstraction_generator",
            "purpose": "To determine the shapes, data types, and sharding specifications for a model's state (parameters and optimizer state) without performing the actual expensive initialization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Create a partial function `init_state_partial` for state initialization using `functools.partial`.",
                "Use `jax.eval_shape` on `init_state_partial` to get an abstract state with shapes and dtypes.",
                "Extract logical partition specifications from the abstract state using `nn.get_partition_spec`.",
                "Convert logical partition specs to concrete mesh shardings using `nn.logical_to_mesh_sharding`.",
                "Optionally modify mesh shardings to offload optimizer state or parameters to pinned host memory based on the configuration.",
                "Get an abstract representation of the sharded state using `jax.jit(...).eval_shape()`.",
                "Unbox the logically partitioned abstract state using `max_utils.unbox_logicallypartioned`.",
                "Convert logical annotations to mesh annotations using `nn.logical_to_mesh`.",
                "Return the unboxed abstract state, mesh annotations, and mesh shardings."
            ],
            "output": {
                "shape": "A tuple containing: (1) an unboxed abstract state (PyTree of ShapeDtypeStructs), (2) a PyTree of mesh annotations, and (3) a PyTree of jax.sharding.Sharding objects."
            },
            "dependencies": [
                "functools.partial",
                "init_initial_state",
                "jax.eval_shape",
                "flax.linen.partitioning.axis_rules",
                "flax.linen.get_partition_spec",
                "flax.linen.logical_to_mesh_sharding",
                "flax.linen.logical_to_mesh",
                "jax.jit",
                "max_utils.unbox_logicallypartioned"
            ],
            "parameters": {
                "model": "The Flax model for which to create an abstract state.",
                "tx": "The Optax optimizer transformation.",
                "config": "A configuration object containing settings like `logical_axis_rules`, `optimizer_memory_host_offload`, and `parameter_memory_host_offload`.",
                "rng": "A JAX PRNG key for initialization.",
                "mesh": "The JAX device mesh for sharding.",
                "is_training": "A boolean flag to determine whether to include the optimizer state in the abstraction."
            },
            "notes": [
                "This function is a key part of the initialization process in a distributed setting, allowing the determination of memory layout and sharding before allocating and initializing the actual large model state.",
                "It uses abstract evaluation (`jax.eval_shape`) to avoid materializing the full state on a single device.",
                "The `is_training` flag controls whether the optimizer state is included in the abstract state."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_prefill_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_prefill_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (\n        config.micro_batch_size_to_train_on,\n        config.max_prefill_predict_length,\n    )\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(config.model_name, batch_size=config.micro_batch_size_to_train_on)\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_PREFILL,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "module_type": "kv_cache_sharding_annotator",
            "purpose": "Determines the mesh-specific sharding annotations for the model's key-value (KV) cache during the prefill phase by performing a shape evaluation of the model's initialization.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a nested function `init_kv_cache` that initializes the model with dummy inputs in `MODEL_MODE_PREFILL` and returns the 'cache' portion of the model variables.",
                "Create a partial function of `init_kv_cache` with the model and config arguments bound.",
                "Use `jax.eval_shape` to get an abstract representation (shapes and dtypes) of the KV cache without performing actual computation.",
                "Use `flax.linen.get_partition_spec` to derive logical sharding annotations from the abstract KV cache structure.",
                "Use `flax.linen.logical_to_mesh` to convert the logical annotations into physical, mesh-specific sharding annotations.",
                "Return the final mesh annotations."
            ],
            "output": {
                "shape": "A PyTree with the same structure as the model's KV cache, where each leaf is a mesh-specific sharding annotation (`jax.sharding.NamedSharding`)."
            },
            "dependencies": [
                "jax.eval_shape",
                "flax.linen.partitioning.axis_rules",
                "flax.linen.get_partition_spec",
                "flax.linen.logical_to_mesh",
                "functools.partial",
                "multimodal_utils.get_dummy_image_shape_for_init",
                "MaxText.inference.page_manager.PageState"
            ],
            "parameters": {
                "model": "The Flax model to be analyzed.",
                "config": "A configuration object containing parameters like `logical_axis_rules`, `micro_batch_size_to_train_on`, and `max_prefill_predict_length`.",
                "rng": "A JAX PRNG key for model initialization.",
                "mesh": "The JAX device mesh over which the KV cache will be sharded.",
                "page_state": "An optional PageState object for managing paged attention."
            },
            "notes": [
                "This function does not perform any actual computation with model weights; it only analyzes shapes and determines sharding rules.",
                "It specifically initializes the model in `MODEL_MODE_PREFILL` to get the correct KV cache structure for the prefill step of inference.",
                "The returned annotations are used by JAX to correctly distribute the KV cache tensors across the device mesh during inference."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_kv_cache_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_kv_cache_annotations(model, config, rng, mesh, page_state: None | PageState = None):\n  \"\"\"Get a shaped abstraction of the state (including optimizer)\"\"\"\n\n  def init_kv_cache(model, config):\n    input_shape = (config.micro_batch_size_to_train_on, 1)\n    image_shape = multimodal_utils.get_dummy_image_shape_for_init(config.model_name, batch_size=config.micro_batch_size_to_train_on)\n\n    model_vars = model.init(\n        {\"params\": rng, \"dropout\": rng, \"aqt\": rng},\n        jnp.ones(input_shape),\n        jnp.ones(input_shape),\n        encoder_images=jnp.ones(image_shape) if config.use_multimodal else None,\n        model_mode=MODEL_MODE_AUTOREGRESSIVE,\n        slot=0,\n        page_state=page_state,\n    )\n    return model_vars[\"cache\"]\n\n  with nn_partitioning.axis_rules(config.logical_axis_rules):\n    init_kv_cache_partial = functools.partial(init_kv_cache, model, config)\n    abstract_state = jax.eval_shape(init_kv_cache_partial)\n  state_logical_annotations = nn.get_partition_spec(abstract_state)\n  with mesh, nn_partitioning.axis_rules(config.logical_axis_rules):\n    state_mesh_annotations = nn.logical_to_mesh(state_logical_annotations)\n  return state_mesh_annotations",
        "analysis": {
            "module_type": "kv_cache_annotation_utility",
            "purpose": "Initializes a model in autoregressive mode to determine the shape and sharding annotations for its Key-Value (KV) cache.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define a nested function `init_kv_cache` that initializes the model in autoregressive mode with dummy inputs and returns the 'cache' part of the model variables.",
                "Create a partial function from `init_kv_cache` with the model and config arguments pre-filled.",
                "Use `jax.eval_shape` on the partial function to get an abstract representation (shapes and dtypes) of the KV cache without actual computation.",
                "Use `nn.get_partition_spec` to get the logical sharding annotations for the abstract KV cache.",
                "Use `nn.logical_to_mesh` to convert the logical sharding annotations into physical, mesh-specific sharding annotations.",
                "Return the final mesh annotations."
            ],
            "output": {
                "shape": "A PyTree of `jax.sharding.NamedSharding` objects matching the structure of the model's KV cache."
            },
            "dependencies": [
                "functools.partial",
                "jax.eval_shape",
                "flax.linen.partitioning.axis_rules",
                "flax.linen.get_partition_spec",
                "flax.linen.logical_to_mesh",
                "multimodal_utils.get_dummy_image_shape_for_init",
                "MaxText.common_types.MODEL_MODE_AUTOREGRESSIVE",
                "MaxText.inference.page_manager.PageState"
            ],
            "parameters": {
                "model": "The Flax model to be initialized.",
                "config": "A configuration object containing parameters like `micro_batch_size_to_train_on`, `logical_axis_rules`, and `use_multimodal`.",
                "rng": "A JAX random number generator key used for model initialization.",
                "mesh": "The JAX device mesh used to determine the physical sharding.",
                "page_state": "An optional `PageState` object for models using paged attention."
            },
            "notes": [
                "This function is used to determine how the KV cache should be sharded across devices for efficient inference.",
                "It cleverly avoids allocating memory for the full KV cache by using `jax.eval_shape`.",
                "The model is initialized with an input sequence length of 1, which is characteristic of the single-token generation step in autoregressive decoding."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#save_quantized_checkpoint_if_configured",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def save_quantized_checkpoint_if_configured(config, params):\n  \"\"\"Save quantized checkpoint if configured\"\"\"\n  assert config.quantization, \"quantization must be configured\"\n  if config.save_quantized_params_path:\n    checkpointing.save_params_to_path(\n        checkpoint_dir=config.save_quantized_params_path,\n        params=params,\n        use_ocdbt=config.checkpoint_storage_use_ocdbt,\n        use_zarr3=config.checkpoint_storage_use_zarr3,\n    )\n  else:\n    max_logging.log(\"Skipping saving quantized checkpoint as save_quantized_params_path is null.\")",
        "analysis": {
            "functionality": "Conditionally saves quantized model parameters to a specified path based on configuration settings.",
            "usage": "Call this function with a configuration object and a PyTree of model parameters. If `config.quantization` is True and `config.save_quantized_params_path` is set, it saves the parameters to that path using `checkpointing.save_params_to_path`. If `config.quantization` is False, it raises an AssertionError. If `config.save_quantized_params_path` is not set, it logs a message and does nothing."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#add_config_to_summary_writer",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def add_config_to_summary_writer(config, summary_writer):\n  \"\"\"Writes config params to tensorboard\"\"\"\n  if jax.process_index() == 0:\n    for key, value in config.get_keys().items():\n      max_utils.add_text_to_summary_writer(key, str(value), summary_writer)",
        "analysis": {
            "module_type": "tensorboard_config_logger",
            "purpose": "Writes key-value pairs from a configuration object to a TensorBoard summary writer, executing only on the main process.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Check if the current process is the main one (jax.process_index() == 0).",
                "Iterate through key-value pairs from the config object's `get_keys()` method.",
                "For each item, call `max_utils.add_text_to_summary_writer` to log the key and its string-converted value."
            ],
            "output": {
                "shape": "N/A"
            },
            "dependencies": [
                "jax.process_index",
                "max_utils.add_text_to_summary_writer"
            ],
            "parameters": {
                "config": "A configuration object with a `get_keys()` method that returns a dictionary of parameters to be logged.",
                "summary_writer": "An instance of a summary writer (e.g., from TensorBoard) to which the text summaries will be written."
            },
            "notes": [
                "This function has a side effect of writing to a summary writer and does not return any value.",
                "The operation is conditional and will only run on the device with process index 0 to avoid redundant logging in a distributed environment."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#logical_axis_rules_pp_act_as_dp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def logical_axis_rules_pp_act_as_dp(logical_rules):\n  \"\"\"Add stage as a physical axes before data for each rule, so stage acts just like data instead of PP.\n  This is used when we want to pipeline only a subset of layers, and leave the rest like DP.\n  \"\"\"\n  new_rules = []\n  for key, physical_axes in logical_rules:\n    if isinstance(physical_axes, str):\n      physical_axes = (physical_axes,)\n    else:\n      physical_axes = tuple(physical_axes)\n    new_physical_axes = tuple(axis for axis in physical_axes if axis != \"stage\")\n    if \"data\" in new_physical_axes:\n      data_idx = new_physical_axes.index(\"data\")\n      new_physical_axes = new_physical_axes[0:data_idx] + (\"stage\",) + new_physical_axes[data_idx:]\n    new_rules.append((key, new_physical_axes))\n  return tuple(new_rules)",
        "analysis": {
            "functionality": "Modifies a list of JAX logical axis rules to treat the 'stage' axis (typically for pipeline parallelism) as a data parallel axis. It achieves this by removing 'stage' from its original position and re-inserting it just before the 'data' axis in each rule.",
            "usage": "Call this function with a list or tuple of logical axis rules. The input `logical_rules` should be an iterable of `(key, physical_axes)` tuples. The function returns a new tuple of rules where the 'stage' axis is positioned to mimic data parallelism. This is used to configure hybrid parallelism schemes where some layers are pipelined and others are data-parallel."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_device_mesh",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_device_mesh(config, devices=None):\n  \"\"\"Creates a device mesh with each slice in its own data parallel group. If there is only one slice, uses two replicas\"\"\"\n  if devices is None:\n    devices = jax.devices()\n  if config.subslice_shape and config.enable_single_controller and config.num_slices == 1:\n    max_logging.log(f\"Trying to create a subslice with shape: {config.subslice_shape}\")\n    subslice_shape = tuple(int(x) for x in config.subslice_shape.split(\",\"))\n    device_coords = [device.coords for device in devices]\n    device_coords_np = np.array(device_coords)\n\n    # Find the minimum coordinates to start the subslice\n    min_coords = device_coords_np.min(axis=0)\n\n    subslice_devices = []\n    for device in devices:\n      coords = device.coords\n      if all(min_coords[i] <= coords[i] < min_coords[i] + subslice_shape[i] for i in range(len(subslice_shape))):\n        subslice_devices.append(device)\n    devices = subslice_devices\n\n  num_devices = len(devices)\n  num_slices = 1 if config.inference_benchmark_test else config.num_slices\n  num_devices_per_slice = num_devices // num_slices\n\n  multi_slice_env = num_slices > 1\n\n  # Find possible unspecified parallelisms\n  ici_parallelism = max_utils.fill_unspecified_mesh_axes(config.ici_parallelism.copy(), num_devices_per_slice, \"ICI\")\n\n  allow_split_physical_axes = config.allow_split_physical_axes if config.allow_split_physical_axes else False\n\n  if multi_slice_env:\n    dcn_parallelism = max_utils.fill_unspecified_mesh_axes(config.dcn_parallelism.copy(), num_slices, \"DCN\")\n    if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n      mesh = max_utils.create_custom_device_mesh(ici_parallelism, dcn_parallelism, devices, config.custom_mesh)\n    else:\n      mesh = mesh_utils.create_hybrid_device_mesh(\n          ici_parallelism,\n          dcn_parallelism,\n          devices,\n          allow_split_physical_axes=allow_split_physical_axes,\n      )\n  else:\n    if allow_split_physical_axes:\n      if max_utils.is_valid_custom_mesh(ici_parallelism, config.custom_mesh):\n        mesh = mesh_utils.create_device_mesh(\n            [16, 16],\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=False,\n        )\n        mesh = max_utils.reshape_mesh_to_rings(mesh, config.custom_mesh)\n        mesh = np.reshape(mesh, ici_parallelism)\n      else:\n        mesh = mesh_utils.create_device_mesh(\n            ici_parallelism,\n            devices,\n            contiguous_submeshes=False,\n            allow_split_physical_axes=allow_split_physical_axes,\n        )\n    else:\n      mesh = mesh_utils.create_device_mesh(\n          ici_parallelism,\n          devices,\n      )\n      if config.optimize_mesh_for_tpu_v6e:\n        mesh = max_utils.optimize_mesh_for_tpu_v6e(mesh, devices)\n\n  max_logging.log(f\"Num_devices: {num_devices}, shape {mesh.shape}\")\n\n  return mesh",
        "analysis": {
            "functionality": "Creates a JAX device mesh for single-slice or multi-slice environments based on a configuration object. It supports creating a mesh from a subset of devices (a 'sub-slice'), handling custom topologies, and applying hardware-specific optimizations.",
            "usage": "Call this function with a configuration object and an optional list of JAX devices. It returns a numpy array representing the device mesh, which is used to distribute computations across hardware. The configuration object should specify parameters like `num_slices`, `ici_parallelism`, and `dcn_parallelism` to define the mesh topology."
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#create_learning_rate_schedule",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def create_learning_rate_schedule(config):\n  \"\"\"Creates a warmup and cosine decay learning rate schedule:\n  We take inspiration from Llama2's learning rate (LR) schedule, see https://arxiv.org/pdf/2307.09288.pdf section 2.2\n  Learning rate schedule has either two or three parts:\n  1) Linear warmup from 0 to [learning_rate] over steps 0 to [learning_rate_schedule_steps * warmup_steps_fraction]\n  2) Cosine from [learning_rate] to [learning_rate * cosine_learning_rate_final_fraction] until learning_rate_schedule_steps\n  3) Constant learning rate of 0 from learning_rate_schedule_steps to steps.\n  The zero learning rate section can be used to more accurately measure the fully trained model's performance.\n  \"\"\"\n\n  def make_cos_schedule(init_lr, final_lr, len_steps):\n    def schedule(step):\n      pct = (step) / len_steps\n      a = 0.5 * (jnp.cos(jnp.pi * pct) + 1)\n      lr = init_lr * a + final_lr * (1 - a)\n      return lr\n\n    return schedule\n\n  lr = config.learning_rate\n  cos_final_lr = lr * config.cosine_learning_rate_final_fraction\n\n  warmup_steps = int(config.learning_rate_schedule_steps * config.warmup_steps_fraction)\n  cos_steps = config.learning_rate_schedule_steps - warmup_steps\n  constant_zero_steps = config.steps - config.learning_rate_schedule_steps\n\n  warmup_schedule = optax.linear_schedule(init_value=0.0, end_value=lr, transition_steps=warmup_steps)\n  cos_schedule = make_cos_schedule(lr, cos_final_lr, cos_steps)\n  constant_schedule = optax.constant_schedule(0.0)\n\n  pieces = [warmup_schedule, cos_schedule]\n  boundaries = [\n      warmup_steps,\n      warmup_steps + cos_steps,\n  ]\n\n  if constant_zero_steps > 0:\n    pieces.append(constant_schedule)\n    boundaries.append(warmup_steps + cos_steps + constant_zero_steps)\n\n  return optax.join_schedules(pieces, boundaries)",
        "analysis": {
            "module_type": "learning_rate_schedule",
            "purpose": "Creates a multi-part learning rate schedule consisting of a linear warmup, cosine decay, and an optional final constant zero phase, inspired by the Llama2 paper.",
            "input": {
                "shape": "N/A",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Define an inner helper function `make_cos_schedule` to generate the cosine decay part of the schedule.",
                "Calculate the number of steps for the warmup, cosine decay, and optional constant zero phases based on the config.",
                "Create the linear warmup schedule using `optax.linear_schedule`.",
                "Create the cosine decay schedule using the `make_cos_schedule` helper function.",
                "Create the constant zero schedule using `optax.constant_schedule`.",
                "Conditionally append the constant zero schedule if its duration is greater than zero.",
                "Combine the individual schedules into a single piecewise schedule using `optax.join_schedules`."
            ],
            "output": {
                "shape": "A callable Optax schedule function that takes a step count and returns a learning rate."
            },
            "dependencies": [
                "optax.linear_schedule",
                "optax.constant_schedule",
                "optax.join_schedules",
                "jax.numpy"
            ],
            "parameters": {
                "config.learning_rate": "The peak learning rate after the warmup phase.",
                "config.cosine_learning_rate_final_fraction": "The fraction of the peak learning rate to decay to at the end of the cosine schedule.",
                "config.learning_rate_schedule_steps": "The total number of steps for the warmup and cosine decay phases combined.",
                "config.warmup_steps_fraction": "The fraction of `learning_rate_schedule_steps` to be used for linear warmup.",
                "config.steps": "The total number of training steps, used to determine the length of the final constant zero phase."
            },
            "notes": [
                "The schedule consists of two or three distinct parts: linear warmup, cosine decay, and an optional constant zero period.",
                "The final constant zero learning rate section is intended to allow for more accurate performance measurement of the fully trained model without further weight updates."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_formatted_sharding_annotations",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_formatted_sharding_annotations(params, mesh=None):\n  \"\"\"\n  Generates a readable string report of sharding annotations for all parameters.\n\n  This function iterates through a PyTree of model parameters and inspects the\n  sharding information attached to each parameter (leaf). It creates a\n  human-readable summary that is useful for debugging sharding configurations.\n\n  Args:\n    params: The PyTree of model parameters to inspect.\n    mesh: (Optional) The device mesh. If provided, its axis names and shape\n          are included in the report for additional context.\n\n  Returns:\n    A single string containing the formatted report of sharding annotations\n    for every parameter, with each entry on a new line.\n  \"\"\"\n  # Initialize a list to hold the lines of the report, starting with a title.\n  annotation_lines = [\"Comprehensice Weight Sharding Annotations:\"]\n\n  # If a mesh object is provided, add its details to the report header.\n  if mesh:\n    annotation_lines.append(f\"Mesh axes: {mesh.axis_names}, Mesh shape: {mesh.shape}\")\n    annotation_lines.append(\"-\" * 30)\n\n  # Get a flattened list of all parameters (leaves) and their corresponding paths in the PyTree.\n  all_params_leaves = jtu.tree_leaves_with_path(params)\n\n  # Loop through each parameter leaf in the flattened list.\n  for path, p_leaf in all_params_leaves:\n    # Convert the parameter's path (a sequence of keys) into a readable string name.\n    param_name_str = jtu.keystr(path)\n    # Get the shape of the parameter as a string.\n    shape_str = str(p_leaf.shape)\n    # Set a default description for sharding, in case none is found.\n    sharding_desc = \"N/A\"\n\n    # Check if the parameter leaf has a 'sharding' attribute.\n    if hasattr(p_leaf, \"sharding\"):\n      # Case 1: Standard JAX sharding with a PartitionSpec.\n      if hasattr(p_leaf.sharding, \"spec\") and p_leaf.sharding.spec is not None:\n        # The spec is a tuple (PartitionSpec), format it for readability.\n        spec_parts = []\n        for item in p_leaf.sharding.spec:\n          # Represent None as \"Replicated\" to make it explicit.\n          spec_parts.append(str(item) if item is not None else \"Replicated\")\n        sharding_desc = f\"PartitionSpec({', '.join(spec_parts)})\"\n      # Case 2: The parameter is explicitly marked as fully replicated.\n      elif hasattr(p_leaf.sharding, \"spec\") and p_leaf.sharding.spec is None:\n        sharding_desc = \"Fully Replicated (spec is None)\"\n      # Case 3: A generic fallback if a sharding object exists but has no recognized spec attribute.\n      else:\n        # Print the string representation of the sharding object itself.\n        sharding_desc = str(p_leaf.sharding)\n    # Case 4: The parameter has no .sharding attribute at all.\n    else:\n      sharding_desc = \"No .sharding attribute found\"\n\n    # Append the formatted details for the current parameter to our list of lines.\n    annotation_lines.append(f\" - Param: {param_name_str}\\n\" f\"   Shape: {shape_str}\\n\" f\"   Sharding: {sharding_desc}\")\n  # Join all the collected lines into a single string, separated by newlines.\n  return \"\\n\".join(annotation_lines)",
        "analysis": {
            "module_type": "sharding_annotation_reporter",
            "purpose": "Generates a human-readable string report detailing the sharding annotations for each parameter in a PyTree, which is useful for debugging sharding configurations.",
            "input": {
                "shape": "A PyTree of tensors (e.g., model parameters).",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Initialize a list for report lines, adding a title and optional mesh details.",
                "Flatten the input `params` PyTree into a list of (path, leaf) pairs using `jtu.tree_leaves_with_path`.",
                "Iterate through each parameter leaf in the flattened list.",
                "For each parameter, extract its string name, shape, and sharding information by inspecting its attributes.",
                "Determine the sharding description by checking for `.sharding` and `.sharding.spec` attributes, handling cases for PartitionSpec, full replication, and missing attributes.",
                "Format the parameter's name, shape, and sharding description into a multi-line string.",
                "Append the formatted string for the current parameter to the list of report lines.",
                "Join all collected lines into a single string, separated by newlines."
            ],
            "output": {
                "shape": "A single string containing the formatted report."
            },
            "dependencies": [
                "jax.tree_util as jtu"
            ],
            "parameters": {
                "params": "The PyTree of model parameters to inspect.",
                "mesh": "An optional JAX device mesh. If provided, its axis names and shape are included in the report for context."
            },
            "notes": [
                "The function handles several cases for sharding: a valid PartitionSpec, an explicitly replicated parameter (spec is None), a generic sharding object, and the absence of a sharding attribute.",
                "The term 'Replicated' is used in the output string to explicitly denote dimensions that are not sharded (where a PartitionSpec item is None)."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#get_physical_spec_no_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def get_physical_spec_no_fsdp(full_logical, mesh, logical_axis_rules):\n  \"\"\"\n  Generates a physical sharding spec for fully replicated weights.\n\n  This function computes a target sharding layout where model parameters are fully\n  replicated across the 'fsdp' mesh axis. It starts with the original logical\n  sharding and removes any rules that shard along the 'fsdp' or\n  'fsdp_transpose' axes.\n\n  Replacing a sharding axis with `None` in a PartitionSpec instructs JAX to\n  replicate the array data along that physical mesh dimension. The resulting\n  specification is used as a target layout for an all-gather operation.\n\n  Args:\n    full_logical: A PyTree of logical PartitionSpecs for the model parameters.\n    mesh: The JAX device mesh.\n    logical_axis_rules: Rules for converting logical axes to physical mesh axes.\n\n  Returns:\n    A PyTree of physical `jax.sharding.NamedSharding` objects that describe a\n    layout where parameters are fully gathered (replicated) across the 'fsdp'\n    mesh axis.\n  \"\"\"\n\n  def remove_fsdp_sharding(sharding_tree):\n    \"\"\"Recursively traverses the sharding tree to remove fsdp axes.\"\"\"\n\n    def _remove_fsdp_from_partition_spec(named_sharding):\n      \"\"\"Removes 'fsdp' and 'fsdp_transpose' from a PartitionSpec.\"\"\"\n      if isinstance(named_sharding, jax.sharding.NamedSharding):\n        new_spec = []\n        # Iterate through each axis in the original PartitionSpec.\n        for axis in named_sharding.spec:\n          if axis is None:\n            new_spec.append(None)\n          elif isinstance(axis, str):\n            # If the axis is 'fsdp', replace it with None to signify replication.\n            if axis not in (\"fsdp\", \"fsdp_transpose\"):\n              new_spec.append(axis)\n            else:\n              new_spec.append(None)\n          elif isinstance(axis, (list, tuple)):\n            # If the axis is a collection, filter out 'fsdp'.\n            new_axis = [a for a in axis if a not in (\"fsdp\", \"fsdp_transpose\")]\n            new_spec.append(tuple(new_axis))\n          else:\n            raise ValueError(f\"Unsupported_axis_type: {type(axis)}\")\n          # Return a new sharding object with the modified spec.\n        return jax.sharding.NamedSharding(named_sharding.mesh, jax.sharding.PartitionSpec(*new_spec))\n      return named_sharding\n\n    return jax.tree.map(_remove_fsdp_from_partition_spec, sharding_tree)\n\n  # Convert the high-level logical spec to a physical one using default rules.\n  physical = nn.logical_to_mesh_sharding(full_logical, mesh=mesh, rules=logical_axis_rules)\n  # Apply the function to remove the FSDP sharding, defining our target layout.\n  physical_no_fsdp = remove_fsdp_sharding(physical)\n  return physical_no_fsdp",
        "analysis": {
            "module_type": "fsdp_sharding_remover",
            "purpose": "Computes a physical sharding specification (a PyTree of `jax.sharding.NamedSharding` objects) where tensors are fully replicated across the 'fsdp' and 'fsdp_transpose' mesh axes.",
            "input": {
                "shape": "A PyTree of logical `jax.sharding.PartitionSpec` objects.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Convert the input logical sharding PyTree (`full_logical`) to a physical sharding PyTree of `jax.sharding.NamedSharding` objects using `nn.logical_to_mesh_sharding`.",
                "Define and apply a nested function `remove_fsdp_sharding` that traverses the physical sharding PyTree using `jax.tree.map`.",
                "For each `jax.sharding.NamedSharding` leaf, iterate through its `PartitionSpec`.",
                "Replace any axis named 'fsdp' or 'fsdp_transpose' with `None` to signify replication.",
                "Construct a new `jax.sharding.NamedSharding` object with the modified `PartitionSpec`.",
                "Return the resulting PyTree of modified physical sharding specifications."
            ],
            "output": {
                "shape": "A PyTree of physical `jax.sharding.NamedSharding` objects with the same structure as the input `full_logical`."
            },
            "dependencies": [
                "flax.linen.logical_to_mesh_sharding",
                "jax.tree.map",
                "jax.sharding.NamedSharding",
                "jax.sharding.PartitionSpec"
            ],
            "parameters": {
                "full_logical": "A PyTree of logical PartitionSpecs for the model parameters.",
                "mesh": "The JAX device mesh.",
                "logical_axis_rules": "Rules for converting logical axes to physical mesh axes."
            },
            "notes": [
                "This function is used to generate a target layout for an all-gather operation.",
                "Replacing a sharding axis with `None` in a `PartitionSpec` instructs JAX to replicate the array data along that physical mesh dimension."
            ]
        }
    },
    {
        "block_name": "src/MaxText/maxtext_utils.py#all_gather_over_fsdp",
        "file_path": "src/MaxText/maxtext_utils.py",
        "code_block": "def all_gather_over_fsdp(variables, sharding_info, mesh, logical_axis_rules):\n  \"\"\"Performs an all-gather on FSDP-sharded variables via a sharding constraint.\n  This function triggers an all-gather operation on the model's parameters.\n  It does so by applying a sharding constraint that specifies a fully\n  replicated layout.\n\n  The JAX compiler satisfies this constraint by automatically inserting the\n  necessary `all-gather` collective communication operations into the\n  computation graph, effectively gathering the sharded weights.\n\n  Args:\n    variables: The PyTree of model parameters, currently sharded across devices.\n    sharding_info: The logical partition spec of the currently sharded `variables`.\n    mesh: The JAX device mesh.\n    logical_axis_rules: Rules for converting logical axes to physical mesh axes.\n\n  Returns:\n    The model's variables with the all-gather operation applied, resulting\n    in the weights being fully replicated on all devices in the 'fsdp' mesh.\n  \"\"\"\n  # Get the target physical layout (weights fully replicated).\n  physical_constraint_no_fsdp = get_physical_spec_no_fsdp(sharding_info, mesh, logical_axis_rules)\n  # Apply the constraint to the model's current variables. This tells JAX to\n  # gather the weights into this layout.\n  return jax.lax.with_sharding_constraint(variables, physical_constraint_no_fsdp)",
        "analysis": {
            "module_type": "sharding_utility",
            "purpose": "Performs an all-gather on FSDP-sharded variables by applying a sharding constraint that specifies a fully replicated layout, which triggers JAX to insert an all-gather collective.",
            "input": {
                "shape": "A PyTree of tensors (variables) with arbitrary shapes.",
                "dtype": "N/A"
            },
            "processing_steps": [
                "Calculates the target physical sharding layout for fully replicated weights by calling `get_physical_spec_no_fsdp`.",
                "Applies the calculated replicated layout as a constraint to the input variables using `jax.lax.with_sharding_constraint`."
            ],
            "output": {
                "shape": "Returns the input PyTree of variables with the sharding constraint applied. After compilation, the tensors will be fully replicated on all devices in the 'fsdp' mesh axis."
            },
            "dependencies": [
                "get_physical_spec_no_fsdp",
                "jax.lax.with_sharding_constraint"
            ],
            "parameters": {},
            "notes": [
                "This function does not execute the all-gather immediately. Instead, it adds a constraint to the JAX computation graph, which the compiler satisfies by inserting the necessary collective communication operations.",
                "The primary use case is to gather model weights that are sharded across the 'fsdp' mesh axis."
            ]
        }
    }
]